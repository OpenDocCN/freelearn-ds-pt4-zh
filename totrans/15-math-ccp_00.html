<html><head></head><body>
		<div id="_idContainer001">
			<p class="hidden"><st c="0">15 Math Concepts Every Data Scientist Should Know</st></p>
			<p class="hidden"><st c="49">Understand and learn how to apply the math behind data science algorithms</st></p>
			<p class="hidden"><st c="123">David Hoyle</st></p>
			<p><img src="image/Packt_Logo_New.png" alt="" role="presentation"/><st c="135"/></p>
			<h1 id="_idParaDest-1"><a id="_idTextAnchor000"/><st c="144">15 Math Concepts Every Data Scientist Should Know</st></h1>
			<p><st c="193">Copyright ¬© 2024 </st><span class="No-Break"><st c="211">Packt Publishing</st></span></p>
			<p><em class="italic"><st c="227">All rights reserved</st></em><st c="247">. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles </st><span class="No-Break"><st c="487">or reviews.</st></span></p>
			<p><st c="498">Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. </st><st c="611">However, the information contained in this book is sold without warranty, either express or implied. </st><st c="712">Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by </st><span class="No-Break"><st c="886">this book.</st></span></p>
			<p><st c="896">Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. </st><st c="1061">However, Packt Publishing cannot guarantee the accuracy of </st><span class="No-Break"><st c="1120">this information.</st></span></p>
			<p><strong class="bold"><st c="1137">Group Product Manager</st></strong><st c="1159">: </st><span class="No-Break"><st c="1162">Niranjan Naikwadi</st></span></p>
			<p><strong class="bold"><st c="1179">Publishing Product Manager</st></strong><st c="1206">: Yasir </st><span class="No-Break"><st c="1215">Ali Khan</st></span></p>
			<p><strong class="bold"><st c="1223">Content Development Editor</st></strong><st c="1250">: </st><span class="No-Break"><st c="1253">Joseph Sunil</st></span></p>
			<p><strong class="bold"><st c="1265">Technical Editor</st></strong><st c="1282">: </st><span class="No-Break"><st c="1285">Seemanjay Ameriya</st></span></p>
			<p><strong class="bold"><st c="1302">Copy Editor</st></strong><st c="1314">: </st><span class="No-Break"><st c="1317">Safis Editing</st></span></p>
			<p><strong class="bold"><st c="1330">Project Coordinator</st></strong><st c="1350">: </st><span class="No-Break"><st c="1353">Urvi Sharma</st></span></p>
			<p><strong class="bold"><st c="1364">Proofreader</st></strong><st c="1376">: </st><span class="No-Break"><st c="1379">Safis Editing</st></span></p>
			<p><strong class="bold"><st c="1392">Indexer</st></strong><st c="1400">: </st><span class="No-Break"><st c="1403">Hemangini Bari</st></span></p>
			<p><strong class="bold"><st c="1417">Production Designer</st></strong><st c="1437">: </st><span class="No-Break"><st c="1440">Joshua Misquitta</st></span></p>
			<p><strong class="bold"><st c="1456">Marketing Coordinator</st></strong><st c="1478">: </st><span class="No-Break"><st c="1481">Vinishka Kalra</st></span></p>
			<p><st c="1495">First published: </st><span class="No-Break"><st c="1513">July 2024</st></span></p>
			<p><st c="1522">Production </st><span class="No-Break"><st c="1534">reference: 1190724</st></span></p>
			<p><st c="1552">Published by Packt </st><span class="No-Break"><st c="1572">Publishing Ltd.</st></span></p>
			<p><span class="No-Break"><st c="1587">Grosvenor House</st></span></p>
			<p><st c="1603">11 St </st><span class="No-Break"><st c="1610">Paul‚Äôs Square</st></span></p>
			<p><span class="No-Break"><st c="1623">Birmingham</st></span></p>
			<p><st c="1634">B3 </st><span class="No-Break"><st c="1638">1RB, UK</st></span></p>
			<p><span class="No-Break"><st c="1645">ISBN 978-1-83763-418-7</st></span></p>
			<p><a href="http://www.packtpub.com"><span class="No-Break"><st c="1668">www.packtpub.com</st></span></a></p>
			

</div>
<div id="charCountTotal" value="1685"/>

<div id="_idContainer001">
<div class="dedication">
<p class="author-quote"><st c="0">To my wife Clare for her unwavering love, support, and inspiration throughout our life together.</st></p>
			<p class="author-quote"><st c="97">‚Äì David Hoyle</st></p>
			
</div>
</div>
<div id="charCountTotal" value="111"/>

<div id="_idContainer001"><h1 id="_idParaDest-2"><a id="_idTextAnchor001"/><st c="0">Contributors</st></h1>
			<h1 id="_idParaDest-3"><a id="_idTextAnchor002"/><st c="13">About the author</st></h1>
			<p><strong class="bold"><st c="30">David Hoyle</st></strong><st c="42"> has over 30 years‚Äô experience in machine learning, statistics, and mathematical modeling. </st><st c="133">He gained a BSc. </st><st c="150">degree in mathematics and physics and a Ph.D. </st><st c="196">in theoretical physics, both from the University of Bristol, UK. </st><st c="261">He then embarked on an academic career that included research at the University of Cambridge and leading his own research groups as an Associate Professor at the University of Exeter and the University of Manchester in the UK. </st><st c="488">For the last 13 years, he has worked in the commercial sector, including for Lloyds Banking Group ‚Äì one of the UK‚Äôs largest retail banks, and as joint Head of Data Science for AutoTrader UK. </st><st c="679">He now works for the global customer data science company dunnhumby, building statistical and machine learning models for the world‚Äôs largest retailers, including Tesco UK and Walmart. </st><st c="864">He lives and works in </st><span class="No-Break"><st c="886">Manchester, UK.</st></span></p>
			<p class="author-quote"><st c="901">This has been a long endeavor. </st><st c="933">I would like to thank my wife and children for their encouragement, and the team at Packt for their patience and support throughout the process.</st></p>
			

</div>
<div id="charCountTotal" value="1077"/>

<div id="_idContainer001"><h1 id="_idParaDest-4"><a id="_idTextAnchor003"/><st c="0">About the reviewer</st></h1>
			<p><strong class="bold"><st c="19">Emmanuel Nyatefe</st></strong><st c="36"> is a data analyst with over 5 years of experience in data analytics, AI, and ML. </st><st c="118">He holds a Masters of Science in Business Analytics from the W. </st><st c="182">P. </st><st c="185">Carey School of Business at Arizona State University and a Bachelors of Science in Business Information Technology from Kwame Nkrumah University of Science and Technology. </st><st c="357">He has led various AI and ML projects, including developing models for detecting crop diseases and applying Generative AI to innovate business solutions and optimize operations. </st><st c="535">His expertise in data engineering, modeling, and visualization, alongside his proficiency in LLMs and advanced analytics, highlights his significant contributions to data science. </st><st c="715">His dedication to data-driven innovation is evident in his </st><span class="No-Break"><st c="774">book review.</st></span></p>
		</div>
	<div id="charCountTotal" value="786"/>

		<div id="_idContainer005" class="Content" epub:type="toc">&#13;
			<h1><st c="0">Table of Contents</st></h1>&#13;
			<h2><a href="B19496_Preface.xhtml#_idTextAnchor004"><st c="18">Preface</st></a></h2>&#13;
			<h2><a href="B19496_Part_1.xhtml#_idTextAnchor013"><st c="26">Part 1: Essential Concepts</st></a></h2>&#13;
			<h2><a href="B19496_01.xhtml#_idTextAnchor014"><st c="53">1</st></a></h2>&#13;
			<h2><a href="B19496_01.xhtml#_idTextAnchor015"><st c="55">Recap of Mathematical Notation and Terminology</st></a></h2>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor016"><st c="101">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor017"><st c="124">Number systems</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor018"><st c="139">Notation for numbers and fields</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor020"><st c="171">Complex numbers</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor022"><st c="187">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor023"><st c="203">Linear algebra</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor024"><st c="218">Vectors</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor025"><st c="226">Matrices</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor027"><st c="235">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor028"><st c="251">Sums, products, and logarithms</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor029"><st c="282">Sums and the </st><span class="_-----MathTools-_Math_Symbol_Extended"><st c="296">ùö∫</st></span><st c="298"> notation</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor034"><st c="307">Products and the </st><img src="image/1.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Œ†&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.675em;width:0.779em"/><st c="325"/><st c="326"> notation</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor035"><st c="335">Logarithms</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor037"><st c="346">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor038"><st c="362">Differential and integral calculus</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor039"><st c="397">Differentiation</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor041"><st c="413">Finding maxima and minima</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor044"><st c="439">Integration</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor047"><st c="451">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor048"><st c="467">Analysis</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor049"><st c="476">Limits</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor050"><st c="483">Order notation</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor053"><st c="498">Taylor series expansions</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor055"><st c="523">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor056"><st c="539">Combinatorics</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor057"><st c="553">Binomial coefficients</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor058"><st c="575">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor059"><st c="591">Summary</st></a></h3>&#13;
			<h3><a href="B19496_01.xhtml#_idTextAnchor060"><st c="599">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_02.xhtml#_idTextAnchor061"><st c="625">2</st></a></h2>&#13;
			<h2><a href="B19496_02.xhtml#_idTextAnchor062"><st c="627">Random Variables and Probability Distributions</st></a></h2>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor063"><st c="673">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor064"><st c="696">All data is random</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor065"><st c="715">A little example</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor067"><st c="732">Systematic variation can be learned ‚Äì random variation can‚Äôt</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor069"><st c="793">Random variation is not just measurement error</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor070"><st c="840">What are the consequences of data being random?</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor071"><st c="888">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor072"><st c="904">Random variables and probability distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor073"><st c="951">A new concept ‚Äì random variables</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor076"><st c="984">Summarizing probability distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor082"><st c="1022">Continuous distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor084"><st c="1047">Transforming and combining random variables</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor093"><st c="1091">Named distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor098"><st c="1111">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor099"><st c="1127">Sampling from distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor100"><st c="1155">How datasets relate to random variables and probability distributions</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor103"><st c="1225">How big is the population from which a dataset is sampled?</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor104"><st c="1284">How to sample</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor106"><st c="1298">Generating your own random numbers code example</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor107"><st c="1346">Sampling from numpy distributions code example</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor108"><st c="1393">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor109"><st c="1409">Understanding statistical estimators</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor113"><st c="1446">Consistency, bias, and efficiency</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor118"><st c="1480">The empirical distribution function</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor124"><st c="1516">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor125"><st c="1532">The Central Limit Theorem</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor126"><st c="1558">Sums of random variables</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor129"><st c="1583">CLT code example</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor131"><st c="1600">CLT example with discrete variables</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor133"><st c="1636">Computational estimation of a PDF from data</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor136"><st c="1680">KDE code example</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor138"><st c="1697">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor139"><st c="1713">Summary</st></a></h3>&#13;
			<h3><a href="B19496_02.xhtml#_idTextAnchor140"><st c="1721">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_03.xhtml#_idTextAnchor141"><st c="1731">3</st></a></h2>&#13;
			<h2><a href="B19496_03.xhtml#_idTextAnchor142"><st c="1733">Matrices and Linear Algebra</st></a></h2>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor143"><st c="1760">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor144"><st c="1783">Inner and outer products of vectors</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor145"><st c="1819">Inner product of two vectors</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor147"><st c="1848">Outer product of two vectors</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor148"><st c="1877">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor149"><st c="1893">Matrices as transformations</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor150"><st c="1921">Matrix multiplication</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor154"><st c="1943">The identity matrix</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor155"><st c="1963">The inverse matrix</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor157"><st c="1982">More examples of matrices as transformations</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor165"><st c="2027">Matrix transformation code example</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor166"><st c="2062">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor167"><st c="2078">Matrix decompositions</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor168"><st c="2100">Eigen-decompositions</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor169"><st c="2121">Eigenvector and eigenvalues</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor173"><st c="2149">Eigen-decomposition of a square matrix</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor180"><st c="2188">Eigen-decomposition code example</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor181"><st c="2221">Singular value decomposition</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor185"><st c="2250">The SVD of a complex matrix</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor186"><st c="2278">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor187"><st c="2294">Matrix properties</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor188"><st c="2312">Trace</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor189"><st c="2318">Determinant</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor191"><st c="2330">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor192"><st c="2346">Matrix factorization and dimensionality reduction</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor194"><st c="2396">Dimensionality reduction</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor196"><st c="2421">Principal component analysis</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor206"><st c="2450">Non-negative matrix factorization</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor210"><st c="2484">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor211"><st c="2500">Summary</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor212"><st c="2508">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_03.xhtml#_idTextAnchor215"><st c="2518">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_04.xhtml#_idTextAnchor216"><st c="2544">4</st></a></h2>&#13;
			<h2><a href="B19496_04.xhtml#_idTextAnchor217"><st c="2546">Loss Functions and Optimization</st></a></h2>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor218"><st c="2577">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor219"><st c="2600">Loss functions ‚Äì what are they?</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor220"><st c="2632">Risk functions</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor221"><st c="2647">There are many loss functions</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor222"><st c="2677">Different loss functions = different end results</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor223"><st c="2726">Loss functions for anything</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor224"><st c="2754">A loss function by any other name</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor225"><st c="2788">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor226"><st c="2804">Least Squares</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor227"><st c="2818">The squared-loss function</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor231"><st c="2844">OLS regression</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor233"><st c="2859">OLS, outliers, and robust regression</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor237"><st c="2896">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor238"><st c="2912">Linear models</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor245"><st c="2926">Practical issues</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor246"><st c="2943">The model residuals</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor247"><st c="2963">OLS regression code example</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor249"><st c="2991">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor250"><st c="3007">Gradient descent</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor251"><st c="3024">Locating the minimum of a simple risk function</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor254"><st c="3071">Gradient descent code example</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor256"><st c="3101">Gradient descent is a general technique</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor257"><st c="3141">Beyond simple gradient descent</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor258"><st c="3172">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor259"><st c="3188">Summary</st></a></h3>&#13;
			<h3><a href="B19496_04.xhtml#_idTextAnchor260"><st c="3196">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_05.xhtml#_idTextAnchor261"><st c="3206">5</st></a></h2>&#13;
			<h2><a href="B19496_05.xhtml#_idTextAnchor262"><st c="3208">Probabilistic Modeling</st></a></h2>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor263"><st c="3230">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor264"><st c="3253">Likelihood</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor265"><st c="3264">A simple probabilistic model</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor266"><st c="3293">Log likelihood</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor267"><st c="3308">Maximum likelihood estimation</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor279"><st c="3338">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor280"><st c="3359">Bayes‚Äô theorem</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor281"><st c="3374">Conditional probability and Bayes‚Äô theorem</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor285"><st c="3417">Priors</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor286"><st c="3424">The posterior</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor288"><st c="3438">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor289"><st c="3459">Bayesian modeling</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor290"><st c="3477">Bayesian model averaging</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor292"><st c="3502">MAP estimation</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor295"><st c="3517">As </st><img src="image/2.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.010em;height:0.658em;width:0.703em"/><st c="3521"/><st c="3522"> becomes large the prior becomes irrelevant</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor296"><st c="3565">Least squares as an approximation to Bayesian modeling</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor297"><st c="3620">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor298"><st c="3641">Bayesian modeling in practice</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor299"><st c="3671">Analytic approximation of the posterior</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor304"><st c="3711">Computational sampling</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor307"><st c="3734">MCMC code example</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor309"><st c="3752">Probabilistic programming languages</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor310"><st c="3788">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor311"><st c="3809">Summary</st></a></h3>&#13;
			<h3><a href="B19496_05.xhtml#_idTextAnchor312"><st c="3817">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_Part_2.xhtml#_idTextAnchor313"><st c="3827">Part 2: Intermediate Concepts</st></a></h2>&#13;
			<h2><a href="B19496_06.xhtml#_idTextAnchor314"><st c="3857">6</st></a></h2>&#13;
			<h2><a href="B19496_06.xhtml#_idTextAnchor315"><st c="3859">Time Series and Forecasting</st></a></h2>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor316"><st c="3886">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor317"><st c="3909">What is time series data?</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor319"><st c="3935">What does auto-correlation mean for modeling time series data?</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor320"><st c="3998">The auto-correlation function (ACF)</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor321"><st c="4034">The partial auto-correlation function (PACF)</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor323"><st c="4079">Other data science implications of time series data</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor324"><st c="4131">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor325"><st c="4152">ARIMA models</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor326"><st c="4165">Integrated</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor333"><st c="4176">Auto-regression</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor341"><st c="4192">Moving average</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor344"><st c="4207">Combining the AR(p), I(d), and MA(q) into an ARIMA model</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor348"><st c="4264">Variants of ARIMA modeling</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor353"><st c="4291">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor354"><st c="4312">ARIMA modeling in practice</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor355"><st c="4339">Unit root testing</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor356"><st c="4357">Interpreting ACF and PACF plots</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor359"><st c="4389">auto.arima</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor360"><st c="4400">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor361"><st c="4421">Machine learning approaches to time series analysis</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor362"><st c="4473">Routine application of machine learning to time series analysis</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor363"><st c="4537">Deep learning approaches to time series analysis</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor364"><st c="4586">AutoML approaches to time series analysis</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor365"><st c="4628">What we have learned</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor366"><st c="4649">Summary</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor367"><st c="4657">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_06.xhtml#_idTextAnchor368"><st c="4667">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_07.xhtml#_idTextAnchor369"><st c="4693">7</st></a></h2>&#13;
			<h2><a href="B19496_07.xhtml#_idTextAnchor370"><st c="4695">Hypothesis Testing</st></a></h2>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor371"><st c="4713">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor372"><st c="4736">What is a hypothesis test?</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor373"><st c="4763">Example</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor375"><st c="4771">The general form of a hypothesis test</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor376"><st c="4809">The p-value</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor377"><st c="4821">The effect of increasing sample size</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor378"><st c="4858">The effect of decreasing noise</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor379"><st c="4889">One-tailed and two-tailed tests</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor382"><st c="4921">Using samples variances in the test statistic ‚Äì the t-test</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor388"><st c="4980">Computationally intensive methods for p-value estimation</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor389"><st c="5037">Parametric versus non-parametric hypothesis tests</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor391"><st c="5087">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor392"><st c="5103">Confidence intervals</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor394"><st c="5124">What does a confidence interval really represent?</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor396"><st c="5174">Confidence intervals for any parameter</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor397"><st c="5213">A confidence interval code example</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor398"><st c="5248">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor399"><st c="5264">Type I and Type II errors, and power</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor402"><st c="5301">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor403"><st c="5317">Summary</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor404"><st c="5325">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_07.xhtml#_idTextAnchor405"><st c="5335">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_08.xhtml#_idTextAnchor406"><st c="5361">8</st></a></h2>&#13;
			<h2><a href="B19496_08.xhtml#_idTextAnchor407"><st c="5363">Model Complexity</st></a></h2>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor408"><st c="5379">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor409"><st c="5402">Generalization, overfitting, and the role of model complexity</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor411"><st c="5464">Overfitting</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor413"><st c="5476">Why overfitting is bad</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor415"><st c="5499">Overfitting increases the variability of predictions</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor417"><st c="5552">Underfitting is also a problem</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor420"><st c="5583">Measuring prediction error</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor422"><st c="5610">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor423"><st c="5626">The bias-variance trade-off</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor426"><st c="5654">Proof of the bias-variance trade-off formula</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor435"><st c="5699">Double descent ‚Äì a modern twist on the generalization error diagram</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor438"><st c="5767">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor439"><st c="5783">Model complexity measures for model selection</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor440"><st c="5829">Selecting between classes of models</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor441"><st c="5865">Akaike Information Criterion</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor443"><st c="5894">Bayesian Information Criterion</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor446"><st c="5925">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor447"><st c="5941">Summary</st></a></h3>&#13;
			<h3><a href="B19496_08.xhtml#_idTextAnchor448"><st c="5949">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_09.xhtml#_idTextAnchor449"><st c="5975">9</st></a></h2>&#13;
			<h2><a href="B19496_09.xhtml#_idTextAnchor450"><st c="5977">Function Decomposition</st></a></h2>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor451"><st c="5999">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor452"><st c="6022">Why do we want to decompose a function?</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor453"><st c="6062">What is a decomposition of a function?</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor454"><st c="6101">Example 1 ‚Äì decomposing a one-dimensional function into symmetric and anti-symmetric parts</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor455"><st c="6192">Example 2 ‚Äì decomposing a time series into its seasonal and non-seasonal components</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor457"><st c="6276">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor458"><st c="6295">Expanding a function in terms of basis functions</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor459"><st c="6344">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor460"><st c="6363">Fourier series</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor467"><st c="6378">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor468"><st c="6397">Fourier transforms</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor481"><st c="6416">The multi-dimensional Fourier transform</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor488"><st c="6456">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor489"><st c="6475">The discrete Fourier transform</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor494"><st c="6506">DFT code example</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor496"><st c="6523">Uses of the DFT</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor497"><st c="6539">What is the difference between the DFT, Fourier series, and the Fourier transform?</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor498"><st c="6622">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor499"><st c="6641">Summary</st></a></h3>&#13;
			<h3><a href="B19496_09.xhtml#_idTextAnchor500"><st c="6649">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_10.xhtml#_idTextAnchor501"><st c="6659">10</st></a></h2>&#13;
			<h2><a href="B19496_10.xhtml#_idTextAnchor502"><st c="6662">Network Analysis</st></a></h2>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor503"><st c="6679">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor504"><st c="6702">Graphs and network data</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor505"><st c="6726">Network data is about relationships</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor506"><st c="6762">Example 1 ‚Äì substituting goods in a supermarket</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor508"><st c="6810">Example 2 ‚Äì international trade</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor510"><st c="6842">What is a graph?</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor512"><st c="6859">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor513"><st c="6878">Basic characteristics of graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor514"><st c="6910">Undirected and directed edges</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor518"><st c="6940">The adjacency matrix</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor522"><st c="6961">In-degree and out-degree</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor524"><st c="6986">Centrality</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor525"><st c="6997">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor526"><st c="7016">Different types of graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor527"><st c="7042">Fully connected graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor529"><st c="7065">Disconnected graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor531"><st c="7085">Directed acyclic graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor533"><st c="7109">Small-world networks</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor536"><st c="7130">Scale-free networks</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor539"><st c="7150">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor540"><st c="7169">Community detection and decomposing graphs</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor541"><st c="7212">What is a community?</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor542"><st c="7233">How to do community detection</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor546"><st c="7263">Community detection algorithms</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor547"><st c="7294">Community detection code example</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor550"><st c="7327">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor551"><st c="7346">Summary</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor552"><st c="7354">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_10.xhtml#_idTextAnchor553"><st c="7364">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_Part_3.xhtml#_idTextAnchor554"><st c="7390">Part 3: Selected Advanced Concepts</st></a></h2>&#13;
			<h2><a href="B19496_11.xhtml#_idTextAnchor555"><st c="7425">11</st></a></h2>&#13;
			<h2><a href="B19496_11.xhtml#_idTextAnchor557"><st c="7428">Dynamical Systems</st></a></h2>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor558"><st c="7446">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor559"><st c="7469">What is a dynamical system and what is an evolution equation?</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor560"><st c="7531">Time can be discrete or continuous</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor561"><st c="7566">Time does not have to mean chronological time</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor562"><st c="7612">Evolution equations</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor563"><st c="7632">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor564"><st c="7648">First-order discrete Markov processes</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor565"><st c="7686">Variations of first-order Markov processes</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor566"><st c="7729">A Markov process is a probabilistic model</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor568"><st c="7771">The transition probability matrix</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor569"><st c="7805">Properties of the transition probability matrix</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor570"><st c="7853">Epidemic modeling with a first-order discrete Markov process</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor572"><st c="7914">The transition probability matrix is a network</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor574"><st c="7961">Using the transition matrix to generate state trajectories</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor575"><st c="8020">Evolution of the state probability distribution</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor579"><st c="8068">Stationary distributions and limiting distributions</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor582"><st c="8120">First-order discrete Markov processes are memoryless</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor583"><st c="8173">Likelihood of the state sequence</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor589"><st c="8206">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor590"><st c="8222">Higher-order discrete Markov processes</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor591"><st c="8261">Second-order discrete Markov processes</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor594"><st c="8300">Evolution of the state probability distribution in higher-order models</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor597"><st c="8371">A higher-order discrete Markov process is a first-order discrete Markov process in disguise</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor599"><st c="8463">Higher-order discrete Markov processes are still memoryless</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor600"><st c="8523">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor601"><st c="8539">Hidden Markov Models</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor603"><st c="8560">Emission probabilities</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor606"><st c="8583">Making inferences with an HMM</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor607"><st c="8613">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor608"><st c="8629">Summary</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor609"><st c="8637">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_11.xhtml#_idTextAnchor611"><st c="8647">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_12.xhtml#_idTextAnchor612"><st c="8673">12</st></a></h2>&#13;
			<h2><a href="B19496_12.xhtml#_idTextAnchor614"><st c="8676">Kernel Methods</st></a></h2>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor615"><st c="8691">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor616"><st c="8714">The role of inner products in common learning algorithms</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor617"><st c="8771">Sometimes we need new features in our inner products</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor620"><st c="8824">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor621"><st c="8840">The kernel trick</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor622"><st c="8857">What is a kernel?</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor623"><st c="8875">Commonly used kernels</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor627"><st c="8897">Kernel functions for other mathematical objects</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor630"><st c="8945">Combining kernels</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor631"><st c="8963">Positive semi-definite kernels</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor635"><st c="8994">Mercer‚Äôs theorem and the kernel trick</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor639"><st c="9032">Kernelized algorithms</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor640"><st c="9054">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor641"><st c="9070">An example of a kernelized learning algorithm</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor642"><st c="9116">kFDA code example</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor643"><st c="9134">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor644"><st c="9150">Summary</st></a></h3>&#13;
			<h3><a href="B19496_12.xhtml#_idTextAnchor645"><st c="9158">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_13.xhtml#_idTextAnchor646"><st c="9168">13</st></a></h2>&#13;
			<h2><a href="B19496_13.xhtml#_idTextAnchor648"><st c="9171">Information Theory</st></a></h2>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor649"><st c="9190">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor650"><st c="9213">What is information and why is it useful?</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor651"><st c="9255">The concept of information</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor652"><st c="9282">The mathematical definition of information</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor654"><st c="9325">Information theory applies to continuous distributions as well</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor655"><st c="9388">Why we measure information on a logarithmic scale</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor656"><st c="9438">Why is quantifying information useful?</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor658"><st c="9477">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor659"><st c="9496">Entropy as expected information</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor663"><st c="9528">Entropy</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor669"><st c="9536">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor670"><st c="9555">Mutual information</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor674"><st c="9574">Conditional entropy</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor678"><st c="9594">Mutual information for continuous variables</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor679"><st c="9638">Mutual information as a measure of correlation</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor682"><st c="9685">Mutual information code example</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor683"><st c="9717">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor684"><st c="9736">The Kullback-Leibler divergence</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor685"><st c="9768">Relative entropy</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor690"><st c="9785">KL-divergence for continuous variables</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor692"><st c="9824">Using the KL-divergence for approximation</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor696"><st c="9866">Variational inference</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor697"><st c="9888">What we‚Äôve learned</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor698"><st c="9907">Summary</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor699"><st c="9915">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_13.xhtml#_idTextAnchor701"><st c="9925">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_14.xhtml#_idTextAnchor702"><st c="9951">14</st></a></h2>&#13;
			<h2><a href="B19496_14.xhtml#_idTextAnchor704"><st c="9954">Non-Parametric Bayesian Methods</st></a></h2>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor705"><st c="9986">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor706"><st c="10009">What are non-parametric Bayesian methods?</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor708"><st c="10051">We still have parameters</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor709"><st c="10076">The different types of non-parametric Bayesian methods</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor710"><st c="10131">The pros and cons of non-parametric Bayesian methods</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor711"><st c="10184">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor712"><st c="10200">Gaussian processes</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor713"><st c="10219">The kernel function</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor715"><st c="10239">Fitting GPR models</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor717"><st c="10258">Prediction using GPR models</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor722"><st c="10286">GPR code example</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor725"><st c="10303">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor726"><st c="10319">Dirichlet processes</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor727"><st c="10339">How do DPs differ from GPs?</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor728"><st c="10367">The DP notation</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor730"><st c="10383">Sampling a function from a DP</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor732"><st c="10413">Generating a sample of data from a DP</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor733"><st c="10451">Bayesian non-parametric inference using a DP</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor736"><st c="10496">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor737"><st c="10512">Summary</st></a></h3>&#13;
			<h3><a href="B19496_14.xhtml#_idTextAnchor738"><st c="10520">Exercises</st></a></h3>&#13;
			<h2><a href="B19496_15.xhtml#_idTextAnchor739"><st c="10530">15</st></a></h2>&#13;
			<h2><a href="B19496_15.xhtml#_idTextAnchor741"><st c="10533">Random Matrices</st></a></h2>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor742"><st c="10549">Technical requirements</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor743"><st c="10572">What is a random matrix?</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor744"><st c="10597">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor745"><st c="10613">Using random matrices to represent interactions in large-scale systems</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor747"><st c="10684">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor748"><st c="10700">Universal behavior of large random matrices</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor749"><st c="10744">The Wigner semicircle law</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor750"><st c="10770">What does RMT study?</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor751"><st c="10791">Universal is universal</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor754"><st c="10814">The classical Gaussian matrix ensembles</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor760"><st c="10854">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor761"><st c="10870">Random matrices and high-dimensional covariance matrices</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor765"><st c="10927">The Marƒçenko-Pastur distribution is a bulk distribution</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor766"><st c="10983">Universality in the singular values of </st><img src="image/3.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.226em;height:0.873em;width:0.819em"/><st c="11023"/></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor767"><st c="11036">The Marƒçenko-Pastur distribution and neural networks</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor769"><st c="11088">What we learned</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor770"><st c="11104">Summary</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor771"><st c="11112">Exercises</st></a></h3>&#13;
			<h3><a href="B19496_15.xhtml#_idTextAnchor772"><st c="11122">Notes and further reading</st></a></h3>&#13;
			<h2><a href="B19496_Index.xhtml#_idTextAnchor773"><st c="11148">Index</st></a></h2>&#13;
			<h2><a href="B19496_BM.xhtml#_idTextAnchor775"><st c="11154">Other Books You May Enjoy</st></a></h2>&#13;
		</div>&#13;
	<div id="charCountTotal" value="11180"/></body></html>