<html><head></head><body>
		<div id="_idContainer007">
			<h1 id="_idParaDest-5"><a id="_idTextAnchor004"/><st c="0">Preface</st></h1>
			<p><st c="8">This is not a book about a specific technology or programming language. </st><st c="81">This is a book about mathematics. </st><st c="115">And mathematics is a language. </st><st c="146">It is the language of science, and so it is the language of data science as well. </st><st c="228">We can say beautiful things with that language. </st><st c="276">Just as a piece of great literature is more than a large collection of individual letters, a mathematical equation is more than just a collection of symbols. </st><st c="434">An equation conveys a way of thinking about a data science problem. </st><st c="502">It conveys a concept or an idea. </st><st c="535">If you want to fully exploit the power of those ideas and adapt them to your own data science work, you need to move beyond just recognizing the symbols in an equation and move towards understanding what that equation is really </st><span class="No-Break"><st c="763">telling you.</st></span></p>
			<p><st c="775">Many people are not confident in reading and interpreting mathematical equations and mathematical ideas. </st><st c="881">And yet, as with great literature, once someone guides us through the nuances and subtexts, their beauty is revealed and becomes obvious. </st><st c="1019">That is what this book aims </st><span class="No-Break"><st c="1047">to do.</st></span></p>
			<p><st c="1053">This book will not make you an expert in every area of mathematics. </st><st c="1122">Instead, it will give you enough skills and confidence to read and navigate mathematical equations and ideas on your own. </st><st c="1244">We do that by walking you through the core concepts that underpin many data science algorithms – the 15 math concepts of the book’s title. </st><st c="1383">We also do that by walking through those concepts slowly and in detail. </st><st c="1455">I am not a fan of mathematics books that consist solely of theorems, lemmas, and proofs. </st><st c="1544">Instead, this book is unapologetically long-form math. </st><st c="1599">When we introduce an equation, we will explain what the equation tells us, what its implications and ramifications are, and how it connects to other parts of math. </st><st c="1763">We also illustrate those concepts with code examples </st><span class="No-Break"><st c="1816">in Python.</st></span></p>
			<p><st c="1826">At the end of the book, you will be equipped to look at the math equations of any data science algorithm and confidently unpack what that algorithm is trying </st><span class="No-Break"><st c="1985">to do.</st></span></p>
			<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/><st c="1991">Who this book is for</st></h1>
			<p><st c="2012">This book is for data scientists and machine learning engineers who have been using data science and machine learning techniques, software, and Python packages such as scikit-learn, but without necessarily fully understanding the mathematics behind the algorithms. </st><st c="2278">This could include the following types </st><span class="No-Break"><st c="2317">of people:</st></span></p>
			<p><st c="2327">Data scientists who have a college/undergraduate degree in a numerate subject and so have a basic understanding of mathematics, but they want to learn more, particularly those bits of mathematics that will be helpful in their roles as </st><span class="No-Break"><st c="2563">data scientists.</st></span></p>
			<p><st c="2579">Data scientists who have a good understanding of some of the mathematics behind bits of data science but want to discover some new math concepts that will be useful to them in their data </st><span class="No-Break"><st c="2767">science work.</st></span></p>
			<p><st c="2780">Data scientists who have business or data science problems they need to solve, but existing software does not provide appropriate algorithms. </st><st c="2923">They want to construct their own algorithms but lack the mathematical guidance on how to apply mathematics to the new data </st><span class="No-Break"><st c="3046">science problems.</st></span></p>
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/><st c="3063">What this book covers</st></h1>
			<p><a href="B19496_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><st c="3085">Chapter 1</st></em></span></a><st c="3095">, </st><em class="italic"><st c="3097">Recap of Mathematical Notation and Terminology</st></em><st c="3143">, provides a summary of the main mathematical notation you will encounter in this book and that we expect you to already be </st><span class="No-Break"><st c="3267">familiar with.</st></span></p>
			<p><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="3281">Chapter 2</st></em></span></a><st c="3291">, </st><em class="italic"><st c="3293">Random Variables and Probability Distributions</st></em><st c="3339">, introduces the idea that all data contains some degree of randomness, and that random variables and their associated probability distributions are the natural way to describe that randomness. </st><st c="3533">The chapter teaches you how to sample from a probability distribution, understand statistical estimators, and about the Central </st><span class="No-Break"><st c="3661">Limit Theorem.</st></span></p>
			<p><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="3675">Chapter 3</st></em></span></a><st c="3685">, </st><em class="italic"><st c="3687">Matrices and Linear Algebra</st></em><st c="3714">, introduces vectors and matrices as the basic mathematical structures we use to represent and transform data. </st><st c="3825">It then shows how matrices can be broken down into simple-to-understand parts using techniques such as eigen-decomposition and singular value decomposition. </st><st c="3982">The chapter finishes with explanations of how these decomposition methods are applied to </st><strong class="bold"><st c="4071">p</st></strong><strong class="bold"><st c="4072">rincipal component analysis</st></strong><st c="4099"> (</st><strong class="bold"><st c="4101">PCA</st></strong><st c="4104">) and </st><strong class="bold"><st c="4111">non-negative matrix </st></strong><span class="No-Break"><strong class="bold"><st c="4131">factorization</st></strong></span><span class="No-Break"><st c="4144"> (</st></span><span class="No-Break"><strong class="bold"><st c="4146">NMF</st></strong></span><span class="No-Break"><st c="4149">).</st></span></p>
			<p><a href="B19496_04.xhtml#_idTextAnchor216"><span class="No-Break"><em class="italic"><st c="4152">Chapter 4</st></em></span></a><st c="4162">, </st><em class="italic"><st c="4164">Loss Functions and Optimization</st></em><st c="4195">, starts by introducing loss functions, risk functions, and empirical risk functions. </st><st c="4281">The concept of minimizing an empirical risk function to estimate the parameters of a model is explained, before introducing Ordinary Least Squares estimation of linear models. </st><st c="4457">Finally, gradient descent is illustrated as a general technique for minimizing </st><span class="No-Break"><st c="4536">risk functions.</st></span></p>
			<p><a href="B19496_05.xhtml#_idTextAnchor261"><span class="No-Break"><em class="italic"><st c="4551">Chapter 5</st></em></span></a><st c="4561">, </st><em class="italic"><st c="4563">Probabilistic Modeling</st></em><st c="4585">, introduces the concept of building predictive models that explicitly account for the random component within data. </st><st c="4702">The chapter starts by introducing likelihood and maximum likelihood estimation, before introducing Bayes’ theorem and Bayesian inference. </st><st c="4840">The chapter finishes with an illustration of Markov Chain Monte Carlo and importance sampling from the posterior distribution of a </st><span class="No-Break"><st c="4971">model’s parameters.</st></span></p>
			<p><a href="B19496_06.xhtml#_idTextAnchor314"><span class="No-Break"><em class="italic"><st c="4990">Chapter 6</st></em></span></a><st c="5000">, </st><em class="italic"><st c="5002">Time Series and Forecasting</st></em><st c="5029">, introduces time series data and the concept of auto-correlation as the main characteristic that distinguishes time series data from other types of data. </st><st c="5184">It then describes the classical ARIMA approach to modeling time series data. </st><st c="5261">Finally, it ends with a summary of concepts behind modern machine learning approaches to time </st><span class="No-Break"><st c="5355">series analysis.</st></span></p>
			<p><a href="B19496_07.xhtml#_idTextAnchor369"><span class="No-Break"><em class="italic"><st c="5371">Chapter 7</st></em></span></a><st c="5381">, </st><em class="italic"><st c="5383">Hypothesis Testing</st></em><st c="5401">, introduces what a hypothesis test is and why they are important in data science. </st><st c="5484">The general form of a hypothesis test is outlined before the concepts of statistical significance and p-values are explained in depth. </st><st c="5619">Next, confidence intervals and their interpretation are introduced. </st><st c="5687">The chapter ends with an explanation of Type-I and Type-II errors, and </st><span class="No-Break"><st c="5758">power calculations.</st></span></p>
			<p><a href="B19496_08.xhtml#_idTextAnchor406"><span class="No-Break"><em class="italic"><st c="5777">Chapter 8</st></em></span></a><st c="5787">, </st><em class="italic"><st c="5789">Model Complexity</st></em><st c="5805">, introduces the concept of how we describe and quantify model complexity and discusses its impact on the predictive accuracy of a model. </st><st c="5943">The classical bias-variance trade-off view of model complexity is introduced, along with the phenomenon of </st><strong class="bold"><st c="6050">double descent</st></strong><st c="6064">. The chapter finishes with an explanation of model complexity measures for </st><span class="No-Break"><st c="6140">model selection.</st></span></p>
			<p><a href="B19496_09.xhtml#_idTextAnchor449"><span class="No-Break"><em class="italic"><st c="6156">Chapter 9</st></em></span></a><st c="6166">, </st><em class="italic"><st c="6168">Function Decomposition</st></em><st c="6190">, introduces the idea of decomposing or building up a function from a set of simpler basis functions. </st><st c="6292">A general approach is explained first before the chapter moves on to introducing Fourier Series, Fourier Transforms, and the Discrete </st><span class="No-Break"><st c="6426">Fourier Transform.</st></span></p>
			<p><a href="B19496_10.xhtml#_idTextAnchor501"><span class="No-Break"><em class="italic"><st c="6444">Chapter 10</st></em></span></a><st c="6455">, </st><em class="italic"><st c="6457">Network Analysis</st></em><st c="6473">, introduces networks, network data, and the concept that a network is a graph. </st><st c="6553">The node-edge description of a graph, along with its adjacency matrix representation is explained. </st><st c="6652">Next, the chapter describes different types of common graphs and their properties. </st><st c="6735">Finally, the decomposition of a graph into sub-graphs or communities is explained, and various community detection algorithms </st><span class="No-Break"><st c="6861">are illustrated.</st></span></p>
			<p><a href="B19496_11.xhtml#_idTextAnchor555"><span class="No-Break"><em class="italic"><st c="6877">Chapter 11</st></em></span></a><st c="6888">, </st><em class="italic"><st c="6890">Dynamical Systems</st></em><st c="6907">, introduces what a dynamical system is and explains how its dynamics are controlled by an evolution equation. </st><st c="7018">The chapter then focuses on discrete Markov processes as these are the most common dynamical systems used by data scientists. </st><st c="7144">First-order discrete Markov processes are explained in depth, before higher-order Markov processes are introduced. </st><st c="7259">The chapter finishes with an explanation of Hidden Markov Models and a discussion of how they can be used in commercial data </st><span class="No-Break"><st c="7384">science applications.</st></span></p>
			<p><a href="B19496_12.xhtml#_idTextAnchor612"><span class="No-Break"><em class="italic"><st c="7405">Chapter 12</st></em></span></a><st c="7416">, </st><em class="italic"><st c="7418">Kernel Methods</st></em><st c="7432">, starts by introducing inner-product-based learning algorithms, then moves on to explaining kernels and the kernel trick. </st><st c="7555">The chapter ends with an illustration of a kernelized learning algorithm. </st><st c="7629">Throughout the chapter, we emphasize how the kernel trick allows us to implicitly and efficiently construct new features and thereby uncover any non-linear structure present in </st><span class="No-Break"><st c="7806">a dataset.</st></span></p>
			<p><a href="B19496_13.xhtml#_idTextAnchor646"><span class="No-Break"><em class="italic"><st c="7816">Chapter 13</st></em></span></a><st c="7827">, </st><em class="italic"><st c="7829">Information Theory</st></em><st c="7847">, introduces the concept of information and how it is measured mathematically. </st><st c="7926">The main information theory concepts of entropy, conditional entropy, mutual information, and relative entropy are then explained, before practical uses of the Kullback-Leibler divergence </st><span class="No-Break"><st c="8114">are illustrated.</st></span></p>
			<p><a href="B19496_14.xhtml#_idTextAnchor702"><span class="No-Break"><em class="italic"><st c="8130">Chapter 14</st></em></span></a><st c="8141">, </st><em class="italic"><st c="8143">Bayesian Non-Parametric Methods</st></em><st c="8174">, introduces the idea of using a Bayesian prior over functions when building probabilistic models. </st><st c="8273">The idea is illustrated through Gaussian Processes and Gaussian Process Regression. </st><st c="8357">The chapter then introduces Dirichlet Processes and how they can be used as priors for </st><span class="No-Break"><st c="8444">probability distributions.</st></span></p>
			<p><a href="B19496_15.xhtml#_idTextAnchor739"><span class="No-Break"><em class="italic"><st c="8470">Chapter 15</st></em></span></a><st c="8481">, </st><em class="italic"><st c="8483">Random Matrices</st></em><st c="8498">, introduces what a random matrix is and why they are ubiquitous in science and data science. </st><st c="8592">The universal properties of large random matrices are illustrated along with the classical Gaussian random matrix ensembles. </st><st c="8717">The chapter finishes with a discussion of where large random matrices occur in statistical and machine </st><span class="No-Break"><st c="8820">learning models.</st></span></p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/><st c="8836">To get the most out of this book</st></h1>
			<p><st c="8869">To get the most out of this book, we assume you have at least some familiarity with high-school mathematics, such as complex numbers, basic calculus, and elementary uses of vectors and matrices. </st><st c="9065">To get the most out of the code examples in the book, you should have some experience of coding in Python. </st><st c="9172">You will also need access to a computer or server with a full Python installation and/or where you have privileges to run and install Python and any additional </st><span class="No-Break"><st c="9332">packages required.</st></span></p>
			<table id="table001" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="9350">Software/hardware covered in </st></strong><span class="No-Break"><strong class="bold"><st c="9380">the book</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="9388">Operating </st></strong><span class="No-Break"><strong class="bold"><st c="9399">system requirements</st></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="9418">Python, </st><span class="No-Break"><st c="9427">Jupyter </st></span><span class="No-Break"><st c="9435">Notebook</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="9443">Windows, macOS, </st><span class="No-Break"><st c="9460">or Linux</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p><st c="9468">The code examples given in each chapter, and the answers to the exercises at the end of each chapter, are available in the book’s GitHub repository as Jupyter notebooks. </st><st c="9639">To run the notebooks, you will need a </st><span class="No-Break"><st c="9677">Jupyter installation.</st></span></p>
			<p><strong class="bold"><st c="9698">If you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). </st><st c="9886">Doing so will help you avoid any potential errors related to the copying and pasting </st></strong><span class="No-Break"><strong class="bold"><st c="9971">of code.</st></strong></span></p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/><st c="9979">Download the example code files</st></h1>
			<p><st c="10011">You can download the example code files for this book from GitHub at </st><a href="https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know"><st c="10081">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know</st></a><st c="10165">. If there’s an update to the code, it will be updated in the </st><span class="No-Break"><st c="10227">GitHub repository.</st></span></p>
			<p><st c="10245">We also have other code bundles from our rich catalog of books and videos available at </st><a href="https://github.com/PacktPublishing/"><st c="10333">https://github.com/PacktPublishing/</st></a><st c="10368">. Check </st><span class="No-Break"><st c="10376">them out!</st></span></p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/><st c="10385">Conventions used</st></h1>
			<p><st c="10402">There are a number of text conventions used throughout </st><span class="No-Break"><st c="10458">this book.</st></span></p>
			<p><strong class="source-inline"><st c="10468">Code in text</st></strong><st c="10481">: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. </st><st c="10634">Here is an example: “The following code example can be found in the </st><strong class="source-inline"><st c="10702">Code_Examples_Chap5.ipynb</st></strong><st c="10727"> notebook in the </st><span class="No-Break"><st c="10744">GitHub repository.”</st></span></p>
			<p><st c="10763">A block of code is set </st><span class="No-Break"><st c="10787">as follows:</st></span></p>
			<pre class="source-code"><st c="10798">
map_estimate = minimize(neg_log_posterior,
                        x0,
                        method='BFGS',
                        options={'disp': True})
# Convert from logit(p) to p
p_optimal = np.exp(map_estimate['x'][0])/ (
    1.0 + np.exp(map_estimate['x'][0]))
print("MAP estimate of success probability = ", p_optimal)</st></pre>			<p><strong class="bold"><st c="11052">Bold</st></strong><st c="11057">: Indicates a new term, an important word, or words that you see onscreen. </st><st c="11133">For instance, words in menus or dialog boxes appear in </st><strong class="bold"><st c="11188">bold</st></strong><st c="11192">. Here is an example: “The name </st><strong class="bold"><st c="11224">ARIMA</st></strong><st c="11229"> stands for Auto-Regressive Integrated Moving </st><span class="No-Break"><st c="11275">Average models</st></span><span class="No-Break"><st c="11289">.”</st></span></p>
			<p class="callout-heading"><st c="11291">Tips or important notes</st></p>
			<p class="callout"><st c="11315">Appear like this.</st></p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/><st c="11333">Get in touch</st></h1>
			<p><st c="11346">Feedback from our readers is </st><span class="No-Break"><st c="11376">always welcome.</st></span></p>
			<p><strong class="bold"><st c="11391">General feedback</st></strong><st c="11408">: If you have questions about any aspect of this book, email us at </st><a href="mailto:customercare@packtpub.com"><st c="11476">customercare@packtpub.com</st></a><st c="11501"> and mention the book title in the subject of </st><span class="No-Break"><st c="11547">your message.</st></span></p>
			<p><strong class="bold"><st c="11560">Errata</st></strong><st c="11567">: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. </st><st c="11663">If you have found a mistake in this book, we would be grateful if you would report this to us. </st><st c="11758">Please visit </st><a href="http://www.packtpub.com/support/errata"><st c="11771">www.packtpub.com/support/errata</st></a><st c="11802"> and fill in </st><span class="No-Break"><st c="11815">the form.</st></span></p>
			<p><strong class="bold"><st c="11824">Piracy</st></strong><st c="11831">: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. </st><st c="12002">Please contact us at </st><a href="mailto:copyright@packt.com"><st c="12023">copyright@packt.com</st></a><st c="12042"> with a link to </st><span class="No-Break"><st c="12058">the material.</st></span></p>
			<p><strong class="bold"><st c="12071">If you are interested in becoming an author</st></strong><st c="12115">: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please </st><span class="No-Break"><st c="12240">visit </st></span><a href="http://authors.packtpub.com"><span class="No-Break"><st c="12246">authors.packtpub.com</st></span></a><span class="No-Break"><st c="12266">.</st></span></p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/><st c="12267">Share your thoughts</st></h1>
			<p><st c="12287">Once you’ve read </st><em class="italic"><st c="12305">15 Math Concepts Every Data Scientist Should Know</st></em><st c="12354">, we’d love to hear your thoughts! </st><st c="12389">Please </st><a href="https://www.packtpub.com/"><st c="12396">click here to go straight to the Amazon review page</st></a><st c="12447"> for this book and share </st><span class="No-Break"><st c="12472">your feedback.</st></span></p>
			<p><st c="12486">Your review is important to us and the tech community and will help us make sure we’re delivering excellent </st><span class="No-Break"><st c="12595">quality content.</st></span></p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/><st c="12611">Download a free PDF copy of this book</st></h1>
			<p><st c="12649">Thanks for purchasing </st><span class="No-Break"><st c="12672">this book!</st></span></p>
			<p><st c="12682">Do you like to read on the go but are unable to carry your print </st><span class="No-Break"><st c="12748">books everywhere?</st></span></p>
			<p><st c="12765">Is your eBook purchase not compatible with the device of </st><span class="No-Break"><st c="12823">your choice?</st></span></p>
			<p><st c="12835">Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at </st><span class="No-Break"><st c="12922">no cost.</st></span></p>
			<p><st c="12930">Read anywhere, any place, on any device. </st><st c="12972">Search, copy, and paste code from your favorite technical books directly into </st><span class="No-Break"><st c="13050">your application.</st></span></p>
			<p><st c="13067">The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your </st><span class="No-Break"><st c="13183">inbox daily</st></span></p>
			<p><st c="13194">Follow these simple steps to get </st><span class="No-Break"><st c="13228">the benefits:</st></span></p>
			<ol>
				<li><st c="13241">Scan the QR code or visit the </st><span class="No-Break"><st c="13272">link below</st></span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B19496_QR_Free_PDF.jpg" alt="" role="presentation"/><st c="13282"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/9781837634187"><st c="13284">https://packt.link/free-ebook/9781837634187</st></a></p>
			<p><st c="13327">2.	</st><st c="13331">Submit your proof </st><span class="No-Break"><st c="13349">of purchase</st></span></p>
			<p><st c="13360">3.	</st><st c="13364">That’s it! </st><st c="13375">We’ll send your free PDF and other benefits to your </st><span class="No-Break"><st c="13427">email directly</st></span></p>
		</div>
	<div id="charCountTotal" value="13441"/>

		<div id="_idContainer008" class="Content">
			<h1 id="_idParaDest-14" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor013"/><st c="0">Part 1: Essential Concepts</st></h1>
			<p><st c="27">In this part, we will introduce the math concepts that you will encounter again and again as a data scientist. </st><st c="139">These concepts are vital to gain a good understanding of. </st><st c="197">After a recap of basic math notation, we look at the concepts related to how data is produced and then move through to concepts related to how to transform data, finally building up to our end goal of how to model data. </st><st c="417">These concepts are essential because you will use and combine them simultaneously in your work. </st><st c="513">By the end of Part 1, you will be comfortable with the math concepts that underpin almost all data science models </st><span class="No-Break"><st c="627">and algorithms.</st></span></p>
			<p><st c="642">This section contains the </st><span class="No-Break"><st c="669">following chapters:</st></span></p>
			<ul>
				<li><a href="B19496_01.xhtml#_idTextAnchor014"><em class="italic"><st c="688">Chapter 1</st></em></a><st c="698">, </st><em class="italic"><st c="700">Recap of Mathematical Notation and Terminology</st></em></li>
				<li><a href="B19496_02.xhtml#_idTextAnchor061"><em class="italic"><st c="746">Chapter 2</st></em></a><st c="756">, </st><em class="italic"><st c="758">Random Variables and Probability Distributions</st></em></li>
				<li><a href="B19496_03.xhtml#_idTextAnchor141"><em class="italic"><st c="804">Chapter 3</st></em></a><em class="italic"><st c="814">, Matrices and Linear Algebra</st></em></li>
				<li><a href="B19496_04.xhtml#_idTextAnchor216"><em class="italic"><st c="843">Chapter 4</st></em></a><em class="italic"><st c="853">, Loss Functions and Optimization</st></em></li>
				<li><a href="B19496_05.xhtml#_idTextAnchor261"><em class="italic"><st c="886">Chapter 5</st></em></a><em class="italic"><st c="896">, Probabilistic Modeling</st></em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer009" class="Basic-Graphics-Frame">
			</div>
		</div>
	<div id="charCountTotal" value="920"/></body></html>