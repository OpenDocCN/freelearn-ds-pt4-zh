- en: <st c="0">2</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Random Variables and Probability Distributions</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="48">In this chapter, we are going to learn about randomness in data and
    how we can use probability distributions to describe and handle this randomness.</st>
    <st c="198">Because of the importance of this chapter, we will deliberately spend
    a lot of time emphasizing in words the math concepts we are introducing.</st>
    <st c="341">This makes this chapter a long one.</st> <st c="377">Take your time
    and digest its contents fully.</st> <st c="423">Doing so will pay dividends because
    so many of the other math concepts we introduce in this book are underpinned by
    what we introduce in this chapter.</st> <st c="574">We will do this in five separate
    topics, each building upon the previous topic.</st> <st c="654">Those topics are</st>
    <st c="671">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="685">All data is random</st>*<st c="704">: In this section, we learn
    how randomness in data arises and why it is important to</st> <st c="790">understand
    it</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="803">Random variables and probability distributions</st>*<st c="850">:
    In this section, we learn the basics of random variables and probability distributions,
    why they are useful for describing randomness in data, how to summarize their
    characteristics, how to transform them, as well as the details of some commonly
    occurring distributions you will encounter as a</st> <st c="1147">data scientist</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1161">Sampling from distributions</st>*<st c="1189">: In this section,
    we learn how datasets are created or sampled from probability distributions, and
    how to generate our</st> <st c="1310">own samples</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1321">Understanding statistical estimators</st>*<st c="1358">: In this
    section, we learn how samples of data differ from the distribution from which
    the data was generated, and how to use our new knowledge of probability distributions
    to make accurate inferences from samples</st> <st c="1574">of data</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1581">The Central Limit Theorem</st>* <st c="1607">(</st>*<st c="1609">CLT</st>*<st
    c="1612">): In this section, we learn why and how the normal distribution is one
    of the most common distributions we will encounter as a</st> <st c="1741">data
    scientist</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1755">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1778">All code examples given in this chapter (and additional examples)
    can be found at the GitHub repository,</st> [<st c="1884">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter02</st>](https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter02)<st
    c="1988">. To run the Jupyter notebooks, you will need a full Python installation,
    including the</st> <st c="2076">following packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="2095">numpy</st>` <st c="2101">(>=1.24.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="2112">scipy</st>` <st c="2118">(>=1.11.1)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="2129">scikit-learn</st>` <st c="2142">(>=1.3.0)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="2152">matplotlib</st>` <st c="2163">(>=3.7.2)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2173">All data is random</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2192">If you</st> <st c="2200">read only one chapter in this book, read
    this one.</st> <st c="2251">Why?</st> <st c="2256">Well, because it explains the
    most important math concept in data science – all data is random.</st> <st c="2352">Or,
    more precisely, all data contains a</st> <st c="2392">random component.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2409">Is this really the case?</st> <st c="2435">Let’s explain.</st>
    <st c="2450">To start, we must explain what we mean by random.</st> <st c="2500">I’m
    not going to give some dry technical definition here, expressed in mathematical
    symbols.</st> <st c="2593">I’m going to give a technical, but intuitive definition:</st>
    *<st c="2650">random</st>* *<st c="2657">means non-predictable</st>*<st c="2678">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2679">What do we mean by that?</st> <st c="2705">Precisely what it says.</st>
    <st c="2729">If something is random, it can’t be computed or calculated</st> <st
    c="2788">in advance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2799">A little example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="2816">I have an</st> <st c="2826">old ship’s barometer that belonged
    to my father (he was a ship’s captain).</st> <st c="2902">The barometer is damaged
    and a bit temperamental, so the measurement is imprecise.</st> <st c="2985">This
    means the measured atmospheric pressure is not the same as the actual atmospheric
    pressure but deviates from it, possibly by as much as +/- 40 mbar.</st> <st c="3139">We
    can capture the idea of that deviation in the following</st> <st c="3198">schematic
    equation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3217">Measured atmospheric pressure = True atmospheric pressure + Measurement
    ‘Noise</st><st c="3296">’</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3298">Eq.</st> <st c="3302">1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3303">I have referred</st> <st c="3319">to this as a schematic equation
    because it expresses a key concept.</st> <st c="3387">The data we have, the measured
    pressure, deviates from what we’d like to know – the true pressure – due to the
    addition of the random measurement noise.</st> <st c="3540">So, the data (the
    observation) contains a</st> <st c="3582">random component.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3599">Sometimes, we also refer to measurement noise as measurement error
    or just error.</st> <st c="3682">In fact,</st> *<st c="3691">error</st>* <st c="3696">is
    a word used a lot to describe the random component in data.</st> <st c="3760">Sometimes
    we will also use the word</st> *<st c="3796">stochastic</st>* <st c="3806">instead
    of random, so you may see the terms</st> *<st c="3851">random error</st>*<st c="3863">,</st>
    *<st c="3865">random component</st>*<st c="3881">,</st> *<st c="3883">stochastic
    error</st>*<st c="3899">, and</st> *<st c="3905">stochastic component</st>* <st
    c="3925">used interchangeably.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3947">You may say, “</st>*<st c="3962">But that measurement error is
    just because you’re using an old barometer.</st> <st c="4037">If you were just
    using a modern digital barometer, the error in the pressure measurement would
    be inconsequential</st>*<st c="4150">.” True, but measurement error would still
    be there because we have to use some proxy physical process by which the force
    of the atmosphere is transferred to some other object whose change we can measure.</st>
    <st c="4355">Philosophically, what matters is that the error is there.</st> <st
    c="4413">One of our jobs as data scientists is to assess/quantify the scale of
    the error and determine how consequential</st> <st c="4525">it is.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4531">One thing we should emphasize is that the use of the word</st>
    *<st c="4590">error</st>* <st c="4595">does not imply a mistake has been made.</st>
    <st c="4636">It simply implies that there is a difference between the measurement
    and the true value.</st> <st c="4725">In our preceding example, the use of the
    word</st> *<st c="4771">error</st>* <st c="4776">means there is a difference between
    the measured pressure and the true (and unobserved) pressure.</st> <st c="4875">That
    difference is natural, and we understand why it comes about.</st> <st c="4941">No
    mistake has been made.</st> <st c="4967">Measurement error is a natural consequence
    of the fact that we measure things via proxy physical processes; for example,
    the atmosphere exerting pressure indirectly on a</st> <st c="5137">metal spring.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5150">Pro tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5158">On this point about what we mean, as data scientists, when we use
    the word</st> *<st c="5234">error</st>*<st c="5239">, be careful.</st> <st c="5253">Don’t
    use the word</st> *<st c="5272">error</st>* <st c="5277">when talking to non-data
    science stakeholders, or be very careful using it.</st> <st c="5354">A non-data
    scientist may hear</st> *<st c="5384">mistake</st>* <st c="5391">when you use
    the word</st> *<st c="5414">error</st>*<st c="5419">. So, instead, I tend to explain
    using words such as</st> *<st c="5472">deviation</st>* <st c="5481">or</st> *<st
    c="5485">difference</st>*<st c="5495">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5496">Systematic variation can be learned – random variation can’t</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5557">Sometimes I’m</st> <st c="5571">interested in just how inaccurate
    the old ship’s barometer is, so I also take a measurement using a small digital
    barometer I have.</st> <st c="5704">The measurement from the digital barometer
    I consider to be accurate – the measurement error is much smaller – and so the
    reading from the digital barometer I interpret as being the true atmospheric pressure.</st>
    <st c="5913">The following scatterplot shows how the two measurements relate to</st>
    <st c="5980">each other:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Scatterplot showing the accuracy of my old ship’s barometer](img/B19496_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="6128">Figure 2.1: Scatterplot showing the accuracy of my old ship’s
    barometer</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6199">The solid red line is the 1:1 line and so shows that on average
    the ship’s barometer reading gives the correct atmospheric pressure, but the vertical
    scatter about that line clearly highlights the random nature of the measurement
    from the ship’s barometer.</st> <st c="6457">What it also highlights is that if
    I wanted to predict what the ship’s barometer measurement would be, I wouldn’t
    be able to do so, even if I knew what the true pressure was.</st> <st c="6632">How
    far above or below the line any particular individual ship’s barometer measurement
    is is determined by the random measurement error.</st> <st c="6769">We can’t predict
    this in advance because, by definition, it is random.</st> <st c="6840">No amount
    of clever mathematics will change this.</st> <st c="6890">No</st> **<st c="6893">machine
    learning</st>** <st c="6909">(</st>**<st c="6911">ML</st>**<st c="6913">) algorithm
    will be able to predict what the next measurement from my ship’s barometer will
    be, no matter how much training data is given to</st> <st c="7055">that algorithm.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7070">What the solid line</st> <st c="7090">on the scatterplot does show,
    though, is that there is something about the ship’s barometer measurement that
    can be predicted – its average behavior.</st> <st c="7241">The average ship’s
    barometer measurement is the same as the true pressure, and so it varies systematically
    with the true pressure.</st> <st c="7372">It is this average or systematic behavior
    that an ML algorithm could learn and would learn with increasing accuracy as we
    used an increasing amount of training data.</st> <st c="7538">An alternative schematic
    way of writing</st> *<st c="7578">Eq.</st> <st c="7582">1</st>* <st c="7583">would
    be</st> <st c="7593">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7607">Observed Data = Systematic component + Random compon</st><st c="7660">ent</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7664">Eq.</st> <st c="7669">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7670">It is the systematic component that data science algorithms can
    learn and not the random component.</st> <st c="7770">In this simple example,
    the random component is additive, and without loss of generality, we can assume
    its average is zero.</st> <st c="7895">Consequently, the systematic component
    here is equal to the average value of the observed data, or more correctly, the
    average value of the observed data is equal to the systematic contribution to
    the data.</st> <st c="8102">This is not always the case, as we</st> <st c="8137">explain
    next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8150">Random variation is not just measurement error</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8197">“But,” I can</st> <st c="8210">hear you say, “what about something
    where there is no measurement error, where it is highly unlikely that we measured
    something incorrectly?” Consider something such as what kind of drink I’m drinking
    now, as I write this at 11:15 a.m.</st> <st c="8447">on a Saturday morning.</st>
    <st c="8470">Surely, I can’t be mistaken about what kind of drink, tea or coffee,
    that I have in my cup?</st> <st c="8562">No, but what type of drink I choose to
    put in the cup can still have a large random component to it.</st> <st c="8663">Let</st>
    <st c="8667">me illustrate.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8681">In the morning when I’m writing, I like to have a hot drink around
    11:00 a.m.</st> <st c="8760">Overall, I choose tea 40% of the time and coffee
    60% of the time.</st> <st c="8826">There is some good logic as to which drink
    I will choose.</st> <st c="8884">Largely, which type of drink depends on whether
    I’m already getting tired by 11:00 a.m., and that depends on how well I’ve slept.</st>
    <st c="9014">How well I’ve slept depends on whether I’ve had my cat trying to
    sit on my head during the night while I’ve been trying to sleep.</st> <st c="9144">Whether
    the cat tries to sit on my head depends on whether the cat wants to be inside
    the house or outside hunting.</st> <st c="9260">This chain of connections is summarized
    in the</st> <st c="9307">following diagram:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: How my cat’s random behavior makes my choice of morning drink
    effectively random](img/B19496_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="9350">Figure 2.2: How my cat’s random behavior makes my choice of morning
    drink effectively random</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9442">Despite there</st> <st c="9456">being a strong deterministic element
    to how I make my choice of drink in the morning, there is a large random variability
    in the outcome because my choice is now largely determined by the whim of my cat’s
    behavior the</st> <st c="9675">night before.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9688">This example highlights three</st> <st c="9719">important things:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9736">The random component within data is not necessarily additive.</st>
    <st c="9799">Random variation does not always present itself in the form of</st>
    *<st c="9862">Eq.</st> <st c="9866">2</st>*<st c="9867">. Instead, random variation
    can be variation in the chosen outcome.</st> <st c="9935">The potential outcomes
    may have well-defined long-run frequencies of occurrence, but any single instance
    of an outcome is unpredictable – that is, random – because in this example, on
    any particular night, I don’t know what my cat is going</st> <st c="10175">to
    do.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="10181">What we consider to be random can be a matter of modeling choice.</st>
    <st c="10248">I’ve said my morning drink is essentially determined by what my
    cat did the night before.</st> <st c="10338">Now, you may think that my cat’s
    behavior could potentially be predicted.</st> <st c="10412">Personally, I doubt
    it, but let’s say that in theory, it could.</st> <st c="10476">However, whatever
    equations are determining my cat’s behavior, they must be so complex as to give
    the appearance of something almost random because I have yet to work my cat out.</st>
    <st c="10655">What this illustrates is that sometimes we have variation in data
    that is systematic – meaning it is deterministic in nature and could in principle
    be predicted in advance – but that systematic variation is so complex, or its
    deterministic causes hidden from us, that we choose to view that variation as
    effectively being random and we model it</st> <st c="11000">as such.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="11008">Wherever</st> <st c="11018">human decisions are involved, we have
    random variability in outcomes, and that variability can be marked.</st> <st c="11124">When
    modeling datasets of human decisions, understanding this</st> <st c="11186">is
    important.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="11199">What are the consequences of data being random?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="11247">Okay – so I may</st> <st c="11263">have convinced you that all
    data contains some random component.</st> <st c="11329">So what?</st> <st c="11338">Why
    is this important for data science?</st> <st c="11378">Unfortunately, if our starting
    data has some random variation within it, then so does everything we derive from
    that data.</st> <st c="11501">This means every downstream quantity we calculate.</st>
    <st c="11552">And I mean everything – every average value calculated from a dataset,
    every loss-function value we calculate from a dataset when training an ML algorithm,
    every parameter value of a</st> **<st c="11735">deep learning</st>** <st c="11748">(</st>**<st
    c="11750">DL</st>**<st c="11752">)</st> **<st c="11755">neural network</st>**
    <st c="11769">(</st>**<st c="11771">NN</st>**<st c="11773">) trained using a dataset,
    every single ML metric</st> <st c="11824">we calculate.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11837">If the random variation within data affects every single calculation
    we do in data science, we’d better learn how to handle this random variation.</st>
    <st c="11985">The rest of this chapter is devoted to giving you the mathematical
    tools and language to do just that.</st> <st c="12088">But it will first require
    learning some new concepts, so this is a good place to finish this section and
    recap what we</st> <st c="12207">have learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12220">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="12236">In this section, we have learned</st> <st c="12270">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12284">All data we will work with as data scientists has a random component</st>
    <st c="12354">to it</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12359">Because of this randomness in data, all quantities derived from
    data also contain a</st> <st c="12444">random component</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12460">Only the non-random or systematic variation in a dataset is learnable
    by a data</st> <st c="12541">science algorithm</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12558">Sometimes, parts of the systematic variation in a dataset will
    be hidden from us – we will be unaware that the variation is systematic, and so
    because of our ignorance we will treat it</st> <st c="12744">as random</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12753">Sometimes, parts of the systematic variation in a dataset will
    be so complex that we deliberately choose to treat it as</st> <st c="12874">effectively
    random</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12892">Having learned how randomness appears in all datasets we work
    with as data scientists, in the next section, we move on to how we describe and
    quantify</st> <st c="13044">that randomness.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13060">Random variables and probability distributions</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="13107">We start this section by introducing a new concept that is necessary
    to describe the randomness we find</st> <st c="13212">in data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13220">A new concept – random variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13253">In</st> <st c="13257">computer code, when we want to use a variable,
    we type something such as</st> `<st c="13330">x=5</st>`<st c="13333">. In many
    programming languages, we may change the value of the variable</st> `<st c="13406">x</st>`<st
    c="13407">. We even use the word</st> *<st c="13430">variable</st>* <st c="13438">to
    indicate that its value may change.</st> <st c="13478">However, those changes
    are caused by us or by code we have written, and so typically they happen in a
    deterministic way; that is, we compute when the changes should happen, and we
    can compute the new value of</st> <st c="13687">the variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13700">For data that contains a random component, we need a new concept.</st>
    <st c="13767">Remember – random means non-predictable.</st> <st c="13808">When
    we record, observe, or capture the value of that variable, its value is not pre-determined.</st>
    <st c="13905">Instead, it could take on a number of values.</st> <st c="13951">The
    new concept we need is that of a</st> **<st c="13988">random variable</st>**<st
    c="14003">. A random variable is a variable, but its value when measured can be
    one of many different potential outcomes, each occurring with different probabilities.</st>
    <st c="14160">For example, the hot drink I will choose tomorrow morning is a random
    variable with two possible outcomes (tea or coffee) with probabilities of 0.4
    and</st> <st c="14312">0.6, respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14330">Listing the possible outcomes of a random variable and their associated
    probabilities gives us all the information we need to work with that random variable.</st>
    <st c="14489">We call this set of probabilities a</st> **<st c="14525">probability
    distribution</st>**<st c="14549">. Graphically, we can display this as a simple
    chart.</st> <st c="14603">For</st> <st c="14607">example, the bar chart in</st>
    *<st c="14633">Figure 2</st>**<st c="14641">.3</st>* <st c="14643">shows the two
    possible outcomes for the tea/coffee example and their</st> <st c="14713">associated
    probabili</st><st c="14733">ties:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: The probability distribution for my morning drink choice](img/B19496_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="14841">Figure 2.3: The probability distribution for my morning drink
    choice</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14909">We could equally have listed the probability distribution as a
    table, such as we have in</st> *<st c="14999">Table</st> <st c="15005">2.1</st>*<st
    c="15008">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 2.1: My morning drink probability distribution displayed in table form](img/B19496_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="15045">Table 2.1: My morning drink probability distribution displayed
    in table form</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15121">Where we have many outcomes, the graphical visualization is more
    intuitive.</st> <st c="15198">We can quickly pick out which outcomes have the
    highest probability, and we can see where the probability becomes very small,
    so outcomes occur very infrequently.</st> <st c="15361">In many situations, the
    possible outcomes of a random variable will have a natural ordering; for example,
    if I was considering the number of items bought from an e-commerce website in
    a day.</st> <st c="15552">Ten items bought in a day is clearly larger than five
    items bought.</st> <st c="15620">The number of items bought in a day probably
    has an upper limit, but some random variables can have an infinite number of</st>
    <st c="15742">potential outcomes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15761">One of the most obvious</st> <st c="15786">characteristics of
    the probability distribution shown in</st> *<st c="15843">Figure 2</st>**<st c="15851">.3</st>*
    <st c="15853">or</st> *<st c="15857">Table 2.1</st>* <st c="15866">is that the
    probabilities for the two possible outcomes add up to 1\.</st> <st c="15936">This
    is true for any probability distribution.</st> <st c="15983">The probabilities
    across all the possible outcomes always sum to 1\.</st> <st c="16051">This is
    because the sum of the probabilities represents the probability of getting any
    outcome, and we have absolute certainty that we will get some outcome.</st> <st
    c="16209">If a probability distribution has outcomes</st> <st c="16252">x</st>
    <st c="16253">and associated probabilities</st> <st c="16283">p</st><st c="16284">(</st><st
    c="16285">x</st><st c="16286">)</st><st c="16287">, we write this condition</st>
    <st c="16313">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16324">∑</st><st c="16326">x</st><st c="16327">p</st><st c="16328">(</st><st
    c="16329">x</st><st c="16330">)</st> <st c="16331">=</st> <st c="16332">1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16333">Eq.</st> <st c="16337">3</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16338">Some types of probability distributions are so commonly used that
    we i) give them a name so that we can conveniently refer to them using shorthand
    rather than having to list all the outcomes and probabilities and ii) because
    we want to study them in depth and share our findings with other scientists who
    may also be working with the same distribution.</st> <st c="16691">Giving something
    a name facilitates easier communication about</st> <st c="16754">that thing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16765">The probability distribution</st> <st c="16795">where we have
    just two possible outcomes is called the</st> **<st c="16850">Bernoulli distribution</st>**
    <st c="16872">(named after Jacob Bernoulli from the famous Bernoulli dynasty of
    Swiss mathematicians).</st> <st c="16962">Without loss of generality, we call
    those outcomes 1 and 0 because the choice of what labels we map or associate to
    the 1 and 0 outcomes is a matter of individual choice, and so doesn’t affect the
    mathematical properties of the Bernoulli distribution.</st> <st c="17214">The
    two outcomes are also sometimes called “success” and “failure,” reflecting the
    fact that one of the outcomes may be more beneficial or preferrable.</st> <st
    c="17366">For the Bernoulli distribution, we also only need to know the probability
    associated with the 1 (or success) outcome.</st> <st c="17484">Let’s denote that
    probability by</st> <st c="17517">p</st><st c="17518">. The fact that the probabilities
    of all possible outcomes must sum to 1 means that the probability of the 0 (or
    failure) outcome is</st> <st c="17651">1</st> <st c="17652">−</st> <st c="17653">p</st><st
    c="17654">. For the tea/coffee example, because the probability of me choosing
    a tea drink was 0.4, then the probability of me choosing coffee was automatically
    0.6\.</st> <st c="17810">So, a Bernoulli distribution is entirely specified once
    we know the value of</st> <st c="17887">p</st><st c="17888">, and so we write
    a Bernoulli distribution using a shorthand notation as</st> <st c="17961">Bernoulli</st><st
    c="17970">(</st><st c="17972">p</st><st c="17973">)</st><st c="17974">. If I see
    the notation</st> <st c="17998">Bernoulli</st><st c="18007">(</st><st c="18009">p</st><st
    c="18010">)</st><st c="18011">, I know we are talking about a probability distribution
    with two possible outcomes, 1 and 0, with the probability of 1 being</st> <st
    c="18137">p</st> <st c="18138">and the probability of 0 being</st> <st c="18170">1</st>
    <st c="18171">−</st> <st c="18172">p</st><st c="18173">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18174">Now, when we have a random variable</st> <st c="18211">X</st>
    <st c="18212">that follows a</st> <st c="18227">Bernoulli distribution, we write</st>
    <st c="18261">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18275">X</st> <st c="18277">∼</st> <st c="18278">Bernoulli</st><st c="18287">(</st><st
    c="18289">p</st><st c="18290">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18291">Eq.</st> <st c="18295">4</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18296">Let’s unpack</st> <st c="18309">that notation.</st> <st c="18324">Firstly,
    the use of</st> <st c="18344">~</st> <st c="18345">(the tilde symbol) and the
    presence of a named distribution on the right-hand side of</st> *<st c="18432">Eq.</st>
    <st c="18436">4</st>* <st c="18437">tells us that</st> <st c="18452">X</st> <st
    c="18453">is a random variable.</st> <st c="18476">We have used a capital letter
    for the random variable.</st> <st c="18531">This is common practice, to distinguish
    it from an ordinary variable that we would more commonly represent by a symbol
    such as</st> <st c="18658">x</st><st c="18659">. However, it is not unusual to
    see lowercase symbols used to represent random variables as well, and so the main
    giveaway that something is a random variable is the presence of the tilde symbol
    (</st><st c="18855">~</st><st c="18857">).</st> <st c="18860">The use of</st>
    <st c="18871">~</st> <st c="18872">means that the random variable on the left-hand
    side of</st> *<st c="18929">Eq.</st> <st c="18933">4</st>* <st c="18934">follows
    the probability distribution on the right-hand side.</st> <st c="18996">Overall,
    we read</st> *<st c="19013">Eq.</st> <st c="19017">4</st>* <st c="19018">as meaning</st>
    <st c="19030">X</st> <st c="19031">is distributed as</st> <st c="19050">Bernoulli</st><st
    c="19059">(</st><st c="19061">p</st><st c="19062">)</st><st c="19063">. This immediately
    tells us that</st> <st c="19096">X</st> <st c="19097">has two possible outcomes,
    1 and 0, with associated probabilities of</st> <st c="19167">p</st><st c="19168">,</st>
    <st c="19169">1</st> <st c="19170">−</st> <st c="19171">p</st><st c="19172">.
    What the outcomes 1 and 0 map to in terms of more useful human interpretable labels
    is usually explained elsewhere in the documentation that had the equation</st>
    <st c="19332">in it.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19338">For my morning drink example, and using the encoding 1=Coffee,
    0=Tea, I can write</st> <st c="19421">the following,</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19435">Drink</st> <st c="19441">~</st> <st c="19443">Bernoulli</st><st
    c="19452">(</st><st c="19454">0.6</st><st c="19457">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19459">Eq.</st> <st c="19463">5</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19464">Having learned about how random variables and their associated
    probability distributions are natural math concepts to describe the randomness
    we find in data, we are now going to learn how to quantify and summarize probability
    distributions in a more</st> <st c="19715">intuitive way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19729">Summarizing probability distributions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="19767">The probability distribution</st> <st c="19797">associated with
    a random variable tells us everything we need to know about that random variable.</st>
    <st c="19895">It tells us what the probability of any particular outcome is.</st>
    <st c="19958">Imagine we wanted to communicate information about the random variable
    to a colleague.</st> <st c="20045">Obviously, the most complete way would be to
    communicate all the individual probabilities for all the individual outcomes.</st>
    <st c="20168">If we have a large or infinite number of outcomes, that is a lot
    of probabilities we must communicate to our colleague.</st> <st c="20288">We could
    send our colleague the bar chart showing the probability distribution.</st> <st
    c="20368">However, while that bar chart is a great visual way of communicating</st>
    <st c="20436">the distribution, it doesn’t easily communicate all the numerical
    values of the probabilities to our colleague.</st> <st c="20549">But what if our
    colleague doesn’t want all the probabilities?</st> <st c="20611">What if they
    just want to get a quantitative feel for what the distribution is about?</st>
    <st c="20697">Is there a single number or a couple of numbers that conveniently
    summarize most of the information contained in the probability distribution?</st>
    <st c="20840">The answer is yes, and we will now introduce the most</st> <st c="20894">important
    ones.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20909">The mean of a distribution</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="20936">The</st> **<st c="20941">mean</st>** <st c="20945">of a</st> <st
    c="20950">distribution is a single number that we calculate from the full probability
    distribution.</st> <st c="21041">The mean is used to give us an idea about the
    average value of the distribution; that is, the typical value we would expect
    to see if we drew lots of values from the same distribution.</st> <st c="21226">However,
    be aware that the mean does not always do a good job of this.</st> <st c="21297">We
    will see why later.</st> <st c="21320">For now, let’s see how the mean</st> <st
    c="21352">is calculated.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21366">The mean of a probability distribution that has outcomes</st>
    <st c="21424">x</st> <st c="21425">and associated probabilities</st> <st c="21455">p</st><st
    c="21456">(</st><st c="21457">x</st><st c="21458">)</st> <st c="21459">is defined</st>
    <st c="21471">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21482">Mean</st> <st c="21487">=</st> <st c="21489">∑</st><st c="21490">x</st><st
    c="21491">x</st><st c="21492">p</st><st c="21493">(</st><st c="21494">x</st><st
    c="21495">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21496">Eq.</st> <st c="21500">6</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21501">Here, the summation is over all the possible outcomes.</st> <st
    c="21556">Let’s make that a bit more concrete.</st> <st c="21593">Imagine I have
    the probability distribution shown in</st> *<st c="21646">F</st><st c="21647">igure
    2</st>**<st c="21654">.4</st>*<st c="21656">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Example probability distribution](img/B19496_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="21693">Figure 2.4: Example probability distribution</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21737">The</st> <st c="21742">outcomes are the integer values 1, 2, …,
    10 (perhaps resulting from throwing a strangely shaped 10-sided die).</st> <st
    c="21853">The mean is then the sum of each of those integer values multiplied
    by the corresponding probability shown in</st> *<st c="21963">Figure 2</st>**<st
    c="21971">.4</st>*<st c="21973">. The mean value turns out to be 5.5 and is shown
    by the vertical dashed line in</st> *<st c="22054">Figure 2</st>**<st c="22062">.4</st>*<st
    c="22064">. Is the mean value useful to us?</st> <st c="22098">Yes – in this case,
    it is very useful.</st> <st c="22137">You can see from the shape of the probability
    distribution in</st> *<st c="22199">Figure 2</st>**<st c="22207">.4</st>* <st
    c="22209">that the most probable outcomes are 5 and 6, with other outcomes having
    a lower probability and hence occurring less frequently.</st> <st c="22339">When
    drawing a single value from this distribution, we would expect to get 5 or 6 a
    lot of the time.</st> <st c="22440">For this reason, the mean is what’s called
    an</st> **<st c="22486">expectation value</st>** <st c="22503">or</st> **<st c="22507">expected
    value</st>**<st c="22521">. The mean of the random variable</st> <st c="22555">X</st>
    <st c="22556">is called the</st> <st c="22571">expectation value of</st> <st c="22592">X</st>
    <st c="22593">or the expected value of</st> <st c="22619">X</st><st c="22620">.
    We have a special symbol for an expectation value.</st> <st c="22673">We use the
    symbol</st> <st c="22691">𝔼</st><st c="22693">(</st><st c="22695">X</st><st c="22696">)</st>
    <st c="22697">for the expectation value of</st> <st c="22727">X</st><st c="22728">.
    So,</st> <st c="22734">𝔼</st><st c="22736">(</st><st c="22738">X</st><st c="22739">)</st>
    <st c="22740">means the same as calculate the mean of</st> <st c="22781">X</st><st
    c="22782">. That is shown in the</st> <st c="22805">following formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22823">𝔼</st><st c="22826">(</st><st c="22828">X</st><st c="22829">)</st>
    <st c="22830">=</st> <st c="22831">∑</st><st c="22832">x</st><st c="22833">x</st>
    <st c="22834">p</st><st c="22835">(</st><st c="22836">x</st><st c="22837">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22838">Eq.</st> <st c="22842">7</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22843">From the notation</st> <st c="22861">𝔼</st><st c="22863">(</st><st
    c="22865">X</st><st c="22866">)</st><st c="22867">, you’ll see that we sort of
    consider</st> <st c="22905">𝔼</st> <st c="22907">to be like a function, in that
    it is applied to the thing inside the brackets.</st> <st c="22987">It is actually
    an operator, but we won’t go into that distinction here.</st> <st c="23059">You
    can think of</st> <st c="23076">𝔼</st> <st c="23078">as a function that gets applied
    to random variables.</st> <st c="23132">That means we can also apply</st> <st
    c="23161">𝔼</st> <st c="23163">in a composite way.</st> <st c="23184">Say we wanted
    to calculate the mean value of taking the exponential of any outcome</st> <st
    c="23267">x</st> <st c="23268">of the random variable</st> <st c="23292">X</st><st
    c="23293">. Taking the exponential of a random variable is just another random
    variable – remember what we said about anything derived or computed from something
    random being also</st> <st c="23463">random.</st> <st c="23471">We’ll denote this
    newly derived random variable as</st> <st c="23522">e</st><st c="23523">X</st>
    <st c="23524">to signify that it is a random variable obtained from taking the
    exponential of the original random variable</st> <st c="23634">X</st><st c="23635">.
    The expectation value of this newly derived random variable</st> <st c="23697">e</st><st
    c="23698">X</st> <st c="23699">is calculated</st> <st c="23714">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23725">𝔼</st><st c="23728">(</st><st c="23730">e</st><st c="23731">X</st><st
    c="23732">)</st> <st c="23733">=</st> <st c="23734">∑</st><st c="23735">x</st><st
    c="23736">e</st><st c="23737">x</st> <st c="23738">p</st><st c="23739">(</st><st
    c="23740">x</st><st c="23741">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23742">Eq.</st> <st c="23746">8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23747">To calculate the expectation value of this newly derived random
    variable, we have simply applied the same function (the exponential function)
    to the outcome values</st> <st c="23911">x</st><st c="23912">. As you might guess,
    for a general function</st> <st c="23957">f</st> <st c="23958">we calculate the
    expectation value of</st> <st c="23997">f</st><st c="23998">(</st><st c="23999">X</st><st
    c="24000">)</st> <st c="24001">by calculating</st> <st c="24017">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24031">𝔼</st><st c="24034">(</st><st c="24036">f</st><st c="24037">(</st><st
    c="24038">X</st><st c="24039">)</st><st c="24040">)</st> <st c="24041">=</st>
    <st c="24042">∑</st><st c="24043">x</st><st c="24044">f</st><st c="24045">(</st><st
    c="24046">x</st><st c="24047">)</st><st c="24048">p</st><st c="24049">(</st><st
    c="24050">x</st><st c="24051">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24052">Eq.</st> <st c="24056">9</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24057">The variance of a distribution</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="24087">Looking</st> <st c="24096">at</st> *<st c="24099">Figure 2</st>**<st
    c="24107">.4</st>*<st c="24109">, we would think from the shape of the distribution
    that it was obvious that the mean of</st> <st c="24198">X</st> <st c="24199">was
    representative of the typical value we would get when we draw a value from the
    distribution, because of how tightly concentrated the distribution is around its
    highest probability outcomes at</st> <st c="24396">x</st> <st c="24397">=</st>
    <st c="24398">5</st> <st c="24399">and</st> <st c="24404">x</st> <st c="24405">=</st>
    <st c="24406">6</st><st c="24407">. What would happen if we had a distribution
    that was more widely spread?</st> <st c="24481">Take a look at</st> *<st c="24495">Figure
    2</st>**<st c="24504">.5</st>*<st c="24506">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Another example probability distribution](img/B19496_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="24544">Figure 2.5: Another example probability distribution</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24596">The mean</st> <st c="24606">of this distribution is also 5.5,
    but clearly, values that are very different from 5.5 are now more likely compared
    to when we were drawing values from the distribution in</st> *<st c="24778">Figure
    2</st>**<st c="24786">.4</st>*<st c="24788">. So, for</st> *<st c="24798">Figure
    2</st>**<st c="24806">.5</st>*<st c="24808">, the mean of</st> <st c="24822">X</st>
    <st c="24823">is the average value we would get when we draw lots of values from
    the distribution in</st> *<st c="24911">Figure 2</st>**<st c="24919">.5</st>*<st
    c="24921">, but as a single number, the mean value of 5.5 is not typical as there
    is a lot spread of values around 5.5\.</st> <st c="25031">Clearly, knowing</st>
    **<st c="25048">only</st>** <st c="25052">the mean isn’t a good way of summarizing
    the distribution in</st> *<st c="25114">Figure 2</st>**<st c="25122">.5</st>*<st
    c="25124">. Whether that spread is small or large is important to know.</st> <st
    c="25186">How can we quantify</st> <st c="25206">that spread?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25218">Just as the mean quantifies the average value drawn from the distribution,
    we can also calculate the average difference from that mean value.</st> <st c="25361">If
    the mean of a distribution is</st> <st c="25394">μ</st> <st c="25395">and we look
    at a specific outcome value</st> <st c="25436">x</st><st c="25437">, then the
    deviation of the outcome from the mean is</st> <st c="25490">x</st> <st c="25491">−</st>
    <st c="25492">μ</st><st c="25493">. We could then just take the average of this
    deviation over all possible outcomes.</st> <st c="25577">This would give us</st>
    <st c="25596">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25610">∑</st><st c="25612">x</st><st c="25613">(</st><st c="25614">x</st>
    <st c="25615">−</st> <st c="25616">μ</st><st c="25617">)</st><st c="25618">p</st><st
    c="25619">(</st><st c="25620">x</st><st c="25621">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25622">Eq.</st> <st c="25626">10</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25628">A quick bit of algebra will tell you that this is always zero,
    so it isn’t a very good way of measuring the spread of a distribution.</st> <st
    c="25763">The reason for this is that for small values of</st> <st c="25811">x</st><st
    c="25812">, the deviation</st> <st c="25828">x</st> <st c="25829">−</st> <st c="25830">μ</st>
    <st c="25831">is negative, while for large values of</st> <st c="25871">x</st>
    <st c="25872">the deviation is positive, and overall, the positive and negative
    values exactly cancel each other out.</st> <st c="25977">We can stop this cancellation
    by using the squared deviation</st> <st c="26038">(</st><st c="26039">x</st><st
    c="26040">−</st> <st c="26041">μ</st><st c="26042">)</st><st c="26043">2</st>
    <st c="26044">instead and calculate the average of this squared deviation because
    the squared deviation is always positive or zero.</st> <st c="26163">This is calculated
    using the</st> <st c="26192">following formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26210">Variance</st> <st c="26219">=</st> <st c="26221">∑</st><st c="26222">x</st><st
    c="26223">(</st><st c="26224">x</st><st c="26225">−</st> <st c="26226">μ</st><st
    c="26227">)</st><st c="26228">2</st> <st c="26229">p</st><st c="26230">(</st><st
    c="26231">x</st><st c="26232">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26233">Eq.</st> <st c="26237">11</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26239">We</st> <st c="26243">call this average squared deviation the</st>
    **<st c="26283">variance</st>** <st c="26291">of the distribution.</st> <st c="26313">Now,
    you’re probably asking how this gives us a measure of the spread of a distribution
    if we have calculated the average squared deviation.</st> <st c="26454">The answer
    is it doesn’t.</st> <st c="26480">To get a measure of spread, we take the inverse
    operation at the end; that is, we take the square root of the variance.</st> <st
    c="26600">The square root of the variance is called</st> <st c="26641">the</st>
    **<st c="26646">standard deviation</st>**<st c="26664">. So, putting that together,
    we have</st> <st c="26701">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26715">Standard Deviation =</st> <st c="26737">√</st><st c="26738">_</st><st
    c="26739">Variance</st> <st c="26747">=</st> <st c="26749">[</st><st c="26750">∑</st><st
    c="26751">x</st><st c="26752">(</st><st c="26753">x</st><st c="26754">−</st> <st
    c="26755">μ</st><st c="26756">)</st><st c="26757">2</st> <st c="26758">p</st><st
    c="26759">(</st><st c="26760">x</st><st c="26761">)</st><st c="26762">]</st><st
    c="26763">1</st><st c="26764">_</st><st c="26765">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26766">Eq.</st> <st c="26770">12</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26772">In general, we have</st> <st c="26793">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26807">Variance =</st> <st c="26819">Standard</st> <st c="26828">Deviation</st><st
    c="26837">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26839">Eq.</st> <st c="26843">13</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26845">We use the symbol</st> <st c="26864">σ</st> <st c="26865">for
    the standard deviation, and so from the preceding equation, we can also use</st>
    <st c="26946">σ</st><st c="26947">2</st> <st c="26948">to denote the variance.</st>
    <st c="26973">Sometimes, you will also see</st> <st c="27002">Var</st> <st c="27005">used
    as shorthand for the variance.</st> <st c="27042">For example, we might write</st>
    <st c="27070">Var</st><st c="27073">(</st><st c="27075">X</st><st c="27076">)</st>
    <st c="27077">to indicate the variance of the random</st> <st c="27117">variable</st>
    <st c="27126">X</st><st c="27127">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27128">What does the standard deviation tell us?</st> <st c="27171">Well,
    the clue is in the name.</st> <st c="27202">It is the standard, typical, or expected
    size of deviation from the mean that we should expect when we draw a number from
    the distribution.</st> <st c="27342">If we draw a number from the distribution,
    we should not be surprised if it differs from the mean by something comparable
    to the standard deviation; for example, by as much as</st> <st c="27518">1</st><st
    c="27519">σ</st> <st c="27520">or</st> <st c="27523">2</st><st c="27524">σ</st><st
    c="27525">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27526">We’ve said that the standard deviation of a distribution is the
    size of the typical deviation we should</st> **<st c="27631">expect</st>** <st
    c="27637">when drawing a number from the distribution.</st> <st c="27683">So,
    you might ask, can the standard deviation be written</st> <st c="27740">as an</st>
    **<st c="27746">expectation value</st>** <st c="27763">of some random variable?</st>
    <st c="27789">Not quite.</st> <st c="27800">But the variance can.</st> <st c="27822">If
    we look at</st> *<st c="27836">Eq.</st> <st c="27840">11</st>*<st c="27842">,
    we can see that it is a weighted average of the squared deviation.</st> <st c="27911">So,
    we can write the variance calculation in</st> *<st c="27956">Eq.</st> <st c="27960">11</st>*
    <st c="27962">as follo</st><st c="27971">ws:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27975">Variance</st><st c="27984">(</st><st c="27986">X</st><st c="27987">)</st>
    <st c="27988">=</st> <st c="27991">𝔼</st><st c="27993">(</st><st c="27995">(</st><st
    c="27996">X</st><st c="27997">−</st> <st c="27998">μ</st><st c="27999">)</st><st
    c="28000">2</st><st c="28001">)</st> <st c="28002">=</st> <st c="28003">𝔼</st><st
    c="28005">(</st><st c="28007">(</st><st c="28008">X</st><st c="28009">−</st> <st
    c="28010">𝔼</st><st c="28012">(</st><st c="28014">X</st><st c="28015">)</st><st
    c="28016">)</st><st c="28017">2</st><st c="28018">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28019">Eq.</st> <st c="28023">14</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28025">The</st> <st c="28030">second part of the formula on the right-hand
    side in</st> *<st c="28083">Eq.</st> <st c="28087">14</st>* <st c="28089">just
    comes from replacing</st> <st c="28116">μ</st> <st c="28117">=</st> <st c="28118">𝔼</st><st
    c="28120">(</st><st c="28122">X</st><st c="28123">)</st><st c="28124">. The formula
    in</st> *<st c="28141">Eq.</st> <st c="28145">14</st>* <st c="28147">tells us
    that the variance of the random variable</st> <st c="28198">X</st> <st c="28199">is
    the same as the expectation value of the random variable</st> <st c="28260">(</st><st
    c="28261">X</st><st c="28262">−</st> <st c="28263">μ</st><st c="28264">)</st><st
    c="28265">2</st><st c="28266">. The left-hand side of the preceding equation shows
    that calculating the variance of a distribution is a function or operation that
    we apply to a random variable, so you might ask whether there is a special symbol
    we use when calculating the variance of a distribution, just like we use the symbol</st>
    <st c="28565">𝔼</st> <st c="28567">when calculating the expectation value of a
    random variable.</st> <st c="28629">Well, there is.</st> <st c="28645">We have
    already used it.</st> <st c="28670">It is</st> <st c="28676">Var</st><st c="28679">(</st><st
    c="28681">X</st><st c="28682">)</st><st c="28683">. But you may also see</st>
    <st c="28706">𝕍</st><st c="28708">(</st><st c="28710">X</st><st c="28711">)</st>
    <st c="28712">used to represent the operation of evaluating the variance of the
    random variable</st> <st c="28795">X</st><st c="28796">. However, in my personal
    experience,</st> <st c="28834">𝕍</st><st c="28836">(</st><st c="28838">X</st><st
    c="28839">)</st> <st c="28840">is a lot less commonly used compared</st> <st c="28878">to</st>
    <st c="28881">Var</st><st c="28884">(</st><st c="28886">X</st><st c="28887">)</st><st
    c="28888">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28889">Other characteristics of a distribution</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="28929">When</st> <st c="28935">summarizing a distribution, it is common
    to give just the mean and standard deviation, or equivalently the mean and the
    variance.</st> <st c="29065">Are these two numbers enough to summarize a distribution?</st>
    <st c="29123">The answer is no.</st> <st c="29141">Sometimes, we may want to quote
    higher-order moments about the mean</st> <st c="29209">μ</st><st c="29210">. For
    example, the third moment looks</st> <st c="29248">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29258">𝔼</st><st c="29261">(</st><st c="29263">(</st><st c="29264">X</st><st
    c="29265">−</st> <st c="29266">μ</st><st c="29267">)</st><st c="29268">3</st><st
    c="29269">)</st> <st c="29270">=</st> <st c="29271">∑</st><st c="29272">x</st><st
    c="29273">(</st><st c="29274">x</st><st c="29275">−</st> <st c="29276">μ</st><st
    c="29277">)</st><st c="29278">3</st> <st c="29279">p</st><st c="29280">(</st><st
    c="29281">x</st><st c="29282">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29283">Eq.</st> <st c="29287">15</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29289">This can tell us about how lop-sided or asymmetric a distribution
    is.</st> <st c="29360">Using this third</st> **<st c="29377">central moment</st>**<st
    c="29391">, we</st> <st c="29396">can calculate the skewness of</st> <st c="29426">a
    distribution:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29441">Skewness =</st> <st c="29453">μ</st><st c="29454">3</st> <st c="29455">=</st>
    <st c="29456">𝔼</st><st c="29458">(</st><st c="29460">(</st><st c="29461">(</st><st
    c="29462">X</st><st c="29463">−</st> <st c="29464">μ</st><st c="29465">)</st><st
    c="29466">_</st><st c="29467">σ</st><st c="29468">)</st><st c="29469">3</st><st
    c="29470">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29471">Eq.</st> <st c="29475">16</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29477">The graph on the left-hand side of</st> *<st c="29513">Figure
    2</st>**<st c="29521">.6</st>* <st c="29523">has a skewness of zero, while the
    middle graph has a</st> <st c="29577">positive skewness, and the right-hand graph
    has a negative skewness.</st> <st c="29646">The dashed line in each of the plots
    shows the position of the mean of each distribution, so we can see how the skewness
    reflects the shift in probability mass from one side of the</st> <st c="29826">mean
    to</st> <st c="29835">the other:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Example probability distributions with different skewness values](img/B19496_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="29998">Figure 2.6: Example probability distributions with different
    skewness values</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30074">The symbol</st> <st c="30086">μ</st><st c="30087">3</st> <st c="30088">is</st>
    <st c="30092">typically used for the skewness.</st> <st c="30125">Likewise, we
    can use the fourth central moment to calculate what is called the</st> **<st c="30204">kurtosis</st>**
    <st c="30212">of a</st> <st c="30218">distribution.</st> <st c="30232">We won’t
    define the kurtosis</st> <st c="30261">here, other than to say that the kurtosis
    measures relatively how fat or thin a distribution is.</st> <st c="30358">If you
    want to convey to another data scientist characteristics such as how lop-sided
    or thin a distribution is, it is often easier just to show a plot of the distribution.</st>
    <st c="30531">So, most data scientists tend to calculate only the mean and standard
    deviation of a distribution and then show a plot if they want to communicate</st>
    <st c="30678">additional details.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30697">Having learned how to summarize and characterize a probability
    distribution for random variables that have discrete outcomes, we are going to
    learn how to extend this to continuous-valued</st> <st c="30886">random variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30903">Continuous distributions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30928">The</st> <st c="30932">sharp-eyed among you will have spotted
    that all the examples of probability distributions that we have given so far in
    this section are of discrete outcomes – outcomes that are clearly distinct, such
    as tea and coffee, or the integer numbers 1, 2, 3…and so on.</st> <st c="31194">Surely
    some outcomes can be continuous.</st> <st c="31234">The ship’s barometer example
    we started the chapter with is an example of a continuous quantity.</st> <st c="31331">So,
    yes – an outcome can be continuous, but there are some subtleties with continuous-valued
    outcomes that we will</st> <st c="31446">now explain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31458">The first subtlety stems from the number of possible outcomes.</st>
    <st c="31522">Imagine I have a random variable</st> <st c="31555">X</st> <st c="31556">that
    is a real number and can take any value between</st> <st c="31610">−</st> <st
    c="31611">∞</st> <st c="31612">and</st> <st c="31617">+</st> <st c="31618">∞</st><st
    c="31619">. It has an infinite number of possible outcomes.</st> <st c="31669">Given
    we have said the sum of the probabilities across all outcomes must be 1, then
    what happens when we sum an infinite number of probabilities?</st> <st c="31815">Doesn’t
    this mean the probability of any particular outcome is zero?</st> <st c="31884">Even
    the answer to this question is slightly complicated, but just asking this question
    does highlight that the concept of a probability distribution needs modification
    when we are dealing with continuous-valued outcomes.</st> <st c="32106">Instead
    of talking about probability distributions, we talk about</st> **<st c="32172">probability
    density</st>** **<st c="32192">functions</st>** <st c="32201">(</st>**<st c="32203">PDFs</st>**<st
    c="32207">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32210">A PDF, as</st> <st c="32221">the name suggests, tells me the density
    of probability in the region around a particular outcome point</st> <st c="32324">x</st><st
    c="32325">. With physical densities, we can calculate the amount of material;
    for example, the number of atoms or molecules, present in a volume</st> <st c="32460">V</st><st
    c="32461">, by multiplying the density</st> <st c="32490">ρ</st> <st c="32491">by
    the volume.</st> <st c="32507">Similarly, with a PDF, to calculate the amount
    of probability in a small interval of width</st> <st c="32598">dx</st> <st c="32600">between</st>
    <st c="32609">x</st> <st c="32610">and</st> <st c="32615">x</st> <st c="32616">+</st>
    <st c="32617">dx</st><st c="32619">, we simply multiply the PDF by the interval
    width</st> <st c="32670">dx</st><st c="32672">. This gives us</st> <st c="32688">the</st>
    <st c="32691">following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32702">Prob</st><st c="32707">(</st><st c="32709">x</st> <st c="32710"><</st>
    <st c="32711">X</st> <st c="32712"><</st> <st c="32713">x</st> <st c="32714">+</st>
    <st c="32715">dx</st><st c="32717">)</st> <st c="32719">=</st> <st c="32720">pdf</st><st
    c="32723">(</st><st c="32725">x</st><st c="32726">)</st> <st c="32727">dx</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32729">Eq.</st> <st c="32734">17</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32736">So, for a continuous-valued outcome, we talk not of the probability
    of having a particular outcome</st> <st c="32836">x</st> <st c="32837">but of
    the probability of having an outcome in</st> <st c="32885">a range.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32893">Note that we have used here the differential calculus concept
    of a small interval</st> <st c="32976">dx</st><st c="32978">. This is necessary
    because, unlike with physical densities and physical material where the density
    may be constant over a significant sized volume, the probability density can vary
    markedly as we change the outcome</st> <st c="33194">value</st> <st c="33200">x</st><st
    c="33201">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33202">Secondly, note</st> <st c="33218">that if we decrease the size
    of the interval</st> <st c="33263">dx</st> <st c="33265">down to zero so that
    we are looking at a single outcome point and not an interval, then the right-rand-side
    of</st> *<st c="33377">Eq.</st> <st c="33381">17</st>* <st c="33383">becomes zero
    – just as we explained previously.</st> <st c="33432">So, the probability of a
    point outcome is zero, but the probability density at that point can</st> <st
    c="33526">be nonzero.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33537">Just in case you’re wondering if the aforementioned line of reasoning
    is dependent on the range of possible outcomes being</st> <st c="33661">−</st>
    <st c="33662">∞</st> <st c="33663">to</st> <st c="33667">+</st> <st c="33668">∞</st><st
    c="33669">, the answer is no.</st> <st c="33689">If the range of outcomes is in
    a finite interval, say between -10 and 10, we still have an infinite number of
    outcomes, and so everything we have said before still holds.</st> <st c="33860">It
    just means the PDF is zero outside of that interval -10</st> <st c="33919">to
    10.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33925">Now that we have explained the subtle difference between a probability
    distribution and a PDF, it is worth highlighting</st> <st c="34046">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34060">Most statisticians and data scientists will use the term</st>
    *<st c="34118">probability distribution</st>* <st c="34142">when they mean PDF,
    with the assumption that the reader will implicitly know what is</st> <st c="34228">really
    meant.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34241">Given an infinite number of possible outcomes and probabilities
    referring to small intervals, all the results we have given about expectation
    values for discrete random variables also hold for continuous random variables,
    with the simple replacement of the probability distribution</st> <st c="34524">p</st><st
    c="34525">(</st><st c="34526">x</st><st c="34527">)</st> <st c="34528">by the
    density function</st> <st c="34552">f</st><st c="34553">(</st><st c="34554">x</st><st
    c="34555">)</st><st c="34556">, and the summation symbol</st> <st c="34583">Σ</st>
    <st c="34584">by the integration symbol</st> <st c="34611">∫</st><st c="34612">.
    We will recap those results now, with</st> <st c="34652">those replacements.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34671">If we have a continuous-valued random variable</st> <st c="34719">X</st>
    <st c="34720">with outcomes denoted by</st> <st c="34746">x</st> <st c="34747">and
    a PDF</st> <st c="34758">f</st><st c="34759">(</st><st c="34760">x</st><st c="34761">)</st><st
    c="34762">, then we have the</st> <st c="34781">following results:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34799">∫</st><st c="34801">−</st><st c="34802">∞</st><st c="34803">+</st><st
    c="34804">∞</st><st c="34805">f</st><st c="34806">(</st><st c="34807">x</st><st
    c="34808">)</st> <st c="34809">dx</st> <st c="34811">=</st> <st c="34813">1</st>
    <st c="34814">Normalization</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34827">Eq.</st> <st c="34832">18</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34834">𝔼</st><st c="34837">(</st><st c="34839">X</st><st c="34840">)</st>
    <st c="34841">=</st> <st c="34842">μ</st> <st c="34843">=</st> <st c="34844">∫</st><st
    c="34845">−</st><st c="34846">∞</st><st c="34847">+</st><st c="34848">∞</st><st
    c="34849">xf</st><st c="34851">(</st><st c="34853">x</st><st c="34854">)</st>
    <st c="34855">dx</st> <st c="34857">Mean</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34862">Eq.</st> <st c="34867">19</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34869">𝔼</st><st c="34872">(</st><st c="34874">(</st><st c="34875">X</st><st
    c="34876">−</st> <st c="34877">μ</st><st c="34878">)</st><st c="34879">2</st><st
    c="34880">)</st> <st c="34881">=</st> <st c="34882">σ</st><st c="34883">2</st>
    <st c="34884">=</st> <st c="34885">∫</st><st c="34886">−</st><st c="34887">∞</st><st
    c="34888">+</st><st c="34889">∞</st><st c="34890">(</st><st c="34891">x</st><st
    c="34892">−</st> <st c="34893">μ</st><st c="34894">)</st><st c="34895">2</st><st
    c="34896">f</st><st c="34897">(</st><st c="34898">x</st><st c="34899">)</st> <st
    c="34900">dx</st> <st c="34902">Variance</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34911">Eq.</st> <st c="34916">20</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34918">Now we have</st> <st c="34931">learned about single random variables,
    whether discrete or continuous, we are next going to learn how to transform and
    combine multiple random variables.</st> <st c="35085">This will enable us to describe
    and handle the impact of randomness in data when we apply transformations and
    aggregations</st> <st c="35208">to data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35216">Transforming and combining random variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="35260">Now we have learned some properties of random variables, we are
    going to learn about what happens when we combine</st> <st c="35375">random variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35392">Why is this important?</st> <st c="35416">Well, often, we will
    want to modify or aggregate our data.</st> <st c="35475">If each observation in
    a dataset is a random variable, then what does that mean for the total of all
    the observations?</st> <st c="35594">We know the total is also a random variable,
    but what are</st> <st c="35652">its properties?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35667">As a simple example, consider whether an individual shopper buys
    an item from a website.</st> <st c="35757">We can model that individual purchase
    decision as a Bernoulli random variable.</st> <st c="35836">But what about the
    total number of items sold in a day, bought by the 1,000 visitors to the website?</st>
    <st c="35937">How do we model</st> <st c="35953">that total?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35964">Linear transformations</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="35987">We’ll start</st> <st c="35999">with something simpler.</st> <st
    c="36024">What happens when we just linearly transform a random variable?</st>
    <st c="36088">If we have a random variable</st> <st c="36117">X</st><st c="36118">,
    we</st> <st c="36122">can create a new random variable</st> <st c="36156">Y</st>
    <st c="36157">=</st> <st c="36158">aX</st> <st c="36160">+</st> <st c="36162">b</st><st
    c="36163">. What does this transformation mean?</st> <st c="36201">Well, if the
    random variable</st> <st c="36230">X</st> <st c="36231">has possible outcomes</st>
    <st c="36254">x</st><st c="36255">, then it means that the random variable</st>
    <st c="36296">Y</st> <st c="36297">has possible outcomes</st> <st c="36320">y</st>
    <st c="36321">=</st> <st c="36322">ax</st> <st c="36324">+</st> <st c="36326">b</st><st
    c="36327">. In terms of a dataset, it means that if we have a particular value
    – say, 10 – for an outcome of</st> <st c="36426">X</st><st c="36427">, then the
    corresponding outcome value of</st> <st c="36469">Y</st> <st c="36470">is just</st>
    <st c="36479">a</st> <st c="36480">×</st> <st c="36481">10</st> <st c="36483">+</st>
    <st c="36485">b</st><st c="36486">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36487">Given the linear transformation</st> <st c="36520">Y</st> <st
    c="36521">=</st> <st c="36522">aX</st> <st c="36524">+</st> <st c="36526">b</st><st
    c="36527">, how does the mean of</st> <st c="36550">Y</st> <st c="36551">relate
    to the mean of</st> <st c="36574">X</st><st c="36575">? From</st> *<st c="36582">Eq.</st>
    <st c="36586">7</st>*<st c="36587">, we find that for discrete random variables,
    the</st> <st c="36637">foll</st><st c="36641">owing applies:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36656">𝔼</st><st c="36659">(</st><st c="36661">Y</st><st c="36662">)</st>
    <st c="36663">=</st> <st c="36664">𝔼</st><st c="36666">(</st><st c="36668">aX</st>
    <st c="36670">+</st> <st c="36672">b</st><st c="36673">)</st> <st c="36674">=</st>
    <st c="36675">∑</st><st c="36676">x</st><st c="36677">(</st><st c="36678">ax</st>
    <st c="36680">+</st> <st c="36682">b</st><st c="36683">)</st> <st c="36684">p</st><st
    c="36685">(</st><st c="36686">x</st><st c="36687">)</st> <st c="36688">=</st>
    <st c="36689">a</st><st c="36690">∑</st><st c="36691">x</st><st c="36692">x</st>
    <st c="36693">p</st><st c="36694">(</st><st c="36695">x</st><st c="36696">)</st>
    <st c="36697">+</st> <st c="36698">b</st><st c="36699">∑</st><st c="36700">x</st><st
    c="36701">p</st><st c="36702">(</st><st c="36703">x</st><st c="36704">)</st> <st
    c="36705">=</st> <st c="36706">a</st><st c="36707">𝔼</st><st c="36709">(</st><st
    c="36711">X</st><st c="36712">)</st> <st c="36713">+</st> <st c="36714">b</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36715">Eq.</st> <st c="36719">21</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36721">So, the</st> <st c="36729">mean of</st> <st c="36738">Y</st> <st
    c="36739">is simply related to the mean of</st> <st c="36773">X</st> <st c="36774">by
    the same linear transformation that we applied to the random variable.</st> <st
    c="36849">An</st> <st c="36852">identical result holds for continuous-valued random
    variables – as you might have guessed, we simply replace</st> <st c="36961">∑</st>
    <st c="36962">with</st> <st c="36968">∫</st><st c="36969">and the probability</st>
    <st c="36989">p</st><st c="36990">(</st><st c="36991">x</st><st c="36992">)</st>
    <st c="36993">with the PDF</st> <st c="37007">f</st><st c="37008">(</st><st c="37009">x</st><st
    c="37010">)</st> <st c="37011">in</st> *<st c="37015">Eq.</st> <st c="37019">21</st>*<st
    c="37021">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37022">What happens to the variance?</st> <st c="37053">A similar simple
    calculation shows that for both discrete and continuous random variables, if</st>
    <st c="37147">Y</st> <st c="37148">=</st> <st c="37149">aX</st> <st c="37151">+</st>
    <st c="37153">b</st><st c="37154">, then the</st> <st c="37165">following applies:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37183">Var(</st><st c="37188">Y</st><st c="37190">)</st> <st c="37191">=</st>
    <st c="37192">a</st><st c="37193">2</st> <st c="37194">Var(X)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37200">Eq.</st> <st c="37205">22</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37207">Non-linear transformations</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="37234">What</st> <st c="37239">happens if we apply a more general</st>
    <st c="37275">non-linear transformation to a random variable?</st> <st c="37323">This
    would be the case if we were to apply a non-linear transformation to our dataset.</st>
    <st c="37410">Let’s apply</st> <st c="37422">the function</st> <st c="37434">h</st><st
    c="37436">(</st><st c="37437">x</st><st c="37438">)</st> <st c="37439">to the
    values</st> <st c="37454">x</st> <st c="37455">in our dataset.</st> <st c="37472">We
    model this by saying we have a random variable</st> <st c="37522">Y</st> <st c="37523">=</st>
    <st c="37524">h</st><st c="37525">(</st><st c="37526">X</st><st c="37527">)</st><st
    c="37528">. For discrete outcomes, to calculate the mean of</st> <st c="37578">Y</st>
    <st c="37579">we just calculate</st> <st c="37598">∑</st><st c="37599">y</st><st
    c="37600">y</st> <st c="37601">p</st><st c="37602">(</st><st c="37603">y</st><st
    c="37604">)</st><st c="37605">, so we just need to know the probability,</st>
    <st c="37648">p</st><st c="37649">(</st><st c="37650">y</st><st c="37651">)</st><st
    c="37652">, of each outcome.</st> <st c="37671">But since for each value of</st>
    <st c="37699">x</st> <st c="37700">we know the corresponding value of</st> <st
    c="37736">y</st> <st c="37737">and we know the probabilities</st> <st c="37768">p</st><st
    c="37769">(</st><st c="37770">x</st><st c="37771">)</st><st c="37772">, then calculating
    the mean of</st> <st c="37803">Y</st> <st c="37804">is easy:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37812">𝔼</st><st c="37815">(</st><st c="37817">Y</st><st c="37818">)</st>
    <st c="37819">=</st> <st c="37820">∑</st><st c="37821">y</st><st c="37822">y</st>
    <st c="37823">p</st><st c="37824">(</st><st c="37825">y</st><st c="37826">)</st>
    <st c="37827">=</st> <st c="37828">∑</st><st c="37829">x</st><st c="37830">h</st><st
    c="37831">(</st><st c="37832">x</st><st c="37833">)</st><st c="37834">p</st><st
    c="37835">(</st><st c="37836">x</st><st c="37837">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37838">Eq.</st> <st c="37842">23</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37844">The formal proof of the right-hand side of the formula in Eq.</st>
    <st c="37907">23 is more nuanced.</st> <st c="37927">In the way we have simply
    written the right-hand side, we are making use of “the law of the unconscious</st>
    <st c="38031">statistician.” For the purposes of this book, we will take the right-hand
    side of</st> *<st c="38113">Eq.</st> <st c="38117">23</st>* <st c="38119">as the
    definition of</st> <st c="38141">𝔼</st><st c="38143">(</st><st c="38145">Y</st><st
    c="38146">)</st> <st c="38147">when</st> <st c="38153">Y</st> <st c="38154">is
    a transformation of another</st> <st c="38186">random variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38202">For the</st> <st c="38211">case where the transformation</st>
    <st c="38241">h</st><st c="38242">(</st><st c="38243">x</st><st c="38244">)</st>
    <st c="38245">is linear, it is easy to see we get the same result as in</st> *<st
    c="38304">Eq.</st> <st c="38308">21</st>*<st c="38310">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38311">Now, for continuous random variables, we have a</st> <st c="38360">similar
    result:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38375">𝔼</st><st c="38378">(</st><st c="38380">Y</st><st c="38381">)</st>
    <st c="38382">=</st> <st c="38383">∫</st><st c="38384">−</st><st c="38385">∞</st><st
    c="38386">+</st><st c="38387">∞</st><st c="38388">h</st><st c="38389">(</st><st
    c="38390">x</st><st c="38391">)</st><st c="38392">f</st><st c="38393">(</st><st
    c="38394">x</st><st c="38395">)</st><st c="38396">dx</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38398">Eq.</st> <st c="38403">24</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38405">To write</st> <st c="38415">𝔼</st><st c="38417">(</st><st c="38419">Y</st><st
    c="38420">)</st> <st c="38421">as an integral over outcomes</st> <st c="38451">y</st>
    <st c="38452">we would have to perform a change of variable under the integration
    sign.</st> <st c="38527">For simplicity, we’ll assume the transformation</st>
    <st c="38575">h</st><st c="38576">(</st><st c="38577">x</st><st c="38578">)</st>
    <st c="38579">is monotonic.</st> <st c="38594">If we do this, w</st><st c="38610">e
    get</st> <st c="38617">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38631">𝔼</st><st c="38634">(</st><st c="38636">Y</st><st c="38637">)</st>
    <st c="38638">=</st> <st c="38639">∫</st><st c="38640">−</st><st c="38641">∞</st><st
    c="38642">+</st><st c="38643">∞</st><st c="38644">h</st><st c="38645">(</st><st
    c="38646">x</st><st c="38647">)</st><st c="38648">f</st><st c="38649">(</st><st
    c="38650">x</st><st c="38651">)</st><st c="38652">dx</st> <st c="38654">=</st>
    <st c="38656">∫</st><st c="38657">f</st><st c="38658">(</st><st c="38659">−</st><st
    c="38660">∞</st><st c="38661">)</st><st c="38662">f</st><st c="38663">(</st><st
    c="38664">+</st><st c="38665">∞</st><st c="38666">)</st><st c="38667">yf</st><st
    c="38669">(</st><st c="38671">x</st> <st c="38672">=</st> <st c="38673">h</st><st
    c="38674">−</st><st c="38675">1</st><st c="38676">(</st><st c="38677">y</st><st
    c="38678">)</st><st c="38679">)</st> <st c="38680">dx</st><st c="38682">_</st><st
    c="38684">dy</st> <st c="38686">dy</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38689">Eq.</st> <st c="38694">25</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38696">Looking at what is inside the integral in the last expression
    on the right-hand side of</st> *<st c="38785">Eq.</st> <st c="38789">25</st>*<st
    c="38791">, we can see we have an effective PDF for the random variable</st> <st
    c="38853">Y</st><st c="38854">. That effective P</st><st c="38872">DF is</st>
    <st c="38879">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38893">f</st><st c="38895">(</st><st c="38896">h</st><st c="38897">−</st><st
    c="38898">1</st><st c="38899">(</st><st c="38900">y</st><st c="38901">)</st><st
    c="38902">)</st><st c="38903">|</st><st c="38904">dx</st><st c="38906">_</st><st
    c="38908">dy</st><st c="38910">|</st> <st c="38912">=</st> <st c="38913">f</st><st
    c="38914">(</st><st c="38915">h</st><st c="38916">−</st><st c="38917">1</st><st
    c="38918">(</st><st c="38919">y</st><st c="38920">)</st><st c="38921">)</st> <st
    c="38922">1</st><st c="38923">_</st><st c="38924">|</st><st c="38925">h</st><st
    c="38926">′</st><st c="38927">(</st><st c="38928">h</st><st c="38929">−</st><st
    c="38930">1</st><st c="38931">(</st><st c="38932">y</st><st c="38933">)</st><st
    c="38934">)</st><st c="38935">|</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38936">Eq.</st> <st c="38940">26</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38942">We can make some immediate comments on the</st> <st c="38986">preceding
    result:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39003">Firstly, the obvious – if we apply a transformation to a continuous
    random variable, the probability</st> <st c="39105">density changes.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39121">We can calculate the PDF of the new transformed random variable
    using the expression in</st> *<st c="39210">Eq.</st> <st c="39214">26</st>*<st
    c="39216">, but only because the function</st> <st c="39248">h</st><st c="39249">−</st><st
    c="39250">1</st><st c="39251">(</st><st c="39252">y</st><st c="39253">)</st> <st
    c="39254">exists and can be calculated.</st> <st c="39285">This is because the
    transformation</st> <st c="39320">y</st> <st c="39321">=</st> <st c="39322">h</st><st
    c="39323">(</st><st c="39324">x</st><st c="39325">)</st> <st c="39326">is monotonic
    and so invertible, meaning that a single value of</st> <st c="39390">y</st> <st
    c="39391">can only have come from a single possible value of</st> <st c="39443">x</st><st
    c="39444">. If the transformation</st> <st c="39468">y</st> <st c="39469">=</st>
    <st c="39470">h</st><st c="39471">(</st><st c="39472">x</st><st c="39473">)</st>
    <st c="39474">is not monotonic it is still possible to determine a PDF for</st>
    <st c="39536">y</st> <st c="39537">by dividing</st> <st c="39550">h</st><st c="39551">(</st><st
    c="39552">x</st><st c="39553">)</st> <st c="39554">into individual monotonic segments,
    but the resulting expression is</st> <st c="39623">more complicated.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39640">The</st> <st c="39645">expression in</st> *<st c="39659">Eq.</st>
    <st c="39663">26</st>* <st c="39665">looks complicated, but underneath it is a
    very simple principle.</st> <st c="39731">Probability is about counting how many
    outcomes of a particular type we see.</st> <st c="39808">If we count how many</st>
    <st c="39829">outcomes we see in an interval between</st> <st c="39868">x</st>
    <st c="39869">and</st> <st c="39874">x</st> <st c="39875">+</st> <st c="39876">dx</st><st
    c="39878">, then that number is simply the density times the volume of the interval
    (using our physical analogy).</st> <st c="39982">That number is also the same
    (it is invariant) whether we count using</st> <st c="40052">x</st> <st c="40053">as
    our measure of volume or whether we count using</st> <st c="40105">y</st> <st
    c="40106">as our measure of volume.</st> <st c="40133">It is like counting the
    number of molecules in a given volume – it is the same number of molecules whether
    we measure volume in millimeters or centimeters.</st> <st c="40289">So, for probability
    densities, we have</st> <st c="40328">the following:</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="40342">Probability Density of</st> <st c="40366">Y</st> <st c="40367">×</st>
    <st c="40368">Volume of interval for</st> <st c="40391">Y</st> <st c="40392">=</st>
    <st c="40393">Probability Density of</st> <st c="40416">X</st> <st c="40417">×
    Volume of interval for</st> <st c="40443">X</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40444">Eq.</st> <st c="40448">27</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40450">Or, more exactly, w</st><st c="40470">e have</st> <st c="40478">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40492">|</st> <st c="40494">f</st><st c="40495">Y</st><st c="40496">(</st><st
    c="40497">y</st><st c="40498">)</st><st c="40499">dy</st><st c="40501">|</st>
    <st c="40503">=</st> <st c="40504">|</st><st c="40505">f</st><st c="40506">X</st><st
    c="40507">(</st><st c="40508">x</st><st c="40509">)</st><st c="40510">dx</st><st
    c="40512">|</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40514">Eq.</st> <st c="40518">28</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40520">Here, we have been more explicit and used</st> <st c="40563">f</st><st
    c="40564">X</st><st c="40565">(</st><st c="40566">x</st><st c="40567">)</st> <st
    c="40568">to denote the PDF of the random variable</st> <st c="40610">X</st><st
    c="40611">, evaluated at outcome value</st> <st c="40640">x</st><st c="40641">,
    and</st> <st c="40647">f</st><st c="40648">Y</st><st c="40649">(</st><st c="40650">y</st><st
    c="40651">)</st> <st c="40652">to denote the PDF of the random variable</st> <st
    c="40694">Y</st><st c="40695">, evaluated at outcome</st> <st c="40718">value</st>
    <st c="40724">y</st><st c="40725">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40726">If you just then plug the transformation</st> <st c="40768">y</st>
    <st c="40769">=</st> <st c="40770">h</st><st c="40771">(</st><st c="40772">x</st><st
    c="40773">)</st> <st c="40774">into</st> *<st c="40780">Eq.</st> <st c="40784">28</st>*<st
    c="40786">, you will get the same result as in</st> *<st c="40823">Eq.</st> <st
    c="40827">26</st>*<st c="40829">. Personally, I prefer this method of working
    out how a PDF transforms because I can never remember the complex formula in</st>
    *<st c="40952">Eq.</st> <st c="40956">26</st>*<st c="40958">, and because the
    result in</st> *<st c="40986">Eq.</st> <st c="40990">28</st>* <st c="40992">is
    based on a simple principle that I can remember – it doesn’t matter how you count
    how many things are in a given volume; the number is always</st> <st c="41138">the
    same.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41147">Combining random variables</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="41174">Now, let’s get</st> <st c="41190">back to our original question
    of how to combine random variables.</st> <st c="41256">Imagine I have two random
    variables</st> <st c="41292">X</st><st c="41293">1</st> <st c="41294">and</st>
    <st c="41299">X</st><st c="41300">2</st> <st c="41301">that have outcomes</st>
    <st c="41321">x</st><st c="41322">1</st> <st c="41323">and</st> <st c="41328">x</st><st
    c="41329">2</st> <st c="41330">respectively.</st> <st c="41345">What is the mean
    of the random variable</st> <st c="41385">Y</st> <st c="41386">=</st> <st c="41387">X</st><st
    c="41388">1</st><st c="41389">+</st> <st c="41390">X</st><st c="41391">2</st><st
    c="41392">? You can probably guess, but let’s wo</st><st c="41430">rk things</st>
    <st c="41441">out properly:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41454">𝔼</st><st c="41457">(</st><st c="41459">Y</st><st c="41460">)</st>
    <st c="41461">=</st> <st c="41462">∑</st><st c="41463">x</st><st c="41464">1</st><st
    c="41465">,</st> <st c="41466">x</st><st c="41467">2</st><st c="41468">(</st><st
    c="41469">x</st><st c="41470">1</st><st c="41471">+</st> <st c="41472">x</st><st
    c="41473">2</st><st c="41474">)</st> <st c="41475">p</st><st c="41476">(</st><st
    c="41477">x</st><st c="41478">1</st><st c="41479">,</st> <st c="41480">x</st><st
    c="41481">2</st><st c="41482">)</st> <st c="41483">=</st> <st c="41484">∑</st><st
    c="41485">x</st><st c="41486">1</st><st c="41487">x</st><st c="41488">1</st> <st
    c="41489">∑</st><st c="41490">x</st><st c="41491">2</st><st c="41492">p</st><st
    c="41493">(</st><st c="41494">x</st><st c="41495">1</st><st c="41496">,</st> <st
    c="41497">x</st><st c="41498">2</st><st c="41499">)</st> <st c="41500">+</st>
    <st c="41501">∑</st><st c="41502">x</st><st c="41503">2</st><st c="41504">x</st><st
    c="41505">2</st><st c="41506">∑</st><st c="41507">x</st><st c="41508">1</st><st
    c="41509">p</st><st c="41510">(</st><st c="41511">x</st><st c="41512">1</st><st
    c="41513">,</st> <st c="41514">x</st><st c="41515">2</st><st c="41516">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41517">Eq.</st> <st c="41521">29</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41523">Now,</st> <st c="41529">p</st><st c="41530">(</st><st c="41531">x</st><st
    c="41532">1</st><st c="41533">,</st> <st c="41534">x</st><st c="41535">2</st><st
    c="41536">)</st> <st c="41537">is the probability of seeing outcomes</st> <st
    c="41576">x</st><st c="41577">1</st> <st c="41578">and</st> <st c="41583">x</st><st
    c="41584">2</st> <st c="41585">together, or jointly, and so is called the</st>
    **<st c="41629">joint probability</st>** <st c="41646">or</st> **<st c="41650">joint
    distribution</st>** <st c="41668">of</st> <st c="41672">X</st><st c="41673">1</st>
    <st c="41674">and</st> <st c="41679">X</st><st c="41680">2</st><st c="41681">.
    One of the properties of the</st> <st c="41712">joint distribution is that if
    we sum over all the possible values of</st> <st c="41781">x</st><st c="41782">2</st>
    <st c="41783">we get</st> <st c="41791">p</st><st c="41792">(</st><st c="41793">x</st><st
    c="41794">1</st><st c="41795">)</st><st c="41796">, or in other words, we get</st>
    <st c="41824">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41838">∑</st><st c="41840">x</st><st c="41841">2</st><st c="41842">p</st><st
    c="41843">(</st><st c="41844">x</st><st c="41845">1</st><st c="41846">,</st> <st
    c="41847">x</st><st c="41848">2</st><st c="41849">)</st> <st c="41850">=</st>
    <st c="41851">p</st><st c="41852">(</st><st c="41853">x</st><st c="41854">1</st><st
    c="41855">)</st> <st c="41856">and similarly</st> <st c="41871">∑</st><st c="41872">x</st><st
    c="41873">1</st><st c="41874">p</st><st c="41875">(</st><st c="41876">x</st><st
    c="41877">1</st><st c="41878">,</st> <st c="41879">x</st><st c="41880">2</st><st
    c="41881">)</st> <st c="41882">=</st> <st c="41883">p</st><st c="41884">(</st><st
    c="41885">x</st><st c="41886">2</st><st c="41887">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41888">Eq.</st> <st c="41892">30</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41894">Plugging this into</st> *<st c="41914">Eq.</st> <st c="41918">2</st><st
    c="41919">9</st>*<st c="41920">, we get</st> <st c="41929">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41943">𝔼</st><st c="41946">(</st><st c="41948">X</st><st c="41949">1</st><st
    c="41950">+</st> <st c="41951">X</st><st c="41952">2</st><st c="41953">)</st>
    <st c="41954">=</st> <st c="41955">∑</st><st c="41956">x</st><st c="41957">1</st><st
    c="41958">x</st><st c="41959">1</st> <st c="41960">p</st><st c="41961">(</st><st
    c="41962">x</st><st c="41963">1</st><st c="41964">)</st><st c="41965">+</st> <st
    c="41966">∑</st><st c="41967">x</st><st c="41968">2</st><st c="41969">x</st><st
    c="41970">2</st> <st c="41971">p</st><st c="41972">(</st><st c="41973">x</st><st
    c="41974">2</st><st c="41975">)</st> <st c="41976">=</st> <st c="41977">𝔼</st><st
    c="41979">(</st><st c="41981">X</st><st c="41982">1</st><st c="41983">)</st><st
    c="41984">+</st> <st c="41985">𝔼</st><st c="41987">(</st><st c="41989">X</st><st
    c="41990">2</st><st c="41991">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41992">Eq.</st> <st c="41996">31</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41998">Hopefully, this is what you will have guessed.</st> <st c="42046">The
    same result holds for continuous-valued random variables, again just changing
    summation</st> <st c="42138">for integration.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42154">We can iteratively apply the result in</st> *<st c="42194">Eq.</st>
    <st c="42198">31</st>* <st c="42200">to adding together many random variabl</st><st
    c="42239">es to get</st> <st c="42250">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42264">𝔼</st><st c="42267">(</st><st c="42269">X</st><st c="42270">1</st><st
    c="42271">+</st> <st c="42272">X</st><st c="42273">2</st><st c="42274">+</st>
    <st c="42275">⋯</st> <st c="42276">+</st> <st c="42277">X</st><st c="42278">N</st><st
    c="42279">)</st> <st c="42280">=</st> <st c="42281">𝔼</st><st c="42283">(</st><st
    c="42285">X</st><st c="42286">1</st><st c="42287">)</st><st c="42288">+</st> <st
    c="42289">𝔼</st><st c="42291">(</st><st c="42293">X</st><st c="42294">2</st><st
    c="42295">)</st><st c="42296">+</st> <st c="42297">⋯</st> <st c="42298">+</st>
    <st c="42299">𝔼</st><st c="42301">(</st><st c="42303">X</st><st c="42304">N</st><st
    c="42305">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42306">Eq.</st> <st c="42310">32</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42312">It is also a simple extension to include linear transformations
    of those</st> <st c="42386">N</st> <st c="42387">random variables to give</st>
    <st c="42413">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42427">𝔼</st><st c="42430">(</st><st c="42432">a</st><st c="42433">1</st>
    <st c="42434">X</st><st c="42435">1</st><st c="42436">+</st> <st c="42437">a</st><st
    c="42438">2</st> <st c="42439">X</st><st c="42440">2</st><st c="42441">+</st>
    <st c="42442">⋯</st> <st c="42443">+</st> <st c="42444">a</st><st c="42445">N</st>
    <st c="42446">X</st><st c="42447">N</st><st c="42448">)</st> <st c="42449">=</st>
    <st c="42450">a</st><st c="42451">1</st> <st c="42452">𝔼</st><st c="42454">(</st><st
    c="42456">X</st><st c="42457">1</st><st c="42458">)</st><st c="42459">+</st> <st
    c="42460">a</st><st c="42461">2</st> <st c="42462">𝔼</st><st c="42464">(</st><st
    c="42466">X</st><st c="42467">2</st><st c="42468">)</st><st c="42469">+</st> <st
    c="42470">⋯</st> <st c="42471">+</st> <st c="42472">a</st><st c="42473">N</st>
    <st c="42474">𝔼</st><st c="42476">(</st><st c="42478">X</st><st c="42479">N</st><st
    c="42480">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42481">Eq.</st> <st c="42485">33</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42487">Here, the weights</st> <st c="42506">a</st><st c="42507">1</st><st
    c="42508">,</st> <st c="42509">a</st><st c="42510">2</st><st c="42511">,</st>
    <st c="42512">⋯</st> <st c="42513">,</st> <st c="42514">a</st><st c="42515">N</st>
    <st c="42516">are</st> <st c="42521">fixed numbers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42535">Okay – but what about the variance of</st> <st c="42574">X</st><st
    c="42575">1</st><st c="42576">+</st> <st c="42577">X</st><st c="42578">2</st><st
    c="42579">+</st> <st c="42580">⋯</st> <st c="42581">+</st> <st c="42582">X</st><st
    c="42583">N</st><st c="42584">? A lengthy, but similar calculation to that in</st>
    *<st c="42632">Eq.</st> <st c="42636">29</st>* <st c="42638">shows that if</st>
    <st c="42653">X</st><st c="42654">1</st><st c="42655">,</st> <st c="42656">X</st><st
    c="42657">2</st><st c="42658">,</st> <st c="42659">⋯</st> <st c="42660">,</st>
    <st c="42661">X</st><st c="42662">N</st> <st c="42663">are independent of each
    other, the</st><st c="42698">n the</st> <st c="42705">following applies:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42723">Var</st><st c="42727">(</st><st c="42729">X</st><st c="42730">1</st><st
    c="42731">+</st> <st c="42732">X</st><st c="42733">2</st><st c="42734">+</st>
    <st c="42735">⋯</st> <st c="42736">+</st> <st c="42737">X</st><st c="42738">N</st><st
    c="42739">)</st> <st c="42740">=</st> <st c="42741">Var</st><st c="42744">(</st><st
    c="42746">X</st><st c="42747">1</st><st c="42748">)</st><st c="42749">+</st> <st
    c="42750">Var</st><st c="42753">(</st><st c="42755">X</st><st c="42756">2</st><st
    c="42757">)</st><st c="42758">+</st> <st c="42759">⋯</st> <st c="42760">+</st>
    <st c="42761">Var</st><st c="42764">(</st><st c="42766">X</st><st c="42767">N</st><st
    c="42768">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42769">Eq.</st> <st c="42773">34</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42775">Likewise, if the different random variables are independent of
    each other, we have</st> <st c="42859">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42873">Var</st><st c="42877">(</st><st c="42879">a</st><st c="42880">1</st>
    <st c="42881">X</st><st c="42882">1</st><st c="42883">+</st> <st c="42884">a</st><st
    c="42885">2</st> <st c="42886">X</st><st c="42887">2</st><st c="42888">+</st>
    <st c="42889">⋯</st> <st c="42890">+</st> <st c="42891">a</st><st c="42892">N</st>
    <st c="42893">X</st><st c="42894">N</st><st c="42895">)</st> <st c="42896">=</st>
    <st c="42897">a</st><st c="42898">1</st><st c="42899">2</st> <st c="42900">Var</st><st
    c="42903">(</st><st c="42905">X</st><st c="42906">1</st><st c="42907">)</st><st
    c="42908">+</st> <st c="42909">a</st><st c="42910">2</st><st c="42911">2</st>
    <st c="42912">Var</st><st c="42915">(</st><st c="42917">X</st><st c="42918">2</st><st
    c="42919">)</st><st c="42920">+</st> <st c="42921">⋯</st> <st c="42922">+</st>
    <st c="42923">a</st><st c="42924">N</st><st c="42925">2</st> <st c="42926">Var</st><st
    c="42929">(</st><st c="42931">X</st><st c="42932">N</st><st c="42933">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42934">Eq.</st> <st c="42938">35</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="42940">Now we</st> <st c="42948">have learned the basics of random variables
    and probability distributions, we will describe in detail some of the most common
    distributions you will encounter as a data scientist.</st> <st c="43128">These
    are the distributions you will make extensive use of in your career as a data
    scientist.</st> <st c="43223">We divide our descriptions into two obvious categories:
    descriptions of discrete-valued distributions and descriptions of</st> <st c="43345">continuous-valued
    distributions.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43377">Named distributions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43397">There are a</st> <st c="43410">few distributions that we encounter
    a lot as data scientists.</st> <st c="43472">These are the distributions that
    have characteristics that match the sort of data we encounter in real-world datasets,
    so it is unsurprising that we should use these common distributions when analyzing
    real data or when building predictive models.</st> <st c="43720">For that reason,
    it is worth understanding these distributions in a little more detail.</st> <st
    c="43808">We will go into that</st> <st c="43829">detail now.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43840">Discrete distributions</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="43863">We’ll start with</st> <st c="43881">some of the most important</st>
    <st c="43907">named</st> <st c="43914">discrete distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43937">The Bernoulli distribution</st>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <st c="43964">We</st> <st c="43968">already met the Bernoulli distribution</st>
    <st c="44006">when we first introduced random variables.</st> <st c="44050">However,
    we can calculate some of its key properties with what we have learned since then.</st>
    <st c="44141">You’ll recall that a Bernoulli random variable has two outcomes,
    0 and 1\.</st> <st c="44215">If</st> <st c="44218">X</st> <st c="44219">~</st>
    <st c="44220">Bernoulli</st><st c="44229">(</st><st c="44231">p</st><st c="44232">)</st>
    <st c="44233">and we draw an observation, then</st> <st c="44267">Prob</st><st
    c="44271">(</st><st c="44273">X</st> <st c="44274">=</st> <st c="44275">0</st><st
    c="44276">)</st> <st c="44277">=</st> <st c="44278">(</st><st c="44279">1</st>
    <st c="44280">−</st> <st c="44281">p</st><st c="44282">)</st> <st c="44283">,</st>
    <st c="44284">Prob</st><st c="44288">(</st><st c="44290">X</st> <st c="44291">=</st>
    <st c="44292">1</st><st c="44293">)</st> <st c="44294">=</st> <st c="44295">p</st><st
    c="44296">. The mean of the Bernoulli distribution is then calculated</st> <st
    c="44356">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44367">μ</st> <st c="44369">=</st> <st c="44370">∑</st><st c="44371">x</st><st
    c="44372">x</st> <st c="44373">p</st><st c="44374">(</st><st c="44375">x</st><st
    c="44376">)</st> <st c="44377">=</st> <st c="44378">[</st><st c="44379">0</st>
    <st c="44380">×</st> <st c="44381">(</st><st c="44382">1</st> <st c="44383">−</st>
    <st c="44384">p</st><st c="44385">)</st><st c="44386">]</st> <st c="44387">+</st>
    <st c="44388">[</st><st c="44389">1</st> <st c="44390">×</st> <st c="44391">p</st><st
    c="44392">]</st> <st c="44393">=</st> <st c="44394">p</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44395">Eq.</st> <st c="44399">36</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44401">Similarly, the variance is calculated</st> <st c="44440">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44451">σ</st><st c="44453">2</st> <st c="44454">=</st> <st c="44455">∑</st><st
    c="44456">x</st><st c="44457">(</st><st c="44458">x</st><st c="44459">−</st> <st
    c="44460">μ</st><st c="44461">)</st><st c="44462">2</st> <st c="44463">p</st><st
    c="44464">(</st><st c="44465">x</st><st c="44466">)</st> <st c="44467">=</st>
    <st c="44468">[</st><st c="44469">(</st><st c="44470">0</st> <st c="44471">−</st>
    <st c="44472">p</st><st c="44473">)</st><st c="44474">2</st> <st c="44475">×</st>
    <st c="44476">(</st><st c="44477">1</st> <st c="44478">−</st> <st c="44479">p</st><st
    c="44480">)</st><st c="44481">]</st><st c="44482">+</st> <st c="44483">[</st><st
    c="44484">(</st><st c="44485">1</st> <st c="44486">−</st> <st c="44487">p</st><st
    c="44488">)</st><st c="44489">2</st> <st c="44490">×</st> <st c="44491">p</st><st
    c="44492">]</st> <st c="44493">=</st> <st c="44494">p</st><st c="44495">(</st><st
    c="44496">1</st> <st c="44497">−</st> <st c="44498">p</st><st c="44499">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44500">Eq.</st> <st c="44504">37</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44506">Note the symmetry in the expression for the variance.</st> <st
    c="44561">If we swapped</st> <st c="44575">p</st> <st c="44576">for</st> <st c="44581">(</st><st
    c="44582">1</st> <st c="44583">−</st> <st c="44584">p</st><st c="44585">)</st>
    <st c="44586">and vice versa, we would get the same expression.</st> <st c="44637">This
    is because the choice of how we encode outcomes is arbitrary; that is, it is our
    subjective choice whether we use 1/0 or 0/1 to represent success/failure, and
    so certain properties of the distribution will be invariant to our choice of encoding.</st>
    <st c="44888">Switching from an encoding of 1/0 to 0/1 would obviously lead to
    us swapping the values</st> <st c="44976">p</st> <st c="44977">and</st> <st c="44982">(</st><st
    c="44983">1</st> <st c="44984">−</st> <st c="44985">p</st><st c="44986">)</st><st
    c="44987">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44988">The binomial distribution</st>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <st c="45014">Obviously, a</st> <st c="45028">Bernoulli random variable</st>
    <st c="45053">can be used to model data where we have two possible outcomes, such
    as whether a user clicked a button on an e-commerce website or whether a shopper
    bought a particular product.</st> <st c="45232">However, in a real-world situation,
    we are more interested in how many items we sell in total or how many website
    users in total click the button.</st> <st c="45379">Each individual choice to
    click or choice to buy is a Bernoulli random variable, but what is the distribution
    of</st> <st c="45492">their sum?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45502">Imagine we</st> <st c="45513">have</st> <st c="45519">N</st> <st
    c="45520">shoppers, each deciding to buy or not, so we can model each shopper
    choice as a Bernoulli random variable,</st> <st c="45628">X</st><st c="45629">i</st>
    <st c="45630">,</st> <st c="45631">i</st> <st c="45632">=</st> <st c="45633">1</st><st
    c="45634">,</st> <st c="45635">⋯</st> <st c="45636">,</st> <st c="45637">N</st><st
    c="45638">. For simplicity, we will assume that the shoppers are all similar so
    that the probability that any shopper makes a purchase is the same; that is,</st>
    <st c="45785">p</st><st c="45786">. This means</st> <st c="45799">X</st><st c="45800">i</st>
    <st c="45801">~</st> <st c="45802">Bernoulli</st><st c="45811">(</st><st c="45813">p</st><st
    c="45814">)</st> <st c="45815">for all values of</st> <st c="45834">i</st> <st
    c="45835">=</st> <st c="45836">1,2</st><st c="45839">,</st> <st c="45840">⋯</st>
    <st c="45841">,</st> <st c="45842">N</st><st c="45843">. Each of these random
    variables has</st> <st c="45880">the same distribution.</st> <st c="45903">Note
    this doesn’t mean they are all the same random variable, nor does it mean that
    all shoppers are making the same choice.</st> <st c="46028">It just means the
    distribution of possible outcomes is the same for each shopper.</st> <st c="46110">We
    consider this to be a reasonable assumption for our problem because our shoppers
    are all similar in other characteristics, and so we might expect them to have
    a similar level of preference for the shopping item we are modeling.</st> <st
    c="46341">When we have a set of random variables that all follow the same distribution,
    we say they are</st> **<st c="46435">identically distributed</st>** <st c="46458">(</st>**<st
    c="46460">i.d.</st>**<st c="46464">).</st> <st c="46468">If the</st> <st c="46474">random
    variables are also</st> **<st c="46501">independent</st>** <st c="46512">of each
    other, (in this case, that means the choice of one shopper does</st> <st c="46585">not
    have an influence on the choice of another shopper), then we say the random variables
    are</st> **<st c="46679">independently and identically distributed</st>**<st c="46720">,
    or</st> **<st c="46725">i.i.d</st>**<st c="46730">.</st> <st c="46732">for short.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46742">Now, what we are interested in is the total number of items sold.</st>
    <st c="46809">This is</st> <st c="46817">S</st> <st c="46818">=</st> <st c="46819">X</st><st
    c="46820">1</st><st c="46821">+</st> <st c="46822">X</st><st c="46823">2</st><st
    c="46824">+</st> <st c="46825">⋯</st> <st c="46826">+</st> <st c="46827">X</st><st
    c="46828">N</st><st c="46829">. What is the distribution of</st> <st c="46859">S</st><st
    c="46860">? Well, let’s use</st> <st c="46878">n</st> <st c="46879">to represent
    the actual number of units sold and consider one particular set of outcomes that
    would give us</st> <st c="46988">n</st> <st c="46989">=</st> <st c="46990">4</st>
    <st c="46991">items sold when we had</st> <st c="47015">N</st> <st c="47016">=</st>
    <st c="47017">10</st> <st c="47019">shoppers.</st> <st c="47030">The seque</st><st
    c="47039">nce in</st> *<st c="47047">Figure 2</st>**<st c="47055">.7</st>* <st
    c="47057">shows one such set</st> <st c="47077">of outcomes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Outcomes from a sequence of 10 Bernoulli trials with their corresponding
    probabilities below](img/B19496_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="47197">Figure 2.7: Outcomes from a sequence of 10 Bernoulli trials with
    their corresponding probabilities below</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47301">Here, we have used</st> *<st c="47321">Yes</st>* <st c="47324">to
    indicate a shopper purchased the item and</st> *<st c="47370">No</st>* <st c="47372">to
    indicate that they didn’t.</st> <st c="47403">What is the probability of this
    outcome?</st> <st c="47444">Since the random variables are independent of each
    other, we can multiply their individual outcome probabilities together, as shown
    in the lower row of</st> *<st c="47596">Figure 2</st>**<st c="47604">.7</st>*<st
    c="47606">. So, the probability of this particular pattern is</st> <st c="47658">p</st><st
    c="47659">4</st> <st c="47660">(</st><st c="47661">1</st> <st c="47662">−</st>
    <st c="47663">p</st><st c="47664">)</st><st c="47665">6</st><st c="47666">. Now,
    you may have spotted that the reason we got a factor of</st> <st c="47729">p</st><st
    c="47730">4</st> <st c="47731">in that result was because four shoppers bought
    the item, but it didn’t matter which four.</st> <st c="47823">So, another but
    different pattern with four shoppers buying the item would also have a probability
    of occurring of</st> <st c="47938">p</st><st c="47939">4</st> <st c="47940">(</st><st
    c="47941">1</st> <st c="47942">−</st> <st c="47943">p</st><st c="47944">)</st><st
    c="47945">6</st><st c="47946">. To find the total probability of</st> <st c="47981">n</st>
    <st c="47982">=</st> <st c="47983">4</st> <st c="47984">items sold in a set of
    ten shoppers, we just need to find how many patterns of ten shoppers we</st> <st
    c="48079">can have where there are four</st> <st c="48109">shoppers who buy.</st>
    <st c="48128">In other words, how many ways can we distribute the</st> <st c="48180">n</st>
    <st c="48181">=</st> <st c="48182">4</st> <st c="48183">successes among</st> <st
    c="48200">N</st> <st c="48201">=</st> <st c="48202">10</st> <st c="48204">attempts
    (trials)?</st> <st c="48224">This is the binomial coefficient</st> <st c="48257">(</st><st
    c="48258">10</st><st c="48260">4</st><st c="48262">)</st> <st c="48263">that we
    recapped in</st> [*<st c="48284">Chapter 1</st>*](B19496_01.xhtml#_idTextAnchor014)<st
    c="48293">. So, the overall probability of four items being bought by ten shoppers
    is</st> <st c="48369">(</st><st c="48370">10</st><st c="48372">4</st><st c="48374">)</st>
    <st c="48375">p</st><st c="48376">4</st> <st c="48377">(</st><st c="48378">1</st>
    <st c="48379">−</st> <st c="48380">p</st><st c="48381">)</st><st c="48382">6</st>
    <st c="48383">. It is straightforward to generalize to</st> <st c="48424">N</st>
    <st c="48425">shoppers and</st> <st c="48439">n</st> <st c="48440">items bought,
    to get the probability of</st> <st c="48481">n</st> <st c="48482">items bought</st>
    <st c="48496">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48507">Prob</st><st c="48512">(</st><st c="48514">S</st> <st c="48515">=</st>
    <st c="48516">n</st><st c="48517">)</st> <st c="48518">=</st> <st c="48519">(</st><st
    c="48520">N</st><st c="48521">n</st><st c="48522">)</st> <st c="48523">p</st><st
    c="48524">n</st> <st c="48525">(</st><st c="48526">1</st> <st c="48527">−</st>
    <st c="48528">p</st><st c="48529">)</st><st c="48530">N</st><st c="48531">−</st><st
    c="48532">n</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48533">Eq.</st> <st c="48537">38</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48539">This is the</st> **<st c="48552">binomial distribution</st>**<st
    c="48573">. It is the distribution of the sum of</st> <st c="48612">N</st> <st
    c="48613">i.i.d.</st> <st c="48621">Bernoulli random variables (or trials) each
    with success probability</st> <st c="48690">p</st><st c="48691">. Notice that
    the probability</st> <st c="48721">Prob</st><st c="48725">(</st><st c="48727">n</st><st
    c="48728">)</st> <st c="48729">depends upon two quantities or parameters:</st>
    <st c="48773">N</st> <st c="48774">and</st> <st c="48779">p</st><st c="48780">.
    When we write that a random variable has a binomial distribution, we write</st>
    <st c="48857">Binomial</st><st c="48865">(</st><st c="48867">N</st><st c="48868">,</st>
    <st c="48869">p</st><st c="48870">)</st> <st c="48871">or</st> <st c="48875">Binom</st><st
    c="48880">(</st><st c="48882">N</st><st c="48883">,</st> <st c="48884">p</st><st
    c="48885">)</st><st c="48886">. In our example, the total number of items sold,</st>
    <st c="48936">S</st><st c="48937">, has a binomial distribution, so we would write</st>
    <st c="48986">S</st> <st c="48987">~</st> <st c="48988">Binomial</st><st c="48996">(</st><st
    c="48998">N</st><st c="48999">,</st> <st c="49000">p</st><st c="49001">)</st><st
    c="49002">. In a modern e-commerce setting where we potentially have tens of millions
    of visitors to a website,</st> <st c="49104">N</st> <st c="49105">can be very,</st>
    <st c="49119">very big.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49128">Now we have derived the probabilities of the binomial distribution,
    we can calculate some of its characteristics, such as its mean</st> <st c="49260">and
    variance:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49273">Mean</st> <st c="49278">=</st> <st c="49280">μ</st> <st c="49281">=</st>
    <st c="49282">∑</st><st c="49283">n</st><st c="49284">=</st><st c="49285">0</st><st
    c="49286">N</st><st c="49287">(</st><st c="49288">N</st><st c="49289">n</st><st
    c="49290">)</st> <st c="49291">n</st> <st c="49292">p</st><st c="49293">n</st>
    <st c="49294">(</st><st c="49295">1</st> <st c="49296">−</st> <st c="49297">p</st><st
    c="49298">)</st><st c="49299">N</st><st c="49300">−</st><st c="49301">n</st> <st
    c="49302">=</st> <st c="49303">Np</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49305">Eq.</st> <st c="49310">39</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49312">Variance</st> <st c="49321">=</st> <st c="49323">σ</st><st c="49324">2</st>
    <st c="49325">=</st> <st c="49326">∑</st><st c="49327">n</st><st c="49328">=</st><st
    c="49329">0</st><st c="49330">N</st><st c="49331">(</st><st c="49332">N</st><st
    c="49333">n</st><st c="49334">)</st> <st c="49335">(</st><st c="49336">n</st>
    <st c="49337">−</st> <st c="49338">Np</st><st c="49340">)</st><st c="49342">2</st>
    <st c="49343">p</st><st c="49344">n</st> <st c="49345">(</st><st c="49346">1</st>
    <st c="49347">−</st> <st c="49348">p</st><st c="49349">)</st><st c="49350">N</st><st
    c="49351">−</st><st c="49352">n</st> <st c="49353">=</st> <st c="49354">Np</st><st
    c="49356">(</st><st c="49358">1</st> <st c="49359">−</st> <st c="49360">p</st><st
    c="49361">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49362">Eq.</st> <st c="49366">40</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49368">So, to summarize, for a binomial distribution the mean is</st>
    <st c="49427">Np</st> <st c="49429">and the variance is</st> <st c="49450">Np</st><st
    c="49452">(</st><st c="49454">1</st> <st c="49455">−</st> <st c="49456">p</st><st
    c="49457">)</st><st c="49458">. These results can be derived very simply.</st>
    <st c="49502">Since we know what the mean and variance of a</st> <st c="49548">Bernoulli</st><st
    c="49557">(</st><st c="49559">p</st><st c="49560">)</st> <st c="49561">random
    variable is, and our total</st> <st c="49596">S</st> <st c="49597">=</st> <st
    c="49598">X</st><st c="49599">1</st> <st c="49600">+</st> <st c="49601">X</st><st
    c="49602">2</st> <st c="49603">+</st> <st c="49604">⋯</st> <st c="49605">+</st>
    <st c="49606">X</st><st c="49607">N</st> <st c="49608">is just a sum of i.i.d.</st>
    <st c="49633">Bernoulli</st><st c="49642">(</st><st c="49644">p</st><st c="49645">)</st>
    <st c="49646">random variables, we can just use the results in</st> *<st c="49696">Eq.</st>
    <st c="49700">32</st>* <st c="49702">and</st> *<st c="49707">Eq.</st> <st c="49711">34</st>*
    <st c="49713">to derive them – see if you can do this</st> <st c="49754">for yourself.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49767">The Poisson distribution</st>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <st c="49792">For</st> <st c="49797">the binomial distribution, we were</st>
    <st c="49832">interested in the distribution of the total number of successes
    when performing</st> <st c="49912">N</st> <st c="49913">independent trials.</st>
    <st c="49934">Another situation where we may be interested in looking at the distribution
    of counts of something is when we want to count how many occurrences of something
    we get within, say, a given time interval or within a given area.</st> <st c="50158">For
    example, continuing our e-commerce example, we may be interested in the number
    of items sold within an hour.</st> <st c="50271">If we make the simplest possible
    assumption that the average rate of sale is constant, then the distribution of
    the number of actual items sold is a</st> **<st c="50420">Poisson</st>** <st c="50427">distribution.</st>
    <st c="50442">The Poisson distribution is a discrete distribution – it tells us
    how counts (that is, integer values) are distributed.</st> <st c="50562">The possible
    outcome,</st> <st c="50584">k</st><st c="50585">, can be</st> <st c="50594">0,1</st><st
    c="50597">,</st> <st c="50598">…</st> <st c="50599">,</st> <st c="50600">∞</st><st
    c="50601">. The probabilities of those outco</st><st c="50635">mes are given by
    the</st> <st c="50657">following formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50675">Prob</st><st c="50680">(</st><st c="50682">k</st><st c="50683">)</st>
    <st c="50684">=</st> <st c="50685">λ</st><st c="50686">k</st> <st c="50687">e</st><st
    c="50688">−</st><st c="50689">λ</st><st c="50690">_</st><st c="50691">k</st> <st
    c="50692">!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50693">Eq.</st> <st c="50697">41</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50699">Here,</st> <st c="50706">λ</st> <st c="50707">is the mean outcome
    value, so it is the constant rate that formed our starting assumption.</st> <st
    c="50799">There are two things to highlight about</st> <st c="50839">λ</st><st
    c="50840">. Firstly,</st> <st c="50851">λ</st> <st c="50852">can be a non-integer
    even though the Poisson distribution is a distribution of integer counts.</st>
    <st c="50948">For example, if</st> <st c="50964">λ</st> <st c="50965">=</st> <st
    c="50966">3.4</st> <st c="50969">in our e-commerce example, it means we sell on
    average 3.4 items per hour.</st> <st c="51045">The actual number of items sold
    in any hour period will be an integer; for example, 2, 3, or 4 items.</st> <st
    c="51147">The second thing to highlight about</st> <st c="51183">λ</st> <st c="51184">is
    that it is the only parameter that is in</st> *<st c="51229">Eq.</st> <st c="51233">41</st>*<st
    c="51235">. This implies that the variance of the Poisson distribution will be
    some function of</st> <st c="51321">λ</st><st c="51322">. Likewise, other characteristics
    such as the skewness and the kurtosis.</st> <st c="51395">The variance of the
    Poisson distribution is easily calculated and turns out to also be</st> <st c="51482">λ</st><st
    c="51483">. To recap, that means that if</st> <st c="51514">X</st> <st c="51515">~</st>
    <st c="51516">Poisson</st><st c="51523">(</st><st c="51525">λ</st><st c="51526">)</st><st
    c="51527">, then the</st> <st c="51538">following applies:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51556">𝔼</st><st c="51559">(</st><st c="51561">X</st><st c="51562">)</st>
    <st c="51563">=</st> <st c="51564">λ</st> <st c="51565">and Var</st><st c="51573">(</st><st
    c="51575">X</st><st c="51576">)</st> <st c="51577">=</st> <st c="51578">λ</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51579">Eq.</st> <st c="51583">42</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51585">An immediate implication of this is that if the mean of a Poisson
    random variable is increased, then so does its variance and so does its standard
    deviation.</st> <st c="51744">As the bulk of a Poisson distribution</st> <st c="51781">is
    shifted to the right, then it also spreads out.</st> *<st c="51833">Figure 2</st>**<st
    c="51841">.8</st>* <st c="51843">shows two examples of a Poisson distribution.</st>
    <st c="51890">The left-hand plot is for</st> <st c="51916">λ</st> <st c="51917">=</st>
    <st c="51918">2.5</st><st c="51921">, while the right-hand plot is for</st> <st
    c="51956">λ</st> <st c="51957">=</st> <st c="51958">7.5</st><st c="51961">. We
    can see from the right-hand plot that more of the distribution</st> <st c="52028">is
    at higher values, but it</st> <st c="52057">is also more spread out compared to
    the</st> <st c="52097">left-hand plot:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Two examples of a Poisson distribution with different means](img/B19496_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="52213">Figure 2.8: Two examples of a Poisson distribution with different
    means</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52284">Continuous distributions</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="52309">Now, we’ll</st> <st c="52321">introduce some of the most important
    named</st> <st c="52364">continuous distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52389">The uniform distribution</st>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <st c="52414">The</st> <st c="52419">uniform distribution is a continuous distribution.</st>
    <st c="52470">It has a</st> <st c="52479">minimum possible outcome value (let’s
    call that</st> <st c="52527">a</st><st c="52528">) and a maximum possible outcome
    value (let’s call that</st> <st c="52584">b</st><st c="52585">).</st> <st c="52588">By
    definition, the probability of getting an outcome smaller than</st> <st c="52654">a</st>
    <st c="52655">or an outcome larger than</st> <st c="52682">b</st> <st c="52683">is
    zero.</st> <st c="52693">As the name suggests, in between</st> <st c="52726">a</st>
    <st c="52727">and</st> <st c="52732">b</st> <st c="52733">the probability of getting
    any outcome is the same; that is, the probability is uniform between</st> <st
    c="52830">a</st> <st c="52831">and</st> <st c="52836">b</st><st c="52837">. Consequently,
    the uniform distribution is a two-parameter distribution, meaning once we know
    the</st> <st c="52935">two parameters</st> <st c="52951">a</st> <st c="52952">and</st>
    <st c="52957">b</st><st c="52958">, we know everything there is to know about
    it.</st> <st c="53006">If a random variable</st> <st c="53027">X</st> <st c="53028">is
    distributed according to a uniform distribution between</st> <st c="53088">a</st>
    <st c="53089">and</st> <st c="53094">b</st><st c="53095">, we write</st> <st c="53106">X</st><st
    c="53107">~</st> <st c="53108">Uniform</st><st c="53115">(</st><st c="53117">a</st><st
    c="53118">,</st> <st c="53119">b</st><st c="53120">)</st> <st c="53121">or using
    a shorthand notation,</st> <st c="53153">X</st> <st c="53154">~</st> <st c="53155">U</st><st
    c="53156">(</st><st c="53157">a</st><st c="53158">,</st> <st c="53159">b</st><st
    c="53160">)</st><st c="53161">. The PDF,</st> <st c="53172">f</st><st c="53173">(</st><st
    c="53174">x</st><st c="53175">)</st><st c="53176">, is given by</st> <st c="53190">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53204">f</st><st c="53206">(</st><st c="53207">x</st><st c="53208">)</st>
    <st c="53209">=</st> <st c="53210">1</st><st c="53211">_</st><st c="53212">b</st>
    <st c="53213">−</st> <st c="53214">a</st> <st c="53215">for</st> <st c="53220">a</st>
    <st c="53221">≤</st> <st c="53222">x</st> <st c="53223">≤</st> <st c="53224">b</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53225">Eq.</st> <st c="53229">43</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53231">This</st> <st c="53236">follows simply from the need to have the
    probabilities of all possible outcomes add up to 1\.</st> <st c="53330">The mean
    and variance are also easily calculated using high</st> <st c="53390">school math:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53402">𝔼</st><st c="53405">(</st><st c="53407">X</st><st c="53408">)</st>
    <st c="53409">=</st> <st c="53410">1</st><st c="53411">_</st><st c="53412">2</st>
    <st c="53413">(</st><st c="53414">a</st> <st c="53415">+</st> <st c="53416">b</st><st
    c="53417">)</st> <st c="53418">Var</st><st c="53421">(</st><st c="53423">X</st><st
    c="53424">)</st> <st c="53425">=</st> <st c="53426">1</st><st c="53427">_</st><st
    c="53428">12</st> <st c="53430">(</st><st c="53432">b</st> <st c="53433">−</st>
    <st c="53434">a</st><st c="53435">)</st><st c="53436">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53437">Eq.</st> <st c="53441">44</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53443">The uniform distribution may look like it is a bit boring, but
    in practical terms, it forms the building block when we want to generate example
    random data from other distributions, so it is a distribution worth</st> <st c="53656">knowing
    about.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53670">The Gaussian distribution</st>
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: <st c="53696">The</st> <st c="53700">Gaussian distribution is a two-parameter</st>
    <st c="53742">continuous distribution, so its density function is characterized
    by two parameters – in this case, its mean</st> <st c="53851">μ</st> <st c="53852">and
    its variance</st> <st c="53870">σ</st><st c="53871">2</st><st c="53872">. The
    formula for its PDF is</st> <st c="53901">given next:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53912">f</st><st c="53914">(</st><st c="53915">x</st><st c="53916">)</st>
    <st c="53917">=</st> <st c="53918">1</st><st c="53919">_</st><st c="53920">√</st><st
    c="53921">_</st><st c="53922">2</st><st c="53923">π</st> <st c="53924">σ</st><st
    c="53925">2</st> <st c="53926">exp</st><st c="53929">(</st><st c="53931">−</st>
    <st c="53932">1</st><st c="53933">_</st><st c="53934">2</st><st c="53935">(</st><st
    c="53936">x</st><st c="53937">−</st> <st c="53938">μ</st><st c="53939">_</st><st
    c="53940">σ</st><st c="53941">)</st><st c="53942">2</st><st c="53943">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53944">Eq.</st> <st c="53948">45</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53950">A plot of the</st> <st c="53964">density function for</st> <st
    c="53986">μ</st> <st c="53987">=</st> <st c="53988">0</st><st c="53989">,</st>
    <st c="53990">σ</st> <st c="53991">=</st> <st c="53992">1</st> <st c="53993">is
    shown in</st> *<st c="54006">Figure 2</st>**<st c="54014">.9</st>*<st c="54016">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: The probability density of the standard normal distribution](img/B19496_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="54068">Figure 2.9: The probability density of the standard normal distribution</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54139">As the</st> <st c="54147">shape of the density function</st> <st
    c="54176">resembles a bell, the Gaussian distribution is sometimes called the
    bell-curve distribution.</st> <st c="54270">However, it is more commonly referred
    to as the normal distribution, so much so that if</st> <st c="54358">X</st> <st
    c="54359">is a Gaussian random variable, we write</st> <st c="54400">X</st> <st
    c="54401">~</st> <st c="54402">Normal</st><st c="54408">(</st><st c="54410">μ</st><st
    c="54411">,</st> <st c="54412">σ</st><st c="54413">2</st><st c="54414">)</st><st
    c="54415">. This notation also highlights that the distribution depends on the
    two parameters</st> <st c="54499">μ</st> <st c="54500">and</st> <st c="54505">σ</st><st
    c="54506">2</st><st c="54507">. A warning – sometimes you will also see some authors
    write</st> <st c="54568">X</st> <st c="54569">~</st> <st c="54570">Normal</st><st
    c="54576">(</st><st c="54578">μ</st><st c="54579">,</st> <st c="54580">σ</st><st
    c="54581">)</st><st c="54582">, meaning that the value they are giving as the
    second argument is the standard deviation, not the variance.</st> <st c="54691">However,
    whether you are given</st> <st c="54722">σ</st> <st c="54723">or</st> <st c="54727">σ</st><st
    c="54728">2</st><st c="54729">, the density is always given by the formula in</st>
    *<st c="54777">Eq.</st> <st c="54781">45</st>*<st c="54783">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54784">Why are we interested in the Gaussian distribution?</st> <st c="54837">The
    Gaussian distribution is probably the most common distribution you will encounter
    as a data scientist.</st> <st c="54944">Many natural processes produce data that
    follow a Gaussian distribution, and so many datasets you will analyze will be
    best modeled as Gaussian random variables.</st> <st c="55106">There are some very
    natural reasons for this, which we will touch upon at the end of</st> <st c="55191">this
    chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55204">From the rules applying to linear transformations of random variables,
    we know that if we have Gaussian random variable</st> <st c="55325">X</st> <st
    c="55326">~</st> <st c="55327">Normal</st><st c="55333">(</st><st c="55335">μ</st><st
    c="55336">,</st> <st c="55337">σ</st><st c="55338">2</st><st c="55339">)</st>
    <st c="55340">and we construct a new random variable</st> <st c="55380">Y</st>
    <st c="55381">=</st> <st c="55382">X</st><st c="55383">−</st> <st c="55384">μ</st><st
    c="55385">_</st><st c="55386">σ</st><st c="55387">, then</st> <st c="55394">Y</st><st
    c="55395">~</st><st c="55396">Normal</st><st c="55402">(</st><st c="55404">0</st><st
    c="55405">,</st> <st c="55406">1</st><st c="55407">)</st><st c="55408">. What
    this means is that we can understand the properties of any Gaussian random variable
    by understanding the behavior of the distribution</st> <st c="55549">Normal</st><st
    c="55555">(</st><st c="55557">0</st><st c="55558">,</st> <st c="55559">1</st><st
    c="55560">)</st><st c="55561">. Given the central importance of the Gaussian distribution,
    this makes the distribution</st> <st c="55650">Normal</st><st c="55656">(</st><st
    c="55658">0</st><st c="55659">,</st> <st c="55660">1</st><st c="55661">)</st>
    <st c="55662">a key distribution in its own right, and it has its own special
    name – it is</st> <st c="55740">called the</st> **<st c="55751">standard</st>**
    <st c="55759">normal distribution, reflecting the fact it has standardized values
    for the mean and variance.</st> <st c="55855">The values</st> <st c="55866">μ</st>
    <st c="55867">=</st> <st c="55868">0</st> <st c="55869">and</st> <st c="55874">σ</st><st
    c="55875">2</st> <st c="55876">=</st> <st c="55877">1</st> <st c="55878">also
    have their own special names.</st> <st c="55914">We refer to these values</st>
    <st c="55939">as</st> **<st c="55942">zero mean and unit variance</st>**<st c="55969">,
    with unit meaning a</st> <st c="55990">value of 1 here.</st> <st c="56008">So,
    the standard normal distribution is a normal distribution with zero mean and</st>
    <st c="56089">unit variance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56103">Phew!</st> <st c="56110">That</st> <st c="56115">was a long section.</st>
    <st c="56135">But it will be worth it.</st> <st c="56160">Data underpins everything
    in data science, and all data contains a random component, so learning the basics
    about how we use math to describe and handle randomness will pay big dividends.</st>
    <st c="56348">For now, though, let’s do a short recap of what we learned in</st>
    <st c="56410">this section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56423">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="56439">In this section, we have learned</st> <st c="56473">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56487">Random variables are the natural math concept to describe randomness.</st>
    <st c="56558">Probability distributions give the probabilities of possible outcomes
    of a discrete random variable, while PDFs perform a similar role for continuous</st>
    <st c="56708">random variables.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56725">Probability distributions and random variables can be characterized
    by their mean and variance, and other characteristics such as their skewness</st>
    <st c="56871">and kurtosis.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56884">How to transform and combine random variables and how to calculate
    the mean and variance of</st> <st c="56977">transformed quantities.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57000">The details of the most important discrete and continuous probability
    distributions we will encounter in</st> <st c="57106">data science.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57119">Now we have learned about the basics of random variables and probability
    distributions, we are going to learn in the next section how datasets are generated
    from</st> <st c="57282">probability distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57308">Sampling from distributions</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="57336">So far, we’ve learned a lot about random variables, probability
    distributions, and how to calculate some of the key characteristics of a distribution
    such as its mean and variance, and we’ve learned about some commonly occurring
    distributions.</st> <st c="57581">But so far, it doesn’t feel like we’ve learned
    much about data.</st> <st c="57645">We’ll now</st> <st c="57655">change that.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57667">How datasets relate to random variables and probability distributions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57737">We</st> <st c="57740">said at the beginning of this chapter that
    all data is random.</st> <st c="57804">This means when data is captured or generated,
    we are drawing or</st> **<st c="57869">sampling</st>** <st c="57877">values from
    some underlying probabil</st><st c="57914">ity distribution.</st> <st c="57933">This
    is illustrated schematically in</st> *<st c="57970">Figure 2</st>**<st c="57978">.10</st>*<st
    c="57981">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: Diagram illustrating how real data is generated as samples from
    a population](img/B19496_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="58309">Figure 2.10: Diagram illustrating how real data is generated
    as samples from a population</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58398">A sample is finite.</st> <st c="58419">It represents a snapshot
    or subset of the entirety of possible outcomes; for example, a subset of all users
    who might visit a website.</st> <st c="58554">But from a business perspective,
    it is the behavior of the collection of all users I want to understand.</st> <st
    c="58659">When we analyze a dataset, what we really want to understand are the
    characteristics of the underlying distribution that we think the data has come
    from.</st> <st c="58813">We call this underlying</st> <st c="58837">distribution
    the</st> **<st c="58854">population</st>** <st c="58864">distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58878">Unfortunately, the underlying population distribution from which
    a dataset is generated is usually hidden from us.</st> <st c="58994">Instead,
    we use the sampled data as a proxy for the underlying population and use the summary
    characteristics of the sample as proxies for their population counterparts.</st>
    <st c="59164">Due to randomness, no sample is a perfect copy of the underlying
    population distribution from which it was taken.</st> <st c="59278">Consequently,
    different samples (datasets) taken from the same population can have different
    characteristics.</st> <st c="59388">This is</st> <st c="59396">called</st> **<st
    c="59403">sampling variation</st>** <st c="59421">and can lead us to different
    conclusions if we don’t know how to correctly account for this randomness inherent
    in any sample.</st> <st c="59549">Alternatively, an individual dataset can give
    misleading conclusions about the underlying population if we analyze it naively
    and aren’t aware of the random nature of the data.</st> <st c="59726">This is
    why we have spent a lot of effort on understanding randomness, random variables,
    and</st> <st c="59819">distributions and why we consider this to be the most important
    chapter in</st> <st c="59894">the book.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59903">Take my e-commerce example.</st> <st c="59932">I want to understand
    what the</st> **<st c="59962">click-through-rate</st>** <st c="59980">(</st>**<st
    c="59982">CTR</st>**<st c="59985">) of a</st> <st c="59993">particular advertisement
    design is.</st> <st c="60029">I want to understand what proportion of the 10 million
    visitors I get to the website each year will click the ad.</st> <st c="60143">So,
    I track 20 visitors to the website, see whether they click or not, and find 12
    of them did.</st> <st c="60239">That CTR of 12/20 = 60% may be good enough for
    my business model to succeed, so I go away with the intention of using that ad
    design for all website visitors.</st> <st c="60398">However, the CTR of all 10
    million visitors can be very, very different from the CTR of 60% of those 20 people
    I tracked.</st> <st c="60520">Those 20 people are human beings, and humans have
    their own quirks and idiosyncrasies and so have a degree of randomness to their
    click behavior.</st> <st c="60666">Those 20 people we tracked are a sample of
    the 10 million – a very small sample.</st> <st c="60747">And we have seen that
    when we take a sample – when we draw each of their click behaviors from a Bernoulli
    click/no-click distribution – then we could get anywhere between 0 and 20 clicks.</st>
    <st c="60935">The true CTR of the 10 million annual visitors – the number I’m
    basing my business decisions on – might be 40%, but I could still see a CTR in
    that sample of 20 people that is anywhere between 0% and 100%.</st> <st c="61141">Suddenly,
    I don’t feel so confident that the sample of 20 people is helping me make good
    business decisions.</st> <st c="61250">What should</st> <st c="61262">we do?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61268">This is where math comes to our rescue.</st> <st c="61309">Our
    inferences about the CTR of all 10 million website visitors have some randomness
    or uncertainty within them.</st> <st c="61422">Using our knowledge of distributions,
    we can quantify that uncertainty and so control the level of risk associated with
    our inferences and decisions.</st> <st c="61572">We are taking the situation sho</st><st
    c="61603">wn in</st> *<st c="61610">Figure 2</st>**<st c="61618">.10</st>* <st
    c="61621">and closing the loop, as illustrated in</st> *<st c="61662">Figure 2</st>**<st
    c="61670">.11</st>*<st c="61673">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: Diagram illustrating how we close the loop and use knowledge
    of probability distributions to go from samples back to making inferences about
    the underlying population](img/B19496_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="62275">Figure 2.11: Diagram illustrating how we close the loop and use
    knowledge of probability distributions to go from samples back to making inferences
    about the underlying population</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62454">How big is the population from which a dataset is sampled?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62513">In theoretical</st> <st c="62529">situations, we can usually consider
    the population to be of infinite size, meaning that I can obtain finite-sized
    sample datasets from it as big as I want.</st> <st c="62685">In real-world situations,
    the population may also be of finite size due to genuine constraints.</st> <st
    c="62781">For example, in our CTR example, we have 10 million visitors a year
    to the website, so the largest sample I could study in a year would consist of
    10 million people.</st> <st c="62947">Often, even if the true population is of
    finite size, it is so big compared to the typical dataset (sample) we will study
    that we can consider the population to be effectively infinite and ignore the
    finite size implications of the population.</st> <st c="63191">However, I say
    this to point out that sometimes, we can’t ignore the finite size of the population
    from which we are drawing our samples, and so it is always worth thinking about
    this aspect; that is, how big is our sample and how big do we think the population
    is from which the sample</st> <st c="63478">was obtained.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63491">Now we have seen that a dataset can be considered as a sample
    from an underlying probability distribution, we will learn how to generate our
    own samples using snippets of Python code.</st> <st c="63676">We will also learn
    why it is useful to be able to generate our</st> <st c="63739">own samples.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63751">How to sample</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="63765">Why is being able to generate a sample useful?</st> <st c="63813">There
    are two</st> <st c="63827">main reasons:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63840">We can use sampling to create simulated data.</st> <st c="63887">Simulated
    data can be used to test data science algorithms and is particularly useful where
    we don’t have any existing real ground-truth data.</st> <st c="64030">We can use
    the simulation process to generate as much data as we want and where we specify
    what the true underlying parameter values</st> <st c="64163">should be.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="64173">Sometimes it is easier to use sampling to approximate a statistical
    calculation, such as the calculation of an expectation value, rather than try
    to calculate the expectation value exactly using rigorous mathematics.</st> <st
    c="64391">Sometimes, we may be dealing with complex data generation processes
    for which exact mathematical evaluation of a mean is just not possible, but it
    is possible to sample from the data generation process.</st> <st c="64594">In
    these situations, we can generate large numbers of samples and use a simple numerical
    average to give us a very accurate approximation of the</st> <st c="64739">population
    mean.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="64755">Let’s say we</st> <st c="64768">have a binomial distribution</st>
    <st c="64798">N</st> <st c="64799">=</st> <st c="64800">20</st><st c="64802">,</st>
    <st c="64804">p</st> <st c="64805">=</st> <st c="64806">0.6</st><st c="64809">.
    Now, given that information, we can use the formula in</st> *<st c="64866">Eq.</st>
    <st c="64870">38</st>* <st c="64872">to calculate the probability of, say, getting
    a count outcome of</st> <st c="64938">n</st> <st c="64939">=</st> <st c="64940">9</st>
    <st c="64941">or fewer successes.</st> <st c="64962">We find this probability
    is 0.1275\.</st> <st c="64998">This means that if I were to repeatedly sample
    from a Binomial(20, 0.6) distribution, I would expect about 12.75% of the time
    I would get a number 9 or less.</st> <st c="65156">In fact, I can do this calculation
    for all possible values of</st> <st c="65218">n</st><st c="65219">. This calculation
    is easy to do – it just involves cumulatively adding up the probabilities from
    the smallest value of the outcome to the largest value of the outcome.</st> <st
    c="65388">This is illustrated in</st> *<st c="65411">Figure 2</st>**<st c="65419">.12</st>*<st
    c="65422">. The resulting curve is called the</st> **<st c="65458">cumulative
    probability distribution</st>** <st c="65493">(</st>**<st c="65495">CDF</st>**<st
    c="65498">) for</st> <st c="65505">obvious reasons.</st> <st c="65522">The CDF
    for a</st><st c="65535">ny distribution always monotonically increases from 0
    to a maximum</st> <st c="65603">of 1:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: The CDF of the Binomial(20,0.6) distribution](img/B19496_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="65700">Figure 2.12: The CDF of the Binomial(20,0.6) distribution</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="65757">Now, let’s</st> <st c="65769">say I draw a number from the uniform
    distribution</st> <st c="65819">U</st><st c="65820">(</st><st c="65821">0,1</st><st
    c="65824">)</st><st c="65826">. The number I get from that uniform distribution
    effectively splits the interval [0,1] into two portions.</st> <st c="65933">Or
    equivalently, it splits the range 0%-100% into two regions.</st> <st c="65996">Let’s
    say the number I got was 0.41, so I’ve split the 0%-100% range into two portions:
    one containing a total probability of 0.41 (or 41%) and the other containing a
    total probability of 0.59 (or 59%).</st> <st c="66199">What I can now do is ask
    what value of</st> <st c="66238">n</st> <st c="66239">in my binomial distribution
    example would give me that same split.</st> <st c="66307">Given the cumulative
    distribution shown in</st> *<st c="66350">Figure 2</st>**<st c="66358">.12</st>*<st
    c="66361">, that is easy to do, and we find we get the same 41%-59% split at</st>
    <st c="66428">n</st> <st c="66429">=</st> <st c="66430">11</st><st c="66432">.
    This is illustrated by the blue horizontal dashed line in the middle of</st> *<st
    c="66506">Figure 2</st>**<st c="66514">.12</st>*<st c="66517">. So, what we now
    have is a method for going from generating uniform random numbers to giving numbers
    drawn from the actual distribution I’m interested in – the Binomial(20,0.6) distribution
    in this example.</st> <st c="66725">Most computer programming languages will provide
    you with an easy way to generate random numbers from</st> <st c="66827">U</st><st
    c="66828">(</st><st c="66829">0,1</st><st c="66832">)</st><st c="66834">. The
    Python code snippet shown next shows how to use the</st> `<st c="66892">numpy</st>`
    <st c="66897">uniform random number generator to do this and shows how to generate
    1,000 Binomial(20, 0.6)</st> <st c="66991">random numbers.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67006">Generating your own random numbers code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="67054">A fuller</st> <st c="67064">version of the following code is given
    in the</st> `<st c="67110">Code_Examples_Chap2.ipynb</st>` <st c="67135">Jupyter
    notebook in the</st> <st c="67160">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="67921">This method</st> <st c="67934">can be applied to any discrete
    distribution.</st> <st c="67979">A modified version can be used to sample from
    continuous distributions, but it can be a</st> <st c="68067">bit harder.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68078">Okay – now I’m going to let you into a little secret.</st> <st
    c="68133">You don’t have to do any of this.</st> <st c="68167">The specialist
    numerical modules or packages of most modern general-purpose programming languages
    come with in-built methods for sampling from the most common distributions you
    are likely to encounter (and some more obscure ones as well).</st> <st c="68406">We’ll
    now take a brief tour of some of those methods with some bits of example</st>
    <st c="68485">Python code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68497">Sampling from numpy distributions code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="68544">First, here’s</st> <st c="68559">how to sample from a binomial
    distribution</st> <st c="68602">using</st> `<st c="68608">numpy</st>`<st c="68613">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="68784">Second, here’s how to sample from a Poisson distribution</st>
    <st c="68842">using</st> `<st c="68848">numpy</st>`<st c="68853">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="69111">Finally, here’s</st> <st c="69128">how to sample from a Gaussian
    distribution</st> <st c="69171">using</st> `<st c="69177">numpy</st>`<st c="69182">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="69740">All the</st> <st c="69749">preceding code snippets can be found
    in full in the</st> `<st c="69801">Code_Examples_Chap2.ipynb</st>` <st c="69826">Jupyter
    notebook in the</st> <st c="69851">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="69869">That code example is a good place to end this shorter section,
    so let’s summarize what we have learned about generating our own</st> <st c="69998">random
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70010">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="70026">In this section, we have learned</st> <st c="70060">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70074">How the datasets we work with as data scientists can be considered
    as samples from</st> <st c="70158">a distribution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="70172">How to generate our own samples from any</st> <st c="70214">probability
    distribution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="70238">Why and when generating our own samples from a probability distribution</st>
    <st c="70311">is useful</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="70320">Having learned how datasets can be viewed as samples from a distribution,
    we will learn in the next section how to characterize and summarize a sample and
    how those summaries of a sample can be used to make accurate inferences about
    the underlying population distribution from which we think the data</st> <st c="70622">was
    drawn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70632">Understanding statistical estimators</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="70669">When we</st> <st c="70678">were looking at various example probability
    distributions, we learned how to calculate their mean and variance.</st> <st c="70790">Now,
    you may ask: Is it possible to calculate the mean and variance of a sample (of
    a dataset)?</st> <st c="70886">The answer is yes.</st> <st c="70905">You have
    probably done this before in high school or college.</st> <st c="70967">So, you
    may be wondering how the mean and variance of a dataset are connected to the mean
    and variance of a</st> <st c="71075">population distribution.</st> <st c="71100">What
    we are going to do now is explain</st> <st c="71139">the following:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71153">How to calculate the mean and variance of</st> <st c="71196">a
    sample</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="71204">How they differ from the mean and variance of the population distribution
    from which the sample</st> <st c="71301">was generated</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="71314">How they are connected to the mean and variance of the population
    distribution from which the sample</st> <st c="71416">was generated</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="71429">How to use our understanding of the population distribution to
    make quantified inferences about it from</st> <st c="71534">the sample</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="71544">Let’s start with the first of those.</st> <st c="71582">Given
    a set of</st> <st c="71597">n</st> <st c="71598">numbers (a s</st><st c="71611">ample)</st>
    <st c="71619">x</st><st c="71620">1</st><st c="71621">,</st> <st c="71622">x</st><st
    c="71623">2</st><st c="71624">,</st> <st c="71625">⋯</st> <st c="71626">,</st>
    <st c="71627">x</st><st c="71628">n</st><st c="71629">, the</st> **<st c="71635">sample
    mean</st>** <st c="71646">is</st> <st c="71650">calculated</st> <st c="71661">as
    follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71672">Sample mean =</st> <st c="71687">m</st> <st c="71688">=</st> <st
    c="71689">1</st><st c="71690">_</st><st c="71691">n</st> <st c="71692">∑</st><st
    c="71693">i</st><st c="71694">=</st><st c="71695">1</st><st c="71696">n</st><st
    c="71697">x</st><st c="71698">i</st> <st c="71699">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71700">Eq.</st> <st c="71705">46</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71707">This is the formula you will have learned in high school.</st>
    <st c="71766">For example, if my sample consists of the five numbers 3.7, 1.2,
    2.3, 4.1, 2.7, then the sample mean is</st> <st c="71870">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71884">1</st><st c="71886">_</st><st c="71887">5</st> <st c="71888">×</st>
    <st c="71889">(</st><st c="71890">3.7</st> <st c="71893">+</st> <st c="71895">1.2</st>
    <st c="71898">+</st> <st c="71900">2.3</st> <st c="71903">+</st> <st c="71905">4.1</st>
    <st c="71908">+</st> <st c="71910">2.7</st><st c="71913">)</st> <st c="71915">=</st>
    <st c="71916">2.8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71919">Eq.</st> <st c="71924">47</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71926">You will see that we have used a different symbol for the sample
    mean.</st> <st c="71998">We have used</st> <st c="72011">m</st> <st c="72012">and
    not</st> <st c="72021">μ</st><st c="72022">. This is to distinguish it from a
    population mean.</st> <st c="72074">In fact, we have explicitly called it “the
    sample mean.” We will show how</st> <st c="72148">m</st> <st c="72149">is related
    to</st> <st c="72164">μ</st> <st c="72165">later.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72172">Similarly, we can calculate</st> <st c="72201">the</st> **<st
    c="72205">sample variance</st>** <st c="72220">as the average squared deviation</st>
    <st c="72254">of the data from its mean.</st> <st c="72281">Here, I’m going to
    define it</st> <st c="72310">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72321">Sample Variance =</st> <st c="72340">s</st><st c="72341">2</st>
    <st c="72342">=</st> <st c="72343">1</st><st c="72344">_</st><st c="72345">n</st>
    <st c="72346">−</st> <st c="72347">1</st> <st c="72348">∑</st><st c="72349">i</st><st
    c="72350">=</st><st c="72351">1</st><st c="72352">n</st><st c="72353">(</st><st
    c="72354">x</st><st c="72355">i</st> <st c="72356">−</st> <st c="72357">m</st><st
    c="72358">)</st><st c="72359">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72360">Eq.</st> <st c="72364">48</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72366">For the example</st> <st c="72383">five numbers shown previously,
    the sample variance is</st> <st c="72437">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72451">1</st><st c="72453">_</st><st c="72454">4</st> <st c="72455">×</st>
    <st c="72456">[</st><st c="72457">(</st><st c="72458">3.7</st> <st c="72461">−</st>
    <st c="72463">2.8</st><st c="72466">)</st><st c="72468">2</st><st c="72469">+</st>
    <st c="72470">(</st><st c="72471">1.2</st> <st c="72474">−</st> <st c="72476">2.8</st><st
    c="72479">)</st><st c="72481">2</st><st c="72482">+</st> <st c="72483">(</st><st
    c="72484">2.3</st> <st c="72487">−</st> <st c="72489">2.8</st><st c="72492">)</st><st
    c="72494">2</st><st c="72495">+</st> <st c="72496">(</st><st c="72497">4.1</st>
    <st c="72500">−</st> <st c="72502">2.8</st><st c="72505">)</st><st c="72507">2</st><st
    c="72508">+</st> <st c="72509">(</st><st c="72510">2.7</st> <st c="72513">−</st>
    <st c="72515">2.8</st><st c="72518">)</st><st c="72520">2</st><st c="72521">]</st>
    <st c="72522">=</st> <st c="72523">1.33</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72527">Eq.</st> <st c="72532">49</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72534">Again, we</st> <st c="72545">have used a different symbol,</st>
    <st c="72575">s</st><st c="72576">2</st><st c="72577">, for the sample variance,
    and not</st> <st c="72612">σ</st><st c="72613">2</st> <st c="72614">that we use
    for the population variance.</st> <st c="72656">As you may have guessed, the sample
    standard deviation is simply the square root of the sample variance, so the sample
    standard deviation</st> <st c="72794">is</st> <st c="72797">s</st><st c="72798">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72799">Now, you will probably be wondering why we had</st> <st c="72847">n</st>
    <st c="72848">−</st> <st c="72849">1</st> <st c="72850">in the denominator on
    the right-hand side of</st> *<st c="72896">Eq.</st> <st c="72900">48</st>*<st
    c="72902">.</st> <st c="72904">Why didn’t we calculate the sample variance using
    a formula,</st> <st c="72965">like so?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72973">1</st><st c="72975">_</st><st c="72976">n</st> <st c="72977">∑</st><st
    c="72978">i</st><st c="72979">=</st><st c="72980">1</st><st c="72981">n</st><st
    c="72982">(</st><st c="72983">x</st><st c="72984">i</st> <st c="72985">−</st>
    <st c="72986">m</st><st c="72987">)</st><st c="72988">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72989">Eq.</st> <st c="72993">50</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72995">The short answer is that using a denominator of</st> <st c="73044">n</st>
    <st c="73045">−</st> <st c="73046">1</st> <st c="73047">means the resulting value
    of</st> <st c="73077">s</st><st c="73078">2</st> <st c="73079">gives us a more
    accurate estimate of the true population variance</st> <st c="73146">σ</st><st
    c="73147">2</st> <st c="73148">than if we had used a denominator of</st> <st c="73186">n</st><st
    c="73187">. Remember – it is the underlying population distribution and its properties
    that we are ultimately</st> <st c="73287">interested in.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="73301">Consistency, bias, and efficiency</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="73335">Now, let’s give the long answer and explain in more detail.</st>
    <st c="73396">Those five numbers in the preceding example came from a normal distribution
    with a mean</st> <st c="73484">μ</st> <st c="73485">=</st> <st c="73486">2.5</st>
    <st c="73489">and a variance</st> <st c="73505">σ</st><st c="73506">2</st> <st
    c="73507">=</st> <st c="73508">1.5</st><st c="73511">. I used the code example
    given in the</st> *<st c="73550">How to sample</st>* <st c="73563">subsection
    to generate them.</st> <st c="73593">I also rounded to 1 decimal place for convenience
    of presentation.</st> <st c="73660">You can see that the sample mean</st> <st
    c="73693">m</st> <st c="73694">and sample variance</st> <st c="73715">s</st><st
    c="73716">2</st> <st c="73717">are different from the mean</st> <st c="73746">μ</st>
    <st c="73747">and variance</st> <st c="73761">σ</st><st c="73762">2</st> <st c="73763">of
    the distribution from which the five numbers were sampled.</st> <st c="73826">Ideally,
    we want</st> <st c="73843">m</st> <st c="73844">and</st> <st c="73849">s</st><st
    c="73850">2</st> <st c="73851">to be good estimates of</st> <st c="73876">μ</st>
    <st c="73877">and</st> <st c="73882">σ</st><st c="73883">2</st><st c="73884">,
    and so be close in numerical value to</st> <st c="73924">μ</st> <st c="73925">and</st>
    <st c="73930">σ</st><st c="73931">2</st><st c="73932">. You would suspect that
    the reason</st> <st c="73968">m</st> <st c="73969">and</st> <st c="73974">s</st><st
    c="73975">2</st> <st c="73976">are different from</st> <st c="73996">μ</st> <st
    c="73997">and</st> <st c="74002">σ</st><st c="74003">2</st> <st c="74004">is the
    small sample</st> <st c="74025">size; that is, because we have used only five
    numbers in our sample.</st> <st c="74094">We would hope that</st> <st c="74113">m</st>
    <st c="74114">and</st> <st c="74119">s</st><st c="74120">2</st> <st c="74121">get
    closer to</st> <st c="74136">μ</st> <st c="74137">and</st> <st c="74142">σ</st><st
    c="74143">2</st> <st c="74144">as we increase the sample size</st> <st c="74176">n</st><st
    c="74177">.</st> *<st c="74179">Figure 2</st>**<st c="74187">.13</st>* <st c="74190">shows
    what happens to</st> <st c="74213">m</st> <st c="74214">and</st> <st c="74219">s</st><st
    c="74220">2</st> <st c="74221">as we increase the</st> <st c="74241">number of
    samples we draw from the distribution</st> <st c="74289">Normal</st><st c="74295">(</st><st
    c="74297">2.5</st><st c="74300">,</st> <st c="74301">1.5</st><st c="74304">)</st><st
    c="74306">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: Running sample mean and sample variance for samples drawn from
    Normal(2.5, 1.5)](img/B19496_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="74492">Figure 2.13: Running sample mean and sample variance for samples
    drawn from Normal(2.5, 1.5)</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="74584">Here, we have generated 5,000 random values from</st> <st c="74634">Normal</st><st
    c="74640">(</st><st c="74642">2.5</st><st c="74645">,</st> <st c="74646">1.5</st><st
    c="74649">)</st> <st c="74651">and then calculated the running sample mean; that
    is, the average of the first</st> <st c="74731">n</st> <st c="74732">numbers,
    for</st> <st c="74746">n</st> <st c="74747">=</st> <st c="74748">50</st><st c="74750">,</st>
    <st c="74751">…</st> <st c="74752">,</st> <st c="74753">5000</st><st c="74757">.
    In the right-hand plot, we have also plotted the running sample variance.</st>
    <st c="74833">In the left-hand and right-hand plots, the horizontal gray lines
    show the values of the population mean and population variance, respectively.</st>
    <st c="74976">We can see that visually, the sample mean and sample variance appear
    to converge to their population values as the sample size increases.</st> <st
    c="75114">That is, we have</st> <st c="75131">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75145">m</st> <st c="75147">→</st> <st c="75148">μ</st> <st c="75149">s</st><st
    c="75150">2</st> <st c="75151">→</st> <st c="75152">σ</st><st c="75153">2</st>
    <st c="75154">as</st> <st c="75156">n</st> <st c="75158">→</st> <st c="75159">∞</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75160">Eq.</st> <st c="75164">51</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75166">Because of this, we say that</st> <st c="75196">m</st> <st c="75197">and</st>
    <st c="75202">s</st><st c="75203">2</st> <st c="75204">are</st> **<st c="75209">consistent
    estimators</st>**<st c="75230">. They get better at estimating the</st> <st c="75266">things
    we want them to estimate as we use more and more data, until ultimately with an
    infinite amount of data they are consistent with (give the same value) as the
    things we want to calculate (</st><st c="75460">μ</st> <st c="75462">and</st>
    <st c="75467">σ</st><st c="75468">2</st><st c="75469">)</st><st c="75470">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75471">Excellent!</st> <st c="75483">Well</st> <st c="75487">not quite.</st>
    <st c="75499">Usually, we don’t have an infinite amount of data in our real-world
    data science problems.</st> <st c="75590">So, while an estimator that is consistent
    is almost a minimum requirement, it still doesn’t guarantee that when we have
    a finite-sized dataset, our estimator is based upon a</st> <st c="75764">good
    formula.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75777">How could we</st> <st c="75791">even tell if a formula gives a
    good estimator at finite</st> <st c="75847">n</st><st c="75848">? Remember what
    we said at the very start of this chapter that all data has a random component,
    and so all quantities derived from data also have a random component?</st> <st
    c="76014">Well, that means that the sample mean and sample variance have a random
    component and so are random variables.</st> <st c="76125">As</st> <st c="76128">m</st>
    <st c="76129">is a random variable, we accept that any individual instance of</st>
    <st c="76194">m</st><st c="76195">, calculated from an individual dataset of size</st>
    <st c="76243">n</st><st c="76244">, will differ from</st> <st c="76263">μ</st><st
    c="76264">, but we would want the average value of</st> <st c="76305">m</st> <st
    c="76306">to be the same as</st> <st c="76325">μ</st> <st c="76326">if we were
    using a good estimator.</st> <st c="76362">That means, that if we repeat lots
    of times our little experiment of drawing five numbers from</st> <st c="76457">Normal</st><st
    c="76463">(</st><st c="76465">2.5</st><st c="76468">,</st> <st c="76469">1.5</st><st
    c="76472">)</st> <st c="76474">and calculate a value of</st> <st c="76500">m</st>
    <st c="76501">for each of those experiments, we want the average value of</st>
    <st c="76562">m</st> <st c="76563">to be equal to</st> <st c="76579">μ</st><st
    c="76580">.</st> *<st c="76582">Table 2.2</st>* <st c="76591">shows 10 such experiments,
    with the values of</st> <st c="76638">m</st> <st c="76639">and</st> <st c="76644">s</st><st
    c="76645">2</st> <st c="76646">given for each of the 10 experiments.</st> <st
    c="76685">The bottom row of the table s</st><st c="76714">hows the average values
    of</st> <st c="76742">m</st> <st c="76743">and</st> <st c="76748">s</st><st c="76749">2</st>
    <st c="76750">across the</st> <st c="76762">10 experiments:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 2.2: Sample mean and sample variance values in 10 experiments drawing
    samples of 5 numbers from Normal(2.5, 1.5)](img/B19496_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="76964">Table 2.2: Sample mean and sample variance values in 10 experiments
    drawing samples of 5 numbers from Normal(2.5, 1.5)</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77082">You can</st> <st c="77091">see from</st> *<st c="77100">Table
    2.2</st>* <st c="77109">that the average across the 10 experiments of the sample
    mean is reasonably close to the true population mean of</st> <st c="77223">μ</st>
    <st c="77224">=</st> <st c="77225">2.5</st><st c="77228">. In fact, if we took
    the average over an infinite number of experiments, we</st> <st c="77304">would
    get an average value of 2.5 exactly.</st> <st c="77348">So,</st> **<st c="77352">on
    average</st>**<st c="77362">, the sample mean gives the same value as the population
    mean.</st> <st c="77425">We will prove</st> <st c="77439">that now.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77448">Since the sample mean is a random variable, we can just calculate
    its expectation value.</st> <st c="77538">The sample mean is defined</st> <st
    c="77565">as fo</st><st c="77570">llows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77577">m</st> <st c="77579">=</st> <st c="77580">1</st><st c="77581">_</st><st
    c="77582">n</st> <st c="77583">∑</st><st c="77584">i</st><st c="77585">=</st><st
    c="77586">1</st><st c="77587">n</st><st c="77588">x</st><st c="77589">i</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77590">Eq.</st> <st c="77594">52</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77596">So, its expectation value is</st> <st c="77626">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77640">𝔼</st><st c="77643">(</st><st c="77645">m</st><st c="77646">)</st>
    <st c="77647">=</st> <st c="77648">𝔼</st><st c="77650">(</st><st c="77652">1</st><st
    c="77653">_</st><st c="77654">n</st> <st c="77655">∑</st><st c="77656">i</st><st
    c="77657">=</st><st c="77658">1</st><st c="77659">n</st><st c="77660">x</st><st
    c="77661">i</st><st c="77662">)</st> <st c="77663">=</st> <st c="77664">1</st><st
    c="77665">_</st><st c="77666">n</st> <st c="77667">∑</st><st c="77668">i</st><st
    c="77669">=</st><st c="77670">1</st><st c="77671">n</st><st c="77672">𝔼</st><st
    c="77674">(</st><st c="77676">x</st><st c="77677">i</st><st c="77678">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77679">Eq.</st> <st c="77683">53</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="77685">The last part on the right-hand side of</st> *<st c="77726">Eq.</st>
    <st c="77730">53</st>* <st c="77732">follows on from the rules of expectations
    of linear transformations of random variables.</st> <st c="77822">Now, if we have
    i.i.d.</st> <st c="77845">data, then since each random variable</st> <st c="77883">x</st><st
    c="77884">i</st> <st c="77885">is drawn from the same population distribution
    we have</st> <st c="77941">𝔼</st><st c="77943">(</st><st c="77945">x</st><st c="77946">i</st><st
    c="77947">)</st> <st c="77948">=</st> <st c="77949">μ</st> <st c="77950">for all</st>
    <st c="77959">i</st><st c="77960">. Plugging that result into</st> *<st c="77988">Eq.</st>
    <st c="77992">53</st>*<st c="77994">, we then get</st> <st c="78008">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78022">𝔼</st><st c="78025">(</st><st c="78027">m</st><st c="78028">)</st>
    <st c="78029">=</st> <st c="78030">n</st><st c="78031">_</st><st c="78032">n</st>
    <st c="78033">μ</st> <st c="78034">=</st> <st c="78035">μ</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78036">Eq.</st> <st c="78040">54</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78042">So, on average, for i.i.d.</st> <st c="78070">data, the sample
    mean will equal the true underlying population mean we are trying to estimate.</st>
    <st c="78166">This is true even though our sample mean is based on a finite sample
    size!</st> <st c="78241">We say that the</st> <st c="78256">sample mean is an</st>
    **<st c="78275">unbiased estimator</st>** <st c="78293">of the population mean.</st>
    <st c="78318">Even if the different random variables</st> <st c="78357">x</st><st
    c="78358">i</st> <st c="78359">are not independent of each other but they still
    have the same mean – that is, we still have</st> <st c="78453">𝔼</st><st c="78455">(</st><st
    c="78457">x</st><st c="78458">i</st><st c="78459">)</st> <st c="78460">=</st>
    <st c="78461">μ</st> <st c="78462">– then the sample mean</st> <st c="78486">m</st>
    <st c="78487">is still an unbiased estimator of the population</st> <st c="78537">mean</st>
    <st c="78542">μ</st><st c="78543">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78544">Now, it turns out that for the sample variance, if we have</st>
    <st c="78604">n</st> <st c="78605">−</st> <st c="78606">1</st> <st c="78607">in
    the denominator of the definition of</st> <st c="78648">s</st><st c="78649">2</st><st
    c="78650">, then</st> <st c="78657">s</st><st c="78658">2</st> <st c="78659">is
    an unbiased estimator of the population variance</st> <st c="78712">σ</st><st
    c="78713">2</st><st c="78714">. That is,</st> <st c="78725">𝔼</st><st c="78727">(</st><st
    c="78729">s</st><st c="78730">2</st><st c="78731">)</st> <st c="78732">=</st>
    <st c="78733">σ</st><st c="78734">2</st> <st c="78735">for any value of</st> <st
    c="78753">n</st><st c="78754">. We can prove this as well using a similar proof
    to that in</st> *<st c="78815">Eq.</st> <st c="78819">53</st>*<st c="78821">,
    but we will simply state it here and leave the proof as an exercise for the reader.</st>
    <st c="78907">If we had used</st> <st c="78922">n</st> <st c="78923">in the denominator
    of</st> *<st c="78946">Eq.</st> <st c="78950">48</st>* <st c="78952">instead of</st>
    <st c="78964">n</st> <st c="78965">−</st> <st c="78966">1</st><st c="78967">,
    our definition of</st> <st c="78987">s</st><st c="78988">2</st> <st c="78989">would
    not give us an unbiased estimator of</st> <st c="79033">σ</st><st c="79034">2</st><st
    c="79035">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79036">Since the difference between having</st> <st c="79073">n</st>
    <st c="79074">−</st> <st c="79075">1</st> <st c="79076">or</st> <st c="79080">n</st>
    <st c="79081">in the denominator of</st> *<st c="79104">Eq.</st> <st c="79108">48</st>*
    <st c="79110">vanishes as</st> <st c="79123">n</st> <st c="79124">→</st> <st c="79125">∞</st><st
    c="79126">, it becomes clear that an estimator can be biased at finite</st> <st
    c="79187">n</st> <st c="79188">but still consistent; that is, if the bias at finite</st>
    <st c="79242">n</st> <st c="79243">vanishes as</st> <st c="79256">n</st> <st c="79257">→</st>
    <st c="79258">∞</st><st c="79259">. So, bias and consistency are two different
    concepts – we will summarize them in</st> <st c="79341">a moment.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79350">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79354">We have said that the sample variance (defined with a denominator
    of</st> <st c="79424">n</st> <st c="79425">−</st> <st c="79426">1</st><st c="79427">)</st>
    <st c="79428">is an unbiased estimator of the population variance.</st> <st c="79482">This
    means</st> <st c="79493">𝔼</st><st c="79495">(</st><st c="79497">s</st><st c="79498">2</st><st
    c="79499">)</st> <st c="79500">=</st> <st c="79501">σ</st><st c="79502">2</st><st
    c="79503">. However, this does not mean that the sample standard deviation is
    an unbiased estimator of the population standard deviation.</st> <st c="79631">We
    do not have</st> <st c="79646">𝔼</st><st c="79648">(</st><st c="79650">s</st><st
    c="79651">)</st> <st c="79652">=</st> <st c="79653">σ</st><st c="79654">. Why
    not?</st> <st c="79665">Well,</st> <st c="79671">s</st> <st c="79672">=</st> <st
    c="79673">√</st><st c="79674">_</st><st c="79675">s</st><st c="79676">2</st> <st
    c="79677">and taking the square root is a non-linear operation.</st> <st c="79732">Wherever
    we introduce a non-linear operation, we potentially introduce new biases when
    we take the expectation.</st> <st c="79844">So, i) be aware that applying a non-linear
    transformation to data can introduce biases in estimates calculated from that
    transformed data, ii) the sample standard deviation is not an unbiased estimate
    of the population standard deviation.</st> <st c="80083">This explains this seeming
    cryptic note in the documentation of the</st> `<st c="80151">numpy.std</st>` <st
    c="80160">function, “</st>*<st c="80172">The standard deviation computed in this
    function is the square root of the estimated variance, so even with ddof=1, it
    will not be an unbiased estimate of the standard deviation per se.</st>*<st c="80358">”
    This note in the documentation always seems to confuse people on first reading,
    but it is just saying</st> <st c="80463">s</st> <st c="80464">is not unbiased
    even if</st> <st c="80489">s</st><st c="80490">2</st> <st c="80491">is.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="80495">Let’s recap what we have learned</st> <st c="80529">so far:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="80536">The sample mean and sample variance converge to their population
    equivalents as we use</st> <st c="80624">more data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80633">The sample mean and sample variance (with appropriate denominator)
    are unbiased estimates of their population counterparts at</st> <st c="80760">finite</st>
    <st c="80767">n</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80768">So, the sample mean and sample variance look like pretty good
    formulas to use, right?</st> <st c="80854">But in the real world, we can’t do
    an infinite number of experiments.</st> <st c="80924">So, knowing that the sample
    mean is an unbiased estimator of the population mean isn’t exactly helpful.</st>
    <st c="81028">For a finite-sized sample, we know that the sample mean will be
    in the right ballpark, but how good is</st> <st c="81131">m</st> <st c="81132">from
    any single dataset?</st> <st c="81158">Look at</st> *<st c="81166">Table 2.2</st>*<st
    c="81175">. In experiment number 4, the sample mean is quite a long way from the
    population value of 2.5\.</st> <st c="81271">Since the sample mean</st> <st c="81293">m</st>
    <st c="81294">is a random variable, we know it will vary around its expectation
    value.</st> <st c="81368">In this case, it looks like for</st> <st c="81400">n</st>
    <st c="81401">=</st> <st c="81402">5</st> <st c="81403">the variations of</st>
    <st c="81422">m</st> <st c="81423">around the population mean of</st> <st c="81454">μ</st>
    <st c="81455">=</st> <st c="81456">2.5</st> <st c="81459">can be large.</st> <st
    c="81474">Those variations are quantified by the standard deviation of</st> <st
    c="81535">m</st><st c="81536">, the size of the typical or expected deviation
    of</st> <st c="81587">m</st> <st c="81588">from</st> <st c="81593">μ</st><st c="81594">.
    Unsurprisingly, the variance of</st> <st c="81628">m</st> <st c="81629">decreases
    as</st> <st c="81643">n</st> <st c="81644">increases, meaning that the sample
    mean calculated from larger samples typically has smaller deviations from</st>
    <st c="81754">μ</st> <st c="81755">than the sample mean calculated from smaller
    samples.</st> <st c="81810">Ultimately, the variance of</st> <st c="81838">m</st>
    <st c="81839">becomes zero as</st> <st c="81856">n</st> <st c="81857">→</st> <st
    c="81858">∞</st><st c="81859">. How quickly the variance of an estimate of a population
    quantity converges to its population equivalent is termed its</st> **<st c="81979">efficiency</st>**<st
    c="81989">. The</st> <st c="81995">efficiency tells us how well the formula makes
    use of the data given to it to construct its estimate of the population quantity.</st>
    <st c="82124">The efficiency of an estimator depends upon the details of the formula
    and the distribution from which we assume the data is</st> <st c="82249">coming.</st>
    <st c="82257">For i.i.d.</st> <st c="82268">data, the variance of the sample mean
    has a</st> <st c="82312">simple form:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="82324">Var(</st><st c="82329">m</st><st c="82331">)</st> <st c="82332">=</st>
    <st c="82333">σ</st><st c="82334">2</st><st c="82335">_</st><st c="82336">n</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="82337">Eq.</st> <st c="82341">55</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="82343">So, the standard deviation of the sample mean (for i.i.d.</st>
    <st c="82402">data) is</st> <st c="82411">σ</st> <st c="82412">/</st> <st c="82413">√</st><st
    c="82414">_</st><st c="82415">n</st> <st c="82416">. This is also referred to
    as</st> <st c="82445">the</st> **<st c="82450">standard error</st>** <st c="82464">of
    the sample mean – it is the typical size of error that the sample mean will make
    when we use it as an estimate of the population mean</st> <st c="82602">μ</st><st
    c="82603">. Remember also here that</st> <st c="82629">σ</st> <st c="82630">is
    the population standard deviation of each individual data point.</st> <st c="82699">So,
    no matter how big</st> <st c="82721">σ</st> <st c="82722">is, we can still come
    up with a good estimate of the population mean if we have a sufficiently big sample
    size</st> <st c="82834">n</st><st c="82835">, and the preceding formula helps
    us work out how big that sample size needs to be.</st> <st c="82919">This is one
    of the beauties of statistical analysis.</st> <st c="82972">No matter how big
    the noise in individual data points is, we can still recover an accurate estimate
    of the population mean by having</st> <st c="83105">enough datapoints.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="83123">To sum up</st>*<st c="83133">: For any statistical estimator
    (that is, for any formula we use to estimate a population quantity), we ideally
    want three criteria to be met.</st> <st c="83277">Those criteria are</st> <st
    c="83296">the following:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="83310">Consistency</st>** <st c="83322">– An</st> <st c="83327">estimator
    converges to the (population) quantity it is attempting to estimate as the sample
    size</st> <st c="83425">n</st> <st c="83426">→</st> <st c="83427">∞</st><st c="83428">.
    In simple terms, the estimator gets better and better as we give it more data.</st>
    <st c="83509">The sample mean and sample variance are</st> <st c="83549">consistent
    estimators.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="83571">Unbiasedness</st>** <st c="83584">– An</st> <st c="83590">unbiased
    estimator has an expectation value equal to the (population) quantity it is attempting
    to estimate.</st> <st c="83699">In simple terms, the estimate is accurate on average,
    even for finite sample sizes.</st> <st c="83783">The sample mean and sample variance
    (with a denominator of</st> <st c="83842">n</st> <st c="83843">−</st> <st c="83844">1</st><st
    c="83845">) are</st> <st c="83851">unbiased estimators.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="83871">Efficiency</st>** <st c="83882">– Efficiency</st> <st c="83896">measures
    how quickly the variance of an unbiased estimator converges to zero as the sample
    size</st> <st c="83992">n</st> <st c="83993">→</st> <st c="83994">∞</st><st c="83995">.
    In simple terms, an efficient unbiased estimator makes good use of the data given
    to it.</st> <st c="84086">It turns out that the sample mean has the best efficiency
    of any linear estimator of the</st> <st c="84175">population mean.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="84191">Applying the concept of efficiency</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="84226">Let’s</st> <st c="84232">return to our CTR example.</st> <st c="84260">We
    had a sample size of</st> <st c="84284">n</st> <st c="84285">=</st> <st c="84286">20</st><st
    c="84288">. Our single sample had a sample mean of 0.6 (60%).</st> <st c="84340">We
    can model each user’s decision to click as a</st> <st c="84388">Bernoulli</st><st
    c="84397">(</st><st c="84399">p</st><st c="84400">)</st> <st c="84401">random
    variable.</st> <st c="84419">We know that the variance of each Bernoulli trial
    is</st> <st c="84472">p</st><st c="84473">(</st><st c="84474">1</st> <st c="84475">−</st>
    <st c="84476">p</st><st c="84477">)</st><st c="84478">, and so the standard error
    of the sample mean is</st> <st c="84528">√</st><st c="84529">_</st><st c="84530">p</st><st
    c="84531">(</st><st c="84532">1</st> <st c="84533">−</st> <st c="84534">p</st><st
    c="84535">)</st> <st c="84536">/</st> <st c="84537">20</st> <st c="84539">. But
    we don’t know what the true value of</st> <st c="84582">p</st> <st c="84583">is,
    the true CTR.</st> <st c="84602">But we do have our estimate of the CTR, so we
    can plug our sample mean of 0.6 into the formula</st> <st c="84697">√</st><st
    c="84698">_</st><st c="84699">p</st><st c="84700">(</st><st c="84701">1</st> <st
    c="84702">−</st> <st c="84703">p</st><st c="84704">)</st> <st c="84705">/</st>
    <st c="84706">20</st> <st c="84708">to get an approximate value of the standard
    error.</st> <st c="84760">In this case, we find</st> <st c="84782">√</st><st c="84783">____________</st><st
    c="84795">0.6</st> <st c="84799">×</st> <st c="84801">0.4</st> <st c="84804">/</st>
    <st c="84806">20</st> <st c="84808">≈</st> <st c="84810">0.11</st><st c="84814">.
    So, we should not be surprised if the real (population) CTR was as low as 0.5
    or 0.4\.</st> <st c="84902">As we have already said, our sample of 20 website
    visitors does not give us confidence in our conclusions.</st> <st c="85009">However,
    we can easily work out how big our sample size would need to be for us to be confident
    in that estimated CTR of 0.6\.</st> <st c="85135">Let’s say I wanted the standard
    error to be smaller than 0.01\.</st> <st c="85198">Again, if we assume our initial
    estimate of 0.6 is in the right ballpark, we can simply equate the formula in</st>
    *<st c="85308">Eq.</st> <st c="85312">55</st>* <st c="85314">for the standard
    error of the sample mean with 0.01 and solve for the required sample size</st>
    <st c="85406">n</st><st c="85407">. That is, we solve for</st> <st c="85431">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85445">p</st><st c="85447">(</st><st c="85448">1</st> <st c="85449">−</st>
    <st c="85450">p</st><st c="85451">)</st><st c="85452">_</st><st c="85453">n</st>
    <st c="85454">=</st> <st c="85455">0.01</st><st c="85459">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85461">Eq.</st> <st c="85465">56</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85467">This is done for</st> <st c="85485">n</st> <st c="85486">and where
    we plug in our sample mean of 0.6 as the value of</st> <st c="85547">p</st><st
    c="85548">. Solving the preceding equation for</st> <st c="85585">n</st><st c="85586">,
    we find the required sample size is</st> <st c="85624">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85638">n</st> <st c="85640">=</st> <st c="85641">0.6</st> <st c="85644">×</st>
    <st c="85646">0.4</st><st c="85649">_</st><st c="85651">0.01</st><st c="85655">2</st>
    <st c="85657">=</st> <st c="85658">2400</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85662">Eq.</st> <st c="85667">57</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85669">So, we need a couple of thousand website visitors for our experiment
    to reach a sound business conclusion.</st> <st c="85777">With 10 million annual
    visitors this is still entirely doable, but now we are basing our method</st>
    <st c="85873">and decisions on some sound statistical analysis and data science
    rather than guesswork.</st> <st c="85962">The calculation of the required sample
    size we have just walked through is somewhat hand-waving in places.</st> <st c="86069">When
    we come to</st> [*<st c="86085">Chapter 7</st>*](B19496_07.xhtml#_idTextAnchor369)<st
    c="86094">, we will learn how to do these calculations of the required sample
    size more rigorously.</st> <st c="86184">They are</st> <st c="86192">called</st>
    **<st c="86200">power calculations</st>**<st c="86218">. However, the principles
    and spirit of those rigorous power calculations are essentially the same as we
    have</st> <st c="86328">just outlined.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="86342">The empirical distribution function</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="86378">We have now linked the properties and characteristics of samples
    of data to the properties and characteristics of the underlying population distributions
    from which we have assumed the data comes.</st> <st c="86576">We’re now going
    to make the link between samples and distributions stronger by asking if we</st>
    <st c="86667">can view a sample as being some sort of distribution.</st> <st c="86722">The
    answer is yes.</st> <st c="86741">Enter the</st> **<st c="86751">empirical distribution
    function</st>** <st c="86782">(</st>**<st c="86784">EDF</st>**<st c="86787">)
    or, equivalently, the</st> **<st c="86812">empirical cumulative probability function</st>**
    <st c="86853">(</st>**<st c="86855">eCDF</st>**<st c="86859">).</st> <st c="86863">We</st>
    <st c="86866">use the word</st> *<st c="86879">empirical</st>* <st c="86888">to
    emphasize that this distribution is based on real observations; that</st> <st
    c="86961">is, data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="86970">If we had a sample of 30 data values</st> <st c="87008">x</st><st
    c="87009">1</st><st c="87010">,</st> <st c="87011">x</st><st c="87012">2</st><st
    c="87013">,</st> <st c="87014">⋯</st> <st c="87015">,</st> <st c="87016">x</st><st
    c="87017">30</st> <st c="87019">and we wanted to think of them as some sort of
    probability distribution, we would have to be able to plot them as a collection
    of bars as in</st> *<st c="87161">Figure 2</st>**<st c="87169">.4</st>*<st c="87171">.
    We have 30 datapoints, so we can think of the distribution as 30 vertical bars,
    each centered on one of the datapoints.</st> <st c="87293">Since the total probability
    in a distribution must add up to 1, each bar has a weight 1/30\.</st> <st c="87385">And
    since we have only seen values corresponding to the actual data values</st> <st
    c="87460">x</st><st c="87461">1</st><st c="87462">,</st> <st c="87463">x</st><st
    c="87464">2</st><st c="87465">,</st> <st c="87466">⋯</st> <st c="87467">,</st>
    <st c="87468">x</st><st c="87469">30</st><st c="87471">, those bars are infinitesimally
    thin – there is no probability of getting any possible value other than</st> <st
    c="87576">x</st><st c="87577">1</st> <st c="87578">or</st> <st c="87582">x</st><st
    c="87583">2</st> <st c="87584">or…..</st><st c="87590">x</st><st c="87592">30</st><st
    c="87594">. So, our empirical distribution is actually a series of infinitely
    narrow s</st><st c="87670">pikes.</st> <st c="87678">We have drawn that schematically
    in</st> *<st c="87714">Figure 2</st>**<st c="87722">.14</st>* <st c="87725">for
    a set of 30 real</st> <st c="87747">x</st> <st c="87748">values:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14: Representing a sample as a series of spikes](img/B19496_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="87797">Figure 2.14: Representing a sample as a series of spikes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="87853">Figure 2</st>**<st c="87862">.14</st>* <st c="87865">shows a</st>
    <st c="87874">schematic representation of the EDF, but how do we represent it
    mathematically?</st> <st c="87954">We’ll build it up mathematically in stages.</st>
    <st c="87998">Mathematically, we want to represent the first spike as a function
    that picks out only the value</st> <st c="88095">x</st><st c="88096">1</st><st
    c="88097">, which is 0.3 in this example.</st> <st c="88129">The mathematical
    function that does this is the Dirac delta function, or Dirac δ function, named
    after the famous physicist Paul Dirac.</st> <st c="88265">The Dirac delta function,</st>
    <st c="88291">δ</st><st c="88292">(</st><st c="88293">x</st><st c="88294">)</st><st
    c="88295">, is actually a distribution.</st> <st c="88325">It is sometimes referred
    to as being a generalized function, but it is easier to loose</st><st c="88411">ly
    think of it as a cont</st><st c="88436">inuous probability distribution with the</st>
    <st c="88478">following properties:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88499">∫</st><st c="88501">c</st><st c="88502">−</st><st c="88503">ε</st><st
    c="88504">c</st><st c="88505">+</st><st c="88506">ε</st><st c="88507">δ</st><st
    c="88508">(</st><st c="88509">x</st> <st c="88510">−</st> <st c="88511">c</st><st
    c="88512">)</st> <st c="88513">dx</st> <st c="88515">=</st> <st c="88517">1</st>
    <st c="88518">for any</st> <st c="88526">ε</st> <st c="88527">></st> <st c="88528">0</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88529">Eq.</st> <st c="88533">58</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88535">∫</st><st c="88537">c</st><st c="88538">−</st><st c="88539">ε</st><st
    c="88540">c</st><st c="88541">+</st><st c="88542">ε</st><st c="88543">δ</st><st
    c="88544">(</st><st c="88545">x</st> <st c="88546">−</st> <st c="88547">c</st><st
    c="88548">)</st><st c="88549">g</st><st c="88550">(</st><st c="88551">x</st><st
    c="88552">)</st><st c="88553">dx</st> <st c="88555">=</st> <st c="88557">g</st><st
    c="88558">(</st><st c="88559">c</st><st c="88560">)</st> <st c="88561">for any</st>
    <st c="88570">ε</st> <st c="88571">> 0</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88575">Eq.</st> <st c="88580">59</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88582">So, if we have a delta function,</st> <st c="88616">δ</st><st
    c="88617">(</st><st c="88618">x</st><st c="88619">−</st> <st c="88620">x</st><st
    c="88621">1</st><st c="88622">)</st><st c="88623">, centered on</st> <st c="88637">x</st><st
    c="88638">1</st><st c="88639">, then using</st> *<st c="88652">Eq.</st> <st c="88656">19</st>*
    <st c="88658">and</st> *<st c="88663">Eq.</st> <st c="88667">20</st>* <st c="88669">for
    the mean and variance of a continuous probability distribution, we can think of</st>
    <st c="88754">δ</st><st c="88755">(</st><st c="88756">x</st><st c="88757">−</st>
    <st c="88758">x</st><st c="88759">1</st><st c="88760">)</st> <st c="88761">as
    a probability distribution with mean</st> <st c="88802">x</st><st c="88803">1</st>
    <st c="88804">and variance zero.</st> <st c="88824">So, sampling from</st> <st
    c="88842">δ</st><st c="88843">(</st><st c="88844">x</st><st c="88845">−</st> <st
    c="88846">x</st><st c="88847">1</st><st c="88848">)</st> <st c="88849">would only
    ever give us the value</st> <st c="88884">x</st><st c="88885">1</st><st c="88886">,
    which is what</st> <st c="88902">we want.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="88910">If we repeat the process by putting a Dirac delta function centered
    on each of our 30 datapoints</st> <st c="89008">x</st><st c="89009">1</st><st
    c="89010">,</st> <st c="89011">x</st><st c="89012">2</st><st c="89013">,</st>
    <st c="89014">⋯</st> <st c="89015">,</st> <st c="89016">x</st><st c="89017">30</st><st
    c="89019">, we can think of our sample as being equivalent to the</st> <st c="89075">following
    PDF:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89089">1</st><st c="89091">_</st><st c="89092">30</st> <st c="89094">∑</st><st
    c="89096">i</st><st c="89097">=</st><st c="89098">1</st><st c="89099">30</st><st
    c="89101">δ</st><st c="89103">(</st><st c="89104">x</st><st c="89105">−</st> <st
    c="89106">x</st><st c="89107">i</st><st c="89108">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89109">Eq.</st> <st c="89113">60</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89115">For a sample,</st> <st c="89130">x</st><st c="89131">1</st><st
    c="89132">,</st> <st c="89133">x</st><st c="89134">2</st><st c="89135">,</st>
    <st c="89136">⋯</st> <st c="89137">,</st> <st c="89138">x</st><st c="89139">n</st><st
    c="89140">, of</st> <st c="89145">n</st> <st c="89146">datapoints, the ge</st><st
    c="89165">neralization is obvious – we can view our sample as being a distribution</st>
    <st c="89239">with density:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89252">1</st><st c="89254">_</st><st c="89255">n</st> <st c="89256">∑</st><st
    c="89257">i</st><st c="89258">=</st><st c="89259">1</st><st c="89260">n</st><st
    c="89261">δ</st><st c="89262">(</st><st c="89263">x</st><st c="89264">−</st> <st
    c="89265">x</st><st c="89266">i</st><st c="89267">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89268">Eq.</st> <st c="89272">61</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89274">Why is this useful?</st> <st c="89295">Well, sometimes</st> <st
    c="89310">we have a metric or expression or concept that we know how to compute
    for continuous probability distributions, but we don’t quite know how to calculate
    the equivalent or corresponding metric or formula for a sample of data.</st> <st
    c="89536">No problem!</st> <st c="89548">Simply plug</st> *<st c="89560">Eq.</st>
    <st c="89564">61</st>* <st c="89566">into the formula for continuous probability
    distributions, simplify using</st> *<st c="89641">Eq.</st> <st c="89645">59</st>*<st
    c="89647">, and you have an answer.</st> <st c="89673">You don’t believe me?</st>
    <st c="89695">Plug the distribution in</st> *<st c="89720">Eq.</st> <st c="89724">61</st>*
    <st c="89726">into</st> *<st c="89732">Eq.</st> <st c="89736">19</st>* <st c="89738">and</st>
    *<st c="89743">Eq.</st> <st c="89747">20</st>* <st c="89749">for the mean and
    variance of a continuous probability distribution and what you get out are the
    expressions in</st> *<st c="89861">Eq.</st> <st c="89865">46</st>* <st c="89867">and</st>
    *<st c="89872">Eq.</st> <st c="89876">48</st>*<st c="89878">, for the sample mean
    and sample variance (up to a factor of</st> <st c="89939">n</st><st c="89940">_</st><st
    c="89941">n</st> <st c="89942">−</st> <st c="89943">1</st> <st c="89944">).</st>
    <st c="89948">Give it</st> <st c="89956">a try.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="89962">From the EDF in</st> *<st c="89979">Eq.</st> <st c="89983">61</st>*<st
    c="89985">, we can calculate the corresponding eCDF.</st> <st c="90028">The cumulative
    probability function</st> <st c="90064">CDF</st><st c="90067">(</st><st c="90069">x</st><st
    c="90070">)</st> <st c="90071">is the probability of getting a value less than
    or equal to</st> <st c="90132">x</st><st c="90133">. So for a continuous ra</st><st
    c="90157">ndom variable with density function</st> <st c="90194">f</st><st c="90195">(</st><st
    c="90196">x</st><st c="90197">)</st><st c="90198">, the CDF is given by</st> <st
    c="90220">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="90234">CDF</st><st c="90238">(</st><st c="90240">x</st><st c="90241">)</st>
    <st c="90242">=</st> <st c="90243">∫</st><st c="90244">−</st><st c="90245">∞</st><st
    c="90246">x</st><st c="90247">f</st><st c="90248">(</st><st c="90249">s</st><st
    c="90250">)</st><st c="90251">ds</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="90253">Eq.</st> <st c="90258">62</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="90260">Plugging the empirical density function in</st> *<st c="90304">Eq.</st>
    <st c="90308">61</st>* <st c="90310">into</st> *<st c="90316">Eq.</st> <st c="90320">62</st>*<st
    c="90322">, we get the eCDF.</st> <st c="90341">Using</st> *<st c="90347">Eq.</st>
    <st c="90351">58</st>*<st c="90353">, we can see that the eCDF goes through a
    series of steps, increasing by</st> <st c="90426">1</st><st c="90427">_</st><st
    c="90428">n</st> <st c="90429">every time</st> <st c="90441">x</st> <st c="90442">passes
    one of the datapoints.</st> <st c="90473">The eCDF for the sample shown in</st>
    *<st c="90506">Figure 2</st>**<st c="90514">.14</st>* <st c="90517">is shown in</st>
    *<st c="90530">Figure 2</st>**<st c="90538">.15</st>*<st c="90541">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15: The eCDF for the series of 30 x values shown in Figure 2.14](img/B19496_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="90570">Figure 2.15: The eCDF for the series of 30 x values shown in
    Figure 2.14</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="90642">The</st> <st c="90647">stepped nature of the eCDF is clearly visible
    in</st> *<st c="90696">Figure 2</st>**<st c="90704">.15</st>*<st c="90707">. As
    the sample size</st> <st c="90728">n</st> <st c="90729">increases, the size of
    those steps decreases and the eCDF looks smoother and smoother.</st> <st c="90817">In
    fact, as</st> <st c="90829">n</st> <st c="90830">→</st> <st c="90831">∞</st><st
    c="90832">, the EDF in</st> *<st c="90845">Eq.</st> <st c="90849">61</st>* <st
    c="90851">and its associated eCDF converge to their population counterparts.</st>
    <st c="90919">So, just like we have used the sample mean and the sample variance
    as approximations of the population mean and population variance, we can use (when</st>
    <st c="91069">n</st> <st c="91070">is reasonably large) the empirical distribution
    in</st> *<st c="91122">Eq.</st> <st c="91126">61</st>* <st c="91128">as an approximation
    of the</st> <st c="91156">population distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="91180">Excellent!</st> <st c="91192">We are making good progress on understanding
    and taming randomness in data.</st> <st c="91268">Having seen how we can use the
    eCDF to approximate a population CDF, let’s summarize what we have learned about
    statistical estimators and formulas in this section before we move on to our final
    section of</st> <st c="91474">the chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="91486">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="91502">In this section, we have learned</st> <st c="91536">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="91550">How to calculate the sample mean and</st> <st c="91588">sample
    variance</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="91603">How the sample mean and sample variance converge to their population
    counterparts as we increase the</st> <st c="91705">sample size</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="91716">How the sample mean and sample variance are unbiased estimators
    of their</st> <st c="91790">population counterparts</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="91813">How to calculate the standard error (standard deviation) of the
    sample mean and how to use this to approximately calculate the sample size required
    to ensure the sample mean is a sufficiently accurate estimate of the</st> <st
    c="92031">population mean</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="92046">How to construct the EDF and use it as an approximation of the</st>
    <st c="92110">population distribution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="92133">Having learned how to relate the characteristics of a sample to
    the characteristics of the underlying population, in the next section we will
    learn why a particular distribution, the normal distribution, is the most common
    distribution from which data</st> <st c="92386">is generated.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="92399">The Central Limit Theorem</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="92425">Earlier in the chapter, when</st> <st c="92454">we were introducing
    specific continuous-valued distributions, we described the Gaussian or normal
    distribution and we said that it was an extremely important distribution because
    it was an extremely common distribution.</st> <st c="92675">By this, we meant
    that many datasets you will encounter will effectively have been drawn from a
    normal distribution, or you will use a normal distribution to model those datasets.</st>
    <st c="92855">We will now</st> <st c="92867">explain why.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="92879">Sums of random variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="92904">Lots of the</st> <st c="92916">quantities we analyze as data scientists
    are aggregations of other data.</st> <st c="92990">Aggregating observations over
    some dimension to simplify the data is a very natural thing</st> <st c="93080">to
    do.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="93086">For example, consider our e-commerce scenario where we are interested
    in how many items are sold.</st> <st c="93185">The number of items sold on any
    day of the year we might model as a binomial random variable, but what about for
    the whole year?</st> <st c="93314">Imagine we have a relatively niche website
    where we only get, say, 20 visitors a day who are thinking about buying a particular
    product, and the probability of any one of those visitors buying the product is
    0.3\.</st> <st c="93527">So, on any day</st> <st c="93542">t</st><st c="93543">,
    we can model the total items sold</st> <st c="93579">X</st><st c="93580">t</st>
    <st c="93581">as</st> <st c="93585">X</st><st c="93586">t</st> <st c="93587">~</st>
    <st c="93588">Binomial</st><st c="93596">(</st><st c="93598">20</st><st c="93600">,</st>
    <st c="93601">0.3</st><st c="93604">)</st><st c="93606">. We know from the properties
    of the binomial distribution that the mean number of items sold on any day is</st>
    <st c="93714">20</st> <st c="93716">×</st> <st c="93718">0.3</st><st c="93721">,
    while its variance is</st> <st c="93745">20</st> <st c="93747">×</st> <st c="93749">0.3</st>
    <st c="93752">×</st> <st c="93754">(</st><st c="93755">1</st> <st c="93756">−</st>
    <st c="93757">0.3</st><st c="93760">)</st><st c="93762">. The total number of
    items sold in a year is, simply,</st> <st c="93817">X</st><st c="93818">1</st><st
    c="93819">+</st> <st c="93820">X</st><st c="93821">2</st><st c="93822">+</st>
    <st c="93823">⋯</st> <st c="93824">+</st> <st c="93825">X</st><st c="93826">365</st><st
    c="93829">. This sum is a random variable itself.</st> <st c="93869">We will denote
    this sum by</st> <st c="93896">S</st><st c="93897">. As</st> <st c="93902">S</st>
    <st c="93903">is a sum of random variables, we can use the rules for combining
    random variables that we explained earlier in the chapter, to find</st> <st c="94036">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94050">𝔼</st><st c="94053">(</st><st c="94055">S</st><st c="94056">)</st>
    <st c="94057">=</st> <st c="94058">365</st> <st c="94061">×</st> <st c="94063">20</st>
    <st c="94065">×</st> <st c="94067">0.3</st> <st c="94070">=</st> <st c="94072">2190</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94076">Eq.</st> <st c="94081">63</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94083">We will also use the aforementioned rules to find</st> <st c="94134">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94148">Var</st><st c="94152">(</st><st c="94154">S</st><st c="94155">)</st>
    <st c="94156">=</st> <st c="94157">365</st> <st c="94160">×</st> <st c="94162">20</st>
    <st c="94164">×</st> <st c="94166">0.3</st> <st c="94169">×</st> <st c="94171">(</st><st
    c="94172">1</st> <st c="94173">−</st> <st c="94174">0.3</st><st c="94177">)</st>
    <st c="94179">=</st> <st c="94180">1533</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94184">Eq.</st> <st c="94189">64</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94191">We can</st> <st c="94199">see that we know the mean and variance
    of the yearly total sales</st> <st c="94264">S</st><st c="94265">, but what about
    the shape of the distribution of</st> <st c="94315">S</st><st c="94316">? There
    are many distributions of different shapes that could have a mean of 2190 and
    a variance</st> <st c="94413">of 1533.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94421">This is where a very famous piece of math comes to our aid.</st>
    <st c="94482">The Central Limit Theorem (CLT) tells us that as we add more and
    more random variables together, their sum behaves more and more like a Gaussian
    random variable.</st> <st c="94644">Since we already know the mean and variance
    of the sum</st> <st c="94699">S</st><st c="94700">, we can write a mathematical
    statement of the CLT.</st> <st c="94752">Specifically, if we have</st> <st c="94777">n</st>
    <st c="94778">i.i.d.</st> <st c="94786">random variables</st> <st c="94803">X</st><st
    c="94804">i</st> <st c="94805">,</st> <st c="94806">i</st> <st c="94807">=</st>
    <st c="94808">1</st><st c="94809">,</st> <st c="94810">⋯</st> <st c="94811">,</st>
    <st c="94812">n</st><st c="94813">, each of which has mean</st> <st c="94838">μ</st>
    <st c="94839">and variance</st> <st c="94853">σ</st><st c="94854">2</st><st c="94855">,
    then if we define</st> <st c="94875">S</st> <st c="94876">=</st> <st c="94877">X</st><st
    c="94878">1</st> <st c="94879">+</st> <st c="94880">X</st><st c="94881">2</st><st
    c="94882">+</st> <st c="94883">⋯</st> <st c="94884">+</st> <st c="94885">X</st><st
    c="94886">n</st><st c="94887">, the distribution of</st> <st c="94909">S</st>
    <st c="94910">has the following</st> <st c="94929">limiting behavior:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94947">S</st> <st c="94949">→</st> <st c="94950">Normal</st><st c="94956">(</st><st
    c="94958">nμ</st><st c="94960">,</st> <st c="94961">n</st> <st c="94962">σ</st><st
    c="94963">2</st><st c="94964">)</st> <st c="94965">as</st> <st c="94968">n</st>
    <st c="94969">→</st> <st c="94970">∞</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94971">Eq.</st> <st c="94975">65</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="94977">As the mean</st> <st c="94990">nμ</st> <st c="94992">and variance</st>
    <st c="95006">n</st> <st c="95007">σ</st><st c="95008">2</st> <st c="95009">of
    the distribution on the right-hand side of</st> *<st c="95056">Eq.</st> <st c="95060">65</st>*
    <st c="95062">become infini</st><st c="95076">te as</st> <st c="95083">n</st>
    <st c="95084">→</st> <st c="95085">∞</st><st c="95086">, we need to write</st>
    *<st c="95105">Eq.</st> <st c="95109">65</st>* <st c="95111">in a slightly more
    rigorous way</st> <st c="95144">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="95155">S</st> <st c="95157">−</st> <st c="95158">nμ</st><st c="95160">_</st><st
    c="95162">σ</st> <st c="95163">√</st><st c="95164">_</st><st c="95165">n</st>
    <st c="95166">→</st> <st c="95167">Normal</st><st c="95173">(</st><st c="95175">0,1</st><st
    c="95178">)</st> <st c="95180">as</st> <st c="95183">n</st> <st c="95184">→</st>
    <st c="95185">∞</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="95186">Eq.</st> <st c="95190">66</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="95192">We won’t go into the proof of the CLT as it involves some additional
    math techniques that we won’t</st> <st c="95292">be covering in this book.</st>
    <st c="95318">What is more important is that you are aware of the CLT, specifically
    that when we sum up lots of random variables, the distribution of that sum will
    be very well approximated by a Gaussian random variable.</st> <st c="95525">Let’s
    illustrate this with a</st> <st c="95554">code example.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="95567">CLT code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="95584">For our first</st> <st c="95599">code example, we’ll start slightly
    simpler.</st> <st c="95643">We’ll add 20</st> <st c="95656">Uniform</st><st c="95663">(</st><st
    c="95665">0,1</st><st c="95668">)</st> <st c="95670">random variables together.</st>
    <st c="95698">We’ll do this lots of times and plot the resulting histogram of
    those totals to see if it looks anything like a normal distribution.</st> <st
    c="95831">The code is shown next and is also in the</st> `<st c="95873">Code_Examples_Chap2.ipynb</st>`
    <st c="95898">Jupyter notebook in the</st> <st c="95923">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="98741">An example</st> <st c="98753">output from the code is shown in</st>
    *<st c="98786">Figure 2</st>**<st c="98794">.16</st>*<st c="98797">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Example of the CLT when adding together the values of 20 Uniform(0,1)
    random variables](img/B19496_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="98962">Figure 2.16: Example of the CLT when adding together the values
    of 20 Uniform(0,1) random variables</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="99061">By comparing</st> <st c="99074">the red line of the CLT approximation
    to the histogram values, you can see how good the approximation of the CLT-based
    approximation is, even though we are only adding 20 random variables together
    in</st> <st c="99275">this example.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="99288">We have said that the CLT gives us an approximation here because
    technically, the CLT is an asymptotic result.</st> *<st c="99400">Eq.</st> <st
    c="99404">66</st>* <st c="99406">tells us we only get a normal distribution when
    we add an infinite number of random variables together.</st> <st c="99511">What
    we have done is to take the CLT and say, well, when we add a finite number of
    random variables together, we will get something that is approximately normal.</st>
    <st c="99673">As you can see from this example, that approximation can be pretty
    good even when we are a long way from adding an infinite number of random values
    together.</st> <st c="99831">As you would expect, the more random variables we
    add together, the better the CLT approximation gets, and the approximation is
    better in the center of the distribution (around the mean) than in the tails (at
    the edges).</st> <st c="100052">How quickly the distribution of the sum of random
    values converges to a normal distribution as</st> <st c="100147">n</st> <st c="100148">increases
    depends not only on</st> <st c="100179">n</st> <st c="100180">(the number of random
    variables) but also on the details of the distribution of each random variable;
    for example, are they uniform random variables, binomial random variables, and</st>
    <st c="100362">so on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="100368">The reason we chose adding</st> <st c="100396">Uniform</st><st
    c="100403">(</st><st c="100405">0,1</st><st c="100408">)</st> <st c="100410">random
    variables to illustrate our first example of the CLT is partly because the shape
    of the density function of each of the random variables is flat, and so it is
    very different from the normal distribution that results from their sum.</st>
    <st c="100650">This emphasizes the fact that the resulting normal distribution
    shape is because we are adding lots of random variables together, not because
    of the properties or shape (within reason) of the individual random variables
    we are adding together.</st> <st c="100894">The</st> <st c="100897">normal distribution
    shape that results from adding lots of random variables together is a</st> <st
    c="100988">universal outcome.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="101006">We also chose to add</st> <st c="101028">Uniform</st><st c="101035">(</st><st
    c="101037">0,1</st><st c="101040">)</st> <st c="101042">random variables to illustrate
    the CLT because</st> <st c="101090">Uniform</st><st c="101097">(</st><st c="101099">0,1</st><st
    c="101102">)</st> <st c="101104">is a continuous distribution.</st> <st c="101135">Consequently,
    the sum of many</st> <st c="101165">Uniform</st><st c="101172">(</st><st c="101174">0,1</st><st
    c="101177">)</st> <st c="101179">random variables will also be continuous, and
    when we compare the normal distribution of the CLT approximation, which is also
    continuous, it is possible to see a very good agreement between the histogram
    of the experimental sums and the CLT approximation, even when we are adding only
    a finite number of random</st> <st c="101491">values together.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="101507">CLT example with discrete variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="101543">When</st> <st c="101548">our starting random variables are discrete,
    then so is their sum.</st> <st c="101615">Let’s simulate our “items sold in a
    year” example, where we add together 365</st> <st c="101692">Binomial</st><st
    c="101700">(</st><st c="101702">20</st><st c="101704">,</st> <st c="101705">0.3</st><st
    c="101708">)</st> <st c="101710">random variables.</st> <st c="101729">We can
    do this using very similar code to that shown previously by simply replacing</st>
    <st c="101813">Uniform</st><st c="101820">(</st><st c="101822">0,1</st><st c="101825">)</st>
    <st c="101827">with</st> <st c="101833">Binomial</st><st c="101841">(</st><st
    c="101843">20</st><st c="101845">,</st> <st c="101846">0.3</st><st c="101849">)</st>
    <st c="101851">and updating the mean and variance calculations of the CLT approximation.</st>
    <st c="101926">Example code to do this simulation is given in the</st> `<st c="101977">Code_Examples_Chap2.ipynb</st>`
    <st c="102002">Jupyter notebook in the GitHub repository, but we only show example
    output from that code in</st> *<st c="102096">Figure 2</st>**<st c="102104">.17</st>*<st
    c="102107">. The granular or discrete na</st><st c="102136">ture of the sum of
    the 365 discrete values is clear, but the CLT still provides us with a</st> <st
    c="102227">good approximation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17: Example of the CLT when adding together the values of 365 Binomial(20,
    0.3) random variables](img/B19496_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="102436">Figure 2.17: Example of the CLT when adding together the values
    of 365 Binomial(20, 0.3) random variables</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="102541">The CLT is incredibly</st> <st c="102564">important when we analyze
    a dataset because it tells us what probability density shape we should expect
    for a quantity that is a sum of many other things.</st> <st c="102719">But what
    if we are analyzing a quantity in a dataset that isn’t an aggregation and we want
    to know its density shape?</st> <st c="102837">We will now show how to do this
    computationally, with some</st> <st c="102896">code examples.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="102910">Computational estimation of a PDF from data</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="102954">Here, we</st> <st c="102963">will show you how to use functions
    from the</st> `<st c="103008">scikit-learn</st>` <st c="103020">package to calculate
    a computational estimate of a PDF.</st> <st c="103077">To do this, we use something
    called</st> **<st c="103113">kernel density estimation</st>** <st c="103138">(</st>**<st
    c="103140">KDE</st>**<st c="103143">).</st> <st c="103147">KDE</st> <st c="103151">works
    by approximating the PDF that underlies the data by a series of fixed-shape distributions,
    one placed at each data point.</st> <st c="103279">If</st> <st c="103282">x</st><st
    c="103283">1</st><st c="103284">,</st> <st c="103285">x</st><st c="103286">2</st><st
    c="103287">,</st> <st c="103288">⋯</st> <st c="103289">,</st> <st c="103290">x</st><st
    c="103291">n</st> <st c="103292">are the data values in our sample, the KDE</st>
    <st c="103336">estimates the PDF as</st> <st c="103357">ˆ</st><st c="103358">f</st><st
    c="103359">(</st><st c="103360">x</st><st c="103361">)</st><st c="103362">, with</st>
    <st c="103369">ˆ</st><st c="103370">f</st><st c="103371">(</st><st c="103372">x</st><st
    c="103373">)</st> <st c="103374">calculated from the</st> <st c="103395">following
    formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="103413">ˆ</st><st c="103415">f</st><st c="103416">(</st><st c="103417">x</st><st
    c="103418">)</st> <st c="103419">=</st> <st c="103420">1</st><st c="103421">_</st><st
    c="103422">nh</st> <st c="103424">∑</st><st c="103426">i</st><st c="103427">=</st><st
    c="103428">1</st><st c="103429">n</st><st c="103430">K</st><st c="103431">(</st><st
    c="103432">x</st><st c="103433">−</st> <st c="103434">x</st><st c="103435">i</st><st
    c="103436">_</st><st c="103437">h</st><st c="103438">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="103439">Eq.</st> <st c="103443">67</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="103445">The hat symbol on</st> <st c="103464">ˆ</st><st c="103465">f</st><st
    c="103466">(</st><st c="103467">x</st><st c="103468">)</st> <st c="103469">is
    used to denote the fact that</st> <st c="103502">ˆ</st><st c="103503">f</st><st
    c="103504">(</st><st c="103505">x</st><st c="103506">)</st> <st c="103507">is
    an</st> **<st c="103514">estimate</st>** <st c="103522">of the true PDF,</st>
    <st c="103540">f</st><st c="103541">(</st><st c="103542">x</st><st c="103543">)</st><st
    c="103544">. In</st> *<st c="103549">Eq.</st> <st c="103553">67</st>*<st c="103555">,
    the function</st> <st c="103570">K</st><st c="103571">(</st><st c="103572">x</st><st
    c="103573">)</st> <st c="103574">is called</st> <st c="103584">the</st> **<st
    c="103589">kernel</st>** <st c="103595">function.</st> <st c="103606">It is also</st>
    <st c="103617">called a</st> **<st c="103626">window</st>** <st c="103632">function
    because, as we will see, it specifies the window or range over which data is smoothed.</st>
    <st c="103729">It is typically a function that is highest at</st> <st c="103775">x</st>
    <st c="103776">=</st> <st c="103777">0</st> <st c="103778">and falls away symmetrically
    to zero on either side, possibly within a finite distance from</st> <st c="103871">x</st>
    <st c="103872">=</st> <st c="103873">0</st><st c="103874">.</st> *<st c="103876">Figure
    2</st>**<st c="103884">.18</st>* <st c="103887">shows two</st> <st c="103897">example
    kernel functions; on the left is the Parzen window, which is a Gaussian, a</st><st
    c="103980">nd on the right is the Tricube kernel function, which has finite support
    (it is zero outside of</st> <st c="104077">|</st><st c="104078">x</st><st c="104079">|</st>
    <st c="104080">=</st> <st c="104081">1</st><st c="104082">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18: Two examples of commonly used kernel functions](img/B19496_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="104233">Figure 2.18: Two examples of commonly used kernel functions</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="104292">Given each datapoint has a total weight</st> <st c="104333">1</st>
    <st c="104334">/</st> <st c="104335">n</st> <st c="104336">when we calculate a
    sample average, we can see that the function</st> <st c="104402">K</st> <st c="104403">smooths
    out the impact of having a datapoint at</st> <st c="104452">x</st><st c="104453">i</st><st
    c="104454">. How much it smooths out that impact is determined by, obviously,
    the shape of the function</st> <st c="104547">K</st><st c="104548">, but also
    the parameter</st> <st c="104573">h</st><st c="104574">, which modifies the width
    of the impact of the kernel smoothing.</st> <st c="104640">The parameter</st>
    <st c="104654">h</st> <st c="104655">is called</st> <st c="104665">the</st> **<st
    c="104670">kernel width</st>**<st c="104682">, or sometimes</st> <st c="104696">the</st>
    **<st c="104701">bandwidth</st>**<st c="104710">. A large value of</st> <st c="104729">h</st>
    <st c="104730">will mean each datapoint is smoothed out considerably, with the
    impact of each datapoint overlapping and the resulting approximation</st> <st
    c="104864">ˆ</st><st c="104865">f</st><st c="104866">(</st><st c="104867">x</st><st
    c="104868">)</st> <st c="104869">looking almost like a uniform distribution.</st>
    <st c="104914">A small value of</st> <st c="104931">h</st> <st c="104932">means
    each datapoint is smoothed out only a little and essentially is still a spike.</st>
    <st c="105018">With a small value of</st> <st c="105040">h</st><st c="105041">,
    the resulting approximation</st> <st c="105071">ˆ</st><st c="105072">f</st><st
    c="105073">(</st><st c="105074">x</st><st c="105075">)</st> <st c="105076">doesn’t
    look that much different from the EDF of the</st> <st c="105130">sample data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="105142">So, how do we choose a value for</st> <st c="105176">h</st><st
    c="105177">, or indeed a kernel function</st> <st c="105207">K</st><st c="105208">(</st><st
    c="105209">x</st><st c="105210">)</st><st c="105211">? We won’t go into that here.</st>
    <st c="105241">There is a whole field of statistics devoted to how to automatically
    determine from the data a suitable value for</st> <st c="105355">h</st><st c="105356">,
    and we will make use of one of them (Silverman’s rule of thumb) now in a</st>
    <st c="105431">code example.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="105444">KDE code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="105461">The following</st> <st c="105475">code constructs kernel</st>
    <st c="105499">density estimates from 30 values drawn from a gamma distribution.</st>
    <st c="105565">We do this for two different kernel functions – a Parzen kernel
    and an exponential kernel.</st> <st c="105656">This code example can be found
    in the</st> `<st c="105694">Code_Examples_Chap2.ipynb</st>` <st c="105719">Jupyter
    notebook in the</st> <st c="105744">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="107580">The output</st> <st c="107591">from the code is shown in</st>
    *<st c="107618">Figure 2</st>**<st c="107626">.19</st>*<st c="107629">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19: Example of KDE using different kernel functions](img/B19496_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="107789">Figure 2.19: Example of KDE using different kernel functions</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="107849">Figure 2</st>**<st c="107858">.19</st>* <st c="107861">also</st>
    <st c="107867">shows the true PDF of the gamma</st> <st c="107898">distribution,
    and we can see that, even though we have only 30 datapoints in our sample, the
    kernel density estimates are close to the</st> <st c="108034">true density.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="108047">That practical example of how to estimate probability densities
    from data brings us neatly to the end of this section, so let’s recap what we
    learned about KDE, and then wrap up by summarizing the chapter as</st> <st c="108256">a
    whole.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="108264">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="108280">In this section, we have learned</st> <st c="108314">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="108328">How adding together lots of i.i.d.</st> <st c="108364">random
    variables produces a random variable whose distribution is</st> <st c="108430">approximately
    normal.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="108451">How many datasets we analyze will naturally contain variables
    that are themselves aggregations of many random values; for example, yearly total
    sales as the aggregation of 365 daily</st> <st c="108634">sales values.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="108647">How the approximation by a normal distribution improves the more
    random values we are adding together.</st> <st c="108751">This is</st> <st c="108759">the
    CLT.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="108767">How to use KDE to construct an estimate of a probability density
    from data even when we can’t or don’t want to make use of</st> <st c="108891">the
    CLT.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="108899">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="108907">This chapter has been a long one.</st> <st c="108942">The effort
    will be worthwhile.</st> <st c="108973">Random variation is a component of any
    dataset, so knowing how to characterize and describe that random variation when
    analyzing data is a key skill for any data scientist.</st> <st c="109146">In this
    chapter, we have learned</st> <st c="109179">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="109193">How and why randomness arises</st> <st c="109224">in data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109231">How random variables are a natural concept to describe randomness</st>
    <st c="109298">in data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109305">Key aspects of random variables, such as their probability distributions,
    and how to use key metrics such as the mean and variance of a distribution to
    characterize</st> <st c="109471">a distribution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109485">How we can think of datasets as being samples drawn from an underlying
    distribution, and it is the underlying distribution we are really interested</st>
    <st c="109634">in understanding</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109650">How to summarize a sample using the sample mean and</st> <st
    c="109703">sample variance</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109718">How sample characteristics, such as the sample mean and sample
    variance, can be related back to the corresponding quantities of the underlying
    population distribution that we are</st> <st c="109898">interested in</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="109911">How the normal or Gaussian distribution is a commonly occurring
    distribution because it arises from the CLT, which tells us what happens when
    we add lots of data values together – a common task in</st> <st c="110109">data
    science</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="110121">Randomness in data also flows through into any downstream calculation
    involving a dataset.</st> <st c="110213">Consequently, when constructing various
    data science algorithms, we need to take into account randomness within the data
    that those algorithms process.</st> <st c="110365">Unsurprisingly, many data science
    algorithms, such as</st> **<st c="110419">maximum likelihood estimation</st>**
    <st c="110448">(</st>**<st c="110450">MLE</st>**<st c="110453">) of model parameters
    and Bayesian probabilistic modeling, start by building from first principles upon
    the random nature of data.</st> <st c="110585">We will cover these concepts in</st>
    [*<st c="110617">Chapter 5</st>*](B19496_05.xhtml#_idTextAnchor261)<st c="110626">.
    We can do so only because we have laid the solid foundations in</st> <st c="110692">this
    chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="110705">Exercises</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="110715">Next is a series of short exercises.</st> <st c="110753">They
    start easy and increase in difficulty.</st> <st c="110797">Answers to all the
    exercises are given in the</st> `<st c="110843">Answers_to_Exercises_Chap2.ipynb</st>`
    <st c="110875">Jupyter notebook in the GitHub repository.</st> <st c="110919">The
    exercises are a mix of code exercises and mathematical derivation exercises –
    this is designed to get you to start flexing your mathematical muscles.</st> <st
    c="111073">Give them a go and</st> <st c="111092">have fun:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111101">Use the</st> `<st c="111110">numpy</st>` <st c="111115">package
    to sample 1,000 random values from a Beta distribution with</st> <st c="111184">α</st>
    <st c="111185">=</st> <st c="111186">2</st><st c="111187">,</st> <st c="111188">β</st>
    <st c="111189">=</st> <st c="111190">5</st><st c="111191">, and plot a histogram
    of the resulting</st> <st c="111231">1,000 values.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="111244">A mixture distribution is a distribution where the random variable
    has a certain specified probability of coming from one distribution, a certain
    specified probability of coming from a second distribution, a certain specified
    probability of coming from a third distribution, and so on.</st> <st c="111531">The
    number of different distributions is called the number of components in the mixture.</st>
    <st c="111620">We have a two-component mixture distribution, which we can write</st>
    <st c="111685">as follows:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="111696">A</st> <st c="111698">~</st> <st c="111699">Bernoulli</st><st
    c="111708">(</st><st c="111710">0.4</st><st c="111713">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111715">z</st><st c="111716">1</st> <st c="111717">~</st> <st c="111718">Normal</st><st
    c="111724">(</st><st c="111726">μ</st> <st c="111727">=</st> <st c="111728">1</st><st
    c="111729">,</st> <st c="111730">σ</st><st c="111731">2</st> <st c="111732">=</st>
    <st c="111733">3.5</st><st c="111736">)</st> <st c="111738">z</st><st c="111739">2</st>
    <st c="111740">~</st> <st c="111741">Laplace</st><st c="111748">(</st><st c="111750">μ</st>
    <st c="111751">=</st> <st c="111752">10</st><st c="111754">,</st> <st c="111755">λ</st>
    <st c="111756">=</st> <st c="111757">1.5</st><st c="111760">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111762">X</st> <st c="111763">=</st> <st c="111764">A</st> <st c="111765">×</st>
    <st c="111766">z</st><st c="111767">1</st> <st c="111768">+</st> <st c="111769">(</st><st
    c="111770">1</st> <st c="111771">−</st> <st c="111772">A</st><st c="111773">)</st>
    <st c="111774">×</st> <st c="111775">z</st><st c="111776">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111777">Eq.</st> <st c="111781">68</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111783">This means that with 40% probability</st> <st c="111821">X</st>
    <st c="111822">is drawn from a normal distribution with mean=1 and variance=3.5,
    and with 60% probability it is drawn from a Laplace distribution with mean=10</st>
    <st c="111967">and scale=1.5.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="111981">Using</st> `<st c="111988">numpy.random.randn</st>` <st c="112006">and</st>
    `<st c="112011">numpy.random.laplace</st>` <st c="112031">functions, draw 3,000
    values from this mixture distribution and plot a histogram of the</st> <st c="112120">resulting
    values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112137">In</st> *<st c="112141">Eq.</st> <st c="112145">14</st>*<st c="112147">,
    we have given a definition of the variance of a random variable</st> <st c="112213">X</st>
    <st c="112214">as follows:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="112225">Variance =</st> <st c="112237">𝔼</st><st c="112239">(</st><st
    c="112241">(</st><st c="112242">X</st><st c="112243">−</st> <st c="112244">𝔼</st><st
    c="112246">(</st><st c="112248">X</st><st c="112249">)</st><st c="112250">)</st><st
    c="112251">2</st><st c="112252">)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112253">Eq.</st> <st c="112257">69</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112259">An alternative way of writing this, which is sometimes useful
    computationally, is</st> <st c="112342">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112356">Variance =</st> <st c="112368">𝔼</st><st c="112370">(</st><st
    c="112372">X</st><st c="112373">2</st><st c="112374">)</st><st c="112375">−</st>
    <st c="112376">(</st><st c="112377">𝔼</st><st c="112379">(</st><st c="112381">X</st><st
    c="112382">)</st><st c="112383">)</st><st c="112384">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112385">Eq.</st> <st c="112389">70</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112391">Derive this second way of writing the variance</st> <st c="112439">of</st>
    <st c="112442">X</st><st c="112443">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="112444">Create a dataset of 30 values sampled from the distribution</st>
    <st c="112505">Normal</st><st c="112511">(</st><st c="112513">μ</st> <st c="112514">=</st>
    <st c="112515">2</st><st c="112516">,</st> <st c="112517">σ</st><st c="112518">2</st>
    <st c="112519">=</st> <st c="112520">1.5</st><st c="112523">)</st><st c="112525">.
    Using this data, create and plot kernel density estimates using a Parzen kernel,
    but with three different bandwidth values,</st> <st c="112651">h</st> <st c="112652">=</st>
    <st c="112653">0.1</st><st c="112656">,</st> <st c="112658">h</st> <st c="112659">=</st>
    <st c="112660">1.0</st><st c="112663">, and</st> <st c="112669">h</st> <st c="112670">=</st>
    <st c="112671">3.0</st><st c="112674">. Add the true probability density to the
    plot.</st> <st c="112722">What do you notice about the three different</st> <st
    c="112767">density estimates?</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="112785">If we have i.i.d.</st> <st c="112804">data values</st> <st c="112816">x</st><st
    c="112817">1</st><st c="112818">,</st> <st c="112819">x</st><st c="112820">2</st><st
    c="112821">,</st> <st c="112822">⋯</st> <st c="112823">,</st> <st c="112824">x</st><st
    c="112825">n</st> <st c="112826">with</st> <st c="112832">𝔼</st><st c="112834">(</st><st
    c="112836">x</st><st c="112837">i</st><st c="112838">)</st> <st c="112839">=</st>
    <st c="112840">μ</st> <st c="112841">and</st> <st c="112846">Var</st><st c="112849">(</st><st
    c="112851">x</st><st c="112852">i</st><st c="112853">)</st> <st c="112854">=</st>
    <st c="112855">σ</st><st c="112856">2</st><st c="112857">, prove that the expression
    in</st> *<st c="112888">Eq.</st> <st c="112892">48</st>* <st c="112894">for the
    sample variance</st> <st c="112919">s</st><st c="112920">2</st> <st c="112921">gives
    an unbiased estimator for the population variance</st> <st c="112977">σ</st><st
    c="112978">2</st><st c="112979">.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
