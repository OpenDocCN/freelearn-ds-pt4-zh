<html><head></head><body>
		<div id="_idContainer1581">
			<h1 id="_idParaDest-116" class="chapter-number"><a id="_idTextAnchor216"/><st c="0">4</st></h1>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor217"/><st c="2">Loss Functions and Optimization</st></h1>
			<p><st c="33">In </st><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="37">Chapter 2</st></em></span></a><st c="46"> and </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="51">Chapter 3</st></em></span></a><st c="60">, we focused on the two most important and core math concepts that are at the heart of virtually all of data science. </st><st c="178">In this chapter, we are going to move on to math concepts behind specific, but still very important, data science activities. </st><st c="304">Specifically, we are going to lay some of the groundwork for building </st><span class="No-Break"><st c="374">predictive models.</st></span></p>
			<p><st c="392">At the end of the last chapter, we hinted that one of the key concepts when building models is knowing or measuring how good a model is. </st><st c="530">When we train or fit a </st><strong class="bold"><st c="553">machine learning</st></strong><st c="569"> (</st><strong class="bold"><st c="571">ML</st></strong><st c="573">) model, we adjust the parameter values of the model so that it gives a “better” fit or explanation of the data. </st><st c="687">But this raises the question: What do we mean by “better”? </st><st c="746">Without an exact quantitative definition of what we mean when we say that one set of parameter values gives a better fit to the data than another, we cannot construct an objective and quantitative training process. </st><st c="961">This is where loss functions come in. </st><st c="999">They measure how well a model fits the training data. </st><st c="1053">This chapter goes into the details behind loss functions and their use in the training of models. </st><st c="1151">We do this by covering the </st><span class="No-Break"><st c="1178">following topics:</st></span></p>
			<ul>
				<li><em class="italic"><st c="1195">Loss functions – what are they?</st></em><st c="1227">: In this section, we learn the basics of loss functions and </st><span class="No-Break"><st c="1289">risk functions</st></span></li>
				<li><em class="italic"><st c="1303">Least squares</st></em><st c="1317"> (</st><em class="italic"><st c="1319">LS</st></em><st c="1321">): In this section, we learn at a high level about least squares minimization as a general technique for estimating </st><span class="No-Break"><st c="1438">model parameters</st></span></li>
				<li><em class="italic"><st c="1454">Linear models</st></em><st c="1468">: In this section, we learn how to use least squares minimization for fitting linear models via </st><strong class="bold"><st c="1565">ordinary least squares</st></strong><st c="1587"> (</st><span class="No-Break"><strong class="bold"><st c="1589">OLS</st></strong></span><span class="No-Break"><st c="1592">) regression</st></span></li>
				<li><em class="italic"><st c="1605">Gradient descent</st></em><st c="1622">: In this section, we learn a powerful and general technique for minimizing risk functions and </st><span class="No-Break"><st c="1718">objective functions</st></span></li>
			</ul>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor218"/><st c="1737">Technical requirements</st></h1>
			<p><st c="1760">All code examples given in this chapter (and additional examples) can be found at the GitHub repository, </st><a href="https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter04"><st c="1866">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter04</st></a><st c="1970">. To run the Jupyter notebooks, you will need a full Python installation including the </st><span class="No-Break"><st c="2057">following packages:</st></span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline"><st c="2076">pandas</st></strong></span><span class="No-Break"><st c="2083"> (&gt;=2.0.3)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2093">numpy</st></strong></span><span class="No-Break"><st c="2099"> (&gt;=1.24.3)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2110">scipy</st></strong></span><span class="No-Break"><st c="2116"> (&gt;=1.11.1)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2127">scikit-learn</st></strong></span><span class="No-Break"><st c="2140"> (&gt;=1.3.0)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2150">matplotlib</st></strong></span><span class="No-Break"><st c="2161"> (&gt;=3.7.2)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2171">statsmodels</st></strong></span><span class="No-Break"><st c="2183"> (&gt;=0.14.0)</st></span></li>
			</ul>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor219"/><st c="2194">Loss functions – what are they?</st></h1>
			<p><st c="2226">A </st><strong class="bold"><st c="2229">loss function</st></strong><st c="2242"> takes two inputs; for example, a model prediction and the corresponding ground-truth value. </st><st c="2335">It then compares the two inputs and summarizes this comparison into a </st><span class="No-Break"><st c="2405">single number.</st></span></p>
			<p><st c="2419">Let’s take that example further. </st><st c="2453">We’ll denote</st><a id="_idIndexMarker380"/><st c="2465"> the ground-truth value by </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.437em"/><st c="2492"/><st c="2515"> and the model prediction by </st><img src="image/1302.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.582em"/><st c="2543"/><st c="2544">. A loss function in this example would then be a function of both </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.436em"/><st c="2611"/><st c="2634"> and </st><img src="image/1302.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.582em"/><st c="2638"/><st c="2639">, which returns a single real number. </st><st c="2677">Let’s call that loss function </st><img src="image/1305.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.638em"/><st c="2707"/><st c="2708">. We’ll meet a concrete example of a loss function in the next section. </st><st c="2780">But for now, it suffices to say that a loss function </st><img src="image/1306.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.662em"/><st c="2833"/><st c="2834"> attempts to measure how similar </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.596em"/><st c="2867"/><st c="2868"> is to </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="2875"/><st c="2898">, with a loss function value of zero indicating that </st><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="2951">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="2952">y</st></span><span class="_-----MathTools-_Math_Variable"/><st c="2953"> is identical </st><span class="No-Break"><st c="2967">to </st></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><st c="2970">y</st></span></span><span class="No-Break"><st c="2971">.</st></span></p>
			<p><st c="2972">In general, a lower value of the function </st><img src="image/1306.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.657em"/><st c="3015"/><st c="3016"> means that </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.611em"/><st c="3028"/><st c="3029"> is closer or more similar to </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.448em"/><st c="3059"/><st c="3069">, while a higher value of the loss function means that </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.596em"/><st c="3124"/><st c="3125"> is further from or less similar </st><span class="No-Break"><st c="3158">to </st></span><span class="No-Break"><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="3161"/></span><span class="No-Break"><st c="3184">.</st></span></p>
			<p><st c="3185">When training a model, our training data will consist of lots of ground-truth values, so we will also have lots of predictions. </st><st c="3314">If our training set consists of </st><span class="_-----MathTools-_Math_Variable"><st c="3346">N</st></span><st c="3347"> datapoints, then we will have </st><img src="image/443.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.698em"/><st c="3378"/><st c="3379"> ground-truth values, </st><img src="image/1315.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.405em;height:0.853em;width:4.238em"/><st c="3401"/><st c="3411">. We can represent these by the vector </st><span class="_-----MathTools-_Math_Variable"><st c="3450">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="3451">_</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3452">=</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><st c="3453">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="3454">y</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="3455">1</st></span><span class="_-----MathTools-_Math_Operator"><st c="3456">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="3457">y</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="3458">2</st></span><span class="_-----MathTools-_Math_Operator"><st c="3459">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3460">…</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3461">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="3462">y</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3463">N</st></span><span class="_-----MathTools-_Math_Base"><st c="3464">)</st></span><st c="3465">. Similarly, we can represent the corresponding set of predictions by the vector </st><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="3546">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3547">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="3548">_</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3549">=</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><st c="3550">(</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="3551">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3552">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="3553">1</st></span><span class="_-----MathTools-_Math_Operator"><st c="3554">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="3555">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3556">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="3557">2</st></span><span class="_-----MathTools-_Math_Operator"><st c="3558">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3559">…</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="3560">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="3561">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3562">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="3563">N</st></span><span class="_-----MathTools-_Math_Base"><st c="3564">)</st></span><st c="3565">. Once we have chosen a particular form for the loss function </st><img src="image/1305.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.639em"/><st c="3627"/><st c="3628"> we can use it to measure</st><a id="_idIndexMarker381"/><st c="3653"> how close the whole set of predictions </st><img src="image/1317.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.631em"/><st c="3693"/><st c="3694"> are to their corresponding ground-truth values </st><img src="image/1318.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.458em"/><st c="3742"/><st c="3743"> by simply calculating the average loss over the entire set. </st><st c="3804">In other words, we calculate the </st><span class="No-Break"><st c="3837">following quantity:</st></span></p>
			<p><img src="image/1319.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.943em;height:2.570em;width:5.399em"/><st c="3856"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="3858">Eq. </st><st c="3862">1</st></p>
			<p><st c="3863">The value of the quantity in </st><em class="italic"><st c="3892">Eq. </st><st c="3896">1</st></em><st c="3897"> tells us how close our fitted model values are to the </st><span class="No-Break"><st c="3952">ground-truth values.</st></span></p>
			<p><st c="3972">When training our model, we adjust the model parameters so that the quantity in </st><em class="italic"><st c="4053">Eq. </st><st c="4057">1</st></em><st c="4058"> is minimized. </st><st c="4073">However, no model is perfect. </st><st c="4103">Even a suitably trained model that has not been overfitted to the training data will not have </st><img src="image/1320.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.617em"/><st c="4197"/><st c="4198"> identical to </st><img src="image/1321.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.467em"/><st c="4212"/><st c="4213">. Using </st><img src="image/1320.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.617em"/><st c="4221"/><st c="4222"> to represent </st><img src="image/1321.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.468em"/><st c="4236"/><st c="4237"> is an approximation. </st><st c="4259">It will be an imperfect approximation in the sense that there will be some loss of the information that was present in </st><img src="image/1324.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.457em"/><st c="4378"/><st c="4379">, hence the name “loss function.” A loss function enables us to measure how much loss we suffer when representing </st><img src="image/1325.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.462em"/><st c="4493"/><st c="4494"> by </st><img src="image/1326.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.605em"/><st c="4498"/><st c="4499">. Or rather, it attempts to quantify how much loss we suffer when we represent the true process that generates the real data </st><img src="image/1327.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.488em"/><st c="4624"/><st c="4625"> by our model, which produces the predictions </st><img src="image/1328.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.603em"/><st c="4671"/><st c="4672">. In this way, the loss function measures how well our model represents the true data generation process – it is a measure of how good our </st><span class="No-Break"><st c="4811">model is.</st></span></p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor220"/><st c="4820">Risk functions</st></h2>
			<p><st c="4835">The quantity in </st><em class="italic"><st c="4852">Eq. </st><st c="4856">1</st></em><st c="4857"> is an example of a </st><strong class="bold"><st c="4877">risk function</st></strong><st c="4890">. A risk function is the expectation value of a loss</st><a id="_idIndexMarker382"/><st c="4942"> function; that is, its </st><span class="No-Break"><st c="4966">average value.</st></span></p>
			<p><st c="4980">Why would we want to calculate the expectation of a loss function? </st><st c="5048">Let’s take a closer look at what a loss function is. </st><st c="5101">For our example, our loss function </st><img src="image/1329.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.576em"/><st c="5136"/><st c="5137"> is a function of data. </st><st c="5161">The model prediction value </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.605em"/><st c="5188"/><st c="5189"> is a function of the feature vector </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.508em"/><st c="5226"/><st c="5227"> that we input into the model, so </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.605em"/><st c="5261"/><st c="5262"> is a function of data. </st><st c="5286">Similarly, the ground-truth value </st><img src="image/209.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.433em"/><st c="5320"/><st c="5325"> is also data. </st><st c="5339">This makes the loss function value </st><img src="image/1334.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:3.701em"/><st c="5374"/><st c="5375"> a random variable – recall from </st><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="5408">Chapter 2</st></em></span></a><st c="5417"> that all data contains a random component, and anything derived from data contains a </st><span class="No-Break"><st c="5503">random component.</st></span></p>
			<p><st c="5520">As we learned in </st><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="5538">Chapter 2</st></em></span></a><st c="5547">, a random variable can take a range of values, and by taking the expectation value, we get a single, deterministic, quantity. </st><st c="5674">When constructing a measure of how good a model is or constructing a measure to minimize as part of a training process, a scalar deterministic quantity is always easier to work with than a random quantity. </st><st c="5880">The risk</st><a id="_idIndexMarker383"/><st c="5888"> function associated with </st><img src="image/1335.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:3.815em"/><st c="5914"/><st c="5915"> is defined as the expectation of </st><img src="image/1336.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:3.828em"/><st c="5949"/><st c="5950"> over the values of the feature vector </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.504em"/><st c="5989"/><st c="5990"> and the ground-truth value </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="6018"/><st c="6041">. So, the risk is calculated using the </st><span class="No-Break"><st c="6080">following formula:</st></span></p>
			<p><img src="image/1339.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.550em;height:1.381em;width:18.965em"/><st c="6098"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6143">Eq. </st><st c="6147">2</st></p>
			<p><st c="6148">Since we have integrated over all possible values of </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.501em"/><st c="6201"/><st c="6202"> and </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="6207"/><st c="6230">, the risk function defined by </st><em class="italic"><st c="6261">Eq. </st><st c="6265">2</st></em><st c="6266"> is now a function of the model parameters only. </st><st c="6315">It is now a function we can use to work out optimal model parameter values because it is deterministic – whenever we minimize the risk function in </st><em class="italic"><st c="6462">Eq. </st><st c="6466">2</st></em><st c="6467"> with respect to the model parameters we will always get the </st><span class="No-Break"><st c="6528">same answer.</st></span></p>
			<p><st c="6540">In practice, calculating the risk function in </st><em class="italic"><st c="6587">Eq. </st><st c="6591">2</st></em><st c="6592"> can be tricky, so instead, if we have a training dataset, we can approximate the expectation value in </st><em class="italic"><st c="6695">Eq. </st><st c="6699">2</st></em><st c="6700"> by the sample average of the loss function. </st><st c="6745">That is, we approximate </st><span class="No-Break"><st c="6769">the following:</st></span></p>
			<p><img src="image/1342.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;≃&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;/mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.943em;height:2.570em;width:8.386em"/><st c="6783"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6785">Eq. </st><st c="6789">3</st></p>
			<p><st c="6790">You’ll recognize the quantity in </st><em class="italic"><st c="6823">Eq. </st><st c="6827">3</st></em><st c="6828"> as being the same as that in </st><em class="italic"><st c="6858">Eq. </st><st c="6862">1</st></em><st c="6863">, which is why we said the quantity in </st><em class="italic"><st c="6902">Eq. </st><st c="6906">1</st></em><st c="6907"> was a risk function. </st><st c="6929">As we learned in </st><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="6946">Chapter 2</st></em></span></a><st c="6955">, we can think of a sample average as an expectation value calculated using the empirical density function. </st><st c="7063">In this case, the empirical density function is calculated as </st><span class="No-Break"><st c="7125">the following:</st></span></p>
			<p><img src="image/1343.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Empirical density function&lt;/mml:mtext&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;δ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;δ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.936em;height:2.566em;width:22.105em"/><st c="7139"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7191">Eq. </st><st c="7195">4</st></p>
			<p><st c="7196">We can use the empirical density function in </st><em class="italic"><st c="7241">Eq. </st><st c="7245">4</st></em><st c="7246"> to approximate the density function </st><img src="image/1344.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.775em;width:2.474em"/><st c="7283"/><st c="7292"> in </st><em class="italic"><st c="7295">Eq. </st><st c="7299">2</st></em><st c="7300">. Plugging it into </st><em class="italic"><st c="7319">Eq. </st><st c="7323">2</st></em><st c="7324">, we then get </st><em class="italic"><st c="7338">Eq. </st><st c="7342">3</st></em><st c="7343">. Because of this, we call the risk function defined by </st><em class="italic"><st c="7399">Eq. </st><st c="7403">3</st></em><st c="7404"> (and </st><em class="italic"><st c="7410">Eq. </st><st c="7414">1</st></em><st c="7415">) the </st><strong class="bold"><st c="7421">empirical risk function</st></strong><st c="7444">. Training a model by minimizing </st><a id="_idIndexMarker384"/><st c="7477">the quantity in </st><em class="italic"><st c="7493">Eq. </st><st c="7497">3</st></em><st c="7498"> with </st><a id="_idIndexMarker385"/><st c="7504">respect to the model parameters is called </st><strong class="bold"><st c="7546">empirical </st></strong><span class="No-Break"><strong class="bold"><st c="7556">risk minimization</st></strong></span><span class="No-Break"><st c="7573">.</st></span></p>
			<p><st c="7574">Note that the empirical risk function still has a dependence on data, because we have approximated the population</st><a id="_idIndexMarker386"/><st c="7688"> distribution </st><img src="image/1344.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.775em;width:2.465em"/><st c="7702"/><st c="7711"> using a finite sample of data. </st><st c="7742">The empirical risk function is a function of the entire training dataset, and so its value will be a random variable. </st><st c="7860">If the dataset is large, then the variance of the empirical risk function may be small enough that we can confidently ignore this variation, but clearly, in general, the model parameter values that result from minimizing the empirical risk function will be sensitive to (depend on) the precise details of the training </st><span class="No-Break"><st c="8178">set used.</st></span></p>
			<p><st c="8187">Finally, we should point out that because an empirical risk function is just a sum of loss function values, the terminology “risk function” and “loss function” are often used interchangeably in a loose fashion. </st><st c="8399">So, sometimes, you may hear someone speak of a loss function when they are referring to a risk function or vice versa. </st><st c="8518">However, the intent is usually clear – they are referring to a function that is to be minimized in order to find good values for the </st><span class="No-Break"><st c="8651">model parameters.</st></span></p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor221"/><st c="8668">There are many loss functions</st></h2>
			<p><st c="8698">So far, we have been vague about the details of our loss function. </st><st c="8766">We have referred to it </st><span class="No-Break"><st c="8789">simply as</st></span>
<span class="_-----MathTools-_Math_Text"><st c="8798">L</st></span><span class="_-----MathTools-_Math_Base"><st c="8800">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="8801">y</st></span><span class="_-----MathTools-_Math_Operator"><st c="8802">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="8803">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="8804">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"><st c="8805">)</st></span><st c="8806">, but we haven’t given an </st><a id="_idIndexMarker387"/><st c="8832">actual formula for our loss function. </st><st c="8870">This is deliberate because there are many potential choices of loss function formula, and so we wanted to keep the explanation of how loss functions are used very general to encompass all these potentially different loss </st><span class="No-Break"><st c="9091">function formulas.</st></span></p>
			<p><st c="9109">One of the simplest loss function formula for </st><img src="image/1346.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.685em"/><st c="9156"/><st c="9157"> is to take the difference between </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.453em"/><st c="9192"/><st c="9202"> and </st><img src="image/1348.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.623em"/><st c="9206"/><st c="9207"> and  square it, to ensure a positive quantity. </st><st c="9254">That is, we use the </st><span class="No-Break"><st c="9274">following formula:</st></span></p>
			<p><img src="image/1349.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:1.043em;width:6.869em"/><st c="9292"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="9294">Eq. </st><st c="9298">5</st></p>
			<p><st c="9299">The loss function</st><a id="_idIndexMarker388"/><st c="9316"> formula in </st><em class="italic"><st c="9328">Eq. </st><st c="9332">5</st></em><st c="9333"> is called the </st><strong class="bold"><st c="9348">squared loss</st></strong><st c="9360">. It is a very simple formula. </st><st c="9391">Because of that, it is an extremely widely used loss function, with some convenient properties. </st><st c="9487">Consequently, in the next section, we go into more detail about this </st><span class="No-Break"><st c="9556">loss function.</st></span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor222"/><st c="9570">Different loss functions = different end results</st></h2>
			<p><st c="9619">The squared loss is just one way of combining </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.442em"/><st c="9666"/><st c="9689"> and </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.598em"/><st c="9693"/><st c="9694"> into a positive number. </st><st c="9719">We could just as well have chosen to use the formula </st><img src="image/1352.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.316em;height:1.149em;width:6.769em"/><st c="9772"/><st c="9773">, which is called the </st><strong class="bold"><st c="9795">absolute loss</st></strong><st c="9808">. Different </st><a id="_idIndexMarker389"/><st c="9820">choices of loss function will lead to different results. </st><st c="9877">If we used an absolute-loss loss function in the empirical risk function in </st><em class="italic"><st c="9953">Eq. </st><st c="9957">3</st></em><st c="9958"> when doing our model training, we would end up with different model parameter estimates compared to if we had used a squared-loss function. </st><st c="10099">How the parameter estimates would differ depends on the different properties of the two loss functions. </st><st c="10203">This highlights </st><span class="No-Break"><st c="10219">the following:</st></span></p>
			<ul>
				<li><st c="10233">The properties of the parameter estimates depend upon the properties of the loss function. </st><st c="10325">Some of these properties are advantageous, and so often, we will choose a particular loss function precisely because we want the parameter estimates to have </st><span class="No-Break"><st c="10482">certain properties.</st></span></li>
				<li><st c="10501">The end results of training a model depend not only on the choice of model and choice of training data but also the whole training process, so also on such things as the choice of loss function; that is, how we choose to measure how good a model is. </st><st c="10752">The end results can also depend on the algorithm we choose to minimize the empirical risk once we make our choice of </st><span class="No-Break"><st c="10869">loss function.</st></span></li>
			</ul>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor223"/><st c="10883">Loss functions for anything</st></h2>
			<p><st c="10911">Up till now, we have been talking about loss functions </st><img src="image/1353.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.043em;width:2.621em"/><st c="10967"/><st c="10968"> that measure the difference between a ground-truth value </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.440em"/><st c="11026"/><st c="11049"> and the </st><a id="_idIndexMarker390"/><st c="11057">corresponding model prediction </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.593em"/><st c="11088"/><st c="11089">. But at the very start of this section, we just said that a loss function took in two inputs and compared them. </st><st c="11202">From this, you’re probably beginning to realize that we can use loss functions to compare more than just predictions and ground-truth values. </st><st c="11344">For example, we might want to measure the difference between our estimates of some model parameters and what should be the true model parameter values. </st><st c="11496">If we denote a true model parameter value by </st><img src="image/1356.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.518em"/><st c="11541"/><st c="11542"> and our estimate of it by </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.717em"/><st c="11569"/><st c="11572">, then we could measure how close </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.717em"/><st c="11606"/><st c="11609"> is to </st><img src="image/1356.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.519em"/><st c="11615"/><st c="11616"> using the squared loss, </st><span class="No-Break"><st c="11641">as follows:</st></span></p>
			<p><span class="_-----MathTools-_Math_Text"><st c="11652">Similarity between </st></span><span class="_-----MathTools-_Math_Variable"><st c="11672">β</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Text"><st c="11673">and </st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="11677">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="11678">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="11679">=</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><st c="11680">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="11681">β</st></span><span class="_-----MathTools-_Math_Operator"><st c="11682">−</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="11683">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="11684">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"><st c="11685">)</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="11686">2</st></span></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11687">Eq. </st><st c="11691">6</st></p>
			<p><st c="11692">In the preceding example, we have effectively defined a loss function </st><img src="image/1360.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;L&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.280em;height:1.282em;width:3.402em"/><st c="11762"/><st c="11770"> that takes model parameter values as input. </st><st c="11814">But it is still just comparing two real numbers. </st><st c="11863">Again, we highlight the fact that at the start of this section, we said that a loss function just takes two inputs. </st><st c="11979">There is no reason why those inputs must always be two real numbers. </st><st c="12048">There are, in fact, many situations where we want to compare other mathematical objects. </st><st c="12137">For </st><a id="_idIndexMarker391"/><st c="12141">example, we may want to compare two continuous probability distributions, particularly if the model we are building is not a model of the ground-truth values </st><span class="_-----MathTools-_Math_Variable"><st c="12299">y</st></span><st c="12300">, but is a model of a </st><span class="No-Break"><st c="12322">probability distribution.</st></span></p>
			<p><st c="12347">One of the most common functions for comparing two probability distributions is the </st><strong class="bold"><st c="12432">Kullback-Leibler</st></strong><st c="12448"> (</st><strong class="bold"><st c="12450">KL</st></strong><st c="12452">) divergence. </st><st c="12467">If the two</st><a id="_idIndexMarker392"/><st c="12477"> continuous probability distributions we wish to compare have probability density functions </st><img src="image/1361.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.680em"/><st c="12569"/><st c="12574"> and </st><img src="image/1362.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.678em"/><st c="12578"/><st c="12583">, and they are distributions of a thing denoted by </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.502em"/><st c="12634"/><st c="12635">, then the KL divergence is defined </st><span class="No-Break"><st c="12671">as follows:</st></span></p>
			<p><img src="image/1364.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;KL&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;∥&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.624em;height:1.860em;width:13.077em"/><st c="12682"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12712">Eq. </st><st c="12716">7</st></p>
			<p><st c="12717">As the KL divergence is the average of the logarithmic difference between </st><img src="image/1365.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.651em"/><st c="12791"/><st c="12796"> and </st><img src="image/1366.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.648em"/><st c="12800"/><st c="12805">, we can think of the KL divergence as a </st><span class="No-Break"><st c="12846">risk function.</st></span></p>
			<p><st c="12860">We haven’t been specific about what the thing </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.513em"/><st c="12907"/><st c="12908"> represents, so you’ll realize that the KL divergence can be used for measuring the similarity of distributions of many exotic mathematical objects – vectors, networks, matrices, and so on. </st><st c="13098">We won’t go into any more details. </st><st c="13133">We’ll meet the KL divergence again in </st><a href="B19496_13.xhtml#_idTextAnchor646"><span class="No-Break"><em class="italic"><st c="13171">Chapter 13</st></em></span></a><st c="13181">, but for now, we’ll leave it by just saying that the KL divergence can be used to measure the expected loss that occurs when we use </st><img src="image/1362.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.677em"/><st c="13314"/><st c="13319"> to </st><span class="No-Break"><st c="13322">approximate </st></span><span class="No-Break"><img src="image/1361.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.805em;width:1.679em"/><st c="13334"/></span><span class="No-Break"><st c="13339">.</st></span></p>
			<p><st c="13340">Hopefully, by now, you’ll have realized that the concept of a loss function is a very general one. </st><st c="13440">A loss function measures the similarity between two mathematical objects. </st><st c="13514">We can do this for many different types of mathematical objects. </st><st c="13579">Even for a given type of mathematical object, there are many different potential choices of formula for the loss function we use, with each different formula leading to subtle differences and nuances in the final results when we use the loss function for, say, </st><span class="No-Break"><st c="13840">model training.</st></span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor224"/><st c="13855">A loss function by any other name</st></h2>
			<p><st c="13889">Since the concept of a loss function is a very general one and they just compare two mathematical objects, it is not surprising that such functions occur in many different branches of science and mathematics and consequently have different names for the same or related concepts. </st><st c="14170">Therefore, you </st><a id="_idIndexMarker393"/><st c="14185">may also see the following concepts and </st><span class="No-Break"><st c="14225">terminology used:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="14242">Cost function</st></strong><st c="14256">: Since our model predictions </st><img src="image/1370.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.619em"/><st c="14287"/><st c="14288"> are an imperfect representation of </st><img src="image/1371.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.468em"/><st c="14324"/><st c="14325">, there may be some consequences or</st><a id="_idIndexMarker394"/><st c="14360"> costs to that imprecision. </st><st c="14388">For example, in a business setting, using </st><img src="image/1372.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.074em;width:0.613em"/><st c="14430"/><st c="14431"> as an imperfect prediction of </st><img src="image/1327.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:0.801em;width:0.475em"/><st c="14462"/><st c="14463"> could result in lost revenue or overstocking of inventory, and so has a real cost to the business. </st><st c="14563">Consequently, a loss function is also sometimes called a </st><em class="italic"><st c="14620">cost function</st></em><st c="14633">. Clearly, we would want to choose the model parameter values to minimize this cost as much as possible, so we also talk of minimizing a </st><span class="No-Break"><st c="14770">cost function.</st></span></li>
				<li><strong class="bold"><st c="14784">Objective function</st></strong><st c="14803">: When minimizing a </st><a id="_idIndexMarker395"/><st c="14824">risk function with respect to our model’s parameters, minimizing the risk function is the aim or </st><em class="italic"><st c="14921">objective</st></em><st c="14930"> of the whole exercise. </st><st c="14954">Consequently, the risk function is also referred to as the </st><em class="italic"><st c="15013">objective function</st></em><st c="15031">. This terminology is particularly common in the field of mathematical optimization, which studies methods and general algorithms for </st><span class="No-Break"><st c="15165">optimizing functions.</st></span></li>
			</ul>
			<p><st c="15186">That note on the widespread use of loss functions across different mathematical fields is a good place to stop for now and recap what we have learned in </st><span class="No-Break"><st c="15340">this section.</st></span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor225"/><st c="15353">What we learned</st></h2>
			<p><st c="15369">In this section, we have learned </st><span class="No-Break"><st c="15403">the following:</st></span></p>
			<ul>
				<li><st c="15417">How a loss function measures the loss incurred when we approximate one mathematical object </st><span class="No-Break"><st c="15509">by another</st></span></li>
				<li><st c="15519">How a risk function is constructed as the expectation value of a </st><span class="No-Break"><st c="15585">loss function</st></span></li>
				<li><st c="15598">How to calculate the empirical risk function from a loss function and </st><span class="No-Break"><st c="15669">a dataset</st></span></li>
				<li><st c="15678">How the optimal parameter values of a model can be estimated by minimizing the empirical </st><span class="No-Break"><st c="15768">risk function</st></span></li>
				<li><st c="15781">How the choice of loss function changes the properties of the model parameter estimates obtained through the empirical risk </st><span class="No-Break"><st c="15906">minimization process</st></span></li>
			</ul>
			<p><st c="15926">Having learned about loss functions in general, in the next section, we are going to focus on one loss function in particular – the squared-loss function. </st><st c="16082">This is because the squared-loss function is so ubiquitous. </st><st c="16142">As we will see in further sections, it is one of the common data science methods for estimating </st><span class="No-Break"><st c="16238">model parameters.</st></span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor226"/><st c="16255">Least Squares</st></h1>
			<p><st c="16269">Least squares or least squares regression is probably a term you’ve heard before. </st><st c="16352">Why is that so? </st><st c="16368">It is because it is an extremely versatile but simple technique. </st><st c="16433">These</st><a id="_idIndexMarker396"/><st c="16438"> characteristics of least squares stem from the properties of the squared-loss function. </st><st c="16527">So to start we’ll delve into the squared-loss function in a bit </st><span class="No-Break"><st c="16591">more detail.</st></span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor227"/><st c="16603">The squared-loss function</st></h2>
			<p><st c="16629">The squared-loss function in </st><em class="italic"><st c="16659">Eq. </st><st c="16663">5</st></em><st c="16664"> is a function </st><a id="_idIndexMarker397"/><st c="16679">of the difference </st><img src="image/1374.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:2.039em"/><st c="16697"/><st c="16698">, and so we </st><a id="_idIndexMarker398"/><st c="16710">can write the squared loss in a slightly </st><span class="No-Break"><st c="16751">simpler fo</st><a id="_idTextAnchor228"/><st c="16761">rm:</st></span></p>
			<p><img src="image/1375.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mtext&gt;L&lt;/mtext&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mtext&gt;with&lt;/mtext&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.547em;height:1.332em;width:18.316em"/><st c="16765"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="16801">Eq. </st><st c="16805">8</st></p>
			<p><st c="16806">The form of the function </st><img src="image/1376.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.547em;height:1.258em;width:1.886em"/><st c="16831"/><st c="16832"> is show</st><a id="_idTextAnchor229"/><st c="16840">n in </st><span class="No-Break"><em class="italic"><st c="16846">Figure 4</st></em></span><span class="No-Break"><em class="italic"><st c="16854">.1</st></em></span><span class="No-Break"><st c="16856">:</st></span></p>
			<div>
				<div id="_idContainer1429" class="IMG---Figure">
					<img src="image/B19496_04_01.jpg" alt="Figure 4.1: The shape of the squared-loss function"/><st c="16858"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="16880">Figure 4.1: The shape of the squared-loss function</st></p>
			<p><st c="16930">For the squared loss, the empirical risk function can be written </st><span class="No-Break"><st c="16996">as follows:</st></span></p>
			<p><img src="image/1377.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.943em;height:2.570em;width:13.326em"/><st c="17007"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17009">Eq. </st><st c="17013">9</st></p>
			<p><st c="17014">The model prediction, </st><img src="image/1378.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.128em;width:0.832em"/><st c="17036"/><st c="17037">, obviously depends upon the model parameters, which we’ll denote by the vector </st><img src="image/1379.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.539em"/><st c="17117"/><st c="17118">, and the </st><a id="_idIndexMarker399"/><st c="17128">vector of feature values, </st><img src="image/1380.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.737em"/><st c="17154"/><st c="17158">, for which we are making the prediction. </st><st c="17200">So, we denote our model as </st><img src="image/1381.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.399em;height:1.341em;width:3.246em"/><st c="17227"/><st c="17233">. The vertical bar in that mathematical expression means “given,” so we can read this mathematical</st><a id="_idIndexMarker400"/><st c="17331"> expression as the value of </st><img src="image/1307.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.979em;width:0.597em"/><st c="17359"/><st c="17360"> evaluated at </st><img src="image/1331.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.096em;height:0.543em;width:0.502em"/><st c="17374"/><st c="17375"> and given the model parameter values </st><img src="image/1379.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.539em"/><st c="17413"/><st c="17414">. The specific model prediction </st><img src="image/1378.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.128em;width:0.832em"/><st c="17446"/><st c="17447"> is obtained by plugging the feature vector </st><img src="image/1380.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.737em"/><st c="17491"/><st c="17495"> into this expression for our model, so </st><img src="image/1387.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.424em;height:1.388em;width:5.568em"/><st c="17534"/><st c="17535">. If we determine </st><img src="image/1388.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.550em"/><st c="17553"/><st c="17554"> by minimizing the empirical risk function with respect to </st><img src="image/1379.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.539em"/><st c="17613"/><st c="17614">, this is equivalent to minimizing </st><span class="No-Break"><st c="17649">the followi</st><a id="_idTextAnchor230"/><st c="17660">ng:</st></span></p>
			<p><img src="image/1390.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.943em;height:2.570em;width:7.586em"/><st c="17664"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17666">Eq. </st><st c="17670">10</st></p>
			<p><st c="17672">We have dropped the pre-factor of </st><img src="image/1391.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.193em;height:0.992em;width:0.548em"/><st c="17707"/><st c="17708"> in </st><em class="italic"><st c="17712">Eq. </st><st c="17716">10</st></em><st c="17718"> because it is a constant and therefore makes no difference to the value of </st><img src="image/1392.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.541em"/><st c="17794"/><st c="17795"> thatminimizes </st><em class="italic"><st c="17810">Eq. </st><st c="17814">10</st></em><st c="17816">. The difference </st><img src="image/1393.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.128em;width:2.540em"/><st c="17833"/><st c="17834"> is the </st><img src="image/1394.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.826em;width:0.835em"/><st c="17842"/> <strong class="bold"><st c="17846">residual</st></strong><st c="17854"> of the model, and so the</st><a id="_idIndexMarker401"/><st c="17879"> quantity in </st><em class="italic"><st c="17892">Eq. </st><st c="17896">10</st></em><st c="17898"> is the </st><strong class="bold"><st c="17906">sum-of-squared-residuals</st></strong><st c="17930">, which is often shortened to </st><strong class="bold"><st c="17960">sum-of-squares</st></strong><st c="17974">. When we determine the model parameters </st><img src="image/1395.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.570em"/><st c="18015"/><st c="18016"> by minimizing the sum-of-squares in </st><em class="italic"><st c="18053">Eq. </st><st c="18057">10</st></em><st c="18059">, we are adjusting </st><img src="image/1388.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.556em"/><st c="18078"/><st c="18079"> until the sum-of-squares reaches its least possible value. </st><st c="18139">Hence this technique for determining a model’s</st><a id="_idIndexMarker402"/><st c="18185"> parameters is known as </st><strong class="bold"><st c="18209">least squares</st></strong><st c="18222"> or </st><strong class="bold"><st c="18226">least </st></strong><span class="No-Break"><strong class="bold"><st c="18232">squares minimization</st></strong></span><span class="No-Break"><st c="18252">.</st></span></p>
			<p><st c="18253">We have said very little about</st><a id="_idIndexMarker403"/><st c="18284"> the mathematical form of our model </st><img src="image/1397.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.399em;height:1.341em;width:3.213em"/><st c="18320"/><st c="18321">. This is because we haven’t needed to. </st><st c="18361">This makes least squares a very general technique for estimating the parameters of a model. </st><st c="18453">We </st><a id="_idIndexMarker404"/><st c="18456">can apply the idea to very many different types of models and very many different situations. </st><st c="18550">We already encountered least squares minimization in disguise when we minimized the dissimilarity between</st><a id="_idIndexMarker405"/><st c="18655"> two matrices in </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="18672">Chapter 3</st></em></span></a><st c="18681"> when we introduced </st><strong class="bold"><st c="18701">Non-negative Matrix </st></strong><span class="No-Break"><strong class="bold"><st c="18721">Factorization</st></strong></span><span class="No-Break"><st c="18734"> (</st></span><span class="No-Break"><strong class="bold"><st c="18736">NMF</st></strong></span><span class="No-Break"><st c="18739">).</st></span></p>
			<p><st c="18742">The idea of least squares minimization is a very intuitive one – simply construct a mathematical expression for your model predictions, which depends on the model parameters, use it to calculate the residuals </st><img src="image/1398.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.128em;width:2.401em"/><st c="18952"/><st c="18953"> , then minimize the sum of the squared residuals. </st><st c="19003">However, least squares minimization is a heuristic idea, meaning that, at the moment, we have not provided a formal or rigorous justification of why we should minimize the sum-of-squared residuals to determine the model parameters. </st><st c="19235">Why, for example, do we square the residuals and not raise them to the fourth power instead? </st><st c="19328">We have given no proof that squaring the residuals is the best choice we can make to turn the residual into a positive quantity. </st><st c="19457">We will provide a formal justification of least squares minimization when we introduce probabilistic models in </st><span class="No-Break"><em class="italic"><st c="19568">Chapter 5</st></em></span><st c="19577">, but for now, we will stick with the heuristic viewpoint – it is a very general, powerful, and extremely useful technique. </st><st c="19701">So, let’s dive into least squares minimization in a bit </st><span class="No-Break"><st c="19757">more detail.</st></span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor231"/><st c="19769">OLS regression</st></h2>
			<p><st c="19784">The scatter plot in</st><a id="_idIndexMarker406"/><st c="19804"> the left-hand plot of </st><span class="No-Break"><em class="italic"><st c="19827">Figure 4</st></em></span><em class="italic"><st c="19835">.2</st></em><st c="19837"> shows some example data that we would like to build a model of. </st><st c="19902">The </st><a id="_idIndexMarker407"/><st c="19906">scatter plot suggests a linear relationship, so we’ll use a linear model to capture the relationship between the </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.478em"/><st c="20019"/><st c="20020"> and </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.447em"/><st c="20025"/><st c="20035"> values. </st><st c="20043">For this 1D data (we have only one feature, </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.478em"/><st c="20087"/><st c="20088">), a linear model is of the </st><span class="No-Break"><st c="20116">following form:</st></span></p>
			<p><img src="image/1402.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.128em;width:4.660em"/><st c="20131"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="20141">Eq. </st><st c="20145">11</st></p>
			<p><st c="20147">The intercept </st><img src="image/1403.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:0.777em"/><st c="20162"/><st c="20163"> and gradient </st><img src="image/1404.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.400em;height:1.111em;width:0.777em"/><st c="20177"/><st c="20178"> are the parameters of our model, so our parameter vector </st><span class="_-----MathTools-_Math_Variable"><st c="20236">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="20237">_</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="20238">=</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><st c="20239">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="20240">β</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="20241">0</st></span><span class="_-----MathTools-_Math_Operator"><st c="20242">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="20243">β</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="20244">1</st></span><span class="_-----MathTools-_Math_Base"><st c="20245">)</st></span><st c="20246">. An</st><a id="_idIndexMarker408"/><st c="20250"> example model is shown on the left-hand side of </st><span class="No-Break"><em class="italic"><st c="20299">Figure 4</st></em></span><em class="italic"><st c="20307">.2</st></em><st c="20309"> by the solid red line. </st><st c="20333">In fact, this line is the optimal or least squares ch</st><a id="_idTextAnchor232"/><st c="20386">oice </st><span class="No-Break"><st c="20392">for </st></span><span class="No-Break"><img src="image/1379.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.539em"/><st c="20396"/></span><span class="No-Break"><st c="20397">:</st></span></p>
			<div>
				<div id="_idContainer1459" class="IMG---Figure">
					<img src="image/B19496_04_02.jpg" alt="Figure 4.2: least squares model optimization for a simple linear model"/><st c="20398"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="20454">Figure 4.2: least squares model optimization for a simple linear model</st></p>
			<p><st c="20524">On the right-hand side of </st><span class="No-Break"><em class="italic"><st c="20551">Figure 4</st></em></span><em class="italic"><st c="20559">.2</st></em><st c="20561">, we have reproduced the scatter plot and least squares optimal model line, but we have also highlighted, with vertical blue line segments, the residuals of each of the datapoints. </st><st c="20742">A residual line segment above the red line indicates a positive residual, while a residual line segment below the red line indicates a negative residual. </st><st c="20896">We can see that there is a mix of positive and negative residuals. </st><st c="20963">At any given value of </st><img src="image/1406.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.459em"/><st c="20985"/><st c="20986"> the red line is passing approximately through the middle of the </st><img src="image/769.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.463em"/><st c="21051"/><st c="21061"> values that are located at that value of </st><img src="image/169.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.495em"/><st c="21102"/><st c="21103">. In other words, the red line, or the least squares model, is attempting the estimate the mean value of </st><img src="image/24.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.440em"/><st c="21208"/><st c="21231"> given the value of </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.471em"/><st c="21250"/><st c="21251">. Because we are estimating parameter values </st><span class="_-----MathTools-_Math_Variable"><st c="21296">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><img src="image/1411.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.510em"/><st c="21297"/></span><span class="_-----MathTools-_Math_Variable"> </span><st c="21298">that make our linear model predict the mean of </st><img src="image/24.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="21345"/><st c="21368"> given </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="21374"/><st c="21375">, the overall estimation process is called regression. </st><st c="21430">And because we are using least squares to estimate the optimal values of </st><span class="_-----MathTools-_Math_Variable"><st c="21503">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><img src="image/1411.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.504em"/><st c="21504"/></span><st c="21505">, we call the overall process </st><strong class="bold"><st c="21535">least squares regression</st></strong><st c="21559">. Furthermore, because we are</st><a id="_idIndexMarker409"/><st c="21588"> using the vertical residuals we call this </st><strong class="bold"><st c="21631">ordinary least squares </st></strong><st c="21654">(</st><span class="No-Break"><strong class="bold"><st c="21655">OLS</st></strong></span><span class="No-Break"><st c="21658">)</st></span><span class="No-Break"><strong class="bold"><st c="21660"> regression</st></strong></span><span class="No-Break"><st c="21671">.</st></span></p>
			<p><st c="21672">You may be wondering what is</st><a id="_idIndexMarker410"/><st c="21701"> ordinary about OLS regression. </st><st c="21733">This refers to the fact that we are using least squares regression in its most common setting – fitting a linear model and using the vertical residuals. </st><st c="21886">There are other types of least squares regression that you may encounter. </st><st c="21960">For example, if we have a non-linear model but still use the vertical residuals, this is unsurprisingly </st><a id="_idIndexMarker411"/><st c="22064">called </st><strong class="bold"><st c="22071">non-linear least squares regression</st></strong><st c="22106"> (</st><strong class="bold"><st c="22108">NLS</st></strong><st c="22111">). </st><st c="22115">Alternatively, if we still want to model a linear relationship but want the relationship to capture how the </st><span class="_-----MathTools-_Math_Variable"><st c="22223">x</st></span><st c="22224"> and </st><span class="_-----MathTools-_Math_Variable"><st c="22229">y</st></span><st c="22230"> values</st><a id="_idIndexMarker412"/><st c="22237"> vary together, then the squares of the orthogonal distances from the model line to the datapoints, rather than the vertical distances, are a better way to measure the loss. </st><st c="22411">This is </st><a id="_idIndexMarker413"/><st c="22419">called </st><strong class="bold"><st c="22426">total least squares</st></strong><st c="22445"> (</st><strong class="bold"><st c="22447">TLS</st></strong><st c="22450">) regression. </st><st c="22465">We already encountered TLS regression in disguise when we learned about </st><strong class="bold"><st c="22537">principal component analysis</st></strong><st c="22565"> (</st><strong class="bold"><st c="22567">PCA</st></strong><st c="22570">) in </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="22576">Chapter 3</st></em></span></a><st c="22585">, when we</st><a id="_idIndexMarker414"/><st c="22594"> chose our principal components to minimize the variance lost by approximating a full dataset through </st><span class="No-Break"><st c="22696">dimensionality reduction.</st></span></p>
			<p><st c="22721">For now, we’re going to focus only on OLS regression. </st><st c="22776">It sounds like a great data science technique to have in our toolkit, right? </st><st c="22853">It is, but that doesn’t mean it doesn’t have its weaknesses. </st><st c="22914">We’ll explore one of its main </st><span class="No-Break"><st c="22944">weaknesses next.</st></span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor233"/><st c="22960">OLS, outliers, and robust regression</st></h2>
			<p><st c="22997">The scatter plot in </st><span class="No-Break"><em class="italic"><st c="23018">Figure 4</st></em></span><em class="italic"><st c="23026">.3</st></em><st c="23028"> shows the influence of outliers on OLS regression. </st><st c="23080">The dataset clearly has two outlier values (with high </st><img src="image/24.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.436em"/><st c="23134"/><st c="23157"> values) toward the right-hand end of the scatter plot. </st><st c="23212">The solid red line shows the OLS model </st><a id="_idIndexMarker415"/><st c="23251">when we use all 31 datapoints, while the red </st><a id="_idIndexMarker416"/><st c="23296">dashed line shows the OLS model when we exclude the two </st><span class="No-Break"><st c="23352">outlier points.</st></span></p>
			<p><st c="23367">Including the outlier points in the OLS regression has clearly pulled the regression line upward despite the number of outliers being small. </st><st c="23509">One look at the shape of the loss function in </st><span class="No-Break"><em class="italic"><st c="23555">Figure 4</st></em></span><em class="italic"><st c="23563">.1</st></em><st c="23565"> explains why this is so. </st><st c="23591">The quadratic shape of the loss function means that outliers – that is, points with large residuals – contribute significantly more to the sum-of-squared residuals value. </st><st c="23762">A residual, </st><img src="image/1416.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.011em;height:0.459em;width:0.365em"/><st c="23774"/><st c="23775">, of size 1.0 contributes a value of 1.0 when we plug it into the squared-loss function </st><img src="image/1417.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.547em;height:1.258em;width:1.743em"/><st c="23863"/><st c="23864">. However, a residual of size </st><img src="image/1418.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2.0&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.646em;width:3.122em"/><st c="23894"/><st c="23895"> contributes a value of 4.0 when we plug it into </st><img src="image/1419.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.547em;height:1.258em;width:1.808em"/><st c="23944"/><st c="23945">. Since OLS regression works by minimizing the sum-of-squared residuals, the OLS algorithm is going to pay disproportionately more attention to the outlier contributions when adjusting the m</st><a id="_idTextAnchor234"/><st c="24135">odel </st><span class="No-Break"><st c="24141">parameters</st></span><span class="No-Break"> </span><span class="No-Break"><img src="image/1379.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.539em"/><st c="24151"/></span><span class="No-Break"><st c="24153">.</st></span></p>
			<div>
				<div id="_idContainer1475" class="IMG---Figure">
					<img src="image/B19496_04_03.jpg" alt="Figure 4.3: The effect of outliers on OLS regression"/><st c="24154"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="24186">Figure 4.3: The effect of outliers on OLS regression</st></p>
			<p><st c="24238">This tells us that OLS is sensitive to the</st><a id="_idIndexMarker417"/><st c="24281"> effect of outliers. </st><st c="24302">Including the outliers in the OLS regression in </st><span class="No-Break"><em class="italic"><st c="24350">Figure 4</st></em></span><em class="italic"><st c="24358">.3</st></em><st c="24360"> has led to a </st><a id="_idIndexMarker418"/><st c="24374">model that is a poor fit for most of the data in the scatterplot, and importantly, it has led to a model that will predict poorly for new datapoints. </st><st c="24524">Can we rectify this? </st><st c="24545">Yes, we can. </st><st c="24558">One way to do so would be to modify the shape of our loss function so that it wasn’t quadratic at large residual values. </st><st c="24679">Such a loss function is shown by the solid black line in </st><span class="No-Break"><em class="italic"><st c="24736">Figure 4</st></em></span><em class="italic"><st c="24744">.4</st></em><st c="24746">. For comparison, we have also plotted in </st><span class="No-Break"><em class="italic"><st c="24788">Figure 4</st></em></span><em class="italic"><st c="24796">.4</st></em><st c="24798"> the squared-loss function </st><a id="_idTextAnchor235"/><st c="24825">of </st><em class="italic"><st c="24828">Eq. </st><st c="24832">8</st></em><st c="24833">, as the dashed </st><span class="No-Break"><st c="24849">red line:</st></span></p>
			<div>
				<div id="_idContainer1476" class="IMG---Figure">
					<img src="image/B19496_04_04.jpg" alt="Figure 4.4: The shape of the pseudo-Huber robust loss function"/><st c="24858"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="24949">Figure 4.4: The shape of the pseudo-Huber robust loss function</st></p>
			<p><st c="25011">This particular loss </st><a id="_idIndexMarker419"/><st c="25033">function is known as a pseudo-Huber loss function, and its mathematical for</st><a id="_idTextAnchor236"/><st c="25108">m is </st><span class="No-Break"><st c="25114">the following:</st></span></p>
			<p><img src="image/1421.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.337em;width:8.255em"/><st c="25128"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25144">Eq. </st><st c="25148">12</st></p>
			<p><st c="25150">At large values of </st><span class="_-----MathTools-_Math_Variable"><st c="25170">x</st></span><st c="25171">, this loss function is </st><a id="_idIndexMarker420"/><st c="25195">approximately linear. </st><st c="25217">The contrast to the squared-loss function at large values of </st><span class="_-----MathTools-_Math_Variable"><st c="25278">x</st></span><st c="25279"> is marked. </st><st c="25291">Using the pseudo-Huber loss function in </st><span class="No-Break"><em class="italic"><st c="25331">Figure 4</st></em></span><em class="italic"><st c="25339">.4</st></em><st c="25341"> would mean</st><a id="_idIndexMarker421"/><st c="25352"> that outlier values would still contribute to the empirical risk function, but not disproportionately so. </st><st c="25459">The regression algorithm would then be robust to the presence of outliers in the dataset, and importantly, we would produce a model that is more accurate in its predictions on new values </st><span class="No-Break"><st c="25646">of </st></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><st c="25649">x</st></span></span><span class="No-Break"><st c="25650">.</st></span></p>
			<p><st c="25651">Not surprisingly, loss functions of the shape shown in </st><span class="No-Break"><em class="italic"><st c="25707">Figure 4</st></em></span><em class="italic"><st c="25715">.4</st></em><st c="25717"> are studied as a part of statistics known as </st><strong class="bold"><st c="25763">robust statistics</st></strong><st c="25780">. A detailed explanation of robust statistics techniques is beyond the scope of this book. </st><st c="25871">However, the</st><a id="_idIndexMarker422"/><st c="25883"> very fact that robust regression techniques</st><a id="_idIndexMarker423"/><st c="25927"> are available to us may make you ask why we still use and study OLS regression. </st><st c="26008">The answer lies in something we haven’t yet spoken about – for OLS, how do we actually do the minimization of the empirical risk function? </st><st c="26147">What are the details of the algorithm we use? </st><st c="26193">For OLS regression, the combination of a linear model with a squared-loss function leads to an extremely efficient solution to the empirical risk minimization problem. </st><st c="26361">It is this solution we will cover in the next section, but for now, let’s summarize what we have learned about the squared-loss function and </st><span class="No-Break"><st c="26502">least squares.</st></span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor237"/><st c="26516">What we learned</st></h2>
			<p><st c="26532">In this section, we have learned about </st><span class="No-Break"><st c="26572">the following:</st></span></p>
			<ul>
				<li><st c="26586">The squared-loss function and its </st><span class="No-Break"><st c="26621">mathematical shape</st></span></li>
				<li><st c="26639">Least squares minimization as a general heuristic technique for estimating optimal model </st><span class="No-Break"><st c="26729">parameter values</st></span></li>
				<li><st c="26745">OLS regression as a technique for estimating the parameters of </st><span class="No-Break"><st c="26809">linear models</st></span></li>
				<li><st c="26822">The sensitivity of OLS regression </st><span class="No-Break"><st c="26857">to outliers</st></span></li>
				<li><st c="26868">Robust loss functions and how they can mitigate the sensitivity of OLS regression </st><span class="No-Break"><st c="26951">to outliers</st></span></li>
			</ul>
			<p><st c="26962">Having learned about the general ideas and principles of OLS regression, we will delve into the mathematical detail behind OLS in the next section. </st><st c="27111">This will be useful because i) OLS is one of the workhorse algorithms of data science, ii) it will help to highlight some ideas about the optimization of objective functions in general that we will want to make use of </st><span class="No-Break"><st c="27329">later on.</st></span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor238"/><st c="27338">Linear models</st></h1>
			<p><st c="27352">We’ve already introduced, at a high level, the idea of OLS regression for a linear model. </st><st c="27443">But this particular combination of squared loss for measuring the risk and a linear model for </st><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="27537">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="27538">y</st></span><span class="_-----MathTools-_Math_Variable"/><st c="27539"> has some very convenient</st><a id="_idIndexMarker424"/><st c="27564"> and simple-to-use properties. </st><st c="27595">This simplicity means that OLS regression is one of the most widely used and studied data science modeling techniques. </st><st c="27714">That is why we are going to look in detail at fitting linear models to data using </st><span class="No-Break"><st c="27796">OLS regression.</st></span></p>
			<p><st c="27811">To start with, we’ll revisit the squared-loss empirical risk function in </st><em class="italic"><st c="27885">Eq. </st><st c="27889">10</st></em><st c="27891"> and look at what happens to it when we have a linear model </st><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="27951">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="27952">y</st></span><span class="_-----MathTools-_Math_Variable"/><st c="27953">. To recap, the squared-loss empirical risk is</st><a id="_idTextAnchor239"/><st c="27999"> given by </st><span class="No-Break"><st c="28009">the following:</st></span></p>
			<p><img src="image/1422.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mtext&gt;Risk&lt;/mml:mtext&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.943em;height:2.570em;width:8.890em"/><st c="28023"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28025">Eq. </st><st c="28029">13</st></p>
			<p><st c="28031">Now, for a linear model with </st><img src="image/596.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.723em;width:0.507em"/><st c="28061"/><st c="28062"> features, </st><img src="image/1424.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:4.322em"/><st c="28073"/><st c="28086">, we can wri</st><a id="_idTextAnchor240"/><st c="28098">te the model </st><span class="No-Break"><st c="28112">as follows:</st></span></p>
			<p><img src="image/1425.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.128em;width:12.172em"/><st c="28123"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28142">Eq. </st><st c="28146">14</st></p>
			<p><st c="28148">The vector of model parameters is </st><img src="image/1426.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.169em;width:7.650em"/><st c="28183"/><st c="28196">. We can write the features in vector form as well. </st><st c="28248">We’ll write</st><a id="_idIndexMarker425"/><st c="28259"> it as a row-vector, </st><img src="image/1427.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.116em;width:8.326em"/><st c="28280"/><st c="28299">. Doing so allows us to write </st><em class="italic"><st c="28329">Eq. </st><st c="28333">14</st></em><st c="28335"> in the </st><span class="No-Break"><st c="28343">following form:</st></span></p>
			<p><img src="image/1428.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.349em;height:1.070em;width:2.968em"/><st c="28358"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28360">Eq. </st><st c="28364">15</st></p>
			<p><st c="28366">We can think of the extra 1 in the feature vector </st><img src="image/1429.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.116em;width:7.899em"/><st c="28417"/><st c="28436"> as being a feature value </st><img src="image/1430.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.774em"/><st c="28461"/><st c="28462"> that multiplies the intercept </st><img src="image/1431.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:0.791em"/><st c="28493"/><st c="28494"> in the linear model in Eq. </st><st c="28522">14. </st><st c="28526">For the </st><img src="image/1394.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.826em;width:0.848em"/><st c="28534"/><st c="28538"> datapoint the feature values can be written in vector form, </st><img src="image/1433.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.897em;width:9.142em"/><st c="28598"/><st c="28599">, with obviously </st><img src="image/1434.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.041em;width:2.976em"/><st c="28616"/><st c="28617"> for all </st><img src="image/102.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.651em;width:0.277em"/><st c="28626"/><st c="28627">. We can combine all the feature vectors </st><img src="image/1380.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.737em"/><st c="28668"/><st c="28672"> from all the datapoints into a </st><span class="No-Break"><st c="28703">data matrix:</st></span></p>
			<p><img src="image/1437.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em 0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto auto auto&quot; columnalign=&quot;center center center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;20&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mtable columnspacing=&quot;0.8000em 0.8000em 0.8000em 0.8000em&quot; columnwidth=&quot;auto auto auto auto auto&quot; columnalign=&quot;center center center center center&quot; rowspacing=&quot;1.0000ex 1.0000ex 1.0000ex&quot; rowalign=&quot;baseline baseline baseline baseline&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;11&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;21&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;22&lt;/mn&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋱&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;⋮&lt;/mo&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-2.708em;height:6.018em;width:24.835em"/><st c="28715"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28724">Eq. </st><st c="28728">16</st></p>
			<p><st c="28730">If we also put all the observed values, </st><img src="image/1438.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.619em"/><st c="28771"/><st c="28772">, into a vector </st><img src="image/1439.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.405em;height:1.166em;width:7.508em"/><st c="28788"/><st c="28807">, then we can write the risk function in </st><em class="italic"><st c="28848">Eq. </st><st c="28852">13</st></em><st c="28854"> in a very </st><a id="_idTextAnchor241"/><st c="28865">succinct form </st><span class="No-Break"><st c="28879">as follows:</st></span></p>
			<p><img src="image/1440.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.530em;height:1.621em;width:11.922em"/><st c="28890"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28892">Eq. </st><st c="28896">17</st></p>
			<p><st c="28898">The data matrix </st><span class="_-----MathTools-_Math_Variable"><st c="28915">X</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="28916">_</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="28917">_</st></span><st c="28918"> is also called the </st><strong class="bold"><st c="28938">design matrix</st></strong><st c="28951">. This terminology originates from statistics, where often the datapoints</st><a id="_idIndexMarker426"/><st c="29024"> and, hence, feature values were part of a scientific experiment to quantify the various influences on the response variable </st><span class="_-----MathTools-_Math_Variable"><st c="29149">y</st></span><st c="29150">. Being part of a scientific experiment, the feature values </st><span class="_-----MathTools-_Math_Variable"><st c="29210">x</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="29211">ij</st></span><st c="29213"> were planned in advance; that </st><span class="No-Break"><st c="29244">is, </st></span><span class="No-Break"><em class="italic"><st c="29248">designed</st></em></span><span class="No-Break"><st c="29256">.</st></span></p>
			<p><st c="29257">The optimal values of the model </st><a id="_idIndexMarker427"/><st c="29290">parameters </st><span class="_-----MathTools-_Math_Variable"><st c="29301">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="29302">_</st></span><st c="29303"> are obtained by minimizing the right-hand side of </st><em class="italic"><st c="29354">Eq. </st><st c="29358">17</st></em><st c="29360"> with respect to </st><span class="_-----MathTools-_Math_Variable"><st c="29377">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="29378">_</st></span><st c="29379">. We’ll denote the optimal values of </st><span class="_-----MathTools-_Math_Variable"><st c="29416">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="29417">_</st></span><st c="29418"> by the symbol </st><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="29433">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="29434">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="29435">_</st></span><st c="29436">. We can use the differential calculus we recapped in </st><a href="B19496_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><st c="29490">Chapter 1</st></em></span></a><st c="29499"> to do the minimization. </st><st c="29524">Differentiating the right-hand side of </st><em class="italic"><st c="29563">Eq. </st><st c="29567">17</st></em><st c="29569"> with respect to </st><span class="_-----MathTools-_Math_Variable"><st c="29586">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="29587">_</st></span><st c="29588"> and setting the derivatives to z</st><a id="_idTextAnchor242"/><st c="29621">ero gives us </st><span class="No-Break"><st c="29635">the following:</st></span></p>
			<p><img src="image/1441.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.427em;height:2.591em;width:13.859em"/><st c="29649"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="29651">Eq. </st><st c="29655">18</st></p>
			<p><st c="29657">Re-arranging </st><em class="italic"><st c="29671">Eq</st><a id="_idTextAnchor243"/><st c="29673">. 18</st></em><st c="29677">, we get </st><span class="No-Break"><st c="29686">the following:</st></span></p>
			<p><img src="image/1442.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.353em;height:1.355em;width:5.546em"/><st c="29700"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="29713">Eq. </st><st c="29717">19</st></p>
			<p><st c="29719">We can solve </st><em class="italic"><st c="29733">Eq. </st><st c="29737">19</st></em><st c="29739"> by applying </st><img src="image/1443.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.214em;height:0.983em;width:2.840em"/><st c="29752"/><st c="29753">to both the left- and right-hand sides of </st><em class="italic"><st c="29795">E</st><a id="_idTextAnchor244"/><st c="29796">q. </st><st c="29799">19</st></em><st c="29801"> to get </st><span class="No-Break"><st c="29809">the following:</st></span></p>
			<p><img src="image/1444.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;msup&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.353em;height:1.355em;width:6.807em"/><st c="29823"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="29825">Eq. </st><st c="29829">20</st></p>
			<p><st c="29831">This solution is very efficient. </st><st c="29865">It is in a closed-form, meaning we have an equation with the thing we want, </st><img src="image/1445.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.743em"/><st c="29941"/><st c="29942">, on its own on the left-hand side, and a mathematical expression that doesn’t involve </st><img src="image/1445.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.744em"/><st c="30029"/><st c="30030"> on the right-hand side. </st><st c="30055">There is no iterative algorithm required. </st><st c="30097">We just perform a couple of matrix calculations, and we have our optimal parameter estimates </st><img src="image/1447.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.815em"/><st c="30190"/><st c="30195">. That we can obtain a closed-form expression for the parameter estimates is one of the most attractive aspects of OLS regression and part of the reason it is so widely used. </st><st c="30370">We’ll walk through some code examples in a moment to illustrate how easy it is to perform </st><span class="No-Break"><st c="30460">OLS regression.</st></span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor245"/><st c="30475">Practical issues</st></h2>
			<p><st c="30492">This doesn’t mean the closed-form expression in </st><em class="italic"><st c="30541">Eq. </st><st c="30545">20</st></em><st c="30547"> doesn’t cause problems. </st><st c="30572">Firstly, you’ll recall from </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="30600">Chapter 3</st></em></span></a><st c="30609"> on linear </st><a id="_idIndexMarker428"/><st c="30620">algebra that we can have square matrices that do not have an inverse. </st><st c="30690">It is very possible that the matrix </st><img src="image/1448.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.214em;height:0.983em;width:3.159em"/><st c="30726"/><st c="30727">does not exist. </st><st c="30743">This happens when there are linear dependencies between the columns of the design matrix </st><img src="image/1449.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.214em;height:0.862em;width:0.733em"/><st c="30832"/><st c="30833">; for example, if one feature is simply a scaled version of another feature, or where combining several features together gives the same numerical value as another feature. </st><st c="31006">In these circumstances, one or more of the features are redundant since they add no </st><span class="No-Break"><st c="31090">new information.</st></span></p>
			<p><st c="31106">Secondly, in a modern-day data science setting where we might have many thousands of features in a model, the </st><img src="image/1450.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.723em;width:2.095em"/><st c="31217"/><st c="31218"> matrix </st><img src="image/1451.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.214em;height:0.976em;width:1.858em"/><st c="31226"/><st c="31230"> can be unwieldy to work with if </st><span class="_-----MathTools-_Math_Variable"><st c="31262">d</st></span><st c="31263"> is of the order of </st><span class="No-Break"><st c="31283">several thousand.</st></span></p>
			<p><st c="31300">How to deal with these computational issues is beyond the scope of the book, but they are something you should be aware of in case they crop up in a problem you are trying </st><span class="No-Break"><st c="31473">to solve.</st></span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor246"/><st c="31482">The model residuals</st></h2>
			<p><st c="31502">Once we have obtained an estimate </st><img src="image/1445.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.750em"/><st c="31537"/><st c="31538"> for the </st><a id="_idIndexMarker429"/><st c="31547">model parameters, using </st><em class="italic"><st c="31571">Eq. </st><st c="31575">20</st></em><st c="31577">, we can calculate the residuals. </st><st c="31611">If we denote the </st><img src="image/1394.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.826em;width:0.832em"/><st c="31628"/><st c="31632"> residual by </st><img src="image/1454.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:0.855em;width:0.542em"/><st c="31644"/><st c="31645">, then obviously we have </st><span class="No-Break"><st c="31670">the following:</st></span></p>
			<p><img src="image/1455.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.671em;height:1.673em;width:13.272em"/><st c="31684"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="31695">Eq. </st><st c="31699">21</st></p>
			<p><st c="31701">What happens if we sum up all the residuals? </st><st c="31747">To answer this question, we make use of </st><em class="italic"><st c="31787">Eq. </st><st c="31791">19</st></em><st c="31793"> and recall that the first row of </st><img src="image/1456.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.214em;height:0.976em;width:1.152em"/><st c="31827"/><st c="31830">is all ones if our model has an intercept. </st><st c="31873">So, </st><em class="italic"><st c="31877">Eq. </st><st c="31881">19</st></em><st c="31883"> tells us </st><span class="No-Break"><st c="31893">the following:</st></span></p>
			<p><img src="image/1457.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfenced open=&quot;&quot; close=&quot;&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt;⇒&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.943em;height:2.570em;width:18.204em"/><st c="31907"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="31909">Eq. </st><st c="31913">22</st></p>
			<p><st c="31915">So, the sum of all the residuals is zero if our model has </st><span class="No-Break"><st c="31974">an intercept.</st></span></p>
			<p><st c="31987">Let’s see these ideas in</st><a id="_idIndexMarker430"/><st c="32012"> action with a </st><span class="No-Break"><st c="32027">code example.</st></span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor247"/><st c="32040">OLS regression code example</st></h2>
			<p><st c="32068">The data in the </st><strong class="source-inline"><st c="32085">Data/power_plant_output.csv</st></strong><st c="32112"> file in the GitHub repository contains measurements of the power output from electricity generation plants. </st><st c="32221">The power (</st><strong class="source-inline"><st c="32232">PE</st></strong><st c="32235">) is generated from a combination of gas turbines, steam turbines, and heat recovery steam generators, and </st><a id="_idIndexMarker431"/><st c="32343">so is affected by environmental factors in which the turbines operate, such as the ambient temperature (</st><strong class="source-inline"><st c="32447">AT</st></strong><st c="32450">) and the steam turbine exhaust vacuum level (</st><strong class="source-inline"><st c="32497">V</st></strong><st c="32499">). </st><st c="32502">The dataset consists of 9,568 observations of the </st><strong class="source-inline"><st c="32552">PE</st></strong><st c="32554">, </st><strong class="source-inline"><st c="32556">AT</st></strong><st c="32558">, and </st><strong class="source-inline"><st c="32564">V</st></strong><st c="32565"> values. </st><st c="32574">The data is a subset of the publicly available dataset held in the </st><em class="italic"><st c="32641">UCI Machine Learning Repository </st></em><st c="32673">(</st><a href="https://archive.ics.uci.edu/datasets"><st c="32674">https://archive.ics.uci.edu/datasets</st></a><st c="32710">). </st><st c="32714">The original data can be found </st><span class="No-Break"><st c="32745">at </st></span><a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"><span class="No-Break"><st c="32748">https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant</st></span></a><span class="No-Break"><st c="32814">.</st></span></p>
			<p><st c="32815">We’ll use the data to build a linear model of the power output </st><strong class="source-inline"><st c="32879">PE</st></strong><st c="32881"> as a function of the </st><strong class="source-inline"><st c="32903">AT</st></strong><st c="32905"> and </st><strong class="source-inline"><st c="32910">V</st></strong><st c="32911"> values. </st><st c="32920">We will build the linear model in two ways – i) using the Python </st><strong class="source-inline"><st c="32985">statsmodels</st></strong><st c="32996"> package, ii) using </st><em class="italic"><st c="33016">Eq. </st><st c="33020">20</st></em><st c="33022"> via an explicit calculation. </st><st c="33052">The following code example can be found in the </st><strong class="source-inline"><st c="33099">Code_Examples_Chap4.ipynb</st></strong><st c="33124"> notebook in the GitHub repository. </st><st c="33160">To begin, we need to read in </st><span class="No-Break"><st c="33189">the data:</st></span></p>
			<pre class="source-code"><st c="33198">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
# Read in the raw data
df = pd.read_csv("../Data/power_plant_output.csv")</st></pre>			<p><st c="33381">We’ll do a quick inspection of the data. </st><st c="33423">First, we’ll compute some summary statistics of </st><span class="No-Break"><st c="33471">the data:</st></span></p>
			<pre class="source-code"><st c="33480">
# Use pd.describe() to get the summary statistics of the data
df.describe()</st></pre>			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33556">AT</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="33559">V</st></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33561">PE</st></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33563">Count</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33569">9568.000000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33581">9568.000000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33593">9568.000000</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33605">Mean</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33610">19.651231</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33620">54.305804</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33630">454.365009</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33641">Std</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33645">7.452473</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33654">12.707893</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33664">17.066995</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33674">Min</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33678">1.810000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33687">25.360000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33697">420.260000</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33708">25%</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33712">13.510000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33722">41.740000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33732">439.750000</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33743">50%</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33747">20.345000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33757">52.080000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33767">451.550000</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33778">75%</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33782">25.720000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33792">66.540000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33802">468.430000</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33813">Max</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33817">37.110000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33827">81.560000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="33837">495.760000</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="33848">Table 4.1: Summary statistics for the power-plant dataset</st></p>
			<p><st c="33906">Next, we’ll visualize the relationship</st><a id="_idIndexMarker432"/><st c="33945"> between the response variable (the target variable) and the features. </st><st c="34016">We’ll start with the relationship between power output (</st><strong class="source-inline"><st c="34072">PE</st></strong><st c="34075">) and ambient </st><span class="No-Break"><st c="34090">temperature (</st></span><span class="No-Break"><strong class="source-inline"><st c="34103">AT</st></strong></span><span class="No-Break"><st c="34106">):</st></span></p>
			<pre class="source-code"><st c="34109">
# Scatterplot between the response variable PE and the AT feature.
</st><st c="34177"># The linear relationship is clear.
</st><st c="34213">plt.scatter(df.AT, df.PE)
plt.title('PE vs AT', fontsize=24)
plt.xlabel('AT', fontsize=20)
plt.ylabel('PE', fontsize=20)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.show()</st></pre>			<div>
				<div id="_idContainer1514" class="IMG---Figure">
					<img src="image/B19496_04_05.jpg" alt="Figure 4.5: Plot of power output (PE) versus ambient temperature (AT)"/><st c="34392"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="34423">Figure 4.5: Plot of power output (PE) versus ambient temperature (AT)</st></p>
			<p><st c="34492">Now, let’s look at the relationship</st><a id="_idIndexMarker433"/><st c="34528"> between power and </st><span class="No-Break"><st c="34547">vacuum (</st></span><span class="No-Break"><strong class="source-inline"><st c="34555">V</st></strong></span><span class="No-Break"><st c="34557">):</st></span></p>
			<pre class="source-code"><st c="34559">
# Scatterplot between the response variable PE and the V feature.
</st><st c="34626"># The linear relationship is clear, but not as strong as the 
# relationship with the AT feature.
</st><st c="34723">plt.scatter(df.V, df.PE)
plt.title('PE vs V', fontsize=24)
plt.xlabel('V', fontsize=20)
plt.ylabel('PE', fontsize=20)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.show()</st></pre>			<div>
				<div id="_idContainer1515" class="IMG---Figure">
					<img src="image/B19496_04_06.jpg" alt="Figure 4.6: Plot of power output (PE) versus vacuum level (V)"/><st c="34899"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="34942">Figure 4.6: Plot of power output (PE) versus vacuum level (V)</st></p>
			<p><st c="35003">Now, we’ll fit a linear model using the </st><strong class="source-inline"><st c="35044">statsmodels</st></strong><st c="35055"> package. </st><st c="35065">The linear model formula is specified in statistical notation as </st><span class="_-----MathTools-_Math_Text"><st c="35130">PE</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended"><st c="35132">∼</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text"><st c="35134">AT + V</st></span><st c="35140">. You can think of it as the statistical formula equivalent</st><a id="_idIndexMarker434"/><st c="35199"> of the mathematical formula </st><span class="_-----MathTools-_Math_Text"><st c="35228">PE = </st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="35233">β</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="35234">0</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="35235">+</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="35236">β</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="35237">AT</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="35239">x</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="35241">AT</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="35243">+</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="35245">β</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="35246">V</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><st c="35247">x</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="35248">V</st></span><st c="35249">. We do this fitting using the </st><span class="No-Break"><st c="35280">following code:</st></span></p>
			<pre class="source-code"><st c="35295">
# First we specify the model using statsmodels.formula.api.ols
model = smf.ols(formula='PE ~ AT + V', data=df)
# Now we fit the model to the data, i.e. </st><st c="35448">we minimize the sum-of-
# squared residuals with respect to the model parameters
model_result = model.fit()
# Now we'll look at a summary of the fitted OLS model
model_result.summary()</st></pre>			<p><st c="35632">This gives the following parameter estimates for our </st><span class="No-Break"><st c="35686">linear model:</st></span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="35699">OLS </st><span class="No-Break"><st c="35704">Regression Results</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<table id="table003-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35722">coef</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35727">std err</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35735">T</st></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35737">P&gt;|t|</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35742">[</st><span class="No-Break"><st c="35744">0.025</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35749">0.975]</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35756">Intercept</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35766">505.4774</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35775">0.240</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35781">2101.855</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35790">0.000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35796">505.006</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35804">505.949</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35812">AT</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35815">-</st><span class="No-Break"><st c="35817">1.7043</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35823">0.013</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35829">-</st><span class="No-Break"><st c="35831">134.429</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35838">0.000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35844">-</st><span class="No-Break"><st c="35846">1.729</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35851">-</st><span class="No-Break"><st c="35853">1.679</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="35858">V</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35860">-</st><span class="No-Break"><st c="35861">0.3245</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35867">0.007</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35873">-</st><span class="No-Break"><st c="35875">43.644</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="35881">0.000</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35887">-</st><span class="No-Break"><st c="35889">0.339</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="35894">-</st><span class="No-Break"><st c="35896">0.310</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor248"/><st c="35901">Table 4.2: OLS regression parameter estimates for our power-plant linear model</st></p>
			<p><st c="35980">We can see from </st><em class="italic"><st c="35997">Table 4.2</st></em><st c="36006"> that, as</st><a id="_idIndexMarker435"/><st c="36015"> expected, we get negative estimates for the parameters corresponding to the </st><strong class="source-inline"><st c="36092">AT</st></strong><st c="36094"> and </st><strong class="source-inline"><st c="36099">V</st></strong><st c="36100"> features. </st><st c="36111">Now, we’ll repeat the calculation explicitly using the formula in </st><em class="italic"><st c="36177">Eq. </st><st c="36181">20</st></em><st c="36183">. We’ll use the linear algebra functions available to us in </st><strong class="source-inline"><st c="36243">numpy</st></strong><st c="36248">. First, we need to extract the data from the </st><strong class="source-inline"><st c="36294">pandas</st></strong><st c="36300"> DataFrame to appropriate </st><span class="No-Break"><strong class="source-inline"><st c="36326">numpy</st></strong></span><span class="No-Break"><st c="36331"> arrays:</st></span></p>
			<pre class="source-code"><st c="36339">
# We extract the design matrix as a 2D numpy array. 
</st><st c="36392"># This initially corresponds to the feature columns of the dataframe.
</st><st c="36462"># In this case it is all but the last column
X = df.iloc[:, 0:(df.shape[1]-1)].to_numpy()
# Now we'll add a column of ones to the design matrix.
</st><st c="36607"># This is the feature that corresponds to the intercept parameter 
# in the moddel
X = np.c_[np.ones(X.shape[0]), X]
# For convenience, we'll create and store the transpose of the 
# design matrix
xT = np.transpose(X)
# Now we'll extract the response vector to a numpy array
y = df.iloc[:, df.shape[1]-1].to_numpy()</st></pre>			<p><st c="36920">Now, we can calculate the OLS parameter estimates using the </st><span class="No-Break"><st c="36981">formula </st></span><span class="No-Break"><img src="image/1444.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.353em;height:1.355em;width:6.807em"/><st c="36989"/></span><span class="No-Break"><st c="36990">::</st></span></p>
			<pre class="source-code"><st c="36992">
# Calculate the inverse of xTx using the numpy linear algebra 
# functions
xTx_inv = np.linalg.inv(np.matmul(xT, X))
# Finally calculate the OLS model parameter estimates using the 
# formula (xTx_inv)*(xT*y).
</st><st c="37201"># Again, we use the numpy linear algebra functions to do this
ols_params = np.matmul(xTx_inv, np.matmul(xT, y))</st></pre>			<p><st c="37312">We can compare the OLS parameter </st><a id="_idIndexMarker436"/><st c="37346">estimates obtained from </st><strong class="source-inline"><st c="37370">statsmodels</st></strong><st c="37381"> with those obtained from the </st><span class="No-Break"><st c="37411">explicit calculation:</st></span></p>
			<pre class="source-code"><st c="37432">
# Now compare the parameter estimates from the explicit calculation 
# with those obtained from the statsmodels fit
df_compare = pd.DataFrame({'statsmodels': model_result.params, 
                           'explicit_ols':ols_params})
df_compare</st></pre>			<table id="table004-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37649">statsmodels</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37661">explicit_ols</st></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37674">Intercept</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37684">505.477434</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37695">505.477434</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="37706">AT</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="37709">-</st><span class="No-Break"><st c="37711">1.704266</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="37719">-</st><span class="No-Break"><st c="37721">1.704266</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="37729">V</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="37731">-</st><span class="No-Break"><st c="37732">0.324487</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="37740">-</st><span class="No-Break"><st c="37742">0.324487</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37750">Table 4.3: A comparison of the parameter estimates from the statsmodels packages and explicit calculation using the OLS formula</st></p>
			<p><st c="37878">The parameter estimates from the two different OLS regression codes are identical to more than 6 </st><span class="No-Break"><st c="37976">decimal places.</st></span></p>
			<p><st c="37991">This walkthrough of a real example highlights the power of the closed-form OLS regression formula in </st><em class="italic"><st c="38093">Eq. </st><st c="38097">20</st></em><st c="38099">. This closed-form arises from the linear (in </st><img src="image/1392.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.545em"/><st c="38145"/><st c="38146">) nature of the optimality criterion in </st><em class="italic"><st c="38186">Eq. </st><st c="38190">18</st></em><st c="38192">, which itself arises from the quadratic nature of the risk function in </st><em class="italic"><st c="38264">Eq. </st><st c="38268">17</st></em><st c="38270">, which ultimately is a consequence of the quadratic form of the squared-loss function in </st><span class="No-Break"><em class="italic"><st c="38360">Eq. </st><st c="38364">8</st></em></span><span class="No-Break"><st c="38365">.</st></span></p>
			<p><st c="38366">But what if we don’t want to use</st><a id="_idIndexMarker437"/><st c="38399"> a linear model or a squared-loss function? </st><st c="38443">Firstly, we can’t use OLS regression! </st><st c="38481">Secondly, a different choice of loss function, such as the absolute loss or the pseudo-Huber robust loss function in </st><em class="italic"><st c="38598">Eq. </st><st c="38602">12</st></em><st c="38604">, will not lead to a closed-form solution for </st><img src="image/1460.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.714em"/><st c="38650"/><st c="38654"> if we minimize the empirical risk in </st><em class="italic"><st c="38691">Eq. </st><st c="38695">3</st></em><st c="38696">. So, how do we minimize the empirical risk to obtain optimal model parameter estimates in these situations? </st><st c="38805">We’ll learn how to address this question in the next section, but for now, let’s review what we have learned in </st><span class="No-Break"><st c="38917">this section.</st></span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor249"/><st c="38930">What we learned</st></h2>
			<p><st c="38946">In this section, we have learned </st><span class="No-Break"><st c="38980">the following:</st></span></p>
			<ul>
				<li><st c="38994">How to write the empirical risk for OLS regression in </st><span class="No-Break"><st c="39049">matrix notation</st></span></li>
				<li><st c="39064">How to derive a closed-form expression for OLS model </st><span class="No-Break"><st c="39118">parameter estimates</st></span></li>
				<li><st c="39137">Some of the properties and practical limitations of </st><span class="No-Break"><st c="39190">OLS regression</st></span></li>
				<li><st c="39204">How to perform OLS regression using available Python packages such </st><span class="No-Break"><st c="39272">as </st></span><span class="No-Break"><strong class="source-inline"><st c="39275">statsmodels</st></strong></span></li>
				<li><st c="39286">How to perform OLS regression by explicitly calculating the closed-form formula for OLS model </st><span class="No-Break"><st c="39381">parameter estimates</st></span></li>
			</ul>
			<p><st c="39400">Having learned how to perform OLS regression, we’ll now learn how to perform least squares regression in more general settings by using gradient descent techniques to minimize the </st><span class="No-Break"><st c="39581">empirical risk.</st></span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor250"/><st c="39596">Gradient descent</st></h1>
			<p><st c="39613">As we just hinted at the end of the last section, we aren’t always in a position where we can use the closed-form OLS solution of </st><em class="italic"><st c="39744">Eq. </st><st c="39748">20</st></em><st c="39750">. What are </st><a id="_idIndexMarker438"/><st c="39761">our options? </st><st c="39774">To construct a more general approach to empirical risk minimization, we’ll have to revisit the shape of the empirical risk function so that we can understand how to locate </st><span class="No-Break"><st c="39946">its minima.</st></span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor251"/><st c="39957">Locating the minimum of a simple risk function</st></h2>
			<p><st c="40004">To understand the shape of the empirical risk function, let’s take a simple example with a model that has a single parameter. </st><st c="40131">We’ll use the risk</st><a id="_idIndexMarker439"/><st c="40149"> function for a linear model and a squared-loss function. </st><st c="40207">We’ll use a linear model with a single feature, and so it is of the </st><span class="No-Break"><st c="40275">following form:</st></span></p>
			<p><img src="image/1461.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.979em;width:2.660em"/><st c="40290"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="40292">Eq. </st><st c="40296">23</st></p>
			<p><st c="40298">The model has a single parameter, </st><span class="_-----MathTools-_Math_Variable"><st c="40333">β</st></span><st c="40334">, which multiplies the single feature </st><span class="_-----MathTools-_Math_Variable"><st c="40372">x</st></span><st c="40373">. In </st><span class="No-Break"><em class="italic"><st c="40378">Figure 4</st></em></span><em class="italic"><st c="40386">.7</st></em><st c="40388"> we have plotted the shape of the empirical risk function against the value of </st><span class="_-----MathTools-_Math_Variable"><st c="40467">β</st></span><st c="40468">, and where we have calculated the empirical risk on a dataset of 100 datapoints. </st><st c="40550">The dataset has been generated via simulation and using a model of the mathematical form in </st><em class="italic"><st c="40642">Eq. </st><st c="40646">23</st></em><st c="40648">. The model we are going to fit to this data by minimizing the empirical risk is of the correct mathematical form (by construction); it is just that we don’t know the true value of </st><span class="_-----MathTools-_Math_Variable"><st c="40829">β</st></span><st c="40830"> – well, I do, but I’m not going to tell you just yet. </st><st c="40885">To estimate the true value of </st><span class="_-----MathTools-_Math_Variable"><st c="40915">β</st></span><st c="40916"> that underlies the data, we’ll have to use the model form in </st><em class="italic"><st c="40978">Eq. </st><st c="40982">23</st></em><st c="40984"> and minimize the </st><span class="No-Break"><st c="41002">empirical risk:</st></span></p>
			<p class="IMG---Figure"><a id="_idTextAnchor252"/></p>
			<div>
				<div id="_idContainer1520" class="IMG---Figure">
					<img src="image/B19496_04_07.jpg" alt="Figure 4.7: Empirical risk function and starting parameter estimate for our simulated dataset"/><st c="41017"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="41081">Figure 4.7: Empirical risk function and starting parameter estimate for our simulated dataset</st></p>
			<p><st c="41174">From the shape of the risk function in </st><span class="No-Break"><em class="italic"><st c="41214">Figure 4</st></em></span><em class="italic"><st c="41222">.7</st></em><st c="41224">, you can see that there is a single minimum close to </st><img src="image/1462.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:3.299em"/><st c="41278"/><st c="41279">. As you may have guessed, </st><img src="image/1463.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:3.305em"/><st c="41306"/><st c="41307"> is the value of </st><img src="image/1356.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.526em"/><st c="41324"/><st c="41325"> that I used to generate the simulated data. </st><st c="41370">Let’s</st><a id="_idIndexMarker440"/><st c="41375"> say we start with a guess for the true value of </st><span class="_-----MathTools-_Math_Variable"><st c="41424">β</st></span><st c="41425"> that is 1. </st><st c="41437">That is, we’re going to initially set our estimate </st><img src="image/1465.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt; &lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:2.266em"/><st c="41488"/><st c="41490">. How good is that initial estimate? </st><st c="41527">We have also plotted the position of this estimate as a red dot in </st><span class="No-Break"><em class="italic"><st c="41594">Figure 4</st></em></span><em class="italic"><st c="41602">.7</st></em><st c="41604"> so that you can see how good an estimate it is by seeing how close it is to the minimum. </st><st c="41694">If it were the optimal (best) estimate, we would be at the minimum of the empirical risk; that is, we would have </st><span class="No-Break"><st c="41807">the following:</st></span></p>
			<p><img src="image/1466.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.307em;height:2.471em;width:6.541em"/><st c="41821"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="41828">Eq. </st><st c="41832">24</st></p>
			<p><st c="41834">We can easily derive a formula for the derivative on the left-hand side of </st><em class="italic"><st c="41910">Eq. </st><st c="41914">24</st></em><st c="41916">. It is </st><span class="No-Break"><st c="41924">the following:</st></span></p>
			<p><img src="image/1467.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.943em;height:2.619em;width:22.125em"/><st c="41938"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="41984">Eq. </st><st c="41988">25</st></p>
			<p><st c="41990">So, we can just plug our current estimate </st><img src="image/1468.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:2.712em"/><st c="42033"/><st c="42034"> into </st><em class="italic"><st c="42040">Eq. </st><st c="42044">25</st></em><st c="42046"> to see how close we are to the optimality criterion in </st><em class="italic"><st c="42102">Eq. </st><st c="42106">24</st></em><st c="42108">. If we are within a specified tolerance of the criterion in </st><em class="italic"><st c="42169">Eq. </st><st c="42173">24</st></em><st c="42175">, our estimate </st><img src="image/1469.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.647em"/><st c="42190"/> <span class="No-Break"><st c="42194">is good.</st></span></p>
			<p><st c="42202">What happens if we are not within the specified tolerance? </st><st c="42262">How should we adjust our estimate </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.728em"/><st c="42296"/><st c="42299">? Clearly, we want to adjust our estimate </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.713em"/><st c="42341"/><st c="42344"> by moving it downhill toward the minimum. </st><st c="42386">How do we work out which direction is downward – that is, which direction reduces the empirical risk? </st><st c="42488">Well, this is what the derivative in </st><em class="italic"><st c="42525">Eq. </st><st c="42529">25</st></em><st c="42531"> tells us. </st><st c="42542">Recall from </st><a href="B19496_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><st c="42554">Chapter 1</st></em></span></a><st c="42563"> that the derivative of a function tells us the gradient or slope of a function. </st><st c="42644">So, the numerical</st><a id="_idIndexMarker441"/><st c="42661"> value of the calculation in </st><em class="italic"><st c="42690">Eq. </st><st c="42694">25</st></em><st c="42696"> evaluated at </st><img src="image/1469.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.656em"/><st c="42710"/><st c="42714"> tells us in which direction we should adjust </st><img src="image/1473.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.757em"/><st c="42759"/><st c="42763">. If the derivative is positive then increasing </st><img src="image/1473.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.758em"/><st c="42811"/><st c="42815"> will increase the empirical risk, so we want to decrease </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.723em"/><st c="42872"/><st c="42875">. Conversely, if the derivative is negative then increasing </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.698em"/><st c="42935"/><st c="42939"> will decrease the empirical risk </st><span class="No-Break"><st c="42972">as desired.</st></span></p>
			<p><st c="42983">The size of the derivative also gives us a guide by how much we should adjust </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.694em"/><st c="43062"/><st c="43066">. If the absolute value of the derivative is large, it tells us we are high on the steep slopes of the risk function, as depicted by the red dot starting point in </st><span class="No-Break"><em class="italic"><st c="43229">Figure 4</st></em></span><em class="italic"><st c="43237">.7</st></em><st c="43239">, and so we are a long way from the minimum. </st><st c="43284">Overall, the bigger the absolute value of the derivative, the more we should </st><span class="No-Break"><st c="43361">adjust </st></span><span class="No-Break"><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.698em"/><st c="43368"/></span><span class="No-Break"><st c="43372">.</st></span></p>
			<p><st c="43373">Putting together the arguments from the previous two paragraphs, we should adjust our estimate of </st><img src="image/1469.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.649em"/><st c="43472"/><st c="43476"> by descending according to the direction and size of the gradient of our objective function. </st><st c="43569">This approach is</st><a id="_idIndexMarker442"/><st c="43585"> called </st><strong class="bold"><st c="43593">gradient descent</st></strong><st c="43609"> and tells us we should adjust our estimate of </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.700em"/><st c="43656"/><st c="43660"> according to an </st><span class="No-Break"><st c="43676">update rule:</st></span></p>
			<p><img src="image/1481.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;&quot; close=&quot;&quot;&gt;&lt;mrow&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.307em;height:2.471em;width:8.931em"/><st c="43688"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="43690">Eq. </st><st c="43694">26</st></p>
			<p><st c="43696">The quantity </st><img src="image/1482.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.512em"/><st c="43710"/><st c="43711"> controls how quickly we adjust </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.707em"/><st c="43743"/><st c="43747">, and so is called the learning rate, since it controls how quickly we move toward or learn the true optimal value of </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.698em"/><st c="43865"/><st c="43869">. A very small value of </st><img src="image/1482.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.508em"/><st c="43893"/><st c="43894"> will mean we make relatively small adjustments to </st><img src="image/1486.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.667em"/><st c="43945"/><st c="43949"> even when the derivative </st><img src="image/1487.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mtext&gt;Risk&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.362em;height:1.211em;width:1.883em"/><st c="43974"/><st c="43980"> is relatively large. </st><st c="44001">Conversely, a larger value of </st><img src="image/1482.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.508em"/><st c="44031"/><st c="44032"> will mean we take larger steps toward the minimum of the </st><span class="No-Break"><st c="44090">empirical risk.</st></span></p>
			<p><st c="44105">Let’s see what happens when we use the update rule in </st><em class="italic"><st c="44160">Eq. </st><st c="44164">26</st></em><st c="44166"> for a few iterations. </st><st c="44189">The results are shown in </st><span class="No-Break"><em class="italic"><st c="44214">Figure 4</st></em></span><span class="No-Break"><em class="italic"><st c="44222">.</st><a id="_idTextAnchor253"/><st c="44223">8</st></em></span><span class="No-Break"><st c="44225">:</st></span></p>
			<div>
				<div id="_idContainer1548" class="IMG---Figure">
					<img src="image/B19496_04_08.jpg" alt="Figure 4.8: Evolution of the model parameter estimate as we iterate the gradient descent update rule"/><st c="44226"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44411">Figure 4.8: Evolution of the model parameter estimate as we iterate the gradient descent update rule</st></p>
			<p><st c="44511">In the left-hand panel of </st><span class="No-Break"><em class="italic"><st c="44538">Figure 4</st></em></span><em class="italic"><st c="44546">.8</st></em><st c="44548">, we see our starting estimate of </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.692em"/><st c="44582"/><st c="44586">=1, shown by the position of the red dot. </st><st c="44628">After one iteration, we have updated our estimate  of </st><img src="image/1357.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.708em"/><st c="44681"/><st c="44684"> according to the update rule in </st><em class="italic"><st c="44716">Eq. </st><st c="44720">26</st></em><st c="44722"> (and using a learning rate of </st><img src="image/1491.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.05&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.891em;width:3.729em"/><st c="44753"/><st c="44763">). </st><st c="44766">This gives us a new value of </st><img src="image/1492.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2.062&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:4.405em"/><st c="44795"/><st c="44805"> for</st><a id="_idIndexMarker443"/><st c="44808"> our estimate </st><img src="image/1469.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.655em"/><st c="44822"/><st c="44826">. This is shown in the middle panel of </st><span class="No-Break"><em class="italic"><st c="44865">Figure 4</st></em></span><em class="italic"><st c="44873">.8</st></em><st c="44875">, with again the red dot showing the position of </st><img src="image/1486.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.681em"/><st c="44924"/><st c="44928"> and its corresponding empirical risk value. </st><st c="44972">In the right-hand panel, the red dot shows the updated value of </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.705em"/><st c="45036"/><st c="45040"> at </st><img src="image/1496.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;2.664&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:4.407em"/><st c="45043"/><st c="45053"> (and corresponding empirical risk) after iteration 2. </st><st c="45107">For each of the iterations in </st><span class="No-Break"><em class="italic"><st c="45137">Figure 4</st></em></span><em class="italic"><st c="45145">.8</st></em><st c="45147">, the direction and size of the update to the current estimate of </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.690em"/><st c="45213"/><st c="45217"> is shown schematically by the arrow. </st><st c="45254">We can see that the updates all move the estimate </st><img src="image/1498.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.733em"/><st c="45304"/><st c="45308"> closer to the minimum, but the magnitude of the updates decrease as we get closer to the minimum. </st><st c="45406">What happens if we carry on iterating? </st><st c="45445">Let’s look at how we would do that in a </st><span class="No-Break"><st c="45485">code example.</st></span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor254"/><st c="45498">Gradient descent code example</st></h2>
			<p><st c="45528">The simulated data is in the </st><strong class="source-inline"><st c="45558">Data/gradient_descent_example.csv</st></strong><st c="45591"> file in the GitHub repository. </st><st c="45623">You’ll also find the following code example (and more) in the </st><strong class="source-inline"><st c="45685">Code_Examples_Chap4.ipynb</st></strong><st c="45710"> Jupyter</st><a id="_idIndexMarker444"/><st c="45718"> notebook in </st><span class="No-Break"><st c="45731">the repository.</st></span></p>
			<p><st c="45746">To begin, we’ll read in </st><span class="No-Break"><st c="45771">the data:</st></span></p>
			<pre class="source-code"><st c="45780">
import pandas as pd
import numpy as np
# Read in the raw data
df_risk = pd.read_csv("../Data/gradient_descent_example.csv")
# Extract the feature and response values to
# numpy arrays
x=df_risk['x'].to_numpy()
y=df_risk['y'].to_numpy()</st></pre>			<p><st c="46016">Then, we’ll define some functions to </st><a id="_idIndexMarker445"/><st c="46054">calculate the empirical risk and </st><span class="No-Break"><st c="46087">its derivative:</st></span></p>
			<pre class="source-code"><st c="46102">
# Define functions for performing gradient descent
def risk(x, y, beta):
    '''
    Function to compute the empirical risk.
    </st><st c="46220">x is a 1D numpy array of the feature values,
    y is a 1D numpy array of the response values.
    </st><st c="46311">beta is the model parameter value.
    </st><st c="46346">'''
    # Initialize the risk value
    risk = 0.0
    # Loop over the data an increment the risk with
    # a squared-loss
    for i in range(x.shape[0]):
        risk += np.power(y[i]-(beta*x[i]), 2.0)
    risk /= x.shape[0]
    return risk
def derivative_risk(x, y, beta):
    '''
    Function to compute the derivative of the empirical risk
    with respect to the model parameter.
    </st><st c="46684">x is a 1D numpy array of the feature values,
    y is a 1D numpy array of the response values.
    </st><st c="46775">beta is the model parameter value.
    </st><st c="46810">'''
    derivative_risk = 0.0
    for i in range(x.shape[0]):
        derivative_risk += - (2.0*x[i]*(y[i]-(beta*x[i])))
    derivative_risk /= x.shape[0]
    return derivative_risk</st></pre>			<p><st c="46967">Now, we’ll perform 20 iterations</st><a id="_idIndexMarker446"/><st c="47000"> of the gradient descent </st><span class="No-Break"><st c="47025">update rule:</st></span></p>
			<pre class="source-code"><st c="47037">
# Set the learning rate and the number of iterations we want to 
# perform
eta=0.05
n_iter=20
# Initialize arrays to hold the sequence of
# parameter estimates and empirical risk values
beta_learn=np.full(1+n_iter, np.nan)
risk_learn=np.full(1+n_iter, np.nan)
# Set the starting estimate for the
# model parameter
beta_learn[0]=1.0
# Iterate using the gradient descent update rule
for iter in range(n_iter):
    risk_learn[iter] = risk(x,y,beta_learn[iter])
    beta_learn[iter+1] = beta_learn[iter] 
    beta_learn[iter+1] -= (eta*derivative_risk(x,y,beta_learn[iter]))</st></pre>			<p><st c="47594">Finally, we can plot the trajectory of the </st><span class="No-Break"><st c="47638">parameter estimates:</st></span></p>
			<pre class="source-code"><st c="47658">
# Plot parameter estimates at each iteration
plt.plot(beta_learn, marker="o")
plt.title(r'$\hat{\beta}$ vs Iteration', fontsize=24)
plt.xlabel('Iteration', fontsize=20)
plt.ylabel(r'$\hat{\beta}$', fontsize=20)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.s</st><a id="_idTextAnchor255"/><st c="47923">how()</st></pre>			<div>
				<div id="_idContainer1559" class="IMG---Figure">
					<img src="image/B19496_04_09.jpg" alt="Figure 4.9: Plot of gradient descent trajectory of the model parameter estimate"/><st c="47929"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="48018">Figure 4.9: Plot of gradient descent trajectory of the model parameter estimate</st></p>
			<p><st c="48097">We can see from </st><span class="No-Break"><em class="italic"><st c="48114">Figure 4</st></em></span><em class="italic"><st c="48122">.9</st></em><st c="48124"> that as we perform more gradient descent iterations, the parameter estimate </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.703em"/><st c="48201"/><st c="48205"> appears to converge to a value close to 3.5. </st><st c="48250">What is the value that </st><img src="image/1476.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.703em"/><st c="48273"/><st c="48277"> converges to? </st><st c="48291">If you run the</st><a id="_idIndexMarker447"/><st c="48305"> code in the notebook, you’ll find that </st><img src="image/1501.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.260em;width:0.662em"/><st c="48345"/><st c="48349"> converges to 3.453. </st><st c="48369">This is not the true value of </st><img src="image/1502.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3.5&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:3.143em"/><st c="48399"/><st c="48400"> that was used to generate the data. </st><st c="48437">Why is this so? </st><st c="48453">What has happened? </st><st c="48472">The answer is that the empirical risk is a function of the dataset, and the data contains a random component. </st><st c="48582">Because of this, the value of </st><img src="image/1503.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.514em"/><st c="48612"/><st c="48613"> that minimizes the empirical risk will be close to but not precisely the same as the true value that was used to generate </st><span class="No-Break"><st c="48736">the data.</st></span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor256"/><st c="48745">Gradient descent is a general technique</st></h2>
			<p><st c="48785">The example we just walked</st><a id="_idIndexMarker448"/><st c="48812"> through, including the code, is very simplistic. </st><st c="48862">What it does highlight, though, is how general the update rule in </st><em class="italic"><st c="48928">Eq. </st><st c="48932">26</st></em><st c="48934"> is. </st><st c="48939">We can extend the update rule to when we have multiple model parameters, which we’ll denote by the vector </st><img src="image/1504.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.060em;width:0.535em"/><st c="49045"/><st c="49046">. The new update rule is </st><span class="No-Break"><st c="49071">the following:</st></span></p>
			<p><img src="image/1505.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;&quot; close=&quot;&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;←&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.427em;height:2.591em;width:9.242em"/><st c="49085"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49087">Eq. </st><st c="49091">27</st></p>
			<p><st c="49093">The gradient descent update rule in </st><em class="italic"><st c="49130">Eq. </st><st c="49134">27</st></em><st c="49136"> can be applied to any empirical risk function, meaning we have a method of constructing optimal parameter estimates </st><img src="image/1445.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.349em;height:1.351em;width:0.752em"/><st c="49253"/><st c="49254"> for any model and any choice of </st><span class="No-Break"><st c="49287">loss function.</st></span></p>
			<p><st c="49301">To illustrate this, imagine </st><a id="_idIndexMarker449"/><st c="49330">we wanted to use the pseudo-Huber loss function of </st><em class="italic"><st c="49381">Eq. </st><st c="49385">12</st></em><st c="49387">. Our empirical risk function is given by </st><span class="No-Break"><st c="49429">the following:</st></span></p>
			<p><span class="_-----MathTools-_Math_Text"><st c="49443">Empirical Risk = </st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="49461">1</st></span><span class="_-----MathTools-_Math_Number"/><span class="_-----MathTools-_Math_Base"><st c="49462">_</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49463">N</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><st c="49464">∑</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49465">i</st></span><span class="_-----MathTools-_Math_Operator"><st c="49466">=</st></span><span class="_-----MathTools-_Math_Number"><st c="49467">1</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49468">N</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"><st c="49469">[</st></span><span class="_-----MathTools-_Math_Base"><st c="49470">√</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="49471">_______________</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="49486">1</st></span><span class="_-----MathTools-_Math_Operator"><st c="49488">+</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><st c="49489">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="49490">y</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49491">i</st></span><span class="_-----MathTools-_Math_Operator"><st c="49492">−</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Base"><st c="49493">ˆ</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49494">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49495">i</st></span><span class="_-----MathTools-_Math_Base"><st c="49496">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="49497">x</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="49498">_</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Variable"><st c="49499">i</st></span><span class="_-----MathTools-_Math_Base"><st c="49500">|</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><st c="49501">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="49502">_</st></span><span class="_-----MathTools-_Math_Base"><st c="49503">)</st></span><span class="_-----MathTools-_Math_Base"><st c="49504">)</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Number"><st c="49505">2</st></span><span class="_-----MathTools-_Math_Base"/><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><st c="49506">−</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><st c="49507">1</st></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><st c="49508">]</st></span></span></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49509">Eq. </st><st c="49513">28</st></p>
			<p><st c="49515">So, we have a gradient descent update rule of the </st><span class="No-Break"><st c="49566">following form:</st></span></p>
			<p class="IMG---Figure"><img src="image/1507.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;←&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;+&lt;/mo&gt;&lt;mn mathvariant=&quot;italic&quot;&gt;2&lt;/mn&gt;&lt;mfrac&gt;&lt;mi&gt;η&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;∂&lt;/mo&gt;&lt;msub&gt;&lt;mover&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;∂&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.602em;height:3.535em;width:19.135em"/><st c="49581"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49607">Eq. </st><st c="49611">29</st></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor257"/><st c="49613">Beyond simple gradient descent</st></h2>
			<p><st c="49644">Although being an iterative technique rather than a closed-form solution, gradient descent is a very general technique. </st><st c="49765">This makes it a very powerful technique. </st><st c="49806">Consequently, it has been widely studied, modified, and adapted in different ways. </st><st c="49889">One of the main drivers of this is the fact that </st><a id="_idIndexMarker450"/><st c="49938">variants of gradient descent are used in the training of </st><strong class="bold"><st c="49995">neural networks</st></strong><st c="50010"> (</st><strong class="bold"><st c="50012">NNs</st></strong><st c="50015">), including </st><strong class="bold"><st c="50029">deep learning</st></strong><st c="50042"> (</st><strong class="bold"><st c="50044">DL</st></strong><st c="50046">) NNs. </st><st c="50054">We don’t have</st><a id="_idIndexMarker451"/><st c="50067"> space here to go into the full mathematical detail of the various adaptations of gradient descent. </st><st c="50167">Instead, we will briefly describe some of the most important modifications </st><span class="No-Break"><st c="50242">and concepts:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="50255">Local minima</st></strong><st c="50268">: The risk function shown in </st><span class="No-Break"><em class="italic"><st c="50298">Figure 4</st></em></span><em class="italic"><st c="50306">.7</st></em><st c="50308"> has a single minimum. </st><st c="50331">For real-world datasets, it is typical for</st><a id="_idIndexMarker452"/><st c="50373"> a risk function to have multiple minima. </st><st c="50415">Since the process of gradient descent is akin to the red dot shown in </st><span class="No-Break"><em class="italic"><st c="50485">Figure 4</st></em></span><em class="italic"><st c="50493">.7</st></em><st c="50495"> rolling down the slope of the risk function, it means gradient descent will roll downhill to the minimum closest to its starting point. </st><st c="50632">Consequently, the result of a simple gradient descent can vary according to where we start. </st><st c="50724">One pragmatic solution to this is to run the gradient descent multiple times and keep the results from the run with the lowest final value of the </st><span class="No-Break"><st c="50870">risk function.</st></span></li>
				<li><strong class="bold"><st c="50884">Stochastic gradient descent (SGD)</st></strong><st c="50918">: If we have a large training dataset, calculating even just a single iteration of the update rule given by </st><em class="italic"><st c="51027">Eq. </st><st c="51031">27</st></em><st c="51033"> may be computationally costly. </st><st c="51065">An alternative </st><a id="_idIndexMarker453"/><st c="51080">approach is to evaluate the empirical risk (and hence the update rule) for just a random subset of the training data. </st><st c="51198">At one extreme, we can just select a single training datapoint to update our model parameter estimate </st><img src="image/1508.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.331em;width:0.669em"/><st c="51300"/><st c="51301">. This leads to quicker updating of </st><img src="image/1508.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.331em;width:0.669em"/><st c="51337"/><st c="51338"> but we must perform multiple updates (that is, random selections of single training datapoints) to obtain a reliable overall estimate </st><img src="image/1510.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.331em;width:0.722em"/><st c="51473"/><st c="51474">. We may cycle through the entire training data but in a random order. </st><st c="51545">Because of this random order in which we use the training data to calculate parameter updates, this </st><a id="_idIndexMarker454"/><st c="51645">approach is called SGD. </st><st c="51669">An alternative version of SGD is to use bigger random subsets of the training data to calculate the empirical risk in the update rule in </st><em class="italic"><st c="51806">Eq. </st><st c="51810">27</st></em><st c="51812">. This approach is known as </st><strong class="bold"><st c="51840">mini-batch</st></strong><st c="51850"> SGD since we are using the training data in small batches, rather than just its entirety as we did in the simple</st><a id="_idIndexMarker455"/><st c="51963"> version of gradient descent. </st><st c="51993">In fact, the simple version of gradient descent is also known as batch gradient descent </st><a id="_idIndexMarker456"/><st c="52081">because we are using the entire batch of training data in </st><span class="No-Break"><st c="52139">one go.</st></span></li>
				<li><strong class="bold"><st c="52146">Adaptive gradient descent algorithms</st></strong><st c="52183">: The simple version of gradient descent we demonstrated had a fixed learning rate. </st><st c="52268">We explained that this learning rate had to be chosen well, but </st><a id="_idIndexMarker457"/><st c="52332">we didn’t explain how to do this. </st><st c="52366">Tuning the learning rate can be an art. </st><st c="52406">However, adaptive gradient descent algorithms attempt to automatically tune parameters such as the learning rate, so we have an adaptive learning rate whose value depends on where we are on the empirical risk function surface. </st><st c="52633">There are a number of these adaptive gradient descent algorithms. </st><st c="52699">Some of the more well-known (and well-used) ones include the AdaGrad optimizer and the Adam optimizer. </st><st c="52802">The Adam optimizer also</st><a id="_idIndexMarker458"/><st c="52825"> makes use of the concept of </st><strong class="bold"><st c="52854">momentum</st></strong><st c="52862">, whereby the updates to the parameter estimate </st><img src="image/1511.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.331em;width:0.622em"/><st c="52910"/><st c="52911"> are based not just on the current risk gradient </st><img src="image/1512.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mfenced open=&quot;&quot; close=&quot;|&quot;&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;mtext&gt;Risk&lt;/mtext&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;∂&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mover&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;ˆ&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.908em;height:1.653em;width:2.850em"/><st c="52960"/><st c="52961">  but also on the gradient at the </st><span class="No-Break"><st c="52994">preceding iterations.</st></span></li>
			</ul>
			<p><st c="53015">That brief summary of some of the improvements on simple gradient descent is a good place to stop and recap what we have learned in this section and summarize the </st><span class="No-Break"><st c="53179">chapter overall.</st></span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor258"/><st c="53195">What we learned</st></h2>
			<p><st c="53211">In this section, we have learned </st><span class="No-Break"><st c="53245">the following:</st></span></p>
			<ul>
				<li><st c="53259">How to use the derivative of the empirical risk to estimate model parameters with any choice of </st><span class="No-Break"><st c="53356">loss function</st></span></li>
				<li><st c="53369">How the simple idea of gradient descent has been extended to create sophisticated gradient-based general </st><span class="No-Break"><st c="53475">optimization algorithms</st></span></li>
			</ul>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor259"/><st c="53498">Summary</st></h1>
			<p><st c="53506">This chapter has focused on a single, but important, concept – loss functions. </st><st c="53586">Loss functions are important because they help us measure how good our predictive models are and, more generally, how well one mathematical object approximates another. </st><st c="53755">They are also important because we can minimize them with respect to our model parameters, and so we can use loss functions, or more specifically risk functions, to fit our models to training data. </st><st c="53953">In this chapter, we have learned about the different aspects of risk functions and how to minimize them. </st><st c="54058">Specifically, we have learned about </st><span class="No-Break"><st c="54094">the following:</st></span></p>
			<ul>
				<li><st c="54108">What a loss function is and what </st><span class="No-Break"><st c="54142">it measures</st></span></li>
				<li><st c="54153">That a risk function is the expectation value of a </st><span class="No-Break"><st c="54205">loss function</st></span></li>
				<li><st c="54218">What the empirical risk function is and how it is calculated from </st><span class="No-Break"><st c="54285">training data</st></span></li>
				<li><st c="54298">How least squares minimization is a form of empirical risk minimization and can be used to estimate optimal parameter values for </st><span class="No-Break"><st c="54428">a model</st></span></li>
				<li><st c="54435">How OLS regression performs least squares minimization for </st><span class="No-Break"><st c="54495">linear models</st></span></li>
				<li><st c="54508">How to derive the closed-form formula for the OLS parameter estimates of a </st><span class="No-Break"><st c="54584">linear model</st></span></li>
				<li><st c="54596">How to perform OLS regression using specialized regression packages such as </st><strong class="source-inline"><st c="54673">statsmodels</st></strong><st c="54684"> and via explicit calculation using the </st><span class="No-Break"><st c="54724">closed-form formula</st></span></li>
				<li><st c="54743">How empirical risk minimization can also be performed very generally via </st><span class="No-Break"><st c="54817">gradient descent</st></span></li>
				<li><st c="54833">Variants of simple gradient descent, such as SGD, and also adaptive gradient descent algorithms such as the </st><span class="No-Break"><st c="54942">Adam optimizer</st></span></li>
			</ul>
			<p><st c="54956">Now we have learned how to measure how good a model is, we’ll move on to the task of learning the math behind the building of those models. </st><st c="55097">Since those models will be built on data and data always contains a random component, it is natural for the building of models to use the probability concepts we introduced in </st><a href="B19496_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic"><st c="55273">Chapter 2</st></em></span></a><st c="55282">. That is why, in the next chapter, we’ll learn about </st><span class="No-Break"><st c="55336">probabilistic models.</st></span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor260"/><st c="55357">Exercises</st></h1>
			<p><st c="55367">Next is a series of exercises. </st><st c="55399">Answers to all the exercises are given in the </st><strong class="source-inline"><st c="55445">Answers_to_Exercises_Chap4.ipynb</st></strong><st c="55477"> Jupyter notebook in the </st><span class="No-Break"><st c="55502">GitHub repository:</st></span></p>
			<ol>
				<li><st c="55520">Look at the documentation for the </st><strong class="source-inline"><st c="55555">scikit-learn</st></strong><st c="55567"> class named </st><strong class="source-inline"><st c="55580">sklearn.linear_model.LinearRegression</st></strong><st c="55617">, which can fit a linear model using OLS regression. </st><st c="55670">See if you can use it to fit a linear model to the power-plant output data that we analyzed in the code example in the </st><em class="italic"><st c="55789">Linear models</st></em><st c="55802"> section of this chapter. </st><st c="55828">Do you get the same parameter estimates as when we used the </st><span class="No-Break"><strong class="source-inline"><st c="55888">statsmodels</st></strong></span><span class="No-Break"><st c="55899"> package?</st></span></li>
				<li><st c="55908">The data plotted in </st><span class="No-Break"><em class="italic"><st c="55929">Figure 4</st></em></span><em class="italic"><st c="55937">.3</st></em><st c="55939"> is stored in the </st><strong class="source-inline"><st c="55957">Data/outliers_example.csv</st></strong><st c="55982"> file of the GitHub repository. </st><st c="56014">Using the pseudo-Huber loss function in </st><em class="italic"><st c="56054">Eq. </st><st c="56058">12</st></em><st c="56060"> and a learning rate of </st><img src="image/1513.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;η&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.05&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.891em;width:3.615em"/><st c="56084"/><st c="56094">, see if you can use the simple gradient descent algorithm to construct robust estimates for both the intercept and the slope for a linear model of </st><span class="No-Break"><st c="56242">the data.</st></span></li>
				<li><st c="56251">The data in the </st><strong class="source-inline"><st c="56268">Data/nls_example.csv</st></strong><st c="56288"> file of the GitHub repository contains data that has been generated according to the </st><span class="No-Break"><st c="56374">following relationship:</st></span></li>
			</ol>
			<p class="IMG---Figure"><img src="image/1514.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;+&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;−&lt;/mo&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.977em;width:5.949em"/><st c="56397"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="56406">Eq. </st><st c="56410">30</st></p>
			<p class="list-inset"><st c="56412">The </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="56417"/><st c="56440"> values in the dataset have been corrupted by noise; that is, they also contain an additive random component. </st><st c="56549">Use least squares minimization to estimate suitable values for the parameters </st><img src="image/1516.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.117em;height:0.780em;width:1.592em"/><st c="56627"/><st c="56628">, and </st><img src="image/461.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.015em;height:0.678em;width:0.633em"/><st c="56634"/><st c="56635">. That is, minimize the empirical risk using a squared-loss function and a model of the form in the preceding equation. </st><st c="56755">You can assume that the parameter </st><img src="image/461.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.015em;height:0.678em;width:0.637em"/><st c="56789"/><st c="56790"> is strictly positive; that </st><span class="No-Break"><st c="56818">is, </st></span><span class="No-Break"><img src="image/1519.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.015em;height:0.678em;width:2.626em"/><st c="56822"/></span><span class="No-Break"><st c="56823">.</st></span></p>
		</div>
	<div id="charCountTotal" value="56824"/></body></html>