["```py\n import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n# Read in the raw data\ndf = pd.read_csv(\"../Data/power_plant_output.csv\")\n```", "```py\n # Use pd.describe() to get the summary statistics of the data\ndf.describe()\n```", "```py\n # Scatterplot between the response variable PE and the AT feature. # The linear relationship is clear. plt.scatter(df.AT, df.PE)\nplt.title('PE vs AT', fontsize=24)\nplt.xlabel('AT', fontsize=20)\nplt.ylabel('PE', fontsize=20)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.show()\n```", "```py\n # Scatterplot between the response variable PE and the V feature. # The linear relationship is clear, but not as strong as the \n# relationship with the AT feature. plt.scatter(df.V, df.PE)\nplt.title('PE vs V', fontsize=24)\nplt.xlabel('V', fontsize=20)\nplt.ylabel('PE', fontsize=20)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.show()\n```", "```py\n # First we specify the model using statsmodels.formula.api.ols\nmodel = smf.ols(formula='PE ~ AT + V', data=df)\n# Now we fit the model to the data, i.e. we minimize the sum-of-\n# squared residuals with respect to the model parameters\nmodel_result = model.fit()\n# Now we'll look at a summary of the fitted OLS model\nmodel_result.summary()\n```", "```py\n # We extract the design matrix as a 2D numpy array. # This initially corresponds to the feature columns of the dataframe. # In this case it is all but the last column\nX = df.iloc[:, 0:(df.shape[1]-1)].to_numpy()\n# Now we'll add a column of ones to the design matrix. # This is the feature that corresponds to the intercept parameter \n# in the moddel\nX = np.c_[np.ones(X.shape[0]), X]\n# For convenience, we'll create and store the transpose of the \n# design matrix\nxT = np.transpose(X)\n# Now we'll extract the response vector to a numpy array\ny = df.iloc[:, df.shape[1]-1].to_numpy()\n```", "```py\n # Calculate the inverse of xTx using the numpy linear algebra \n# functions\nxTx_inv = np.linalg.inv(np.matmul(xT, X))\n# Finally calculate the OLS model parameter estimates using the \n# formula (xTx_inv)*(xT*y). # Again, we use the numpy linear algebra functions to do this\nols_params = np.matmul(xTx_inv, np.matmul(xT, y))\n```", "```py\n # Now compare the parameter estimates from the explicit calculation \n# with those obtained from the statsmodels fit\ndf_compare = pd.DataFrame({'statsmodels': model_result.params, \n                           'explicit_ols':ols_params})\ndf_compare\n```", "```py\n import pandas as pd\nimport numpy as np\n# Read in the raw data\ndf_risk = pd.read_csv(\"../Data/gradient_descent_example.csv\")\n# Extract the feature and response values to\n# numpy arrays\nx=df_risk['x'].to_numpy()\ny=df_risk['y'].to_numpy()\n```", "```py\n # Define functions for performing gradient descent\ndef risk(x, y, beta):\n    '''\n    Function to compute the empirical risk. x is a 1D numpy array of the feature values,\n    y is a 1D numpy array of the response values. beta is the model parameter value. '''\n    # Initialize the risk value\n    risk = 0.0\n    # Loop over the data an increment the risk with\n    # a squared-loss\n    for i in range(x.shape[0]):\n        risk += np.power(y[i]-(beta*x[i]), 2.0)\n    risk /= x.shape[0]\n    return risk\ndef derivative_risk(x, y, beta):\n    '''\n    Function to compute the derivative of the empirical risk\n    with respect to the model parameter. x is a 1D numpy array of the feature values,\n    y is a 1D numpy array of the response values. beta is the model parameter value. '''\n    derivative_risk = 0.0\n    for i in range(x.shape[0]):\n        derivative_risk += - (2.0*x[i]*(y[i]-(beta*x[i])))\n    derivative_risk /= x.shape[0]\n    return derivative_risk\n```", "```py\n # Set the learning rate and the number of iterations we want to \n# perform\neta=0.05\nn_iter=20\n# Initialize arrays to hold the sequence of\n# parameter estimates and empirical risk values\nbeta_learn=np.full(1+n_iter, np.nan)\nrisk_learn=np.full(1+n_iter, np.nan)\n# Set the starting estimate for the\n# model parameter\nbeta_learn[0]=1.0\n# Iterate using the gradient descent update rule\nfor iter in range(n_iter):\n    risk_learn[iter] = risk(x,y,beta_learn[iter])\n    beta_learn[iter+1] = beta_learn[iter] \n    beta_learn[iter+1] -= (eta*derivative_risk(x,y,beta_learn[iter]))\n```", "```py\n # Plot parameter estimates at each iteration\nplt.plot(beta_learn, marker=\"o\")\nplt.title(r'$\\hat{\\beta}$ vs Iteration', fontsize=24)\nplt.xlabel('Iteration', fontsize=20)\nplt.ylabel(r'$\\hat{\\beta}$', fontsize=20)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()\n```"]