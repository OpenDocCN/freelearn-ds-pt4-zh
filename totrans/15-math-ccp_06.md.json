["```py\n import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import loggamma\nfrom scipy.optimize import minimize\n def get_neg_log_binomial_posterior(n_trial, n_success, alpha, beta):\n     '''\n     A function to construct a callable that returns the negative of\n     the log-posterior for binomially distributed data, with a Beta\n     prior for the success probability of the Bernoulli trials. Returns a callable that returns the negative of the log-posterior\n     (up to a global constant) and takes the logit of the success\n     probability logit(p) as input. '''\n     def neg_log_binomial_posterior(logit_p):\n         '''\n         A function to compute the negative log-posterior (up to a\n         global constant) for binomially distributed data, with a Beta\n         prior for the success probability of the Bernoulli trials. logit_p is the logit of the success probability. Returns the negative of the sum of the log-likelihood and the\n         the log-prior\n         '''\n         # Compute the success probability p from logit(p)\n         p = np.exp(logit_p)/ (1.0 + np.exp(logit_p))\n         # Compute the log-prior\n         log_prior = loggamma(alpha + beta) - loggamma(alpha) - \\\n            loggamma(beta)\n         log_prior += ((alpha-1.0)*np.log(p)) + \\\n            ((beta-1.0)*np.log(1.0 - p))\n         # Compute the log-likelihood\n         log_likelihood = loggamma(n_trial +1.0)\n         log_likelihood -= loggamma(n_trial - n_success +1.0)\n         log_likelihood -= loggamma(n_success +1.0)\n         log_likelihood += (n_success*np.log(p))\n         log_likelihood += ((n_trial-n_success)*np.log(1.0-p))\n         # Compute the log-posterior, \n         #up to the global normalization factor,\n         # as the sum of the log prior and log-likelihood\n         log_posterior = log_likelihood + log_prior\n         return -log_posterior\n     return neg_log_binomial_posterior\n```", "```py\n # Specify the sample size and the number of successes\nn_trial = 10\nn_success = 5\n# Specify the parameters of the prior\nalpha = 8\nbeta = 2\n# Get the objective function to minimized\nneg_log_posterior = get_neg_log_binomial_posterior(n_trial, n_success, \n                                                   alpha, beta)\n# Construct an initial estimate for the optimal parameter. # We'll use the sample success proportion to do this \n# (and take the logit)\np0 = float(n_success)/float(n_trial)\nlogit_p0 = np.log(p0/(1.0-p0))\nx0 = np.array([logit_p0])\n```", "```py\n map_estimate = minimize(neg_log_posterior,\n                        x0,\n                        method='BFGS',\n                        options={'disp': True})\n# Convert from logit(p) to p\np_optimal = np.exp(map_estimate['x'][0])/ (\n    1.0 + np.exp(map_estimate['x'][0]))\nprint(\"MAP estimate of success probability = \", p_optimal)\n```", "```py\n MAP estimate of success probability = 0.666666667917668\n```", "```py\n     import numpy as np\n    def perform_mh_trial(x, log_post, delta_x, neg_log_posterior):\n         '''\n         Function to perform a Metropolis-Hastings trial move. x is the current logit(p) value. log_post is the current log-posterior value. delta_x is the half-width of the range from which the trial\n         adjustments to logit(p) are made. neg_log_posterior is a callable that returns the negative \n         log-posterior. We return a tuple of the updated logit(p) value and updated \n         log-posterior value. '''\n         accept_trial = False\n         x_trial = x + (delta_x*(2.0*np.random.rand(1) -0))\n         p_trial = np.exp(x_trial)/(1.0 + np.exp(x_trial))\n         # Calculate the log-posterior for the trial point. # Note we'll need to flip the sign of neg_log_posterior, as\n         # our callable returns the negative of the log-posterior. log_post_trial = -neg_log_posterior(x_trial) + np.log(p_\n         trial*(1.0 â€“ p_trial))\n         # Calculate the change in log-posterior if we move to the \n         # trial point\n         delta_log_post = log_post_trial - log_post\n         # Work out if should accept the trial point\n         if delta_log_post > 0.0:\n             accept_trial = True\n         else:\n             if np.log(np.random.rand(1)) < delta_log_post:\n                 accept_trial = True\n         # If we accept the trial point then update the current \n         # value of the parameter\n         # and the log-posterior\n         if accept_trial==True:\n             x = x_trial\n             log_post = log_post_trial\n         return x, log_post\n    ```", "```py\n     def mh_mcmc(n_burnin, n_iter, x0, delta_x, neg_log_posterior):\n         '''\n         A function to run a simple Metropolis-Hastings MCMC\n         calculation. n_burnin is the number of burnin iterations to be run. n_iter is the number of sampling iterations to be run. x0 is the starting value for logit(p). delta_x is the half-width of the range from which the trial\n         adjustments to logit(p) are made. neg_log_posterior is a callable that returns the negative \n         log-posterior. We return an array of the sampled logit(p) values\n         '''\n         #Calculate starting log_posterior\n         x = x0\n         p0 = np.exp(x0)/(1.0 + np.exp(x0))\n         log_post = -neg_log_posterior(x) + np.log(p0*(1.0-p0))\n         # Run the chain for the specified burn-in length\n         for iter in range(n_burnin):\n             x, log_post = perform_mh_trial(\n                x, log_post, delta_x, neg_log_posterior)\n         # Initialize an empty array to hold the sampled \n         # parameter values\n         x_chain = np.zeros(n_iter)\n         # Continue the chain for the specified number of \n         # sampling points\n         # Store the sampled parameter values\n         for iter in range(n_iter):\n             x, log_post = perform_mh_trial(\n                x, log_post, delta_x, neg_log_posterior)\n             x_chain[iter] = x\n         return x_chain\n    ```", "```py\n     # Specify the sample size and the number of successes\n    n_trial = 10\n    n_success = 5\n    # Specify the parameters of the Beta prior\n    alpha = 8\n    beta = 2\n    # Construct a starting point for the MCMC calculation. # We'll use the sample success proportion to do this \n    # (and take the logit)\n    p0 = float(n_success)/float(n_trial)\n    logit_p0 = np.log(p0/(1.0-p0))\n    ```", "```py\n     # Run the MCMC calculation\n    x_chain = mh_mcmc(n_burnin=20000,\n                      n_iter=1000000,\n                      x0=logit_p0,\n                      delta_x=0.1,\n                      neg_log_posterior=neg_log_posterior)\n    # Convert the MCMC sampled logit(p) values back to values of\n    # the success probability\n    p_mcmc = np.exp(x_chain)/ (1.0 + np.exp(x_chain))\n    ```", "```py\n     import matplotlib.pyplot as plt\n    # Plot the histogram of posterior sampled success probabilities\n    # and overlay the true posterior distribution\n    hist = plt.hist(p_mcmc, bins=100, density=True)\n    posterior = plt.plot(p_sequence, true_posterior_sequence)\n    plt.title('Histogram of MCMC samples', fontsize=24)\n    plt.xlabel(r'$p$', fontsize=20)\n    plt.ylabel('Probability Density', fontsize=20)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.show()\n    ```"]