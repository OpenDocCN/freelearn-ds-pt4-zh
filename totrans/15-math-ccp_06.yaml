- en: <st c="0">5</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Probabilistic Modeling</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="24">At the heart of probabilistic modeling is the idea that because data
    is random, and so follows a</st> <st c="122">probability distribution, our models
    of that data must also follow a</st> <st c="191">probability distribution and
    be probabilistic models from the outset.</st> <st c="261">To understand how to
    build those models, we must first understand the probability distribution that
    the data follows.</st> <st c="379">From this, we can calculate the distribution
    that our model parameters follow by using one of the most famous theorems in probability
    theory.</st> <st c="521">To do all of this, we will cover the</st> <st c="558">following
    topics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="575">Likelihood</st>*<st c="586">: In this section, we will learn about
    the probability distribution of the data given</st> <st c="673">a model</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="680">Bayes’ theorem</st>*<st c="695">: In this section, we will learn
    how to work with conditional probabilities and calculate the probability of a
    model given</st> <st c="819">the data</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="827">Bayesian modeling</st>*<st c="845">: In this section, we will
    learn how to use the probability of the model given the data to make</st> <st
    c="942">useful inferences</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="959">Bayesian modeling in practice</st>*<st c="989">: In this section,
    we will learn practical computational techniques and mathematical approximations
    for doing</st> <st c="1100">Bayesian modeling</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1117">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1140">All code examples given in this chapter (and additional examples)
    can be found at the GitHub repository:</st> [<st c="1246">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter05</st>](https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter05)<st
    c="1350">. To run the Jupyter notebooks, you will need a full Python installation,
    including the</st> <st c="1438">following packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="1457">pandas</st>` <st c="1464">(>=2.0.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1474">numpy</st>` <st c="1480">(>=1.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1488">scipy</st>` <st c="1494">(>=1.1)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1502">matplotlib</st>` <st c="1513">(>=3.7.2)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1523">Likelihood</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1534">To</st> <st c="1537">understand the probability distribution that
    the data follows, we’ll look at an explicit example of how a random component
    is incorporated</st> <st c="1677">into data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1687">A simple probabilistic model</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="1716">We’ll start</st> <st c="1729">with the simplest way in which we
    can introduce a random component into our observations of the response (target)
    variable</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="1852"><st c="1875">, namely by adding noise to a deterministic quantity.</st>
    <st c="1929">In fact, we’ll just consider the observations</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1521.png)
    <st c="1975"><st c="1976">in our dataset to be noise-corrupted versions of a model
    output</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1522.png)<st
    c="2041"><st c="2050">. So, we have</st> <st c="2064">this relationship:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:math>](img/1523.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="2084">Eq.</st> <st c="2088">1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2089">Here,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1524.png)
    <st c="2095"><st c="2096">is the noise value that has been added to the model
    output</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub><mo>=</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1525.png)
    <st c="2156"><st c="2157">to get the observation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1526.png)
    <st c="2181"><st c="2182">for the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/1186.png)
    <st c="2191"><st c="2195">datapoint.</st> <st c="2206">The value</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1528.png)
    <st c="2216"><st c="2217">is a random variable.</st> <st c="2240">Without loss
    of generality, we can assume its expectation value is zero, so we have</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/1529.png)<st
    c="2324"><st c="2325">. We can make this assumption because if the expectation
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1528.png)
    <st c="2385"><st c="2386">was non-zero, it would mean we have a non-zero deterministic
    average contribution from</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1528.png)
    <st c="2474"><st c="2475">that we could just absorb into the definition of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1532.png)<st
    c="2525"><st c="2526">. This is just the same as saying that we</st> <st c="2568">can
    write:</st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>ε</mi><mi>i</mi></msub><mo
    mathvariant="italic">=</mo><mi>a</mi><mo mathvariant="italic">+</mo><msub><mover><mi>ε</mi><mo
    stretchy="true">~</mo></mover><mi>i</mi></msub></mrow></mrow></math>](img/1533.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="2585">Eq.</st> <st c="2589">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>a</mi><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><msub><mi>ε</mi><mi>i</mi></msub></mfenced></mrow></mrow></math>](img/1534.png)<st
    c="2590"><st c="2591">, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1535.png)
    <st c="2597"><st c="2598">is just a new random variable that does have zero expectation
    (by construction).</st> <st c="2680">We then just add</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>a</mml:mi></mml:math>](img/285.png)
    <st c="2697"><st c="2698">into the definition of our</st> <st c="2726">model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/1537.png)<st
    c="2732"><st c="2733">.</st></st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2734">The simple scenario we shall consider is where the noise values</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1538.png)
    <st c="2799"><st c="2800">for the different data points are uncorrelated with
    each other.</st> <st c="2865">Mathematically, we write this assumption as</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="double-struck">E</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/1539.png)<st
    c="2909"><st c="2910">, for</st> <st c="2916">all</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:math>](img/1540.png)<st
    c="2920"><st c="2921">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2922">Now, if each</st> <st c="2936">noise value</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>ε</mi><mi>i</mi></msub><mo>,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi></mrow></mrow></math>](img/1541.png)
    <st c="2948"><st c="2949">is a random variable; what probability distributions
    should those random noise values come from?</st> <st c="3047">Again, we consider
    the simplest situation, where each noise value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1542.png)
    <st c="3113"><st c="3114">is drawn from the same distribution.</st> <st c="3152">Note
    that this doesn’t mean that all the noise values are identical; it means that
    they are</st> <st c="3244">identically distributed.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3268">What should we use for this common noise distribution?</st> <st
    c="3324">Let’s use one of the most common distributions, the Gaussian (or normal)
    distribution.</st> <st c="3411">So, we can write the probability density of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1542.png)
    <st c="3455"><st c="3456">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>p</mml:mi><mml:mfenced separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo> </mml:mo></mml:math>](img/1544.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="3469">Eq.</st> <st c="3473">3</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3474">The quantity</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="3487"><st c="3488">is the standard deviation of the noise value</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1546.png)<st
    c="3534"><st c="3535">, and this is the same for all data points</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>i</mml:mi></mml:math>](img/106.png)<st
    c="3578"><st c="3579">. The bigger the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1548.png)<st
    c="3605"><st c="3606">, the bigger the typical values of the noise will be, i.e.,
    the more the observations</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1521.png)
    <st c="3692"><st c="3693">will differ from the deterministic output from</st>
    <st c="3741">our model.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3751">Since we are using a Gaussian distribution for each of the noise
    values, the fact that different noise values are uncorrelated means they are statistically
    independent.</st> <st c="3921">So, our noise values</st> <st c="3941">are</st>
    **<st c="3946">independently and identically distributed</st>**<st c="3987">,</st>
    <st c="3989">or</st> **<st c="3992">i.i.d</st>**<st c="3997">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3998">In</st> [*<st c="4002">Chapter 2</st>*](B19496_02.xhtml#_idTextAnchor061)<st
    c="4011">, we said that anything derived from a random variable was itself a random
    variable.</st> <st c="4096">Well, if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1550.png)
    <st c="4105"><st c="4106">is a random variable, then Eq.</st> <st c="4138">1 means
    that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1551.png)
    <st c="4151"><st c="4152">is a random variable – as we would expect, it is data
    after all.</st> <st c="4218">But what is the probability distribution for</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1521.png)<st
    c="4263"><st c="4264">? The combination of Eq.</st> <st c="4289">1 and Eq.</st>
    <st c="4299">3 tells us.</st> <st c="4311">If we re-arrange Eq.</st> <st c="4332">1
    we</st> <st c="4337">get this:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>.</mml:mo></mml:math>](img/1553.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="4348">Eq.</st> <st c="4352">4</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4353">If we plug the expression for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1554.png)
    <st c="4383"><st c="4384">in Eq.</st> <st c="4392">4 into Eq.</st> <st c="4403">3,
    we</st> <st c="4409">get this:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>p</mml:mi><mml:mfenced separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo> </mml:mo></mml:math>](img/1555.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="4420">Eq.</st> <st c="4424">5</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4425">This means that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1556.png)
    <st c="4441"><st c="4442">is a Gaussian random variable distributed around a mean
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1557.png)
    <st c="4502"><st c="4503">with a standard deviation of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)<st
    c="4533"><st c="4534">. Since</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1559.png)
    <st c="4542"><st c="4543">is a deterministic function of the input features</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1560.png)
    <st c="4594"><st c="4598">and the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)<st
    c="4623"><st c="4624">, then</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub></mrow></mfenced></mrow></mrow></math>](img/1562.png)
    <st c="4631"><st c="4640">means</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>,</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1563.png)<st
    c="4646"><st c="4647">, so we can re-write the probability density for</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1564.png)
    <st c="4696"><st c="4697">in Eq.</st> <st c="4705">5</st> <st c="4707">as follows:</st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>p</mml:mi><mml:mfenced separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo> </mml:mo></mml:math>](img/1565.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="4720">Eq.</st> <st c="4724">6</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4725">This is the</st> <st c="4737">probability density for the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/1566.png)
    <st c="4765"><st c="4769">observation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1567.png)<st
    c="4781"><st c="4782">. Since the observations are independent, then the probability
    density for the whole set of observations is just the product of the individual
    densities, and so we</st> <st c="4946">have this:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>p</mml:mi><mml:mfenced separators=""><mml:mrow><mml:munder><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo> </mml:mo></mml:math>](img/1568.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="4958">Eq.</st> <st c="4962">7</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4963">This probability</st> <st c="4980">density function gives the probability
    density of the observed data</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)<st
    c="5048"><st c="5049">. It is called the</st> **<st c="5068">likelihood of the
    data</st>**<st c="5090">, or simply the</st> **<st c="5106">likelihood</st>**<st
    c="5116">, and is usually denoted by the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>L</mml:mi></mml:math>](img/1570.png)<st
    c="5155"><st c="5156">. If we were dealing with discrete observations instead
    of continuous ones then we would compute the probability of the data, rather than
    the probability density of the data, but in either case, we refer to the resulting
    function as</st> <st c="5389">the likelihood.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5404">The expression in Eq.</st> <st c="5427">7 is the general form for
    the likelihood when we have independent observations.</st> <st c="5507">If we
    plug in the expression in Eq.</st> <st c="5543">6 into Eq.</st> <st c="5554">7,
    we obtain an explicit expression for the likelihood</st> <st c="5609">L</st> <st
    c="5610">for our specific model.</st> <st c="5635">In this case, the likelihood
    is</st> <st c="5667">as</st> <st c="5670">follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>L</mi><mo>=</mo><mspace
    width="0.25em" /><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mspace
    width="0.25em" /><mtext>exp</mtext><mfenced open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mspace
    width="0.25em" /><msup><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mspace
    width="0.25em" /><mover><mi>y</mi><mo stretchy="true">ˆ</mo></mover><mfenced open="("
    close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><mspace
    width="0.25em" /><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></mrow></mrow><mspace
    width="0.25em" /></mrow></mrow></math>](img/1571.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="5680">Eq.</st> <st c="5684">8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5685">In shorthand, whether we are dealing with a probability or a probability
    density, we often refer to the likelihood using the notation</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><mtext>Model</mtext></mrow></mfenced></mrow></mrow></math>](img/1572.png)<st
    c="5819"><st c="5835">,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><mtext>Model</mtext></mrow></mfenced></mrow></mrow></math>](img/1573.png)<st
    c="5837"><st c="5854">, or</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1574.png)<st
    c="5859"><st c="5872">. This is sometimes shortened to</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>D</mtext><mtext>|</mtext><mtext>Model</mtext></mrow></mfenced></mrow></mrow></math>](img/1575.png)<st
    c="5905"><st c="5918">,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>D</mtext><mtext>|</mtext><mtext>Model</mtext></mrow></mfenced></mrow></mrow></math>](img/1576.png)<st
    c="5920"><st c="5933">, or</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>D</mtext><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1577.png)<st
    c="5938"><st c="5947">, with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>D</mml:mtext></mml:math>](img/1578.png)
    <st c="5954"><st c="5955">standing</st> <st c="5965">for data.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5974">The</st> <st c="5978">likelihood is an extremely useful quantity.</st>
    <st c="6023">In fact, it is the central quantity when developing probabilistic
    models, and as we have learned, all models should be probabilistic.</st> <st c="6157">Given
    the importance of the likelihood, we’re going to look at some of its properties</st>
    <st c="6243">in detail.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6253">Log likelihood</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="6268">The</st> <st c="6272">expression in Eq.</st> <st c="6291">8 looks
    complicated.</st> <st c="6312">It’s a horrible, complicated product of expressions.</st>
    <st c="6365">We can simplify it a bit by converting it to a sum.</st> <st c="6417">We
    do this by taking the logarithm.</st> <st c="6453">From our rules of logarithms
    recapped in</st> [*<st c="6494">Chapter 1</st>*](B19496_01.xhtml#_idTextAnchor014)<st
    c="6503">, we</st> <st c="6508">have</st> <st c="6513">this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>log</mtext><mtext>Likelihood</mtext><mtext>=</mtext><mtext>log</mtext><mi>L</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mi>log</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>,</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/1579.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="6565">Eq.</st> <st c="6569">9</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6570">And for our example in Eq.</st> <st c="6597">8, we</st> <st c="6603">have
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>log</mtext><mtext>Likelihood</mtext><mtext>=</mtext><mtext>log</mtext><mi>L</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfenced
    open="[" close="]"><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mfenced
    open="(" close=")"><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></mrow></mrow></mrow></math>](img/1580.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mfenced separators=""><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo> </mml:mo></mml:math>](img/1581.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="6720">Eq.</st> <st c="6724">10</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6726">The first thing to point out is that the typical value of the log-likelihood
    scales linearly with the number of data points</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="6851"><st c="6852">. For independent observations, each data point will make
    a contribution</st> <st c="6925">log</st><st c="6928">p</st><st c="6930">(</st><st
    c="6931">y</st><st c="6932">i</st> <st c="6933">|</st> <st c="6934">ˆ</st><st
    c="6935">y</st><st c="6936">i</st><st c="6937">)</st><st c="6938">, and these
    contributions build up with each observation.</st> <st c="6996">Even when the
    observations are not independent, but are correlated, we would still expect the</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><mrow><mtext>log</mtext><mtext>Likelihood</mtext></mrow></mfenced></mrow></mrow></math>](img/1583.png)
    <st c="7090"><st c="7108">to scale linearly with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="7131"><st c="7132">, as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1585.png)<st
    c="7137"><st c="7138">, in most cases.</st> <st c="7155">The correlations between
    observations will reduce the amount of information that each observation adds
    to the log-likelihood, but each observation will still add some information.</st>
    <st c="7335">Overall, this means that the larger the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/443.png)<st
    c="7375"><st c="7376">, the larger the magnitude of</st> <st c="7406">the log-likelihood.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7425">The</st> <st c="7429">second thing we can point out about the log-likelihood
    is that it is not only a function of the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1587.png)<st
    c="7543"><st c="7544">, but also of the parameters that control the random component
    in our data; in this case the</st> <st c="7637">parameter</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>σ</mi><mo
    mathvariant="italic">.</mo></mrow></mrow></math>](img/1588.png)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7648">Maximum likelihood estimation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="7677">The</st> <st c="7682">likelihood is</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1589.png)<st
    c="7696"><st c="7709">, where we mean either probability or probability density
    depending on whether we are talking about discrete</st> <st c="7817">data or continuous
    data.</st> <st c="7843">How can we use the likelihood to estimate model parameter
    values that are a good fit for</st> <st c="7932">the data?</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7941">If we have a good model, we expect the probability of the observed
    data to be high.</st> <st c="8026">After all, we have observed the data.</st>
    <st c="8064">If we had parameter values for which</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1590.png)
    <st c="8101"><st c="8113">is low, that would say the probability of the data we
    have actually observed is low.</st> <st c="8198">This would seem strange and perhaps
    just the result of the random sampling variation – it is possible but unlikely.</st>
    <st c="8314">In contrast, parameter values for which</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1591.png)<st
    c="8354">![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1592.png)
    <st c="8355"><st c="8366">is high appear to be more appropriate and a more credible
    explanation of the data.</st> <st c="8449">This gives us a means of estimating
    the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)<st
    c="8506"><st c="8507">. We estimate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="8521"><st c="8522">by maximizing</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1595.png)
    <st c="8537"><st c="8549">with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)<st
    c="8565"><st c="8566">. This is called</st> **<st c="8583">maximum</st>** **<st
    c="8591">likelihood estimation</st>**<st c="8612">.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8613">Pro tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8621">As a technique, maximum likelihood is widely used in data science.</st>
    <st c="8689">However, maximum likelihood was used long before the term data science
    was coined.</st> <st c="8772">Maximum likelihood is taught as part of traditional
    courses in statistics.</st> <st c="8847">The abbreviation</st> **<st c="8864">ML</st>**
    <st c="8866">for</st> **<st c="8871">maximum likelihood</st>** <st c="8889">is
    still used in some research areas.</st> <st c="8928">In data science, this has
    the potential to confuse, since most practitioners of data science would interpret</st>
    **<st c="9037">ML</st>** <st c="9039">to mean</st> **<st c="9048">machine</st>**
    **<st c="9055">learning</st>**<st c="9064">. So, be aware there may be data science
    circumstances where</st> **<st c="9125">ML</st>** <st c="9127">means</st> **<st
    c="9134">maximum likelihood</st>**<st c="9152">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9153">Least squares as an example of maximum likelihood</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="9203">We’ll look at what happens if we estimate the parameters of our
    model</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1597.png)
    <st c="9274"><st c="9275">via</st> <st c="9280">maximum likelihood.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9299">You’ll</st> <st c="9307">recall from the recap in</st> [*<st c="9332">Chapter
    1</st>*](B19496_01.xhtml#_idTextAnchor014) <st c="9341">that the location of the
    maximum of a function is the same as the location of the maximum of the logarithm
    of the function.</st> <st c="9466">This is because the logarithm is a monotonically
    increasing transformation.</st> <st c="9542">Maximizing the likelihood</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>L</mml:mi></mml:math>](img/1598.png)
    <st c="9568"><st c="9569">with respect to the model parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="9607"><st c="9608">is the same as maximizing the log-likelihood,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math>](img/1600.png)<st
    c="9655"><st c="9656">, with respect to the model parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)<st
    c="9695"><st c="9696">. The latter is easier to do.</st> <st c="9726">For our
    model, the log-likelihood is given by Eq.</st> <st c="9776">10\.</st> <st c="9780">So,
    let’s maximize</st> <st c="9799">Eq.</st> <st c="9803">10.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9806">Technically, we should maximize</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math>](img/1602.png)
    <st c="9839"><st c="9844">with respect to all the parameters of the log-likelihood,
    so with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1603.png)
    <st c="9921"><st c="9922">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1604.png)<st
    c="9927"><st c="9928">. We will do this, but for now, let’s take</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1604.png)
    <st c="9971"><st c="9972">as given, i.e., fixed.</st> <st c="9996">We can, in
    theory, maximize with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)
    <st c="10040"><st c="10041">at fixed</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="10051"><st c="10052">to find the maximum likelihood values</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1608.png)
    <st c="10091"><st c="10096">as a function of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="10113"><st c="10114">and plug</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1608.png)
    <st c="10124"><st c="10129">back into Eq.</st> <st c="10143">10, and then finally
    maximize the resulting expression with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)<st
    c="10214"><st c="10215">. However, for now</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="10234"><st c="10235">is fixed.</st> <st c="10246">From the relatively simple
    form of Eq.</st> <st c="10285">10, we can see that maximizing Eq.</st> <st c="10320">10
    with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="10339"><st c="10340">at fixed</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="10350"><st c="10351">is the same as maximizing</st> <st c="10378">the following:</st></st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mrow></math>](img/1615.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="10411">Eq.</st> <st c="10415">11</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10417">This is clearly the same as minimizing (on dropping the</st> <st
    c="10474">minus sign):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:math>](img/1616.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="10504">Eq.</st> <st c="10508">12</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10510">So, to maximize the likelihood, we minimize the expression in
    Eq.</st> <st c="10577">12\.</st> <st c="10581">Wait a minute!</st> <st c="10596">Haven’t
    we seen the expression in Eq.</st> <st c="10634">12 somewhere before, and even
    minimized it?</st> <st c="10678">The answer is yes.</st> <st c="10697">Estimating
    the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)
    <st c="10729"><st c="10730">by minimizing the expression in Eq.</st> <st c="10767">12
    is least</st> <st c="10779">squares estimation!</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10798">We covered least squares in</st> [*<st c="10827">Chapter 4</st>*](B19496_04.xhtml#_idTextAnchor216)<st
    c="10836">, so we don’t need to go into further detail in this chapter.</st> <st
    c="10898">However, it does show that we can begin to put least squares onto a
    more</st> <st c="10971">rigorous footing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10988">So far, we</st> <st c="11000">have kept the noise standard deviation</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="11039"><st c="11040">fixed.</st> <st c="11048">Do we still have this equivalence
    between maximum likelihood estimation and least squares estimation if we maximize
    the likelihood with respect to all parameters, not just</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)<st
    c="11220"><st c="11221">? Let’s see.</st> <st c="11234">To locate the maximum
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math>](img/1620.png)<st
    c="11259"><st c="11260">, we use the differential calculus we recapped in</st>
    [*<st c="11310">Chapter 1</st>*](B19496_01.xhtml#_idTextAnchor014)<st c="11319">.
    Specifically, we solve the</st> <st c="11348">following equations:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mfenced open="" close="|" separators=""><mml:mrow><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo> </mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mfenced
    open="" close="|" separators=""><mml:mrow><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo> </mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo> </mml:mo></mml:math>](img/1621.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11375">Eq.</st> <st c="11379">13</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11381">Solving the equations in Eq.</st> <st c="11411">13 will give us
    the maximum likelihood estimates</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1622.png)
    <st c="11460"><st c="11461">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1623.png)<st
    c="11466"><st c="11467">. If we take the left-hand equation in Eq.</st> <st c="11510">13
    for our particular log-likelihood expression in Eq.</st> <st c="11565">10, then
    differentiating with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="11606"><st c="11607">gives the</st> <st c="11618">following equation:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mover><mi>σ</mi><mo
    stretchy="true">ˆ</mo></mover><mn>2</mn></msup></mrow></mfrac><msub><mfenced open=""
    close="|"><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mfenced><mrow><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><munder><mover><mi>β</mi><mo stretchy="true">ˆ</mo></mover><mo
    stretchy="true">_</mo></munder></mrow></msub><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/1625.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11639">Eq.</st> <st c="11643">14</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11645">We can re-arrange this to get</st> <st c="11676">the</st> <st
    c="11680">following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mfenced open="" close="|"
    separators=""><mml:mrow><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mn>0</mml:mn><mml:mo> </mml:mo></mml:math>](img/1626.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11692">Eq.</st> <st c="11696">15</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11698">This is the criterion we would get if we were minimizing Eq.</st>
    <st c="11760">12\.</st> <st c="11764">So, in this instance, solving for the maximum
    likelihood value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="11830"><st c="11831">is the same as least squares minimization, irrespective
    of the particular value</st> <st c="11912">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)<st
    c="11915"><st c="11916">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11917">If we take the right-hand equation in Eq.</st> <st c="11960">13
    for our log-likelihood expression in Eq.</st> <st c="12004">10, then differentiating
    with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)
    <st c="12045"><st c="12046">gives the</st> <st c="12057">following equation:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mfrac><mi>N</mi><mover><mi>σ</mi><mo
    stretchy="true">ˆ</mo></mover></mfrac><mo>+</mo><mfrac><mn>1</mn><msup><mover><mi>σ</mi><mo
    stretchy="true">ˆ</mo></mover><mn>3</mn></msup></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mover><mi>β</mi><mo
    stretchy="true">ˆ</mo></mover><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow></mrow></mrow></mrow></math>](img/1630.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12078">Eq.</st> <st c="12082">16</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12084">We can re-arrange this to get</st> <st c="12115">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:munder><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:math>](img/1631.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12131">Eq.</st> <st c="12135">17</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12137">Eq.</st> <st c="12142">17 tells</st> <st c="12151">us that our
    estimate of the noise variance is just the mean squared residual once we have
    solved Eq.</st> <st c="12252">15 to get the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1622.png)<st
    c="12275"><st c="12276">. So, we can determine the maximum likelihood estimate</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1623.png)
    <st c="12331"><st c="12332">after and separately from calculating the maximum
    likelihood</st> <st c="12394">estimate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1622.png)<st
    c="12403"><st c="12404">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12405">This convenient separation of the maximum likelihood estimation
    into the estimation of the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1593.png)
    <st c="12514"><st c="12515">distinctly from the estimation of the parameters controlling
    the random component is a consequence of the noise being Gaussian additive.</st>
    <st c="12653">For many modeling problems, the random component is not Gaussian
    additive.</st> <st c="12728">So, what does maximum likelihood estimation look
    like in</st> <st c="12785">these situations?</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12802">Maximum likelihood estimation for non-additive randomness</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="12860">We</st> <st c="12863">often start with the form of the distribution
    that we think the data follows.</st> <st c="12942">For example, for an observation</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1636.png)
    <st c="12974"><st c="12975">that is a positive and continuous quantity, we might
    have good reason to model the observation using an exponential distribution.</st>
    <st c="13106">The exponential distribution has a single parameter,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/826.png)<st
    c="13159"><st c="13160">. In terms of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/826.png)
    <st c="13174"><st c="13175">we would</st> <st c="13185">write this:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>~</mo><mtext>Exponential</mtext><mfenced
    open="(" close=")"><mi>λ</mi></mfenced></mrow></mrow></math>](img/1639.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13215">Eq.</st> <st c="13219">18</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13221">The mean of an exponential distribution is easily calculated in
    terms of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/817.png)<st
    c="13295"><st c="13296">, and so we have</st> <st c="13313">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>μ</mi><mo>=</mo><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><msub><mi>y</mi><mi>i</mi></msub></mfenced><mo>=</mo><mfrac><mn>1</mn><mi>λ</mi></mfrac></mrow></mrow></math>](img/1641.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13329">Eq.</st> <st c="13333">19</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13335">Obviously, the mean</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/1642.png)
    <st c="13356"><st c="13357">is the typical value we would see for</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1643.png)<st
    c="13396"><st c="13397">, and so</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/1642.png)
    <st c="13406"><st c="13407">specifies the systematic component in the data.</st>
    <st c="13456">Now, our model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1645.png)
    <st c="13471"><st c="13472">is deterministic, so we should relate it to</st> <st
    c="13516">the systematic component of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1521.png)
    <st c="13545"><st c="13546">in some form.</st> <st c="13561">We often construct
    our model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1645.png)
    <st c="13590"><st c="13591">as being a model of the mean of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1648.png)<st
    c="13624"><st c="13625">, so we can write</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msub><mi>y</mi><mi>i</mi></msub></mfenced></mrow></mrow></math>](img/1649.png)<st
    c="13643"><st c="13644">. Given the relation in Eq.</st> <st c="13672">19 and
    equating</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msub><mi>y</mi><mi>i</mi></msub></mfenced></mrow></mrow></math>](img/1649.png)<st
    c="13688"><st c="13689">, we can re-arrange to get</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/1651.png)<st
    c="13716"><st c="13717">. Plugging this into Eq.</st> <st c="13742">18 gives</st>
    <st c="13751">us this:</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>~</mo><mtext>Exponential</mtext><mfenced
    open="(" close=")"><mfrac><mn>1</mn><msub><mover><mi>y</mi><mo stretchy="true">ˆ</mo></mover><mi>i</mi></msub></mfrac></mfenced></mrow></mrow></math>](img/1652.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13781">Eq.</st> <st c="13785">20</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13787">The probability density of the exponential distribution in Eq.</st>
    <st c="13851">18 is given by</st> <st c="13866">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><mi>λ</mi></mrow></mfenced><mo>=</mo><mi>λ</mi><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mi>λ</mi><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></math>](img/1653.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13901">Eq.</st> <st c="13905">21</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13907">So, if we plug</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/1654.png)
    <st c="13923"><st c="13924">into Eq.</st> <st c="13934">21 and take logs, we can
    write</st> <st c="13965">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub></mrow></mfenced><mo>=</mo><mo>−</mo><mi>log</mi><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub><mo>−</mo><mfrac><msub><mi>y</mi><mi>i</mi></msub><msub><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mi>i</mi></msub></mfrac></mrow></mrow></math>](img/1655.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14007">Eq.</st> <st c="14011">22</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14013">From Eq.</st> <st c="14023">22, we can then write down the log-likelihood
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="14072"><st c="14073">exponentially distributed independent observations</st>
    <st c="14125">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mi>L</mi><mo>=</mo><mo>−</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfenced
    open="[" close="]"><mrow><mi>log</mi><mover><mi>y</mi><mo stretchy="true">ˆ</mo></mover><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>+</mo><mfrac><msub><mi>y</mi><mi>i</mi></msub><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfrac></mrow></mfenced></mrow></mrow></mrow></math>](img/1657.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14161">Eq.</st> <st c="14165">23</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14167">From Eq.</st> <st c="14177">18 to Eq.</st> <st c="14187">23, we
    have gone from specifying what shape of random variation we think we will see
    in each observation through to deriving the log-likelihood in Eq.</st> <st c="14338">23\.</st>
    <st c="14342">We have done this by linking the parameters of the distribution,
    in this case, its mean</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/826.png)<st
    c="14430"><st c="14431">, to our model.</st> <st c="14447">This is the general
    approach we take when building probabilistic models.</st> <st c="14520">This approach
    can be summarized</st> <st c="14552">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14563">Specify the mathematical form of the distribution of the random
    component in</st> <st c="14641">the data.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14650">Specify a predictive model from which we calculate the parameters
    of the distribution of the</st> <st c="14744">random component.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14761">Calculate the likelihood of the data in terms of the distribution
    parameters and hence in terms of the</st> <st c="14865">predictive model.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14882">Later on in</st> <st c="14894">this chapter, we will meet other
    ways of estimating the model parameters that make use of the likelihood but do
    not use maximum likelihood estimation.</st> <st c="15046">However, even for these
    alternative methods of parameter estimation, we will still follow the three broad
    steps</st> <st c="15158">outlined above.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15173">At first sight, the process we went through to derive the log-likelihood
    in Eq.</st> <st c="15254">23 looks very different to the process we used to derive
    the log-likelihood in Eq.</st> <st c="15337">10 for independent observations with
    Gaussian additive noise.</st> <st c="15399">Actually, it isn’t.</st> <st c="15419">We’ll
    show this by re-deriving the log-likelihood in Eq.</st> <st c="15476">10 for independent
    observations with Gaussian additive noise but using the steps</st> <st c="15557">outlined
    above.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="15572">Step 1</st>**<st c="15579">: We have observations</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1521.png)
    <st c="15603"><st c="15604">that are continuous, and we think their random variation
    will cause them to follow a Gaussian (normal) distribution.</st> <st c="15722">So
    we will</st> <st c="15733">write this:</st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>~</mo><mtext>Normal</mtext><mfenced
    open="(" close=")"><mrow><msub><mi>μ</mi><mi>i</mi></msub><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/1660.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="15763">Eq.</st> <st c="15767">24</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15769">Here, we have been explicit that the mean of each observation
    can be different, while we consider the standard deviation of the random variation
    to be the same for</st> <st c="15934">each observation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="15951">Step 2</st>**<st c="15958">: We specify a mathematical form
    for a predictive model</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1661.png)<st
    c="16015"><st c="16016">, which we will use as a model of the mean parameter</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1662.png)<st
    c="16069"><st c="16070">. That is, we will put</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>μ</mi><mi>i</mi></msub><mo>=</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1663.png)<st
    c="16093"><st c="16094">, and so we</st> <st c="16106">have this:</st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>~</mo><mtext>Normal</mtext><mfenced
    open="(" close=")"><mrow><mover><mi>y</mi><mo stretchy="true">ˆ</mo></mover><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/1664.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16145">Eq.</st> <st c="16149">25</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16151">We have not specified the precise details of the predictive model
    at this stage.</st> <st c="16233">That’s because we don’t need to yet.</st> <st
    c="16270">You are free to choose.</st> <st c="16294">You could use a simple linear
    model for</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1661.png)<st
    c="16334"><st c="16335">, or you could use a neural network if you felt that was
    justified.</st> <st c="16403">You can use your favorite machine learning model
    to model the relationship between the features</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1666.png)
    <st c="16499"><st c="16503">and the expectation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="double-struck">E</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/1667.png)<st
    c="16523"><st c="16524">, as long as it is a suitably</st> <st c="16554">appropriate
    model.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="16572">Step 3</st>**<st c="16579">: The log</st> <st c="16590">of
    the probability density of a Gaussian random variable is, in terms of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1668.png)
    <st c="16663"><st c="16664">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1669.png)<st
    c="16669"><st c="16672">,</st> <st c="16674">as follow</st><st c="16683">s:</st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mi>i</mi></msub></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/1670.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16728">Eq.</st> <st c="16732">26</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16734">Using Eq.</st> <st c="16745">26 and plugging in our model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/1537.png)
    <st c="16774"><st c="16775">for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1662.png)<st
    c="16780"><st c="16781">, we get the log-likelihood of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="16812"><st c="16813">Gaussian distributed</st> <st c="16835">independent
    observations:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mi>L</mi><mo>=</mo><mo>−</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mrow></math>](img/1674.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16900">Eq.</st> <st c="16904">27</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16906">This is the same as</st> <st c="16927">Eq.</st> <st c="16931">10.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16934">Maximum likelihood for non-linear models</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="16975">It is worth noting</st> <st c="16994">that for both of the preceding
    examples, our predictive model was directly a model of the expectation value of
    the observations.</st> <st c="17124">That is, we equated,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>|</mo><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub></mrow></mfenced><mo>=</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1675.png)<st
    c="17145"><st c="17146">. This doesn’t always have to be the case.</st> <st c="17189">The
    wording of step 2 was very deliberate –</st> *<st c="17233">“Specify a predictive
    model from which we calculate the parameters of the distribution of the random
    component.”</st>* <st c="17345">Imagine we have a predictive model</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1676.png)<st
    c="17381"><st c="17382">. We can apply transformations to the output of our predictive
    model</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1677.png)
    <st c="17451"><st c="17452">to calculate the expectation value of the observation
    variable</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)<st
    c="17516"><st c="17526">. In this case, we would equate</st> <st c="17558">the
    following:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><mi>y</mi><mo>|</mo><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/1679.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="17593">Eq.</st> <st c="17597">28</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17599">The function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="17613"><st c="17614">is called a</st> **<st c="17627">link function</st>**<st
    c="17640">. It links</st> <st c="17650">the expectation of the observations to
    the predictive model.</st> <st c="17712">It is traditional to specify the transformation
    as the inverse of a function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1681.png)<st
    c="17789"><st c="17790">. Now, you may ask, why not just include the final transformation</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/1682.png)
    <st c="17856"><st c="17857">in the definition of our predictive model</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi></mrow></math>](img/1683.png)<st
    c="17900"><st c="17901">? This is because the terminology of “link functions”
    comes from classical statistics, where our predictive model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="18015"><st c="18069">would typically be linear.</st> <st c="18096">The
    downstream application of a transformation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>](img/1685.png)
    <st c="18143"><st c="18144">was then a way of modeling non-linear relationships
    in</st> <st c="18200">classical statistics.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18221">The best</st> <st c="18231">way to understand how link functions
    are used is to see one used in practice.</st> <st c="18309">We’ll model shoppers’
    decisions about whether to buy a product or not.</st> <st c="18380">We have</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="18388"><st c="18389">shoppers in our dataset and we know whether they bought
    the product in question or not.</st> <st c="18478">We’ll represent the outcome
    of the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/909.png)
    <st c="18513"><st c="18525">shopper’s decision by the binary outcome variable</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1688.png)<st
    c="18575"><st c="18576">. So,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/1689.png)
    <st c="18582"><st c="18583">if the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/1690.png)
    <st c="18591"><st c="18595">shopper didn’t buy the product, and</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/1691.png)
    <st c="18631"><st c="18632">if they did.</st> <st c="18646">The outcome</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1692.png)
    <st c="18658"><st c="18659">can be affected by several factors, such as the price
    of the product at the time, or the age demographic the shopper is in.</st> <st
    c="18784">We can put all the relevant factors into our feature</st> <st c="18837">vector</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1693.png)<st
    c="18844"><st c="18848">.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18849">Now let’s follow our 3-step recipe to derive</st> <st c="18895">the
    log-likelihood.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="18914">Step 1</st>**<st c="18921">: The outcome variable</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1694.png)
    <st c="18945"><st c="18946">is a random variable.</st> <st c="18969">Why is this
    so?</st> <st c="18985">Well, even if the price and other factors in</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1695.png)
    <st c="19030"><st c="19034">are such that you’d think a particular shopper was
    ideally suited to the product, they may not buy it.</st> <st c="19137">The expectation
    of this random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="double-struck">E</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/1696.png)<st
    c="19178"><st c="19179">, is the probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1697.png)
    <st c="19200"><st c="19201">that the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/909.png)
    <st c="19211"><st c="19223">shopper will buy the product.</st> <st c="19253">The
    variable</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1699.png)
    <st c="19266"><st c="19267">is an example of a Bernoulli random variable that
    we introduced in</st> [*<st c="19335">Chapter 2</st>*](B19496_02.xhtml#_idTextAnchor061)<st
    c="19344">. Modeling the shopper choice as a Bernoulli random variable is our
    choice for the mathematical form of the randomness in the</st> <st c="19470">observed
    data.</st></st></st></st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="19484">Step 2</st>**<st c="19491">: Since</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1700.png)
    <st c="19500"><st c="19501">is a discrete random variable, unlike our previous
    two examples, we write down the probability distribution for</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1701.png)
    <st c="19614"><st c="19615">instead of a probability density.</st> <st c="19650">We
    need to express this probability distribution in terms of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1697.png)<st
    c="19711"><st c="19712">. For a Bernoulli distribution, this is straightforward
    to do, and we</st> <st c="19782">get t</st><st c="19787">his:</st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>P</mtext><mfenced
    open="(" close=")"><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>|</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mfenced><mo>=</mo><msubsup><mi>p</mi><mi>i</mi><msub><mi>n</mi><mi>i</mi></msub></msubsup><msup><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mfenced><mrow><mn>1</mn><mo>−</mo><msub><mi>n</mi><mi>i</mi></msub></mrow></msup></mrow></mrow></math>](img/1703.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="19815">Eq.</st> <st c="19819">29</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19821">Plugging in values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/1704.png)
    <st c="19844"><st c="19845">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/1691.png)
    <st c="19850"><st c="19851">into Eq.</st> <st c="19861">29 should convince you
    that we recover the correct probabilities.</st> <st c="19927">On the log scale,
    we have</st> <st c="19953">the following:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mfenced
    open="(" close=")"><mrow><mtext>P</mtext><mfenced open="(" close=")"><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>|</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi>n</mi><mi>i</mi></msub><mi>log</mi><msub><mi>p</mi><mi>i</mi></msub><mo>+</mo><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>n</mi><mi>i</mi></msub></mrow></mfenced><mi>log</mi><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></math>](img/1706.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="20012">Eq.</st> <st c="20016">30</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20018">The probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1707.png)
    <st c="20035"><st c="20036">depends upon the feature vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1708.png)<st
    c="20069"><st c="20073">, and it is this relationship that we will build our predictive
    model for.</st> <st c="20148">We’ll consider the simplest possible form for our
    predictive</st> <st c="20209">model, namely a linear one.</st> <st c="20237">Without
    loss of generality, we can write this linear model in the form</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1709.png)
    <st c="20308"><st c="20309">, where we have used our usual trick of including
    a constant by having a feature</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/1710.png)
    <st c="20390"><st c="20391">for all shoppers, with a corresponding parameter</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/1711.png)<st
    c="20441"><st c="20442">. However, we have a problem!</st> <st c="20472">Our linear
    model can potentially take any value between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1712.png)
    <st c="20528"><st c="20529">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1713.png)<st
    c="20534"><st c="20535">, but a probability can only be between 0 and 1\.</st>
    <st c="20584">So, we need a transformation that will map the whole real line</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>(</mo><mo>−</mo><mi
    mathvariant="normal">∞</mi><mo>,</mo><mi mathvariant="normal">∞</mi><mo>)</mo></mrow></mrow></mrow></math>](img/1714.png)
    <st c="20647"><st c="20648">to the interval [0,1].</st> <st c="20672">To do this,
    we</st> <st c="20687">use a simple sigmoid transformation called the</st> **<st
    c="20734">logistic fu</st><st c="20745">nction</st>**<st c="20752">:</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mi>p</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mfrac><mrow><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><msubsup><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi><mi
    mathvariant="normal">⊤</mi></msubsup><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow><mrow><mn>1</mn><mo>+</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><msubsup><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi><mi
    mathvariant="normal">⊤</mi></msubsup><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><msubsup><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi><mi
    mathvariant="normal">⊤</mi></msubsup><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/1715.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="20798">Eq.</st> <st c="20802">31</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20804">The quantity</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1716.png)
    <st c="20818"><st c="20819">is called</st> <st c="20830">the</st> **<st c="20834">linear
    predictor</st>** <st c="20850">because it is the linear part of the prediction
    model.</st> <st c="20906">It is often denoted by the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi></mml:math>](img/1717.png)<st
    c="20940"><st c="20941">. The plot in</st> *<st c="20955">Figure 5</st>**<st c="20963">.1</st>*
    <st c="20965">shows how the logistic function transformation function in Eq.</st>
    <st c="21029">31 varies with the linear</st> <st c="21055">pred</st><st c="21059">ictor</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi></mml:math>](img/1482.png)<st
    c="21066"><st c="21067">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure\uFEFF 5.1: The logistic function](img/B19496_05_1.jpg)"
  prefs: []
  type: TYPE_IMG
- en: '<st c="21091">Figure 5.1: The logistic function</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21124">The plot in</st> *<st c="21137">Figure 5</st>**<st c="21145">.1</st>*
    <st c="21147">is a plot of the function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1719.png)<st
    c="21174"><st c="21180">, with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced
    separators="|"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1719.png)
    <st c="21187"><st c="21193">given by</st> <st c="21202">the fol</st><st c="21209">lowing:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msup><mi>g</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mfenced
    open="(" close=")"><mi>η</mi></mfenced><mo>=</mo><mfrac><mtext>1</mtext><mrow><mn>1</mn><mo>+</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mi>η</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math>](img/1721.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21237">Eq.</st> <st c="21241">32</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21243">This</st> <st c="21248">function is the inverse of our link function,
    so what does the link function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1722.png)
    <st c="21326"><st c="21327">look like?</st> <st c="21339">If we invert Eq.</st>
    <st c="21356">32, we</st> <st c="21363">get this:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>g</mi><mfenced
    open="(" close=")"><mi>p</mi></mfenced><mo>=</mo><mi>log</mi><mfenced open="("
    close=")"><mfrac><mi>p</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mfenced></mrow></mrow></math>](img/1723.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21389">Eq.</st> <st c="21393">33</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21395">This is our link function.</st> <st c="21423">It is called</st>
    <st c="21435">the</st> **<st c="21440">logit function</st>**<st c="21454">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="21455">Step 3</st>**<st c="21462">: Now that we have our predictive
    model, we can calculate the log-likelihood.</st> <st c="21541">For each of our</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="21557"><st c="21558">shoppers, we have the observation</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1725.png)
    <st c="21593"><st c="21594">and the feature vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1693.png)<st
    c="21618"><st c="21622">. Putting together Eq.</st> <st c="21645">30 and Eq.</st>
    <st c="21656">31, we</st> <st c="21663">get this:</st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:munderover><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mfenced
    open="[" close="]" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mfenced separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mfenced
    separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mfenced separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mfenced
    separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:munder><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/1727.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfenced
    open="[" close="]"><mrow><msub><mi>n</mi><mi>i</mi></msub><msubsup><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder><mo>−</mo><mi>log</mi><mfenced open="(" close=")"><mrow><mn>1</mn><mo>+</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><msubsup><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi><mi
    mathvariant="normal">⊤</mi></msubsup><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mfenced></mrow></mrow></mrow></math>](img/1728.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21705">Eq.</st> <st c="21709">34</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21711">The log-likelihood in Eq.</st> <st c="21738">34 can then be maximized
    with respect to the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)
    <st c="21800"><st c="21801">to give the maximum likelihood estimate for</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1730.png)<st
    c="21846"><st c="21847">. From this example, we can see how we can use maximum
    likelihood to train a model that includes a</st> <st c="21946">link function.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21960">That is enough for now about likelihood, so let’s finish this
    section by summarizing what we</st> <st c="22054">have learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22067">What we have learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="22088">In this section, we have learned</st> <st c="22122">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22136">How the likelihood measures the probability of a dataset given
    the parameters of a predictive model and the parameters of a</st> <st c="22261">random
    process</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="22275">How maximum likelihood provides us with a way to use the likelihood
    to estimate the model parameters</st> <st c="22377">from data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="22386">How maximum likelihood estimation of a predictive model’s parameters
    corresponds to least squares estimation of the parameters, when we have data that
    contains i.i.d.</st> <st c="22554">Gaussian</st> <st c="22563">additive noise</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="22577">How to formulate a probabilistic model for any modeling problem
    using a short series</st> <st c="22663">of steps</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="22671">Having learned about likelihood as the probability of the data
    given the model parameters, in the next section, we will move on to learning how
    to formulate the probability of the model parameters given</st> <st c="22875">the
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22884">Bayes’ theorem</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="22899">When</st> <st c="22905">we learned about maximum likelihood for
    estimating the parameters of a model, it felt like an intuitively sensible thing
    to do.</st> <st c="23033">Who can argue with the idea of choosing the model parameters</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)
    <st c="23094"><st c="23095">so that we have the highest possible probability of
    obtaining the data we have actually observed?</st> <st c="23194">But we didn’t
    really derive maximum likelihood in any formal way.</st> <st c="23260">Yes, choosing
    parameters by maximizing</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model</mtext></mrow></mrow></math>](img/1732.png)<st
    c="23299"><st c="23314">) seems reasonable, but aren’t we really interested in
    the probability of the parameters given the data, that is</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Model</mtext></mrow></mrow></math>](img/1733.png)<st
    c="23427"><st c="23436">| Data)?</st> <st c="23445">Working with the likelihood</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model)</mtext></mrow></mrow></math>](img/1734.png)
    <st c="23473"><st c="23489">seems close to what we want, but not quite there.</st>
    <st c="23539">If only there was a way we could calculate</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext></mrow></mrow></math>](img/1735.png)
    <st c="23582"><st c="23599">from</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model)</mtext></mrow></mrow></math>](img/1734.png)<st
    c="23604"><st c="23620">. There is.</st> <st c="23632">Enter</st> **<st c="23638">Bayes’
    theorem</st>**<st c="23652">.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23653">This section will be relatively short as we will only introduce
    Bayes’ theorem here.</st> <st c="23739">In the next two sections, we will explain
    how Bayes’ theorem is used</st> <st c="23808">in practice.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23820">Conditional probability and Bayes’ theorem</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23863">Bayes’ theorem, named</st> <st c="23885">after the Reverend Thomas
    Bayes, is about conditional probabilities.</st> <st c="23955">The probability</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model)</mtext></mrow></mrow></math>](img/1737.png)
    <st c="23971"><st c="23987">is a conditional probability.</st> <st c="24017">It
    is the probability of the data</st> **<st c="24051">given</st>** <st c="24056">the
    model, or in other words the probability of the data conditional on having that
    particular model form with that particular set of model</st> <st c="24197">parameter
    values.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24214">Bayes’ theorem says that if we have two events</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1738.png)
    <st c="24262"><st c="24263">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1739.png)
    <st c="24268"><st c="24269">(things whose probabilities we are interested in knowing),
    then the conditional probabilities</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1740.png)
    <st c="24364"><st c="24371">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    close="|" separators="|"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1741.png)
    <st c="24375"><st c="24376">are related via</st> <st c="24393">th</st><st c="24395">is
    formula:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>B</mi><mo>|</mo><mi>A</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>A</mi><mo>|</mo><mi>B</mi></mrow></mfenced><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/1742.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="24433">Eq.</st> <st c="24437">35</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24439">You may wonder where Eq.</st> <st c="24465">35 comes from.</st>
    <st c="24480">The form in Eq.</st> <st c="24496">35 hides its simplicity.</st>
    <st c="24521">Consider, instead the joint probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1743.png)<st
    c="24561"><st c="24562">. Now, from the rules of conditional probability,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1744.png)<st
    c="24612"><st c="24632">. In words, the probability of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1738.png)
    <st c="24663"><st c="24664">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1746.png)
    <st c="24669"><st c="24670">happening together is the probability of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1738.png)
    <st c="24712"><st c="24713">happening given I know</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1739.png)
    <st c="24737"><st c="24738">has happened, multiplied by the probability that</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1739.png)
    <st c="24788"><st c="24789">has indeed actually happened.</st> <st c="24820">But
    here’s the neat trick.</st> <st c="24847">The joint probability</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></mfenced><mo>=</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>,</mo><mi>A</mi><mo>)</mo></mrow></mrow></mrow></math>](img/1750.png)
    <st c="24869"><st c="24886">is just the probability of both</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1751.png)
    <st c="24918"><st c="24919">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1752.png)
    <st c="24924"><st c="24925">happening together, or both being true, so the ordering
    doesn’t matter.</st> <st c="24998">This means we can also write</st> <st c="25027">P</st><st
    c="25028">(</st><st c="25029">A</st><st c="25030">,</st> <st c="25031">B</st><st
    c="25032">)</st> <st c="25033">=</st> <st c="25034">P</st><st c="25035">(</st><st
    c="25036">B</st> <st c="25037">|</st> <st c="25038">A</st><st c="25039">)</st><st
    c="25040">P</st><st c="25041">(</st><st c="25042">A</st><st c="25043">)</st><st
    c="25044">. Again, in words this make sense – it is the probability of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi></mml:math>](img/1753.png)
    <st c="25105"><st c="25106">happening given I know</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1754.png)
    <st c="25130"><st c="25131">has happened, multiplied by the probability that</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi></mml:math>](img/1754.png)
    <st c="25181"><st c="25182">has indeed actually happened.</st> <st c="25213">If
    we now equate these two expressions for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:math>](img/1743.png)
    <st c="25256"><st c="25257">w</st><st c="25259">e</st> <st c="25261">get this:</st></st></st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1757.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25303">Eq.</st> <st c="25307">36</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25309">If we re-arrange Eq.</st> <st c="25331">36, we get Bayes’ theorem
    in Eq.</st> <st c="25364">35\.</st> <st c="25368">In fact, the expression in Eq.</st>
    <st c="25399">36 is how I always remember Bayes’ theorem when doing Bayesian modeling.</st>
    <st c="25472">If I have to derive or understand what looks like a complicated
    Bayesian formula, I just write out the joint probability of whatever events (</st><st
    c="25613">A</st><st c="25615">,</st> <st c="25616">B</st><st c="25617">,</st>
    <st c="25618">…</st> <st c="25619">.</st><st c="25620">)</st> <st c="25622">I’m
    interested in (in the multiple ways involving conditional probabilities), and
    then I</st> <st c="25712">re-arrange it.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25726">So, how does Bayes’ theorem help us with working out</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext></mrow></mrow></math>](img/1758.png)<st
    c="25780"><st c="25797">? Well, let’s put</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mtext>Data</mml:mtext></mml:math>](img/1759.png)
    <st c="25815"><st c="25816">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mtext>Model</mml:mtext></mml:math>](img/1760.png)<st
    c="25821"><st c="25831">, and plug them into Bayes’ theorem in Eq.</st> <st c="25874">35\.</st>
    <st c="25878">W</st><st c="25879">e</st> <st c="25881">get this:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data</mtext><mtext>)</mtext><mtext>=</mtext><mfrac><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model)</mtext><mtext>×</mtext><mi>P</mi><mtext>(Model)</mtext></mrow><mrow><mi>P</mi><mtext>(Data)</mtext></mrow></mfrac></mrow></mrow></math>](img/1761.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25949">Eq.</st> <st c="25953">37</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25955">Excellent!</st> <st c="25967">We have a means of</st> <st c="25986">calculating</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext></mrow></mrow></math>](img/1762.png)<st
    c="25998"><st c="26015">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26016">We have</st> <st c="26025">written Bayes’ theorem in terms of
    probabilities.</st> <st c="26075">However, for many of our discussions on likelihood,
    we have been talking about probability densities.</st> <st c="26177">As you might
    expect, we can also apply Bayes’ theorem to probability densities by replacing
    the probabilities in Eq.</st> <st c="26294">37 with the corresponding densities.</st>
    <st c="26331">From now on, when discussing Bayes’ theorem, we will for brevity
    refer to probabilities even when we mean probability densities.</st> <st c="26460">It
    will be clear from the context whether we are dealing with a probability or a</st>
    <st c="26541">probability density.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26561">Finally, we should emphasize that when we refer to a model, for
    example, in Eq.</st> <st c="26642">37, we mean both the parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1561.png)
    <st c="26674"><st c="26675">for the predictive model, but also the parameters
    that control the random component of the data, such as the noise variance</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1764.png)
    <st c="26800"><st c="26801">in our model in Eq.</st> <st c="26822">1\.</st> <st
    c="26825">We will use the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1765.png)
    <st c="26848"><st c="26849">in general to represent the combined parameters from
    the predictive model and the random component, so</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><mfenced open="(" close=")"><mrow><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/1766.png)<st
    c="26953">![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><mfenced open="(" close=")"><mrow><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/1767.png)
    <st c="26954"><st c="26955">for our model in Eq.</st> <st c="26977">1\.</st> <st
    c="26980">We will use the word “model” and the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1765.png)
    <st c="27024"><st c="27025">interchangeably.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27041">Priors</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="27048">Let’s</st> <st c="27054">unpack Eq.</st> <st c="27066">37 in a
    bit more detail.</st> <st c="27091">Firstly, what is the quantity</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mtext>(Model)</mml:mtext></mml:math>](img/1769.png)<st
    c="27121"><st c="27131">? At face value it is the probability of the model.</st>
    <st c="27183">But conditional on what?</st> <st c="27208">Well, nothing.</st>
    <st c="27223">Not on the data.</st> <st c="27240">So, it is just the probability
    of the model before, or prior, to us receiving the data.</st> <st c="27328">For
    this reason, it is called a prior probability, or simply</st> <st c="27389">a</st>
    **<st c="27391">prior</st>**<st c="27396">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27397">The symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1770.png)
    <st c="27409"><st c="27410">summarizes the model, so</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mtext>(Model)</mml:mtext></mml:math>](img/1771.png)
    <st c="27436"><st c="27446">is</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1772.png)
    <st c="27449"><st c="27450">and is the probability we attach to the parameters</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1770.png)
    <st c="27502"><st c="27503">before we have seen any data.</st> <st c="27534">It
    therefore encapsulates our pre-defined beliefs about what values</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1765.png)
    <st c="27602"><st c="27603">should take.</st> <st c="27617">So, in most cases,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>)</mml:mo></mml:math>](img/1775.png)
    <st c="27636"><st c="27637">is subjective – the probability that you might attach
    to a particular value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1765.png)
    <st c="27717"><st c="27718">could be different from the probability that I attach
    to that same value</st> <st c="27792">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1765.png)<st
    c="27795"><st c="27796">.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27797">This subjective element of the prior is the reason why some people
    are not keen on Bayesian methods.</st> <st c="27899">Consequently, an alternative
    and often used approach is to use an</st> **<st c="27965">uninformative prior</st>**<st
    c="27984">; that is a</st> <st c="27996">prior that is not based on any subjective
    belief, but only on incontrovertible facts we know about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="28096"><st c="28097">, such as the upper and lower bounds for parameter values
    or based on the geometry of the space in which the</st> <st c="28206">parameters
    lie.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28221">Whether we use</st> <st c="28237">a subjective prior or uninformative
    prior, in both cases we are constructing the prior from information.</st> <st
    c="28342">In the case of an uninformative prior, that information is the minimal
    properties that the parameters must satisfy, while in the case where I have constructed
    the prior from my own subjective beliefs, I am constructing the prior based upon
    information coming from my expert judgment, possibly based on years of domain
    experience.</st> <st c="28672">So, another way to think about the prior</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1779.png)
    <st c="28713"><st c="28714">is that it is the distribution of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1780.png)
    <st c="28749"><st c="28750">based upon whatever information we have available
    to us before we have received the data for the current analysis.</st> <st c="28866">The
    prior information that we have available to us could even be from a</st> <st c="28938">previous
    analysis.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28956">The posterior</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28970">We dealt</st> <st c="28980">with and explained the factor</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1781.png)
    <st c="29010"><st c="29011">in the Eq.</st> <st c="29023">37 form of Bayes’ theorem.</st>
    <st c="29050">What about the probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mtext>Data)</mml:mtext></mml:math>](img/1782.png)
    <st c="29077"><st c="29085">in the denominator of Eq.</st> <st c="29111">37?</st>
    <st c="29115">It looks like a prior.</st> <st c="29138">The more appropriate way
    to interpret</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mtext>(Data)</mml:mtext></mml:math>](img/1783.png)
    <st c="29176"><st c="29185">is as a normalizing factor for the probability</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext></mrow></mrow></math>](img/1784.png)<st
    c="29232"><st c="29249">. From the rules of probability, we</st> <st c="29285">have
    this:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mtext>(Data)</mtext><mtext>=</mtext><mo>∫</mo><mrow><mrow><mi>P</mi><mo>(</mo><mtext>Data,</mtext><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow><mo>)</mo><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><mo>∫</mo><mi>P</mi><mfenced open="("
    close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mfenced><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></mrow></math>](img/1785.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="29346">Eq.</st> <st c="29350">38</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29352">This means we can write Bayes’ theorem in Eq.</st> <st c="29399">37</st>
    <st c="29402">as</st> <st c="29405">follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mfenced><mtext>Data)</mtext><mo>=</mo><mfrac><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mi>P</mi><mfenced open="(" close=")"><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mfenced></mrow><mrow><mo>∫</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Data</mtext><mtext>|</mtext><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mi>P</mi><mfenced open="(" close=")"><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mfenced><mi>d</mi><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mrow></mfrac></mrow></mrow></math>](img/1786.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="29463">Eq.</st> <st c="29467">39</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29469">The denominator is just making sure the probabilities over all
    possible values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="29552"><st c="29553">add up</st> <st c="29561">to 1.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29566">Let’s return to the numerator of Eq.</st> <st c="29604">37\.</st>
    <st c="29608">The probability</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mtext>(Data</mtext><mtext>|</mtext><mtext>Model)</mtext></mrow></mrow></math>](img/1788.png)
    <st c="29624"><st c="29640">is the likelihood.</st> <st c="29659">This means Bayes’
    theorem can be written</st> <st c="29700">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext><mtext>∝</mtext><mtext>Likelihood</mtext><mtext>×</mtext><mtext>Prior</mtext></mrow></mrow></math>](img/1789.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="29750">Eq.</st> <st c="29754">40</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29756">We can just re-arrange tha</st><st c="29783">t to</st> <st c="29789">write
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mtext>(Model</mtext><mtext>|</mtext><mtext>Data)</mtext><mtext>∝</mtext><mtext>Prior</mtext><mtext>×</mtext><mtext>Likelihood</mtext></mrow></mrow></math>](img/1790.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="29839">Eq.</st> <st c="29843">41</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29845">How</st> <st c="29850">does Eq.</st> <st c="29859">41 help us?</st>
    <st c="29871">It tells us that the probability of the model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1791.png)
    <st c="29917"><st c="29918">given the data is our prior probability multiplied
    by the likelihood (and appropriately normalized).</st> <st c="30020">The likelihood
    has updated the distribution of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="30067"><st c="30068">from what we believed before we got the data – the
    prior</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1793.png)
    <st c="30126"><st c="30127">– to what we believe after we get the data – the distribution</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1794.png)<st
    c="30190"><st c="30202">. Because of this,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1795.png)
    <st c="30221"><st c="30233">is called the</st> **<st c="30247">posterior distribution</st>**
    <st c="30269">or</st> <st c="30273">simply the</st> **<st c="30284">posterior</st>**<st
    c="30293">. In simple terms, the prior is what we think is the distribution of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="30362">**<st c="30363">before</st>** <st c="30369">we get the data, while
    the posterior is what we think is the distribution of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="30447">**<st c="30448">after</st>** <st c="30453">we get the data – hence
    the</st> <st c="30482">name posterior.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30497">We said this section would be short, so we’ll now recap what we</st>
    <st c="30562">have learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30575">What we have learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30596">In this section, we have learned about</st> <st c="30636">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30650">Bayes’ theorem and how we can use it to calculate the probability
    distribution of the model parameters given</st> <st c="30760">the data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30768">Prior distributions and how they encode the beliefs we already
    have about model parameters before we have received any new data</st> <st c="30897">or
    information</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30911">The posterior distribution and how it is calculated by multiplying
    the likelihood of the data and</st> <st c="31010">the prior</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="31019">How the posterior distribution represents our belief of the distribution
    of the model parameters after we have received the new data</st> <st c="31153">or
    information</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="31167">Having introduced Bayes’ theorem and the concepts of prior and
    posterior distributions, in the next section, we’re going to move onto how the
    posterior distribution is used in</st> <st c="31344">Bayesian modeling.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31362">Bayesian modeling</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="31380">The</st> <st c="31385">posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1798.png)
    <st c="31395"><st c="31407">encapsulates the philosophy of Bayesian modeling and
    changes how we view the model represented by</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="31505"><st c="31506">. In Bayesian modeling, there is not a single “correct”
    underlying value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="31582"><st c="31583">, for which we construct uncertain estimates.</st> <st
    c="31629">Instead, different values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="31658"><st c="31659">have different probabilities given the available data
    or evidence.</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1791.png)
    <st c="31727"><st c="31728">is a random variable, and we update what we think
    is the distribution of that random variable using Bayes’ theorem and the additional
    data or information</st> <st c="31883">we receive.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31894">With that statement about the philosophical interpretation of
    the posterior made, we now move on to how we use the posterior in a calculational
    sense.</st> <st c="32046">There are two potential ways in which we can use the</st>
    <st c="32099">posterior distribution:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32122">To evaluate expectation values.</st> <st c="32155">Here, we are
    using the posterior as it is intended, as a distribution.</st> <st c="32226">Here,
    the posterior is used to calculate predictions over lots of different</st> <st
    c="32301">models.</st> <st c="32310">This is called</st> *<st c="32325">Bayesian</st>*
    *<st c="32334">model averaging</st>*<st c="32349">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="32350">To identify a suitable single value, or point estimate, of the
    parameter vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="32431"><st c="32432">that we can use as a single model to make predictions
    with.</st> <st c="32493">The most obvious point value we can use is the value
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="32549"><st c="32550">that is the most probable given the data.</st> <st
    c="32593">We can identify this value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="32623"><st c="32624">by maximizing the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1806.png)
    <st c="32653"><st c="32665">with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="32681"><st c="32682">. Since we are maximizing the</st> <st c="32712">posterior,
    this is known as the</st> **<st c="32744">maximum a posteriori</st>** <st c="32764">estimate,
    or</st> **<st c="32778">MAP</st>** <st c="32781">estimate for short.</st> <st
    c="32802">The MAP estimate of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="32822"><st c="32823">satisfies this:</st></st></st></st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub><mo>=</mo><mtable
    columnwidth="auto" columnalign="center" rowspacing="1.0000ex" rowalign="baseline
    baseline"><mtr><mtd><mtext>argmax</mtext></mtd></mtr><mtr><mtd><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mtd></mtr></mtable><mi>P</mi><mfenced open="("
    close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1809.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="32865">Eq.</st> <st c="32869">42</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32871">We will normally try and identify MAP estimates by solving the</st>
    <st c="32935">stationarity condition:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mfenced
    open="" close="|"><mfrac><mrow><mo>∂</mo><mi>P</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow><mrow><mo>∂</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfrac></mfenced><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></msub><mo>=</mo><munder><mn>0</mn><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/1810.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="32975">Eq.</st> <st c="32979">43</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32981">Bayesian model averaging and MAP estimation both have their advantages
    and disadvantages.</st> <st c="33072">It is worth covering those advantages and
    disadvantages in detail, so we will do</st> <st c="33153">so next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33161">Bayesian model averaging</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33186">Bayesian model averaging</st> <st c="33212">can be as simple as
    wanting to know what the typical value of the model parameters is given all the
    data or information available to us to date.</st> <st c="33357">In this case,
    we would calculate the expectation</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1811.png)
    <st c="33406"><st c="33418">over the posterior distribution, so in other words,
    we</st> <st c="33473">calculate this:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>=</mo><mo>∫</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>P</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/1812.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="33520">Eq.</st> <st c="33524">44</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33526">We can also use Bayesian model averaging to calculate the posterior
    covariance of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="33609"><st c="33610">. This would be calculated</st> <st c="33637">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><msup><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi
    mathvariant="normal">⊤</mi></msup><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>−</mo><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><msup><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi mathvariant="normal">⊤</mi></msup><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>=</mo><mo>∫</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><msup><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi
    mathvariant="normal">⊤</mi></msup><mi>P</mi><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>−</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><msup><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi mathvariant="normal">⊤</mi></msup><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1814.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="33734">Eq.</st> <st c="33738">45</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33740">Bayesian averaging gives us a convenient method for quantifying
    the spread of values that are possible for the model parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="33869"><st c="33870">given the data we</st> <st c="33889">have received.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33903">In another situation, we might want to calculate the model prediction
    for some new feature value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="34001"><st c="34002">. We want to calculate</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1817.png)<st
    c="34025"><st c="34026">. But what value of model parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)
    <st c="34063"><st c="34064">do we use?</st> <st c="34076">Obviously, we want to
    use values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)
    <st c="34112"><st c="34113">that are guided by the data.</st> <st c="34143">However,
    remember in a Bayesian framework that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)
    <st c="34190"><st c="34191">is a random variable, so this makes the prediction</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1821.png)
    <st c="34243"><st c="34244">also a random variable.</st> <st c="34269">The solution
    is to calculate the expectation value of the prediction</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mover
    accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math>](img/1537.png)
    <st c="34338"><st c="34339">over the posterior distribution.</st> <st c="34373">So,
    we ca</st><st c="34382">lculate</st> <st c="34391">the following:</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>=</mo><mo>∫</mo><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/1823.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="34455">Eq.</st> <st c="34459">46</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34461">You may ask why the expression in Eq.</st> <st c="34500">46 is
    an average over the parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="34537"><st c="34538">and not over just</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1730.png)<st
    c="34557"><st c="34558">? After all, it is the average of the prediction we are
    interested in calculating, and the predictive model only depends on the parameter</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1826.png)<st
    c="34696"><st c="34697">. The answer is that there may be correlations</st> <st
    c="34743">between the parameter</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1730.png)
    <st c="34766"><st c="34767">and the parameters controlling the random component
    in the data.</st> <st c="34833">So when, say,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)
    <st c="34847"><st c="34850">changes, we can’t ignore the fact that this will change
    the probability of a given value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1829.png)<st
    c="34942"><st c="34943">. Consequently, we must average over both</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1830.png)
    <st c="34985"><st c="34986">and any parameters controlling the random component
    in the data, i.e., we average</st> <st c="35069">over</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="35074"><st c="35075">.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35076">Bayesian model averaging constructs a consensus from many models</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="35141">The</st> <st c="35146">expression in Eq.</st> <st c="35164">46
    is useful because it allows us to calculate the typical value we would expect
    from our prediction, given the new input feature</st> <st c="35294">vector</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="35301"><st c="35302">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35303">One of the real benefits of Bayesian model averaging is the fact
    that we are taking an average over multiple models.</st> <st c="35421">The models
    that contribute most to the average in Eq.</st> <st c="35475">46 will be those
    that have the highest probability given the data.</st> <st c="35542">There can
    be many models that are nearly all equally likely but some of which may give very
    different values for the prediction</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1833.png)<st
    c="35670"><st c="35671">. Averaging over all these high-probability models ensures
    we have a suitable and sensible consensus for our prediction of what will happen
    at the new feature vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="35837"><st c="35838">. In this way, we can think of Bayesian model averaging
    as a formal and rigorous way of doing model averaging, or committee voting, which
    are well-known techniques in</st> <st c="36005">machine learning.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36022">The expression in Eq.</st> <st c="36045">46 is a mathematical
    one.</st> <st c="36071">It doesn’t tell us how to calculate the Bayesian average
    in practice.</st> <st c="36141">Doing so can be hard.</st> <st c="36163">We will
    learn about some computational</st> <st c="36201">techniques in the next section,
    such as</st> **<st c="36242">Markov Chain Monte Carlo</st>** <st c="36266">(</st>**<st
    c="36268">MCMC</st>**<st c="36272">) simulation, that approximate the calculation
    of the Bayesian average in</st> <st c="36347">Eq.</st> <st c="36351">46.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36354">Another way to approximate the calculation in Eq.</st> <st c="36405">46
    is to assume that a single model is representative of all the high probability
    models, and also representative in any downstream calculations, such as the calculation
    of the prediction</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><munder><mi>β</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1817.png)<st
    c="36593"><st c="36594">. If we are comfortable with this assumption, then the
    most sensible single model to use is the MAP</st> <st c="36694">estimate</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)<st
    c="36703"><st c="36708">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36709">MAP estimation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="36724">For</st> <st c="36729">our model in Eq.</st> <st c="36746">1,
    our model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="36770"><st c="36771">are</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/1838.png)<st
    c="36776"><st c="36777">, so</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub><mo>=</mo><mfenced
    open="(" close=")"><mrow><msub><munder><mi>β</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub><mo>,</mo><msubsup><mi>σ</mi><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow><mn>2</mn></msubsup></mrow></mfenced></mrow></mrow></math>](img/1839.png)
    <st c="36782"><st c="36800">and we would make predictions using the model</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mi>y</mi><mo
    stretchy="true">ˆ</mo></mover><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><munder><mi>β</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/1840.png)<st
    c="36846"><st c="36847">. Obviously, when using just</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1841.png)
    <st c="36876"><st c="36881">we don’t get the benefits</st> <st c="36906">of Bayesian
    model averaging, but sometimes the posterior distribution is so tightly distributed
    around its maximum at</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1842.png)
    <st c="37025"><st c="37030">that only a small range of models</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1843.png)
    <st c="37064"><st c="37065">have any reasonable posterior probability associated
    with them, and they are all reasonably well approximated by the MAP</st> <st c="37187">value</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)<st
    c="37193"><st c="37198">.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37199">This is equivalent to approximating the true posterior</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced open="("
    close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1845.png)
    <st c="37255"><st c="37267">by a delta function.</st> <st c="37288">That is,</st>
    <st c="37296">we are approximating in the</st> <st c="37325">following way:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>≈</mo><mi>δ</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/1846.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37368">Eq.</st> <st c="37372">47</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37374">Now, remember from</st> [*<st c="37394">Chapter 2</st>*](B19496_02.xhtml#_idTextAnchor061)
    <st c="37403">that we can loosely think of a Dirac delta function as an infinitely
    narrow, infinitely high spike.</st> <st c="37504">When would this be a good approximation
    of the true posterior</st> <st c="37566">P</st><st c="37567">(</st><st c="37568">θ</st><st
    c="37569">_</st> <st c="37570">|</st> <st c="37571">Data</st><st c="37575">)</st><st
    c="37577">? To answer that, we need to understand when the posterior becomes a
    narrow, tall, distribution centered around the MAP</st> <st c="37697">estimate,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)<st
    c="37707"><st c="37712">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37713">Let’s look at the definition of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1848.png)<st
    c="37746"><st c="37752">. It is the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="37773"><st c="37774">that maximizes</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1850.png)<st
    c="37790"><st c="37801">. Again, applying our trick of maximizing the logarithm,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)
    <st c="37858"><st c="37863">is also the value that maximizes</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1852.png)<st
    c="37896"><st c="37910">. Using Bayes’ theorem to calculate</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1853.png)<st
    c="37946"><st c="37961">, we find that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)
    <st c="37976"><st c="37981">is the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="37997"><st c="37998">that maximizes</st> <st c="38014">the following:</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mtext>Likelihood</mtext><mo>+</mo><mi>log</mi><mtext>Prior</mtext><mo>−</mo><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mtext>Data</mtext></mfenced></mrow></mrow></math>](img/1856.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="38065">Eq.</st> <st c="38069">48</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38071">Since</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow></mml:mfenced></mml:math>](img/1857.png)
    <st c="38078"><st c="38079">doesn’t depend on</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="38098"><st c="38099">we can ignore it when identifying</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)<st
    c="38134"><st c="38139">, and so</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)
    <st c="38148"><st c="38153">is the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>_</mml:mo></mml:mrow></mml:munder></mml:math>](img/1778.png)
    <st c="38169"><st c="38170">that maximizes,</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mtext>Likelihood</mtext><mtext>+</mtext><mi>log</mi><mtext>Prior</mtext></mrow></mrow></math>](img/1862.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="38211">Eq.</st> <st c="38215">49</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38217">For large sample sizes, the likelihood dominates</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="38266">Now, recall we said that the log-likelihood scales linearly with
    the number of data points</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="38358"><st c="38359">. So, the magnitude of the log-likelihood increases with</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/629.png)<st
    c="38416"><st c="38417">. In contrast, the prior does not, by definition, depend
    on the data, so the magnitude of the log-prior doesn’t scale with</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/686.png)<st
    c="38540"><st c="38541">. Firstly, this means that as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/686.png)
    <st c="38571"><st c="38572">increases, the expression in Eq.</st> <st c="38606">49
    is dominated by the log-likelihood.</st> <st c="38645">So, as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1867.png)<st
    c="38652"><st c="38653">, the MAP estimate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1868.png)
    <st c="38672"><st c="38677">tends to the maximum-likelihood estimate</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math>](img/1869.png)<st
    c="38718"><st c="38727">. Secondly, since the magnitude of the log-likelihood
    is increasing as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1870.png)<st
    c="38798"><st c="38799">, this means the magnitude of the posterior at</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1871.png)
    <st c="38846"><st c="38851">is increasing</st> <st c="38864">indefinitely as</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1872.png)<st
    c="38881"><st c="38882">. So, the posterior is becoming infinitely high.</st>
    <st c="38931">Since the posterior is a properly normalized probability density,
    it also becomes infinitely thin as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1873.png)<st
    c="39032"><st c="39033">. So, as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1874.png)
    <st c="39042"><st c="39043">the approximation in Eq.</st> <st c="39069">47 becomes
    accurate.</st> <st c="39090">While we have only given a hand-waving demonstration
    of this, it is still valid.</st> <st c="39171">A rigorous proof of th</st><st
    c="39193">is point is beyond the scope of</st> <st c="39226">this book.</st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure\uFEFF 5.2: Plots of the prior and posterior distributions for three\
    \ games of coin tossing with different sample sizes](img/B19496_05_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: '<st c="39324">Figure 5.2: Plots of the prior and posterior distributions for
    three games of coin tossing with different sample sizes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="39442">Figure 5</st>**<st c="39451">.2</st>* <st c="39453">shows a</st>
    <st c="39462">numerical demonstration of this.</st> <st c="39495">We have plotted
    the prior (the blue line) and posterior (the black line) distributions for a series
    of three experiments.</st> <st c="39617">In each plot, the sample data is the
    number of times a fair coin turns up heads when tossed</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="39709"><st c="39710">times.</st> <st c="39718">This is a chapter on Bayesian
    modeling, so we couldn’t not have an example about tossing coins, could we?</st>
    <st c="39824">The plots are for different values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="39862"><st c="39863">with</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>N</mi><mo>=</mo><mn>10</mn><mo>,</mo><mn>30</mn><mo>,</mo><mn>100</mn></mrow></mrow></math>](img/1877.png)
    <st c="39869"><st c="39873">in the plots from left to right, respectively.</st>
    <st c="39920">To make this interesting, I have set the coin tossing in the wild
    west of America in the 1840s.</st> <st c="40016">I’m betting against “cowboy Duke”,
    a hardened gambler.</st> <st c="40071">Every time the coin is tossed and lands
    tails up, I win $1\.</st> <st c="40131">I suspect that Duke is cheating and has
    an unfair coin.</st> <st c="40187">It turns out this isn’t true, but in the wild
    west of the 1840s I’m on my guard and suspicious, so my prior is that the probability
    of heads,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>p</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Head</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1878.png)<st
    c="40329"><st c="40336">, for this coin is 0.8\.</st> <st c="40360">I have used
    a Beta(8, 2) distribution for my prior so that the mean of my prior is 0.8, but
    it has some spread, as you can see by the blue line in the plots.</st> <st c="40518">The
    prior, the blue line, is the same in each of the plots, as you expect because
    the prior does not depend on the data and so can’t depend on the sample</st> <st
    c="40672">size</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="40677"><st c="40678">.</st></st></st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40679">For the</st> <st c="40687">left-hand plot, where</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math>](img/1880.png)<st
    c="40710"><st c="40711">, the number of heads was 5, but the influence of the
    prior on the posterior is clear.</st> <st c="40798">The prior has pulled the posterior
    away from being centered around the true value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>p</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Head</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/1881.png)
    <st c="40883"><st c="40891">(shown by the vertical dashed line), even though the
    observed proportion of heads was</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/1882.png)<st
    c="40977"><st c="40978">. In this case, the sample size of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math>](img/1880.png)<st
    c="41013"><st c="41014">, isn’t big enough for the likelihood to override the
    influence of a poorly</st> <st c="41090">chosen prior.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41103">For the</st> <st c="41112">middle plot, with</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:math>](img/1884.png)
    <st c="41130"><st c="41131">and 16 heads observed, we can see the posterior distribution
    is closer to being centered around the true value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>p</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Head</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/1885.png)<st
    c="41246"><st c="41254">, and it is narrower and taller compared to the left-hand
    plot, but the influence of the prior is still clear.</st> <st c="41365">The maximum
    of the posterior is still noticeably different</st> <st c="41424">from</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>p</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Head</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/1886.png)<st
    c="41429"><st c="41437">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41438">In contrast, in the right-hand plot where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math>](img/1887.png)
    <st c="41481"><st c="41482">and 49 heads are observed, the likelihood dominates
    the posterior and the influence of the prior is small.</st> <st c="41590">The
    posterior distribution is very tall and narrow and centered almost exactly over</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>p</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Head</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/1888.png)<st
    c="41674"><st c="41683">. The influence of my poorly chosen prior has been almost
    completely counteracted by the observation of</st> <st c="41787">the data.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41796">Let’s use the data from</st> *<st c="41821">Figure 5</st>**<st
    c="41829">.2</st>* <st c="41831">in a MAP estimation</st> <st c="41852">code example.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41865">MAP estimation code example</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="41893">The</st> <st c="41898">following code example can be found in
    the</st> `<st c="41941">Code_Examples_Chap5.ipynb</st>` <st c="41966">notebook
    in the</st> <st c="41983">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42001">We’ll calculate the MAP estimate for the binomial data we illustrated
    in the left-hand panel of</st> *<st c="42098">Figure 5</st>**<st c="42106">.2</st>*<st
    c="42108">. We have data from a series of Bernoulli trials.</st> <st c="42158">The
    likelihood only depends on the number of trials, the number of successes (number
    of heads for the example in</st> *<st c="42271">Figure 5</st>**<st c="42279">.2</st>*<st
    c="42281">), and the success probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1889.png)<st
    c="42313"><st c="42314">. We’ll use the in-built optimization algorithms in the
    SciPy package to maximize the posterior.</st> <st c="42411">We do this by minimizing
    the negative of the log-posterior.</st> <st c="42471">We only need to calculate
    the sum of the log-likelihood and the log-prior since we can drop the normalizing
    constant from the log-posterior, as this does not depend upon the success probability.</st>
    <st c="42666">The parameter we maximize the log-posterior</st> <st c="42710">with
    respect to will actually be the logit of the success probability.</st> <st c="42781">Since
    the logistic function is a monotonic function, stationary points of the log-posterior
    with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="42889"><st c="42890">will also be stationary points of the log-posterior
    with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/1891.png)<st
    c="42959"><st c="42974">. First, we’ll need to define the</st> <st c="43008">log-posterior
    function:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44684">Now we’ll set</st> <st c="44698">the data.</st> <st c="44709">In
    this example, we’ll use the values from the left-hand panel of</st> *<st c="44775">Figure
    5</st>**<st c="44783">.2</st>*<st c="44785">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45280">Now we’ll</st> <st c="45291">do the minimization</st> <st c="45311">of
    the negative log-posterior using the SciPy implementation of the</st> **<st c="45379">Broyden-Fletcher-Goldfarb-Shanno</st>**
    <st c="45411">(</st>**<st c="45413">BFGS</st>**<st c="45417">) algorithm:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45684">This gives the</st> <st c="45700">following output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45773">The MAP estimate of the success probability</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="45818"><st c="45819">corresponds to the position of the maximum of the
    posterior that we can see in the left-hand panel of</st> *<st c="45922">Figure
    5</st>**<st c="45930">.2</st>*<st c="45932">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45933">As</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold-italic">N</mml:mi></mml:math>](img/1893.png)
    <st c="45937"><st c="45938">becomes large the prior becomes irrelevant</st></st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="45981">We have already illustrated the main consequence of the sample
    size</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/629.png)
    <st c="46050"><st c="46051">becoming large – the likelihood dominates the posterior
    – but let’s unpack that conclusion a bit more.</st> <st c="46155">Effectively,
    the prior becomes irrelevant as the sample size increases.</st> <st c="46227">How
    quickly the prior becomes irrelevant depends upon the precise details of the likelihood
    – the predictive model and the nature of the random component in the data – but
    also the precise details of the prior.</st> <st c="46438">A narrow prior,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1781.png)<st
    c="46454"><st c="46455">, indicates a high degree of confidence in that prior
    belief, and this would take a greater amount of information (data) in the likelihood
    for the influence of the prior to be overruled.</st> <st c="46642">A high degree
    of confidence does not mean that the prior is correct.</st> <st c="46711">Instead,
    it means that we have a strong</st> **<st c="46751">a priori</st>** <st c="46759">belief
    that the only values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="46791"><st c="46792">that are possible are those that are in the narrow
    range where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1897.png)
    <st c="46856"><st c="46857">is high.</st> <st c="46867">But even the influence
    of a narrow poorly chosen prior can be overcome with sufficient data.</st> <st
    c="46960">With one exception.</st> <st c="46980">If our prior</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/1897.png)
    <st c="46993"><st c="46994">is itself a delta function, then no finite amount
    of data can overcome the influence of the prior.</st> <st c="47094">In other words,
    if we are 100% certain in our</st> **<st c="47140">a priori</st>** <st c="47148">beliefs
    about the model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="47173"><st c="47174">, then no amount of data can convince us otherwise, even
    if those beliefs</st> <st c="47248">are wrong.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47258">Least squares as an approximation to Bayesian modeling</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="47313">As a</st> <st c="47318">final comment on the two main ways of
    using the posterior in calculations, we’ll return to a comment we made in</st>
    [*<st c="47431">Chapter 4</st>*](B19496_04.xhtml#_idTextAnchor216) <st c="47440">about
    least squares being a heuristic algorithm, but that it was possible to provide
    more formal under-pinning to least squares estimation of</st> <st c="47583">model
    parameters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47600">The delta function approximation in Eq.</st> <st c="47641">47
    highlights that we can view MAP estimation as an approximation to the correct
    posterior.</st> <st c="47733">We also know that MAP estimation becomes more appropriate
    when we have large sample sizes</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="47823"><st c="47824">. We also know that in the limit</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1873.png)
    <st c="47857"><st c="47858">the MAP estimate of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="47879"><st c="47880">becomes the maximum likelihood estimate of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="47924"><st c="47925">. And we know that when the random component in our data
    is of the form of additive Gaussian noise, then the model parameter</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)<st
    c="48050"><st c="48051">, from the maximum-likelihood estimate of</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>=</mo><mfenced
    open="(" close=")"><mrow><munder><mi>β</mi><mo stretchy="true">_</mo></munder><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/1905.png)<st
    c="48093"><st c="48094">, is the same as the least squares estimate of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)<st
    c="48141"><st c="48142">. So, finally, we have the more rigorous justification
    for the least squares estimation of model parameters that we promised back in</st>
    [*<st c="48275">Chapter 4</st>*](B19496_04.xhtml#_idTextAnchor216)<st c="48284">.
    We can view the least squares estimation of the model parameters</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)
    <st c="48351"><st c="48352">as the result of a chain of approximations applied
    to the posterior expectation</st> <st c="48433">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1729.png)<st
    c="48436"><st c="48437">.</st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48438">We have covered a lot of the theory behind Bayesian modeling in
    this section, so it is time to recap what we have learned before we move on to
    how we put that theory into practice in numerical calculations in the</st> <st
    c="48652">next section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48665">What we have learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="48686">In this section, we have learned about</st> <st c="48726">the
    following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48740">Bayesian model averaging and how it uses the probabilities given
    by the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1909.png)
    <st c="48823"><st c="48835">to perform a weighted average calculated over a set
    of</st> <st c="48890">models</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="48898">Maximum a posteriori</st>** <st c="48918">(</st>**<st c="48920">MAP</st>**<st
    c="48923">) estimation and how it approximates the posterior distribution by a
    single representative model,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)<st
    c="49022"><st c="49027">, that has the highest probability given</st> <st c="49068">the
    data</st></st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49076">How the MAP estimate for a model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="49110"><st c="49111">tends to the maximum likelihood estimate of the model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="49166"><st c="49167">, as the sample</st> <st c="49183">size</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math>](img/1914.png)</st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49189">How MAP estimation provides a justification for the least squares
    estimation of a model’s parameters when the random component in the data is additive</st>
    <st c="49340">Gaussian noise</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49354">Having learned about the theory of Bayesian modeling, in the next
    section we’ll learn about some of the practical aspects, and</st> <st c="49482">about
    a class of modeling tools called</st> **<st c="49521">Probabilistic Programming</st>**
    **<st c="49547">Languages</st>** <st c="49556">(</st>**<st c="49558">PPLs</st>**<st
    c="49562">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49565">Bayesian modeling in practice</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="49595">Bayesian model averaging, as</st> <st c="49625">encapsulated by
    Eq.</st> <st c="49645">46, is a very powerful tool for any data scientist to have
    in their toolkit.</st> <st c="49722">In practice, it can take a bit more experience
    to fully make use of its potential.</st> <st c="49805">We haven’t yet said how
    one goes about computing the expectation value in Eq.</st> <st c="49883">46\.</st>
    <st c="49887">This is the practice of</st> <st c="49911">Bayesian modeling.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49929">To make Bayesian modeling averaging a practical tool, there are
    two main approaches we</st> <st c="50017">can take:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50026">Analytical calculation, whereby we approximate the posterior to
    the extent that calculation of the expectation in Eq.</st> <st c="50145">46 can
    be done in closed-form or nearly in closed-form, and so we only need to perform
    a small number of</st> <st c="50250">numerical calculations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="50272">Computationally intensive sampling, whereby we numerically approximate
    the integration in Eq.</st> <st c="50367">46 by sampling many different model
    values</st> <st c="50410">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="50414">We will now cover those two approaches in</st> <st c="50456">more
    detail.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50468">Analytic approximation of the posterior</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50508">We have</st> <st c="50516">already introduced an analytic approximation
    to the posterior in Eq.</st> <st c="50586">47\.</st> <st c="50590">When we introduced
    the MAP estimate we explained that the approximation in Eq.</st> <st c="50669">47
    is appropriate if the posterior is tall and narrow.</st> <st c="50724">We know
    this happens as the dataset size</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="50765"><st c="50766">becomes large.</st> <st c="50782">But what happens
    if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="50802"><st c="50803">is not large?</st> <st c="50818">What happens if the
    posterior is not tall and narrow?</st> <st c="50872">What analytic approximation
    can we use then?</st> <st c="50917">The next most obvious step is to approximate
    the posterior by a multi-variate Gaussian distribution centered around the maximum
    of the posterior, i.e., centered around</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>MAP</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1918.png)
    <st c="51085"><st c="51090">. Effectively, we are approximating the log of</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1919.png)
    <st c="51137"><st c="51148">by its second order Taylor-expansion about</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>MAP</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1920.png)<st
    c="51191"><st c="51197">. Doing s</st><st c="51206">o gives us the</st> <st c="51222">following
    approximation:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>≈</mo><mfrac><mn>1</mn><msup><mfenced
    open="(" close=")"><mrow><mn>2</mn><mi>π</mi></mrow></mfenced><mstyle scriptlevel="+1"><mfrac><mfenced
    open="|" close="|"><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mfenced><mn>2</mn></mfrac></mstyle></msup></mfrac><msqrt><mrow><mtext>det</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><munder><munder><mi>H</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></msqrt><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced open="("
    close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><munder><munder><mi>H</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/1921.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51304">Eq.</st> <st c="51308">50</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51310">Here,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>|</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo></mml:math>](img/1922.png)
    <st c="51317"><st c="51318">denotes the cardinality of the vector</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="51357"><st c="51358">, that is, the number of components in the vector</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="51408"><st c="51409">. The matrix</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1275.png)
    <st c="51422"><st c="51423">is the Hessian of the log-posterior evaluated at the
    MAP point</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1926.png)<st
    c="51487"><st c="51493">, and so is calculated</st> <st c="51516">as follows:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><munder><mi>H</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder><mo>=</mo><msub><mfenced
    open="" close="|"><mfrac><mrow><msup><mo>∂</mo><mn>2</mn></msup><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow><mrow><mo>∂</mo><msup><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mn>2</mn></msup></mrow></mfrac></mfenced><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></msub></mrow></mrow></math>](img/1927.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51562">Eq.</st> <st c="51566">51</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51568">If we</st> <st c="51575">have a function,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1928.png)<st
    c="51592"><st c="51600">, that depends upon the model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="51630"><st c="51631">and we want to calculate its expectation,</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi mathvariant="double-struck">E</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></msub><mfenced open="(" close=")"><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/1930.png)<st
    c="51674"><st c="51675">, over the posterior, then we can now</st> <st c="51713">approximate
    that expectation</st> <st c="51742">as follows:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></msub><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mo>≈</mo><mfrac><mn>1</mn><msup><mfenced
    open="(" close=")"><mrow><mn>2</mn><mi>π</mi></mrow></mfenced><mstyle scriptlevel="+1"><mfrac><mfenced
    open="|" close="|"><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mfenced><mn>2</mn></mfrac></mstyle></msup></mfrac><msqrt><mrow><mtext>det</mtext><mfenced
    open="(" close=")"><munder><mrow><mo>−</mo><munder><mi>H</mi><mo stretchy="true">_</mo></munder></mrow><mo
    stretchy="true">_</mo></munder></mfenced></mrow></msqrt><mo>∫</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mtext>exp</mtext><mfenced open="("
    close=")"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><munder><munder><mi>H</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder><mfenced open="(" close=")"><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mi>d</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/1931.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51819">Eq.</st> <st c="51823">52</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51825">Many integrals with Gaussian distributions can be evaluated exactly,
    i.e., in closed-form, and this is one of the main benefits of the analytic approximation
    approach.</st> <st c="51994">When the integral in Eq.</st> <st c="52019">52 cannot
    be evaluated exactly, a common approach is to also expand the function</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1932.png)
    <st c="52100"><st c="52101">about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1933.png)<st
    c="52108"><st c="52114">, so we</st> <st c="52122">have this:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>≈</mo><mi>f</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><msub><mfenced open="" close="|"><mfrac><mrow><mo>∂</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow><mrow><mo>∂</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfrac></mfenced><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><msub><mfenced open="" close="|"><mfrac><mrow><msup><mo>∂</mo><mn>2</mn></msup><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow><mrow><mo>∂</mo><msup><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mn>2</mn></msup></mrow></mfrac></mfenced><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></msub><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>−</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mo>+</mo><mo>⋯</mo></mrow></mrow></math>](img/1934.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52153">Eq.</st> <st c="52157">53</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52159">Plugging Eq.</st> <st c="52173">53 into Eq.</st> <st c="52185">52
    and evaluating th</st><st c="52205">e integral in Eq.</st> <st c="52224">52 then
    gives</st> <st c="52238">us this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></msub><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mo>≈</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></mfenced><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext>tr</mtext><mfenced
    open="(" close=")"><mrow><msup><munder><munder><mi>H</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mfenced
    open="" close="|"><mfrac><mrow><msup><mo>∂</mo><mn>2</mn></msup><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow><mrow><mo>∂</mo><msup><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mn>2</mn></msup></mrow></mfrac></mfenced><mrow><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>=</mo><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mrow><mi>M</mi><mi>A</mi><mi>P</mi></mrow></msub></mrow></msub></mrow></mfenced><mo>+</mo><mo>⋯</mo></mrow></mrow></math>](img/1935.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52306">Eq.</st> <st c="52310">54</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52312">We have now reduced the calculation down to an optimization problem
    – locating the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1936.png)
    <st c="52405"><st c="52411">– and a linear algebra calculation in Eq.</st> <st
    c="52453">54, both of which we have established software packages</st> <st c="52509">to
    do.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52515">One of the</st> <st c="52527">benefits of using a Gaussian approximation
    centered on the MAP estimate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)
    <st c="52599"><st c="52604">is that we have already located the MAP value</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1836.png)
    <st c="52650"><st c="52655">when we did MAP estimation.</st> <st c="52683">In
    fact, we can think of the delta function approximation in Eq.</st> <st c="52748">47
    as the first order approximation to the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1939.png)<st
    c="52801"><st c="52813">, compared to the second-order approximation that Eq.</st>
    <st c="52867">50 represents.</st> <st c="52882">One could construct higher-order
    approximations to the posterior by continuing the Taylor expansion of</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1940.png)
    <st c="52985"><st c="53000">about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math>](img/1941.png)
    <st c="53006"><st c="53011">to third order or higher.</st> <st c="53037">However,
    this is very rarely done in machine learning</st> <st c="53091">or statistics.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53105">Another benefit of the analytic approximation in Eq.</st> <st
    c="53159">50 is that it is very general.</st> <st c="53190">We can apply the approximate
    posterior distribution given in Eq.</st> <st c="53255">50 to calculate the expectation
    of any function</st> <st c="53303">f</st><st c="53304">(</st><st c="53305">x</st><st
    c="53306">_</st><st c="53307">,</st> <st c="53308">θ</st><st c="53309">_</st><st
    c="53310">)</st><st c="53311">.</st> <st c="53312">Likewise, the approximation
    in Eq.</st> <st c="53348">54 can be applied to any</st> <st c="53373">function</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>.</mo></mrow></mrow></math>](img/1942.png)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53383">Computational sampling</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="53405">The idea</st> <st c="53415">behind sampling approaches to Bayesian
    averaging is very simple.</st> <st c="53480">If we want to calculate the expectation,</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi mathvariant="double-struck">E</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></msub><mfenced open="(" close=")"><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/1943.png)<st
    c="53521"><st c="53533">, over a distribution of models, then we could approximate
    this population average by a sample average.</st> <st c="53637">In other words,
    we generate a random sample of values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="53694"><st c="53695">from the distribution in question, plug those sample
    values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="53759"><st c="53760">into</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1946.png)
    <st c="53766"><st c="53774">and calculate the sample mean.</st> <st c="53805">For
    our Bayesian averaging challenge the distribution in question from which we sample
    values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1808.png)
    <st c="53902"><st c="53903">is the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1939.png)<st
    c="53921"><st c="53933">. If we generate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>K</mml:mi></mml:math>](img/589.png)
    <st c="53950"><st c="53963">sample values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="53980"><st c="53981">from</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1939.png)<st
    c="53987"><st c="53999">, then the expectation o</st><st c="54023">f</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1952.png)
    <st c="54026"><st c="54027">is approximated</st> <st c="54044">by this:</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></msub><mfenced
    open="(" close=")"><mrow><mi>f</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mo>≈</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub></mrow></mfenced></mrow></mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>~</mo><mtext>Posterior(Data)</mtext></mrow></mrow></math>](img/1953.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="54074">Eq.</st> <st c="54078">55</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54080">The larger</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>K</mml:mi></mml:math>](img/589.png)
    <st c="54092"><st c="54105">is, i.e., the larger the number of samples of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="54151"><st c="54152">we generate from the posterior, the more accurate
    the approximation in Eq.</st> <st c="54228">55 will be.</st> <st c="54240">As
    with the analytic approximation techniques, this is a completely general approach
    – we haven’t said what the function</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1952.png)
    <st c="54361"><st c="54362">is, and so this method can be applied to any</st>
    <st c="54408">function</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/1952.png)<st
    c="54417"><st c="54418">.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54419">The</st> <st c="54424">only question that now remains is, how
    do we generate samples of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="54489"><st c="54490">from the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1959.png)<st
    c="54510"><st c="54522">? We covered a bit about sampling from distributions in</st>
    [*<st c="54578">Chapter 2</st>*](B19496_02.xhtml#_idTextAnchor061)<st c="54587">,
    but we commented that sampling from continuous distributions can sometimes be
    challenging.</st> <st c="54680">Fortunately, there is a general computational
    method that comes to our rescue –</st> **<st c="54760">Markov Chain Monte Carlo</st>**
    <st c="54784">(</st>**<st c="54786">MCMC</st>**<st c="54790">).</st> <st c="54794">MCMC</st>
    <st c="54798">is a Monte Carlo method, meaning we generate values of our random
    variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="54875"><st c="54876">in this case, at random.</st> <st c="54902">The Markov
    Chain part of the algorithm name means that when we generate our next value of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="54992"><st c="54993">at random, we do so conditionally on our current value
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="55052"><st c="55053">, and so we get a sequence or a</st> **<st c="55085">chain</st>**
    <st c="55090">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="55094"><st c="55095">values.</st> <st c="55104">MCMC algorithms typically
    take the</st> <st c="55139">following form:</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55154">Set the iteration number</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/1964.png)
    <st c="55180"><st c="55181">and set</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="55190"><st c="55191">to some initial</st> <st c="55208">value</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/1966.png)<st
    c="55214"><st c="55217">.</st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="55218">From the current value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1967.png)<st
    c="55242"><st c="55245">, propose a new value trial value</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mtext>trial</mtext></msub><mo>=</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/1968.png)<st c="55279"><st
    c="55286">, where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">Δ</mml:mi><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1969.png)
    <st c="55294"><st c="55295">is an adjustment that we sample from a simple fixed
    distribution, e.g.</st> <st c="55367">a uniform distribution with a</st> <st c="55397">small
    range.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55409">Accept or reject the trial value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1970.png)
    <st c="55443"><st c="55449">by applying a stochastic comparison rule to the pair
    of values</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1971.png)
    <st c="55512"><st c="55523">and</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mtext>trial</mtext></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1972.png)<st
    c="55527"><st c="55543">.</st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="55544">If we accept the trial value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1973.png)
    <st c="55574"><st c="55580">then we set</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mtext>trial</mtext></msub></mrow></mrow></math>](img/1974.png)<st
    c="55592"><st c="55606">, else we set</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub></mrow></mrow></math>](img/1975.png)<st
    c="55620"><st c="55621">. Record</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/1976.png)<st
    c="55630"><st c="55633">. Increment the</st> <st c="55649">iteration,</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>i</mi><mo>→</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow></math>](img/1977.png)<st
    c="55660"><st c="55661">.</st></st></st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="55662">Repeat steps 2 – 4 until</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/332.png)
    <st c="55688"><st c="55689">iterations have</st> <st c="55706">been performed.</st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="55721">One of the simplest</st> <st c="55742">comparison rules is the</st>
    **<st c="55766">Metropolis-Hastings importance sam</st><st c="55800">pling</st>**
    <st c="55806">scheme, which takes the</st> <st c="55831">following</st> <st c="55841">form:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Accept</mtext><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mtext>trial</mtext></msub><mtext>with</mtext><mtext>probability</mtext><mtext>min</mtext><mfenced
    open="(" close=")"><mrow><mfrac><mrow><mi>P</mi><mfenced open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mtext>trial</mtext></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mfrac><mo>,</mo><mn>1</mn></mrow></mfenced></mrow></mrow></math>](img/1979.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="55922">Eq.</st> <st c="55926">56</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55928">Eq.</st> <st c="55933">56 says that if our trial value</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1980.png)
    <st c="55965"><st c="55971">has higher probability than our current value</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1981.png)<st
    c="56017"><st c="56020">, then we move from</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1981.png)
    <st c="56040"><st c="56043">to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1980.png)
    <st c="56046"><st c="56052">with probability 1, i.e., we definitely move.</st>
    <st c="56098">While if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1980.png)
    <st c="56107"><st c="56113">has lower probability than</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/1981.png)
    <st c="56140"><st c="56143">we can still move to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1986.png)<st
    c="56164"><st c="56170">, but it is not guaranteed.</st> <st c="56198">The smaller
    the probability</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mtext>trial</mtext></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1987.png)
    <st c="56226"><st c="56239">is compared to</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1988.png)<st
    c="56254"><st c="56265">, the less likely we are to move to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>trial</mml:mtext></mml:mrow></mml:msub></mml:math>](img/1989.png)
    <st c="56301"><st c="56307">and so the more likely we are to stay in</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mtext>i</mtext></msub></mrow></math>](img/1990.png)<st
    c="56348"><st c="56351">. Overall, this means we move, over time and in a stochastic
    fashion, to regions of higher and higher</st> <st c="56453">posterior probability.</st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56475">You rightly ask</st> <st c="56492">how we choose the initial value</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/1991.png)<st
    c="56524"><st c="56527">. It doesn’t matter.</st> <st c="56548">After we have
    performed a reasonable number of iterations, the values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="56621"><st c="56622">being generated will be correctly sampled from the
    posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/1993.png)<st
    c="56684"><st c="56696">. However, the early values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="56727"><st c="56728">will not be correctly sampled from the posterior,
    and therefore it is usual to discard these values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="56832"><st c="56833">from the early part of the chain.</st> <st c="56868">The
    period where we are just iterating the MCMC algorithm but not recording the generated
    values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="56968"><st c="56969">is called</st> <st c="56980">the</st> **<st c="56984">burn-in</st>**
    <st c="56991">period.</st> <st c="57000">After the burn-in period, we collect
    and use the generated values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="57069"><st c="57070">as our sample to plug into the sample average calculation
    in Eq.</st> <st c="57136">55\.</st> <st c="57140">Let’s illustrate these concepts
    with a</st> <st c="57179">code example.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57192">MCMC code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57210">We’ll</st> <st c="57217">code up a very basic version of the Metropolis-Hastings
    algorithm and use it to sample</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>logit</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/1998.png)
    <st c="57304"><st c="57313">values from the posterior</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>log</mi><mrow><mrow><mfenced open="(" close=")"><mstyle
    scriptlevel="+1"><mfrac><mi>p</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mstyle></mfenced><mo>|</mo><mtext>Data</mtext></mrow></mrow></mrow></mfenced></mrow></mrow></math>](img/1999.png)
    <st c="57339"><st c="57359">for the data in the left-hand plot of</st> *<st c="57397">Figure
    5</st>**<st c="57405">.2</st>*<st c="57407">. As with our earlier MAP estimation
    example, we are working with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>logit</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2000.png)
    <st c="57473"><st c="57482">rather than the success probability</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="57518"><st c="57519">so that we can sample a parameter that is unconstrained,
    i.e., it lies in the range</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfenced
    open="(" close=")"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi><mo>,</mo><mo>+</mo><mi
    mathvariant="normal">∞</mi></mrow></mfenced></mrow></math>](img/2002.png)<st c="57604"><st
    c="57605">. This means we’ll need to calculate the posterior for</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/2003.png)<st
    c="57660"><st c="57661">, but we can do this using the rule for transforming probability
    distributions that we covered in</st> *<st c="57759">Chapter</st>*<st c="57766">.
    Doing so gives</st> <st c="57783">us this:</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>log</mi><mrow><mrow><mfenced open="(" close=")"><mfrac><mi>p</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mfenced><mo>|</mo><mtext>Data</mtext></mrow></mrow></mrow></mfenced><mo>=</mo><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>p</mi><mo>|</mo><mtext>Data</mtext></mrow></mfenced><mo>+</mo><mi>log</mi><mfenced
    open="(" close=")"><mrow><mi>p</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mfenced></mrow></mrow></math>](img/2004.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="57849">Eq.</st> <st c="57853">57</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57855">We’ll reuse the code from the previous MAP estimation code example
    to generate a callable Python function that</st> <st c="57967">returns</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>p</mi><mo>|</mo><mtext>Data</mtext></mrow></mfenced></mrow></mrow></math>](img/2005.png)<st
    c="57975"><st c="57991">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57992">The following code example can be found in the</st> `<st c="58040">Code_Examples_Chap5.ipynb</st>`
    <st c="58065">notebook in</st> <st c="58078">the</st> <st c="58082">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58100">First, we’ll define a function to perform a single trial</st>
    <st c="58158">Metropolis-Hastings move:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="59485">Now, we can use the preceding function to define another function
    that runs the</st> <st c="59566">Markov chain for a user-specified number of burn-in
    iterations, followed by a user-specified number of</st> <st c="59669">sampling
    iterations:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="60804">Now, we’ll</st> <st c="60816">set the data.</st> <st c="60830">Again,
    in this example we’ll use the values from the left-hand panel of</st> *<st c="60902">Figure
    5</st>**<st c="60910">.2</st>*<st c="60912">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="61256">Now, we’ll</st> <st c="61267">run the MCMC calculation.</st> <st
    c="61294">We’ll run a long burn-in period of 20,000 iterations to be sure, and
    then we’ll take 1 million samples.</st> <st c="61398">Finally, we convert the
    sampled values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>logit</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2006.png)<st
    c="61440">![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>logit</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2007.png)
    <st c="61446"><st c="61447">to values</st> <st c="61458">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="61461"><st c="61462">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]</st></st></st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="61740">Now, let’s look at the histogram of the MCMC sampled success probabilities
    and compare it to the true posterior curve.</st> <st c="61860">The true posterior
    curve will be the same posterior</st> <st c="61912">curve that is shown in the
    left-hand panel of</st> *<st c="61958">Figure 5</st>**<st c="61966">.2</st>*<st
    c="61968">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "![Figure\uFEFF 5.3: Histogram of MCMC samples and the true posterior](img/B19496_05_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: '<st c="62441">Figure 5.3: Histogram of MCMC samples and the true posterior</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62501">We can see in</st> *<st c="62516">Figure 5</st>**<st c="62524">.3</st>*
    <st c="62526">how the values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2009.png)
    <st c="62545"><st c="62546">sampled using the Metropolis-Hastings algorithm match
    the true posterior distribution (the orange curve in</st> *<st c="62654">Figure</st>*
    *<st c="62661">5</st>**<st c="62662">.3</st>*<st c="62664">) exactly.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62675">That was a lengthy code example, and it can be tedious to write
    the MCMC code yourself.</st> <st c="62764">If only it was easier.</st> <st c="62787">This
    brings us neatly on to our next topic,</st> **<st c="62831">probabilistic</st>**
    **<st c="62845">programming languages</st>**<st c="62866">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62867">Probabilistic programming languages</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62903">In</st> <st c="62907">practice, choosing the length of the burn-in
    period can be a bit of an art form.</st> <st c="62988">There are also other variants
    of MCMC algorithms that are more sophisticated than the basic Metropolis-Hastings
    rule.</st> <st c="63106">Again, these more sophisticated MCMC algorithms can be
    applied to evaluate the expectation of any function and for any posterior.</st>
    <st c="63236">Due to the wide applicability of these MCMC algorithms, many of
    them are coded into specialist languages</st> <st c="63341">called</st> **<st
    c="63348">Probabilistic Programming Languages</st>**<st c="63383">, (</st>**<st
    c="63386">PPLs</st>**<st c="63391">).</st> <st c="63395">These programming languages
    allow the user to easily express a probabilistic model and configure the MCMC
    calculation, and then the PPL takes care of running the MCMC.</st> <st c="63563">The
    main benefit of PPLs is that the user does not have to write any MCMC calculation
    code.</st> <st c="63655">The user only writes code that expresses</st> <st c="63696">the
    model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63706">Since the goal of PPLs is to make Bayesian modeling and inference
    easy by abstracting away a lot of the technical code that runs the MCMC calculation,
    many PPLs also make MAP estimation easy.</st> <st c="63899">We have already mentioned
    that the way in which we use MAP estimates is very generic, such as through the
    analytic approximations to the posterior in Eq.</st> <st c="64053">47 and Eq.</st>
    <st c="64064">50\.</st> <st c="64068">Consequently, most PPLs also provide functionality
    to obtain MAP estimates without the user having to write additional MAP estimation
    code.</st> <st c="64208">Again, the user only has to write code that expresses</st>
    <st c="64262">the model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64272">As you might have guessed, there are many different PPLs available.</st>
    <st c="64341">Some of the more well-established ones are</st> <st c="64384">listed
    here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="64396">PyMC</st>**<st c="64401">: One of the most widely used Python
    open source PPLs that uses Theano as a backend.</st> <st c="64487">Details are
    available</st> <st c="64509">at</st> [<st c="64512">https://www.pymc.io/welcome.html</st>](https://www.pymc.io/welcome.html)<st
    c="64544">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="64545">Stan</st>**<st c="64550">: Probably the other most widely used
    PPL.</st> <st c="64594">Stan has its own model specification language, which is
    then translated to C++ code that is then compiled to machine code.</st> <st c="64717">It
    is perhaps more widely used within the R programming community, but it has good
    interfaces to a number of languages, including Python.</st> <st c="64855">General
    details are available at</st> [<st c="64888">https://mc-stan.org/</st>](https://mc-stan.org/)<st
    c="64908">. Details on the Python interface, PyStan, can be found</st> <st c="64964">at</st>
    [<st c="64967">https://pystan.readthedocs.io/en/latest/</st>](https://pystan.readthedocs.io/en/latest/)<st
    c="65007">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="65008">Pyro</st>**<st c="65013">: A PPL created by Uber AI Labs with
    a PyTorch back-end.</st> <st c="65071">Details are available</st> <st c="65093">at</st>
    [<st c="65096">https://pyro.ai/</st>](https://pyro.ai/)<st c="65112">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="65113">NumPyro</st>**<st c="65121">: A variant of Pyro that uses a
    backend based on NumPy and JAX, and so can provide a speedup over Pyro for a subset
    of model types.</st> <st c="65254">Details are available</st> <st c="65276">at</st>
    [<st c="65279">https://num.pyro.ai/en/stable/</st>](https://num.pyro.ai/en/stable/)<st
    c="65309">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="65310">You’ll</st> <st c="65318">have spotted that each PPL either makes
    use of an existing computational backend, such as Theano, PyTorch, TensorFlow,
    or JAX, or has its own language that ultimately can be compiled to machine code.</st>
    <st c="65518">This computational power is necessary because of the computationally
    intensive nature of MCMC calculations, and also because PPLs have to be able to
    handle what can be very complex user-specified probabilistic models.</st> <st
    c="65736">Because of this need for PPLs to have access to a computational backend,
    they can be trickier to install and set up than, say, your average Python package.</st>
    <st c="65892">However, once you are comfortable with Bayesian modeling concepts
    and have gone through the pain of installing a PPL, they are very useful and fun
    to</st> <st c="66042">work with.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66052">That finishes this subsection on PPLs, so we’ll conclude by summarizing
    what we have learned about Bayesian modeling in practice, and then we’ll summarize
    the</st> <st c="66212">chapter overall.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66228">What we have learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="66249">In this section, we have learned</st> <st c="66283">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66297">How to make simple analytical approximations to the posterior
    to use in closed-form Bayesian</st> <st c="66391">averaging calculations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66413">About</st> **<st c="66420">Markov Chain Monte Carlo</st>** <st
    c="66444">(</st>**<st c="66446">MCMC</st>**<st c="66450">) algorithms and how
    they can be used to generate samples from a posterior distribution, which then
    allow us to approximate an expectation over the posterior via a</st> <st c="66615">sample
    mean</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66626">About</st> **<st c="66633">Probabilistic Programming Languages</st>**
    <st c="66668">(</st>**<st c="66670">PPLs</st>**<st c="66674">) and how they automate
    away a lot of the cumbersome and repeated tasks involved in MCMC or MAP</st> <st
    c="66771">estimation calculations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66794">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="66802">This chapter has been a culmination of the many ideas and concepts
    we have introduced in the previous three chapters.</st> <st c="66921">At the heart
    of this chapter is the idea that because data is random, predictive models that
    attempt to explain and use that data should be probabilistic.</st> <st c="67076">To
    build probabilistic models, we have had to learn about the probability distributions
    that describe the data and the distributions that describe the models.</st> <st
    c="67235">Specifically, we have had to learn about</st> <st c="67276">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67290">Likelihood as the probability of data given</st> <st c="67335">a
    model</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67342">How to use the likelihood to estimate model parameters via</st>
    <st c="67402">maximum likelihood</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67420">Bayes’ theorem and about prior and</st> <st c="67456">posterior
    distributions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67479">How the posterior distribution quantifies the probability of the
    model parameters given the data or information we</st> <st c="67595">have received</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67608">How we can use the posterior in Bayesian model averaging, or MAP</st>
    <st c="67674">estimation calculations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67697">How to perform those Bayesian model averaging and MAP estimation
    calculations</st> <st c="67776">in practice</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="67787">Probabilistic Programming Languages</st>** <st c="67823">(</st>**<st
    c="67825">PPLs</st>**<st c="67829">) and how they automate a lot of the practical
    tasks in</st> <st c="67886">probabilistic modeling</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="67908">This chapter represents the last of the core math concepts and
    techniques we cover in this book.</st> <st c="68006">This chapter and the preceding
    three chapters cover what I consider to be the absolute minimum core math concepts
    and techniques that any data scientist should be familiar with.</st> <st c="68184">For
    the rest of the book, we will get more specialized, covering individual math concepts
    that tend to be focused on a particular type of data or a particular domain.</st>
    <st c="68351">To start, in the next chapter, we move onto time</st> <st c="68400">series
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68412">Exercises</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="68422">Here is a series of exercises.</st> <st c="68454">Answers to all
    the exercises are given in the Jupyter</st> `<st c="68508">Answers_to_Exercises_Chap5.ipynb</st>`
    <st c="68540">notebook in the</st> <st c="68557">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68575">For the MAP estimation code example in the text, we used the</st>
    `<st c="68637">scipy.optimize.minimize</st>` <st c="68660">function to do the
    optimization of the log-posterior.</st> <st c="68715">The</st> `<st c="68719">minimize</st>`
    <st c="68727">function has the option for the user to supply a callable function
    that calculates the gradient of the objective function with respect to the objective
    function parameters.</st> <st c="68901">Work out on paper the gradient of the
    log-posterior and implement a function that returns the gradient of the log-posterior.</st>
    <st c="69026">Re-run the MAP estimation process using</st> `<st c="69066">scipy.optimize.minimize</st>`<st
    c="69089">, but when you pass in your log-posterior gradient function, you’ll
    need to look at the online documentation for the</st> `<st c="69206">scipy.optimize.minimize</st>`
    <st c="69229">function to see how your callable gradient function should be</st>
    <st c="69292">passed in.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="69302">The</st> `<st c="69307">Data/coffee_or_tea.csv</st>` <st c="69329">file
    in the GitHub repository contains two columns of data, corresponding to 250 days’
    worth of observations on what drink I had each morning (tea or coffee) when I
    was at home, and also whether it rained the night before.</st> <st c="69553">The
    two columns are called</st> `<st c="69580">rained</st>`<st c="69586">, with a
    value of</st> `<st c="69604">1</st>` <st c="69605">indicating that it rained the
    night before (</st>`<st c="69650">0</st>` <st c="69652">indicating it did not),
    and</st> `<st c="69681">coffee</st>`<st c="69687">, with a value of</st> `<st
    c="69705">1</st>` <st c="69706">indicating that I drank coffee that morning (</st>`<st
    c="69752">0</st>` <st c="69754">indicating I drank tea instead).</st> <st c="69788">Formulate
    a probabilistic model (hint – look at the binomial shopper decision example in
    the text) that models my decision to drink coffee or not, and that uses the</st>
    `<st c="69953">rained</st>` <st c="69959">variable as a predictive feature.</st>
    <st c="69994">The linear predictor for the model will take the form,</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>η</mi><mrow><mi>c</mi><mi>o</mi><mi>f</mi><mi>f</mi><mi>e</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mo>×</mo><mtext>rained</mtext></mrow></mrow></math>](img/2010.png)<st
    c="70049"><st c="70074">. Use the</st> `<st c="70084">scipy.optimize.minimize</st>`
    <st c="70107">function to obtain maximum likelihood estimates for the model</st>
    <st c="70170">parameters,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub></mrow></mrow></math>](img/2011.png)<st
    c="70182"><st c="70183">.</st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="70184">Using a</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>N</mi><mo>(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>)</mo></mrow></mrow></mrow></math>](img/2012.png)
    <st c="70193"><st c="70194">prior for both</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/2013.png)
    <st c="70210"><st c="70211">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/2014.png)
    <st c="70216"><st c="70217">in the model you formulated in Q2, obtain MAP estimates
    for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/2015.png)
    <st c="70278"><st c="70279">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/2016.png)
    <st c="70284"><st c="70285">using the same dataset used</st> <st c="70314">in
    Q2.</st></st></st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="70320">Using the posterior you developed in Q3, adapt the MCMC code example
    in the text and obtain 10,000 samples of the tuple</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfenced
    open="(" close=")"><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub></mrow></mfenced></mrow></math>](img/2017.png)<st
    c="70441"><st c="70442">. Plot separate histograms of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/2018.png)
    <st c="70472"><st c="70473">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/2019.png)
    <st c="70478"><st c="70479">from those</st> <st c="70491">10,000 samples.</st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="0">Part 2: Intermediate Concepts</st>'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="30">In this part, we will introduce you to more math concepts that you
    are very likely to encounter the longer you work in data science.</st> <st c="164">In
    contrast to Part 1, each chapter is focused on a standalone data science task,
    modeling technique, or type of data.</st> <st c="283">By the end of Part 2, you
    will have gained a solid understanding of time series data, how to run a hypothesis
    test, model complexity, how to build up a function from a set of simpler parts,
    and</st> <st c="477">network data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="490">This section contains the</st> <st c="517">following chapters:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[*<st c="536">Chapter 6</st>*](B19496_06.xhtml#_idTextAnchor314)<st c="546">,</st>
    *<st c="548">Time Series and Forecasting</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="575">Chapter 7</st>*](B19496_07.xhtml#_idTextAnchor369)<st c="585">,</st>
    *<st c="587">Hypothesis Testing</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="605">Chapter 8</st>*](B19496_08.xhtml#_idTextAnchor406)*<st c="615">,
    Model Complexity</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="633">Chapter 9</st>*](B19496_09.xhtml#_idTextAnchor449)*<st c="643">,
    Function Decomposition</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*<st c="667">Chapter 10</st>*](B19496_10.xhtml#_idTextAnchor501)*<st c="678">,
    Network Analysis</st>*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
