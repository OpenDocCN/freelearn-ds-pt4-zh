<html><head></head><body>
		<div id="_idContainer2989">
			<h1 id="_idParaDest-225" class="chapter-number"><a id="_idTextAnchor406"/><st c="0">8</st></h1>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor407"/><st c="2">Model Complexity</st></h1>
			<p><st c="18">Model complexity may seem like a strange title for a chapter. </st><st c="81">Why should we care about complexity? </st><st c="118">One concept you may have already encountered as a data scientist is that of overfitting and how an overfitted model will not make accurate predictions. </st><st c="270">However, that overfitting stems from using a model whose complexity is greater than that justified by the data. </st><st c="382">The impact of model complexity on model prediction accuracy is a nuanced one. </st><st c="460">More specifically, how you decide what is the right level of model complexity can be challenging. </st><st c="558">To address this challenge requires exploring several new concepts. </st><st c="625">We will do that exploration in this chapter and do so by covering the </st><span class="No-Break"><st c="695">following topics:</st></span></p>
			<ul>
				<li><em class="italic"><st c="712">Generalization, overfitting, and the role of model complexity</st></em><st c="774">: Here, we understand how model complexity affects the accuracy of model predictions on </st><span class="No-Break"><st c="863">unseen data</st></span></li>
				<li><em class="italic"><st c="874">The bias-variance trade-off</st></em><st c="902">: Here, we dig into the mathematical details behind the prediction accuracy ideas we introduced in the </st><span class="No-Break"><st c="1006">preceding section</st></span></li>
				<li><em class="italic"><st c="1023">Model complexity measures for model selection</st></em><st c="1069">: Here, we introduce commonly used model complexity measures and discuss their strengths </st><span class="No-Break"><st c="1159">and weaknesses</st></span></li>
			</ul>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor408"/><st c="1173">Technical requirements</st></h1>
			<p><st c="1196">This chapter will mainly be a visual one. </st><st c="1239">We will introduce the mathematical ideas and concepts through illustrative plots and schematic figures, which we will then unpack and explain at length. </st><st c="1392">Because of this, there are no code examples in this chapter, so no </st><span class="No-Break"><st c="1459">technical requirements.</st></span></p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor409"/><st c="1482">Generalization, overfitting, and the role of model complexity</st></h1>
			<p><st c="1544">What do we mean by a complex model? </st><st c="1581">Very loosely, we think of a more complex model as </st><a id="_idIndexMarker737"/><st c="1631">having more parameters or using more features. </st><st c="1678">This statement is imprecise, but the idea that model complexity broadly follows the number of model parameters/features will be precise enough for the mainly qualitative discussions of </st><span class="No-Break"><st c="1863">this chapter.</st></span></p>
			<p><st c="1876">A more complex model can fit a training dataset more closely, as it can use the extra features to explain the variation in the response variable/target variable. </st><st c="2039">What are the consequences of this increased flexibility? </st><st c="2096">As a simple example, we’ll take a look at </st><span class="No-Break"><em class="italic"><st c="2138">Figure 8</st></em></span><em class="italic"><st c="2146">.1</st></em><st c="2148">, which shows three different models fitted to a small dataset. </st><st c="2212">The black circles in each plot show the training data, while the blue circles show the hold-out sample data points, which, as you can see, represent an extrapolation challenge, since the hold-out data points are all to the right of the training data points. </st><st c="2470">Although using machine learning models in interpolation settings is more common, we’ve used an extrapolation example here because it is easier to immediately see how well each fitted model predicts the holdout data, and so it is a good experiment to start to introduce some ideas about</st><a id="_idIndexMarker738"/><st c="2755"> model complexity. </st><st c="2774">All the data points (training and hold-out) were generated with a quadratic equation (a 2</st><span class="superscript"><st c="2863">nd</st></span><st c="2866">-degree polynomial) in a single variable, </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.469em"/><st c="2909"/><st c="2910">. A significant amount of additive noise was also added to the output of the </st><span class="No-Break"><st c="2987">generated data.</st></span></p>
			<div>
				<div id="_idContainer2847" class="IMG---Figure">
					<img src="image/B19496_08_1.jpg" alt="" role="presentation"/><st c="3002"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor410"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="3111">Figure 8.1: Different polynomials fitted to a quadratic dataset</st></p>
			<p><st c="3174">The left-hand plot shows a</st><a id="_idIndexMarker739"/><st c="3201"> linear equation in </st><img src="image/2761.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.457em"/><st c="3221"/><st c="3222"> fitted to the training data. </st><st c="3252">The line shows the predictions from the fitted model across the full range of the feature </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.477em"/><st c="3342"/><st c="3343">. The linear model is obviously incorrect. </st><st c="3386">It contains fewer parameters than the model that generated the data. </st><st c="3455">It clearly doesn’t fit the training data well, since it doesn’t capture the curvature present in the training data, and there are marked differences between the line (predictions) and the dark circles of the training set. </st><st c="3677">The linear model is also very poor at predicting the hold-out data points. </st><st c="3752">The difference between the line and the lighter circles of the hold-out data points is stark – the trend in the predictions is in a different direction to that of the </st><span class="No-Break"><st c="3919">hold-out data.</st></span></p>
			<p><st c="3933">The middle plot of </st><span class="No-Break"><em class="italic"><st c="3953">Figure 8</st></em></span><em class="italic"><st c="3961">.1</st></em><st c="3963"> shows the predictions (line) from a quadratic model fitted to the training data. </st><st c="4045">The model fits the training data well and makes good predictions on the hold-out set. </st><st c="4131">Any differences between the line and the black or light circles are evenly scattered and look like the result of the significant additive noise present in the data. </st><st c="4296">It is perhaps not surprising that the quadratic model does well. </st><st c="4361">It is of precisely the same form (a 2</st><span class="superscript"><st c="4398">nd</st></span><st c="4401">-degree polynomial) as the source of the training and </st><span class="No-Break"><st c="4456">hold-out data.</st></span></p>
			<p><st c="4470">Or does the quadratic model do better than the linear model simply because it has more parameters and so its shape can “wiggle” more and, therefore, capture more of the variations present in the training data? </st><st c="4681">To test this idea, let’s increase the number of parameters in our model even further. </st><st c="4767">The right-hand plot of </st><span class="No-Break"><em class="italic"><st c="4790">Figure 8</st></em></span><em class="italic"><st c="4798">.1</st></em><st c="4800"> shows the predictions (red line) from a quartic (a 4</st><span class="superscript"><st c="4853">th</st></span><st c="4856">-degree polynomial) model. </st><st c="4884">This model has two more parameters than the quadratic model in the middle plot and three more parameters than the linear model in the left-hand plot. </st><st c="5034">The quartic model fits the training data as well as the quadratic model but predicts poorly on the hold-out data. </st><st c="5148">The trend of the predictions diverges from the trend in the hold-out data. </st><st c="5223">The red line for the quartic model is even beginning to move downwards at the right-hand edge of the plot. </st><st c="5330">If we made predictions for hold-out data at even higher values of the </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.477em"/><st c="5400"/><st c="5401"> variable, the quartic </st><a id="_idIndexMarker740"/><st c="5424">model would move in completely the opposite direction to </st><span class="No-Break"><st c="5481">the data.</st></span></p>
			<p><st c="5490">Let’s summarize our findings from that </st><span class="No-Break"><st c="5530">little experiment:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="5548">Linear model</st></strong><st c="5561">: One</st><a id="_idIndexMarker741"/><st c="5567"> parameter. </st><st c="5579">Fits the training data poorly. </st><span class="No-Break"><st c="5610">Predicts poorly.</st></span></li>
				<li><strong class="bold"><st c="5626">Quadratic model</st></strong><st c="5642">: Two</st><a id="_idIndexMarker742"/><st c="5648"> parameters. </st><st c="5661">Fit the training data well. </st><span class="No-Break"><st c="5689">Predict well.</st></span></li>
				<li><strong class="bold"><st c="5702">Quartic model</st></strong><st c="5716">: Four </st><a id="_idIndexMarker743"/><st c="5724">parameters. </st><st c="5736">Fit the training data well. </st><span class="No-Break"><st c="5764">Predict poorly.</st></span></li>
			</ul>
			<p><st c="5779">From this simple example, we can see that there appears to be an optimal level of model complexity – a model that is neither too complex nor </st><span class="No-Break"><st c="5921">too simple.</st></span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor411"/><st c="5932">Overfitting</st></h2>
			<p><st c="5944">Now, you may wonder whether the</st><a id="_idIndexMarker744"/><st c="5976"> poor performance of the quartic model was due to the hold-out data essentially being a test of the extrapolation abilities of the fitted models. </st><st c="6122">Again, we can test this idea with a simple experiment. </st><st c="6177">In </st><span class="No-Break"><em class="italic"><st c="6180">Figure 8</st></em></span><em class="italic"><st c="6188">.2</st></em><st c="6190">, we have plotted the predictions (red line) from a 12</st><span class="superscript"><st c="6244">th</st></span><st c="6247">-degree polynomial, fitted to all the data that was present in our previous experiment in </st><span class="No-Break"><em class="italic"><st c="6338">Figure 8</st></em></span><em class="italic"><st c="6346">.1</st></em><st c="6348">. In this case, our model has lots of parameters (13 in total) and has been fitted to all the data (training and hold-out) that we had in </st><span class="No-Break"><em class="italic"><st c="6486">Figure 8</st></em></span><span class="No-Break"><em class="italic"><st c="6494">.</st><a id="_idTextAnchor412"/><st c="6495">1</st></em></span><span class="No-Break"><st c="6497">.</st></span></p>
			<div>
				<div id="_idContainer2851" class="IMG---Figure">
					<img src="image/B19496_08_2.jpg" alt="" role="presentation"/><st c="6498"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6541">Figure 8.2: A plot of predictions (the red line) from a 12th-degree polynomial, fitted to the same data in the previous figure</st></p>
			<p><st c="6667">The dashed light-blue line is the true equation that was used to generate the data. </st><st c="6752">The scatter of the individual data points (the black dots) around the dashed line is just the effect of the additive noise we included when generating the data. </st><st c="6913">The dashed line represents the ground truth and is the best we can hope for from any model we fit to the data. </st><st c="7024">A fitted model whose red line coincided with the dashed blue line would be considered “exact,” “optimal,” </st><span class="No-Break"><st c="7130">or “perfect.”</st></span></p>
			<p><st c="7143">The red line for the 12</st><span class="superscript"><st c="7167">th</st></span><st c="7170">-degree polynomial model we have fitted in </st><span class="No-Break"><em class="italic"><st c="7214">Figure 8</st></em></span><em class="italic"><st c="7222">.2</st></em><st c="7224"> wiggles around the dashed blue line. </st><st c="7262">This tells us that, first, the fitted model in </st><span class="No-Break"><em class="italic"><st c="7309">Figure 8</st></em></span><em class="italic"><st c="7317">.2</st></em><st c="7319"> is not perfect, and second, while the model has broadly fitted the quadratic trend of the true equation, it has also fitted to some of the wiggles in the data that are due to the noise in it. </st><st c="7512">In this case, we say</st><a id="_idIndexMarker745"/><st c="7532"> that the fitted model has </st><em class="italic"><st c="7559">overfitted</st></em> <span class="No-Break"><st c="7569">the data.</st></span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor413"/><st c="7579">Why overfitting is bad</st></h2>
			<p><st c="7602">Is overfitting a </st><a id="_idIndexMarker746"/><st c="7620">problem? </st><st c="7629">Yes, it is. </st><st c="7641">Take another look at </st><span class="No-Break"><em class="italic"><st c="7662">Figure 8</st></em></span><em class="italic"><st c="7670">.2</st></em><st c="7672">. What would happen if our fitted model made a prediction for a value of </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="7745"/><st c="7746"> somewhere in the middle of the training data but where we didn’t already have an existing data point? </st><st c="7849">What would happen if we then measured a new data point at </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="7907"/><st c="7908">? How close would our prediction be to the new measured data point? </st><st c="7976">We wouldn’t expect them to be identical; after all, the new data point, like those in the training set, has noise added </st><span class="No-Break"><st c="8096">to it.</st></span></p>
			<p><st c="8102">The predicted value of the target variable, </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.447em"/><st c="8147"/><st c="8157">, is given by the position on the red line, corresponding to where the position </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.467em"/><st c="8237"/><st c="8238"> is. </st><st c="8243">Our </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.466em"/><st c="8247"/><st c="8248"> value may correspond to where those wiggles in the red line are, and so our prediction would follow the pattern present in the noise in the training set. </st><st c="8403">The problem is that for the new data point, the noise, which is random and therefore unpredictable, is highly unlikely to follow the same wiggles that were present in the noise in the training data. </st><st c="8602">The prediction from our overfitted model is likely to be further away from the true value than if we had used a model that didn’t follow the wiggles of the noise in the training data. </st><st c="8786">This means that because our model overfitted to the noise in the training set, we will have increased the size of the error the model makes on new </st><span class="No-Break"><st c="8933">unseen data.</st></span></p>
			<p><st c="8945">Our predictions would have been better on average if our prediction model just followed the more general trend present in the training data, as represented by the dashed blue line. </st><st c="9127">Predictions that follow the blue line would be correct on average. </st><st c="9194">Since fitting to the general trends present in a training set produces a model that predicts well, we say that a good model is one that </st><span class="No-Break"><em class="italic"><st c="9330">generalizes</st></em></span><span class="No-Break"><st c="9341"> well.</st></span></p>
			<p><st c="9347">As we increase the </st><a id="_idIndexMarker747"/><st c="9367">number of parameters in our model, we increase its ability to follow the noise present in its training data. </st><st c="9476">A fitted model with a very high number of parameters would pass almost exactly through the data. </st><st c="9573">This is illustrated schematically in </st><span class="No-Break"><em class="italic"><st c="9610">Figure 8</st></em></span><em class="italic"><st c="9618">.3</st></em><st c="9620">, which shows a highly flexible model (the line) overfitted to a small number of data points (the circles) that broadly follow a </st><span class="No-Break"><st c="9749">quadratic tr</st><a id="_idTextAnchor414"/><st c="9761">end.</st></span></p>
			<div>
				<div id="_idContainer2857" class="IMG---Figure">
					<img src="image/B19496_08_3.jpg" alt="" role="presentation"/><st c="9766"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="9793">Figure 8.3: A schematic of a highly overfitted model</st></p>
			<p><st c="9845">Although </st><span class="No-Break"><em class="italic"><st c="9855">Figure 8</st></em></span><em class="italic"><st c="9863">.3</st></em><st c="9865"> is schematic and the line does not represent a model actually fitted to a dataset, it is still useful to draw out </st><span class="No-Break"><st c="9980">some conclusions:</st></span></p>
			<ul>
				<li><st c="9997">The smaller the training dataset is, the easier it is for a complex model to overfit. </st><st c="10084">This highlights that overfitting is dependent on both the model complexity and the </st><span class="No-Break"><st c="10167">training data.</st></span></li>
				<li><st c="10181">If we were to use the fitted model in </st><span class="No-Break"><em class="italic"><st c="10220">Figure 8</st></em></span><em class="italic"><st c="10228">.3</st></em><st c="10230"> to make a prediction for a data point it has already seen (i.e., one in the training dataset), the model would make a near-perfect prediction. </st><st c="10374">This is because the high complexity of the model has allowed it to effectively “memorize” the training data. </st><st c="10483">No “learning” has occurred. </st><st c="10511">The </st><a id="_idIndexMarker748"/><st c="10515">model is just retrieving data, rather than identifying and learning </st><span class="No-Break"><st c="10583">general trends.</st></span></li>
			</ul>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor415"/><st c="10598">Overfitting increases the variability of predictions</st></h2>
			<p><st c="10651">Let’s look at another </st><a id="_idIndexMarker749"/><st c="10674">consequence of overfitting. </st><st c="10702">So far, we have discussed what happens to predictive accuracy when we overfit to a single particular dataset. </st><st c="10812">We could get lucky. </st><st c="10832">It could be that we have a highly complex model but, when fitted to our training dataset, we get a model that follows the general trends well and, therefore, generalizes well. </st><st c="11008">What we’ll now illustrate is that with increasing model complexity, that is increasingly unlikely to happen. </st><st c="11117">We are unlikely to </st><span class="No-Break"><st c="11136">be lucky.</st></span></p>
			<p><st c="11145">Take a look at </st><span class="No-Break"><em class="italic"><st c="11161">Figure 8</st></em></span><em class="italic"><st c="11169">.4</st></em><st c="11171">. It shows (dashed) prediction curves from 12</st><span class="superscript"><st c="11216">th</st></span><st c="11219">-degree polynomial models, obtained by fitting to different training datasets. </st><st c="11299">The different training datasets were all generated by the same underlying “ground-truth” model that was used to generate the data in </st><span class="No-Break"><em class="italic"><st c="11432">Figure 8</st></em></span><em class="italic"><st c="11440">.1</st></em><st c="11442"> and </st><span class="No-Break"><em class="italic"><st c="11447">Figure</st><a id="_idTextAnchor416"/><st c="11453"> 8</st></em></span><span class="No-Break"><em class="italic"><st c="11455">.2</st></em></span><span class="No-Break"><st c="11457">.</st></span></p>
			<div>
				<div id="_idContainer2858" class="IMG---Figure">
					<img src="image/B19496_08_4.jpg" alt="" role="presentation"/><st c="11458"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11525">Figure 8.4: The prediction curves of the 12th-degree polynomial models obtained from different training sets</st></p>
			<p><st c="11633">Since each 12</st><span class="superscript"><st c="11647">th</st></span><st c="11650">-degree </st><a id="_idIndexMarker750"/><st c="11659">polynomial model will overfit to the noise present in its corresponding training data, each model displays wiggles (as we can see in </st><span class="No-Break"><em class="italic"><st c="11792">Figure 8</st></em></span><em class="italic"><st c="11800">.4</st></em><st c="11802">). </st><st c="11806">However, each training dataset is slightly different, so each 12</st><span class="superscript"><st c="11870">th</st></span><st c="11873">-degree polynomial displays different wiggles. </st><st c="11921">We can see this in the variation of the shapes of the different dashed lines. </st><st c="11999">But the dashed lines show what each model would predict for the given value of </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.471em"/><st c="12078"/><st c="12079"> on the </st><em class="italic"><st c="12087">x</st></em><st c="12088">-axis. </st><st c="12095">For a given value of </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.471em"/><st c="12116"/><st c="12117">, there will be significant variation in the prediction at </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="12176"/><st c="12177"> across the different models fitted to the different </st><span class="No-Break"><st c="12230">training datasets.</st></span></p>
			<p><st c="12248">As we increase the model complexity (e.g., go from fitting 12</st><span class="superscript"><st c="12310">th</st></span><st c="12313">-degree polynomials to fitting 20</st><span class="superscript"><st c="12347">th</st></span><st c="12350">-degree polynomials), those fitted models will follow ever more closely the wiggles in the noise in their training dataset. </st><st c="12475">Consequently, the size of the wiggles in each fitted model will get bigger, and the resulting variability in predictions across training sets </st><span class="No-Break"><st c="12617">will increase.</st></span></p>
			<p><st c="12631">This is a problem. </st><st c="12651">Why so? </st><st c="12659">What </st><span class="No-Break"><em class="italic"><st c="12664">Figure 8</st></em></span><em class="italic"><st c="12672">.4</st></em><st c="12674"> illustrates is that at the same value of </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.470em"/><st c="12716"/><st c="12717">, different training sets give different predictions. </st><st c="12771">There is a random element to our prediction, and the variance of that random element increases with increasing model complexity. </st><st c="12900">However, in real life, we will only ever use one training dataset. </st><st c="12967">Are we going to hope that we are lucky and that our particular training data leads to predictions that are close to the true value? </st><st c="13099">A better strategy would be to ensure that we have low sensitivity of the predictions to the choice of training set, by not using an overly </st><span class="No-Break"><st c="13238">complex model.</st></span></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor417"/><st c="13252">Underfitting is also a problem</st></h2>
			<p><st c="13283">Since overfitting to noise in a</st><a id="_idIndexMarker751"/><st c="13315"> training dataset arises because we have too many parameters in our model relative to the amount of data/signal in the training data, you might get the impression that we should use a model with as few parameters as possible. </st><st c="13541">This is not the case. </st><st c="13563">It is possible for a model to be too simple, to contain too few parameters. </st><span class="No-Break"><em class="italic"><st c="13639">Figure 8</st></em></span><em class="italic"><st c="13647">.5</st></em><st c="13649"> shows the same dataset as in </st><span class="No-Break"><em class="italic"><st c="13679">Figure 8</st></em></span><em class="italic"><st c="13687">.2</st></em><st c="13689">, but the fitted model represented by the red line in </st><span class="No-Break"><em class="italic"><st c="13743">Figure 8</st></em></span><em class="italic"><st c="13751">.5</st></em><st c="13753"> is now just a constant and corresponds to the mean value of the target variable, </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.439em"/><st c="13835"/><st c="13858">, in the training data. </st><st c="13882">Clearly, this fitted model does not follow any wiggles in the data that are due to noise. </st><st c="13972">This model is definitely not overfitted. </st><st c="14013">But, equally, the model is not complex enough to follow the general quadratic trend in the data indicated by the dashed blue line. </st><st c="14144">We say that this model </st><span class="No-Break"><st c="14167">is </st></span><span class="No-Break"><em class="italic"><st c="14170">under</st><a id="_idTextAnchor418"/><st c="14175">fitted</st></em></span><span class="No-Break"><st c="14182">.</st></span></p>
			<div>
				<div id="_idContainer2864" class="IMG---Figure">
					<img src="image/B19496_08_5.jpg" alt="" role="presentation"/><st c="14183"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14208">Figure 8.5: A low complexity model consisting of a constant equal to the mean of y</st></p>
			<p><st c="14290">What are the consequences of an underfitted model? </st><st c="14342">It should be qualitatively clear that a low complexity model that underfits a training dataset will make poor predictions on both training data points and on any unseen data points, as it has not learned the general trends that all data points follow. </st><st c="14594">Prediction errors will be large for both training data and any holdout data. </st><st c="14671">As with an overfitted model, an underfitted model </st><span class="No-Break"><st c="14721">generalizes poorly.</st></span></p>
			<p><st c="14740">In contrast to overly</st><a id="_idIndexMarker752"/><st c="14762"> complex overfitted models, low complexity models do not display much variability in their predictions, at a given holdout point </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.468em"/><st c="14891"/><st c="14892">, across different training datasets. </st><span class="No-Break"><em class="italic"><st c="14930">Figure 8</st></em></span><em class="italic"><st c="14938">.6</st></em><st c="14940"> shows the prediction curves (dashed lines) for a model that is only a constant (a 0</st><span class="superscript"><st c="15024">th</st></span><st c="15027">-degree polynomial) when fitted to the same training datasets used in </st><em class="italic"> </em><span class="No-Break"><em class="italic"><st c="15098">Figure 8</st></em></span><em class="italic"><st c="15106">.4</st></em><st c="15108">. For comparison, we have also added the true model line, which is a quadratic and shown by the light blue </st><span class="No-Break"><st c="15215">dashe</st><a id="_idTextAnchor419"/><st c="15220">d line.</st></span></p>
			<div>
				<div id="_idContainer2866" class="IMG---Figure">
					<img src="image/B19496_08_6.jpg" alt="" role="presentation"/><st c="15228"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15274">Figure 8.6: The prediction curves of 0th-degree polynomial models obtained from different training sets</st></p>
			<p><st c="15377">There is some variation in the prediction curves of our 0</st><span class="superscript"><st c="15435">th</st></span><st c="15438">-degree polynomial, but not much. </st><st c="15473">In fact, the variation is so small that two of the prediction curves lie almost on top of each other – there are, in fact, four horizontal dashed lines plotted in </st><span class="No-Break"><em class="italic"><st c="15636">Figure 8</st></em></span><em class="italic"><st c="15644">.6</st></em><st c="15646">. The variation is considerably smaller than the variation in prediction curves we got from fitting 12</st><span class="superscript"><st c="15748">th</st></span><st c="15751">-degree polynomials to the same </st><span class="No-Break"><st c="15784">training datasets.</st></span></p>
			<p><st c="15802">The benefit of this is that, in contrast to using an overly complex model, it doesn’t really matter which training dataset we use; our predictions will come out similar. </st><st c="15973">There is little random variation in predictions between using different training sets of the same size. </st><st c="16077">The downside is that those predictions will be consistently poor. </st><st c="16143">No matter which training dataset we use, predictions from our low complexity model are typically a long way from the true value represented by the curved dashed light blue line. </st><st c="16321">Our low-complexity model </st><span class="No-Break"><st c="16346">displays </st></span><span class="No-Break"><em class="italic"><st c="16355">bias</st></em></span><span class="No-Break"><st c="16359">.</st></span></p>
			<p><st c="16360">Clearly, there is a </st><a id="_idIndexMarker753"/><st c="16381">sweet spot in terms of the number of model parameters. </st><st c="16436">At that sweet spot, the model complexity is sufficient to capture the general trends that led to the training data, but it is not sufficient to overfit to the noise in the data. </st><st c="16614">At the sweet spot, the fitted model will generalize well. </st><st c="16672">To identify that sweet spot, we need to make things more quantitative and explain how we define prediction errors and measure generalization. </st><st c="16814">We will do </st><span class="No-Break"><st c="16825">that now.</st></span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor420"/><st c="16834">Measuring prediction error</st></h2>
			<p><st c="16861">A data point</st><a id="_idIndexMarker754"/><st c="16874"> is a pair</st><a id="_idIndexMarker755"/><st c="16884"> of values </st><img src="image/2775.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.307em;height:0.805em;width:1.876em"/><st c="16895"/><st c="16896">. The </st><img src="image/2776.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.428em"/><st c="16902"/><st c="16903"> value is the value of the target variable (or response variable) we observed when we had the corresponding vector, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.473em"/><st c="17019"/><st c="17020">, for the feature variables. </st><st c="17049">Remember that for the same</st><img src="image/2778.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.742em"/><st c="17075"/><st c="17077"> value, we could get multiple different values for </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.455em"/><st c="17128"/><st c="17138"> because the observation </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.455em"/><st c="17162"/><st c="17172"> contains a random component. </st><st c="17201">A dataset is just a set of multiple datapoints. </st><st c="17249">In general, we can denote a dataset </st><span class="No-Break"><st c="17285">as </st></span><span class="No-Break"><img src="image/2781.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;{&quot; close=&quot;}&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1,2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.440em;height:1.138em;width:11.031em"/><st c="17288"/></span><span class="No-Break"><st c="17289">.</st></span></p>
			<p><st c="17290">Let’s use </st><img src="image/2782.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.390em;height:1.288em;width:5.382em"/><st c="17301"/><st c="17314"> to denote the model equation of our trained model. </st><st c="17365">The notation </st><img src="image/2783.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:1.188em;width:3.049em"/><st c="17378"/><st c="17389"> is used to denote the fact that the trained model will depend on some parameters, </st><img src="image/1778.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.823em;width:0.483em"/><st c="17471"/><st c="17472">, and the training data, </st><img src="image/2785.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.988em;width:1.771em"/><st c="17497"/><st c="17503">. The difference between the model prediction and the observed value of the target variable for the </st><img src="image/909.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.760em;width:0.813em"/><st c="17603"/><st c="17615"> datapoint is </st><img src="image/2787.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.390em;height:1.288em;width:6.731em"/><st c="17628"/><st c="17646">, which we will shorten to just </st><img src="image/2788.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:0.888em;width:3.483em"/><st c="17678"/><st c="17679">. To get a measure of the typical error made by our model, we calculate </st><a id="_idIndexMarker756"/><st c="17751">the </st><strong class="bold"><st c="17755">mean squared </st></strong><span class="No-Break"><strong class="bold"><st c="17768">error</st></strong></span><span class="No-Break"><st c="17773"> (</st></span><span class="No-Break"><strong class="bold"><st c="17775">MSE</st></strong></span><span class="No-Break"><st c="17778">):</st></span></p>
			<p><img src="image/2789.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;MSE&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.767em;height:2.191em;width:9.571em"/><st c="17781"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17783">Eq. </st><st c="17787">1</st></p>
			<p><st c="17788">This is like when we studied least-squares model fitting in </st><a href="B19496_04.xhtml#_idTextAnchor216"><span class="No-Break"><em class="italic"><st c="17848">Chapter 4</st></em></span></a><st c="17857">. We take the square of the individual errors to ensure a positive number so that positive and negative errors don’t just cancel out to zero. </st><st c="17999">To get back to a “typical” error, we can finally take the square root of the MSE to get </st><a id="_idIndexMarker757"/><st c="18087">the </st><strong class="bold"><st c="18091">root mean squared </st></strong><span class="No-Break"><strong class="bold"><st c="18109">error</st></strong></span><span class="No-Break"><st c="18114"> (</st></span><span class="No-Break"><strong class="bold"><st c="18116">RMSE</st></strong></span><span class="No-Break"><st c="18120">):</st></span></p>
			<p><img src="image/2790.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;RMSE&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;msqrt&gt;&lt;mtext&gt;MSE&lt;/mtext&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.055em;height:0.889em;width:6.475em"/><st c="18123"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="18125">Eq. </st><st c="18129">2</st></p>
			<p><st c="18130">We can calculate the </st><a id="_idIndexMarker758"/><st c="18151">MSE for </st><a id="_idIndexMarker759"/><st c="18159">a given dataset, </st><img src="image/2791.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.689em"/><st c="18176"/><st c="18177">. This doesn’t necessarily have to be the training dataset. </st><st c="18237">We can evaluate the MSE for the hold-out dataset, </st><img src="image/2792.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.988em;width:2.313em"/><st c="18287"/><st c="18291">, if we want to. </st><st c="18308">Since the accuracy of predictions on the hold-out dataset gives us a feel for how well a model generalizes, we call </st><img src="image/2793.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;RMSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:5.783em"/><st c="18424"/><st c="18440"> the generalization error, while we call </st><img src="image/2794.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;RMSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:5.090em"/><st c="18480"/><st c="18493"> the training error. </st><st c="18513">However, you will probably find that MSE and RMSE are used interchangeably – for example, </st><img src="image/2795.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:5.067em"/><st c="18603"/><st c="18619"> can be referred to as the generalization error even though it is obviously an average squared error. </st><st c="18720">From a qualitative perspective, it is inconsequential, and I also tend to use the term “generalization error” when I’m referring </st><span class="No-Break"><st c="18849">to </st></span><span class="No-Break"><img src="image/2796.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:5.039em"/><st c="18852"/></span><span class="No-Break"><st c="18867">.</st></span></p>
			<p><st c="18868">We can now go back and take our previous qualitative conclusions about overfitting and use the MSE to make them more quantitative. </st><st c="19000">We know that as we increase the model complexity by increasing the number of parameters/features in our model, we can fit the training data points evermore closely. </st><st c="19165">This means that for a given training dataset, </st><img src="image/2797.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.988em;width:1.804em"/><st c="19211"/><st c="19217">, we expect </st><img src="image/2798.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:4.438em"/><st c="19229"/><st c="19242"> to decrease monotonically as we increase the model complexity. </st><st c="19305">We also know the generalization ability of our model initially improves as we increase the number of parameters and then deteriorates. </st><st c="19440">This means we expect </st><img src="image/2799.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.015em;height:0.679em;width:1.908em"/><st c="19461"/><span class="_-----MathTools-_Math_Text"><img src="image/2800.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;MSE&lt;/mml:mtext&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.390em;height:1.088em;width:3.098em"/><st c="19462"/></span><st c="19472"> to go through a minimum. </st><st c="19497">These two quantitative statements are summarized schematically i</st><a id="_idTextAnchor421"/><st c="19561">n </st><span class="No-Break"><em class="italic"><st c="19564">Figure 8</st></em></span><span class="No-Break"><em class="italic"><st c="19572">.7</st></em></span><span class="No-Break"><st c="19574">.</st></span></p>
			<div>
				<div id="_idContainer2893" class="IMG---Figure">
					<img src="image/B19496_08_7.jpg" alt="" role="presentation"/><st c="19575"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="19660">Figure 8.7: The behavior of training and generalization errors with increasing model complexity</st></p>
			<p><st c="19755">In </st><span class="No-Break"><em class="italic"><st c="19759">Figure 8</st></em></span><em class="italic"><st c="19767">.7</st></em><st c="19769">, we have indicated the position where the generalization error is at its minimum. </st><st c="19852">We have marked this as “optimal model complexity.” Why does the smallest generalization error identify the optimal model? </st><st c="19974">Why do we want the generalization error to be minimal and not the training error? </st><st c="20056">Well, it is for predictions we want to use the model. </st><st c="20110">We don’t need to predict the training data – we already have it. </st><st c="20175">The point where the generalization error is smallest represents the point where the performance of the model is optimal for its intended use. </st><st c="20317">The minimum in the generalization error is the sweet spot we</st><a id="_idIndexMarker760"/><st c="20377"> referred </st><span class="No-Break"><st c="20387">to earlier.</st></span></p>
			<p><span class="No-Break"><em class="italic"><st c="20398">Figure 8</st></em></span><em class="italic"><st c="20407">.7</st></em><st c="20409"> succinctly represents the key ideas that we have introduced in this section, so this is a good point to summarize what we </st><span class="No-Break"><st c="20532">have learned.</st></span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor422"/><st c="20545">What we learned</st></h2>
			<p><st c="20561">In this section, we learned </st><span class="No-Break"><st c="20590">the following:</st></span></p>
			<ul>
				<li><st c="20604">Model complexity broadly correlates with the number of features or parameters in </st><span class="No-Break"><st c="20686">a model.</st></span></li>
				<li><st c="20694">A model that is too complex will overfit to the noise in training data. </st><st c="20767">It will predict well on the training data points but predict poorly on </st><span class="No-Break"><st c="20838">unseen data.</st></span></li>
				<li><st c="20850">A model that is not complex enough will underfit the general trends and patterns present in training data. </st><st c="20958">It will predict poorly on both the training data points and </st><span class="No-Break"><st c="21018">unseen data.</st></span></li>
				<li><st c="21030">A model that predicts well on unseen data is said to generalize. </st><st c="21096">It does so by not fitting to the noise in the training data and, instead, by learning the general trends and </st><span class="No-Break"><st c="21205">patterns present.</st></span></li>
				<li><st c="21222">Overly complex models have a large random component to their predictions, arising from the choice of training data used. </st><st c="21344">In contrast, low-complexity models will show little variation in their predictions across different training sets but will </st><span class="No-Break"><st c="21467">display bias.</st></span></li>
				<li><st c="21480">We can use MSE and RMSE to quantify </st><span class="No-Break"><st c="21517">prediction errors.</st></span></li>
				<li><st c="21535">The MSE and RMSE on the training dataset monotonically decrease with increasing model complexity, while the MSE and RMSE on a holdout dataset will display a minimum – first decreasing and then increasing as we increase </st><span class="No-Break"><st c="21755">model complexity.</st></span></li>
			</ul>
			<p><st c="21772">Having learned the basic ideas behind model complexity and how it affects the generalization abilities of a model, in the next section we will dig deeper into the mathematical detail behind the generalization error curve and introduce a modern twist to </st><span class="No-Break"><st c="22026">its behavior.</st></span></p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor423"/><st c="22039">The bias-variance trade-off</st></h1>
			<p><st c="22067">The generalization</st><a id="_idIndexMarker761"/><st c="22086"> error curve in </st><span class="No-Break"><em class="italic"><st c="22102">Figure 8</st></em></span><em class="italic"><st c="22110">.7</st></em><st c="22112"> shows a minimum. </st><st c="22130">In the preceding section, we gave a qualitative explanation of why we expected the generalization error to first decrease and then increase with increasing model complexity and why, therefore, this leads to a minimum in the generalization error curve. </st><st c="22382">But to get a quantitative idea of why the generalization error curve displays a minimum and what controls its position, we need to dig into the math behind </st><span class="No-Break"><st c="22538">the curve.</st></span></p>
			<p><st c="22548">The generalization error curve</st><a id="_idIndexMarker762"/><st c="22579"> is made up of two competing contributions, one increasing with model complexity and the other decreasing. </st><st c="22686">It is the competition between these two contributions that leads to the minimum. </st><st c="22767">Those two contributions are, first, the </st><em class="italic"><st c="22807">bias</st></em><st c="22811"> in a model’s prediction at a holdout point, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.470em"/><st c="22856"/><st c="22857">, and second, the </st><em class="italic"><st c="22875">variance</st></em><st c="22883"> in the model’s prediction at the holdout point, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="22932"/><st c="22933"> , with the variance arising from the sensitivity of the model’s prediction to the precise choice of training data. </st><st c="23048">These two competing contributions are essentially what we highlighted in </st><span class="No-Break"><em class="italic"><st c="23121">Figure 8</st></em></span><em class="italic"><st c="23129">.4</st></em><st c="23131"> and </st><span class="No-Break"><em class="italic"><st c="23136">Figure 8</st></em></span><em class="italic"><st c="23144">.6 </st></em><st c="23147">in the </st><span class="No-Break"><st c="23154">previous section.</st></span></p>
			<p><st c="23171">Mathematically, we </st><span class="No-Break"><st c="23191">find tha</st><a id="_idTextAnchor424"/><st c="23199">t,</st></span></p>
			<p><img src="image/2803.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;MSE&lt;/mtext&gt;&lt;mtext&gt;on&lt;/mtext&gt;&lt;mtext&gt;holdout&lt;/mtext&gt;&lt;mtext&gt;data&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;msup&gt;&lt;mtext&gt;Bias&lt;/mtext&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Variance&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.015em;height:0.726em;width:15.937em"/><st c="23202"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23241">Eq. </st><st c="23245">3</st></p>
			<p><st c="23246">We’ll go through the derivation of </st><em class="italic"><st c="23281">Eq. </st><st c="23285">3</st></em><st c="23286"> in a moment, as it is instructive to do so and because the derivation of </st><em class="italic"><st c="23360">Eq. </st><st c="23364">3</st></em><st c="23365"> is more subtle than we have hinted at. </st><st c="23405">We know from our qualitative discussions of </st><span class="No-Break"><em class="italic"><st c="23449">Figure 8</st></em></span><em class="italic"><st c="23457">.4</st></em><st c="23459"> and </st><span class="No-Break"><em class="italic"><st c="23464">Figure 8</st></em></span><em class="italic"><st c="23472">.6</st></em><st c="23474"> that with increasing model complexity, first, the bias of predictions decreases, and second, the variance of predictions increases. </st><st c="23607">Schematically, we illustrated this in </st><span class="No-Break"><em class="italic"><st c="23645">Figure 8</st></em></span><em class="italic"><st c="23653">.8</st></em><st c="23655">, which shows the original generalization error curve from </st><span class="No-Break"><em class="italic"><st c="23714">Figure 8</st></em></span><em class="italic"><st c="23722">.7</st></em><st c="23724">, with the bias and variance curves</st><a id="_idTextAnchor425"/> <span class="No-Break"><st c="23759">now overlayed.</st></span></p>
			<div>
				<div id="_idContainer2897" class="IMG---Figure">
					<img src="image/B19496_08_8.jpg" alt="" role="presentation"/><st c="23774"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23853">Figure 8.8: A schematic plot of the bias and variance decomposition of the generalization error</st></p>
			<p><st c="23948">What are the </st><a id="_idIndexMarker763"/><st c="23962">consequences of </st><em class="italic"><st c="23978">Eq. </st><st c="23982">3</st></em><st c="23983">? </st><span class="No-Break"><em class="italic"><st c="23985">Figure 8</st></em></span><em class="italic"><st c="23993">.8</st></em><st c="23995"> shows that we must always make a trade-off between bias and variance. </st><st c="24066">In fact, the decomposition of the generalization error that </st><em class="italic"><st c="24126">Eq. </st><st c="24130">3</st></em><st c="24131"> represents is referred to as the “bias-variance trade-off.” At the point of the minimum generalization error, we have managed to optimize this trade-off. </st><st c="24286">But the fact remains that, at any point on the generalization error curve, we have traded bias against variance. </st><st c="24399">For a given training dataset, if we want a model with a smaller bias, we can do so by increasing the model complexity, but we pay the price of increased uncertainty in our model predictions. </st><st c="24590">We must be aware of that; there is no free lunch. </st><st c="24640">Likewise, if we want a model with smaller variance (uncertainty) in its predictions, we can do so by decreasing the model complexity, but we pay the price of increased bias in </st><span class="No-Break"><st c="24816">those predictions.</st></span></p>
			<p><st c="24834">The only way we can decrease both bias and variance is to increase the size of the training data. </st><st c="24933">This will ensure that any increased model complexity has to be used to explain the extra fine-grained trends and patterns that the extra data reveals, rather than used to overfit to noise in </st><span class="No-Break"><st c="25124">the data.</st></span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor426"/><st c="25133">Proof of the bias-variance trade-off formula</st></h2>
			<p><st c="25178">To identify the two contributions</st><a id="_idIndexMarker764"/><st c="25212"> to the generalization error curve, we’ll calculate the expected generalization error curve. </st><st c="25305">This gives us the typical shape of the generalization error curve, not specific to any particular training dataset. </st><st c="25421">We’ll specifically look at the expected MSE of the holdout dataset. </st><st c="25489">This is </st><span class="No-Break"><st c="25497">defined </st><a id="_idTextAnchor427"/><st c="25505">as,</st></span></p>
			<p><img src="image/2804.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-1.014em;height:2.270em;width:15.163em"/><st c="25508"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25557">Eq. </st><st c="25561">4</st></p>
			<p><st c="25562">For simplicity of notation, we have omitted the dependence of </st><img src="image/2805.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.777em;width:1.653em"/><st c="25624"/><st c="25625"> on the model parameters </st><img src="image/1778.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.823em;width:0.473em"/><st c="25650"/><st c="25651"> in </st><span class="No-Break"><em class="italic"><st c="25655">Eq. </st><st c="25659">4</st></em></span><span class="No-Break"><st c="25660">.</st></span></p>
			<p><st c="25661">To understand </st><em class="italic"><st c="25676">Eq. </st><st c="25680">4</st></em><st c="25681">, we need to understand </st><img src="image/2807.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.532em;height:1.715em;width:9.843em"/><st c="25705"/><st c="25725"> averaged over all the holdout data points </st><img src="image/2808.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.307em;height:0.805em;width:1.959em"/><st c="25767"/><st c="25768">. </st></p>
			<p><st c="25770">If we expand out </st><img src="image/2809.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.440em;height:1.574em;width:6.791em"/><st c="25787"/><st c="25788">, we can write </st><span class="No-Break"><st c="25803">this </st><a id="_idTextAnchor428"/><st c="25808">as,</st></span></p>
			<p><img src="image/2810.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.532em;height:1.715em;width:30.456em"/><st c="25811"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25886">Eq. </st><st c="25890">5</st></p>
			<p><st c="25891">We’ll take each of the terms on the right-hand side of </st><em class="italic"><st c="25946">Eq. </st><st c="25950">5</st></em><st c="25951"> one at a time. </st><st c="25967">For the first term, we’re going to add the assumption that the observed value of the holdout target value, </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.439em"/><st c="26074"/><st c="26097">, is just a noise-corrupted version of a ground-truth value, so </st><span class="No-Break"><st c="26161">we have,</st></span></p>
			<p><img src="image/2812.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:0.860em;width:6.386em"/><st c="26169"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26183">Eq. </st><st c="26187">6</st></p>
			<p><st c="26188">We’ll assume that </st><img src="image/384.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.398em"/><st c="26206"/><st c="26207"> is a random variable with zero mean and variance </st><img src="image/1828.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.715em;width:0.874em"/><st c="26257"/><st c="26260">. The </st><img src="image/2815.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.860em;width:2.619em"/><st c="26266"/><st c="26267"> function is a deterministic function – it has no randomness in it. </st><st c="26335">It is the value we would get for the target variable at a holdout point, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.471em"/><st c="26408"/><st c="26409">, if there were no added noise. </st><st c="26441">We’ll calculate the expectation of </st><img src="image/2817.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.440em;height:1.574em;width:6.682em"/><st c="26476"/><st c="26494"> over </st><img src="image/2224.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.385em"/><st c="26499"/><st c="26500"> to represent the averaging over the holdout value, </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="26552"/><st c="26575">. This means in </st><em class="italic"><st c="26591">Eq. </st><st c="26595">5</st></em><st c="26596">, we replace </st><img src="image/2820.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.532em;height:1.193em;width:1.713em"/><st c="26609"/><st c="26616"> with </st><img src="image/2821.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.532em;height:1.193em;width:2.071em"/><st c="26621"/><st c="26634">. With that additional change, </st><span class="No-Break"><st c="26665">we g</st><a id="_idTextAnchor429"/><st c="26669">et,</st></span></p>
			<p><img src="image/2822.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.532em;height:1.284em;width:23.406em"/><st c="26673"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26724">Eq. </st><st c="26728">7</st></p>
			<p><st c="26729">In deriving the very right-hand side of </st><em class="italic"><st c="26769">Eq. </st><st c="26773">7</st></em><st c="26774">, we have made use of the fact that the random additive noise </st><img src="image/2224.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ε&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.385em"/><st c="26836"/><st c="26837"> has mean zero, and that </st><img src="image/2824.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.788em;width:1.364em"/><st c="26862"/><st c="26863"> is just a fixed number with </st><span class="No-Break"><st c="26892">no randomness.</st></span></p>
			<p><st c="26906">For the second term in </st><em class="italic"><st c="26930">Eq. </st><st c="26934">5</st></em><st c="26935">, </st><span class="No-Break"><st c="26937">we fi</st><a id="_idTextAnchor430"/><st c="26942">nd,</st></span></p>
			<p><img src="image/2825.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.532em;height:1.480em;width:28.246em"/><st c="26946"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27020">Eq. </st><st c="27024">8</st></p>
			<p><st c="27025">The final term on the right-hand side of </st><em class="italic"><st c="27066">Eq. </st></em><span class="No-Break"><em class="italic"><st c="27070">5</st></em></span><span class="No-Break"> <a id="_idTextAnchor431"/><st c="27071">is,</st></span></p>
			<p><img src="image/2826.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.582em;height:1.765em;width:32.931em"/><st c="27074"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27076">Eq. </st><st c="27080">9</st></p>
			<p><st c="27081">The very </st><a id="_idIndexMarker765"/><st c="27090">right-hand side of </st><em class="italic"><st c="27109">Eq. </st><st c="27113">9</st></em><st c="27114"> follows from the fact that for any random variable </st><img src="image/22.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.020em;height:0.482em;width:0.417em"/><st c="27166"/><st c="27167"> we have, </st><img src="image/2828.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.181em;height:0.933em;width:9.228em"/><st c="27177"/><st c="27201">, and so re-arranging we </st><span class="No-Break"><st c="27226">have </st></span><span class="No-Break"><img src="image/2829.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.181em;height:0.933em;width:9.228em"/><st c="27231"/></span><span class="No-Break"><st c="27255">.</st></span></p>
			<p><st c="27256">Plugging the results from </st><em class="italic"><st c="27283">Eq. </st><st c="27287">7</st></em><st c="27288">, </st><em class="italic"><st c="27290">Eq. </st><st c="27294">8</st></em><st c="27295">, and </st><em class="italic"><st c="27301">Eq. </st><st c="27305">9</st></em><st c="27306"> into </st><em class="italic"><st c="27312">Eq. </st><st c="27316">5</st></em><st c="27317">, </st><span class="No-Break"><st c="27319">we </st><a id="_idTextAnchor432"/><st c="27322">get,</st></span></p>
			<p><img src="image/2830.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.582em;height:1.765em;width:39.608em"/><st c="27326"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27439">Eq. </st><st c="27443">10</st></p>
			<p><st c="27445">We can rearrange the terms on the right-hand side of </st><em class="italic"><st c="27499">Eq. </st><st c="27503">10</st></em> <span class="No-Break"><st c="27505">to g</st><a id="_idTextAnchor433"/><st c="27510">ive,</st></span></p>
			<p><img src="image/2831.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;ε&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.582em;height:1.765em;width:31.630em"/><st c="27515"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27588">Eq. </st><st c="27592">11</st></p>
			<p><st c="27594">Now, </st><img src="image/2832.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.532em;height:1.480em;width:9.196em"/><st c="27600"/><st c="27601"> is just the expected difference between the prediction of our model at the holdout point, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.478em"/><st c="27692"/><st c="27693">, and the true (noise-free) value at that point. </st><st c="27742">This is just the bias. </st><st c="27765">Similarly, the expression </st><img src="image/2834.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;Var&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.532em;height:1.480em;width:7.393em"/><st c="27791"/><st c="27810"> is just the variance of our model’s prediction as we train it on different training datasets. </st><st c="27904">This means we can use </st><em class="italic"><st c="27926">Eq. </st><st c="27930">11</st></em> <span class="No-Break"><st c="27932">to wr</st><a id="_idTextAnchor434"/><st c="27938">ite,</st></span></p>
			<p><img src="image/2835.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;MSE&lt;/mtext&gt;&lt;mtext&gt;on&lt;/mtext&gt;&lt;mtext&gt;holdout&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mtext&gt;Bias&lt;/mtext&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;mtext&gt;prediction&lt;/mtext&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Variance&lt;/mtext&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mtext&gt;prediction&lt;/mtext&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;Variance&lt;/mtext&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mtext&gt;noise&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.240em;height:0.951em;width:32.659em"/><st c="27943"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28025">Eq. </st><st c="28029">12</st></p>
			<p><em class="italic"><st c="28031">Eq. </st><st c="28036">12</st></em><st c="28038"> is the</st><a id="_idIndexMarker766"/><st c="28045"> same as </st><em class="italic"><st c="28054">Eq. </st><st c="28058">3</st></em><st c="28059">, with the addition of the last term, the variance </st><img src="image/1828.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.715em;width:0.866em"/><st c="28110"/><st c="28113"> of the additive noise. </st><st c="28136">This means that the presence of noise in the data sets a minimum value for the generalization error. </st><st c="28237">No amount of trading off bias against variance will get us below </st><img src="image/2090.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.715em;width:0.839em"/><st c="28302"/><st c="28307"> for the MSE on the holdout data. </st><st c="28340">This </st><span class="No-Break"><st c="28345">is intuitive.</st></span></p>
			<p><st c="28358">We have already unpacked the consequences of the bias-variance trade-off equation. </st><st c="28442">This equation neatly summarizes the classical view of model complexity. </st><st c="28514">The schematic plot shown in </st><span class="No-Break"><em class="italic"><st c="28542">Figure 8</st></em></span><em class="italic"><st c="28550">.7</st></em><st c="28552"> also neatly summarizes this classical view of model complexity. </st><st c="28617">You may have seen it before, as it is used in many introductions to machine learning. </st><st c="28703">However, for highly parameterized machine learning models, the plot in </st><span class="No-Break"><em class="italic"><st c="28774">Figure 8</st></em></span><em class="italic"><st c="28782">.7</st></em><st c="28784"> is not the full story. </st><st c="28808">An interesting twist has arisen in the last few years, emerging from the study of deep learning neural networks. </st><st c="28921">We’ll now explain what that </st><span class="No-Break"><st c="28949">twist is.</st></span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor435"/><st c="28958">Double descent – a modern twist on the generalization error diagram</st></h2>
			<p><st c="29026">In our previous qualitative </st><a id="_idIndexMarker767"/><st c="29055">discussions, where we fitted various polynomials (from the 0</st><span class="superscript"><st c="29115">th</st></span><st c="29118"> degree to the 12</st><span class="superscript"><st c="29135">th</st></span><st c="29138"> degree) to the dataset in </st><span class="No-Break"><em class="italic"><st c="29165">Figure 8</st></em></span><em class="italic"><st c="29173">.2</st></em><st c="29175">, it was probably obvious to you where the sweet spot lay in terms of number of model parameters. </st><st c="29273">The data in </st><span class="No-Break"><em class="italic"><st c="29285">Figure 8</st></em></span><em class="italic"><st c="29293">.2</st></em><st c="29295"> was generated using a quadratic equation, so a 2nd-degree polynomial was clearly optimal to use to fit to the training data. </st><st c="29421">In these circumstances, it was self-evident where the generalization error minimum should be because the models we were fitting to the training data were in the same class – polynomials – as the process that generated the data. </st><st c="29649">This will not always be </st><span class="No-Break"><st c="29673">the case.</st></span></p>
			<p><st c="29682">Imagine that we have a</st><a id="_idIndexMarker768"/><st c="29705"> real-world dataset that we model using a neural network. </st><st c="29763">It is highly unlikely that the data was generated using a neural network. </st><st c="29837">However, our qualitative arguments about overfitting that led us to </st><span class="No-Break"><em class="italic"><st c="29905">Figure 8</st></em></span><em class="italic"><st c="29913">.7</st></em><st c="29915"> still hold. </st><st c="29928">We still expect to see the training error decrease monotonically as we increase the number of parameters in the neural network, and for the generalization error to display a minimum. </st><st c="30111">For modern machine learning models, what we tend to see is illustrated schemati</st><a id="_idTextAnchor436"/><st c="30190">cally in </st><span class="No-Break"><em class="italic"><st c="30200">Figure 8</st></em></span><span class="No-Break"><em class="italic"><st c="30208">.9</st></em></span><span class="No-Break"><st c="30210">.</st></span></p>
			<div>
				<div id="_idContainer2932" class="IMG---Figure">
					<img src="image/B19496_08_9.jpg" alt="" role="presentation"/><st c="30211"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="30292">Figure 8.9: A generalization error curve for a modern machine learning model</st></p>
			<p><st c="30368">Schematically, the generalization curve for our neural network model looks like the generalization curve in </st><span class="No-Break"><em class="italic"><st c="30477">Figure 8</st></em></span><em class="italic"><st c="30485">.7</st></em><st c="30487">. The difference is that now the </st><em class="italic"><st c="30520">x</st></em><st c="30521">-axis explicitly represents the number of model parameters, </st><img src="image/2838.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.474em"/><st c="30581"/><st c="30582">. The generalization error curve increases steeply as we increase </st><img src="image/2838.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.474em"/><st c="30648"/><st c="30649">, with the steep increase occurring as </st><img src="image/2840.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.485em"/><st c="30688"/><st c="30689"> approaches the size of the training dataset, </st><img src="image/115.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.704em"/><st c="30735"/><st c="30736">. At first sight, it looks like nothing more interesting is happening and there is no need to explore any further. </st><st c="30851">However, if we continue to increase the number of parameters in our neural network, what we tend to see is illustrated schematical</st><a id="_idTextAnchor437"/><st c="30981">ly in </st><span class="No-Break"><em class="italic"><st c="30988">Figure 8</st></em></span><span class="No-Break"><em class="italic"><st c="30996">.10</st></em></span><span class="No-Break"><st c="30999">.</st></span></p>
			<div>
				<div id="_idContainer2937" class="IMG---Figure">
					<img src="image/B19496_08_10.jpg" alt="" role="presentation"/><st c="31000"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="31080">Figure 8.10: A “double descent” generalization error curve for a modern machine learning model</st></p>
			<p><span class="No-Break"><em class="italic"><st c="31174">Figure 8</st></em></span><em class="italic"><st c="31183">.10</st></em><st c="31186"> shows that as </st><a id="_idIndexMarker769"/><st c="31201">we increase the number of parameters in our neural network beyond </st><img src="image/443.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.699em"/><st c="31267"/><st c="31268">, the generalization error decreases again. </st><st c="31312">This phenomenon is termed </st><em class="italic"><st c="31338">double descent</st></em><st c="31352"> because the generalization error decreases for a second time as we increase the model complexity. </st><st c="31451">For </st><img src="image/2843.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.900em;width:2.756em"/><st c="31455"/><st c="31460">, both the training error and generalization error continue </st><span class="No-Break"><st c="31520">to decrease.</st></span></p>
			<h3><st c="31532">The consequences of double descent</st></h3>
			<p><span class="No-Break"><em class="italic"><st c="31567">Figure 8</st></em></span><em class="italic"><st c="31576">.10</st></em><st c="31579"> tells </st><a id="_idIndexMarker770"/><st c="31586">us that it is possible to have an </st><em class="italic"><st c="31620">over-parameterized</st></em><st c="31638"> model and still have good generalization performance. </st><st c="31693">This has big implications for large neural network models and, in particular, </st><strong class="bold"><st c="31771">Large Language Models</st></strong><st c="31792"> (</st><strong class="bold"><st c="31794">LLMs</st></strong><st c="31798">), which can have billions of parameters. </st><span class="No-Break"><em class="italic"><st c="31841">Figure 8</st></em></span><em class="italic"><st c="31849">.10</st></em><st c="31852"> shows that this</st><a id="_idIndexMarker771"/><st c="31868"> number of parameters in a model is not an obstacle to the model genuinely learning, so double descent gives support to the use and development of deep learning neural networks, LLMs, and other massively </st><span class="No-Break"><st c="32072">parameterized models.</st></span></p>
			<p><st c="32093">However, that’s not to say we fully understand all the details behind double-descent. </st><st c="32180">The phenomenon of double descent is still very much an active area of research. </st><st c="32260">Indeed, at the time of writing (2023), one of the most talked-about papers at the top machine learning conference, NeurIPS, was about double-descent and how you should effectively count parameters in a model – see the </st><em class="italic"><st c="32478">Notes and further reading</st></em><st c="32503"> section at the end of the chapter for details on this paper. </st><st c="32565">Furthermore, the debate continues in academic circles and on social media about whether LLMs have truly learned from their training data and are displaying understanding, or whether they have simply memorized the training data and stored it in the massive number of </st><span class="No-Break"><st c="32831">model parameters.</st></span></p>
			<p><st c="32848">Since double</st><a id="_idIndexMarker772"/><st c="32861"> descent brings us up to the present day in terms of what is known about generalization error and model predictive accuracy, this seems like a good place to finish this section and recap what we </st><span class="No-Break"><st c="33056">have learned.</st></span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor438"/><st c="33069">What we learned</st></h2>
			<p><st c="33085">In this section, we learned </st><span class="No-Break"><st c="33114">the following:</st></span></p>
			<ul>
				<li><st c="33128">The generalization error is made up of two contributions, one from the bias in the model predictions and one from the variance of the </st><span class="No-Break"><st c="33263">model predictions</st></span></li>
				<li><st c="33280">The bias-variance trade-off and </st><span class="No-Break"><st c="33313">its implications</st></span></li>
				<li><st c="33329">The mathematical detail behind the bias-variance </st><span class="No-Break"><st c="33379">trade-off equation</st></span></li>
				<li><st c="33397">The phenomenon of double descent in over-parameterized machine learning models and </st><span class="No-Break"><st c="33481">its implications</st></span></li>
			</ul>
			<p><st c="33497">Having learned more mathematical details about generalization error, in the next section, we will learn about various model complexity metrics and how to use them to select models of the appropriate complexity for a given </st><span class="No-Break"><st c="33720">training set.</st></span></p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor439"/><st c="33733">Model complexity measures for model selection</st></h1>
			<p><st c="33779">Practical model </st><a id="_idIndexMarker773"/><st c="33796">complexity measures tend not to measure model complexity directly. </st><st c="33863">Instead, they measure some sort of trade-off – for example, how much information has been lost by approximating the patterns present in a dataset by using a particular model form, or what evidence a dataset provides for a model form of this level of complexity. </st><st c="34125">These practical metrics don’t directly measure model complexity, but they take it </st><span class="No-Break"><st c="34207">into account.</st></span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor440"/><st c="34220">Selecting between classes of models</st></h2>
			<p><st c="34256">In the preceding paragraph, we referred to model form. </st><st c="34312">But what do we mean by model form? </st><st c="34347">We mean the mathematical form of the equation that defines a model. </st><st c="34415">So, two models that differ only in their parameter values but otherwise have the same form of mathematical equation have the same model form (e.g., two linear models that use the </st><span class="No-Break"><st c="34594">same features).</st></span></p>
			<p><st c="34609">A model form</st><a id="_idIndexMarker774"/><st c="34622"> represents a whole class of models. </st><st c="34659">Let’s go back to our polynomial model example to illustrate. </st><st c="34720">For our polynomial models in </st><span class="No-Break"><em class="italic"><st c="34749">Figure 8</st></em></span><em class="italic"><st c="34757">.1</st></em><st c="34759"> and </st><span class="No-Break"><em class="italic"><st c="34764">Figure 8</st></em></span><em class="italic"><st c="34772">.2</st></em><st c="34774">, we had 1</st><span class="superscript"><st c="34784">st</st></span><st c="34787">-degree polynomials, 2</st><span class="superscript"><st c="34810">nd</st></span><st c="34813">-degree polynomials, 4</st><span class="superscript"><st c="34836">th</st></span><st c="34839">-degree polynomials, and 12</st><span class="superscript"><st c="34867">th</st></span><st c="34870">-degree polynomials. </st><st c="34892">The model class in this case is just the degree of the polynomial. </st><st c="34959">The 12</st><span class="superscript"><st c="34965">th</st></span><st c="34968">-degree model class is the set of all possible 12</st><span class="superscript"><st c="35018">th</st></span><st c="35021">-degree polynomials. </st><st c="35043">In this case, the model class is related simply to the number of parameters </st><img src="image/2008.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.485em"/><st c="35119"/><st c="35120"> in the </st><span class="No-Break"><st c="35128">model because,</st></span></p>
			<p class="IMG---Figure"><img src="image/2845.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo mathvariant=&quot;italic&quot;&gt;=&lt;/mo&gt;&lt;mtext mathvariant=&quot;italic&quot;&gt;degree&lt;/mtext&gt;&lt;mtext mathvariant=&quot;italic&quot;&gt;of&lt;/mtext&gt;&lt;mtext mathvariant=&quot;italic&quot;&gt;polynomial&lt;/mtext&gt;&lt;mtext mathvariant=&quot;italic&quot;&gt;+&lt;/mtext&gt;&lt;mtext&gt;1&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.968em;width:11.831em"/><st c="35142"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="35172">Eq. </st><st c="35176">13</st></p>
			<p><st c="35178">We can denote each model class by </st><img src="image/2846.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.480em;height:1.128em;width:1.113em"/><st c="35213"/><st c="35214">, and in this instance, the different model classes </st><img src="image/2846.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.480em;height:1.128em;width:1.113em"/><st c="35266"/><st c="35267"> represent different levels of </st><span class="No-Break"><st c="35298">model complexity.</st></span></p>
			<p><st c="35315">The challenge we face is working out which degree of polynomial model to use to model our data. </st><st c="35412">In other words, we</st><a id="_idIndexMarker775"/><st c="35430"> need to work out which class of models to use. </st><st c="35478">This process is called </st><span class="No-Break"><strong class="bold"><st c="35501">model selection</st></strong></span><span class="No-Break"><st c="35516">.</st></span></p>
			<p><st c="35517">Once we have selected which class of models to use, identifying which model to use within that class is just a case of parameter estimation (e.g., via least-squares fitting) or maximum likelihood estimation. </st><st c="35726">In fact, when comparing two model classes, we often just compare the maximum likelihood models from each class, since this uses the best model in each class as a representative of </st><span class="No-Break"><st c="35906">that class.</st></span></p>
			<p><st c="35917">We have already met a measure that helps us select between different model forms or classes, the generalization error. </st><st c="36037">We have also seen how it is affected by model complexity. </st><st c="36095">In this section, we will introduce two other commonly used model complexity measures</st><a id="_idIndexMarker776"/><st c="36179"> that are used for model selection, the </st><strong class="bold"><st c="36219">Akaike Information Criterion</st></strong><st c="36247"> (</st><strong class="bold"><st c="36249">AIC</st></strong><st c="36252">) and </st><a id="_idIndexMarker777"/><st c="36259">the </st><strong class="bold"><st c="36263">Bayesian Information </st></strong><span class="No-Break"><strong class="bold"><st c="36284">Criterion</st></strong></span><span class="No-Break"><st c="36293"> (</st></span><span class="No-Break"><strong class="bold"><st c="36295">BIC</st></strong></span><span class="No-Break"><st c="36298">).</st></span></p>
			<p><st c="36301">To start, we’ll define our model form, </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.438em"/><st c="36341"/><st c="36342">, for which we want to compute the AIC and the BIC. </st><st c="36394">The model form </st><img src="image/2849.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.403em"/><st c="36409"/><st c="36410"> is a function, </st><img src="image/2850.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.018em;width:2.158em"/><st c="36426"/><st c="36427">, that takes a feature vector, </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.466em"/><st c="36458"/><st c="36459">, as input and uses its </st><img src="image/2852.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.467em"/><st c="36483"/><st c="36484"> model parameters, represented by the vector </st><img src="image/1778.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.823em;width:0.477em"/><st c="36529"/><st c="36530">, to compute the model output. </st><st c="36561">From now on, we’ll use </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.420em"/><st c="36584"/><st c="36585"> when we mean either a model or a model form. </st><st c="36631">We also assume we know how to calculate the likelihood, </st><img src="image/1598.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.553em"/><st c="36687"/><st c="36688">, of a dataset, </st><img src="image/2856.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.117em;height:0.765em;width:0.933em"/><st c="36704"/><st c="36705"> given the model – that is, we know how to calculate </st><img src="image/2857.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mfenced&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:1.022em;width:3.973em"/><st c="36758"/><st c="36770">. With that in place, we can begin to compute the AIC </st><span class="No-Break"><st c="36824">and BIC.</st></span></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor441"/><st c="36832">Akaike Information Criterion</st></h2>
			<p><st c="36861">The </st><a id="_idIndexMarker778"/><st c="36866">AIC</st><a id="_idTextAnchor442"/><st c="36869"> is </st><span class="No-Break"><st c="36873">defined as,</st></span></p>
			<p><img src="image/2858.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;AIC&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mtext&gt;-&lt;/mtext&gt;&lt;mtext&gt;2&lt;/mtext&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:1.051em;width:7.702em"/><st c="36884"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="36902">Eq. </st><st c="36906">14</st></p>
			<p><st c="36908">In </st><em class="italic"><st c="36912">Eq. </st><st c="36916">14</st></em><st c="36918">, </st><img src="image/2859.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.988em;width:1.445em"/><st c="36920"/><st c="36924"> is the maximum likelihood of the model </st><img src="image/2849.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.406em"/><st c="36963"/><st c="36964">. The AIC attempts to measure the information lost by the model </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.424em"/><st c="37028"/><st c="37029"> when we use it as an approximation of the true process that generated the data. </st><st c="37110">We haven’t yet defined what “information” is, and we don’t do so until </st><a href="B19496_13.xhtml#_idTextAnchor646"><span class="No-Break"><em class="italic"><st c="37181">Chapter 13</st></em></span></a><st c="37191">. Basically, we can think of the information loss as a measure of how different the general trends and patterns that model </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.419em"/><st c="37314"/><st c="37315"> produces are, compared to the general trends and patterns from the true </st><span class="No-Break"><st c="37388">data-generating process.</st></span></p>
			<p><st c="37412">Obviously, a good candidate model will be close to the true generating process and will have a small information loss, leading to a small AIC value. </st><st c="37562">We can select an optimal model class from a set of candidate model classes by choosing the one that has the </st><span class="No-Break"><st c="37670">smallest AIC.</st></span></p>
			<p><st c="37683">Since the information loss is not a comparison of the model </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.419em"/><st c="37744"/><st c="37745"> to the training data, but a comparison of the model </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.419em"/><st c="37798"/><st c="37799"> to the true data-generating process, minimizing the AIC metric automatically guards against overfitting. </st><st c="37905">But wait, I hear you say – doesn’t the AIC include the log-likelihood that depends on the training data? </st><st c="38010">Yes, that is true. </st><st c="38029">However, to compute the information loss, we’d have to know the true generating process. </st><st c="38118">Instead, we said that the AIC “attempts” to measure the information loss. </st><st c="38192">The AIC approximates the information loss, but it still retains many of the desirable properties of the information loss, so we can still use it for model selection. </st><st c="38358">Let’s look at the AIC formula in </st><em class="italic"><st c="38391">Eq. </st><st c="38395">14</st></em><st c="38397"> to see how this model selection </st><span class="No-Break"><st c="38430">is done.</st></span></p>
			<p><st c="38438">The AIC consists of two contributions, </st><img src="image/2865.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.886em;width:0.929em"/><st c="38478"/><st c="38479"> and </st><img src="image/2866.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:1.051em;width:3.654em"/><st c="38484"/><st c="38491">. As we increase model complexity by increasing </st><img src="image/2008.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.485em"/><st c="38539"/><st c="38540">, we obviously increase the first of these contributions. </st><st c="38598">However, a higher complexity model will be able to fit the training data more closely, so will have a smaller value of </st><img src="image/2868.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:1.051em;width:3.349em"/><st c="38717"/><st c="38718">. We can see that the two contributions to the AIC work in competition against each other. </st><st c="38809">Minimizing the AIC will find the optimal trade-off between the two contributions – that is, the optimal balance between </st><img src="image/1890.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.503em"/><st c="38929"/><st c="38930"> (the model complexity) and </st><img src="image/2870.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:1.051em;width:2.727em"/><st c="38958"/><st c="38959"> (the measure of the </st><span class="No-Break"><st c="38980">model fit).</st></span></p>
			<h3><st c="38991">Weaknesses of the AIC</st></h3>
			<p><st c="39013">The previous sentence makes it</st><a id="_idIndexMarker779"/><st c="39044"> sound like the AIC is ideal for what we want to do – find the sweet spot of model complexity. </st><st c="39139">Not quite. </st><st c="39150">There are a few issues we </st><span class="No-Break"><st c="39176">should highlight:</st></span></p>
			<ul>
				<li><st c="39193">Firstly, the AIC formula in </st><em class="italic"><st c="39222">Eq. </st><st c="39226">14</st></em><st c="39228"> is an asymptotic (i.e., a large sample) result. </st><st c="39277">We said that the AIC approximates the information loss. </st><st c="39333">That approximation is increasingly accurate as, </st><img src="image/115.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.700em"/><st c="39381"/><st c="39382">, the size of the training dataset increases. </st><st c="39428">It is most accurate when </st><img src="image/629.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.685em"/><st c="39453"/><st c="39454"> is large. </st><st c="39465">The consequence of this is that for small training datasets, the AIC won’t accurately approximate the information loss, and a model selection based on </st><em class="italic"><st c="39616">Eq. </st><st c="39620">14</st></em><st c="39622"> may perform poorly. </st><st c="39643">There is a modified or corrected form of the AIC, usually denoted as AICc, that attempts to correct this deficiency. </st><st c="39760">We won’t explain the AICc here, other than to say that you can use it for model selection in the same way that you use the AIC – you select the model with the smallest value </st><span class="No-Break"><st c="39934">of AICc.</st></span></li>
				<li><st c="39942">Secondly, the use of the AIC follows a different philosophy to how we use the generalization error for model selection. </st><st c="40063">You’re probably already familiar with using the MSE on a validation set to perform hyper-parameter optimization for a machine learning model. </st><st c="40205">Also, you’ve no doubt run cross-validation calculations before. </st><st c="40269">In a machine learning context, we are directly trying to optimize, albeit in an empirical fashion, the thing we care about – the future predictive accuracy of our model. </st><st c="40439">In contrast, when we minimize the AIC, we are optimizing a proxy measure that we believe should be correlated with good </st><span class="No-Break"><st c="40559">predictive accuracy.</st></span></li>
			</ul>
			<p><st c="40579">The preceding second point is not unique to the AIC. </st><st c="40633">It is also a criticism that we can make of the BIC, which we will </st><span class="No-Break"><st c="40699">explain next.</st></span></p>
			<p><st c="40712">Why should we use the AIC, then, if it appears to be not as good as the generalization error as a means of selecting the optimal model? </st><st c="40849">The AIC is still a </st><a id="_idIndexMarker780"/><st c="40868">useful metric for </st><span class="No-Break"><st c="40886">many reasons:</st></span></p>
			<ul>
				<li><st c="40899">Calculating the MSE on a validation set requires us to have sufficient data to divide into training, validation, and test splits. </st><st c="41030">Similarly, calculating cross-validation measures can be </st><span class="No-Break"><st c="41086">computationally expensive.</st></span></li>
				<li><st c="41112">In contrast, the AIC is quick to calculate. </st><st c="41157">If we have fitted our model using maximum likelihood, then we have the AIC essentially with minimal </st><span class="No-Break"><st c="41257">extra computation.</st></span></li>
				<li><st c="41275">The theoretical underpinnings of the AIC are easier to understand. </st><st c="41343">Minimizing the generalization error or doing a cross-validation analysis have an intuitive empirical justification to them, but a detailed theoretical analysis of the performance of model selection/hyper-parameter optimization based on validation/cross-validation can be harder to do, so we may have a less-detailed understanding of the weaknesses of </st><span class="No-Break"><st c="41694">such approaches.</st></span></li>
			</ul>
			<p><st c="41710">Personally, I like to use the AIC in addition to other model selection metrics. </st><st c="41791">The ease of calculation of the AIC is its biggest selling point, but you must always be aware that, first, it is an approximation, and second, it is measures information loss, not </st><span class="No-Break"><st c="41971">predictive accuracy.</st></span></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor443"/><st c="41991">Bayesian Information Criterion</st></h2>
			<p><st c="42022">The</st><a id="_idIndexMarker781"/><st c="42026"> B</st><a id="_idTextAnchor444"/><st c="42028">IC is </st><span class="No-Break"><st c="42034">defined as:</st></span></p>
			<p><img src="image/2873.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;BIC&lt;/mtext&gt;&lt;mtext&gt;=&lt;/mtext&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.310em;height:1.021em;width:9.321em"/><st c="42045"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="42070">Eq. </st><st c="42074">15</st></p>
			<p><st c="42076">where again </st><img src="image/2874.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:0.988em;width:1.479em"/><st c="42089"/><st c="42093"> is the maximum likelihood for our model, </st><img src="image/1681.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.415em"/><st c="42134"/><st c="42135">. Like the AIC, the BIC is extremely easy to calculate, particularly if we have already fitted our model </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.419em"/><st c="42240"/><st c="42241"> via maximum </st><span class="No-Break"><st c="42254">likelihood estimation.</st></span></p>
			<p><st c="42276">The formula in </st><em class="italic"><st c="42292">Eq. </st><st c="42296">15</st></em><st c="42298"> hides the idea behind the BIC. </st><st c="42330">As you might guess, the BIC is based upon ideas from Bayesian analysis of models. </st><st c="42412">The BIC approximates the </st><em class="italic"><st c="42437">Bayesian evidence</st></em><st c="42454"> of the model class. </st><st c="42475">Given a dataset, </st><img src="image/2134.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.707em"/><st c="42492"/><st c="42493">, the Bayesian evidence for a model class, </st><img src="image/332.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.875em"/><st c="42536"/><st c="42537">, is </st><span class="No-Break"><st c="42542">defined as:</st></span></p>
			<p><img src="image/2879.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;Bayesian&lt;/mtext&gt;&lt;mtext&gt;evidence&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.364em;height:1.274em;width:24.760em"/><st c="42553"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="42607">Eq. </st><st c="42611">16</st></p>
			<p><st c="42613">In </st><em class="italic"><st c="42617">Eq. </st><st c="42621">16</st></em><st c="42623">, we have used </st><img src="image/2880.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.044em;width:1.015em"/><st c="42638"/><st c="42641"> to denote the model parameters for a model in the class </st><img src="image/678.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.864em"/><st c="42697"/><st c="42698">, and the symbol </st><img src="image/2882.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;munder&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.383em;height:1.274em;width:3.818em"/><st c="42715"/><st c="42724"> represents the prior distribution on </st><img src="image/2883.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.044em;width:1.022em"/><st c="42761"/><st c="42764"> given the model class </st><img src="image/678.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.867em"/><st c="42786"/><st c="42787">. For our polynomial model class </st><img src="image/2885.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;12&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.981em;width:1.381em"/><st c="42820"/><st c="42821"> this would be the Bayesian prior we put on the 13 coefficients of the </st><span class="No-Break"><st c="42892">12</st></span><span class="No-Break"><span class="superscript"><st c="42894">th</st></span></span><span class="No-Break"><st c="42897">-degree polynomial.</st></span></p>
			<p><st c="42917">In general, the larger the Bayesian evidence of a model class, </st><img src="image/556.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.850em"/><st c="42981"/><st c="42982">, the more evidence the data, </st><img src="image/2887.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.000em;height:0.648em;width:0.686em"/><st c="43012"/><st c="43013">, provides that the model class is the one that generated the data. </st><st c="43081">However, as with the AIC, we run into a technical issue here. </st><st c="43143">Calculating </st><em class="italic"><st c="43155">Eq. </st><st c="43159">16</st></em><st c="43161"> is not generally easy. </st><st c="43185">However, when the size of the training dataset, </st><img src="image/115.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.704em"/><st c="43233"/><st c="43234">, is large, we can come up with a general approximation. </st><st c="43291">That approximation is </st><a id="_idTextAnchor445"/><st c="43313">the BIC. </st><span class="No-Break"><st c="43322">In fact:</st></span></p>
			<p><img src="image/2889.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;BIC&lt;/mtext&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;Bayesian&lt;/mtext&gt;&lt;mtext&gt;Evidence&lt;/mtext&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mtext&gt;as&lt;/mtext&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.298em;height:1.069em;width:18.019em"/><st c="43330"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="43370">Eq. </st><st c="43374">17</st></p>
			<p><em class="italic"><st c="43376">Eq. </st><st c="43381">17</st></em><st c="43383"> indicates that maximizing the Bayesian evidence is the same as minimizing the BIC. </st><st c="43467">Consequently, we perform model selection by minimizing </st><span class="No-Break"><st c="43522">the BIC.</st></span></p>
			<p><st c="43530">As with the AIC, there are</st><a id="_idIndexMarker782"/><st c="43557"> several comments and observations we can make about </st><span class="No-Break"><st c="43610">the BIC:</st></span></p>
			<ul>
				<li><st c="43618">Like the AIC, the BIC is easy </st><span class="No-Break"><st c="43649">to calculate.</st></span></li>
				<li><st c="43662">Again, like the AIC, the BIC is based on a large sample size approximation, so at smaller sample sizes, it may not select the true optimal model class. </st><st c="43815">For smaller sample sizes, it would be better to compute the Bayesian evidence exactly via </st><em class="italic"><st c="43905">Eq. </st><st c="43909">16</st></em><st c="43911">, although this can require considerable mathematical skill or using advanced computationally intensive Monte </st><span class="No-Break"><st c="44021">Carlo techniques.</st></span></li>
				<li><st c="44038">The formula for the BIC looks very similar to that for the AIC. </st><st c="44103">The difference between the BIC and AIC is just in the penalty terms; </st><img src="image/2890.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.886em;width:0.967em"/><st c="44172"/><st c="44173"> for the AIC compared to </st><img src="image/2891.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:2.424em"/><st c="44198"/><st c="44204"> for the BIC. </st><st c="44217">So, as </st><img src="image/115.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:0.706em"/><st c="44224"/><st c="44225"> increases, the penalty we pay for increasing the model complexity becomes higher in the BIC than in the AIC, meaning the BIC tends to be a more stringent selection criterion than </st><span class="No-Break"><st c="44405">the AIC.</st></span></li>
			</ul>
			<p><st c="44413">Having discussed the AIC and BIC at length, it is time to wrap up this section. </st><st c="44494">Let’s first summarize what we have learned in this section about practical model complexity measures, and then we’ll summarize the </st><span class="No-Break"><st c="44625">chapter overall.</st></span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor446"/><st c="44641">What we learned</st></h2>
			<p><st c="44657">In this section, we learned </st><span class="No-Break"><st c="44686">the following:</st></span></p>
			<ul>
				<li><st c="44700">The concept of </st><span class="No-Break"><st c="44716">model selection</st></span></li>
				<li><st c="44731">The AIC and how we can use it to perform model selection by selecting the model class with the lowest </st><span class="No-Break"><st c="44834">AIC value</st></span></li>
				<li><st c="44843">The BIC and how we can use it to perform model selection by selecting the model class with the lowest </st><span class="No-Break"><st c="44946">BIC value</st></span></li>
				<li><st c="44955">The strengths and weaknesses of the AIC </st><span class="No-Break"><st c="44996">and BIC</st></span></li>
			</ul>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor447"/><st c="45003">Summary</st></h1>
			<p><st c="45011">This chapter has been a shorter one and largely a visual one. </st><st c="45074">The only heavy math came in the proof of the bias-variance decomposition of the generalization error. </st><st c="45176">However, the visual approach has been useful in explaining concepts of overfitting, underfitting, and generalization. </st><st c="45294">At a superficial level, these concepts are intuitive and need very little explanation. </st><st c="45381">You will have probably encountered them before. </st><st c="45429">However, a more thorough understanding of these concepts is crucial if we’re not to be misled by them when we’re building predictive models. </st><st c="45570">That thorough understanding has required us to learn additional concepts. </st><st c="45644">Across the whole chapter, the concepts we have learned about included </st><span class="No-Break"><st c="45714">the following:</st></span></p>
			<ul>
				<li><st c="45728">Model complexity and how we broadly think of this as being related to the number of parameters in </st><span class="No-Break"><st c="45827">a model</st></span></li>
				<li><st c="45834">Overfitting to the noise in a dataset and how it increases as we increase the complexity of </st><span class="No-Break"><st c="45927">a model</st></span></li>
				<li><st c="45934">Underfitting to the general trends in a dataset and how it increases as we decrease the complexity of </st><span class="No-Break"><st c="46037">a model</st></span></li>
				<li><st c="46044">Generalization and how a model that generalizes well is the one that makes accurate predictions on </st><span class="No-Break"><st c="46144">unseen data</st></span></li>
				<li><st c="46155">The bias-variance trade-off and how it makes mathematically precise the ideas behind how the minimum in the generalization </st><span class="No-Break"><st c="46279">error arises</st></span></li>
				<li><st c="46291">A classical picture of how model complexity affects the </st><span class="No-Break"><st c="46348">generalization error</st></span></li>
				<li><st c="46368">That over-parameterized machine learning models such as neural networks have revealed the phenomenon of </st><span class="No-Break"><st c="46473">double descent</st></span></li>
				<li><st c="46487">Model selection and how we use model complexity measures to select between different </st><span class="No-Break"><st c="46573">model classes</st></span></li>
				<li><st c="46586">The </st><strong class="bold"><st c="46591">Akaike Information Criterion</st></strong><st c="46619"> (</st><strong class="bold"><st c="46621">AIC</st></strong><st c="46624">) as a model </st><span class="No-Break"><st c="46638">selection measure</st></span></li>
				<li><st c="46655">The </st><strong class="bold"><st c="46660">Bayesian Information Criterion</st></strong><st c="46690"> (</st><strong class="bold"><st c="46692">BIC</st></strong><st c="46695">) as a model </st><span class="No-Break"><st c="46709">selection measure</st></span></li>
			</ul>
			<p><st c="46726">Our next chapter is another self-contained topic, function decomposition. </st><st c="46801">In that chapter, we will learn mathematical tools and tricks to build up </st><span class="No-Break"><st c="46874">a function.</st></span></p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor448"/><st c="46885">Notes and further reading</st></h1>
			<ol>
				<li><st c="46911">The NeurIPS 2023 conference paper on double descent we referred to in this chapter is </st><em class="italic"><st c="46998">A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</st></em><st c="47079">, by A. </st><st c="47087">Curth, A. </st><st c="47097">Jeffares, and M. </st><st c="47114">van der Schaar. </st><st c="47130">A preprint version of the paper can be found on the arXiv archive </st><span class="No-Break"><st c="47196">at </st></span><a href="https://arxiv.org/pdf/2310.18988.pdf"><span class="No-Break"><st c="47199">https://arxiv.org/pdf/2310.18988.pdf</st></span></a><span class="No-Break"><st c="47235">.</st></span></li>
			</ol>
		</div>
	<div id="charCountTotal" value="47236"/></body></html>