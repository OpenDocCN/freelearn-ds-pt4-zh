- en: <st c="0">8</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">8</st>
- en: <st c="2">Model Complexity</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2">模型复杂度</st>
- en: <st c="18">Model complexity may seem like a strange title for a chapter.</st>
    <st c="81">Why should we care about complexity?</st> <st c="118">One concept you
    may have already encountered as a data scientist is that of overfitting and how
    an overfitted model will not make accurate predictions.</st> <st c="270">However,
    that overfitting stems from using a model whose complexity is greater than that
    justified by the data.</st> <st c="382">The impact of model complexity on model
    prediction accuracy is a nuanced one.</st> <st c="460">More specifically, how
    you decide what is the right level of model complexity can be challenging.</st>
    <st c="558">To address this challenge requires exploring several new concepts.</st>
    <st c="625">We will do that exploration in this chapter and do so by covering
    the</st> <st c="695">following topics:</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18">模型复杂度可能看起来是一个奇怪的章节标题。</st> <st c="81">为什么我们要关心复杂度呢？</st> <st c="118">作为数据科学家，你可能已经接触过一个概念，那就是过拟合，过拟合的模型无法做出准确的预测。</st>
    <st c="270">然而，过拟合源于使用一个复杂度超过数据所能支持的模型。</st> <st c="382">模型复杂度对模型预测准确性的影响是一个微妙的问题。</st>
    <st c="460">更具体地说，如何决定模型的合适复杂度是具有挑战性的。</st> <st c="558">要解决这个问题，需要探讨一些新的概念。</st>
    <st c="625">我们将在本章中进行探索，并通过以下主题来进行讲解：</st> <st c="695">以下是我们将要讨论的内容：</st>
- en: '*<st c="712">Generalization, overfitting, and the role of model complexity</st>*<st
    c="774">: Here, we understand how model complexity affects the accuracy of model
    predictions on</st> <st c="863">unseen data</st>'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*<st c="712">泛化、过拟合和模型复杂度的作用</st>*<st c="774">：在这一部分，我们将理解模型复杂度如何影响模型在未见数据上的预测准确性。</st>'
- en: '*<st c="874">The bias-variance trade-off</st>*<st c="902">: Here, we dig into
    the mathematical details behind the prediction accuracy ideas we introduced in
    the</st> <st c="1006">preceding section</st>'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*<st c="874">偏差-方差权衡</st>*<st c="902">：在这一部分，我们将深入探讨我们在前一部分中提出的预测准确性背后的数学细节。</st> '
- en: '*<st c="1023">Model complexity measures for model selection</st>*<st c="1069">:
    Here, we introduce commonly used model complexity measures and discuss their strengths</st>
    <st c="1159">and weaknesses</st>'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*<st c="1023">模型选择中的模型复杂度度量</st>*<st c="1069">：在这一部分，我们将介绍常用的模型复杂度度量方法，并讨论它们的优缺点。</st>
    <st c="1159">以及它们的优势与劣势。</st>'
- en: <st c="1173">Technical requirements</st>
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1173">技术要求</st>
- en: <st c="1196">This chapter will mainly be a visual one.</st> <st c="1239">We
    will introduce the mathematical ideas and concepts through illustrative plots
    and schematic figures, which we will then unpack and explain at length.</st> <st
    c="1392">Because of this, there are no code examples in this chapter, so no</st>
    <st c="1459">technical requirements.</st>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1196">本章主要是以视觉化方式呈现的。</st> <st c="1239">我们将通过示意图和示意图来介绍数学思想和概念，然后进行详细解释。</st>
    <st c="1392">因此，本章没有代码示例，也没有</st> <st c="1459">技术要求。</st>
- en: <st c="1482">Generalization, overfitting, and the role of model complexity</st>
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1482">泛化、过拟合和模型复杂度的作用</st>
- en: <st c="1544">What do we mean by a complex model?</st> <st c="1581">Very loosely,
    we think of a more complex model as</st> <st c="1631">having more parameters or
    using more features.</st> <st c="1678">This statement is imprecise, but the idea
    that model complexity broadly follows the number of model parameters/features
    will be precise enough for the mainly qualitative discussions of</st> <st c="1863">this
    chapter.</st>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1544">我们所说的复杂模型是什么意思？</st> <st c="1581">很宽泛地说，我们把一个更复杂的模型视为</st> <st
    c="1631">具有更多的参数或使用更多的特征。</st> <st c="1678">这个说法不够精确，但模型复杂度大致与模型参数/特征数量相关的观点，对于本章主要的定性讨论来说已经足够精确。</st>
    <st c="1863">本章的讨论将围绕这一点展开。</st>
- en: <st c="1876">A more complex model can fit a training dataset more closely, as
    it can use the extra features to explain the variation in the response variable/target
    variable.</st> <st c="2039">What are the consequences of this increased flexibility?</st>
    <st c="2096">As a simple example, we’ll take a look at</st> *<st c="2138">Figure
    8</st>**<st c="2146">.1</st>*<st c="2148">, which shows three different models
    fitted to a small dataset.</st> <st c="2212">The black circles in each plot show
    the training data, while the blue circles show the hold-out sample data points,
    which, as you can see, represent an extrapolation challenge, since the hold-out
    data points are all to the right of the training data points.</st> <st c="2470">Although
    using machine learning models in interpolation settings is more common, we’ve
    used an extrapolation example here because it is easier to immediately see how
    well each fitted model predicts the holdout data, and so it is a good experiment
    to start to introduce some ideas about</st> <st c="2755">model complexity.</st>
    <st c="2774">All the data points (training and hold-out) were generated with a
    quadratic equation (a 2</st><st c="2863">nd</st><st c="2866">-degree polynomial)
    in a single variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="2909"><st c="2910">. A significant amount of additive noise was also added
    to the output of the</st> <st c="2987">generated data.</st></st>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1876">更复杂的模型可以更紧密地拟合训练数据集，因为它可以利用额外的特征来解释响应变量/目标变量的变化。</st> <st c="2039">这种增加的灵活性会带来什么后果？</st>
    <st c="2096">作为一个简单的例子，我们来看一下</st> *<st c="2138">图 8</st>**<st c="2146">.1</st>*<st
    c="2148">，它展示了三种不同模型拟合到一个小数据集。</st> <st c="2212">每个图中的黑色圆点表示训练数据，而蓝色圆点表示验证样本数据点，正如你所看到的，它们代表了一个外推的挑战，因为验证数据点都在训练数据点的右侧。</st>
    <st c="2470">虽然在插值设置中使用机器学习模型更为常见，但我们在这里使用了一个外推的例子，因为这样更容易立即看出每个拟合模型预测验证数据的效果，因此这是一个很好的实验，可以开始介绍一些关于</st>
    <st c="2755">模型复杂度的想法。</st> <st c="2774">所有数据点（包括训练和验证数据点）都是通过一个二次方程（一个二次多项式）生成的，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="2909"><st c="2910">. 生成的数据输出中还添加了大量的加性噪声。</st> <st c="2987">生成数据的结果。</st></st>
- en: '![](img/B19496_08_1.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_1.jpg)'
- en: '<st c="3111">Figure 8.1: Different polynomials fitted to a quadratic dataset</st>'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3111">图 8.1：不同多项式拟合到二次数据集</st>
- en: <st c="3174">The left-hand plot shows a</st> <st c="3201">linear equation in</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/2761.png)
    <st c="3221"><st c="3222">fitted to the training data.</st> <st c="3252">The line
    shows the predictions from the fitted model across the full range of the feature</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="3342"><st c="3343">. The linear model is obviously incorrect.</st> <st c="3386">It
    contains fewer parameters than the model that generated the data.</st> <st c="3455">It
    clearly doesn’t fit the training data well, since it doesn’t capture the curvature
    present in the training data, and there are marked differences between the line
    (predictions) and the dark circles of the training set.</st> <st c="3677">The
    linear model is also very poor at predicting the hold-out data points.</st> <st
    c="3752">The difference between the line and the lighter circles of the hold-out
    data points is stark – the trend in the predictions is in a different direction
    to that of the</st> <st c="3919">hold-out data.</st></st></st>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3174">左侧图示展示了一个</st> <st c="3201">线性方程</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/2761.png)
    <st c="3221"><st c="3222">拟合训练数据。</st> <st c="3252">这条线展示了从拟合模型对特征</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="3342"><st c="3343">的完整范围内的预测结果。</st> <st c="3386">线性模型显然是不正确的。</st> <st c="3455">它的参数比生成数据的模型要少。</st>
    <st c="3677">它显然没有很好地拟合训练数据，因为它没有捕捉到训练数据中的曲率，并且拟合线（预测值）与训练集的深色圆点之间存在明显差异。</st>
    <st c="3752">线性模型在预测验证数据点时也表现得很差。</st> <st c="3919">拟合线和验证数据点的浅色圆点之间的差异非常明显——预测趋势的方向与</st>
    <st c="3919">验证数据的趋势不同。</st></st></st>
- en: <st c="3933">The middle plot of</st> *<st c="3953">Figure 8</st>**<st c="3961">.1</st>*
    <st c="3963">shows the predictions (line) from a quadratic model fitted to the
    training data.</st> <st c="4045">The model fits the training data well and makes
    good predictions on the hold-out set.</st> <st c="4131">Any differences between
    the line and the black or light circles are evenly scattered and look like the
    result of the significant additive noise present in the data.</st> <st c="4296">It
    is perhaps not surprising that the quadratic model does well.</st> <st c="4361">It
    is of precisely the same form (a 2</st><st c="4398">nd</st><st c="4401">-degree
    polynomial) as the source of the training and</st> <st c="4456">hold-out data.</st>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3933">中间的图表</st> *<st c="3953">图 8</st>**<st c="3961">.1</st>* <st c="3963">展示了拟合到训练数据的二次模型的预测（线）。</st>
    <st c="4045">该模型很好地拟合了训练数据，并在保留集上做出了良好的预测。</st> <st c="4131">线与黑色或浅色圆点之间的任何差异都均匀分散，看起来像是数据中存在的显著加性噪声的结果。</st>
    <st c="4296">二次模型表现良好或许并不令人意外。</st> <st c="4361">它与训练和保留数据的来源正好是相同的形式（一个二次多项式）。</st>
    <st c="4398">训练和保留数据的形式正好是一个二次多项式。</st> <st c="4456">这也解释了为什么它能很好地拟合数据。</st>
- en: <st c="4470">Or does the quadratic model do better than the linear model simply
    because it has more parameters and so its shape can “wiggle” more and, therefore,
    capture more of the variations present in the training data?</st> <st c="4681">To
    test this idea, let’s increase the number of parameters in our model even further.</st>
    <st c="4767">The right-hand plot of</st> *<st c="4790">Figure 8</st>**<st c="4798">.1</st>*
    <st c="4800">shows the predictions (red line) from a quartic (a 4</st><st c="4853">th</st><st
    c="4856">-degree polynomial) model.</st> <st c="4884">This model has two more
    parameters than the quadratic model in the middle plot and three more parameters
    than the linear model in the left-hand plot.</st> <st c="5034">The quartic model
    fits the training data as well as the quadratic model but predicts poorly on the
    hold-out data.</st> <st c="5148">The trend of the predictions diverges from the
    trend in the hold-out data.</st> <st c="5223">The red line for the quartic model
    is even beginning to move downwards at the right-hand edge of the plot.</st> <st
    c="5330">If we made predictions for hold-out data at even higher values of the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="5400"><st c="5401">variable, the quartic</st> <st c="5424">model would
    move in completely the opposite direction to</st> <st c="5481">the data.</st></st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4470">或者，二次模型之所以比线性模型表现更好，仅仅是因为它有更多的参数，从而能够“摆动”得更多，因而能够捕捉到训练数据中更多的变化吗？</st>
    <st c="4681">为了验证这一想法，让我们进一步增加模型中的参数数量。</st> <st c="4767">右侧的图表</st> *<st c="4790">图
    8</st>**<st c="4798">.1</st>* <st c="4800">展示了四次模型（四次多项式）预测（红线）的情况。</st> <st c="4853">该模型比中间图中的二次模型多了两个参数，比左侧图中的线性模型多了三个参数。</st>
    <st c="4884">四次模型与二次模型一样很好地拟合了训练数据，但在保留数据上的预测效果较差。</st> <st c="5034">预测趋势与保留数据的趋势发生了偏离。</st>
    <st c="5148">四次模型的红线在图的右侧边缘甚至开始向下移动。</st> <st c="5223">如果我们在更高的</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="5400"><st c="5401">变量值下对保留数据进行预测，四次模型将完全朝着与数据相反的方向移动。</st> <st c="5424">模型将完全与数据的变化趋势背离。</st>
- en: <st c="5490">Let’s summarize our findings from that</st> <st c="5530">little
    experiment:</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5490">让我们总结一下从这个</st> <st c="5530">小实验中得到的发现：</st>
- en: '**<st c="5548">Linear model</st>**<st c="5561">: One</st> <st c="5567">parameter.</st>
    <st c="5579">Fits the training data poorly.</st> <st c="5610">Predicts poorly.</st>'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5548">线性模型</st>**<st c="5561">：一个</st> <st c="5567">参数。</st> <st c="5579">拟合训练数据较差。</st>
    <st c="5610">预测效果较差。</st>'
- en: '**<st c="5626">Quadratic model</st>**<st c="5642">: Two</st> <st c="5648">parameters.</st>
    <st c="5661">Fit the training data well.</st> <st c="5689">Predict well.</st>'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5626">二次模型</st>**<st c="5642">：两个</st> <st c="5648">参数。</st> <st c="5661">很好地拟合训练数据。</st>
    <st c="5689">预测效果良好。</st>'
- en: '**<st c="5702">Quartic model</st>**<st c="5716">: Four</st> <st c="5724">parameters.</st>
    <st c="5736">Fit the training data well.</st> <st c="5764">Predict poorly.</st>'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5702">四次模型</st>**<st c="5716">：四个</st> <st c="5724">参数。</st> <st c="5736">很好地拟合训练数据。</st>
    <st c="5764">预测效果较差。</st>'
- en: <st c="5779">From this simple example, we can see that there appears to be an
    optimal level of model complexity – a model that is neither too complex nor</st>
    <st c="5921">too simple.</st>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5779">通过这个简单的例子，我们可以看到，似乎存在一个最佳的模型复杂度水平——一个既不太复杂也不太简单的模型。</st>
- en: <st c="5932">Overfitting</st>
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="5932">过拟合</st>
- en: <st c="5944">Now, you may wonder whether the</st> <st c="5976">poor performance
    of the quartic model was due to the hold-out data essentially being a test of
    the extrapolation abilities of the fitted models.</st> <st c="6122">Again, we
    can test this idea with a simple experiment.</st> <st c="6177">In</st> *<st c="6180">Figure
    8</st>**<st c="6188">.2</st>*<st c="6190">, we have plotted the predictions (red
    line) from a 12</st><st c="6244">th</st><st c="6247">-degree polynomial, fitted
    to all the data that was present in our previous experiment in</st> *<st c="6338">Figure
    8</st>**<st c="6346">.1</st>*<st c="6348">. In this case, our model has lots of
    parameters (13 in total) and has been fitted to all the data (training and hold-out)
    that we had in</st> *<st c="6486">Figure 8</st>**<st c="6494">.</st><st c="6495">1</st>*<st
    c="6497">.</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5944">现在，你可能会想，四次模型的差劲表现是否是由于保留数据本质上是在测试拟合模型的外推能力。</st> <st c="6122">同样，我们可以通过一个简单的实验来验证这个想法。</st>
    <st c="6177">在</st> *<st c="6180">图 8</st>**<st c="6188">.2</st>*<st c="6190">中，我们绘制了一个12阶多项式的预测（红线），它是基于我们在</st>
    *<st c="6338">图 8</st>**<st c="6346">.1</st>*中之前实验中的所有数据拟合得到的。</st> <st c="6348">在这种情况下，我们的模型有很多参数（共13个），并且已经拟合了我们在</st>
    *<st c="6486">图 8</st>**<st c="6494">.1</st>*中所拥有的所有数据（训练数据和保留数据）。</st>
- en: '![](img/B19496_08_2.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_2.jpg)'
- en: '<st c="6541">Figure 8.2: A plot of predictions (the red line) from a 12th-degree
    polynomial, fitted to the same data in the previous figure</st>'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6541">图 8.2：基于前一图中相同数据拟合的12阶多项式预测（红线）的图示</st>
- en: <st c="6667">The dashed light-blue line is the true equation that was used to
    generate the data.</st> <st c="6752">The scatter of the individual data points
    (the black dots) around the dashed line is just the effect of the additive noise
    we included when generating the data.</st> <st c="6913">The dashed line represents
    the ground truth and is the best we can hope for from any model we fit to the
    data.</st> <st c="7024">A fitted model whose red line coincided with the dashed
    blue line would be considered “exact,” “optimal,”</st> <st c="7130">or “perfect.”</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6667">虚线浅蓝色线是用于生成数据的真实方程。</st> <st c="6752">每个数据点的散布（黑点）围绕虚线波动，只是我们在生成数据时加入的加性噪声的效果。</st>
    <st c="6913">虚线代表了地面真实值，是我们对任何拟合数据的模型所能期望的最佳结果。</st> <st c="7024">一个拟合模型的红线与虚线蓝线重合，将被认为是“精确的”，“最优的”或“完美的”。</st>
- en: <st c="7143">The red line for the 12</st><st c="7167">th</st><st c="7170">-degree
    polynomial model we have fitted in</st> *<st c="7214">Figure 8</st>**<st c="7222">.2</st>*
    <st c="7224">wiggles around the dashed blue line.</st> <st c="7262">This tells
    us that, first, the fitted model in</st> *<st c="7309">Figure 8</st>**<st c="7317">.2</st>*
    <st c="7319">is not perfect, and second, while the model has broadly fitted the
    quadratic trend of the true equation, it has also fitted to some of the wiggles
    in the data that are due to the noise in it.</st> <st c="7512">In this case, we
    say</st> <st c="7532">that the fitted model has</st> *<st c="7559">overfitted</st>*
    <st c="7569">the data.</st>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7143">我们在*<st c="7214">图 8</st>**<st c="7222">.2</st>*中拟合的12阶多项式模型的红线围绕虚线蓝线波动。</st>
    <st c="7262">这告诉我们，首先，拟合的模型在</st> *<st c="7309">图 8</st>**<st c="7317">.2</st>*中并不完美，其次，虽然模型大致拟合了真实方程的二次趋势，但它也拟合了数据中的一些波动，这些波动是由数据中的噪声引起的。</st>
    <st c="7512">在这种情况下，我们说</st> <st c="7532">拟合的模型已</st> *<st c="7559">过拟合</st>*
    <st c="7569">了数据。</st>
- en: <st c="7579">Why overfitting is bad</st>
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="7579">为什么过拟合不好</st>
- en: <st c="7602">Is overfitting a</st> <st c="7620">problem?</st> <st c="7629">Yes,
    it is.</st> <st c="7641">Take another look at</st> *<st c="7662">Figure 8</st>**<st
    c="7670">.2</st>*<st c="7672">. What would happen if our fitted model made a prediction
    for a value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="7745"><st c="7746">somewhere in the middle of the training data but where
    we didn’t already have an existing data point?</st> <st c="7849">What would happen
    if we then measured a new data point at</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7907"><st c="7908">? How close would our prediction be to the new measured
    data point?</st> <st c="7976">We wouldn’t expect them to be identical; after all,
    the new data point, like those in the training set, has noise added</st> <st c="8096">to
    it.</st></st></st>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7602">过拟合是一个</st> <st c="7620">问题吗？</st> <st c="7629">是的，确实是。</st> <st
    c="7641">再看看</st> *<st c="7662">图 8</st>**<st c="7670">.2</st>*<st c="7672">。如果我们拟合的模型对某个在训练数据中间的位置做出预测，而这个位置没有现有的数据点会发生什么呢？</st>
    <st c="7745"><st c="7746">如果我们接着在</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7849">处测量了一个新的数据点，会发生什么呢？我们的预测与新测得的数据点有多接近呢？</st> <st c="7907"><st c="7908">我们不期望它们完全相同；毕竟，新的数据点就像训练集中的数据点一样，也有噪声</st>
    <st c="7976">。</st> <st c="8096">我们会期望它们不会完全一致。</st></st></st>
- en: <st c="8102">The predicted value of the target variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)<st
    c="8147"><st c="8157">, is given by the position on the red line, corresponding
    to where the position</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8237"><st c="8238">is.</st> <st c="8243">Our</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8247"><st c="8248">value may correspond to where those wiggles in the red
    line are, and so our prediction would follow the pattern present in the noise
    in the training set.</st> <st c="8403">The problem is that for the new data point,
    the noise, which is random and therefore unpredictable, is highly unlikely to
    follow the same wiggles that were present in the noise in the training data.</st>
    <st c="8602">The prediction from our overfitted model is likely to be further
    away from the true value than if we had used a model that didn’t follow the wiggles
    of the noise in the training data.</st> <st c="8786">This means that because our
    model overfitted to the noise in the training set, we will have increased the
    size of the error the model makes on new</st> <st c="8933">unseen data.</st></st></st></st>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8102">目标变量的预测值，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)<st
    c="8147"><st c="8157">，由红线上的位置给出，对应于位置</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8237"><st c="8238">所在的地方。</st> <st c="8243">我们的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8247"><st c="8248">值可能对应于红线中的这些波动，因此我们的预测将遵循训练集中的噪声模式。</st> <st c="8403">问题在于，对于新的数据点，噪声是随机的，因此不可预测的，极不可能与训练数据中噪声的波动相同。</st>
    <st c="8602">我们过拟合模型的预测很可能会比我们使用未跟随训练数据中噪声波动的模型更远离真实值。</st> <st c="8786">这意味着，因为我们的模型过拟合了训练集中的噪声，我们将增加模型在新</st>
    <st c="8933">未见数据上的误差。</st></st></st></st>
- en: <st c="8945">Our predictions would have been better on average if our prediction
    model just followed the more general trend present in the training data, as represented
    by the dashed blue line.</st> <st c="9127">Predictions that follow the blue line
    would be correct on average.</st> <st c="9194">Since fitting to the general trends
    present in a training set produces a model that predicts well, we say that a good
    model is one that</st> *<st c="9330">generalizes</st>* <st c="9341">well.</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8945">如果我们的预测模型仅仅遵循训练数据中更普遍的趋势（如虚线蓝线所示），我们的预测在平均水平上会更好。</st> <st c="9127">遵循蓝线的预测在平均水平上是正确的。</st>
    <st c="9194">因为拟合到训练集中的一般趋势会产生一个预测准确的模型，所以我们说一个好的模型是一个能够</st> *<st c="9330">良好泛化</st>*
    <st c="9341">的模型。</st>
- en: <st c="9347">As we increase the</st> <st c="9367">number of parameters in our
    model, we increase its ability to follow the noise present in its training data.</st>
    <st c="9476">A fitted model with a very high number of parameters would pass almost
    exactly through the data.</st> <st c="9573">This is illustrated schematically
    in</st> *<st c="9610">Figure 8</st>**<st c="9618">.3</st>*<st c="9620">, which
    shows a highly flexible model (the line) overfitted to a small number of data
    points (the circles) that broadly follow a</st> <st c="9749">quadratic tr</st><st
    c="9761">end.</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9347">随着我们增加模型中的</st> <st c="9367">参数数量，我们增加了模型跟随训练数据中噪声的能力。</st> <st
    c="9476">一个具有非常高参数数量的拟合模型将几乎完全通过数据。</st> <st c="9573">这一点在</st> *<st c="9610">图
    8</st>**<st c="9618">.3</st>*<st c="9620">中得到了示意展示，图中显示了一个高度灵活的模型（线条）过拟合到少量数据点（圆点），这些数据点大体上遵循一个</st>
    <st c="9749">二次趋势</st><st c="9761">。</st>
- en: '![](img/B19496_08_3.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_3.jpg)'
- en: '<st c="9793">Figure 8.3: A schematic of a highly overfitted model</st>'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9793">图 8.3：高度过拟合模型的示意图</st>
- en: <st c="9845">Although</st> *<st c="9855">Figure 8</st>**<st c="9863">.3</st>*
    <st c="9865">is schematic and the line does not represent a model actually fitted
    to a dataset, it is still useful to draw out</st> <st c="9980">some conclusions:</st>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9845">虽然</st> *<st c="9855">图 8</st>**<st c="9863">.3</st>* <st c="9865">是示意图，线条并不代表一个实际拟合到数据集的模型，但它仍然有助于得出</st>
    <st c="9980">一些结论：</st>
- en: <st c="9997">The smaller the training dataset is, the easier it is for a complex
    model to overfit.</st> <st c="10084">This highlights that overfitting is dependent
    on both the model complexity and the</st> <st c="10167">training data.</st>
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="9997">训练数据集越小，复杂模型越容易发生过拟合。</st> <st c="10084">这强调了过拟合依赖于模型的复杂度以及</st>
    <st c="10167">训练数据的大小。</st>
- en: <st c="10181">If we were to use the fitted model in</st> *<st c="10220">Figure
    8</st>**<st c="10228">.3</st>* <st c="10230">to make a prediction for a data point
    it has already seen (i.e., one in the training dataset), the model would make
    a near-perfect prediction.</st> <st c="10374">This is because the high complexity
    of the model has allowed it to effectively “memorize” the training data.</st>
    <st c="10483">No “learning” has occurred.</st> <st c="10511">The</st> <st c="10515">model
    is just retrieving data, rather than identifying and learning</st> <st c="10583">general
    trends.</st>
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="10181">如果我们使用拟合的模型来</st> *<st c="10220">图 8</st>**<st c="10228">.3</st>*
    <st c="10230">对其已经见过的数据点（即训练数据集中的数据点）进行预测，模型将会做出几乎完美的预测。</st> <st c="10374">这是因为模型的高复杂性使它能够有效地“记住”训练数据。</st>
    <st c="10483">没有发生“学习”。</st> <st c="10511">模型只是检索数据，而不是识别和学习</st> <st c="10583">一般趋势。</st>
- en: <st c="10598">Overfitting increases the variability of predictions</st>
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="10598">过拟合增加了预测的变异性</st>
- en: <st c="10651">Let’s look at another</st> <st c="10674">consequence of overfitting.</st>
    <st c="10702">So far, we have discussed what happens to predictive accuracy when
    we overfit to a single particular dataset.</st> <st c="10812">We could get lucky.</st>
    <st c="10832">It could be that we have a highly complex model but, when fitted
    to our training dataset, we get a model that follows the general trends well and,
    therefore, generalizes well.</st> <st c="11008">What we’ll now illustrate is that
    with increasing model complexity, that is increasingly unlikely to happen.</st>
    <st c="11117">We are unlikely to</st> <st c="11136">be lucky.</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10651">让我们来看一下另一个</st> <st c="10674">过拟合的后果。</st> <st c="10702">到目前为止，我们已经讨论了当我们过拟合到单一数据集时，预测准确性会发生什么。</st>
    <st c="10812">我们可能会碰运气。</st> <st c="10832">可能我们的模型非常复杂，但当它拟合到训练数据集时，我们得到了一个能够很好地跟随一般趋势的模型，因此能够很好地泛化。</st>
    <st c="11008">我们现在将要说明的是，随着模型复杂度的增加，这种情况变得越来越不可能发生。</st> <st c="11117">我们不太可能</st>
    <st c="11136">碰到好运。</st>
- en: <st c="11145">Take a look at</st> *<st c="11161">Figure 8</st>**<st c="11169">.4</st>*<st
    c="11171">. It shows (dashed) prediction curves from 12</st><st c="11216">th</st><st
    c="11219">-degree polynomial models, obtained by fitting to different training
    datasets.</st> <st c="11299">The different training datasets were all generated
    by the same underlying “ground-truth” model that was used to generate the data
    in</st> *<st c="11432">Figure 8</st>**<st c="11440">.1</st>* <st c="11442">and</st>
    *<st c="11447">Figure</st> <st c="11453">8</st>**<st c="11455">.2</st>*<st c="11457">.</st>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11145">请看一下</st> *<st c="11161">图 8</st>**<st c="11169">.4</st>*<st c="11171">，它展示了来自12</st><st
    c="11216">阶</st><st c="11219">多项式模型的（虚线）预测曲线，这些曲线是通过拟合不同训练数据集得到的。</st> <st c="11299">不同的训练数据集都是通过相同的基础“真实模型”生成的，这个模型用于生成</st>
    *<st c="11432">图 8</st>**<st c="11440">.1</st>* <st c="11442">和</st> *<st c="11447">图</st>
    <st c="11453">8</st>**<st c="11455">.2</st>*<st c="11457">中的数据。</st>
- en: '![](img/B19496_08_4.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_4.jpg)'
- en: '<st c="11525">Figure 8.4: The prediction curves of the 12th-degree polynomial
    models obtained from different training sets</st>'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11525">图 8.4：从不同训练集获得的12阶多项式模型的预测曲线</st>
- en: <st c="11633">Since each 12</st><st c="11647">th</st><st c="11650">-degree</st>
    <st c="11659">polynomial model will overfit to the noise present in its corresponding
    training data, each model displays wiggles (as we can see in</st> *<st c="11792">Figure
    8</st>**<st c="11800">.4</st>*<st c="11802">).</st> <st c="11806">However, each
    training dataset is slightly different, so each 12</st><st c="11870">th</st><st
    c="11873">-degree polynomial displays different wiggles.</st> <st c="11921">We
    can see this in the variation of the shapes of the different dashed lines.</st>
    <st c="11999">But the dashed lines show what each model would predict for the
    given value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12078"><st c="12079">on the</st> *<st c="12087">x</st>*<st c="12088">-axis.</st>
    <st c="12095">For a given value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12116"><st c="12117">, there will be significant variation in the prediction
    at</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12176"><st c="12177">across the different models fitted to the different</st>
    <st c="12230">training datasets.</st></st></st></st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11633">由于每个12</st><st c="11647">阶</st><st c="11650">多项式模型将过度拟合其对应训练数据中的噪声，因此每个模型都会显示波动（如我们在</st>
    *<st c="11792">图 8</st>**<st c="11800">.4</st>*<st c="11802">中所见</st>）。<st c="11806">然而，每个训练数据集略有不同，因此每个12</st><st
    c="11870">阶</st><st c="11873">多项式都会显示不同的波动。</st> <st c="11921">我们可以通过不同虚线形状的变化看到这一点。</st>
    <st c="11999">但是，虚线展示了每个模型在给定的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12078"><st c="12079">上的</st> *<st c="12087">x</st>*<st c="12088">轴的预测值。</st>
    <st c="12095">对于给定的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12116"><st c="12117">，不同模型对</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12176"><st c="12177">的预测会有显著的变化</st> <st c="12230">，这取决于不同的训练数据集。</st></st></st></st>
- en: <st c="12248">As we increase the model complexity (e.g., go from fitting 12</st><st
    c="12310">th</st><st c="12313">-degree polynomials to fitting 20</st><st c="12347">th</st><st
    c="12350">-degree polynomials), those fitted models will follow ever more closely
    the wiggles in the noise in their training dataset.</st> <st c="12475">Consequently,
    the size of the wiggles in each fitted model will get bigger, and the resulting
    variability in predictions across training sets</st> <st c="12617">will increase.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12248">随着我们增加模型的复杂度（例如，从拟合12</st><st c="12310">阶</st><st c="12313">多项式到拟合20</st><st
    c="12347">阶</st><st c="12350">多项式），这些拟合的模型将更加紧密地跟随其训练数据集中的噪声波动。</st> <st c="12475">因此，每个拟合模型中的波动幅度将增大，跨训练集的预测结果的变异性</st>
    <st c="12617">将增加。</st>
- en: <st c="12631">This is a problem.</st> <st c="12651">Why so?</st> <st c="12659">What</st>
    *<st c="12664">Figure 8</st>**<st c="12672">.4</st>* <st c="12674">illustrates
    is that at the same value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12716"><st c="12717">, different training sets give different predictions.</st>
    <st c="12771">There is a random element to our prediction, and the variance of
    that random element increases with increasing model complexity.</st> <st c="12900">However,
    in real life, we will only ever use one training dataset.</st> <st c="12967">Are
    we going to hope that we are lucky and that our particular training data leads
    to predictions that are close to the true value?</st> <st c="13099">A better strategy
    would be to ensure that we have low sensitivity of the predictions to the choice
    of training set, by not using an overly</st> <st c="13238">complex model.</st></st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12631">这是一个问题。</st> <st c="12651">为什么会这样呢？</st> <st c="12659">什么</st>
    *<st c="12664">图 8.4</st>* <st c="12674">所示的是，在相同的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12716"><st c="12717">值下，不同的训练集会给出不同的预测结果。</st> <st c="12771">我们的预测中包含有随机因素，而这种随机因素的方差随着模型复杂度的增加而增大。</st>
    <st c="12900">然而，在现实生活中，我们只会使用一个训练数据集。</st> <st c="12967">我们是否希望自己能幸运一些，让特定的训练数据得出的预测结果接近真实值呢？</st>
    <st c="13099">一个更好的策略是，通过避免使用过于</st> <st c="13238">复杂的模型，确保我们的预测对训练集的选择具有较低的敏感性。</st></st>
- en: <st c="13252">Underfitting is also a problem</st>
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="13252">欠拟合也是一个问题</st>
- en: <st c="13283">Since overfitting to noise in a</st> <st c="13315">training dataset
    arises because we have too many parameters in our model relative to the amount
    of data/signal in the training data, you might get the impression that we should
    use a model with as few parameters as possible.</st> <st c="13541">This is not
    the case.</st> <st c="13563">It is possible for a model to be too simple, to contain
    too few parameters.</st> *<st c="13639">Figure 8</st>**<st c="13647">.5</st>*
    <st c="13649">shows the same dataset as in</st> *<st c="13679">Figure 8</st>**<st
    c="13687">.2</st>*<st c="13689">, but the fitted model represented by the red
    line in</st> *<st c="13743">Figure 8</st>**<st c="13751">.5</st>* <st c="13753">is
    now just a constant and corresponds to the mean value of the target variable,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="13835"><st c="13858">, in the training data.</st> <st c="13882">Clearly, this
    fitted model does not follow any wiggles in the data that are due to noise.</st>
    <st c="13972">This model is definitely not overfitted.</st> <st c="14013">But,
    equally, the model is not complex enough to follow the general quadratic trend
    in the data indicated by the dashed blue line.</st> <st c="14144">We say that
    this model</st> <st c="14167">is</st> *<st c="14170">under</st><st c="14175">fitted</st>*<st
    c="14182">.</st></st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13283">由于过拟合噪声出现在训练数据集中，是因为我们的模型相对于训练数据中的数据/信号量来说，参数太多，你可能会得到这样的印象：我们应该使用一个参数尽可能少的模型。</st>
    <st c="13541">但事实并非如此。</st> <st c="13563">一个模型可能过于简单，包含的参数过少。</st> *<st c="13639">图
    8.5</st>* <st c="13649">展示了与</st> *<st c="13679">图 8.2</st>* <st c="13689">相同的数据集，但在</st>
    *<st c="13743">图 8.5</st>* <st c="13753">中的红线所代表的拟合模型现在只是一个常数，并且对应于训练数据中目标变量的平均值，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="13835"><st c="13858">。</st> <st c="13882">显然，这个拟合模型没有遵循数据中因噪声而产生的任何波动。</st>
    <st c="13972">这个模型肯定没有过拟合。</st> <st c="14013">但同样地，这个模型也不够复杂，无法跟随数据中的一般二次趋势，该趋势由虚线蓝色线表示。</st>
    <st c="14144">我们说这个模型</st> <st c="14167">是</st> *<st c="14170">欠</st><st c="14175">拟合</st>*<st
    c="14182">的。</st></st>
- en: '![](img/B19496_08_5.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_5.jpg)'
- en: '<st c="14208">Figure 8.5: A low complexity model consisting of a constant equal
    to the mean of y</st>'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14208">图 8.5：一个低复杂度模型，其值为 y 的均值</st>
- en: <st c="14290">What are the consequences of an underfitted model?</st> <st c="14342">It
    should be qualitatively clear that a low complexity model that underfits a training
    dataset will make poor predictions on both training data points and on any unseen
    data points, as it has not learned the general trends that all data points follow.</st>
    <st c="14594">Prediction errors will be large for both training data and any holdout
    data.</st> <st c="14671">As with an overfitted model, an underfitted model</st>
    <st c="14721">generalizes poorly.</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14290">欠拟合模型的后果是什么？</st> <st c="14342">应该很清楚，欠拟合训练数据集的低复杂度模型，会在训练数据点和任何未见过的数据点上做出差的预测，因为它没有学到所有数据点遵循的普遍趋势。</st>
    <st c="14594">预测误差在训练数据和任何保留数据上都会很大。</st> <st c="14671">与过拟合模型一样，欠拟合模型</st> <st
    c="14721">泛化能力差。</st>
- en: <st c="14740">In contrast to overly</st> <st c="14762">complex overfitted models,
    low complexity models do not display much variability in their predictions, at
    a given holdout point</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="14891"><st c="14892">, across different training datasets.</st> *<st c="14930">Figure
    8</st>**<st c="14938">.6</st>* <st c="14940">shows the prediction curves (dashed
    lines) for a model that is only a constant (a 0</st><st c="15024">th</st><st c="15027">-degree
    polynomial) when fitted to the same training datasets used in </st>*<st c="15098">Figure
    8</st>**<st c="15106">.4</st>*<st c="15108">. For comparison, we have also added
    the true model line, which is a quadratic and shown by the light blue</st> <st
    c="15215">dashe</st><st c="15220">d line.</st></st>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14740">与过于复杂的过拟合模型相比，低复杂度模型在给定的保留点上，在不同的训练数据集之间的预测变化不大</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="14891"><st c="14892">，在不同的训练数据集之间。</st> *<st c="14930">图 8</st>**<st c="14938">.6</st>*
    <st c="14940">展示了一个模型的预测曲线（虚线），该模型仅为常数（0</st><st c="15024">阶</st><st c="15027">多项式），当拟合到与
    </st>*<st c="15098">图 8</st>**<st c="15106">.4</st>*<st c="15108"> 中使用的相同训练数据集时。为了比较，我们还添加了真实模型线，该线为二次曲线，表示为浅蓝色的</st>
    <st c="15215">虚线</st><st c="15220">。</st></st>
- en: '![](img/B19496_08_6.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_6.jpg)'
- en: '<st c="15274">Figure 8.6: The prediction curves of 0th-degree polynomial models
    obtained from different training sets</st>'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15274">图 8.6：来自不同训练集的0阶多项式模型的预测曲线</st>
- en: <st c="15377">There is some variation in the prediction curves of our 0</st><st
    c="15435">th</st><st c="15438">-degree polynomial, but not much.</st> <st c="15473">In
    fact, the variation is so small that two of the prediction curves lie almost on
    top of each other – there are, in fact, four horizontal dashed lines plotted in</st>
    *<st c="15636">Figure 8</st>**<st c="15644">.6</st>*<st c="15646">. The variation
    is considerably smaller than the variation in prediction curves we got from fitting
    12</st><st c="15748">th</st><st c="15751">-degree polynomials to the same</st>
    <st c="15784">training datasets.</st>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15377">我们的0阶多项式的预测曲线有一些变化，但不大。</st> <st c="15473">事实上，这种变化如此之小，以至于两条预测曲线几乎重叠—实际上，图中画出了四条水平虚线</st>
    *<st c="15636">图 8</st>**<st c="15644">.6</st>*<st c="15646">。这个变化明显小于我们从拟合12</st><st
    c="15748">阶</st><st c="15751">多项式得到的预测曲线的变化，这些曲线是通过拟合相同的</st> <st c="15784">训练数据集得到的。</st>
- en: <st c="15802">The benefit of this is that, in contrast to using an overly complex
    model, it doesn’t really matter which training dataset we use; our predictions
    will come out similar.</st> <st c="15973">There is little random variation in
    predictions between using different training sets of the same size.</st> <st c="16077">The
    downside is that those predictions will be consistently poor.</st> <st c="16143">No
    matter which training dataset we use, predictions from our low complexity model
    are typically a long way from the true value represented by the curved dashed
    light blue line.</st> <st c="16321">Our low-complexity model</st> <st c="16346">displays</st>
    *<st c="16355">bias</st>*<st c="16359">.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15802">这样做的好处是，与使用过于复杂的模型相比，我们使用的训练数据集实际上并不重要；我们的预测结果将会相似。</st> <st c="15973">在使用不同的训练集（大小相同）时，预测之间几乎没有随机波动。</st>
    <st c="16077">缺点是，这些预测将始终较差。</st> <st c="16143">无论我们使用哪个训练数据集，来自低复杂度模型的预测通常与由曲线虚线浅蓝色线表示的真实值相距甚远。</st>
    <st c="16321">我们的低复杂度模型</st> <st c="16346">表现出</st> *<st c="16355">偏差</st>*<st
    c="16359">。</st>
- en: <st c="16360">Clearly, there is a</st> <st c="16381">sweet spot in terms of
    the number of model parameters.</st> <st c="16436">At that sweet spot, the model
    complexity is sufficient to capture the general trends that led to the training
    data, but it is not sufficient to overfit to the noise in the data.</st> <st c="16614">At
    the sweet spot, the fitted model will generalize well.</st> <st c="16672">To identify
    that sweet spot, we need to make things more quantitative and explain how we define
    prediction errors and measure generalization.</st> <st c="16814">We will do</st>
    <st c="16825">that now.</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16360">显然，在模型参数数量上存在一个</st> <st c="16381">最佳平衡点。</st> <st c="16436">在这个最佳平衡点上，模型的复杂度足以捕捉到训练数据中所代表的普遍趋势，但不足以过度拟合数据中的噪声。</st>
    <st c="16614">在最佳平衡点上，拟合的模型将具有很好的泛化能力。</st> <st c="16672">为了确定这个最佳平衡点，我们需要将问题量化，并解释我们如何定义预测误差以及如何衡量泛化能力。</st>
    <st c="16814">我们现在就来</st> <st c="16825">做这件事。</st>
- en: <st c="16834">Measuring prediction error</st>
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="16834">衡量预测误差</st>
- en: <st c="16861">A data point</st> <st c="16874">is a pair</st> <st c="16884">of
    values</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2775.png)<st
    c="16895"><st c="16896">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/2776.png)
    <st c="16902"><st c="16903">value is the value of the target variable (or response
    variable) we observed when we had the corresponding vector,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="17019"><st c="17020">, for the feature variables.</st> <st c="17049">Remember
    that for the same</st>![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder></mrow></math>](img/2778.png) <st c="17075"><st
    c="17077">value, we could get multiple different values for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17128"><st c="17138">because the observation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17162"><st c="17172">contains a random component.</st> <st c="17201">A
    dataset is just a set of multiple datapoints.</st> <st c="17249">In general, we
    can denote a dataset</st> <st c="17285">as</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>D</mi><mo>=</mo><mfenced
    open="{" close="}"><mrow><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>,</mo><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi></mrow></mfenced></mrow></mrow></math>](img/2781.png)<st
    c="17288"><st c="17289">.</st></st></st></st></st></st></st></st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16861">一个数据点</st> <st c="16874">是由一对</st> <st c="16884">值组成的</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2775.png)<st
    c="16895"><st c="16896">。这个</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/2776.png)
    <st c="16902"><st c="16903">值是我们在观察到相应的向量</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="17019"><st c="17020">时得到的目标变量（或响应变量）的值。</st> <st c="17049">记住，对于相同的</st>![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mrow></math>](img/2778.png)
    <st c="17075"><st c="17077">值，我们可以得到多个不同的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17128"><st c="17138">值</st> <st c="17162"><st c="17172">因为观察值</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17162"><st c="17172">包含了随机成分。</st> <st c="17201">一个数据集只是多个数据点的集合。</st>
    <st c="17249">一般来说，我们可以将一个数据集表示为</st> <st c="17285">如</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>D</mi><mo>=</mo><mfenced
    open="{" close="}"><mrow><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>,</mo><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi></mrow></mfenced></mrow></mrow></math>](img/2781.png)<st
    c="17288"><st c="17289">。</st></st></st></st></st></st></st></st>
- en: <st c="17290">Let’s use</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2782.png)
    <st c="17301"><st c="17314">to denote the model equation of our trained model.</st>
    <st c="17365">The notation</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mrow></mrow></math>](img/2783.png)
    <st c="17378"><st c="17389">is used to denote the fact that the trained model
    will depend on some parameters,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="17471"><st c="17472">, and the training data,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2785.png)<st
    c="17497"><st c="17503">. The difference between the model prediction and the
    observed value of the target variable for the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/909.png)
    <st c="17603"><st c="17615">datapoint is</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2787.png)<st
    c="17628"><st c="17646">, which we will shorten to just</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2788.png)<st
    c="17678"><st c="17679">. To get a measure of the typical error made by our model,
    we calculate</st> <st c="17751">the</st> **<st c="17755">mean squared</st>** **<st
    c="17768">error</st>** <st c="17773">(</st>**<st c="17775">MSE</st>**<st c="17778">):</st></st></st></st></st></st></st></st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17290">让我们使用</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2782.png)
    <st c="17301"><st c="17314">来表示我们训练模型的模型方程。</st> <st c="17365">这个符号</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mrow></mrow></math>](img/2783.png)
    <st c="17378"><st c="17389">用于表示训练后的模型将依赖于一些参数，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="17471"><st c="17472">，以及训练数据，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2785.png)<st
    c="17497"><st c="17503">。模型预测与目标变量观察值之间的差异为</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2787.png)<st
    c="17628"><st c="17646">，我们将其简化为</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2788.png)<st
    c="17678"><st c="17679">。为了衡量我们模型的典型误差，我们计算</st> <st c="17751">**<st c="17755">均方</st>**
    **<st c="17768">误差</st>** <st c="17773">(</st>**<st c="17775">MSE</st>**<st c="17778">)：</st></st></st></st></st></st></st></st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>=</mtext><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mrow></math>](img/2789.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>=</mtext><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mrow></math>](img/2789.png)'
- en: <st c="17783">Eq.</st> <st c="17787">1</st>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17783">公式</st> <st c="17787">1</st>
- en: <st c="17788">This is like when we studied least-squares model fitting in</st>
    [*<st c="17848">Chapter 4</st>*](B19496_04.xhtml#_idTextAnchor216)<st c="17857">.
    We take the square of the individual errors to ensure a positive number so that
    positive and negative errors don’t just cancel out to zero.</st> <st c="17999">To
    get back to a “typical” error, we can finally take the square root of the MSE
    to get</st> <st c="18087">the</st> **<st c="18091">root mean squared</st>** **<st
    c="18109">error</st>** <st c="18114">(</st>**<st c="18116">RMSE</st>**<st c="18120">):</st>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17788">这就像我们在</st> [*<st c="17848">第4章</st>*](B19496_04.xhtml#_idTextAnchor216)<st
    c="17857">学习最小二乘法模型拟合时一样。我们取单个误差的平方，以确保结果为正数，这样正误差和负误差就不会相互抵消为零。</st> <st c="17999">为了得到“典型”的误差，我们最终可以对均方误差（MSE）开方，得到</st>
    <st c="18087">该</st> **<st c="18091">均方根</st>** **<st c="18109">误差</st>** <st
    c="18114">(</st>**<st c="18116">RMSE</st>**<st c="18120">):</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>RMSE</mtext><mtext>=</mtext><msqrt><mtext>MSE</mtext></msqrt></mrow></mrow></math>](img/2790.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>RMSE</mtext><mtext>=</mtext><msqrt><mtext>MSE</mtext></msqrt></mrow></mrow></math>](img/2790.png)'
- en: <st c="18125">Eq.</st> <st c="18129">2</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18125">公式</st> <st c="18129">2</st>
- en: <st c="18130">We can calculate the</st> <st c="18151">MSE for</st> <st c="18159">a
    given dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2791.png)<st
    c="18176"><st c="18177">. This doesn’t necessarily have to be the training dataset.</st>
    <st c="18237">We can evaluate the MSE for the hold-out dataset,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/2792.png)<st
    c="18287"><st c="18291">, if we want to.</st> <st c="18308">Since the accuracy
    of predictions on the hold-out dataset gives us a feel for how well a model generalizes,
    we call</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2793.png)
    <st c="18424"><st c="18440">the generalization error, while we call</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2794.png)
    <st c="18480"><st c="18493">the training error.</st> <st c="18513">However, you
    will probably find that MSE and RMSE are used interchangeably – for example,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2795.png)
    <st c="18603"><st c="18619">can be referred to as the generalization error even
    though it is obviously an average squared error.</st> <st c="18720">From a qualitative
    perspective, it is inconsequential, and I also tend to use the term “generalization
    error” when I’m referring</st> <st c="18849">to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2796.png)<st
    c="18852"><st c="18867">.</st></st></st></st></st></st></st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18130">我们可以计算</st> <st c="18151">给定数据集的MSE，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2791.png)<st
    c="18176"><st c="18177">。这不一定非得是训练数据集。</st> <st c="18237">我们也可以评估保留数据集的MSE，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/2792.png)<st
    c="18287"><st c="18291">，如果我们愿意的话。</st> <st c="18308">由于对保留数据集上的预测准确性能让我们感知模型的泛化能力，因此我们称</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2793.png)
    <st c="18424"><st c="18440">为泛化误差，而将</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2794.png)
    <st c="18480"><st c="18493">称为训练误差。</st> <st c="18513">然而，你可能会发现MSE和RMSE被交替使用——例如，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2795.png)
    <st c="18603"><st c="18619">也可以称为泛化误差，尽管它显然是一个平均平方误差。</st> <st c="18720">从定性角度来看，这并不重要，我也倾向于在提到</st>
    <st c="18849">时使用“泛化误差”这一术语，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2796.png)<st
    c="18852"><st c="18867">。</st></st></st></st></st></st></st>
- en: <st c="18868">We can now go back and take our previous qualitative conclusions
    about overfitting and use the MSE to make them more quantitative.</st> <st c="19000">We
    know that as we increase the model complexity by increasing the number of parameters/features
    in our model, we can fit the training data points evermore closely.</st> <st c="19165">This
    means that for a given training dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2797.png)<st
    c="19211"><st c="19217">, we expect</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2798.png)
    <st c="19229"><st c="19242">to decrease monotonically as we increase the model
    complexity.</st> <st c="19305">We also know the generalization ability of our
    model initially improves as we increase the number of parameters and then deteriorates.</st>
    <st c="19440">This means we expect</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2799.png)<st
    c="19461">![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2800.png)
    <st c="19462"><st c="19472">to go through a minimum.</st> <st c="19497">These
    two quantitative statements are summarized schematically i</st><st c="19561">n</st>
    *<st c="19564">Figure 8</st>**<st c="19572">.7</st>*<st c="19574">.</st></st></st></st></st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18868">现在我们可以回顾并结合之前关于过拟合的定性结论，使用均方误差（MSE）使其更加量化。</st> <st c="19000">我们知道，当我们通过增加模型中的参数/特征数量来增加模型复杂度时，我们可以越来越精确地拟合训练数据点。</st>
    <st c="19165">这意味着，对于给定的训练数据集，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2797.png)<st
    c="19211"><st c="19217">，我们预计</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2798.png)
    <st c="19229"><st c="19242">将随着模型复杂度的增加而单调减少。</st> <st c="19305">我们还知道，随着参数数量的增加，模型的泛化能力最初会提高，然后会恶化。</st>
    <st c="19440">这意味着我们预计</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2799.png)<st
    c="19461">![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2800.png)
    <st c="19462"><st c="19472">将经历一个最小值。</st> <st c="19497">这两条定量结论在*图 8.7*中进行了示意总结。</st>
- en: '![](img/B19496_08_7.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_7.jpg)'
- en: '<st c="19660">Figure 8.7: The behavior of training and generalization errors
    with increasing model complexity</st>'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19660">图 8.7：随着模型复杂度增加，训练误差和泛化误差的变化</st>
- en: <st c="19755">In</st> *<st c="19759">Figure 8</st>**<st c="19767">.7</st>*<st
    c="19769">, we have indicated the position where the generalization error is at
    its minimum.</st> <st c="19852">We have marked this as “optimal model complexity.”
    Why does the smallest generalization error identify the optimal model?</st> <st
    c="19974">Why do we want the generalization error to be minimal and not the training
    error?</st> <st c="20056">Well, it is for predictions we want to use the model.</st>
    <st c="20110">We don’t need to predict the training data – we already have it.</st>
    <st c="20175">The point where the generalization error is smallest represents
    the point where the performance of the model is optimal for its intended use.</st>
    <st c="20317">The minimum in the generalization error is the sweet spot we</st>
    <st c="20377">referred</st> <st c="20387">to earlier.</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19755">在</st> *<st c="19759">图 8</st>**<st c="19767">.7</st>*<st c="19769">中，我们标出了泛化误差最小的位置。</st>
    <st c="19852">我们将其标记为“最优模型复杂度”。为什么最小的泛化误差能标识最优模型呢？</st> <st c="19974">为什么我们希望泛化误差最小而不是训练误差最小呢？</st>
    <st c="20056">嗯，因为我们希望模型用于预测。</st> <st c="20110">我们不需要预测训练数据——我们已经拥有它了。</st> <st
    c="20175">泛化误差最小的点代表了模型在其预期用途上的最佳表现。</st> <st c="20317">泛化误差中的最小值就是我们</st> <st
    c="20377">之前提到的</st> <st c="20387">“甜点”位置。</st>
- en: '*<st c="20398">Figure 8</st>**<st c="20407">.7</st>* <st c="20409">succinctly
    represents the key ideas that we have introduced in this section, so this is a
    good point to summarize what we</st> <st c="20532">have learned.</st>'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="20398">图 8</st>**<st c="20407">.7</st>* <st c="20409">简洁地表示了我们在本节中介绍的关键思想，因此这是一个总结我们所学内容的好时机。</st>'
- en: <st c="20545">What we learned</st>
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="20545">我们学到的内容</st>
- en: <st c="20561">In this section, we learned</st> <st c="20590">the following:</st>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20561">在本节中，我们学到了以下内容：</st>
- en: <st c="20604">Model complexity broadly correlates with the number of features
    or parameters in</st> <st c="20686">a model.</st>
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20604">模型复杂度通常与模型中功能或参数的数量相关。</st>
- en: <st c="20694">A model that is too complex will overfit to the noise in training
    data.</st> <st c="20767">It will predict well on the training data points but
    predict poorly on</st> <st c="20838">unseen data.</st>
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20694">一个过于复杂的模型会对训练数据中的噪声发生过拟合。</st> <st c="20767">它在训练数据点上预测得很好，但在</st>
    <st c="20838">未见过的数据上预测得很差。</st>
- en: <st c="20850">A model that is not complex enough will underfit the general trends
    and patterns present in training data.</st> <st c="20958">It will predict poorly
    on both the training data points and</st> <st c="21018">unseen data.</st>
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20850">一个不够复杂的模型会忽视训练数据中存在的普遍趋势和模式。</st> <st c="20958">它在训练数据点和</st>
    <st c="21018">未见过的数据上预测都不好。</st>
- en: <st c="21030">A model that predicts well on unseen data is said to generalize.</st>
    <st c="21096">It does so by not fitting to the noise in the training data and,
    instead, by learning the general trends and</st> <st c="21205">patterns present.</st>
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="21030">一个在未见过的数据上表现良好的模型被称为具有泛化能力。</st> <st c="21096">它通过不拟合训练数据中的噪声，而是通过学习数据中普遍存在的趋势和</st>
    <st c="21205">模式来实现这一点。</st>
- en: <st c="21222">Overly complex models have a large random component to their predictions,
    arising from the choice of training data used.</st> <st c="21344">In contrast,
    low-complexity models will show little variation in their predictions across different
    training sets but will</st> <st c="21467">display bias.</st>
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="21222">过于复杂的模型在其预测中包含了大量随机成分，这些成分来自于所选训练数据的不同。</st> <st c="21344">相反，低复杂度的模型在不同的训练集上表现出很小的预测变化，但会</st>
    <st c="21467">表现出偏差。</st>
- en: <st c="21480">We can use MSE and RMSE to quantify</st> <st c="21517">prediction
    errors.</st>
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="21480">我们可以使用 MSE 和 RMSE 来量化</st> <st c="21517">预测误差。</st>
- en: <st c="21535">The MSE and RMSE on the training dataset monotonically decrease
    with increasing model complexity, while the MSE and RMSE on a holdout dataset
    will display a minimum – first decreasing and then increasing as we increase</st>
    <st c="21755">model complexity.</st>
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="21535">训练数据集上的 MSE 和 RMSE 会随着模型复杂度的增加而单调减小，而在持出数据集上的 MSE 和 RMSE 会呈现最小值——首先减小，然后随着模型复杂度的增加而增加。</st>
- en: <st c="21772">Having learned the basic ideas behind model complexity and how
    it affects the generalization abilities of a model, in the next section we will
    dig deeper into the mathematical detail behind the generalization error curve
    and introduce a modern twist to</st> <st c="22026">its behavior.</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21772">在学习了解模型复杂性及其对模型泛化能力影响的基本思想后，接下来我们将深入探讨泛化误差曲线背后的数学细节，并介绍其行为的现代化变化。</st>
    <st c="22026">的行为。</st>
- en: <st c="22039">The bias-variance trade-off</st>
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="22039">偏差-方差权衡</st>
- en: <st c="22067">The generalization</st> <st c="22086">error curve in</st> *<st
    c="22102">Figure 8</st>**<st c="22110">.7</st>* <st c="22112">shows a minimum.</st>
    <st c="22130">In the preceding section, we gave a qualitative explanation of why
    we expected the generalization error to first decrease and then increase with
    increasing model complexity and why, therefore, this leads to a minimum in the
    generalization error curve.</st> <st c="22382">But to get a quantitative idea
    of why the generalization error curve displays a minimum and what controls its
    position, we need to dig into the math behind</st> <st c="22538">the curve.</st>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22067">泛化</st> <st c="22086">误差曲线在</st> *<st c="22102">图 8</st>**<st
    c="22110">.7</st>* <st c="22112">显示了一个最小值。</st> <st c="22130">在前一节中，我们定性地解释了为什么我们预期泛化误差随着模型复杂性的增加而先减少后增加，因此这导致了泛化误差曲线的最小值。</st>
    <st c="22382">但要量化地理解泛化误差曲线为什么显示出最小值及其位置的控制因素，我们需要深入研究曲线背后的数学。</st> <st c="22538">曲线。</st>
- en: <st c="22548">The generalization error curve</st> <st c="22579">is made up of
    two competing contributions, one increasing with model complexity and the other
    decreasing.</st> <st c="22686">It is the competition between these two contributions
    that leads to the minimum.</st> <st c="22767">Those two contributions are, first,
    the</st> *<st c="22807">bias</st>* <st c="22811">in a model’s prediction at a
    holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="22856"><st c="22857">, and second, the</st> *<st c="22875">variance</st>* <st
    c="22883">in the model’s prediction at the holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="22932"><st c="22933">, with the variance arising from the sensitivity of
    the model’s prediction to the precise choice of training data.</st> <st c="23048">These
    two competing contributions are essentially what we highlighted in</st> *<st c="23121">Figure
    8</st>**<st c="23129">.4</st>* <st c="23131">and</st> *<st c="23136">Figure 8</st>**<st
    c="23144">.6</st>* <st c="23147">in the</st> <st c="23154">previous section.</st></st></st>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22548">泛化误差曲线</st> <st c="22579">由两个竞争贡献组成，一个随模型复杂性增加而增加，另一个则减少。</st>
    <st c="22686">正是这两个贡献的竞争导致了最小值。</st> <st c="22767">这两个贡献是模型预测在保留点的</st> *<st c="22807">偏差</st>*
    <st c="22811">，以及模型预测在保留点的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="22856"><st c="22857">，第二个是模型预测在保留点的</st> *<st c="22875">方差</st>* <st c="22883">，方差来源于模型预测对训练数据精确选择的敏感性。</st>
    <st c="22932">这两个竞争贡献本质上是我们在</st> *<st c="23121">图 8</st>**<st c="23129">.4</st>*
    <st c="23131">和</st> *<st c="23136">图 8</st>**<st c="23144">.6</st>* <st c="23147">中在前一节中突出显示的内容。</st></st></st>
- en: <st c="23171">Mathematically, we</st> <st c="23191">find tha</st><st c="23199">t,</st>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23171">从数学上讲，我们发现</st> <st c="23191">这一点，</st><st c="23199">。</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>on</mtext><mtext>holdout</mtext><mtext>data</mtext><mtext>=</mtext><msup><mtext>Bias</mtext><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext></mrow></mrow></math>](img/2803.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>验证集上的均方误差</mtext><mo>=</mo><msup><mtext>偏差</mtext><mn>2</mn></msup><mo>+</mo><mtext>方差</mtext></mrow></mrow></math>](img/2803.png)'
- en: <st c="23241">Eq.</st> <st c="23245">3</st>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23241">方程</st> <st c="23245">3</st>
- en: <st c="23246">We’ll go through the derivation of</st> *<st c="23281">Eq.</st>
    <st c="23285">3</st>* <st c="23286">in a moment, as it is instructive to do so
    and because the derivation of</st> *<st c="23360">Eq.</st> <st c="23364">3</st>*
    <st c="23365">is more subtle than we have hinted at.</st> <st c="23405">We know
    from our qualitative discussions of</st> *<st c="23449">Figure 8</st>**<st c="23457">.4</st>*
    <st c="23459">and</st> *<st c="23464">Figure 8</st>**<st c="23472">.6</st>* <st
    c="23474">that with increasing model complexity, first, the bias of predictions
    decreases, and second, the variance of predictions increases.</st> <st c="23607">Schematically,
    we illustrated this in</st> *<st c="23645">Figure 8</st>**<st c="23653">.8</st>*<st
    c="23655">, which shows the original generalization error curve from</st> *<st
    c="23714">Figure 8</st>**<st c="23722">.7</st>*<st c="23724">, with the bias and
    variance curves</st> <st c="23759">now overlayed.</st>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23246">我们接下来将推导*方程*<st c="23281">3</st>，因为这样做有助于理解，并且推导*方程*<st c="23360">3</st>比我们之前暗示的要更微妙。</st>
    <st c="23405">从我们对*图 8*<st c="23457">.4</st>和*图 8*<st c="23472">.6</st>的定性讨论中我们知道，随着模型复杂度的增加，首先，预测的偏差减少，其次，预测的方差增加。</st>
    <st c="23607">我们在*图 8*<st c="23653">.8</st>中示意性地展示了这一点，图中显示了来自*图 8*<st c="23722">.7</st>的原始泛化误差曲线，偏差和方差曲线现在叠加在一起。</st>
- en: '![](img/B19496_08_8.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_8.jpg)'
- en: '<st c="23853">Figure 8.8: A schematic plot of the bias and variance decomposition
    of the generalization error</st>'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23853">图 8.8：泛化误差的偏差和方差分解的示意图</st>
- en: <st c="23948">What are the</st> <st c="23962">consequences of</st> *<st c="23978">Eq.</st>
    <st c="23982">3</st>*<st c="23983">?</st> *<st c="23985">Figure 8</st>**<st c="23993">.8</st>*
    <st c="23995">shows that we must always make a trade-off between bias and variance.</st>
    <st c="24066">In fact, the decomposition of the generalization error that</st>
    *<st c="24126">Eq.</st> <st c="24130">3</st>* <st c="24131">represents is referred
    to as the “bias-variance trade-off.” At the point of the minimum generalization
    error, we have managed to optimize this trade-off.</st> <st c="24286">But the
    fact remains that, at any point on the generalization error curve, we have traded
    bias against variance.</st> <st c="24399">For a given training dataset, if we
    want a model with a smaller bias, we can do so by increasing the model complexity,
    but we pay the price of increased uncertainty in our model predictions.</st> <st
    c="24590">We must be aware of that; there is no free lunch.</st> <st c="24640">Likewise,
    if we want a model with smaller variance (uncertainty) in its predictions, we
    can do so by decreasing the model complexity, but we pay the price of increased
    bias in</st> <st c="24816">those predictions.</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23948">*方程*<st c="23962">3</st>的<st c="23983">后果是什么？</st> *<st c="23985">图
    8</st>**<st c="23993">.8</st>* <st c="23995">显示了我们必须始终在偏差和方差之间进行权衡。</st> <st c="24066">事实上，方程
    3 所表示的泛化误差的分解被称为“偏差-方差权衡”。在最小泛化误差点，我们已经成功地优化了这个权衡。</st> <st c="24286">但事实仍然是，在泛化误差曲线的任何一点，我们都在权衡偏差和方差。</st>
    <st c="24399">对于给定的训练数据集，如果我们想要一个偏差较小的模型，可以通过增加模型复杂性来实现，但我们必须付出增加模型预测不确定性的代价。</st>
    <st c="24590">我们必须意识到这一点；没有免费的午餐。</st> <st c="24640">同样，如果我们想要一个预测方差（不确定性）较小的模型，可以通过减少模型复杂性来实现，但我们必须付出增加偏差的代价。</st>
    <st c="24816">那些预测。</st>
- en: <st c="24834">The only way we can decrease both bias and variance is to increase
    the size of the training data.</st> <st c="24933">This will ensure that any increased
    model complexity has to be used to explain the extra fine-grained trends and patterns
    that the extra data reveals, rather than used to overfit to noise in</st> <st
    c="25124">the data.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24834">减少偏差和方差的唯一方法是增加训练数据的规模。</st> <st c="24933">这将确保任何增加的模型复杂性必须用于解释额外数据揭示的更精细的趋势和模式，而不是用来过度拟合数据中的噪声。</st>
    <st c="25124">数据。</st>
- en: <st c="25133">Proof of the bias-variance trade-off formula</st>
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="25133">偏差-方差权衡公式的证明</st>
- en: <st c="25178">To identify the two contributions</st> <st c="25212">to the generalization
    error curve, we’ll calculate the expected generalization error curve.</st> <st
    c="25305">This gives us the typical shape of the generalization error curve, not
    specific to any particular training dataset.</st> <st c="25421">We’ll specifically
    look at the expected MSE of the holdout dataset.</st> <st c="25489">This is</st>
    <st c="25497">defined</st> <st c="25505">as,</st>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25178">为了确定两个贡献</st> <st c="25212">到泛化误差曲线，我们将计算期望的泛化误差曲线。</st> <st c="25305">这将给我们一个典型的泛化误差曲线形状，而不特定于任何特定的训练数据集。</st>
    <st c="25421">我们将特别关注留出数据集的期望均方误差（MSE）。</st> <st c="25489">这被</st> <st c="25497">定义为</st>
    <st c="25505">，</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><msub><mi>N</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac><mrow><munder><mo>∑</mo><mrow><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>,</mo><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mo>∈</mo><msub><mi>D</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></munder><msup><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mfenced></mrow></mrow></math>](img/2804.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><msub><mi>N</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac><mrow><munder><mo>∑</mo><mrow><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>,</mo><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mo>∈</mo><msub><mi>D</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></munder><msup><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mfenced></mrow></mrow></math>](img/2804.png)'
- en: <st c="25557">Eq.</st> <st c="25561">4</st>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25557">方程</st> <st c="25561">4</st>
- en: <st c="25562">For simplicity of notation, we have omitted the dependence of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2805.png)
    <st c="25624"><st c="25625">on the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="25650"><st c="25651">in</st> *<st c="25655">Eq.</st> <st c="25659">4</st>*<st
    c="25660">.</st></st></st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25562">为了简化符号，我们省略了对</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2805.png)
    <st c="25624"><st c="25625">对模型参数</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="25650"><st c="25651">的依赖</st> *<st c="25655">方程</st> <st c="25659">4</st>*<st
    c="25660">。</st></st></st>
- en: <st c="25661">To understand</st> *<st c="25676">Eq.</st> <st c="25680">4</st>*<st
    c="25681">, we need to understand</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced></mrow></mrow></math>](img/2807.png)
    <st c="25705"><st c="25725">averaged over all the holdout data points</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2808.png)<st
    c="25767"><st c="25768">.</st></st></st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解*<st c="25676">方程</st> <st c="25680">4</st>*<st c="25681">，我们需要理解</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced></mrow></mrow></math>](img/2807.png)
    <st c="25705"><st c="25725">在所有保持数据点的平均值上</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2808.png)<st
    c="25767"><st c="25768">。</st></st></st>
- en: <st c="25770">If we expand out</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2809.png)<st
    c="25787"><st c="25788">, we can write</st> <st c="25803">this</st> <st c="25808">as,</st></st>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们展开</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2809.png)<st
    c="25787"><st c="25788">，我们可以写作</st> <st c="25803">这个</st> <st c="25808">为，</st></st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>−</mo><msub><mrow><mn>2</mn><mi
    mathvariant="double-struck">E</mi></mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2810.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>−</mo><msub><mrow><mn>2</mn><mi
    mathvariant="double-struck">E</mi></mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2810.png)'
- en: <st c="25886">Eq.</st> <st c="25890">5</st>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25886">等式</st> <st c="25890">5</st>
- en: <st c="25891">We’ll take each of the terms on the right-hand side of</st> *<st
    c="25946">Eq.</st> <st c="25950">5</st>* <st c="25951">one at a time.</st> <st
    c="25967">For the first term, we’re going to add the assumption that the observed
    value of the holdout target value,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26074"><st c="26097">, is just a noise-corrupted version of a ground-truth
    value, so</st> <st c="26161">we have,</st></st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25891">我们将逐一讨论右侧的每个项</st> *<st c="25946">等式</st> <st c="25950">5</st>*
    <st c="25951">的每一项。</st> <st c="25967">对于第一个项，我们假设观察到的保留目标值</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26074"><st c="26097">只是一个受噪声影响的真实值版本，因此</st> <st c="26161">我们得到</st>，</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>+</mo><mi>ε</mi></mrow></mrow></math>](img/2812.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></under></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>+</mo><mi>ε</mi></mrow></mrow></math>](img/2812.png)'
- en: <st c="26183">Eq.</st> <st c="26187">6</st>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26183">等式</st> <st c="26187">6</st>
- en: <st c="26188">We’ll assume that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/384.png)
    <st c="26206"><st c="26207">is a random variable with zero mean and variance</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)<st
    c="26257"><st c="26260">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2815.png)
    <st c="26266"><st c="26267">function is a deterministic function – it has no randomness
    in it.</st> <st c="26335">It is the value we would get for the target variable
    at a holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="26408"><st c="26409">, if there were no added noise.</st> <st c="26441">We’ll
    calculate the expectation of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2817.png)
    <st c="26476"><st c="26494">over</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)
    <st c="26499"><st c="26500">to represent the averaging over the holdout value,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26552"><st c="26575">. This means in</st> *<st c="26591">Eq.</st> <st c="26595">5</st>*<st
    c="26596">, we replace</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/2820.png)
    <st c="26609"><st c="26616">with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:msub></mml:math>](img/2821.png)<st
    c="26621"><st c="26634">. With that additional change,</st> <st c="26665">we g</st><st
    c="26669">et,</st></st></st></st></st></st></st></st></st></st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26188">我们假设</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/384.png)
    <st c="26206"><st c="26207">是一个均值为零，方差为</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)<st
    c="26257"><st c="26260">的随机变量。</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2815.png)
    <st c="26266"><st c="26267">这个函数是一个确定性函数——它不包含任何随机性。</st> <st c="26335">它是我们在保留点处得到的目标变量的值，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="26408"><st c="26409">，如果没有额外的噪声。</st> <st c="26441">我们将计算</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2817.png)
    <st c="26476"><st c="26494">关于</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)
    <st c="26499"><st c="26500">的期望值，以表示对保留值的平均化，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26552"><st c="26575">。这意味着在</st> *<st c="26591">公式</st> <st c="26595">5</st>*<st
    c="26596">中，我们将</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/2820.png)
    <st c="26609"><st c="26616">替换为</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:msub></mml:math>](img/2821.png)<st
    c="26621"><st c="26634">。通过这个额外的更改，</st> <st c="26665">我们得到，</st><st c="26669">...
    </st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup></mfenced><mo>+</mo><mn>2</mn><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>ε</mi><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>σ</mi><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2822.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup></mfenced><mo>+</mo><mn>2</mn><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>ε</mi><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>σ</mi><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2822.png)'
- en: <st c="26724">Eq.</st> <st c="26728">7</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**式**<st c="26728">7</st>'
- en: <st c="26729">In deriving the very right-hand side of</st> *<st c="26769">Eq.</st>
    <st c="26773">7</st>*<st c="26774">, we have made use of the fact that the random
    additive noise</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)
    <st c="26836"><st c="26837">has mean zero, and that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2824.png)
    <st c="26862"><st c="26863">is just a fixed number with</st> <st c="26892">no
    randomness.</st></st></st>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在推导**式**<st c="26769">7</st>的右侧时，我们利用了以下事实：随机加性噪声！[<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)的均值为零，并且！[<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2824.png)只是一个固定的数值，**没有随机性**。
- en: <st c="26906">For the second term in</st> *<st c="26930">Eq.</st> <st c="26934">5</st>*<st
    c="26935">,</st> <st c="26937">we fi</st><st c="26942">nd,</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**式**<st c="26930">5</st>中的第二项，我们得出：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>ε</mi></mrow></mfenced><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2825.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>ε</mi></mrow></mfenced><mi>g</mi><mfenced
    open=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2825.png)'
- en: <st c="27020">Eq.</st> <st c="27024">8</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27020">方程式</st> <st c="27024">8</st>
- en: <st c="27025">The final term on the right-hand side of</st> *<st c="27066">Eq.</st>*
    *<st c="27070">5</st>* <st c="27071">is,</st>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27025">右侧的最后一项是，</st> *<st c="27066">方程式</st>* *<st c="27070">5</st>*
    <st c="27071">中的，</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2826.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2826.png)'
- en: <st c="27076">Eq.</st> <st c="27080">9</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27076">方程</st> <st c="27080">9</st>
- en: <st c="27081">The very</st> <st c="27090">right-hand side of</st> *<st c="27109">Eq.</st>
    <st c="27113">9</st>* <st c="27114">follows from the fact that for any random
    variable</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)
    <st c="27166"><st c="27167">we have,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>−</mo><msup><mfenced
    open="(" close=")"><mrow><mi mathvariant="double-struck">E</mi><mfenced open="("
    close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2828.png)<st
    c="27177"><st c="27201">, and so re-arranging we</st> <st c="27226">have</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>=</mo><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>+</mo><msup><mfenced open="(" close=")"><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2829.png)<st
    c="27231"><st c="27255">.</st></st></st></st>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27081">右边</st> <st c="27090">的</st> *<st c="27109">方程</st> <st c="27113">9</st>*
    <st c="27114">来自于以下事实：对于任何随机变量</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)
    <st c="27166"><st c="27167">我们有，</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>−</mo><msup><mfenced
    open="(" close=")"><mrow><mi mathvariant="double-struck">E</mi><mfenced open="("
    close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2828.png)<st
    c="27177"><st c="27201">，因此重新排列后我们得到</st> <st c="27226">，</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>=</mo><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>+</mo><msup><mfenced open="(" close=")"><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2829.png)<st
    c="27231"><st c="27255">。</st></st></st></st>
- en: <st c="27256">Plugging the results from</st> *<st c="27283">Eq.</st> <st c="27287">7</st>*<st
    c="27288">,</st> *<st c="27290">Eq.</st> <st c="27294">8</st>*<st c="27295">,
    and</st> *<st c="27301">Eq.</st> <st c="27305">9</st>* <st c="27306">into</st>
    *<st c="27312">Eq.</st> <st c="27316">5</st>*<st c="27317">,</st> <st c="27319">we</st>
    <st c="27322">get,</st>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将来自*<st c="27283">方程</st> <st c="27287">7</st>*<st c="27288">、</st> *<st c="27290">方程</st>
    <st c="27294">8</st>*<st c="27295">和</st> *<st c="27301">方程</st> <st c="27305">9</st>*
    <st c="27306">的结果代入</st> *<st c="27312">方程</st> <st c="27316">5</st>*<st c="27317">，</st>
    <st c="27319">我们得到，</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mn>2</mn><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2830.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mn>2</mn><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2830.png)'
- en: <st c="27439">Eq.</st> <st c="27443">10</st>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式10
- en: <st c="27445">We can rearrange the terms on the right-hand side of</st> *<st
    c="27499">Eq.</st> <st c="27503">10</st>* <st c="27505">to g</st><st c="27510">ive,</st>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新排列右边的项，得到
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>−</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mn>2</mn></msup><mo>+</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2831.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>−</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mn>2</mn></msup><mo>+</mo><msub><mtext>方差</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2831.png)'
- en: <st c="27588">Eq.</st> <st c="27592">11</st>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27588">等式</st> <st c="27592">11</st>
- en: <st c="27594">Now,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2832.png)
    <st c="27600"><st c="27601">is just the expected difference between the prediction
    of our model at the holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="27692"><st c="27693">, and the true (noise-free) value at that point.</st>
    <st c="27742">This is just the bias.</st> <st c="27765">Similarly, the expression</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Var</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/2834.png)
    <st c="27791"><st c="27810">is just the variance of our model’s prediction as
    we train it on different training datasets.</st> <st c="27904">This means we can
    use</st> *<st c="27926">Eq.</st> <st c="27930">11</st>* <st c="27932">to wr</st><st
    c="27938">ite,</st></st></st></st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27594">现在，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2832.png)
    <st c="27600"><st c="27601">仅仅是我们模型在保留点的预测与该点的真实（无噪声）值之间的期望差异。</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="27692"><st c="27693">，以及该点的真实（无噪声）值。</st> <st c="27742">这只是偏差。</st> <st c="27765">类似地，表达式</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Var</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/2834.png)
    <st c="27791"><st c="27810">仅仅是我们模型的预测方差，当我们在不同的训练数据集上训练它时。</st> <st c="27904">这意味着我们可以使用</st>
    *<st c="27926">方程</st> <st c="27930">11</st>* <st c="27932">来编写，</st></st></st></st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>on</mtext><mtext>holdout</mtext><mo>=</mo><msup><mrow><mtext>Bias</mtext><mtext>in</mtext><mtext>prediction</mtext></mrow><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext><mtext>of</mtext><mtext>prediction</mtext><mo>+</mo><mtext>Variance</mtext><mtext>of</mtext><mtext>noise</mtext></mrow></mrow></math>](img/2835.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>均方误差</mtext><mtext>在</mtext><mtext>留出集上的</mtext><mo>=</mo><msup><mrow><mtext>预测的偏差</mtext></mrow><mn>2</mn></msup><mo>+</mo><mtext>预测的方差</mtext><mtext>+</mtext><mtext>噪声的方差</mtext></mrow></mrow></math>](img/2835.png)'
- en: <st c="28025">Eq.</st> <st c="28029">12</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28025">方程</st> <st c="28029">12</st>
- en: '*<st c="28031">Eq.</st> <st c="28036">12</st>* <st c="28038">is the</st> <st
    c="28045">same as</st> *<st c="28054">Eq.</st> <st c="28058">3</st>*<st c="28059">,
    with the addition of the last term, the variance</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)
    <st c="28110"><st c="28113">of the additive noise.</st> <st c="28136">This means
    that the presence of noise in the data sets a minimum value for the generalization
    error.</st> <st c="28237">No amount of trading off bias against variance will
    get us below</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)
    <st c="28302"><st c="28307">for the MSE on the holdout data.</st> <st c="28340">This</st>
    <st c="28345">is intuitive.</st></st></st>'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="28031">方程</st> <st c="28036">12</st>* <st c="28038">与</st> *<st c="28054">方程</st>
    <st c="28058">3</st>*<st c="28059">相同，只是多了最后一项，即方差</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)
    <st c="28110"><st c="28113">加性噪声的方差。</st> <st c="28136">这意味着数据中的噪声设定了泛化误差的最低值。</st>
    <st c="28237">无论如何权衡偏差与方差，都无法将泛化误差降到</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)
    <st c="28302"><st c="28307">以下，针对留出数据的均方误差（MSE）。</st> <st c="28340">这</st> <st
    c="28345">是直观的。</st>'
- en: <st c="28358">We have already unpacked the consequences of the bias-variance
    trade-off equation.</st> <st c="28442">This equation neatly summarizes the classical
    view of model complexity.</st> <st c="28514">The schematic plot shown in</st>
    *<st c="28542">Figure 8</st>**<st c="28550">.7</st>* <st c="28552">also neatly
    summarizes this classical view of model complexity.</st> <st c="28617">You may
    have seen it before, as it is used in many introductions to machine learning.</st>
    <st c="28703">However, for highly parameterized machine learning models, the plot
    in</st> *<st c="28774">Figure 8</st>**<st c="28782">.7</st>* <st c="28784">is
    not the full story.</st> <st c="28808">An interesting twist has arisen in the
    last few years, emerging from the study of deep learning neural networks.</st>
    <st c="28921">We’ll now explain what that</st> <st c="28949">twist is.</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28358">我们已经解析了偏差-方差权衡方程的后果。</st> <st c="28442">这个方程简明扼要地总结了模型复杂度的经典观点。</st>
    <st c="28514">如</st> *<st c="28542">图 8</st>**<st c="28550">.7</st>* <st c="28552">所示的示意图，也简明扼要地总结了这一经典观点。</st>
    <st c="28617">你可能之前见过它，因为它在许多机器学习入门书籍中都有使用。</st> <st c="28703">然而，对于高度参数化的机器学习模型，</st>
    *<st c="28774">图 8</st>**<st c="28782">.7</st>* <st c="28784">中的图并不能完整描述情况。</st>
    <st c="28808">近年来，随着深度学习神经网络研究的发展，一个有趣的变化出现了。</st> <st c="28921">接下来我们将解释这个</st>
    <st c="28949">变化是什么。</st>
- en: <st c="28958">Double descent – a modern twist on the generalization error diagram</st>
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="28958">双重下降——对泛化误差图的现代演绎</st>
- en: <st c="29026">In our previous qualitative</st> <st c="29055">discussions, where
    we fitted various polynomials (from the 0</st><st c="29115">th</st> <st c="29118">degree
    to the 12</st><st c="29135">th</st> <st c="29138">degree) to the dataset in</st>
    *<st c="29165">Figure 8</st>**<st c="29173">.2</st>*<st c="29175">, it was probably
    obvious to you where the sweet spot lay in terms of number of model parameters.</st>
    <st c="29273">The data in</st> *<st c="29285">Figure 8</st>**<st c="29293">.2</st>*
    <st c="29295">was generated using a quadratic equation, so a 2nd-degree polynomial
    was clearly optimal to use to fit to the training data.</st> <st c="29421">In
    these circumstances, it was self-evident where the generalization error minimum
    should be because the models we were fitting to the training data were in the
    same class – polynomials – as the process that generated the data.</st> <st c="29649">This
    will not always be</st> <st c="29673">the case.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29026">在我们之前的定性</st> <st c="29055">讨论中，我们将不同的多项式（从 0</st><st c="29115">次</st>
    <st c="29118">到 12</st><st c="29135">次</st> <st c="29138">）拟合到数据集中</st> *<st c="29165">图
    8</st>**<st c="29173">.2</st>*<st c="29175">，你应该能很明显地看出，模型参数数量的最佳位置。</st> <st
    c="29273">数据在</st> *<st c="29285">图 8</st>**<st c="29293">.2</st>* <st c="29295">是通过二次方程生成的，所以二次多项式显然是最适合拟合训练数据的。</st>
    <st c="29421">在这种情况下，泛化误差最小值应该出现在显而易见的地方，因为我们拟合的模型与生成数据的过程属于同一类——多项式。</st> <st
    c="29649">然而，这种情况并非总是如此。</st> <st c="29673">并非每次都会如此。</st>
- en: <st c="29682">Imagine that we have a</st> <st c="29705">real-world dataset that
    we model using a neural network.</st> <st c="29763">It is highly unlikely that
    the data was generated using a neural network.</st> <st c="29837">However, our
    qualitative arguments about overfitting that led us to</st> *<st c="29905">Figure
    8</st>**<st c="29913">.7</st>* <st c="29915">still hold.</st> <st c="29928">We
    still expect to see the training error decrease monotonically as we increase the
    number of parameters in the neural network, and for the generalization error to
    display a minimum.</st> <st c="30111">For modern machine learning models, what
    we tend to see is illustrated schemati</st><st c="30190">cally in</st> *<st c="30200">Figure
    8</st>**<st c="30208">.9</st>*<st c="30210">.</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29682">假设我们有一个</st> <st c="29705">使用神经网络建模的现实世界数据集。</st> <st c="29763">数据生成方式极不可能是通过神经网络来实现的。</st>
    <st c="29837">然而，关于过拟合的定性论证仍然适用于我们之前提到的</st> *<st c="29905">图 8</st>**<st c="29913">.7</st>*
    <st c="29915">。我们仍然期望随着神经网络中参数数量的增加，训练误差单调下降，而泛化误差则会出现一个最小值。</st> <st c="30111">对于现代机器学习模型，我们通常会看到的情况在下图中有所示意</st><st
    c="30190">cally in</st> *<st c="30200">图 8</st>**<st c="30208">.9</st>*<st c="30210">。</st>
- en: '![](img/B19496_08_9.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_9.jpg)'
- en: '<st c="30292">Figure 8.9: A generalization error curve for a modern machine
    learning model</st>'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30292">图 8.9：现代机器学习模型的泛化误差曲线</st>
- en: <st c="30368">Schematically, the generalization curve for our neural network
    model looks like the generalization curve in</st> *<st c="30477">Figure 8</st>**<st
    c="30485">.7</st>*<st c="30487">. The difference is that now the</st> *<st c="30520">x</st>*<st
    c="30521">-axis explicitly represents the number of model parameters,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30581"><st c="30582">. The generalization error curve increases steeply as
    we increase</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30648"><st c="30649">, with the steep increase occurring as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2840.png)
    <st c="30688"><st c="30689">approaches the size of the training dataset,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="30735"><st c="30736">. At first sight, it looks like nothing more interesting
    is happening and there is no need to explore any further.</st> <st c="30851">However,
    if we continue to increase the number of parameters in our neural network, what
    we tend to see is illustrated schematical</st><st c="30981">ly in</st> *<st c="30988">Figure
    8</st>**<st c="30996">.10</st>*<st c="30999">.</st></st></st></st></st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30368">从示意图看，我们神经网络模型的泛化曲线与</st> *<st c="30477">图 8</st>**<st c="30485">.7</st>*<st
    c="30487">中的泛化曲线相似。不同之处在于，</st> *<st c="30520">x</st>*<st c="30521">轴明确表示模型参数的数量，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30581"><st c="30582">。当我们增加</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30648"><st c="30649">时，泛化误差曲线急剧上升，急剧增加发生在</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2840.png)
    <st c="30688"><st c="30689">接近训练数据集的大小</st>，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="30735"><st c="30736">时。</st>乍一看，似乎没有更有趣的现象发生，且没有必要进一步探究。</st> <st c="30851">然而，如果我们继续增加神经网络中的参数数量，所看到的现象如示意图所示</st><st
    c="30981">ly in</st> *<st c="30988">图 8</st>**<st c="30996">.10</st>*<st c="30999">。</st></st></st></st></st>
- en: '![](img/B19496_08_10.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19496_08_10.jpg)'
- en: '<st c="31080">Figure 8.10: A “double descent” generalization error curve for
    a modern machine learning model</st>'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31080">图 8.10：现代机器学习模型的“双重下降”泛化误差曲线</st>
- en: '*<st c="31174">Figure 8</st>**<st c="31183">.10</st>* <st c="31186">shows that
    as</st> <st c="31201">we increase the number of parameters in our neural network
    beyond</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/443.png)<st
    c="31267"><st c="31268">, the generalization error decreases again.</st> <st c="31312">This
    phenomenon is termed</st> *<st c="31338">double descent</st>* <st c="31352">because
    the generalization error decreases for a second time as we increase the model
    complexity.</st> <st c="31451">For</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mo>></mml:mo><mml:mi>N</mml:mi></mml:math>](img/2843.png)<st
    c="31455"><st c="31460">, both the training error and generalization error continue</st>
    <st c="31520">to decrease.</st></st></st>'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="31174">图 8</st>**<st c="31183">.10</st>* <st c="31186">显示，当</st> <st
    c="31201">我们将神经网络的参数数量增加超过</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/443.png)<st
    c="31267"><st c="31268">时，泛化误差再次下降。</st> <st c="31312">这一现象被称为</st> *<st c="31338">双重下降</st>*
    <st c="31352">，因为随着我们增加模型复杂度，泛化误差第二次下降。</st> <st c="31451">对于</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mo>></mml:mo><mml:mi>N</mml:mi></mml:math>](img/2843.png)<st
    c="31455"><st c="31460">，训练误差和泛化误差都继续</st> <st c="31520">下降。</st></st></st>'
- en: <st c="31532">The consequences of double descent</st>
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="31532">双重下降的后果</st>
- en: '*<st c="31567">Figure 8</st>**<st c="31576">.10</st>* <st c="31579">tells</st>
    <st c="31586">us that it is possible to have an</st> *<st c="31620">over-parameterized</st>*
    <st c="31638">model and still have good generalization performance.</st> <st c="31693">This
    has big implications for large neural network models and, in particular,</st>
    **<st c="31771">Large Language Models</st>** <st c="31792">(</st>**<st c="31794">LLMs</st>**<st
    c="31798">), which can have billions of parameters.</st> *<st c="31841">Figure
    8</st>**<st c="31849">.10</st>* <st c="31852">shows that this</st> <st c="31868">number
    of parameters in a model is not an obstacle to the model genuinely learning, so
    double descent gives support to the use and development of deep learning neural
    networks, LLMs, and other massively</st> <st c="32072">parameterized models.</st>'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="31567">图 8</st>**<st c="31576">.10</st>* <st c="31579">告诉我们，有可能拥有一个</st>
    *<st c="31620">过参数化</st>* <st c="31638">的模型，并且仍然能够保持良好的泛化性能。</st> <st c="31693">这对大型神经网络模型，特别是</st>
    **<st c="31771">大型语言模型</st>** <st c="31792">(</st>**<st c="31794">LLMs</st>**<st
    c="31798">)，它们可能拥有数十亿个参数，具有重要影响。</st> *<st c="31841">图 8</st>**<st c="31849">.10</st>*
    <st c="31852">表明，模型中的参数数量并不是模型真正学习的障碍，因此，双重下降为深度学习神经网络、大型语言模型（LLMs）以及其他大规模</st>
    <st c="32072">参数化模型的使用和发展提供了支持。</st>'
- en: <st c="32093">However, that’s not to say we fully understand all the details
    behind double-descent.</st> <st c="32180">The phenomenon of double descent is
    still very much an active area of research.</st> <st c="32260">Indeed, at the
    time of writing (2023), one of the most talked-about papers at the top machine
    learning conference, NeurIPS, was about double-descent and how you should effectively
    count parameters in a model – see the</st> *<st c="32478">Notes and further reading</st>*
    <st c="32503">section at the end of the chapter for details on this paper.</st>
    <st c="32565">Furthermore, the debate continues in academic circles and on social
    media about whether LLMs have truly learned from their training data and are displaying
    understanding, or whether they have simply memorized the training data and stored
    it in the massive number of</st> <st c="32831">model parameters.</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32093">然而，这并不意味着我们完全理解双重下降背后的所有细节。</st> <st c="32180">双重下降现象仍然是一个非常活跃的研究领域。</st>
    <st c="32260">事实上，在写作时（2023年），在顶级机器学习会议NeurIPS上讨论最多的论文之一就是关于双重下降以及如何有效地计算模型中的参数——详情请参见本章末尾的</st>
    *<st c="32478">笔记与进一步阅读</st>* <st c="32503">部分。</st> <st c="32565">此外，关于LLMs是否真的从其训练数据中学习并展示理解，还是仅仅将训练数据记忆并存储在大量的</st>
    <st c="32831">模型参数中，这一辩论仍在学术界和社交媒体上继续进行。</st>
- en: <st c="32848">Since double</st> <st c="32861">descent brings us up to the present
    day in terms of what is known about generalization error and model predictive
    accuracy, this seems like a good place to finish this section and recap what we</st>
    <st c="33056">have learned.</st>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32848">由于双重下降将我们带到了关于泛化误差和模型预测准确性目前已知的阶段，这似乎是一个很好的地方来结束本节并回顾我们</st> <st
    c="32861">所学到的内容。</st>
- en: <st c="33069">What we learned</st>
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="33069">我们学到的内容</st>
- en: <st c="33085">In this section, we learned</st> <st c="33114">the following:</st>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33085">在本节中，我们学到了</st> <st c="33114">以下内容：</st>
- en: <st c="33128">The generalization error is made up of two contributions, one
    from the bias in the model predictions and one from the variance of the</st> <st
    c="33263">model predictions</st>
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="33128">泛化误差由两个部分组成，一个是来自模型预测中的偏差，另一个是来自</st> <st c="33263">模型预测的方差</st>
- en: <st c="33280">The bias-variance trade-off and</st> <st c="33313">its implications</st>
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="33280">偏差-方差权衡及其</st> <st c="33313">影响</st>
- en: <st c="33329">The mathematical detail behind the bias-variance</st> <st c="33379">trade-off
    equation</st>
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="33329">偏差-方差</st> <st c="33379">权衡方程背后的数学细节</st>
- en: <st c="33397">The phenomenon of double descent in over-parameterized machine
    learning models and</st> <st c="33481">its implications</st>
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="33397">过参数化机器学习模型中的双重下降现象及其</st> <st c="33481">影响</st>
- en: <st c="33497">Having learned more mathematical details about generalization
    error, in the next section, we will learn about various model complexity metrics
    and how to use them to select models of the appropriate complexity for a given</st>
    <st c="33720">training set.</st>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33497">在进一步学习了有关泛化误差的更多数学细节后，在下一节中，我们将学习各种模型复杂度度量，并了解如何使用它们为给定的</st>
    <st c="33720">训练集选择适当复杂度的模型。</st>
- en: <st c="33733">Model complexity measures for model selection</st>
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="33733">模型选择的模型复杂度度量</st>
- en: <st c="33779">Practical model</st> <st c="33796">complexity measures tend not
    to measure model complexity directly.</st> <st c="33863">Instead, they measure
    some sort of trade-off – for example, how much information has been lost by approximating
    the patterns present in a dataset by using a particular model form, or what evidence
    a dataset provides for a model form of this level of complexity.</st> <st c="34125">These
    practical metrics don’t directly measure model complexity, but they take it</st>
    <st c="34207">into account.</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33779">实际模型</st> <st c="33796">复杂性度量通常不会直接衡量模型复杂性。</st> <st c="33863">相反，它们衡量某种权衡——例如，通过使用特定的模型形式来逼近数据集中的模式，丧失了多少信息，或者数据集为具有此复杂度的模型形式提供了多少证据。</st>
    <st c="34125">这些实际度量并不直接衡量模型复杂性，但它们考虑到了这一点。</st>
- en: <st c="34220">Selecting between classes of models</st>
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="34220">在不同模型类别之间进行选择</st>
- en: <st c="34256">In the preceding paragraph, we referred to model form.</st> <st
    c="34312">But what do we mean by model form?</st> <st c="34347">We mean the mathematical
    form of the equation that defines a model.</st> <st c="34415">So, two models that
    differ only in their parameter values but otherwise have the same form of mathematical
    equation have the same model form (e.g., two linear models that use the</st> <st
    c="34594">same features).</st>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34256">在前一段中，我们提到了模型形式。</st> <st c="34312">那么，我们所说的模型形式到底是什么意思呢？</st>
    <st c="34347">我们指的是定义模型的方程的数学形式。</st> <st c="34415">因此，两个仅在参数值上有所不同，但数学方程形式相同的模型具有相同的模型形式（例如，使用相同特征的两个线性模型）。</st>
- en: <st c="34609">A model form</st> <st c="34622">represents a whole class of models.</st>
    <st c="34659">Let’s go back to our polynomial model example to illustrate.</st>
    <st c="34720">For our polynomial models in</st> *<st c="34749">Figure 8</st>**<st
    c="34757">.1</st>* <st c="34759">and</st> *<st c="34764">Figure 8</st>**<st c="34772">.2</st>*<st
    c="34774">, we had 1</st><st c="34784">st</st><st c="34787">-degree polynomials,
    2</st><st c="34810">nd</st><st c="34813">-degree polynomials, 4</st><st c="34836">th</st><st
    c="34839">-degree polynomials, and 12</st><st c="34867">th</st><st c="34870">-degree
    polynomials.</st> <st c="34892">The model class in this case is just the degree
    of the polynomial.</st> <st c="34959">The 12</st><st c="34965">th</st><st c="34968">-degree
    model class is the set of all possible 12</st><st c="35018">th</st><st c="35021">-degree
    polynomials.</st> <st c="35043">In this case, the model class is related simply
    to the number of parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)
    <st c="35119"><st c="35120">in the</st> <st c="35128">model because,</st></st>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34609">模型形式</st> <st c="34622">代表着一个完整的模型类别。</st> <st c="34659">让我们回到我们的多项式模型示例来进行说明。</st>
    <st c="34720">对于我们在</st> *<st c="34749">图 8</st>**<st c="34757">.1</st>* <st c="34759">和</st>
    *<st c="34764">图 8</st>**<st c="34772">.2</st>*<st c="34774">中展示的多项式模型，我们有 1</st><st
    c="34784">次</st><st c="34787">-度多项式、2</st><st c="34810">次</st><st c="34813">-度多项式、4</st><st
    c="34836">次</st><st c="34839">-度多项式以及 12</st><st c="34867">次</st><st c="34870">-度多项式。</st>
    <st c="34892">此时，模型类别仅与多项式的次数有关。</st> <st c="34959">12</st><st c="34965">次</st><st
    c="34968">-度模型类别是所有可能的 12</st><st c="35018">次</st><st c="35021">-度多项式的集合。</st>
    <st c="35043">在这种情况下，模型类别与参数的数量直接相关</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)
    <st c="35119"><st c="35120">在模型中，因为</st></st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>p</mi><mo
    mathvariant="italic">=</mo><mtext mathvariant="italic">degree</mtext><mtext mathvariant="italic">of</mtext><mtext
    mathvariant="italic">polynomial</mtext><mtext mathvariant="italic">+</mtext><mtext>1</mtext></mrow></mrow></math>](img/2845.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>p</mi><mo
    mathvariant="italic">=</mo><mtext mathvariant="italic">degree</mtext><mtext mathvariant="italic">of</mtext><mtext
    mathvariant="italic">polynomial</mtext><mtext mathvariant="italic">+</mtext><mtext>1</mtext></mrow></mrow></math>](img/2845.png)'
- en: <st c="35172">Eq.</st> <st c="35176">13</st>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35172">方程</st> <st c="35176">13</st>
- en: <st c="35178">We can denote each model class by</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)<st
    c="35213"><st c="35214">, and in this instance, the different model classes</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)
    <st c="35266"><st c="35267">represent different levels of</st> <st c="35298">model
    complexity.</st></st></st>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过<st c="35178">表示每个模型类别</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)<st
    c="35213"><st c="35214">，在此情况下，不同的模型类别</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)
    <st c="35266"><st c="35267">代表不同层次的</st> <st c="35298">模型复杂度。</st></st></st>
- en: <st c="35315">The challenge we face is working out which degree of polynomial
    model to use to model our data.</st> <st c="35412">In other words, we</st> <st
    c="35430">need to work out which class of models to use.</st> <st c="35478">This
    process is called</st> **<st c="35501">model selection</st>**<st c="35516">.</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35315">我们面临的挑战是确定使用哪种多项式模型来拟合我们的数据。</st> <st c="35412">换句话说，我们</st> <st
    c="35430">需要确定使用哪种模型类别。</st> <st c="35478">这个过程叫做</st> **<st c="35501">模型选择</st>**<st
    c="35516">。</st>
- en: <st c="35517">Once we have selected which class of models to use, identifying
    which model to use within that class is just a case of parameter estimation (e.g.,
    via least-squares fitting) or maximum likelihood estimation.</st> <st c="35726">In
    fact, when comparing two model classes, we often just compare the maximum likelihood
    models from each class, since this uses the best model in each class as a representative
    of</st> <st c="35906">that class.</st>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35517">一旦我们确定了要使用的模型类别，选择该类别中要使用的具体模型就只是一个参数估计问题（例如，通过最小二乘拟合）或最大似然估计。</st>
    <st c="35726">实际上，在比较两个模型类别时，我们通常只比较每个类别中的最大似然模型，因为这使用每个类别中的最佳模型作为该类别的代表。</st>
    <st c="35906">该类别。</st>
- en: <st c="35917">We have already met a measure that helps us select between different
    model forms or classes, the generalization error.</st> <st c="36037">We have also
    seen how it is affected by model complexity.</st> <st c="36095">In this section,
    we will introduce two other commonly used model complexity measures</st> <st c="36179">that
    are used for model selection, the</st> **<st c="36219">Akaike Information Criterion</st>**
    <st c="36247">(</st>**<st c="36249">AIC</st>**<st c="36252">) and</st> <st c="36259">the</st>
    **<st c="36263">Bayesian Information</st>** **<st c="36284">Criterion</st>** <st
    c="36293">(</st>**<st c="36295">BIC</st>**<st c="36298">).</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35917">我们已经遇到了一种度量方法，帮助我们在不同的模型形式或类别之间进行选择，那就是泛化误差。</st> <st c="36037">我们也看到它如何受到模型复杂度的影响。</st>
    <st c="36095">在这一部分，我们将介绍另外两种常用的模型复杂度度量方法</st> <st c="36179">，它们用于模型选择，分别是</st>
    **<st c="36219">赤池信息量准则</st>** <st c="36247">(</st>**<st c="36249">AIC</st>**<st
    c="36252">) 和</st> <st c="36259">贝叶斯信息量</st>**<st c="36263">准则</st>** <st c="36284">（</st>**<st
    c="36295">BIC</st>**<st c="36298">）。</st>
- en: <st c="36301">To start, we’ll define our model form,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)<st
    c="36341"><st c="36342">, for which we want to compute the AIC and the BIC.</st>
    <st c="36394">The model form</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)
    <st c="36409"><st c="36410">is a function,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2850.png)<st
    c="36426"><st c="36427">, that takes a feature vector,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="36458"><st c="36459">, as input and uses its</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2852.png)
    <st c="36483"><st c="36484">model parameters, represented by the vector</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="36529"><st c="36530">, to compute the model output.</st> <st c="36561">From
    now on, we’ll use</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="36584"><st c="36585">when we mean either a model or a model form.</st>
    <st c="36631">We also assume we know how to calculate the likelihood,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>L</mml:mi></mml:math>](img/1598.png)<st
    c="36687"><st c="36688">, of a dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi><mml:mo>,</mml:mo></mml:math>](img/2856.png)
    <st c="36704"><st c="36705">given the model – that is, we know how to calculate</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>,</mo><mi>g</mi><mo>)</mo></mrow></mrow></mrow></math>](img/2857.png)<st
    c="36758"><st c="36770">. With that in place, we can begin to compute the AIC</st>
    <st c="36824">and BIC.</st></st></st></st></st></st></st></st></st></st></st>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36301">首先，我们将定义我们的模型形式，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)<st
    c="36341"><st c="36342">，我们希望计算 AIC 和 BIC。</st> <st c="36394">该模型形式</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)
    <st c="36409"><st c="36410">是一个函数，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2850.png)<st
    c="36426"><st c="36427">，它接受一个特征向量，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="36458"><st c="36459">，作为输入，并使用它的</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2852.png)
    <st c="36483"><st c="36484">模型参数，通过向量</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="36529"><st c="36530">，来计算模型输出。</st> <st c="36561">从现在开始，我们将使用</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="36584"><st c="36585">来表示模型或模型形式。</st> <st c="36631">我们还假设我们知道如何计算数据集的似然，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>L</mml:mi></mml:math>](img/1598.png)<st
    c="36687"><st c="36688">，给定模型——也就是说，我们知道如何计算</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>,</mo><mi>g</mi><mo>)</mo></mrow></mrow></mrow></math>](img/2857.png)<st
    c="36758"><st c="36770">。有了这些基础，我们可以开始计算 AIC</st> <st c="36824">和 BIC。</st>
- en: <st c="36832">Akaike Information Criterion</st>
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="36832">赤池信息量准则</st>
- en: <st c="36861">The</st> <st c="36866">AIC</st> <st c="36869">is</st> <st c="36873">defined
    as,</st>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36861">AIC</st> <st c="36866">定义为：</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>AIC</mtext><mtext>=</mtext><mn>2</mn><mi>p</mi><mtext>-</mtext><mtext>2</mtext><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2858.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>AIC</mtext><mtext>=</mtext><mn>2</mn><mi>p</mi><mtext>-</mtext><mtext>2</mtext><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2858.png)'
- en: <st c="36902">Eq.</st> <st c="36906">14</st>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36902">公式</st> <st c="36906">14</st>
- en: <st c="36908">In</st> *<st c="36912">Eq.</st> <st c="36916">14</st>*<st c="36918">,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/2859.png)
    <st c="36920"><st c="36924">is the maximum likelihood of the model</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)<st
    c="36963"><st c="36964">. The AIC attempts to measure the information lost by
    the model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37028"><st c="37029">when we use it as an approximation of the true process
    that generated the data.</st> <st c="37110">We haven’t yet defined what “information”
    is, and we don’t do so until</st> [*<st c="37181">Chapter 13</st>*](B19496_13.xhtml#_idTextAnchor646)<st
    c="37191">. Basically, we can think of the information loss as a measure of how
    different the general trends and patterns that model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37314"><st c="37315">produces are, compared to the general trends and patterns
    from the true</st> <st c="37388">data-generating process.</st></st></st></st></st>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36908">在</st> *<st c="36912">公式</st> <st c="36916">14</st>*<st c="36918">中，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/2859.png)
    <st c="36920"><st c="36924">是模型的最大似然</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)<st
    c="36963"><st c="36964">。AIC试图衡量模型所丧失的信息</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37028"><st c="37029">，当我们将其作为数据生成过程的近似时。</st> <st c="37110">我们还没有定义“信息”是什么，直到</st>
    [*<st c="37181">第13章</st>*](B19496_13.xhtml#_idTextAnchor646)<st c="37191">，我们才开始定义它。基本上，我们可以将信息损失视为模型</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37314"><st c="37315">所产生的总体趋势和模式，与真实的数据生成过程的总体趋势和模式之间的差异。</st></st></st></st></st>
- en: <st c="37412">Obviously, a good candidate model will be close to the true generating
    process and will have a small information loss, leading to a small AIC value.</st>
    <st c="37562">We can select an optimal model class from a set of candidate model
    classes by choosing the one that has the</st> <st c="37670">smallest AIC.</st>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37412">显然，一个好的候选模型将接近真实的生成过程，并且具有较小的信息损失，从而导致较小的AIC值。</st> <st c="37562">我们可以通过选择具有最小AIC值的候选模型类，从一组候选模型类中选择最优模型。</st>
- en: <st c="37683">Since the information loss is not a comparison of the model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37744"><st c="37745">to the training data, but a comparison of the model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37798"><st c="37799">to the true data-generating process, minimizing the
    AIC metric automatically guards against overfitting.</st> <st c="37905">But wait,
    I hear you say – doesn’t the AIC include the log-likelihood that depends on the
    training data?</st> <st c="38010">Yes, that is true.</st> <st c="38029">However,
    to compute the information loss, we’d have to know the true generating process.</st>
    <st c="38118">Instead, we said that the AIC “attempts” to measure the information
    loss.</st> <st c="38192">The AIC approximates the information loss, but it still
    retains many of the desirable properties of the information loss, so we can still
    use it for model selection.</st> <st c="38358">Let’s look at the AIC formula in</st>
    *<st c="38391">Eq.</st> <st c="38395">14</st>* <st c="38397">to see how this model
    selection</st> <st c="38430">is done.</st></st></st>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37683">由于信息损失不是模型与训练数据的比较，而是模型与真实数据生成过程的比较，最小化AIC指标会自动防止过拟合。</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37744"><st c="37745">但是等一下，我听到你说——AIC不包括依赖于训练数据的对数似然吗？</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37798"><st c="37799">是的，这是真的。</st> <st c="37905">然而，为了计算信息损失，我们必须知道真实的生成过程。</st>
    <st c="38010">但是，我们说AIC“试图”衡量信息损失。</st> <st c="38029">AIC近似信息损失，但它仍然保留了信息损失的许多理想属性，因此我们仍然可以用它进行模型选择。</st>
    <st c="38118">我们来看一下AIC公式，见</st> *<st c="38391">公式</st> <st c="38395">14</st>*
    <st c="38397">，以了解如何进行模型选择。</st>
- en: <st c="38438">The AIC consists of two contributions,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2865.png)
    <st c="38478"><st c="38479">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2866.png)<st
    c="38484"><st c="38491">. As we increase model complexity by increasing</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="38539"><st c="38540">, we obviously increase the first of these contributions.</st>
    <st c="38598">However, a higher complexity model will be able to fit the training
    data more closely, so will have a smaller value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2868.png)<st
    c="38717"><st c="38718">. We can see that the two contributions to the AIC work
    in competition against each other.</st> <st c="38809">Minimizing the AIC will
    find the optimal trade-off between the two contributions – that is, the optimal
    balance between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="38929"><st c="38930">(the model complexity) and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2870.png)
    <st c="38958"><st c="38959">(the measure of the</st> <st c="38980">model fit).</st></st></st></st></st></st></st>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38438">AIC 包含两个贡献，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2865.png)
    <st c="38478"><st c="38479">和</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2866.png)<st
    c="38484"><st c="38491">。随着模型复杂度的增加，我们增加</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="38539"><st c="38540">，显然会增加第一个贡献。</st> <st c="38598">然而，更高复杂度的模型能够更紧密地拟合训练数据，因此会有更小的</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2868.png)<st
    c="38717"><st c="38718">。我们可以看到，AIC 的两个贡献是相互竞争的。</st> <st c="38809">最小化 AIC 将找到这两者之间的最佳权衡——即在</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="38929"><st c="38930">（模型复杂度）和</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2870.png)
    <st c="38958"><st c="38959">（模型拟合度）的最佳平衡之间。</st></st></st></st></st></st></st>
- en: <st c="38991">Weaknesses of the AIC</st>
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="38991">AIC 的弱点</st>
- en: <st c="39013">The previous sentence makes it</st> <st c="39044">sound like the
    AIC is ideal for what we want to do – find the sweet spot of model complexity.</st>
    <st c="39139">Not quite.</st> <st c="39150">There are a few issues we</st> <st
    c="39176">should highlight:</st>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39013">前一句话让人感觉 AIC 很适合我们想做的事情——找到模型复杂度的最佳平衡点。</st> <st c="39044">但其实并非如此。</st>
    <st c="39139">原因如下。</st> <st c="39150">有几个问题我们</st> <st c="39176">需要指出：</st>
- en: <st c="39193">Firstly, the AIC formula in</st> *<st c="39222">Eq.</st> <st c="39226">14</st>*
    <st c="39228">is an asymptotic (i.e., a large sample) result.</st> <st c="39277">We
    said that the AIC approximates the information loss.</st> <st c="39333">That approximation
    is increasingly accurate as,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="39381"><st c="39382">, the size of the training dataset increases.</st> <st
    c="39428">It is most accurate when</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/629.png)
    <st c="39453"><st c="39454">is large.</st> <st c="39465">The consequence of this
    is that for small training datasets, the AIC won’t accurately approximate the
    information loss, and a model selection based on</st> *<st c="39616">Eq.</st>
    <st c="39620">14</st>* <st c="39622">may perform poorly.</st> <st c="39643">There
    is a modified or corrected form of the AIC, usually denoted as AICc, that attempts
    to correct this deficiency.</st> <st c="39760">We won’t explain the AICc here,
    other than to say that you can use it for model selection in the same way that
    you use the AIC – you select the model with the smallest value</st> <st c="39934">of
    AICc.</st></st></st>
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39193">首先，AIC 公式在</st> *<st c="39222">Eq.</st> <st c="39226">14</st>*
    <st c="39228">中是一个渐近（即大样本）结果。</st> <st c="39277">我们说过，AIC 近似信息损失。</st> <st c="39333">这种近似随着</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="39381"><st c="39382">，训练数据集的大小增加而变得越来越准确。</st> <st c="39428">当</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/629.png)
    <st c="39453"><st c="39454">很大时，它最为准确。</st> <st c="39465">因此，训练数据集较小的时候，AIC 不能准确近似信息损失，基于</st>
    *<st c="39616">Eq.</st> <st c="39620">14</st>* <st c="39622">的模型选择可能表现不佳。</st>
    <st c="39643">有一种修改版或修正后的 AIC，通常记作 AICc，旨在修正这一不足。</st> <st c="39760">我们在这里不解释
    AICc，除了说明你可以像使用 AIC 一样使用它进行模型选择——你选择具有最小值</st> <st c="39934">的 AICc。</st></st></st>
- en: <st c="39942">Secondly, the use of the AIC follows a different philosophy to
    how we use the generalization error for model selection.</st> <st c="40063">You’re
    probably already familiar with using the MSE on a validation set to perform hyper-parameter
    optimization for a machine learning model.</st> <st c="40205">Also, you’ve no
    doubt run cross-validation calculations before.</st> <st c="40269">In a machine
    learning context, we are directly trying to optimize, albeit in an empirical fashion,
    the thing we care about – the future predictive accuracy of our model.</st> <st
    c="40439">In contrast, when we minimize the AIC, we are optimizing a proxy measure
    that we believe should be correlated with good</st> <st c="40559">predictive accuracy.</st>
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39942">其次，AIC 的使用遵循了与我们使用泛化误差进行模型选择的哲学不同的思路。</st> <st c="40063">你可能已经熟悉了使用验证集上的
    MSE 来进行机器学习模型的超参数优化。</st> <st c="40205">另外，你无疑已经进行过交叉验证计算。</st> <st c="40269">在机器学习的背景下，我们直接试图优化我们关心的事物——虽然是通过经验方式——那就是我们模型的未来预测准确性。</st>
    <st c="40439">相比之下，当我们最小化 AIC 时，我们是在优化一个代理度量，我们认为它应该与良好的</st> <st c="40559">预测准确性相关。</st>
- en: <st c="40579">The preceding second point is not unique to the AIC.</st> <st
    c="40633">It is also a criticism that we can make of the BIC, which we will</st>
    <st c="40699">explain next.</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40579">前面提到的第二点并非 AIC 独有。</st> <st c="40633">这也是我们可以对 BIC 提出的批评，接下来我们会</st>
    <st c="40699">解释这一点。</st>
- en: <st c="40712">Why should we use the AIC, then, if it appears to be not as good
    as the generalization error as a means of selecting the optimal model?</st> <st
    c="40849">The AIC is still a</st> <st c="40868">useful metric for</st> <st c="40886">many
    reasons:</st>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40712">那么，为什么我们要使用 AIC 呢，既然它似乎不如泛化误差作为选择最佳模型的手段？</st> <st c="40849">尽管如此，AIC
    仍然是一个</st> <st c="40868">有用的指标，原因有很多：</st>
- en: <st c="40899">Calculating the MSE on a validation set requires us to have sufficient
    data to divide into training, validation, and test splits.</st> <st c="41030">Similarly,
    calculating cross-validation measures can be</st> <st c="41086">computationally
    expensive.</st>
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="40899">在验证集上计算 MSE 要求我们有足够的数据，能够划分成训练集、验证集和测试集。</st> <st c="41030">类似地，计算交叉验证指标可能是</st>
    <st c="41086">计算量较大的。</st>
- en: <st c="41112">In contrast, the AIC is quick to calculate.</st> <st c="41157">If
    we have fitted our model using maximum likelihood, then we have the AIC essentially
    with minimal</st> <st c="41257">extra computation.</st>
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="41112">相比之下，AIC的计算速度较快。</st> <st c="41157">如果我们已经使用最大似然估计拟合了模型，那么我们几乎可以以最小的</st>
    <st c="41257">额外计算量得到AIC。</st>
- en: <st c="41275">The theoretical underpinnings of the AIC are easier to understand.</st>
    <st c="41343">Minimizing the generalization error or doing a cross-validation
    analysis have an intuitive empirical justification to them, but a detailed theoretical
    analysis of the performance of model selection/hyper-parameter optimization based
    on validation/cross-validation can be harder to do, so we may have a less-detailed
    understanding of the weaknesses of</st> <st c="41694">such approaches.</st>
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="41275">AIC的理论基础较容易理解。</st> <st c="41343">最小化泛化误差或进行交叉验证分析具有直观的经验依据，但基于验证/交叉验证的模型选择/超参数优化的性能详细理论分析可能较难进行，因此我们可能对</st>
    <st c="41694">这些方法的局限性了解不够详细。</st>
- en: <st c="41710">Personally, I like to use the AIC in addition to other model selection
    metrics.</st> <st c="41791">The ease of calculation of the AIC is its biggest
    selling point, but you must always be aware that, first, it is an approximation,
    and second, it is measures information loss, not</st> <st c="41971">predictive
    accuracy.</st>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41710">就个人而言，我喜欢在使用AIC的同时，结合其他模型选择指标。</st> <st c="41791">AIC的计算简便性是其最大卖点，但你必须始终意识到，首先，它是一种近似值，其次，它衡量的是信息丧失，而非</st>
    <st c="41971">预测准确度。</st>
- en: <st c="41991">Bayesian Information Criterion</st>
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="41991">贝叶斯信息准则</st>
- en: <st c="42022">The</st> <st c="42026">B</st><st c="42028">IC is</st> <st c="42034">defined
    as:</st>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42022">B</st><st c="42026">IC定义为：</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mtext>=</mtext><mi>p</mi><mi>log</mi><mi>N</mi><mo>−</mo><mn>2</mn><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2873.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mtext>=</mtext><mi>p</mi><mi>log</mi><mi>N</mi><mo>−</mo><mn>2</mn><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2873.png)'
- en: <st c="42070">Eq.</st> <st c="42074">15</st>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42070">方程式</st> <st c="42074">15</st>
- en: <st c="42076">where again</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/2874.png)
    <st c="42089"><st c="42093">is the maximum likelihood for our model,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1681.png)<st
    c="42134"><st c="42135">. Like the AIC, the BIC is extremely easy to calculate,
    particularly if we have already fitted our model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="42240"><st c="42241">via maximum</st> <st c="42254">likelihood estimation.</st></st></st></st>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42076">其中，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:rrow></mml:msub></mml:math>](img/2874.png)
    <st c="42089"><st c="42093">是我们模型的最大似然估计，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1681.png)<st
    c="42134"><st c="42135">. 与AIC类似，BIC的计算非常简单，尤其是在我们已经通过最大似然估计拟合了模型的情况下</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="42240"><st c="42241">进行最大似然估计时。</st></st></st></st>
- en: <st c="42276">The formula in</st> *<st c="42292">Eq.</st> <st c="42296">15</st>*
    <st c="42298">hides the idea behind the BIC.</st> <st c="42330">As you might guess,
    the BIC is based upon ideas from Bayesian analysis of models.</st> <st c="42412">The
    BIC approximates the</st> *<st c="42437">Bayesian evidence</st>* <st c="42454">of
    the model class.</st> <st c="42475">Given a dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2134.png)<st
    c="42492"><st c="42493">, the Bayesian evidence for a model class,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/332.png)<st
    c="42536"><st c="42537">, is</st> <st c="42542">defined as:</st></st></st>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42276">公式</st> *<st c="42292">Eq.</st> <st c="42296">15</st>* <st c="42298">隐藏了
    BIC 背后的思想。</st> <st c="42330">正如你可能猜到的，BIC 基于贝叶斯模型分析的思想。</st> <st c="42412">BIC
    近似于</st> *<st c="42437">贝叶斯证据</st>* <st c="42454">对于模型类的评估。</st> <st c="42475">给定一个数据集，</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2134.png)<st
    c="42492"><st c="42493">，模型类的贝叶斯证据</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/332.png)<st
    c="42536"><st c="42537">，定义为：</st></st></st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Bayesian</mtext><mtext>evidence</mtext><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>M</mi></mrow></mfenced><mo>=</mo><mo>∫</mo><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>,</mo><mi>M</mi><mo>)</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow><mi>d</mi><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>M</mi></msub></mrow></mrow></math>](img/2879.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>贝叶斯</mtext><mtext>证据</mtext><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>M</mi></mrow></mfenced><mo>=</mo><mo>∫</mo><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>,</mo><mi>M</mi><mo>)</mo><mi>P</mi><mfenced
    open=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow><mi>d</mi><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>M</mi></msub></mrow></mrow></math>](img/2879.png)'
- en: <st c="42607">Eq.</st> <st c="42611">16</st>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42607">Eq.</st> <st c="42611">16</st>
- en: <st c="42613">In</st> *<st c="42617">Eq.</st> <st c="42621">16</st>*<st c="42623">,
    we have used</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/2880.png)
    <st c="42638"><st c="42641">to denote the model parameters for a model in the
    class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42697"><st c="42698">, and the symbol</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow></math>](img/2882.png)
    <st c="42715"><st c="42724">represents the prior distribution on</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/2883.png)
    <st c="42761"><st c="42764">given the model class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42786"><st c="42787">. For our polynomial model class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math>](img/2885.png)
    <st c="42820"><st c="42821">this would be the Bayesian prior we put on the 13
    coefficients of the</st> <st c="42892">12</st><st c="42894">th</st><st c="42897">-degree
    polynomial.</st></st></st></st></st></st></st>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42613">在</st> *<st c="42617">式</st> <st c="42621">16</st>*<st c="42623">中，我们使用了</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/2880.png)
    <st c="42638"><st c="42641">来表示模型类中的模型参数</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42697"><st c="42698">，符号</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow></math>](img/2882.png)
    <st c="42715"><st c="42724">表示在给定模型类</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42786"><st c="42787">的条件下，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math>](img/2885.png)
    <st c="42820"><st c="42821">这是我们为</st> <st c="42892">12</st><st c="42894">次</st><st
    c="42897">方程多项式的13个系数设置的贝叶斯先验。</st></st></st></st></st></st></st>
- en: <st c="42917">In general, the larger the Bayesian evidence of a model class,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/556.png)<st
    c="42981"><st c="42982">, the more evidence the data,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2887.png)<st
    c="43012"><st c="43013">, provides that the model class is the one that generated
    the data.</st> <st c="43081">However, as with the AIC, we run into a technical
    issue here.</st> <st c="43143">Calculating</st> *<st c="43155">Eq.</st> <st c="43159">16</st>*
    <st c="43161">is not generally easy.</st> <st c="43185">However, when the size
    of the training dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="43233"><st c="43234">, is large, we can come up with a general approximation.</st>
    <st c="43291">That approximation is</st> <st c="43313">the BIC.</st> <st c="43322">In
    fact:</st></st></st></st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42917">一般而言，模型类别的贝叶斯证据越大，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/556.png)<st
    c="42981"><st c="42982">，数据提供的证据越多，</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2887.png)<st
    c="43012"><st c="43013">，表明该模型类别就是生成数据的模型。</st> <st c="43081">然而，像AIC一样，我们在这里遇到了一个技术问题。</st>
    <st c="43143">计算</st> *公式* <st c="43155">16</st> <st c="43161">通常不容易。</st> <st
    c="43185">然而，当训练数据集的大小</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="43233"><st c="43234">较大时，我们可以得出一个大致的近似值。</st> <st c="43291">这个近似值就是</st> <st
    c="43313">BIC。</st> <st c="43322">事实上：</st>
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mo>≈</mo><mo>−</mo><mn>2</mn><mi>log</mi><mfenced
    open="(" close=")"><mrow><mtext>Bayesian</mtext><mtext>Evidence</mtext></mrow></mfenced><mtext>as</mtext><mi>N</mi><mo>→</mo><mi
    mathvariant="normal">∞</mi></mrow></mrow></math>](img/2889.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mo>≈</mo><mo>−</mo><mn>2</mn><mi>log</mi><mfenced
    open="(" close=")"><mrow><mtext>Bayesian</mtext><mtext>Evidence</mtext></mrow></mfenced><mtext>as</mtext><mi>N</mi><mo>→</mo><mi
    mathvariant="normal">∞</mi></mrow></mrow></math>](img/2889.png)'
- en: <st c="43370">Eq.</st> <st c="43374">17</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*公式* <st c="43370">17</st>'
- en: '*<st c="43376">Eq.</st> <st c="43381">17</st>* <st c="43383">indicates that
    maximizing the Bayesian evidence is the same as minimizing the BIC.</st> <st c="43467">Consequently,
    we perform model selection by minimizing</st> <st c="43522">the BIC.</st>'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*公式* <st c="43376">17</st> *指出最大化贝叶斯证据等同于最小化BIC。* <st c="43467">因此，我们通过最小化BIC来进行模型选择。</st>'
- en: <st c="43530">As with the AIC, there are</st> <st c="43557">several comments
    and observations we can make about</st> <st c="43610">the BIC:</st>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与AIC类似，我们可以对BIC做出几个评论和观察：
- en: <st c="43618">Like the AIC, the BIC is easy</st> <st c="43649">to calculate.</st>
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="43618">像AIC一样，BIC很容易</st> <st c="43649">计算。</st>
- en: <st c="43662">Again, like the AIC, the BIC is based on a large sample size approximation,
    so at smaller sample sizes, it may not select the true optimal model class.</st>
    <st c="43815">For smaller sample sizes, it would be better to compute the Bayesian
    evidence exactly via</st> *<st c="43905">Eq.</st> <st c="43909">16</st>*<st c="43911">,
    although this can require considerable mathematical skill or using advanced computationally
    intensive Monte</st> <st c="44021">Carlo techniques.</st>
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="43662">同样，像AIC一样，BIC基于大样本量的近似，因此在小样本量下，它可能无法选择正确的最优模型类别。</st> <st c="43815">对于较小的样本量，最好通过*公式*
    <st c="43905">16</st> <st c="43911">精确计算贝叶斯证据，尽管这可能需要相当的数学技能或使用计算密集型的蒙特卡洛技术。</st>
- en: <st c="44038">The formula for the BIC looks very similar to that for the AIC.</st>
    <st c="44103">The difference between the BIC and AIC is just in the penalty terms;</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2890.png)
    <st c="44172"><st c="44173">for the AIC compared to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math>](img/2891.png)
    <st c="44198"><st c="44204">for the BIC.</st> <st c="44217">So, as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="44224"><st c="44225">increases, the penalty we pay for increasing the model
    complexity becomes higher in the BIC than in the AIC, meaning the BIC tends to
    be a more stringent selection criterion than</st> <st c="44405">the AIC.</st></st></st></st>
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44038">BIC 的公式与 AIC 的公式非常相似。</st> <st c="44103">BIC 和 AIC 的区别仅在于惩罚项；</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2890.png)
    <st c="44172"><st c="44173">对于 AIC，相比于</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math>](img/2891.png)
    <st c="44198"><st c="44204">对于 BIC。</st> <st c="44217">因此，当</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="44224"><st c="44225">增加时，BIC 相较于 AIC 在增加模型复杂度时支付的惩罚更高，这意味着 BIC 倾向于成为比 AIC
    更严格的选择标准。</st></st></st></st>
- en: <st c="44413">Having discussed the AIC and BIC at length, it is time to wrap
    up this section.</st> <st c="44494">Let’s first summarize what we have learned
    in this section about practical model complexity measures, and then we’ll summarize
    the</st> <st c="44625">chapter overall.</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44413">在详细讨论了 AIC 和 BIC 后，是时候总结这一部分内容了。</st> <st c="44494">我们首先总结一下在这一部分中关于实际模型复杂度度量的学习内容，然后再总结一下整个章节的内容。</st>
- en: <st c="44641">What we learned</st>
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="44641">我们学到的内容</st>
- en: <st c="44657">In this section, we learned</st> <st c="44686">the following:</st>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44657">在这一部分，我们学习了</st> <st c="44686">以下内容：</st>
- en: <st c="44700">The concept of</st> <st c="44716">model selection</st>
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44700">模型选择的概念</st> <st c="44716">模型选择</st>
- en: <st c="44731">The AIC and how we can use it to perform model selection by selecting
    the model class with the lowest</st> <st c="44834">AIC value</st>
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44731">AIC 及其如何通过选择具有最低</st> <st c="44834">AIC 值的模型类来进行模型选择</st>
- en: <st c="44843">The BIC and how we can use it to perform model selection by selecting
    the model class with the lowest</st> <st c="44946">BIC value</st>
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44843">BIC 及其如何通过选择具有最低</st> <st c="44946">BIC 值的模型类来进行模型选择</st>
- en: <st c="44955">The strengths and weaknesses of the AIC</st> <st c="44996">and
    BIC</st>
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="44955">AIC 和 BIC 的优缺点</st> <st c="44996">及 BIC</st>
- en: <st c="45003">Summary</st>
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="45003">总结</st>
- en: <st c="45011">This chapter has been a shorter one and largely a visual one.</st>
    <st c="45074">The only heavy math came in the proof of the bias-variance decomposition
    of the generalization error.</st> <st c="45176">However, the visual approach has
    been useful in explaining concepts of overfitting, underfitting, and generalization.</st>
    <st c="45294">At a superficial level, these concepts are intuitive and need very
    little explanation.</st> <st c="45381">You will have probably encountered them
    before.</st> <st c="45429">However, a more thorough understanding of these concepts
    is crucial if we’re not to be misled by them when we’re building predictive models.</st>
    <st c="45570">That thorough understanding has required us to learn additional
    concepts.</st> <st c="45644">Across the whole chapter, the concepts we have learned
    about included</st> <st c="45714">the following:</st>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45011">这一章比较简短，主要是以视觉化的方式呈现内容。</st> <st c="45074">唯一的复杂数学内容出现在对泛化误差的偏差-方差分解的证明中。</st>
    <st c="45176">然而，视觉化的方法在解释过拟合、欠拟合和泛化的概念时非常有帮助。</st> <st c="45294">从表面上看，这些概念是直观的，几乎不需要过多解释。</st>
    <st c="45381">你可能已经接触过它们了。</st> <st c="45429">然而，要深入理解这些概念至关重要，这样我们在构建预测模型时才不会被误导。</st>
    <st c="45570">这种深入理解需要我们学习额外的概念。</st> <st c="45644">在整个章节中，我们学习的概念包括</st> <st
    c="45714">以下内容：</st>
- en: <st c="45728">Model complexity and how we broadly think of this as being related
    to the number of parameters in</st> <st c="45827">a model</st>
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="45728">模型复杂度及我们如何通常认为它与</st> <st c="45827">模型中的参数数量相关</st>
- en: <st c="45834">Overfitting to the noise in a dataset and how it increases as
    we increase the complexity of</st> <st c="45927">a model</st>
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="45834">对数据集中噪声的过拟合，以及当我们增加模型复杂度时，它如何增加</st> <st c="45927">的现象</st>
- en: <st c="45934">Underfitting to the general trends in a dataset and how it increases
    as we decrease the complexity of</st> <st c="46037">a model</st>
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="45934">对数据集中一般趋势的欠拟合，以及当我们降低模型复杂度时，它如何增加</st> <st c="46037">的现象</st>
- en: <st c="46044">Generalization and how a model that generalizes well is the one
    that makes accurate predictions on</st> <st c="46144">unseen data</st>
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46044">泛化能力及一个能够很好泛化的模型是能够在</st> <st c="46144">未见数据</st> <st c="46911">上做出准确预测的模型</st>
- en: <st c="46155">The bias-variance trade-off and how it makes mathematically precise
    the ideas behind how the minimum in the generalization</st> <st c="46279">error
    arises</st>
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46155">偏差-方差权衡及其如何使得泛化误差中的最小值产生的数学原理更加精确</st> <st c="46279">的理论</st>
- en: <st c="46291">A classical picture of how model complexity affects the</st> <st
    c="46348">generalization error</st>
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46291">模型复杂度如何影响</st> <st c="46348">泛化误差的经典图像</st>
- en: <st c="46368">That over-parameterized machine learning models such as neural
    networks have revealed the phenomenon of</st> <st c="46473">double descent</st>
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46368">那些过度参数化的机器学习模型，如神经网络，揭示了</st> <st c="46473">双重下降的现象</st>
- en: <st c="46487">Model selection and how we use model complexity measures to select
    between different</st> <st c="46573">model classes</st>
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46487">模型选择及如何利用模型复杂度衡量标准在不同的</st> <st c="46573">模型类别之间进行选择</st>
- en: <st c="46586">The</st> **<st c="46591">Akaike Information Criterion</st>** <st
    c="46619">(</st>**<st c="46621">AIC</st>**<st c="46624">) as a model</st> <st
    c="46638">selection measure</st>
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46586">**<st c="46591">赤池信息准则</st>** <st c="46619">(</st>**<st c="46621">AIC</st>**<st
    c="46624">) 作为一个模型</st> <st c="46638">选择标准</st>
- en: <st c="46655">The</st> **<st c="46660">Bayesian Information Criterion</st>**
    <st c="46690">(</st>**<st c="46692">BIC</st>**<st c="46695">) as a model</st>
    <st c="46709">selection measure</st>
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="46655">**<st c="46660">贝叶斯信息准则</st>** <st c="46690">(</st>**<st c="46692">BIC</st>**<st
    c="46695">) 作为一个模型</st> <st c="46709">选择标准</st>
- en: <st c="46726">Our next chapter is another self-contained topic, function decomposition.</st>
    <st c="46801">In that chapter, we will learn mathematical tools and tricks to
    build up</st> <st c="46874">a function.</st>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46726">我们的下一章是另一个独立的主题，函数分解。</st> <st c="46801">在这一章中，我们将学习构建</st> <st
    c="46874">函数的数学工具和技巧。</st>
- en: <st c="46885">Notes and further reading</st>
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="46885">注释与进一步阅读</st>
- en: '<st c="46911">The NeurIPS 2023 conference paper on double descent we referred
    to in this chapter is</st> *<st c="46998">A U-turn on Double Descent: Rethinking
    Parameter Counting in Statistical Learning</st>*<st c="47079">, by A.</st> <st
    c="47087">Curth, A.</st> <st c="47097">Jeffares, and M.</st> <st c="47114">van
    der Schaar.</st> <st c="47130">A preprint version of the paper can be found on
    the arXiv archive</st> <st c="47196">at</st> [<st c="47199">https://arxiv.org/pdf/2310.18988.pdf</st>](https://arxiv.org/pdf/2310.18988.pdf)<st
    c="47235">.</st>'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="46911">我们在本章中提到的关于双重下降的NeurIPS 2023会议论文是</st> *<st c="46998">《双重下降的转折：重新思考统计学习中参数计数》</st>*<st
    c="47079">，作者为A.</st> <st c="47087">Curth, A.</st> <st c="47097">Jeffares, 和 M.</st>
    <st c="47114">van der Schaar。</st> <st c="47130">该论文的预印本可以在arXiv上找到，链接为</st> [<st
    c="47199">https://arxiv.org/pdf/2310.18988.pdf</st>](https://arxiv.org/pdf/2310.18988.pdf)<st
    c="47235">。</st>
