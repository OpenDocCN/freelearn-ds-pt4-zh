- en: <st c="0">8</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Model Complexity</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="18">Model complexity may seem like a strange title for a chapter.</st>
    <st c="81">Why should we care about complexity?</st> <st c="118">One concept you
    may have already encountered as a data scientist is that of overfitting and how
    an overfitted model will not make accurate predictions.</st> <st c="270">However,
    that overfitting stems from using a model whose complexity is greater than that
    justified by the data.</st> <st c="382">The impact of model complexity on model
    prediction accuracy is a nuanced one.</st> <st c="460">More specifically, how
    you decide what is the right level of model complexity can be challenging.</st>
    <st c="558">To address this challenge requires exploring several new concepts.</st>
    <st c="625">We will do that exploration in this chapter and do so by covering
    the</st> <st c="695">following topics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="712">Generalization, overfitting, and the role of model complexity</st>*<st
    c="774">: Here, we understand how model complexity affects the accuracy of model
    predictions on</st> <st c="863">unseen data</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="874">The bias-variance trade-off</st>*<st c="902">: Here, we dig into
    the mathematical details behind the prediction accuracy ideas we introduced in
    the</st> <st c="1006">preceding section</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1023">Model complexity measures for model selection</st>*<st c="1069">:
    Here, we introduce commonly used model complexity measures and discuss their strengths</st>
    <st c="1159">and weaknesses</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1173">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1196">This chapter will mainly be a visual one.</st> <st c="1239">We
    will introduce the mathematical ideas and concepts through illustrative plots
    and schematic figures, which we will then unpack and explain at length.</st> <st
    c="1392">Because of this, there are no code examples in this chapter, so no</st>
    <st c="1459">technical requirements.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1482">Generalization, overfitting, and the role of model complexity</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1544">What do we mean by a complex model?</st> <st c="1581">Very loosely,
    we think of a more complex model as</st> <st c="1631">having more parameters or
    using more features.</st> <st c="1678">This statement is imprecise, but the idea
    that model complexity broadly follows the number of model parameters/features
    will be precise enough for the mainly qualitative discussions of</st> <st c="1863">this
    chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1876">A more complex model can fit a training dataset more closely, as
    it can use the extra features to explain the variation in the response variable/target
    variable.</st> <st c="2039">What are the consequences of this increased flexibility?</st>
    <st c="2096">As a simple example, we’ll take a look at</st> *<st c="2138">Figure
    8</st>**<st c="2146">.1</st>*<st c="2148">, which shows three different models
    fitted to a small dataset.</st> <st c="2212">The black circles in each plot show
    the training data, while the blue circles show the hold-out sample data points,
    which, as you can see, represent an extrapolation challenge, since the hold-out
    data points are all to the right of the training data points.</st> <st c="2470">Although
    using machine learning models in interpolation settings is more common, we’ve
    used an extrapolation example here because it is easier to immediately see how
    well each fitted model predicts the holdout data, and so it is a good experiment
    to start to introduce some ideas about</st> <st c="2755">model complexity.</st>
    <st c="2774">All the data points (training and hold-out) were generated with a
    quadratic equation (a 2</st><st c="2863">nd</st><st c="2866">-degree polynomial)
    in a single variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="2909"><st c="2910">. A significant amount of additive noise was also added
    to the output of the</st> <st c="2987">generated data.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="3111">Figure 8.1: Different polynomials fitted to a quadratic dataset</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3174">The left-hand plot shows a</st> <st c="3201">linear equation in</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/2761.png)
    <st c="3221"><st c="3222">fitted to the training data.</st> <st c="3252">The line
    shows the predictions from the fitted model across the full range of the feature</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="3342"><st c="3343">. The linear model is obviously incorrect.</st> <st c="3386">It
    contains fewer parameters than the model that generated the data.</st> <st c="3455">It
    clearly doesn’t fit the training data well, since it doesn’t capture the curvature
    present in the training data, and there are marked differences between the line
    (predictions) and the dark circles of the training set.</st> <st c="3677">The
    linear model is also very poor at predicting the hold-out data points.</st> <st
    c="3752">The difference between the line and the lighter circles of the hold-out
    data points is stark – the trend in the predictions is in a different direction
    to that of the</st> <st c="3919">hold-out data.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3933">The middle plot of</st> *<st c="3953">Figure 8</st>**<st c="3961">.1</st>*
    <st c="3963">shows the predictions (line) from a quadratic model fitted to the
    training data.</st> <st c="4045">The model fits the training data well and makes
    good predictions on the hold-out set.</st> <st c="4131">Any differences between
    the line and the black or light circles are evenly scattered and look like the
    result of the significant additive noise present in the data.</st> <st c="4296">It
    is perhaps not surprising that the quadratic model does well.</st> <st c="4361">It
    is of precisely the same form (a 2</st><st c="4398">nd</st><st c="4401">-degree
    polynomial) as the source of the training and</st> <st c="4456">hold-out data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4470">Or does the quadratic model do better than the linear model simply
    because it has more parameters and so its shape can “wiggle” more and, therefore,
    capture more of the variations present in the training data?</st> <st c="4681">To
    test this idea, let’s increase the number of parameters in our model even further.</st>
    <st c="4767">The right-hand plot of</st> *<st c="4790">Figure 8</st>**<st c="4798">.1</st>*
    <st c="4800">shows the predictions (red line) from a quartic (a 4</st><st c="4853">th</st><st
    c="4856">-degree polynomial) model.</st> <st c="4884">This model has two more
    parameters than the quadratic model in the middle plot and three more parameters
    than the linear model in the left-hand plot.</st> <st c="5034">The quartic model
    fits the training data as well as the quadratic model but predicts poorly on the
    hold-out data.</st> <st c="5148">The trend of the predictions diverges from the
    trend in the hold-out data.</st> <st c="5223">The red line for the quartic model
    is even beginning to move downwards at the right-hand edge of the plot.</st> <st
    c="5330">If we made predictions for hold-out data at even higher values of the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="5400"><st c="5401">variable, the quartic</st> <st c="5424">model would
    move in completely the opposite direction to</st> <st c="5481">the data.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5490">Let’s summarize our findings from that</st> <st c="5530">little
    experiment:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="5548">Linear model</st>**<st c="5561">: One</st> <st c="5567">parameter.</st>
    <st c="5579">Fits the training data poorly.</st> <st c="5610">Predicts poorly.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="5626">Quadratic model</st>**<st c="5642">: Two</st> <st c="5648">parameters.</st>
    <st c="5661">Fit the training data well.</st> <st c="5689">Predict well.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="5702">Quartic model</st>**<st c="5716">: Four</st> <st c="5724">parameters.</st>
    <st c="5736">Fit the training data well.</st> <st c="5764">Predict poorly.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5779">From this simple example, we can see that there appears to be an
    optimal level of model complexity – a model that is neither too complex nor</st>
    <st c="5921">too simple.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5932">Overfitting</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5944">Now, you may wonder whether the</st> <st c="5976">poor performance
    of the quartic model was due to the hold-out data essentially being a test of
    the extrapolation abilities of the fitted models.</st> <st c="6122">Again, we
    can test this idea with a simple experiment.</st> <st c="6177">In</st> *<st c="6180">Figure
    8</st>**<st c="6188">.2</st>*<st c="6190">, we have plotted the predictions (red
    line) from a 12</st><st c="6244">th</st><st c="6247">-degree polynomial, fitted
    to all the data that was present in our previous experiment in</st> *<st c="6338">Figure
    8</st>**<st c="6346">.1</st>*<st c="6348">. In this case, our model has lots of
    parameters (13 in total) and has been fitted to all the data (training and hold-out)
    that we had in</st> *<st c="6486">Figure 8</st>**<st c="6494">.</st><st c="6495">1</st>*<st
    c="6497">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="6541">Figure 8.2: A plot of predictions (the red line) from a 12th-degree
    polynomial, fitted to the same data in the previous figure</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6667">The dashed light-blue line is the true equation that was used to
    generate the data.</st> <st c="6752">The scatter of the individual data points
    (the black dots) around the dashed line is just the effect of the additive noise
    we included when generating the data.</st> <st c="6913">The dashed line represents
    the ground truth and is the best we can hope for from any model we fit to the
    data.</st> <st c="7024">A fitted model whose red line coincided with the dashed
    blue line would be considered “exact,” “optimal,”</st> <st c="7130">or “perfect.”</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7143">The red line for the 12</st><st c="7167">th</st><st c="7170">-degree
    polynomial model we have fitted in</st> *<st c="7214">Figure 8</st>**<st c="7222">.2</st>*
    <st c="7224">wiggles around the dashed blue line.</st> <st c="7262">This tells
    us that, first, the fitted model in</st> *<st c="7309">Figure 8</st>**<st c="7317">.2</st>*
    <st c="7319">is not perfect, and second, while the model has broadly fitted the
    quadratic trend of the true equation, it has also fitted to some of the wiggles
    in the data that are due to the noise in it.</st> <st c="7512">In this case, we
    say</st> <st c="7532">that the fitted model has</st> *<st c="7559">overfitted</st>*
    <st c="7569">the data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7579">Why overfitting is bad</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="7602">Is overfitting a</st> <st c="7620">problem?</st> <st c="7629">Yes,
    it is.</st> <st c="7641">Take another look at</st> *<st c="7662">Figure 8</st>**<st
    c="7670">.2</st>*<st c="7672">. What would happen if our fitted model made a prediction
    for a value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="7745"><st c="7746">somewhere in the middle of the training data but where
    we didn’t already have an existing data point?</st> <st c="7849">What would happen
    if we then measured a new data point at</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7907"><st c="7908">? How close would our prediction be to the new measured
    data point?</st> <st c="7976">We wouldn’t expect them to be identical; after all,
    the new data point, like those in the training set, has noise added</st> <st c="8096">to
    it.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8102">The predicted value of the target variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)<st
    c="8147"><st c="8157">, is given by the position on the red line, corresponding
    to where the position</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8237"><st c="8238">is.</st> <st c="8243">Our</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="8247"><st c="8248">value may correspond to where those wiggles in the red
    line are, and so our prediction would follow the pattern present in the noise
    in the training set.</st> <st c="8403">The problem is that for the new data point,
    the noise, which is random and therefore unpredictable, is highly unlikely to
    follow the same wiggles that were present in the noise in the training data.</st>
    <st c="8602">The prediction from our overfitted model is likely to be further
    away from the true value than if we had used a model that didn’t follow the wiggles
    of the noise in the training data.</st> <st c="8786">This means that because our
    model overfitted to the noise in the training set, we will have increased the
    size of the error the model makes on new</st> <st c="8933">unseen data.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8945">Our predictions would have been better on average if our prediction
    model just followed the more general trend present in the training data, as represented
    by the dashed blue line.</st> <st c="9127">Predictions that follow the blue line
    would be correct on average.</st> <st c="9194">Since fitting to the general trends
    present in a training set produces a model that predicts well, we say that a good
    model is one that</st> *<st c="9330">generalizes</st>* <st c="9341">well.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9347">As we increase the</st> <st c="9367">number of parameters in our
    model, we increase its ability to follow the noise present in its training data.</st>
    <st c="9476">A fitted model with a very high number of parameters would pass almost
    exactly through the data.</st> <st c="9573">This is illustrated schematically
    in</st> *<st c="9610">Figure 8</st>**<st c="9618">.3</st>*<st c="9620">, which
    shows a highly flexible model (the line) overfitted to a small number of data
    points (the circles) that broadly follow a</st> <st c="9749">quadratic tr</st><st
    c="9761">end.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="9793">Figure 8.3: A schematic of a highly overfitted model</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9845">Although</st> *<st c="9855">Figure 8</st>**<st c="9863">.3</st>*
    <st c="9865">is schematic and the line does not represent a model actually fitted
    to a dataset, it is still useful to draw out</st> <st c="9980">some conclusions:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9997">The smaller the training dataset is, the easier it is for a complex
    model to overfit.</st> <st c="10084">This highlights that overfitting is dependent
    on both the model complexity and the</st> <st c="10167">training data.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="10181">If we were to use the fitted model in</st> *<st c="10220">Figure
    8</st>**<st c="10228">.3</st>* <st c="10230">to make a prediction for a data point
    it has already seen (i.e., one in the training dataset), the model would make
    a near-perfect prediction.</st> <st c="10374">This is because the high complexity
    of the model has allowed it to effectively “memorize” the training data.</st>
    <st c="10483">No “learning” has occurred.</st> <st c="10511">The</st> <st c="10515">model
    is just retrieving data, rather than identifying and learning</st> <st c="10583">general
    trends.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="10598">Overfitting increases the variability of predictions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10651">Let’s look at another</st> <st c="10674">consequence of overfitting.</st>
    <st c="10702">So far, we have discussed what happens to predictive accuracy when
    we overfit to a single particular dataset.</st> <st c="10812">We could get lucky.</st>
    <st c="10832">It could be that we have a highly complex model but, when fitted
    to our training dataset, we get a model that follows the general trends well and,
    therefore, generalizes well.</st> <st c="11008">What we’ll now illustrate is that
    with increasing model complexity, that is increasingly unlikely to happen.</st>
    <st c="11117">We are unlikely to</st> <st c="11136">be lucky.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11145">Take a look at</st> *<st c="11161">Figure 8</st>**<st c="11169">.4</st>*<st
    c="11171">. It shows (dashed) prediction curves from 12</st><st c="11216">th</st><st
    c="11219">-degree polynomial models, obtained by fitting to different training
    datasets.</st> <st c="11299">The different training datasets were all generated
    by the same underlying “ground-truth” model that was used to generate the data
    in</st> *<st c="11432">Figure 8</st>**<st c="11440">.1</st>* <st c="11442">and</st>
    *<st c="11447">Figure</st> <st c="11453">8</st>**<st c="11455">.2</st>*<st c="11457">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="11525">Figure 8.4: The prediction curves of the 12th-degree polynomial
    models obtained from different training sets</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11633">Since each 12</st><st c="11647">th</st><st c="11650">-degree</st>
    <st c="11659">polynomial model will overfit to the noise present in its corresponding
    training data, each model displays wiggles (as we can see in</st> *<st c="11792">Figure
    8</st>**<st c="11800">.4</st>*<st c="11802">).</st> <st c="11806">However, each
    training dataset is slightly different, so each 12</st><st c="11870">th</st><st
    c="11873">-degree polynomial displays different wiggles.</st> <st c="11921">We
    can see this in the variation of the shapes of the different dashed lines.</st>
    <st c="11999">But the dashed lines show what each model would predict for the
    given value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12078"><st c="12079">on the</st> *<st c="12087">x</st>*<st c="12088">-axis.</st>
    <st c="12095">For a given value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12116"><st c="12117">, there will be significant variation in the prediction
    at</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="12176"><st c="12177">across the different models fitted to the different</st>
    <st c="12230">training datasets.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12248">As we increase the model complexity (e.g., go from fitting 12</st><st
    c="12310">th</st><st c="12313">-degree polynomials to fitting 20</st><st c="12347">th</st><st
    c="12350">-degree polynomials), those fitted models will follow ever more closely
    the wiggles in the noise in their training dataset.</st> <st c="12475">Consequently,
    the size of the wiggles in each fitted model will get bigger, and the resulting
    variability in predictions across training sets</st> <st c="12617">will increase.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12631">This is a problem.</st> <st c="12651">Why so?</st> <st c="12659">What</st>
    *<st c="12664">Figure 8</st>**<st c="12672">.4</st>* <st c="12674">illustrates
    is that at the same value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="12716"><st c="12717">, different training sets give different predictions.</st>
    <st c="12771">There is a random element to our prediction, and the variance of
    that random element increases with increasing model complexity.</st> <st c="12900">However,
    in real life, we will only ever use one training dataset.</st> <st c="12967">Are
    we going to hope that we are lucky and that our particular training data leads
    to predictions that are close to the true value?</st> <st c="13099">A better strategy
    would be to ensure that we have low sensitivity of the predictions to the choice
    of training set, by not using an overly</st> <st c="13238">complex model.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13252">Underfitting is also a problem</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13283">Since overfitting to noise in a</st> <st c="13315">training dataset
    arises because we have too many parameters in our model relative to the amount
    of data/signal in the training data, you might get the impression that we should
    use a model with as few parameters as possible.</st> <st c="13541">This is not
    the case.</st> <st c="13563">It is possible for a model to be too simple, to contain
    too few parameters.</st> *<st c="13639">Figure 8</st>**<st c="13647">.5</st>*
    <st c="13649">shows the same dataset as in</st> *<st c="13679">Figure 8</st>**<st
    c="13687">.2</st>*<st c="13689">, but the fitted model represented by the red
    line in</st> *<st c="13743">Figure 8</st>**<st c="13751">.5</st>* <st c="13753">is
    now just a constant and corresponds to the mean value of the target variable,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="13835"><st c="13858">, in the training data.</st> <st c="13882">Clearly, this
    fitted model does not follow any wiggles in the data that are due to noise.</st>
    <st c="13972">This model is definitely not overfitted.</st> <st c="14013">But,
    equally, the model is not complex enough to follow the general quadratic trend
    in the data indicated by the dashed blue line.</st> <st c="14144">We say that
    this model</st> <st c="14167">is</st> *<st c="14170">under</st><st c="14175">fitted</st>*<st
    c="14182">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="14208">Figure 8.5: A low complexity model consisting of a constant equal
    to the mean of y</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14290">What are the consequences of an underfitted model?</st> <st c="14342">It
    should be qualitatively clear that a low complexity model that underfits a training
    dataset will make poor predictions on both training data points and on any unseen
    data points, as it has not learned the general trends that all data points follow.</st>
    <st c="14594">Prediction errors will be large for both training data and any holdout
    data.</st> <st c="14671">As with an overfitted model, an underfitted model</st>
    <st c="14721">generalizes poorly.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14740">In contrast to overly</st> <st c="14762">complex overfitted models,
    low complexity models do not display much variability in their predictions, at
    a given holdout point</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="14891"><st c="14892">, across different training datasets.</st> *<st c="14930">Figure
    8</st>**<st c="14938">.6</st>* <st c="14940">shows the prediction curves (dashed
    lines) for a model that is only a constant (a 0</st><st c="15024">th</st><st c="15027">-degree
    polynomial) when fitted to the same training datasets used in </st>*<st c="15098">Figure
    8</st>**<st c="15106">.4</st>*<st c="15108">. For comparison, we have also added
    the true model line, which is a quadratic and shown by the light blue</st> <st
    c="15215">dashe</st><st c="15220">d line.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="15274">Figure 8.6: The prediction curves of 0th-degree polynomial models
    obtained from different training sets</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15377">There is some variation in the prediction curves of our 0</st><st
    c="15435">th</st><st c="15438">-degree polynomial, but not much.</st> <st c="15473">In
    fact, the variation is so small that two of the prediction curves lie almost on
    top of each other – there are, in fact, four horizontal dashed lines plotted in</st>
    *<st c="15636">Figure 8</st>**<st c="15644">.6</st>*<st c="15646">. The variation
    is considerably smaller than the variation in prediction curves we got from fitting
    12</st><st c="15748">th</st><st c="15751">-degree polynomials to the same</st>
    <st c="15784">training datasets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15802">The benefit of this is that, in contrast to using an overly complex
    model, it doesn’t really matter which training dataset we use; our predictions
    will come out similar.</st> <st c="15973">There is little random variation in
    predictions between using different training sets of the same size.</st> <st c="16077">The
    downside is that those predictions will be consistently poor.</st> <st c="16143">No
    matter which training dataset we use, predictions from our low complexity model
    are typically a long way from the true value represented by the curved dashed
    light blue line.</st> <st c="16321">Our low-complexity model</st> <st c="16346">displays</st>
    *<st c="16355">bias</st>*<st c="16359">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16360">Clearly, there is a</st> <st c="16381">sweet spot in terms of
    the number of model parameters.</st> <st c="16436">At that sweet spot, the model
    complexity is sufficient to capture the general trends that led to the training
    data, but it is not sufficient to overfit to the noise in the data.</st> <st c="16614">At
    the sweet spot, the fitted model will generalize well.</st> <st c="16672">To identify
    that sweet spot, we need to make things more quantitative and explain how we define
    prediction errors and measure generalization.</st> <st c="16814">We will do</st>
    <st c="16825">that now.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16834">Measuring prediction error</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16861">A data point</st> <st c="16874">is a pair</st> <st c="16884">of
    values</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2775.png)<st
    c="16895"><st c="16896">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/2776.png)
    <st c="16902"><st c="16903">value is the value of the target variable (or response
    variable) we observed when we had the corresponding vector,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="17019"><st c="17020">, for the feature variables.</st> <st c="17049">Remember
    that for the same</st>![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder></mrow></math>](img/2778.png) <st c="17075"><st
    c="17077">value, we could get multiple different values for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17128"><st c="17138">because the observation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="17162"><st c="17172">contains a random component.</st> <st c="17201">A
    dataset is just a set of multiple datapoints.</st> <st c="17249">In general, we
    can denote a dataset</st> <st c="17285">as</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>D</mi><mo>=</mo><mfenced
    open="{" close="}"><mrow><mfenced open="(" close=")"><mrow><msub><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></mfenced><mo>,</mo><mi>i</mi><mo>=</mo><mn>1,2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi></mrow></mfenced></mrow></mrow></math>](img/2781.png)<st
    c="17288"><st c="17289">.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17290">Let’s use</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2782.png)
    <st c="17301"><st c="17314">to denote the model equation of our trained model.</st>
    <st c="17365">The notation</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mrow></mrow></math>](img/2783.png)
    <st c="17378"><st c="17389">is used to denote the fact that the trained model
    will depend on some parameters,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="17471"><st c="17472">, and the training data,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2785.png)<st
    c="17497"><st c="17503">. The difference between the model prediction and the
    observed value of the target variable for the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/909.png)
    <st c="17603"><st c="17615">datapoint is</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub><mo>|</mo><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mo>,</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>](img/2787.png)<st
    c="17628"><st c="17646">, which we will shorten to just</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2788.png)<st
    c="17678"><st c="17679">. To get a measure of the typical error made by our model,
    we calculate</st> <st c="17751">the</st> **<st c="17755">mean squared</st>** **<st
    c="17768">error</st>** <st c="17773">(</st>**<st c="17775">MSE</st>**<st c="17778">):</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>=</mtext><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mfenced
    open="(" close=")"><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><msub><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mi>i</mi></msub></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mrow></math>](img/2789.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="17783">Eq.</st> <st c="17787">1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17788">This is like when we studied least-squares model fitting in</st>
    [*<st c="17848">Chapter 4</st>*](B19496_04.xhtml#_idTextAnchor216)<st c="17857">.
    We take the square of the individual errors to ensure a positive number so that
    positive and negative errors don’t just cancel out to zero.</st> <st c="17999">To
    get back to a “typical” error, we can finally take the square root of the MSE
    to get</st> <st c="18087">the</st> **<st c="18091">root mean squared</st>** **<st
    c="18109">error</st>** <st c="18114">(</st>**<st c="18116">RMSE</st>**<st c="18120">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>RMSE</mtext><mtext>=</mtext><msqrt><mtext>MSE</mtext></msqrt></mrow></mrow></math>](img/2790.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="18125">Eq.</st> <st c="18129">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18130">We can calculate the</st> <st c="18151">MSE for</st> <st c="18159">a
    given dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2791.png)<st
    c="18176"><st c="18177">. This doesn’t necessarily have to be the training dataset.</st>
    <st c="18237">We can evaluate the MSE for the hold-out dataset,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/2792.png)<st
    c="18287"><st c="18291">, if we want to.</st> <st c="18308">Since the accuracy
    of predictions on the hold-out dataset gives us a feel for how well a model generalizes,
    we call</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2793.png)
    <st c="18424"><st c="18440">the generalization error, while we call</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>RMSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2794.png)
    <st c="18480"><st c="18493">the training error.</st> <st c="18513">However, you
    will probably find that MSE and RMSE are used interchangeably – for example,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2795.png)
    <st c="18603"><st c="18619">can be referred to as the generalization error even
    though it is obviously an average squared error.</st> <st c="18720">From a qualitative
    perspective, it is inconsequential, and I also tend to use the term “generalization
    error” when I’m referring</st> <st c="18849">to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2796.png)<st
    c="18852"><st c="18867">.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18868">We can now go back and take our previous qualitative conclusions
    about overfitting and use the MSE to make them more quantitative.</st> <st c="19000">We
    know that as we increase the model complexity by increasing the number of parameters/features
    in our model, we can fit the training data points evermore closely.</st> <st c="19165">This
    means that for a given training dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math>](img/2797.png)<st
    c="19211"><st c="19217">, we expect</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2798.png)
    <st c="19229"><st c="19242">to decrease monotonically as we increase the model
    complexity.</st> <st c="19305">We also know the generalization ability of our
    model initially improves as we increase the number of parameters and then deteriorates.</st>
    <st c="19440">This means we expect</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2799.png)<st
    c="19461">![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mtext>MSE</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/2800.png)
    <st c="19462"><st c="19472">to go through a minimum.</st> <st c="19497">These
    two quantitative statements are summarized schematically i</st><st c="19561">n</st>
    *<st c="19564">Figure 8</st>**<st c="19572">.7</st>*<st c="19574">.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="19660">Figure 8.7: The behavior of training and generalization errors
    with increasing model complexity</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19755">In</st> *<st c="19759">Figure 8</st>**<st c="19767">.7</st>*<st
    c="19769">, we have indicated the position where the generalization error is at
    its minimum.</st> <st c="19852">We have marked this as “optimal model complexity.”
    Why does the smallest generalization error identify the optimal model?</st> <st
    c="19974">Why do we want the generalization error to be minimal and not the training
    error?</st> <st c="20056">Well, it is for predictions we want to use the model.</st>
    <st c="20110">We don’t need to predict the training data – we already have it.</st>
    <st c="20175">The point where the generalization error is smallest represents
    the point where the performance of the model is optimal for its intended use.</st>
    <st c="20317">The minimum in the generalization error is the sweet spot we</st>
    <st c="20377">referred</st> <st c="20387">to earlier.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="20398">Figure 8</st>**<st c="20407">.7</st>* <st c="20409">succinctly
    represents the key ideas that we have introduced in this section, so this is a
    good point to summarize what we</st> <st c="20532">have learned.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20545">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="20561">In this section, we learned</st> <st c="20590">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20604">Model complexity broadly correlates with the number of features
    or parameters in</st> <st c="20686">a model.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20694">A model that is too complex will overfit to the noise in training
    data.</st> <st c="20767">It will predict well on the training data points but
    predict poorly on</st> <st c="20838">unseen data.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20850">A model that is not complex enough will underfit the general trends
    and patterns present in training data.</st> <st c="20958">It will predict poorly
    on both the training data points and</st> <st c="21018">unseen data.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21030">A model that predicts well on unseen data is said to generalize.</st>
    <st c="21096">It does so by not fitting to the noise in the training data and,
    instead, by learning the general trends and</st> <st c="21205">patterns present.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21222">Overly complex models have a large random component to their predictions,
    arising from the choice of training data used.</st> <st c="21344">In contrast,
    low-complexity models will show little variation in their predictions across different
    training sets but will</st> <st c="21467">display bias.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21480">We can use MSE and RMSE to quantify</st> <st c="21517">prediction
    errors.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21535">The MSE and RMSE on the training dataset monotonically decrease
    with increasing model complexity, while the MSE and RMSE on a holdout dataset
    will display a minimum – first decreasing and then increasing as we increase</st>
    <st c="21755">model complexity.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21772">Having learned the basic ideas behind model complexity and how
    it affects the generalization abilities of a model, in the next section we will
    dig deeper into the mathematical detail behind the generalization error curve
    and introduce a modern twist to</st> <st c="22026">its behavior.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22039">The bias-variance trade-off</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="22067">The generalization</st> <st c="22086">error curve in</st> *<st
    c="22102">Figure 8</st>**<st c="22110">.7</st>* <st c="22112">shows a minimum.</st>
    <st c="22130">In the preceding section, we gave a qualitative explanation of why
    we expected the generalization error to first decrease and then increase with
    increasing model complexity and why, therefore, this leads to a minimum in the
    generalization error curve.</st> <st c="22382">But to get a quantitative idea
    of why the generalization error curve displays a minimum and what controls its
    position, we need to dig into the math behind</st> <st c="22538">the curve.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22548">The generalization error curve</st> <st c="22579">is made up of
    two competing contributions, one increasing with model complexity and the other
    decreasing.</st> <st c="22686">It is the competition between these two contributions
    that leads to the minimum.</st> <st c="22767">Those two contributions are, first,
    the</st> *<st c="22807">bias</st>* <st c="22811">in a model’s prediction at a
    holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="22856"><st c="22857">, and second, the</st> *<st c="22875">variance</st>* <st
    c="22883">in the model’s prediction at the holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="22932"><st c="22933">, with the variance arising from the sensitivity of
    the model’s prediction to the precise choice of training data.</st> <st c="23048">These
    two competing contributions are essentially what we highlighted in</st> *<st c="23121">Figure
    8</st>**<st c="23129">.4</st>* <st c="23131">and</st> *<st c="23136">Figure 8</st>**<st
    c="23144">.6</st>* <st c="23147">in the</st> <st c="23154">previous section.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23171">Mathematically, we</st> <st c="23191">find tha</st><st c="23199">t,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>on</mtext><mtext>holdout</mtext><mtext>data</mtext><mtext>=</mtext><msup><mtext>Bias</mtext><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext></mrow></mrow></math>](img/2803.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="23241">Eq.</st> <st c="23245">3</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23246">We’ll go through the derivation of</st> *<st c="23281">Eq.</st>
    <st c="23285">3</st>* <st c="23286">in a moment, as it is instructive to do so
    and because the derivation of</st> *<st c="23360">Eq.</st> <st c="23364">3</st>*
    <st c="23365">is more subtle than we have hinted at.</st> <st c="23405">We know
    from our qualitative discussions of</st> *<st c="23449">Figure 8</st>**<st c="23457">.4</st>*
    <st c="23459">and</st> *<st c="23464">Figure 8</st>**<st c="23472">.6</st>* <st
    c="23474">that with increasing model complexity, first, the bias of predictions
    decreases, and second, the variance of predictions increases.</st> <st c="23607">Schematically,
    we illustrated this in</st> *<st c="23645">Figure 8</st>**<st c="23653">.8</st>*<st
    c="23655">, which shows the original generalization error curve from</st> *<st
    c="23714">Figure 8</st>**<st c="23722">.7</st>*<st c="23724">, with the bias and
    variance curves</st> <st c="23759">now overlayed.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="23853">Figure 8.8: A schematic plot of the bias and variance decomposition
    of the generalization error</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23948">What are the</st> <st c="23962">consequences of</st> *<st c="23978">Eq.</st>
    <st c="23982">3</st>*<st c="23983">?</st> *<st c="23985">Figure 8</st>**<st c="23993">.8</st>*
    <st c="23995">shows that we must always make a trade-off between bias and variance.</st>
    <st c="24066">In fact, the decomposition of the generalization error that</st>
    *<st c="24126">Eq.</st> <st c="24130">3</st>* <st c="24131">represents is referred
    to as the “bias-variance trade-off.” At the point of the minimum generalization
    error, we have managed to optimize this trade-off.</st> <st c="24286">But the
    fact remains that, at any point on the generalization error curve, we have traded
    bias against variance.</st> <st c="24399">For a given training dataset, if we
    want a model with a smaller bias, we can do so by increasing the model complexity,
    but we pay the price of increased uncertainty in our model predictions.</st> <st
    c="24590">We must be aware of that; there is no free lunch.</st> <st c="24640">Likewise,
    if we want a model with smaller variance (uncertainty) in its predictions, we
    can do so by decreasing the model complexity, but we pay the price of increased
    bias in</st> <st c="24816">those predictions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24834">The only way we can decrease both bias and variance is to increase
    the size of the training data.</st> <st c="24933">This will ensure that any increased
    model complexity has to be used to explain the extra fine-grained trends and patterns
    that the extra data reveals, rather than used to overfit to noise in</st> <st
    c="25124">the data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25133">Proof of the bias-variance trade-off formula</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="25178">To identify the two contributions</st> <st c="25212">to the generalization
    error curve, we’ll calculate the expected generalization error curve.</st> <st
    c="25305">This gives us the typical shape of the generalization error curve, not
    specific to any particular training dataset.</st> <st c="25421">We’ll specifically
    look at the expected MSE of the holdout dataset.</st> <st c="25489">This is</st>
    <st c="25497">defined</st> <st c="25505">as,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><msub><mi>N</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mfrac><mrow><munder><mo>∑</mo><mrow><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>,</mo><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mo>∈</mo><msub><mi>D</mi><mrow><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></munder><msup><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></mfenced></mrow></mrow></math>](img/2804.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25557">Eq.</st> <st c="25561">4</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25562">For simplicity of notation, we have omitted the dependence of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2805.png)
    <st c="25624"><st c="25625">on the model parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)
    <st c="25650"><st c="25651">in</st> *<st c="25655">Eq.</st> <st c="25659">4</st>*<st
    c="25660">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25661">To understand</st> *<st c="25676">Eq.</st> <st c="25680">4</st>*<st
    c="25681">, we need to understand</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced></mrow></mrow></math>](img/2807.png)
    <st c="25705"><st c="25725">averaged over all the holdout data points</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/2808.png)<st
    c="25767"><st c="25768">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25770">If we expand out</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2809.png)<st
    c="25787"><st c="25788">, we can write</st> <st c="25803">this</st> <st c="25808">as,</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>−</mo><msub><mrow><mn>2</mn><mi
    mathvariant="double-struck">E</mi></mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2810.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25886">Eq.</st> <st c="25890">5</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25891">We’ll take each of the terms on the right-hand side of</st> *<st
    c="25946">Eq.</st> <st c="25950">5</st>* <st c="25951">one at a time.</st> <st
    c="25967">For the first term, we’re going to add the assumption that the observed
    value of the holdout target value,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26074"><st c="26097">, is just a noise-corrupted version of a ground-truth
    value, so</st> <st c="26161">we have,</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>y</mi><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mfenced
    open="(" close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>+</mo><mi>ε</mi></mrow></mrow></math>](img/2812.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26183">Eq.</st> <st c="26187">6</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26188">We’ll assume that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/384.png)
    <st c="26206"><st c="26207">is a random variable with zero mean and variance</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)<st
    c="26257"><st c="26260">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2815.png)
    <st c="26266"><st c="26267">function is a deterministic function – it has no randomness
    in it.</st> <st c="26335">It is the value we would get for the target variable
    at a holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="26408"><st c="26409">, if there were no added noise.</st> <st c="26441">We’ll
    calculate the expectation of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2817.png)
    <st c="26476"><st c="26494">over</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)
    <st c="26499"><st c="26500">to represent the averaging over the holdout value,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="26552"><st c="26575">. This means in</st> *<st c="26591">Eq.</st> <st c="26595">5</st>*<st
    c="26596">, we replace</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math>](img/2820.png)
    <st c="26609"><st c="26616">with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:msub></mml:math>](img/2821.png)<st
    c="26621"><st c="26634">. With that additional change,</st> <st c="26665">we g</st><st
    c="26669">et,</st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>y</mi><mn>2</mn></msup></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup></mfenced><mo>+</mo><mn>2</mn><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>ε</mi><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mo>+</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mi>σ</mi><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2822.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26724">Eq.</st> <st c="26728">7</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26729">In deriving the very right-hand side of</st> *<st c="26769">Eq.</st>
    <st c="26773">7</st>*<st c="26774">, we have made use of the fact that the random
    additive noise</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ε</mml:mi></mml:math>](img/2224.png)
    <st c="26836"><st c="26837">has mean zero, and that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2824.png)
    <st c="26862"><st c="26863">is just a fixed number with</st> <st c="26892">no
    randomness.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26906">For the second term in</st> *<st c="26930">Eq.</st> <st c="26934">5</st>*<st
    c="26935">,</st> <st c="26937">we fi</st><st c="26942">nd,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mfenced open="(" close=")"><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>+</mo><mi>ε</mi></mrow></mfenced><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/2825.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27020">Eq.</st> <st c="27024">8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27025">The final term on the right-hand side of</st> *<st c="27066">Eq.</st>*
    *<st c="27070">5</st>* <st c="27071">is,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><msup><mi>g</mi><mn>2</mn></msup><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>=</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2826.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27076">Eq.</st> <st c="27080">9</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27081">The very</st> <st c="27090">right-hand side of</st> *<st c="27109">Eq.</st>
    <st c="27113">9</st>* <st c="27114">follows from the fact that for any random
    variable</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)
    <st c="27166"><st c="27167">we have,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>−</mo><msup><mfenced
    open="(" close=")"><mrow><mi mathvariant="double-struck">E</mi><mfenced open="("
    close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2828.png)<st
    c="27177"><st c="27201">, and so re-arranging we</st> <st c="27226">have</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="double-struck">E</mi><mfenced
    open="(" close=")"><msup><mi>z</mi><mn>2</mn></msup></mfenced><mo>=</mo><mtext>Var</mtext><mfenced
    open="(" close=")"><mi>z</mi></mfenced><mo>+</mo><msup><mfenced open="(" close=")"><mrow><mi
    mathvariant="double-struck">E</mi><mfenced open="(" close=")"><mi>z</mi></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2829.png)<st
    c="27231"><st c="27255">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27256">Plugging the results from</st> *<st c="27283">Eq.</st> <st c="27287">7</st>*<st
    c="27288">,</st> *<st c="27290">Eq.</st> <st c="27294">8</st>*<st c="27295">,
    and</st> *<st c="27301">Eq.</st> <st c="27305">9</st>* <st c="27306">into</st>
    *<st c="27312">Eq.</st> <st c="27316">5</st>*<st c="27317">,</st> <st c="27319">we</st>
    <st c="27322">get,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msubsup><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow><mn>2</mn></msubsup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mn>2</mn><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><msub><mi
    mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/2830.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27439">Eq.</st> <st c="27443">10</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27445">We can rearrange the terms on the right-hand side of</st> *<st
    c="27499">Eq.</st> <st c="27503">10</st>* <st c="27505">to g</st><st c="27510">ive,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi
    mathvariant="double-struck">E</mi><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>,</mo><mi>ε</mi></mrow></msub><mfenced
    open="(" close=")"><msup><mfenced open="(" close=")"><mrow><mi>y</mi><mo>−</mo><mi>g</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mn>2</mn></msup></mfenced><mo>=</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi mathvariant="double-struck">E</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>−</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></mfenced><mn>2</mn></msup><mo>+</mo><msub><mtext>Var</mtext><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></msub><mfenced
    open="(" close=")"><mrow><mi>g</mi><mfenced open="(" close=")"><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>|</mo><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/2831.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27588">Eq.</st> <st c="27592">11</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27594">Now,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>](img/2832.png)
    <st c="27600"><st c="27601">is just the expected difference between the prediction
    of our model at the holdout point,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="27692"><st c="27693">, and the true (noise-free) value at that point.</st>
    <st c="27742">This is just the bias.</st> <st c="27765">Similarly, the expression</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Var</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/2834.png)
    <st c="27791"><st c="27810">is just the variance of our model’s prediction as
    we train it on different training datasets.</st> <st c="27904">This means we can
    use</st> *<st c="27926">Eq.</st> <st c="27930">11</st>* <st c="27932">to wr</st><st
    c="27938">ite,</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>MSE</mtext><mtext>on</mtext><mtext>holdout</mtext><mo>=</mo><msup><mrow><mtext>Bias</mtext><mtext>in</mtext><mtext>prediction</mtext></mrow><mn>2</mn></msup><mo>+</mo><mtext>Variance</mtext><mtext>of</mtext><mtext>prediction</mtext><mo>+</mo><mtext>Variance</mtext><mtext>of</mtext><mtext>noise</mtext></mrow></mrow></math>](img/2835.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="28025">Eq.</st> <st c="28029">12</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="28031">Eq.</st> <st c="28036">12</st>* <st c="28038">is the</st> <st
    c="28045">same as</st> *<st c="28054">Eq.</st> <st c="28058">3</st>*<st c="28059">,
    with the addition of the last term, the variance</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/1828.png)
    <st c="28110"><st c="28113">of the additive noise.</st> <st c="28136">This means
    that the presence of noise in the data sets a minimum value for the generalization
    error.</st> <st c="28237">No amount of trading off bias against variance will
    get us below</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)
    <st c="28302"><st c="28307">for the MSE on the holdout data.</st> <st c="28340">This</st>
    <st c="28345">is intuitive.</st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28358">We have already unpacked the consequences of the bias-variance
    trade-off equation.</st> <st c="28442">This equation neatly summarizes the classical
    view of model complexity.</st> <st c="28514">The schematic plot shown in</st>
    *<st c="28542">Figure 8</st>**<st c="28550">.7</st>* <st c="28552">also neatly
    summarizes this classical view of model complexity.</st> <st c="28617">You may
    have seen it before, as it is used in many introductions to machine learning.</st>
    <st c="28703">However, for highly parameterized machine learning models, the plot
    in</st> *<st c="28774">Figure 8</st>**<st c="28782">.7</st>* <st c="28784">is
    not the full story.</st> <st c="28808">An interesting twist has arisen in the
    last few years, emerging from the study of deep learning neural networks.</st>
    <st c="28921">We’ll now explain what that</st> <st c="28949">twist is.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28958">Double descent – a modern twist on the generalization error diagram</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29026">In our previous qualitative</st> <st c="29055">discussions, where
    we fitted various polynomials (from the 0</st><st c="29115">th</st> <st c="29118">degree
    to the 12</st><st c="29135">th</st> <st c="29138">degree) to the dataset in</st>
    *<st c="29165">Figure 8</st>**<st c="29173">.2</st>*<st c="29175">, it was probably
    obvious to you where the sweet spot lay in terms of number of model parameters.</st>
    <st c="29273">The data in</st> *<st c="29285">Figure 8</st>**<st c="29293">.2</st>*
    <st c="29295">was generated using a quadratic equation, so a 2nd-degree polynomial
    was clearly optimal to use to fit to the training data.</st> <st c="29421">In
    these circumstances, it was self-evident where the generalization error minimum
    should be because the models we were fitting to the training data were in the
    same class – polynomials – as the process that generated the data.</st> <st c="29649">This
    will not always be</st> <st c="29673">the case.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29682">Imagine that we have a</st> <st c="29705">real-world dataset that
    we model using a neural network.</st> <st c="29763">It is highly unlikely that
    the data was generated using a neural network.</st> <st c="29837">However, our
    qualitative arguments about overfitting that led us to</st> *<st c="29905">Figure
    8</st>**<st c="29913">.7</st>* <st c="29915">still hold.</st> <st c="29928">We
    still expect to see the training error decrease monotonically as we increase the
    number of parameters in the neural network, and for the generalization error to
    display a minimum.</st> <st c="30111">For modern machine learning models, what
    we tend to see is illustrated schemati</st><st c="30190">cally in</st> *<st c="30200">Figure
    8</st>**<st c="30208">.9</st>*<st c="30210">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="30292">Figure 8.9: A generalization error curve for a modern machine
    learning model</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30368">Schematically, the generalization curve for our neural network
    model looks like the generalization curve in</st> *<st c="30477">Figure 8</st>**<st
    c="30485">.7</st>*<st c="30487">. The difference is that now the</st> *<st c="30520">x</st>*<st
    c="30521">-axis explicitly represents the number of model parameters,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30581"><st c="30582">. The generalization error curve increases steeply as
    we increase</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2838.png)<st
    c="30648"><st c="30649">, with the steep increase occurring as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2840.png)
    <st c="30688"><st c="30689">approaches the size of the training dataset,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="30735"><st c="30736">. At first sight, it looks like nothing more interesting
    is happening and there is no need to explore any further.</st> <st c="30851">However,
    if we continue to increase the number of parameters in our neural network, what
    we tend to see is illustrated schematical</st><st c="30981">ly in</st> *<st c="30988">Figure
    8</st>**<st c="30996">.10</st>*<st c="30999">.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="31080">Figure 8.10: A “double descent” generalization error curve for
    a modern machine learning model</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="31174">Figure 8</st>**<st c="31183">.10</st>* <st c="31186">shows that
    as</st> <st c="31201">we increase the number of parameters in our neural network
    beyond</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/443.png)<st
    c="31267"><st c="31268">, the generalization error decreases again.</st> <st c="31312">This
    phenomenon is termed</st> *<st c="31338">double descent</st>* <st c="31352">because
    the generalization error decreases for a second time as we increase the model
    complexity.</st> <st c="31451">For</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mo>></mml:mo><mml:mi>N</mml:mi></mml:math>](img/2843.png)<st
    c="31455"><st c="31460">, both the training error and generalization error continue</st>
    <st c="31520">to decrease.</st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31532">The consequences of double descent</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*<st c="31567">Figure 8</st>**<st c="31576">.10</st>* <st c="31579">tells</st>
    <st c="31586">us that it is possible to have an</st> *<st c="31620">over-parameterized</st>*
    <st c="31638">model and still have good generalization performance.</st> <st c="31693">This
    has big implications for large neural network models and, in particular,</st>
    **<st c="31771">Large Language Models</st>** <st c="31792">(</st>**<st c="31794">LLMs</st>**<st
    c="31798">), which can have billions of parameters.</st> *<st c="31841">Figure
    8</st>**<st c="31849">.10</st>* <st c="31852">shows that this</st> <st c="31868">number
    of parameters in a model is not an obstacle to the model genuinely learning, so
    double descent gives support to the use and development of deep learning neural
    networks, LLMs, and other massively</st> <st c="32072">parameterized models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32093">However, that’s not to say we fully understand all the details
    behind double-descent.</st> <st c="32180">The phenomenon of double descent is
    still very much an active area of research.</st> <st c="32260">Indeed, at the
    time of writing (2023), one of the most talked-about papers at the top machine
    learning conference, NeurIPS, was about double-descent and how you should effectively
    count parameters in a model – see the</st> *<st c="32478">Notes and further reading</st>*
    <st c="32503">section at the end of the chapter for details on this paper.</st>
    <st c="32565">Furthermore, the debate continues in academic circles and on social
    media about whether LLMs have truly learned from their training data and are displaying
    understanding, or whether they have simply memorized the training data and stored
    it in the massive number of</st> <st c="32831">model parameters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32848">Since double</st> <st c="32861">descent brings us up to the present
    day in terms of what is known about generalization error and model predictive
    accuracy, this seems like a good place to finish this section and recap what we</st>
    <st c="33056">have learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33069">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33085">In this section, we learned</st> <st c="33114">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33128">The generalization error is made up of two contributions, one
    from the bias in the model predictions and one from the variance of the</st> <st
    c="33263">model predictions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33280">The bias-variance trade-off and</st> <st c="33313">its implications</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33329">The mathematical detail behind the bias-variance</st> <st c="33379">trade-off
    equation</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33397">The phenomenon of double descent in over-parameterized machine
    learning models and</st> <st c="33481">its implications</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33497">Having learned more mathematical details about generalization
    error, in the next section, we will learn about various model complexity metrics
    and how to use them to select models of the appropriate complexity for a given</st>
    <st c="33720">training set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33733">Model complexity measures for model selection</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="33779">Practical model</st> <st c="33796">complexity measures tend not
    to measure model complexity directly.</st> <st c="33863">Instead, they measure
    some sort of trade-off – for example, how much information has been lost by approximating
    the patterns present in a dataset by using a particular model form, or what evidence
    a dataset provides for a model form of this level of complexity.</st> <st c="34125">These
    practical metrics don’t directly measure model complexity, but they take it</st>
    <st c="34207">into account.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34220">Selecting between classes of models</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34256">In the preceding paragraph, we referred to model form.</st> <st
    c="34312">But what do we mean by model form?</st> <st c="34347">We mean the mathematical
    form of the equation that defines a model.</st> <st c="34415">So, two models that
    differ only in their parameter values but otherwise have the same form of mathematical
    equation have the same model form (e.g., two linear models that use the</st> <st
    c="34594">same features).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34609">A model form</st> <st c="34622">represents a whole class of models.</st>
    <st c="34659">Let’s go back to our polynomial model example to illustrate.</st>
    <st c="34720">For our polynomial models in</st> *<st c="34749">Figure 8</st>**<st
    c="34757">.1</st>* <st c="34759">and</st> *<st c="34764">Figure 8</st>**<st c="34772">.2</st>*<st
    c="34774">, we had 1</st><st c="34784">st</st><st c="34787">-degree polynomials,
    2</st><st c="34810">nd</st><st c="34813">-degree polynomials, 4</st><st c="34836">th</st><st
    c="34839">-degree polynomials, and 12</st><st c="34867">th</st><st c="34870">-degree
    polynomials.</st> <st c="34892">The model class in this case is just the degree
    of the polynomial.</st> <st c="34959">The 12</st><st c="34965">th</st><st c="34968">-degree
    model class is the set of all possible 12</st><st c="35018">th</st><st c="35021">-degree
    polynomials.</st> <st c="35043">In this case, the model class is related simply
    to the number of parameters</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)
    <st c="35119"><st c="35120">in the</st> <st c="35128">model because,</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>p</mi><mo
    mathvariant="italic">=</mo><mtext mathvariant="italic">degree</mtext><mtext mathvariant="italic">of</mtext><mtext
    mathvariant="italic">polynomial</mtext><mtext mathvariant="italic">+</mtext><mtext>1</mtext></mrow></mrow></math>](img/2845.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="35172">Eq.</st> <st c="35176">13</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35178">We can denote each model class by</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)<st
    c="35213"><st c="35214">, and in this instance, the different model classes</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math>](img/2846.png)
    <st c="35266"><st c="35267">represent different levels of</st> <st c="35298">model
    complexity.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35315">The challenge we face is working out which degree of polynomial
    model to use to model our data.</st> <st c="35412">In other words, we</st> <st
    c="35430">need to work out which class of models to use.</st> <st c="35478">This
    process is called</st> **<st c="35501">model selection</st>**<st c="35516">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35517">Once we have selected which class of models to use, identifying
    which model to use within that class is just a case of parameter estimation (e.g.,
    via least-squares fitting) or maximum likelihood estimation.</st> <st c="35726">In
    fact, when comparing two model classes, we often just compare the maximum likelihood
    models from each class, since this uses the best model in each class as a representative
    of</st> <st c="35906">that class.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35917">We have already met a measure that helps us select between different
    model forms or classes, the generalization error.</st> <st c="36037">We have also
    seen how it is affected by model complexity.</st> <st c="36095">In this section,
    we will introduce two other commonly used model complexity measures</st> <st c="36179">that
    are used for model selection, the</st> **<st c="36219">Akaike Information Criterion</st>**
    <st c="36247">(</st>**<st c="36249">AIC</st>**<st c="36252">) and</st> <st c="36259">the</st>
    **<st c="36263">Bayesian Information</st>** **<st c="36284">Criterion</st>** <st
    c="36293">(</st>**<st c="36295">BIC</st>**<st c="36298">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36301">To start, we’ll define our model form,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)<st
    c="36341"><st c="36342">, for which we want to compute the AIC and the BIC.</st>
    <st c="36394">The model form</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)
    <st c="36409"><st c="36410">is a function,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/2850.png)<st
    c="36426"><st c="36427">, that takes a feature vector,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="36458"><st c="36459">, as input and uses its</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2852.png)
    <st c="36483"><st c="36484">model parameters, represented by the vector</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1778.png)<st
    c="36529"><st c="36530">, to compute the model output.</st> <st c="36561">From
    now on, we’ll use</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="36584"><st c="36585">when we mean either a model or a model form.</st>
    <st c="36631">We also assume we know how to calculate the likelihood,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>L</mml:mi></mml:math>](img/1598.png)<st
    c="36687"><st c="36688">, of a dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi><mml:mo>,</mml:mo></mml:math>](img/2856.png)
    <st c="36704"><st c="36705">given the model – that is, we know how to calculate</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mo>,</mo><mi>g</mi><mo>)</mo></mrow></mrow></mrow></math>](img/2857.png)<st
    c="36758"><st c="36770">. With that in place, we can begin to compute the AIC</st>
    <st c="36824">and BIC.</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36832">Akaike Information Criterion</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="36861">The</st> <st c="36866">AIC</st> <st c="36869">is</st> <st c="36873">defined
    as,</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>AIC</mtext><mtext>=</mtext><mn>2</mn><mi>p</mi><mtext>-</mtext><mtext>2</mtext><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2858.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="36902">Eq.</st> <st c="36906">14</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36908">In</st> *<st c="36912">Eq.</st> <st c="36916">14</st>*<st c="36918">,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/2859.png)
    <st c="36920"><st c="36924">is the maximum likelihood of the model</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/2849.png)<st
    c="36963"><st c="36964">. The AIC attempts to measure the information lost by
    the model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37028"><st c="37029">when we use it as an approximation of the true process
    that generated the data.</st> <st c="37110">We haven’t yet defined what “information”
    is, and we don’t do so until</st> [*<st c="37181">Chapter 13</st>*](B19496_13.xhtml#_idTextAnchor646)<st
    c="37191">. Basically, we can think of the information loss as a measure of how
    different the general trends and patterns that model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37314"><st c="37315">produces are, compared to the general trends and patterns
    from the true</st> <st c="37388">data-generating process.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37412">Obviously, a good candidate model will be close to the true generating
    process and will have a small information loss, leading to a small AIC value.</st>
    <st c="37562">We can select an optimal model class from a set of candidate model
    classes by choosing the one that has the</st> <st c="37670">smallest AIC.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37683">Since the information loss is not a comparison of the model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37744"><st c="37745">to the training data, but a comparison of the model</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="37798"><st c="37799">to the true data-generating process, minimizing the
    AIC metric automatically guards against overfitting.</st> <st c="37905">But wait,
    I hear you say – doesn’t the AIC include the log-likelihood that depends on the
    training data?</st> <st c="38010">Yes, that is true.</st> <st c="38029">However,
    to compute the information loss, we’d have to know the true generating process.</st>
    <st c="38118">Instead, we said that the AIC “attempts” to measure the information
    loss.</st> <st c="38192">The AIC approximates the information loss, but it still
    retains many of the desirable properties of the information loss, so we can still
    use it for model selection.</st> <st c="38358">Let’s look at the AIC formula in</st>
    *<st c="38391">Eq.</st> <st c="38395">14</st>* <st c="38397">to see how this model
    selection</st> <st c="38430">is done.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38438">The AIC consists of two contributions,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2865.png)
    <st c="38478"><st c="38479">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2866.png)<st
    c="38484"><st c="38491">. As we increase model complexity by increasing</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="38539"><st c="38540">, we obviously increase the first of these contributions.</st>
    <st c="38598">However, a higher complexity model will be able to fit the training
    data more closely, so will have a smaller value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2868.png)<st
    c="38717"><st c="38718">. We can see that the two contributions to the AIC work
    in competition against each other.</st> <st c="38809">Minimizing the AIC will
    find the optimal trade-off between the two contributions – that is, the optimal
    balance between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="38929"><st c="38930">(the model complexity) and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>](img/2870.png)
    <st c="38958"><st c="38959">(the measure of the</st> <st c="38980">model fit).</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38991">Weaknesses of the AIC</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="39013">The previous sentence makes it</st> <st c="39044">sound like the
    AIC is ideal for what we want to do – find the sweet spot of model complexity.</st>
    <st c="39139">Not quite.</st> <st c="39150">There are a few issues we</st> <st
    c="39176">should highlight:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39193">Firstly, the AIC formula in</st> *<st c="39222">Eq.</st> <st c="39226">14</st>*
    <st c="39228">is an asymptotic (i.e., a large sample) result.</st> <st c="39277">We
    said that the AIC approximates the information loss.</st> <st c="39333">That approximation
    is increasingly accurate as,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="39381"><st c="39382">, the size of the training dataset increases.</st> <st
    c="39428">It is most accurate when</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/629.png)
    <st c="39453"><st c="39454">is large.</st> <st c="39465">The consequence of this
    is that for small training datasets, the AIC won’t accurately approximate the
    information loss, and a model selection based on</st> *<st c="39616">Eq.</st>
    <st c="39620">14</st>* <st c="39622">may perform poorly.</st> <st c="39643">There
    is a modified or corrected form of the AIC, usually denoted as AICc, that attempts
    to correct this deficiency.</st> <st c="39760">We won’t explain the AICc here,
    other than to say that you can use it for model selection in the same way that
    you use the AIC – you select the model with the smallest value</st> <st c="39934">of
    AICc.</st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39942">Secondly, the use of the AIC follows a different philosophy to
    how we use the generalization error for model selection.</st> <st c="40063">You’re
    probably already familiar with using the MSE on a validation set to perform hyper-parameter
    optimization for a machine learning model.</st> <st c="40205">Also, you’ve no
    doubt run cross-validation calculations before.</st> <st c="40269">In a machine
    learning context, we are directly trying to optimize, albeit in an empirical fashion,
    the thing we care about – the future predictive accuracy of our model.</st> <st
    c="40439">In contrast, when we minimize the AIC, we are optimizing a proxy measure
    that we believe should be correlated with good</st> <st c="40559">predictive accuracy.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="40579">The preceding second point is not unique to the AIC.</st> <st
    c="40633">It is also a criticism that we can make of the BIC, which we will</st>
    <st c="40699">explain next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40712">Why should we use the AIC, then, if it appears to be not as good
    as the generalization error as a means of selecting the optimal model?</st> <st
    c="40849">The AIC is still a</st> <st c="40868">useful metric for</st> <st c="40886">many
    reasons:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40899">Calculating the MSE on a validation set requires us to have sufficient
    data to divide into training, validation, and test splits.</st> <st c="41030">Similarly,
    calculating cross-validation measures can be</st> <st c="41086">computationally
    expensive.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41112">In contrast, the AIC is quick to calculate.</st> <st c="41157">If
    we have fitted our model using maximum likelihood, then we have the AIC essentially
    with minimal</st> <st c="41257">extra computation.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41275">The theoretical underpinnings of the AIC are easier to understand.</st>
    <st c="41343">Minimizing the generalization error or doing a cross-validation
    analysis have an intuitive empirical justification to them, but a detailed theoretical
    analysis of the performance of model selection/hyper-parameter optimization based
    on validation/cross-validation can be harder to do, so we may have a less-detailed
    understanding of the weaknesses of</st> <st c="41694">such approaches.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41710">Personally, I like to use the AIC in addition to other model selection
    metrics.</st> <st c="41791">The ease of calculation of the AIC is its biggest
    selling point, but you must always be aware that, first, it is an approximation,
    and second, it is measures information loss, not</st> <st c="41971">predictive
    accuracy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41991">Bayesian Information Criterion</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="42022">The</st> <st c="42026">B</st><st c="42028">IC is</st> <st c="42034">defined
    as:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mtext>=</mtext><mi>p</mi><mi>log</mi><mi>N</mi><mo>−</mo><mn>2</mn><mi>log</mi><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow></mrow></math>](img/2873.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="42070">Eq.</st> <st c="42074">15</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42076">where again</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>](img/2874.png)
    <st c="42089"><st c="42093">is the maximum likelihood for our model,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1681.png)<st
    c="42134"><st c="42135">. Like the AIC, the BIC is extremely easy to calculate,
    particularly if we have already fitted our model</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="42240"><st c="42241">via maximum</st> <st c="42254">likelihood estimation.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42276">The formula in</st> *<st c="42292">Eq.</st> <st c="42296">15</st>*
    <st c="42298">hides the idea behind the BIC.</st> <st c="42330">As you might guess,
    the BIC is based upon ideas from Bayesian analysis of models.</st> <st c="42412">The
    BIC approximates the</st> *<st c="42437">Bayesian evidence</st>* <st c="42454">of
    the model class.</st> <st c="42475">Given a dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2134.png)<st
    c="42492"><st c="42493">, the Bayesian evidence for a model class,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/332.png)<st
    c="42536"><st c="42537">, is</st> <st c="42542">defined as:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Bayesian</mtext><mtext>evidence</mtext><mo>=</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>D</mi><mo>|</mo><mi>M</mi></mrow></mfenced><mo>=</mo><mo>∫</mo><mrow><mrow><mi>P</mi><mfenced
    open="(" close="|"><mi>D</mi></mfenced><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>,</mo><mi>M</mi><mo>)</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow><mi>d</mi><msub><munder><mi>θ</mi><mo
    stretchy="true">_</mo></munder><mi>M</mi></msub></mrow></mrow></math>](img/2879.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="42607">Eq.</st> <st c="42611">16</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42613">In</st> *<st c="42617">Eq.</st> <st c="42621">16</st>*<st c="42623">,
    we have used</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/2880.png)
    <st c="42638"><st c="42641">to denote the model parameters for a model in the
    class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42697"><st c="42698">, and the symbol</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><munder><mi>θ</mi><mo stretchy="true">_</mo></munder><mi>M</mi></msub><mo>|</mo><mi>M</mi></mrow></mfenced></mrow></mrow></math>](img/2882.png)
    <st c="42715"><st c="42724">represents the prior distribution on</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math>](img/2883.png)
    <st c="42761"><st c="42764">given the model class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/678.png)<st
    c="42786"><st c="42787">. For our polynomial model class</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math>](img/2885.png)
    <st c="42820"><st c="42821">this would be the Bayesian prior we put on the 13
    coefficients of the</st> <st c="42892">12</st><st c="42894">th</st><st c="42897">-degree
    polynomial.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42917">In general, the larger the Bayesian evidence of a model class,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>M</mml:mi></mml:math>](img/556.png)<st
    c="42981"><st c="42982">, the more evidence the data,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>D</mml:mi></mml:math>](img/2887.png)<st
    c="43012"><st c="43013">, provides that the model class is the one that generated
    the data.</st> <st c="43081">However, as with the AIC, we run into a technical
    issue here.</st> <st c="43143">Calculating</st> *<st c="43155">Eq.</st> <st c="43159">16</st>*
    <st c="43161">is not generally easy.</st> <st c="43185">However, when the size
    of the training dataset,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)<st
    c="43233"><st c="43234">, is large, we can come up with a general approximation.</st>
    <st c="43291">That approximation is</st> <st c="43313">the BIC.</st> <st c="43322">In
    fact:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>BIC</mtext><mo>≈</mo><mo>−</mo><mn>2</mn><mi>log</mi><mfenced
    open="(" close=")"><mrow><mtext>Bayesian</mtext><mtext>Evidence</mtext></mrow></mfenced><mtext>as</mtext><mi>N</mi><mo>→</mo><mi
    mathvariant="normal">∞</mi></mrow></mrow></math>](img/2889.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="43370">Eq.</st> <st c="43374">17</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="43376">Eq.</st> <st c="43381">17</st>* <st c="43383">indicates that
    maximizing the Bayesian evidence is the same as minimizing the BIC.</st> <st c="43467">Consequently,
    we perform model selection by minimizing</st> <st c="43522">the BIC.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43530">As with the AIC, there are</st> <st c="43557">several comments
    and observations we can make about</st> <st c="43610">the BIC:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43618">Like the AIC, the BIC is easy</st> <st c="43649">to calculate.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="43662">Again, like the AIC, the BIC is based on a large sample size approximation,
    so at smaller sample sizes, it may not select the true optimal model class.</st>
    <st c="43815">For smaller sample sizes, it would be better to compute the Bayesian
    evidence exactly via</st> *<st c="43905">Eq.</st> <st c="43909">16</st>*<st c="43911">,
    although this can require considerable mathematical skill or using advanced computationally
    intensive Monte</st> <st c="44021">Carlo techniques.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="44038">The formula for the BIC looks very similar to that for the AIC.</st>
    <st c="44103">The difference between the BIC and AIC is just in the penalty terms;</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:mi>p</mml:mi></mml:math>](img/2890.png)
    <st c="44172"><st c="44173">for the AIC compared to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math>](img/2891.png)
    <st c="44198"><st c="44204">for the BIC.</st> <st c="44217">So, as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi></mml:math>](img/115.png)
    <st c="44224"><st c="44225">increases, the penalty we pay for increasing the model
    complexity becomes higher in the BIC than in the AIC, meaning the BIC tends to
    be a more stringent selection criterion than</st> <st c="44405">the AIC.</st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="44413">Having discussed the AIC and BIC at length, it is time to wrap
    up this section.</st> <st c="44494">Let’s first summarize what we have learned
    in this section about practical model complexity measures, and then we’ll summarize
    the</st> <st c="44625">chapter overall.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44641">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="44657">In this section, we learned</st> <st c="44686">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44700">The concept of</st> <st c="44716">model selection</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="44731">The AIC and how we can use it to perform model selection by selecting
    the model class with the lowest</st> <st c="44834">AIC value</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="44843">The BIC and how we can use it to perform model selection by selecting
    the model class with the lowest</st> <st c="44946">BIC value</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="44955">The strengths and weaknesses of the AIC</st> <st c="44996">and
    BIC</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="45003">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="45011">This chapter has been a shorter one and largely a visual one.</st>
    <st c="45074">The only heavy math came in the proof of the bias-variance decomposition
    of the generalization error.</st> <st c="45176">However, the visual approach has
    been useful in explaining concepts of overfitting, underfitting, and generalization.</st>
    <st c="45294">At a superficial level, these concepts are intuitive and need very
    little explanation.</st> <st c="45381">You will have probably encountered them
    before.</st> <st c="45429">However, a more thorough understanding of these concepts
    is crucial if we’re not to be misled by them when we’re building predictive models.</st>
    <st c="45570">That thorough understanding has required us to learn additional
    concepts.</st> <st c="45644">Across the whole chapter, the concepts we have learned
    about included</st> <st c="45714">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45728">Model complexity and how we broadly think of this as being related
    to the number of parameters in</st> <st c="45827">a model</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="45834">Overfitting to the noise in a dataset and how it increases as
    we increase the complexity of</st> <st c="45927">a model</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="45934">Underfitting to the general trends in a dataset and how it increases
    as we decrease the complexity of</st> <st c="46037">a model</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46044">Generalization and how a model that generalizes well is the one
    that makes accurate predictions on</st> <st c="46144">unseen data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46155">The bias-variance trade-off and how it makes mathematically precise
    the ideas behind how the minimum in the generalization</st> <st c="46279">error
    arises</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46291">A classical picture of how model complexity affects the</st> <st
    c="46348">generalization error</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46368">That over-parameterized machine learning models such as neural
    networks have revealed the phenomenon of</st> <st c="46473">double descent</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46487">Model selection and how we use model complexity measures to select
    between different</st> <st c="46573">model classes</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46586">The</st> **<st c="46591">Akaike Information Criterion</st>** <st
    c="46619">(</st>**<st c="46621">AIC</st>**<st c="46624">) as a model</st> <st
    c="46638">selection measure</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46655">The</st> **<st c="46660">Bayesian Information Criterion</st>**
    <st c="46690">(</st>**<st c="46692">BIC</st>**<st c="46695">) as a model</st>
    <st c="46709">selection measure</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46726">Our next chapter is another self-contained topic, function decomposition.</st>
    <st c="46801">In that chapter, we will learn mathematical tools and tricks to
    build up</st> <st c="46874">a function.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46885">Notes and further reading</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="46911">The NeurIPS 2023 conference paper on double descent we referred
    to in this chapter is</st> *<st c="46998">A U-turn on Double Descent: Rethinking
    Parameter Counting in Statistical Learning</st>*<st c="47079">, by A.</st> <st
    c="47087">Curth, A.</st> <st c="47097">Jeffares, and M.</st> <st c="47114">van
    der Schaar.</st> <st c="47130">A preprint version of the paper can be found on
    the arXiv archive</st> <st c="47196">at</st> [<st c="47199">https://arxiv.org/pdf/2310.18988.pdf</st>](https://arxiv.org/pdf/2310.18988.pdf)<st
    c="47235">.</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
