<html><head></head><body>
		<div id="_idContainer4131">
			<h1 id="_idParaDest-334" class="chapter-number"><a id="_idTextAnchor612"/><st c="0">1</st><a id="_idTextAnchor613"/><st c="2">2</st></h1>
			<h1 id="_idParaDest-335"><a id="_idTextAnchor614"/><st c="3">Kernel Methods</st></h1>
			<p><st c="17">Our remaining chapters will now focus on more advanced topics. </st><st c="81">Due to their advanced nature, we will not attempt to cover them in the same level of detail as we have done for the topics of earlier chapters. </st><st c="225">Instead, we will focus on getting the essential concepts and ideas behind these topics across. </st><st c="320">The aim is not to make you an expert in these topics but to introduce you to them so that you can recognize them when you see them again, or if you want to learn more at a later date. </st><st c="504">This focus on the essentials means that each of these chapters on advanced topics will be shorter than </st><span class="No-Break"><st c="607">previous chapters.</st></span></p>
			<p><st c="625">The first of our advanced topics </st><a id="_idIndexMarker1021"/><st c="659">is </st><strong class="bold"><st c="662">kernel methods</st></strong><st c="676">. Kernel methods, or </st><strong class="bold"><st c="697">kernelized learning algorithms</st></strong><st c="727">, are very widely used. </st><st c="751">The math they are based on is both advanced and</st><a id="_idIndexMarker1022"/><st c="798"> elegant. </st><st c="808">That math relates to machine learning algorithms that make use of similarities between feature vectors. </st><st c="912">To understand kernel methods, we will need to understand why similarity-based learning algorithms are common. </st><st c="1022">We will also need to understand the main principles behind the elegant math of kernel functions. </st><st c="1119">To do that we will cover the </st><span class="No-Break"><st c="1148">following topics:</st></span></p>
			<ul>
				<li><em class="italic"><st c="1165">The role of inner-products in common learning algorithms</st></em><st c="1222">: In this section, we will see why inner products are at the heart of many machine </st><span class="No-Break"><st c="1306">learning algorithms</st></span></li>
				<li><em class="italic"><st c="1325">The kernel trick</st></em><st c="1342">: In this section, we will learn about kernel functions and how kernel functions allow us to compute inner products in new feature spaces implicitly and </st><span class="No-Break"><st c="1496">very simply</st></span></li>
				<li><em class="italic"><st c="1507">An example kernelized learning algorithm</st></em><st c="1548">: In this section, we will see, using a code example, the simplicity and power of a kernelized </st><span class="No-Break"><st c="1644">learning algorithm</st></span></li>
			</ul>
			<h1 id="_idParaDest-336"><a id="_idTextAnchor615"/><st c="1662">Technical requirements</st></h1>
			<p><st c="1685">All code examples given in this chapter can be found at the GitHub repository at </st><a href="https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter12"><st c="1767">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter12</st></a><st c="1871">. To run the Jupyter notebooks you will need a full Python installation, including the </st><span class="No-Break"><st c="1958">following packages:</st></span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline"><st c="1977">pandas</st></strong></span><span class="No-Break"><st c="1984"> (&gt;=2.0.3)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="1994">numpy</st></strong></span><span class="No-Break"><st c="2000"> (&gt;=1.24.3)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2011">scikit-learn</st></strong></span><span class="No-Break"><st c="2024"> (&gt;=1.3.0)</st></span></li>
				<li><span class="No-Break"><strong class="source-inline"><st c="2034">matplotlib</st></strong></span><span class="No-Break"><st c="2045"> (&gt;=3.7.2)</st></span></li>
			</ul>
			<h1 id="_idParaDest-337"><a id="_idTextAnchor616"/><st c="2055">The role of inner products in common learning algorithms</st></h1>
			<p><st c="2112">In </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="2116">Chapter 3</st></em></span></a><st c="2125">, we introduced</st><a id="_idIndexMarker1023"/><st c="2140"> the </st><strong class="bold"><st c="2145">Principal Component Analysis</st></strong><st c="2173"> (</st><strong class="bold"><st c="2175">PCA</st></strong><st c="2178">) unsupervised learning algorithm and showed how all the calculations in PCA could be expressed in terms of inner products between the feature vectors of the different points in the </st><span class="No-Break"><st c="2361">training data.</st></span></p>
			<p><st c="2375">In </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="2379">Chapter 3</st></em></span></a><st c="2388">, we also explained that the inner product </st><img src="image/3796.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.075em;width:1.375em"/><st c="2431"/><st c="2432"> between vectors </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.477em"/><st c="2449"/><st c="2450"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.446em"/><st c="2455"/><st c="2456"> gives a measure of how similar vectors </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.470em"/><st c="2496"/><st c="2497"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="2502"/><st c="2503"> are to each other. </st><st c="2523">Since many learning algorithms are based on the idea that similar datapoints behave similarly, it is not surprising that PCA and indeed many other learning algorithms make use of </st><span class="No-Break"><st c="2702">inner products.</st></span></p>
			<p><st c="2717">Like PCA, many </st><a id="_idIndexMarker1024"/><st c="2733">classical statistical and machine learning </st><a id="_idIndexMarker1025"/><st c="2776">algorithms can be expressed solely in </st><a id="_idIndexMarker1026"/><st c="2814">terms of inner </st><a id="_idIndexMarker1027"/><st c="2829">products, such as </st><strong class="bold"><st c="2847">Linear Discriminant Analysis</st></strong><st c="2875"> (</st><strong class="bold"><st c="2877">LDA</st></strong><st c="2880">), </st><strong class="bold"><st c="2884">Fisher Discriminant Analysis</st></strong><st c="2912"> (</st><strong class="bold"><st c="2914">FDA</st></strong><st c="2917">), </st><strong class="bold"><st c="2921">Canonical Correlation Analysis</st></strong><st c="2951"> (</st><strong class="bold"><st c="2953">CCA</st></strong><st c="2956">), and </st><strong class="bold"><st c="2964">Support Vector Machines</st></strong><st c="2987"> (</st><strong class="bold"><st c="2989">SVMs</st></strong><st c="2993">). </st><st c="2997">We will </st><a id="_idIndexMarker1028"/><st c="3005">refer to these types of algorithms as </st><strong class="bold"><st c="3043">inner-product based </st></strong><span class="No-Break"><strong class="bold"><st c="3063">learning algorithms</st></strong></span><span class="No-Break"><st c="3082">.</st></span></p>
			<p><st c="3083">Given the prevalence of inner-product based learning algorithms, it is natural to ask whether inner products between the feature vectors in a training dataset are all we need. </st><st c="3260">The answer is </st><span class="No-Break"><st c="3274">more subtle.</st></span></p>
			<h2 id="_idParaDest-338"><a id="_idTextAnchor617"/><st c="3286">Sometimes we need new features in our inner products</st></h2>
			<p><st c="3339">We can only calculate inner products between datapoints using the features we already have. </st><st c="3432">OK, but we already know this. </st><st c="3462">What is the big deal? </st><st c="3484">Sometimes this is OK and we don’t need to construct any new features. </st><st c="3554">Sometimes it </st><span class="No-Break"><st c="3567">is not.</st></span></p>
			<p><st c="3574">Imagine that we are trying to construct a linear discriminant. </st><st c="3638">A linear discriminant</st><a id="_idIndexMarker1029"/><st c="3659"> is a simple algorithm that constructs a decision rule in the form of a linear combination of the features. </st><st c="3767">The linear discriminant is essentially a line drawn in the feature space. </st><st c="3841">If a point falls on one side of the line, we classify it as being in one class, and if it falls on the other side of the line, we classify it as being in the other class. </st><span class="No-Break"><em class="italic"><st c="4012">Figure 12</st></em></span><em class="italic"><st c="4021">.1</st></em><st c="4023"> shows a simple example. </st><st c="4048">We have just two features, </st><img src="image/3801.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.742em"/><st c="4075"/><st c="4076"> and </st><img src="image/3802.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.742em"/><st c="4081"/><st c="4082">. Our two classes are the red class and the blue class. </st><st c="4138">You can easily see from </st><span class="No-Break"><em class="italic"><st c="4162">Figure 12</st></em></span><em class="italic"><st c="4171">.1</st></em><st c="4173"> that the class that a given point is in is completely determined by which side of the straight line </st><img src="image/3803.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.333em;height:0.781em;width:3.325em"/><st c="4274"/><st c="4275"> the point is on. </st><st c="4293">In other words, knowing whether the value of the </st><img src="image/3804.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.819em;width:2.588em"/><st c="4342"/><st c="4350"> linear combination is greater than or less than </st><strong class="source-inline"><st c="4398">0</st></strong><st c="4399"> is enough to determine the class. </st><st c="4434">In this case, we would say that the classes </st><a id="_idIndexMarker1030"/><st c="4478">are </st><strong class="bold"><st c="4482">linearly separable</st></strong><st c="4500">. The existing features, </st><img src="image/3805.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.703em"/><st c="4525"/><st c="4526"> and </st><img src="image/3806.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.797em"/><st c="4531"/><st c="4532">, that we have on our dataset are enough to construct an accurate discriminant function from any training data. </st><st c="4644">We don’t need to construct any </st><span class="No-Break"><st c="4675">additional fea</st><a id="_idTextAnchor618"/><st c="4689">tures.</st></span></p>
			<div>
				<div id="_idContainer3943" class="IMG---Figure">
					<img src="image/B19496_12_1.jpg" alt="" role="presentation"/><st c="4696"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="4756">Figure 12.1: Existing features can linearly separate the red and blue classes</st></p>
			<p><st c="4833">This is not always the case. </st><st c="4863">Take the </st><a id="_idIndexMarker1031"/><st c="4872">example in </st><span class="No-Break"><em class="italic"><st c="4883">Figure 12</st></em></span><em class="italic"><st c="4892">.2</st></em><st c="4894">. Clearly, the two classes, red and blue, are still separable by a simple boundary. </st><st c="4978">However, that boundary is not a line. </st><st c="5016">We would say that the two classes in this second example are not </st><span class="No-Break"><st c="5081">linearly sepa</st><a id="_idTextAnchor619"/><st c="5094">rable.</st></span></p>
			<div>
				<div id="_idContainer3944" class="IMG---Figure">
					<img src="image/B19496_12_2.jpg" alt="" role="presentation"/><st c="5101"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5163">Figure 12.2: Two classes that cannot be linearly separated by existing features</st></p>
			<p><st c="5242">You may be able to notice that the boundary separating the red and blue dots in </st><span class="No-Break"><em class="italic"><st c="5323">Figure 12</st></em></span><em class="italic"><st c="5332">.2</st></em><st c="5334"> is a circle and is given by the </st><img src="image/3807.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:4.601em"/><st c="5367"/><st c="5383"> relation. </st><st c="5393">So, knowing whether the value of </st><img src="image/3808.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:2.774em"/><st c="5426"/><st c="5433"> is greater than or less than </st><strong class="source-inline"><st c="5462">0.5</st></strong><st c="5465"> is enough to tell us which class each point is in. </st><st c="5517">We can do this calculation if we have the values of </st><img src="image/3809.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:0.838em"/><st c="5569"/><st c="5570"> and </st><img src="image/3810.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:0.838em"/><st c="5575"/><st c="5576"> in our training dataset. </st><st c="5602">OK, calculating </st><img src="image/3809.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:0.838em"/><st c="5618"/><st c="5619"> and </st><img src="image/3810.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:0.838em"/><st c="5624"/><st c="5625"> from the values of </st><img src="image/3234.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.745em"/><st c="5645"/><st c="5646">and </st><img src="image/3814.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.745em"/><st c="5650"/><st c="5651"> is not difficult. </st><st c="5670">However, it does illustrate that to correctly separate the two classes, we would have to construct some new feature values. </st><st c="5794">Moreover, the example we have shown in </st><span class="No-Break"><st c="5833">Figure 12</st></span><st c="5842">.2 is a very simple one. </st><st c="5867">In real-world datasets, we will not be able to simply visualize the data and spot the new features required to separate the classes. </st><st c="6000">In real-world datasets, we must go through a lengthy process of constructing new features and trying them out. </st><st c="6111">This can be inefficient. </st><st c="6136">Fortunately, using the </st><strong class="bold"><st c="6159">kernel trick</st></strong><st c="6171"> can</st><a id="_idIndexMarker1032"/><st c="6175"> help us perform this feature construction process implicitly and allows us to explore whole families of new features in a very simple way. </st><st c="6315">We will learn about the kernel trick in a moment, but for now, let’s summarize what we have learned in </st><span class="No-Break"><st c="6418">this section.</st></span></p>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor620"/><st c="6431">What we learned</st></h2>
			<p><st c="6447">In this section we have learned </st><span class="No-Break"><st c="6480">the following:</st></span></p>
			<ul>
				<li><st c="6494">Many machine learning algorithms are based on computing the inner product between datapoints in the </st><span class="No-Break"><st c="6595">training dataset.</st></span></li>
				<li><st c="6612">The existing features in a dataset may not be sufficient to construct an accurate learning algorithm and so new features need to </st><span class="No-Break"><st c="6742">be constructed.</st></span></li>
			</ul>
			<p><st c="6757">Having highlighted the issue that we may need to construct new features to make our inner-product based learning algorithms work, in the next section, we will learn about the kernel trick and how it can allow us to do feature </st><span class="No-Break"><st c="6984">construction implicitly.</st></span></p>
			<h1 id="_idParaDest-340"><a id="_idTextAnchor621"/><st c="7008">The kernel trick</st></h1>
			<p><st c="7025">To learn how the </st><a id="_idIndexMarker1033"/><st c="7043">kernel trick allows us to do feature construction implicitly and efficiently, we will first have to learn what a </st><span class="No-Break"><st c="7156">kernel is.</st></span></p>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor622"/><st c="7166">What is a kernel?</st></h2>
			<p><st c="7184">The simplest way to think</st><a id="_idIndexMarker1034"/><st c="7210"> about a kernel is to consider it as a mapping that takes two vectors as input and returns a scalar. </st><st c="7311">It is a mapping that maps </st><img src="image/3815.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.028em;height:0.776em;width:5.741em"/><st c="7337"/><st c="7347">. This means that a kernel is a function </st><img src="image/3816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:3.662em"/><st c="7388"/><st c="7397">, with the input vectors being </st><img src="image/3817.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:1.191em"/><st c="7428"/><st c="7429"> and </st><img src="image/3818.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:1.362em"/><st c="7434"/><st c="7435"> The value of </st><img src="image/126.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.276em"/><st c="7449"/><st c="7503"> is a real number. </st><st c="7521">This means that the inner product </st><img src="image/3820.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.075em;width:2.951em"/><st c="7555"/><st c="7556"> is an example of a </st><span class="No-Break"><st c="7576">kernel function.</st></span></p>
			<p><st c="7592">That is a high-level mathematical definition of what a kernel is, but what is the intuition behind this? </st><st c="7698">An </st><img src="image/3821.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:2.304em"/><st c="7701"/><st c="7702"> kernel function applied to the vectors </st><img src="image/2281.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.496em"/><st c="7742"/><st c="7743"> and </st><img src="image/3823.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.465em"/><st c="7748"/><st c="7749"> is typically used to measure the similarity between those vectors. </st><st c="7817">Consequently, we usually want our kernel function to have its largest values when </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.467em"/><st c="7899"/><st c="7900"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.437em"/><st c="7905"/><st c="7906"> are most similar and its lowest values when </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.467em"/><st c="7951"/><st c="7952"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.437em"/><st c="7957"/><st c="7958"> are least similar. </st><st c="7978">We want the function to decrease smoothly and monotonically in between those </st><span class="No-Break"><st c="8055">two scenarios.</st></span></p>
			<h2 id="_idParaDest-342"><a id="_idTextAnchor623"/><st c="8069">Commonly used kernels</st></h2>
			<p><st c="8091">You may recall from </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="8112">Chapter 3</st></em></span></a><st c="8121"> that the simple inner product </st><img src="image/3796.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.075em;width:1.364em"/><st c="8152"/><st c="8153"> between two vectors </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="8174"/><st c="8175"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="8180"/><st c="8181"> is also called the </st><strong class="bold"><st c="8201">dot-product</st></strong><st c="8212"> because</st><a id="_idIndexMarker1035"/><st c="8220"> we can also write it as </st><img src="image/3831.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:1.640em"/><st c="8245"/><st c="8249"> . Consequently, the </st><img src="image/3832.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.125em;width:4.651em"/><st c="8269"/><st c="8270"> function is</st><a id="_idIndexMarker1036"/><st c="8282"> an example of a </st><strong class="bold"><st c="8299">dot-product kernel</st></strong><st c="8317">. More </st><a id="_idIndexMarker1037"/><st c="8324">generally, a dot-product kernel is any function of </st><img src="image/3831.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:1.640em"/><st c="8375"/><st c="8379">. So, the function that follows, where </st><img src="image/1680.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.419em"/><st c="8418"/><st c="8419"> is any univariate function, is a </st><span class="No-Break"><st c="8453">dot-product kernel:</st></span></p>
			<p><img src="image/3835.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.118em;width:6.773em"/><st c="8472"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8488">Eq.1</st></p>
			<p><st c="8492">Commonly used forms of dot-product kernels are where the function </st><img src="image/3836.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.941em;width:5.678em"/><st c="8559"/><st c="8576">. These are called polynomial dot-product kernels because the resulting kernel function </st><img src="image/2990.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.253em"/><st c="8664"/><st c="8665"> is a polynomial of the dot-product </st><img src="image/3831.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:1.640em"/><st c="8701"/><st c="8705"> .</st></p>
			<p><st c="8706">Another commonly used class of kernel</st><a id="_idIndexMarker1038"/><st c="8744"> functions is</st><a id="_idIndexMarker1039"/><st c="8757"> the </st><strong class="bold"><st c="8762">translationally invariant</st></strong><st c="8787"> kernels. </st><st c="8797">These are kernels of the </st><span class="No-Break"><st c="8822">following form:</st></span></p>
			<p><img src="image/3839.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.118em;width:6.968em"/><st c="8837"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8839">Eq.2</st></p>
			<p><st c="8843">They are called translationally invariant because translating the </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="8910"/><st c="8911"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="8916"/><st c="8917"> vectors by adding a constant vector </st><img src="image/469.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.485em"/><st c="8954"/><st c="8955"> to both does not change the value output by the kernel function. </st><st c="9021">One of the </st><a id="_idIndexMarker1040"/><st c="9032">most used kernels of this type is the </st><strong class="bold"><st c="9070">squared-exponential kernel</st></strong><st c="9096">, as </st><span class="No-Break"><st c="9101">seen</st></span><span class="No-Break"><a id="_idIndexMarker1041"/></span><span class="No-Break"><st c="9105"> her</st><a id="_idTextAnchor624"/><st c="9109">e:</st></span></p>
			<p><img src="image/3843.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.563em;height:1.577em;width:11.421em"/><st c="9112"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="9141">Eq.3</st></p>
			<p><st c="9145">This kernel is also </st><a id="_idIndexMarker1042"/><st c="9166">known as</st><a id="_idIndexMarker1043"/><st c="9174"> the </st><strong class="bold"><st c="9179">Radial Basis Function</st></strong><st c="9200"> (</st><strong class="bold"><st c="9202">RBF</st></strong><st c="9205">) kernel. </st><st c="9216">It is also sometimes called</st><a id="_idIndexMarker1044"/><st c="9243"> the </st><strong class="bold"><st c="9248">Gaussian kernel</st></strong><st c="9263">. The </st><img src="image/286.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.723em;width:0.486em"/><st c="9269"/><st c="9270"> parameter in </st><em class="italic"><st c="9284">Eq.3</st></em><st c="9288"> provides a scale on which kernel</st><a id="_idIndexMarker1045"/><st c="9321"> function </st><img src="image/126.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.271em"/><st c="9331"/><st c="9385"> measures the differences between the </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="9422"/><st c="9423"> vector and the </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="9439"/><st c="9440"> vector . </st><st c="9450">The smaller the value of </st><img src="image/3848.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.117em;height:0.828em;width:0.713em"/><st c="9475"/><st c="9476"> the smaller the output value of </st><img src="image/126.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.270em"/><st c="9509"/><st c="9563"> for the given </st><span class="No-Break"><st c="9577">vectors </st></span><span class="No-Break"><img src="image/3850.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;≠&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.357em;height:0.871em;width:2.208em"/><st c="9585"/></span><span class="No-Break"><st c="9586">.</st></span></p>
			<p><st c="9587">You’ll also realize that if in addition, we restrict the vectors </st><img src="image/3851.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:1.267em"/><st c="9653"/><st c="9654"> to have a fixed length ( say </st><strong class="source-inline"><st c="9684">1</st></strong><st c="9685">) so that they live on the surface of the unit-sphere, then the kernel function in </st><em class="italic"><st c="9768">Eq.3</st></em><st c="9772"> will look </st><span class="No-Break"><st c="9783">as fo</st><a id="_idTextAnchor625"/><st c="9788">llows:</st></span></p>
			<p><img src="image/3852.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.563em;height:1.577em;width:23.698em"/><st c="9795"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="9797">Eq.4</st></p>
			<p><st c="9801">The right-hand side of </st><em class="italic"><st c="9825">Eq.4</st></em><st c="9829"> is an example of a dot-product kernel. </st><st c="9869">By restricting the space from which the </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.471em"/><st c="9909"/><st c="9910"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.440em"/><st c="9915"/><st c="9916"> vectors come, we can change the properties of the kernel function. </st><st c="9984">In the example in </st><em class="italic"><st c="10002">Eq.4</st></em><st c="10006">, we have changed a translationally invariant kernel to a dot-product kernel. </st><st c="10084">In fact, using the usual Taylor series representation of the exponential function, we can write the right-hand side of </st><em class="italic"><st c="10203">Eq.4</st></em> <span class="No-Break"><st c="10207">as fo</st><a id="_idTextAnchor626"/><st c="10213">llows:</st></span></p>
			<p><img src="image/3855.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;exp&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;!&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;/mfenced&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.809em;height:2.253em;width:21.834em"/><st c="10220"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="10222">Eq.5</st></p>
			<p><st c="10226">So, when the </st><img src="image/3856.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.538em"/><st c="10240"/><st c="10241"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.434em"/><st c="10246"/><st c="10247"> vectors lie on the surface of the unit-sphere, we can think of the squared-exponential kernel as being the same as a polynomial dot-product kernel of </st><span class="No-Break"><st c="10398">infinite order.</st></span></p>
			<p><st c="10413">Perhaps more interesting is the fact that the kernel in </st><em class="italic"><st c="10470">Equations 3 and 4</st></em><st c="10487"> has a parameter, </st><img src="image/3848.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.117em;height:0.828em;width:0.720em"/><st c="10505"/><st c="10506"> that we as a user specify and can vary. </st><st c="10547">In fact, our simple polynomial dot-product kernel </st><img src="image/3859.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.118em;width:7.784em"/><st c="10597"/><st c="10615"> had user-specified parameters. </st><st c="10646">We can think of both </st><img src="image/3860.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.383em"/><st c="10667"/><st c="10668"> and </st><img src="image/2008.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.487em"/><st c="10673"/><st c="10674"> as being parameters of the polynomial dot-product kernel that we can vary. </st><st c="10750">It is the fact that our kernel functions have parameters that we can vary that allows us to explore whole classes of new features efficiently and implicitly. </st><st c="10908">We will learn about this later in </st><span class="No-Break"><st c="10942">this section.</st></span></p>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor627"/><st c="10955">Kernel functions for other mathematical objects</st></h2>
			<p><st c="11003">Can we generalize </st><a id="_idIndexMarker1046"/><st c="11022">kernels further? </st><st c="11039">Yes, we can. </st><st c="11052">We can usually reduce any mathematical object to a set of numbers, whether real or complex. </st><st c="11144">For example, a matrix is just a set of matrix elements, that is, a set of numbers, but they are usually arranged in a rectangular structure. </st><st c="11285">It is the rectangular arrangement of the matrix elements that gives the matrix its overall interesting properties, but it is still just a set of numbers. </st><st c="11439">That means we can store or represent the matrix as one long vector of numbers. </st><st c="11518">Consequently, we can in principle construct a kernel function that takes two matrices as inputs and returns a scalar value. </st><st c="11642">The precise details of how that matrix kernel function is constructed are up to us. </st><st c="11726">For example, if we had two </st><img src="image/733.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:2.489em"/><st c="11753"/><st c="11754"> symmetric real matrices, </st><img src="image/607.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.201em;height:0.863em;width:0.663em"/><st c="11780"/><st c="11781"> and </st><img src="image/573.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.201em;height:0.848em;width:0.583em"/><st c="11786"/><st c="11787">, we could define a function that measures the distance or dissimilarity between them. </st><st c="11874">An example dissimilarity function might take the </st><span class="No-Break"><st c="11923">followin</st><a id="_idTextAnchor628"/><st c="11931">g form:</st></span></p>
			<p><img src="image/3865.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mtext&gt;dissimilarity&lt;/mtext&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;tr&lt;/mtext&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.301em;height:1.068em;width:16.239em"/><st c="11939"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11983">Eq.6</st></p>
			<p><st c="11987">The expression in </st><em class="italic"><st c="12006">Eq.6</st></em><st c="12010"> is the square of</st><a id="_idIndexMarker1047"/><st c="12027"> the </st><strong class="bold"><st c="12032">Frobenius norm</st></strong><st c="12046"> of the </st><img src="image/3866.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.201em;height:0.863em;width:2.291em"/><st c="12054"/><st c="12061"> matrix. </st><st c="12069">The Frobenius norm is a standard matrix norm or way of measuring the size of a matrix. </st><st c="12156">So, in </st><em class="italic"><st c="12163">Eq.6</st></em><st c="12167">, we are measuring the size of the difference between </st><img src="image/564.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.201em;height:0.863em;width:0.670em"/><st c="12221"/><st c="12222"> and </st><img src="image/580.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.201em;height:0.848em;width:0.627em"/><st c="12227"/><st c="12228">. From the dissimilarity measure in </st><em class="italic"><st c="12264">Eq.6</st></em><st c="12268">, we can define similarity as </st><strong class="source-inline"><st c="12298">1</st></strong><st c="12299"> minus the dissimilarity. </st><st c="12325">Since we tend to use kernel functions for measuring similarity that would give us a kernel function for matrices of the </st><span class="No-Break"><st c="12445">followi</st><a id="_idTextAnchor629"/><st c="12452">ng form:</st></span></p>
			<p><img src="image/3869.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mtext&gt;tr&lt;/mtext&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;munder&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.301em;height:1.068em;width:13.585em"/><st c="12461"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="12463">Eq.7</st></p>
			<p><st c="12467">What this illustrates</st><a id="_idIndexMarker1048"/><st c="12489"> is that we can construct kernel functions on spaces of any mathematical objects. </st><st c="12571">We can consider a kernel function to be, more generally, a function that takes two objects, </st><img src="image/3870.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:1.045em"/><st c="12663"/><st c="12664"> and </st><img src="image/3871.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:1.052em"/><st c="12669"/><st c="12670">, as input and returns and scalar value. </st><st c="12711">The mathematical objects </st><img src="image/3872.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:1.069em"/><st c="12736"/><st c="12737"> and </st><img src="image/3873.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:1.039em"/><st c="12742"/><st c="12743"> could be two matrices, two graphs, two strings, or two documents, as long as we can ultimately represent them mathematically. </st><st c="12870">This means that when we use the kernel trick to implicitly construct new features, we can use it to implicitly construct new features for matrices, graphs, strings, documents, and so on. </st><st c="13057">This gives us a powerful tool for extending learning algorithms that we typically think of as only applying to vectors and getting those learning algorithms to work for more exotic classes </st><span class="No-Break"><st c="13246">of objects.</st></span></p>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor630"/><st c="13257">Combining kernels</st></h2>
			<p><st c="13275">The</st><a id="_idIndexMarker1049"/><st c="13279"> kernel functions in </st><em class="italic"><st c="13300">Eq.1</st></em><st c="13304"> – </st><em class="italic"><st c="13307">3</st></em><st c="13308"> are very simple. </st><st c="13326">This does not mean that all kernels have a mathematically simple form. </st><st c="13397">In fact, sometimes we want to construct more complex kernel functions from simpler ones by applying mathematical operations to combine several simple </st><span class="No-Break"><st c="13547">kernel functions.</st></span></p>
			<p><st c="13564">The simplest mathematical operation we can consider is to just combine them linearly. </st><st c="13651">That is, given two kernel functions, </st><img src="image/3874.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.044em;width:2.354em"/><st c="13688"/><st c="13694"> and </st><img src="image/3875.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.174em;width:2.575em"/><st c="13698"/><st c="13699">, that operate on mathematical objects </st><img src="image/3876.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.458em"/><st c="13738"/><st c="13739"> and </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="13744"/><st c="13767">, we can construct a </st><span class="No-Break"><st c="13788">new function:</st></span></p>
			<p><img src="image/3878.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:1.051em;width:11.243em"/><st c="13801"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="13828">Eq.8</st></p>
			<p><st c="13832">If the </st><img src="image/3879.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.044em;width:0.513em"/><st c="13840"/><st c="13841"> and </st><img src="image/3880.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.044em;width:0.513em"/><st c="13846"/><st c="13847"> functions are valid kernel functions, then so is the </st><img src="image/3881.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.340em;height:1.051em;width:0.513em"/><st c="13901"/><st c="13902"> function. </st><st c="13913">Linear combination of kernel functions is also usually the most complex mathematical combination operation we are likely </st><span class="No-Break"><st c="14034">to perform.</st></span></p>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor631"/><st c="14045">Positive semi-definite kernels</st></h2>
			<p><st c="14076">Having introduced what a kernel function is, we’ll now highlight a specific subset of kernels: the positive semi-definite kernels. </st><st c="14208">These are the kernel functions that we will use in the </st><span class="No-Break"><st c="14263">kernel trick.</st></span></p>
			<h3><st c="14276">What is a positive semi-definite kernel?</st></h3>
			<p><st c="14317">We have already </st><a id="_idIndexMarker1050"/><st c="14334">said that </st><a id="_idIndexMarker1051"/><st c="14344">we can think of a kernel as taking mathematical objects </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="14400"/><st c="14401"> and </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="14406"/><st c="14429"> that live in some mathematical space and mapping them to a scalar value. </st><st c="14502">We can also use a kernel function to perform a mapping within the space from which </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.468em"/><st c="14585"/><st c="14586"> and </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.437em"/><st c="14591"/><st c="14614"> come. </st><st c="14620">Consider the calculation </st><span class="No-Break"><st c="14645">t</st><a id="_idTextAnchor632"/><st c="14646">hat follows:</st></span></p>
			<p><img src="image/3886.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.307em;height:1.097em;width:6.882em"/><st c="14658"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14672">Eq.9</st></p>
			<p><st c="14676">The integral in </st><em class="italic"><st c="14693">Eq.9</st></em><st c="14697"> is over the space from which the mathematical objects </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.483em"/><st c="14752"/><st c="14753"> and </st><img src="image/769.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.452em"/><st c="14758"/><st c="14768"> are drawn. </st><st c="14779">The result of the integration in </st><em class="italic"><st c="14812">Eq.9</st></em><st c="14816"> is a function of </st><img src="image/3889.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.431em"/><st c="14834"/><st c="14835">, so we can think of the integration in </st><em class="italic"><st c="14875">Eq.9</st></em><st c="14879"> as mapping the </st><img src="image/3890.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.098em;height:0.703em;width:1.603em"/><st c="14895"/><st c="14896"> function to some function of </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.440em"/><st c="14926"/><st c="14949">. The </st><img src="image/3892.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ρ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.827em;width:1.634em"/><st c="14955"/><st c="14960"> function is a weighting or measure function. </st><st c="15005">Often, we take </st><img src="image/3892.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;ρ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.827em;width:1.641em"/><st c="15020"/><st c="15025"> to be some probability density function that gives the probability of getting the object </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="15114"/><st c="15115"> in the mathematical space we are </st><span class="No-Break"><st c="15149">dealing with.</st></span></p>
			<p><st c="15162">Now, what if the output function of </st><img src="image/24.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.705em;width:0.441em"/><st c="15199"/><st c="15222"> that we got from the integration in </st><em class="italic"><st c="15258">Eq.9</st></em><st c="15262"> was just proportional to the </st><img src="image/3896.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.098em;height:0.703em;width:1.611em"/><st c="15292"/><st c="15293"> input function? </st><st c="15310">We would have the specific result </st><span class="No-Break"><st c="15344">th</st><a id="_idTextAnchor633"/><st c="15346">at follows:</st></span></p>
			<p><img src="image/3897.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.305em;height:1.095em;width:9.955em"/><st c="15358"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15380">Eq.10</st></p>
			<p><st c="15385">Hang on a minute! </st><st c="15404">We </st><a id="_idIndexMarker1052"/><st c="15407">recognize </st><em class="italic"><st c="15417">Eq.10</st></em><st c="15422">. It is our old friend, the </st><strong class="bold"><st c="15450">eigenfunction equation</st></strong><st c="15472">. This means that if we can find particular functions </st><img src="image/3898.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.098em;height:0.703em;width:1.583em"/><st c="15526"/><st c="15527"> that satisfy </st><em class="italic"><st c="15541">Eq.10</st></em><st c="15546">, then </st><img src="image/3899.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.098em;height:0.703em;width:1.582em"/><st c="15553"/><st c="15554"> is said to be an </st><strong class="bold"><st c="15572">eigenfunction</st></strong><st c="15585"> of the </st><img src="image/3900.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.307em;height:1.018em;width:2.230em"/><st c="15593"/><st c="15601"> kernel and the constant of proportionality, </st><img src="image/826.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.723em;width:0.486em"/><st c="15645"/><st c="15646">, in </st><em class="italic"><st c="15651">Eq.10</st></em><st c="15656"> is the </st><a id="_idIndexMarker1053"/><span class="No-Break"><st c="15664">corresponding </st></span><span class="No-Break"><strong class="bold"><st c="15678">eigenvalue</st></strong></span><span class="No-Break"><st c="15688">.</st></span></p>
			<p><st c="15689">How does this help us? </st><st c="15713">The eigenfunctions and eigenvalues help us characterize the kernel. </st><st c="15781">There is one characteristic we are interested in for the kernel trick, and that is positive definiteness. </st><st c="15887">A kernel function is said to be </st><strong class="bold"><st c="15919">positive definite</st></strong><st c="15936"> if all its eigenvalues are</st><a id="_idIndexMarker1054"/><st c="15963"> greater than zero, and </st><strong class="bold"><st c="15987">positive semi-definite</st></strong><st c="16009"> if all its eigenvalues are non-negative. </st><st c="16051">From the fact that all the eigenvalues are non-negative, we can show that the property of positive semi-definiteness is equivalent to </st><a id="_idIndexMarker1055"/><st c="16185">the relationship</st><a id="_idTextAnchor634"/> <span class="No-Break"><st c="16201">that follows:</st></span></p>
			<p><img src="image/3902.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;∫&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;≥&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mtext&gt;any&lt;/mtext&gt;&lt;mtext&gt;function&lt;/mtext&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.307em;height:1.097em;width:21.469em"/><st c="16215"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="16262">Eq.11</st></p>
			<p><st c="16267">The relationship in </st><em class="italic"><st c="16288">Eq.11</st></em><st c="16293"> is the more fundamental definition of positive semi-definiteness, but the standard way to prove that a kernel function </st><img src="image/3903.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.022em;width:2.303em"/><st c="16413"/><st c="16421"> is positive semi-definite is to determine its eigenfunctions and eigenvalues and see whether any of the eigenvalues </st><span class="No-Break"><st c="16537">are negative.</st></span></p>
			<p><st c="16550">Great. </st><st c="16558">We now have all the mathematical pieces in place to introduce the </st><span class="No-Break"><st c="16624">kernel trick.</st></span></p>
			<h2 id="_idParaDest-346"><a id="_idTextAnchor635"/><st c="16637">Mercer’s theorem and the kernel trick</st></h2>
			<p><st c="16675">The kernel trick is </st><a id="_idIndexMarker1056"/><st c="16696">based upon </st><strong class="bold"><st c="16707">Mercer’s theorem</st></strong><st c="16723">. Mercer’s theorem says the following: if we have a positive semi-definite kernel function </st><img src="image/3904.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.022em;width:2.128em"/><st c="16814"/><st c="16822"> then the value of </st><img src="image/3905.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.260em"/><st c="16840"/><st c="16841"> represents the value of the inner product between </st><img src="image/3906.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.150em;height:0.807em;width:1.928em"/><st c="16892"/><st c="16893"> and </st><img src="image/3907.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.022em;width:1.897em"/><st c="16898"/><st c="16899">, where the mathematical object </st><img src="image/3906.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.150em;height:0.807em;width:1.929em"/><st c="16931"/><st c="16932"> is some mapping of the mathematical object </st><img src="image/10.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.472em"/><st c="16976"/><st c="16977"> to a new </st><span class="No-Break"><st c="16987">mathematical space.</st></span></p>
			<p><st c="17006">Okay, that definition of Mercer’s theorem is a bit abstract, so we’ll make it more concrete by going back to using </st><img src="image/596.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.723em;width:0.507em"/><st c="17122"/><st c="17123">-dimensional vectors </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="17144"/><st c="17145"> and </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="17150"/><st c="17151">. If we have a positive semi-definite kernel function, </st><span class="_-----MathTools-_Math_Variable"><st c="17206">f</st></span><span class="_-----MathTools-_Math_Base"><st c="17207">(</st></span><span class="_-----MathTools-_Math_Variable"><st c="17208">x</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="17209">_</st></span><span class="_-----MathTools-_Math_Operator"><st c="17210">,</st></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><st c="17211">y</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="17212">_</st></span><span class="_-----MathTools-_Math_Base"><st c="17213">)</st></span><st c="17214">, then the value of </st><img src="image/126.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.269em"/><st c="17234"/><st c="17288"> represents the inner product between the </st><img src="image/3914.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.909em"/><st c="17329"/><st c="17330"> and </st><img src="image/3915.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.064em;width:1.878em"/><st c="17335"/><st c="17336"> vectors, where </st><img src="image/3914.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.908em"/><st c="17352"/><st c="17353"> is the mapping of the vector </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="17383"/><st c="17384"> to a new vector </st><img src="image/3918.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.912em"/><st c="17401"/><st c="17402">, and likewise </st><img src="image/3919.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.064em;width:1.881em"/><st c="17417"/><st c="17418"> is the mapping of the vector </st><img src="image/1569.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.441em"/><st c="17448"/><st c="17449"> to a new vector </st><img src="image/3921.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.307em;height:0.964em;width:1.865em"/><st c="17466"/><st c="17467">. This means that from </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.471em"/><st c="17490"/><st c="17491">, we have constructed new features </st><img src="image/3923.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.890em"/><st c="17526"/><st c="17527"> and calculated the inner product </st><img src="image/3924.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.064em;width:4.532em"/><st c="17561"/><st c="17562"> using the </st><img src="image/3925.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;k&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;l&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;f&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;u&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;c&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;t&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;i&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.118em;width:8.392em"/><st c="17573"/><st c="17596">. In math, we are saying </st><span class="No-Break"><st c="17621">the following:</st></span></p>
			<p><img src="image/3926.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.403em;height:1.114em;width:8.038em"/><st c="17635"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17637">Eq.12</st></p>
			<p><st c="17642">However, here is the subtlety: at no point did we say what the new features </st><img src="image/3927.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.828em"/><st c="17719"/><st c="17720"> were. </st><st c="17727">Mercer’s theorem told us that we didn’t have to. </st><st c="17776">Mercer’s theorem told us that using the </st><img src="image/3928.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:2.150em"/><st c="17816"/><st c="17817"> kernel function is equivalent to implicitly computing inner products in some new feature space. </st><st c="17914">This is the </st><span class="No-Break"><st c="17926">kernel trick.</st></span></p>
			<p><st c="17939">Why is this useful? </st><st c="17960">Remember</st><a id="_idIndexMarker1057"/><st c="17968"> that many of the kernels that we have already introduced have some parameters in them. </st><st c="18056">By varying the kernel function parameters, we are effectively varying the new features that we are implicitly creating. </st><st c="18176">Varying the parameters is the same as varying or exploring across a whole set of new </st><span class="No-Break"><st c="18261">feature spaces.</st></span></p>
			<h3><st c="18276">Mercer’s theorem – an example</st></h3>
			<p><st c="18306">Let’s look at a simple but</st><a id="_idIndexMarker1058"/><st c="18333"> explicit example of Mercer’s theorem in action. </st><st c="18382">We’ll return to our features </st><img src="image/3929.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:1.937em"/><st c="18411"/><st c="18412"> in </st><em class="italic"><st c="18416">Figures 12.1</st></em><st c="18428"> and </st><em class="italic"><st c="18433">12.2</st></em><st c="18437">.  If we have two points, </st><img src="image/3930.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.383em;height:0.881em;width:4.766em"/><st c="18462"/><st c="18472"> and </st><img src="image/3931.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.383em;height:0.881em;width:4.369em"/><st c="18476"/><st c="18480">, in our original feature space, then the inner product between t</st><a id="_idTextAnchor636"/><st c="18545">hem is </st><span class="No-Break"><st c="18553">as follows:</st></span></p>
			<p><img src="image/3932.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.357em;height:0.843em;width:6.672em"/><st c="18564"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="18579">Eq.13</st></p>
			<p><st c="18584">Now we’ll explicitly construct a new feature space, </st><img src="image/3933.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.101em;height:0.757em;width:0.768em"/><st c="18637"/><st c="18638">. From the existing </st><img src="image/3934.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.383em;height:0.881em;width:4.618em"/><st c="18658"/><st c="18668"> feature vector, we define our feature vector </st><img src="image/3935.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.918em"/><st c="18713"/><st c="18714"> to be </st><span class="No-Break"><st c="18721">as follows:</st></span></p>
			<p><img src="image/3936.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msqrt&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msqrt&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.383em;height:1.237em;width:9.243em"/><st c="18732"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="18758">Eq.14</st></p>
			<p><st c="18763">Similarly, the </st><img src="image/3937.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.383em;height:0.881em;width:4.454em"/><st c="18779"/><st c="18780"> point in the original feature space maps to </st><span class="No-Break"><st c="18825">the following:</st></span></p>
			<p><img src="image/3938.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msqrt&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msqrt&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.403em;height:1.313em;width:9.768em"/><st c="18839"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="18862">Eq.15</st></p>
			<p><st c="18867">The inner product, </st><img src="image/3939.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.064em;width:4.760em"/><st c="18887"/><st c="18888"> in the new feature space is then given</st><a id="_idTextAnchor637"/><st c="18927"> by </st><span class="No-Break"><st c="18931">the following:</st></span></p>
			<p><img src="image/3940.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;msubsup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msubsup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.110em;width:20.970em"/><st c="18945"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="18997">Eq.16</st></p>
			<p><st c="19002">Comparing the right-hand side of </st><em class="italic"><st c="19036">Eq.16</st></em><st c="19041"> to </st><em class="italic"><st c="19045">Eq.13</st></em><st c="19050">, we can see that it is the same as </st><img src="image/3941.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.110em;width:2.628em"/><st c="19086"/><st c="19087">. This means that, for this example, we h</st><a id="_idTextAnchor638"/><st c="19128">ave </st><span class="No-Break"><st c="19133">the following:</st></span></p>
			<p><img src="image/3942.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.110em;width:8.252em"/><st c="19147"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="19168">Eq.17</st></p>
			<p><st c="19173">In this example, we have explicitly specified what the mapping from the original feature space </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.489em"/><st c="19269"/><st c="19270"> to the new feature space </st><img src="image/3944.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.101em;height:0.757em;width:0.753em"/><st c="19296"/><st c="19297"> was. </st><st c="19303">However, we didn’t have to. </st><em class="italic"><st c="19331">Eq.17</st></em><st c="19336"> tells us that if all we want or need to do is compute inner products in the new feature space, then we don’t have to know what the mapping from the original to the new feature space is. </st><st c="19523">We can just use the </st><img src="image/3945.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.110em;width:2.649em"/><st c="19543"/><st c="19544"> polynomial dot-product kernel to compute inner products in the new feature space. </st><st c="19627">This is the kernel trick </st><span class="No-Break"><st c="19652">in action.</st></span></p>
			<p><st c="19662">For our explicit example, the </st><img src="image/3946.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.184em;height:0.841em;width:1.890em"/><st c="19693"/><st c="19694"> mapping only produced a finite number of features. </st><st c="19746">We went from a two-dimensional feature vector to a three-dimensional feature vector. </st><st c="19831">Is this always the case? </st><st c="19856">You can probably guess by looking at </st><em class="italic"><st c="19893">Eq.17</st></em><st c="19898"> that the fact that using the </st><img src="image/3947.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.110em;width:2.739em"/><st c="19928"/><st c="19929"> polynomial dot-product kernel is equivalent to constructing a finite-dimensional feature vector stems from the fact that the order of the polynomial is finite, that is, we just have a quadratic dot-product kernel. </st><st c="20144">You’d be right. </st><st c="20160">Using a higher-order polynomial dot-product, such as </st><img src="image/3948.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.108em;width:4.584em"/><st c="20213"/><st c="20228">, will lead to more implicit features, but still a finite number </st><span class="No-Break"><st c="20293">of</st></span><span class="No-Break"><a id="_idIndexMarker1059"/></span><span class="No-Break"><st c="20295"> them.</st></span></p>
			<p><st c="20301">Wait, I hear you say. </st><em class="italic"><st c="20324">Eq.5</st></em><st c="20328"> tells us that when </st><img src="image/2281.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.497em"/><st c="20348"/><st c="20349"> and </st><img src="image/3823.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:0.805em;width:0.465em"/><st c="20354"/><st c="20355"> are on the surface of the unit-sphere, then the squared-exponential kernel is equivalent to a polynomial dot-product kernel of infinite order. </st><st c="20499">So, does using a squared-exponential kernel mean that we would implicitly be creating an infinite number of new features? </st><st c="20621">Yes, it does. </st><st c="20635">By using different kernel functions, we can explore a wide range of different new feature spaces. </st><st c="20733">This is the power </st><a id="_idIndexMarker1060"/><st c="20751">of the kernel trick – it allows us to efficiently explore new </st><span class="No-Break"><st c="20813">feature spaces.</st></span></p>
			<p><st c="20828">So, how do we use Mercer’s theorem and the kernel trick in a learning algorithm? </st><st c="20910">This is what we will </st><span class="No-Break"><st c="20931">cover next.</st></span></p>
			<h2 id="_idParaDest-347"><a id="_idTextAnchor639"/><st c="20942">Kernelized algorithms</st></h2>
			<p><st c="20964">Mercer’s theorem tells us that we</st><a id="_idIndexMarker1061"/><st c="20998"> can take a positive semi-definite kernel function </st><img src="image/3951.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.968em;width:0.292em"/><st c="21049"/><st c="21050"> and use it to calculate the inner-product values in our inner-product based learning algorithm. </st><st c="21147">This is often </st><a id="_idIndexMarker1062"/><st c="21161">called </st><strong class="bold"><st c="21168">kernelizing</st></strong> <span class="No-Break"><st c="21179">the algorithm.</st></span></p>
			<p><st c="21194">Take PCA as an example of an inner-product based learning algorithm. </st><st c="21264">We know from Mercer’s theorem that computing the inner products using a kernel function will be equivalent to doing the PCA in some new feature space, even though we do not (necessarily) know what that new feature space is. </st><st c="21488">The new kernelized version of the learning algorithm is called</st><a id="_idIndexMarker1063"/> <span class="No-Break"><strong class="bold"><st c="21550">kernel PCA</st></strong></span><span class="No-Break"><st c="21561">.</st></span></p>
			<p><st c="21562">The great thing about using Mercer’s theorem in this way is that if our kernel function has some parameters, we can treat those parameters as hyper-parameters and vary them until we minimize some loss function, or until we achieve maximal prediction accuracy on a validation set. </st><st c="21843">Each kernel function parameter value is equivalent to performing a PCA in some new feature space. </st><st c="21941">Therefore, varying the parameters is equivalent to performing a whole family of PCAs across a whole family of new feature spaces until we find the feature space that gives us the best </st><span class="No-Break"><st c="22125">prediction accuracy.</st></span></p>
			<p><st c="22145">In general, the approach to kernelizing any inner-product based learning algorithm is simple. </st><st c="22240">It is </st><span class="No-Break"><st c="22246">as follows:</st></span></p>
			<ol>
				<li><st c="22257">Take your inner-product based </st><span class="No-Break"><st c="22288">learning algorithm.</st></span></li>
				<li><st c="22307">Replace the inner product calculations with kernel </st><span class="No-Break"><st c="22359">function evaluations.</st></span></li>
			</ol>
			<p><st c="22380">That’s it. </st><st c="22392">You’re good </st><span class="No-Break"><st c="22404">to go.</st></span></p>
			<p><st c="22410">In practice, thinking about the original learning algorithm solely in terms of inner product calculations and learning how to express all calculations solely in terms of inner products can take a bit of getting used to, but once you do, the preceding steps are all there is </st><span class="No-Break"><st c="22685">to it.</st></span></p>
			<p><st c="22691">If kernelizing an inner-product based learning algorithm is simple, you might ask: is there much of a difference between the original learning algorithm and its kernelized version? </st><st c="22873">The answer is no. </st><st c="22891">We can think of the original learning algorithm as just a special case of its</st><a id="_idIndexMarker1064"/><st c="22968"> kernelized version that uses a linear kernel function. </st><st c="23024">For example, our original PCA algorithm, explained in </st><a href="B19496_03.xhtml#_idTextAnchor141"><span class="No-Break"><em class="italic"><st c="23078">Chapter 3</st></em></span></a><st c="23087">, is just an example of kernel PCA that uses a linear dot-product kernel, and so uses a kernel function of the </st><span class="No-Break"><st c="23198">following form:</st></span></p>
			<p><img src="image/3952.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.403em;height:1.114em;width:5.116em"/><st c="23213"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23215">Eq.18</st></p>
			<p><st c="23220">That short section on how we can, in principle, use the kernel trick concludes this section on kernels, so let’s recap what we have learned about kernels and the </st><span class="No-Break"><st c="23383">kernel trick.</st></span></p>
			<h2 id="_idParaDest-348"><a id="_idTextAnchor640"/><st c="23396">What we learned</st></h2>
			<p><st c="23412">In this section, we have learned </st><span class="No-Break"><st c="23446">the following:</st></span></p>
			<ul>
				<li><st c="23460">A kernel function maps two mathematical objects to a </st><span class="No-Break"><st c="23514">scalar value</st></span></li>
				<li><st c="23526">About commonly used kernels, such as dot-product kernels and translationally invariant kernels, that are applied </st><span class="No-Break"><st c="23640">to vectors</st></span></li>
				<li><st c="23650">How kernel functions can be combined in </st><span class="No-Break"><st c="23691">linear combinations</st></span></li>
				<li><st c="23710">About positive definite and positive semi-definite </st><span class="No-Break"><st c="23762">kernel functions</st></span></li>
				<li><st c="23778">How Mercer’s theorem tells us that a positive semi-definite kernel function corresponds to an inner product calculation in some unknown (implicit) new </st><span class="No-Break"><st c="23930">feature space</st></span></li>
				<li><st c="23943">The kernel trick uses Mercer’s theorem to replace the inner products in an inner-product based learning algorithm with a kernel function to create a kernelized version of the </st><span class="No-Break"><st c="24119">learning algorithm</st></span></li>
			</ul>
			<p><st c="24137">Having learned about the ideas behind how to create kernelized learning algorithms, in the next section, we will see an example of a kernelized algorithm </st><span class="No-Break"><st c="24292">in action.</st></span></p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor641"/><st c="24302">An example of a kernelized learning algorithm</st></h1>
			<p><st c="24348">To illustrate the simplicity of kernelized algorithms we’ll demonstrate with a code example for a specific inner-product based learning algorithm. </st><st c="24496">The algorithm we’ll use is </st><strong class="bold"><st c="24523">Fisher Discriminant Analysis</st></strong><st c="24551"> (</st><strong class="bold"><st c="24553">FDA</st></strong><st c="24556">), which is an algorithm for assigning points to class labels. </st><st c="24620">The standard version of FDA is a form of </st><strong class="bold"><st c="24661">Linear Discriminant Analysis</st></strong><st c="24689"> (</st><strong class="bold"><st c="24691">LDA</st></strong><st c="24694">). </st><st c="24698">When we run the</st><a id="_idIndexMarker1065"/><st c="24713"> kernelized version of the FDA, we will be doing </st><strong class="bold"><st c="24762">Kernel Fisher Discriminant </st></strong><span class="No-Break"><strong class="bold"><st c="24789">Analysis</st></strong></span><span class="No-Break"><st c="24797"> (</st></span><span class="No-Break"><strong class="bold"><st c="24799">kFDA</st></strong></span><span class="No-Break"><st c="24803">).</st></span></p>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor642"/><st c="24806">kFDA code example</st></h2>
			<p><st c="24824">We will start with the example</st><a id="_idIndexMarker1066"/><st c="24855"> data in </st><span class="No-Break"><em class="italic"><st c="24864">Figure 12</st></em></span><em class="italic"><st c="24873">.1</st></em><st c="24875">. The classes are linearly separable, so we’ll use a linear Fisher discriminant to construct a classifier. </st><st c="24982">A linear discriminant for a two-class problem uses the orthogonal distance of a point </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.470em"/><st c="25068"/><st c="25069"> from a line </st><img src="image/3954.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.665em"/><st c="25082"/><st c="25083"> to determine which class the point is in. </st><st c="25126">If the point </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="25139"/><st c="25140"> is one side of the line, we say it is in class </st><strong class="source-inline"><st c="25188">1</st></strong><st c="25189">, while if it is on the other side of the line, we say it is in class </st><strong class="source-inline"><st c="25259">2</st></strong><st c="25260">. The line </st><img src="image/3954.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.669em"/><st c="25271"/><st c="25272"> is our predictive model that we need </st><span class="No-Break"><st c="25310">to train.</st></span></p>
			<p><st c="25319">Measuring how far a point is from the line </st><img src="image/3954.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.669em"/><st c="25363"/><st c="25364"> is equivalent to measuring how far the point </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="25410"/><st c="25411"> is along the line </st><img src="image/3959.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.068em;width:0.563em"/><st c="25430"/><st c="25431">, which is orthogonal to the line </st><img src="image/3960.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.729em"/><st c="25465"/><st c="25466">. This means that we can express the classifier as the mathematical condition </st><img src="image/3961.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.075em;width:3.327em"/><st c="25544"/><st c="25545">, where </st><img src="image/3860.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.460em;width:0.381em"/><st c="25553"/><st c="25554"> is some constant. </st><st c="25573">If the point </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="25586"/><st c="25587"> satisfies this condition the point is in one class, if it doesn’t satisfy this mathematical condition it is in the </st><span class="No-Break"><st c="25703">other class.</st></span></p>
			<p><st c="25715">Training the linear discriminant is the process of determining the optimal line </st><img src="image/3954.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.683em"/><st c="25796"/><st c="25797"> which minimizes the classification error on the training dataset. </st><st c="25864">Determining the optimal line </st><img src="image/3954.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.664em"/><st c="25893"/><st c="25894"> is equivalent to finding the optimal line </st><img src="image/3966.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;β&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.357em;height:1.068em;width:0.508em"/><st c="25937"/><st c="25938">. Since we can see from </st><span class="No-Break"><em class="italic"><st c="25962">Figure 12</st></em></span><em class="italic"><st c="25971">.1</st></em><st c="25973"> that the two classes (the red and the blue points) can be separated by a straight line, we know that a linear discriminant using just the features we have, </st><img src="image/3234.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="26130"/><st c="26131"> and </st><img src="image/3814.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="26136"/><st c="26137">, will be sufficient to achieve a high-level </st><span class="No-Break"><st c="26182">of accuracy.</st></span></p>
			<p><st c="26194">I have written a Python class, </st><strong class="source-inline"><st c="26226">KFDA_Poly</st></strong><st c="26235">, which allows us to do kFDA. </st><st c="26265">I have kept things simple; it only allows us to do kFDA using pure polynomial dot-product kernels of the form </st><span class="No-Break"><st c="26375">that follows:</st></span></p>
			<p><img src="image/3969.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.407em;height:1.118em;width:6.255em"/><st c="26388"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26405">Eq.19</st></p>
			<p><st c="26410">The </st><strong class="source-inline"><st c="26415">KFDA_Poly</st></strong><st c="26424"> class is defined in the </st><strong class="source-inline"><st c="26449">kernel_fda.py</st></strong><st c="26462"> module, which can be found in the Chapter12 directory of the GitHub repository. </st><st c="26543">When calling the constructor for the </st><strong class="source-inline"><st c="26580">KFDA_Poly</st></strong><st c="26589"> class, we specify the degree </st><img src="image/1890.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.770em;width:0.510em"/><st c="26619"/><st c="26620"> of the kernel that we want to use. </st><st c="26656">To do linear FDA, we specify the </st><span class="No-Break"><st c="26689">degree </st></span><span class="No-Break"><img src="image/3971.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" style="vertical-align:-0.252em;height:0.886em;width:2.505em"/><st c="26696"/></span><span class="No-Break"><st c="26697">.</st></span></p>
			<p><st c="26698">The data for </st><span class="No-Break"><em class="italic"><st c="26712">Figure 12</st></em></span><em class="italic"><st c="26721">.1</st></em><st c="26723"> is in the </st><strong class="source-inline"><st c="26734">lda_ex1.csv</st></strong><st c="26745"> file in the </st><strong class="source-inline"><st c="26758">Data</st></strong><st c="26762"> directory of the GitHub repository. </st><st c="26799">We have labeled the classes </st><strong class="source-inline"><st c="26827">1</st></strong><st c="26828"> and </st><strong class="source-inline"><st c="26833">2</st></strong><st c="26834"> rather than </st><strong class="source-inline"><st c="26847">red</st></strong><st c="26850"> and </st><strong class="source-inline"><st c="26855">blue</st></strong><st c="26859">. Class </st><strong class="source-inline"><st c="26867">1</st></strong><st c="26868"> corresponds to the blue points, while class </st><strong class="source-inline"><st c="26913">2</st></strong><st c="26914"> corresponds to the red points. </st><st c="26946">The code example that follows</st><a id="_idIndexMarker1067"/><st c="26975"> can be found in the </st><strong class="source-inline"><st c="26996">Code_Examples_Chap12.ipynb</st></strong><st c="27022"> Jupyter notebook in the GitHub repository. </st><st c="27066">First, we’ll read in </st><span class="No-Break"><st c="27087">the data:</st></span></p>
			<pre class="source-code"><st c="27096">
import pandas as pd
import kernel_fda
# Read in the data
df_LDA_ex1 = pd.read_csv('../Data/lda_ex1.csv')</st></pre>			<p><st c="27201">Let’s look at </st><span class="No-Break"><st c="27216">the data:</st></span></p>
			<pre class="source-code"><st c="27225">
# Take a quick look at the dataframe
df_LDA_ex1</st></pre>			<p><st c="27273">This gives the table that follows, from which we can see that we have 1,000 datapoints, each consisting of the two features </st><img src="image/3234.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="27398"/><st c="27399">, </st><img src="image/3814.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:0.781em;width:0.744em"/><st c="27401"/><st c="27402"> and the </st><span class="No-Break"><st c="27411">class label:</st></span></p>
			<pre class="source-code"><st c="27423">
      x1           x2     class
0    -0.297128   0.477975  2
1    -0.575548  -0.274354  1
2    -0.793637  -0.681858  1
3     0.842911  -0.766655  2
4    -0.566261   0.621195  2
...   ...   ...   ...
</st><st c="27569">995  -0.797437   0.922392  2
996  -0.247480   0.526261  2
997  -0.562740  -0.328817  1
998  -0.376398   0.211520  1
999   0.799909   0.486870  2</st></pre>			<p><st c="27693">Now we’ll build a linear Fisher discriminant. </st><st c="27740">We’ll instantiate a </st><strong class="source-inline"><st c="27760">KFDA_Poly</st></strong><st c="27769"> object with a linear kernel (</st><strong class="source-inline"><st c="27799">degree=1</st></strong><st c="27808">). </st><st c="27812">Specifying a linear kernel is saying that we are going to do kFDA but with a linear kernel, so</st><a id="_idIndexMarker1068"/><st c="27906"> this is equivalent to </st><span class="No-Break"><st c="27929">linear FDA:</st></span></p>
			<pre class="source-code"><st c="27940">
# Create the linear classifier
linear_classifier_ex1 = kernel_fda.KFDA_Poly(degree=1)</st></pre>			<p><st c="28026">Now we’ll fit the linear classifier using the training data we have just </st><span class="No-Break"><st c="28100">read in:</st></span></p>
			<pre class="source-code"><st c="28108">
# Fit the linear classifier
linear_classifier_ex1.fit(X=df_LDA_ex1[['x1','x2']], 
                          y=df_LDA_ex1['class'])</st></pre>			<p><st c="28212">We can then score the trained linear classifier on the training set using the built-in </st><span class="No-Break"><st c="28300">score function:</st></span></p>
			<pre class="source-code"><st c="28315">
linear_classifier_ex1.score(X=df_LDA_ex1[['x1','x2']], 
                            y_true=df_LDA_ex1['class'])</st></pre>			<p><st c="28398">This gives an output of </st><strong class="source-inline"><st c="28423">0.998</st></strong><st c="28428">. We can see that the model scores very well on the training set. </st><st c="28494">The proportion of the training points that the classifier correctly classifies is </st><strong class="source-inline"><st c="28576">0.998</st></strong><st c="28581">, that is, nearly 100% accuracy on the training set. </st><st c="28634">This is to be expected, as we know just by looking at </st><span class="No-Break"><em class="italic"><st c="28688">Figure 12</st></em></span><em class="italic"><st c="28697">.1</st></em><st c="28699"> that the two classes are separable by a straight line, so a properly trained linear classifier should be capable of fitting the training data nearly perfectly. </st><st c="28860">We also know that this trained classifier would predict any hold-out datapoints accurately provided they are also drawn from the same distribution as the training data. </st><st c="29029">Therefore, for the purposes of this example, there is no need to test our classifier on a </st><span class="No-Break"><st c="29119">holdout sample.</st></span></p>
			<p><st c="29134">Now we’ll repeat the process using the data from </st><span class="No-Break"><em class="italic"><st c="29184">Figure 12</st></em></span><em class="italic"><st c="29193">.2</st></em><st c="29195">. We know from looking at </st><span class="No-Break"><em class="italic"><st c="29221">Figure 12</st></em></span><em class="italic"><st c="29230">.2</st></em><st c="29232"> that a straight line can’t separate the two classes perfectly. </st><st c="29296">Consequently, a trained linear Fisher discriminant should score poorly on the training data in </st><span class="No-Break"><em class="italic"><st c="29391">Figure 12</st></em></span><em class="italic"><st c="29400">.2</st></em><st c="29402">. </st><span class="No-Break"><st c="29404">Let’s check.</st></span></p>
			<p><st c="29416">The data for </st><span class="No-Break"><em class="italic"><st c="29430">Figure 12</st></em></span><em class="italic"><st c="29439">.2</st></em><st c="29441"> is in the </st><strong class="source-inline"><st c="29452">lda_ex2.csv</st></strong><st c="29463"> file in the </st><strong class="source-inline"><st c="29476">Data</st></strong><st c="29480"> directory of the GitHub repository. </st><st c="29517">It is in the same format as the previous example. </st><st c="29567">First, we’ll read in </st><span class="No-Break"><st c="29588">the data:</st></span></p>
			<pre class="source-code"><st c="29597">
# Read in the data
df_LDA_ex2 = pd.read_csv('../Data/lda_ex2.csv')</st></pre>			<p><st c="29664">Now we’ll repeat the </st><a id="_idIndexMarker1069"/><st c="29686">process we went through with the first example and train a linear Fisher discriminant on this data. </st><st c="29786">We will start by instantiating the </st><span class="No-Break"><st c="29821">linear classifier:</st></span></p>
			<pre class="source-code"><st c="29839">
# Create the linear classifier
linear_classifier_ex2 = kernel_fda.KFDA_Poly(degree=1)</st></pre>			<p><st c="29925">Next, we’ll fit it to the training data from </st><span class="No-Break"><em class="italic"><st c="29971">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="29980">.2</st></em></span><span class="No-Break"><st c="29982">:</st></span></p>
			<pre class="source-code"><st c="29984">
# Fit the linear classifier
linear_classifier_ex2.fit(X=df_LDA_ex2[['x1','x2']], 
                          y=df_LDA_ex2['class'])</st></pre>			<p><st c="30088">Now, we’ll score the trained linear classifier on the training </st><span class="No-Break"><st c="30152">set data:</st></span></p>
			<pre class="source-code"><st c="30161">
linear_classifier_ex2.score(X=df_LDA_ex2[['x1','x2']], 
                            y_true=df_LDA_ex2['class'])</st></pre>			<p><st c="30244">This gives an output of </st><strong class="source-inline"><st c="30269">0.502</st></strong><st c="30274">. We can see the score on the training set is close to </st><strong class="source-inline"><st c="30329">0.5</st></strong><st c="30332">, that is, only about 50% accuracy. </st><st c="30368">This is a lot lower than in our first example. </st><st c="30415">This is to be expected. </st><st c="30439">No straight line can separate the two classes in </st><span class="No-Break"><em class="italic"><st c="30488">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="30497">.2</st></em></span><span class="No-Break"><st c="30499">.</st></span></p>
			<p><st c="30500">Can you think why the accuracy on the training set was close to </st><strong class="source-inline"><st c="30565">0.5</st></strong><st c="30568">, even though we have trained (or rather, optimized) this linear classifier on the </st><span class="No-Break"><st c="30651">training data?</st></span></p>
			<p><st c="30665">We know that the red and blue points in </st><span class="No-Break"><em class="italic"><st c="30706">Figure 12</st></em></span><em class="italic"><st c="30715">.2</st></em><st c="30717"> are separated by the </st><img src="image/3807.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:4.603em"/><st c="30739"/><st c="30755"> boundary. </st><st c="30765">So, if a point </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="30780"/><st c="30781"> has </st><img src="image/3976.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:4.520em"/><st c="30786"/><st c="30796"> , it is in the red class, while if </st><img src="image/1816.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.112em;height:0.560em;width:0.472em"/><st c="30831"/><st c="30832"> has </st><img src="image/3978.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:4.519em"/><st c="30837"/><st c="30847"> , it is in the blue class. </st><st c="30874">This tells us that if we had a perfect classifier for this dataset, we could write our classifier condition as </st><img src="image/3979.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:1.636em"/><st c="30985"/><span class="_-----MathTools-_Math_Base"><img src="image/3980.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.333em;height:1.036em;width:2.610em"/><st c="30992"/></span><st c="30993">. This classifier condition can also be written as </st><img src="image/3981.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;munder&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/msup&gt;&lt;munder&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;_&lt;/mo&gt;&lt;/munder&gt;&lt;mo&gt;&gt;&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.357em;height:1.075em;width:3.215em"/><st c="31044"/><st c="31045"> , where </st><img src="image/3982.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" style="vertical-align:-0.383em;height:1.237em;width:8.374em"/><st c="31053"/><st c="31074"> and </st><span class="_-----MathTools-_Math_Variable"><st c="31078">β</st></span><span class="_-----MathTools-_Math_Variable"/><span class="_-----MathTools-_Math_Variable"><st c="31079">_</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><st c="31080">=</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><st c="31081">(</st></span><span class="_-----MathTools-_Math_Number"><st c="31082">1,1</st></span><span class="_-----MathTools-_Math_Operator"><st c="31085">,</st></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><st c="31086">0</st></span><span class="_-----MathTools-_Math_Base"><st c="31087">)</st></span><st c="31088">. This is in the form of a linear discriminant classifier, but one where we are using a new feature vector </st><img src="image/3944.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.101em;height:0.757em;width:0.753em"/><st c="31195"/><st c="31196">. However, the </st><img src="image/3944.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Φ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" style="vertical-align:-0.101em;height:0.757em;width:0.753em"/><st c="31211"/><st c="31212"> vector is precisely the new feature vector that was implicitly created when we used a quadratic dot-product kernel in our Mercer’s theorem example in the previous section. </st><st c="31385">This suggests that if we train a kernel Fisher discriminant classifier using a quadratic dot-product kernel </st><img src="image/3985.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.407em;height:1.118em;width:6.433em"/><st c="31493"/><st c="31494">, the trained classifier should be capable of perfectly separating the red and the blue points in the training data shown in </st><span class="No-Break"><em class="italic"><st c="31619">Figure 12</st></em></span><em class="italic"><st c="31628">.2</st></em><st c="31630">. </st><span class="No-Break"><st c="31632">Let’s see.</st></span></p>
			<p><st c="31642">First, we will instantiate a</st><a id="_idIndexMarker1070"/><st c="31671"> kernel classifier object by specifying a polynomial dot-product kernel of </st><span class="No-Break"><st c="31746">degree </st></span><span class="No-Break"><strong class="source-inline"><st c="31753">2</st></strong></span><span class="No-Break"><st c="31754">:</st></span></p>
			<pre class="source-code"><st c="31755">
# Create a quadratic dot-product kernel Fisher Discriminant
kernel_classifier = kernel_fda.KFDA_Poly(degree=2)</st></pre>			<p><st c="31866">Next, we will fit the kernel Fisher discriminant to the training data from </st><span class="No-Break"><em class="italic"><st c="31942">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="31951">.2</st></em></span><span class="No-Break"><st c="31953">:</st></span></p>
			<pre class="source-code"><st c="31955">
# Fit the kernel classifier to the training data
kernel_classifier.fit(X=df_LDA_ex2[['x1','x2']], 
                      y=df_LDA_ex2['class'])</st></pre>			<p><st c="32076">Finally, we’ll score the trained kernel Fisher discriminant classifier on the training data. </st><st c="32170">We should get something a lot higher than </st><strong class="source-inline"><st c="32212">0.5</st></strong><st c="32215"> and much closer </st><span class="No-Break"><st c="32232">to </st></span><span class="No-Break"><strong class="source-inline"><st c="32235">1</st></strong></span><span class="No-Break"><st c="32236">:</st></span></p>
			<pre class="source-code"><st c="32237">
# Score the trained classifier on the training data
kernel_classifier.score(X=df_LDA_ex2[['x1','x2']], 
                        y_true=df_LDA_ex2['class'])</st></pre>			<p><st c="32368">This gives an output of </st><strong class="source-inline"><st c="32393">0.911</st></strong><st c="32398">, so we do get a trained classifier that fits the training data much better than a standard linear Fisher discriminant. </st><st c="32518">The reason the trained classifier doesn’t fit the training data perfectly, that is, that the accuracy proportion is not </st><strong class="source-inline"><st c="32638">1</st></strong><st c="32639">, is simply due to sampling variation. </st><st c="32678">If we increased the size of the training data, we would get closer and closer to a score </st><span class="No-Break"><st c="32767">of </st></span><span class="No-Break"><strong class="source-inline"><st c="32770">1</st></strong></span><span class="No-Break"><st c="32771">.</st></span></p>
			<p><st c="32772">The code example shows how easy it is to use kernelized algorithms in practice. </st><st c="32853">It is usually as simple as specifying the kernel that we want to use and then running the algorithm as we would normally run the un-kernelized version. </st><st c="33005">The use of non-linear kernels in the algorithm means we can learn the non-linear structure present in the data by implicitly creating new features. </st><st c="33153">Learning this non-linear structure is not something that the un-kernelized version of the algorithm is </st><span class="No-Break"><st c="33256">capable of.</st></span></p>
			<p><st c="33267">In the code example, we were </st><a id="_idIndexMarker1071"/><st c="33297">able to deduce which kernel to use a priori. </st><st c="33342">In real-world situations, the choice of kernel is something we would typically experiment with or optimize as a hyper-parameter of the algorithm. </st><st c="33488">Due to the simplicity of using the kernelized algorithm, varying the parameters of the kernel is </st><span class="No-Break"><st c="33585">not difficult.</st></span></p>
			<p><st c="33599">That concludes the demonstration of a real kernelized algorithm. </st><st c="33665">We’ll recap what we have learned from in this section and then wrap up the </st><span class="No-Break"><st c="33740">chapter overall.</st></span></p>
			<h2 id="_idParaDest-351"><a id="_idTextAnchor643"/><st c="33756">What we learned</st></h2>
			<p><st c="33772">In this section, we have learned </st><span class="No-Break"><st c="33806">the following:</st></span></p>
			<ul>
				<li><st c="33820">How running a kernelized algorithm is as simple as selecting </st><span class="No-Break"><st c="33882">a kernel</st></span></li>
				<li><st c="33890">How a kernelized algorithm can correctly learn the non-linear structure present in a dataset, while the standard linear version of the </st><span class="No-Break"><st c="34026">algorithm cannot</st></span></li>
			</ul>
			<h1 id="_idParaDest-352"><a id="_idTextAnchor644"/><st c="34042">Summary</st></h1>
			<p><st c="34050">This chapter has been focused on kernel methods, which are also called kernelized algorithms. </st><st c="34145">The chapter has been short so that we can focus on the most important concepts underpinning kernel methods. </st><st c="34253">Those concepts are </st><span class="No-Break"><st c="34272">as follows:</st></span></p>
			<ul>
				<li><st c="34283">Inner-product based learning algorithms are very common because an inner product captures the similarity between feature vectors, and learning by similarity is a natural basis for many machine </st><span class="No-Break"><st c="34477">learning algorithms.</st></span></li>
				<li><st c="34497">Inner products calculated from the existing features on a dataset may not be sufficient to learn the non-linear structure present in </st><span class="No-Break"><st c="34631">the dataset.</st></span></li>
				<li><st c="34643">Construction of new features can be necessary to make our learning </st><span class="No-Break"><st c="34711">algorithms accurate.</st></span></li>
				<li><st c="34731">Mercer’s theorem tells us that positive semi-definite kernel functions implicitly construct new features and calculate inner products in those new </st><span class="No-Break"><st c="34879">feature spaces.</st></span></li>
				<li><st c="34894">There are different types of </st><span class="No-Break"><st c="34924">kernel functions.</st></span></li>
				<li><st c="34941">We can use the kernel trick to kernelize any inner-product based </st><span class="No-Break"><st c="35007">learning algorithm.</st></span></li>
				<li><st c="35026">Using kernelized algorithms in practice can be as simple as specifying a choice of kernel and </st><span class="No-Break"><st c="35121">its parameters.</st></span></li>
				<li><st c="35136">By varying the parameters of a kernel, we can effectively explore many different new feature spaces. </st><st c="35238">This can be an efficient way to learn the non-linear structure in </st><span class="No-Break"><st c="35304">a dataset.</st></span></li>
			</ul>
			<p><st c="35314">Our next chapter is focused on another advanced topic. </st><st c="35370">It is one you have probably heard of but may not be that familiar with. </st><st c="35442">The topic of the next chapter is </st><span class="No-Break"><strong class="bold"><st c="35475">information theory</st></strong></span><span class="No-Break"><st c="35493">.</st></span></p>
			<h1 id="_idParaDest-353"><a id="_idTextAnchor645"/><st c="35494">Exercises</st></h1>
			<p><st c="35504">We have already given a lengthy code example of a simple kernelized algorithm in the main part of this chapter. </st><st c="35617">Therefore, for the exercises, we will demonstrate some of the more complex aspects of kernel methods. </st><st c="35719">Due to this increase in complexity, we will only ask a single question. </st><st c="35791">It is intentionally challenging, so don’t be surprised if you don’t manage to complete it fully. </st><st c="35888">Have a go at answering the exercise and then compare your answer to the one in the </st><strong class="source-inline"><st c="35971">Answers_to_Exercises_Chap12.ipynb</st></strong><st c="36004"> Jupyter notebook in the </st><span class="No-Break"><st c="36029">GitHub repository.</st></span></p>
			<ul>
				<li><st c="36047">The data in the </st><strong class="source-inline"><st c="36064">kernel_PCA_matrix_data.csv</st></strong><st c="36090"> file in the </st><strong class="source-inline"><st c="36103">Data</st></strong><st c="36107"> directory of the GitHub repository contains the matrix elements of </st><strong class="source-inline"><st c="36175">N=200</st></strong><st c="36180">, 4x4 matrices. </st><st c="36196">Each matrix corresponds to a single row of the </st><strong class="source-inline"><st c="36243">.csv</st></strong><st c="36247"> file. </st><st c="36254">The column headings are of the form </st><strong class="source-inline"><st c="36290">i_j</st></strong><st c="36293">, where </st><strong class="source-inline"><st c="36301">i</st></strong><st c="36302"> and </st><strong class="source-inline"><st c="36307">j</st></strong><st c="36308"> are integers, representing the i,j matrix element. </st><st c="36360">For example, the column with the </st><strong class="source-inline"><st c="36393">1_3</st></strong><st c="36396"> heading holds the </st><strong class="source-inline"><st c="36415">1,3</st></strong><st c="36418"> matrix elements of each matrix. </st><st c="36451">Use the data to perform a kernel PCA of the matrix data, where each matrix is a single datapoint. </st><st c="36549">Use the matrix kernel function in </st><em class="italic"><st c="36583">Eq.7</st></em><st c="36587"> to calculate inner products between any two matrices. </st><st c="36642">You should plot the datapoints in a PCA score plot and comment on what </st><span class="No-Break"><st c="36713">you see.</st></span><ul><li><st c="36721">You will find it useful to reshape the data in the </st><strong class="source-inline"><st c="36773">.csv</st></strong><st c="36777"> file into a Python list of matrices, that is, </st><span class="No-Break"><st c="36824">square arrays.</st></span></li><li><st c="36838">You can use the </st><strong class="source-inline"><st c="36855">numpy.linalg.norm</st></strong><st c="36872"> NumPy function to calculate the Frobenius norm used in </st><em class="italic"><st c="36928">Eqs. </st><st c="36933">6</st></em> <span class="No-Break"><st c="36934">and </st></span><span class="No-Break"><em class="italic"><st c="36938">7</st></em></span><span class="No-Break"><st c="36939">.</st></span></li><li><st c="36940">Review the material in </st><a href="B19496_03.xhtml#_idTextAnchor141"><em class="italic"><st c="36964">Chapter 3</st></em></a><st c="36973"> on how to do PCA using just the Gram matrix of the centered data matrix. </st><st c="37047">The </st><img src="image/3986.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:0.903em;width:0.853em"/><st c="37051"/><st c="37052"> matrix element of the Gram matrix is the inner product between the </st><img src="image/909.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.012em;height:0.760em;width:0.809em"/><st c="37120"/><st c="37132"> centered datapoint and the </st><img src="image/1061.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" style="vertical-align:-0.257em;height:1.005em;width:0.801em"/><st c="37159"/> <span class="No-Break"><st c="37163">centered datapoint.</st></span></li><li><st c="37182">You will need to center the data, but this centering needs to be done in the new feature space that is implicitly created by our kernel function. </st><st c="37329">Unfortunately, you don’t know what the new features are. </st><st c="37386">Fortunately, you don’t have to. </st><st c="37418">If the </st><img src="image/726.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" style="vertical-align:-0.008em;height:0.656em;width:2.451em"/><st c="37425"/><st c="37441"> Gram matrix of the uncentered data has matrix elements </st><img src="image/3990.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;G&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.483em;height:1.146em;width:0.914em"/><st c="37496"/><st c="37497">, then the matrix elements, </st><img src="image/3991.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" style="vertical-align:-0.483em;height:1.131em;width:0.821em"/><st c="37525"/><st c="37526">, of the Gram matrix of the centered data can be calculated via the </st><span class="No-Break"><st c="37594">following equation:</st></span></li></ul></li>
			</ul>
			<p><img src="image/3992.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;G&lt;/mi&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.854em;height:1.898em;width:15.810em"/><st c="37613"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37615">Eq.20</st></p>
			<ul>
				<li><st c="37620">You will need to do an eigen-decomposition of the centered Gram matrix. </st><st c="37693">The centered Gram matrix is real and symmetric, so you should use the </st><strong class="source-inline"><st c="37763">numpy.linalg.eigh</st></strong><st c="37780"> NumPy function to do </st><span class="No-Break"><st c="37802">this eigen-decomposition.</st></span></li>
			</ul>
		</div>
	<div id="charCountTotal" value="37827"/></body></html>