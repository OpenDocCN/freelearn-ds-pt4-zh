- en: <st c="0">1</st><st c="2">2</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Kernel Methods</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="17">Our remaining chapters will now focus on more advanced topics.</st>
    <st c="81">Due to their advanced nature, we will not attempt to cover them in
    the same level of detail as we have done for the topics of earlier chapters.</st>
    <st c="225">Instead, we will focus on getting the essential concepts and ideas
    behind these topics across.</st> <st c="320">The aim is not to make you an expert
    in these topics but to introduce you to them so that you can recognize them when
    you see them again, or if you want to learn more at a later date.</st> <st c="504">This
    focus on the essentials means that each of these chapters on advanced topics will
    be shorter than</st> <st c="607">previous chapters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="625">The first of our advanced topics</st> <st c="659">is</st> **<st
    c="662">kernel methods</st>**<st c="676">. Kernel methods, or</st> **<st c="697">kernelized
    learning algorithms</st>**<st c="727">, are very widely used.</st> <st c="751">The
    math they are based on is both advanced and</st> <st c="798">elegant.</st> <st
    c="808">That math relates to machine learning algorithms that make use of similarities
    between feature vectors.</st> <st c="912">To understand kernel methods, we will
    need to understand why similarity-based learning algorithms are common.</st> <st
    c="1022">We will also need to understand the main principles behind the elegant
    math of kernel functions.</st> <st c="1119">To do that we will cover the</st>
    <st c="1148">following topics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="1165">The role of inner-products in common learning algorithms</st>*<st
    c="1222">: In this section, we will see why inner products are at the heart of
    many machine</st> <st c="1306">learning algorithms</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1325">The kernel trick</st>*<st c="1342">: In this section, we will
    learn about kernel functions and how kernel functions allow us to compute inner
    products in new feature spaces implicitly and</st> <st c="1496">very simply</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="1507">An example kernelized learning algorithm</st>*<st c="1548">:
    In this section, we will see, using a code example, the simplicity and power of
    a kernelized</st> <st c="1644">learning algorithm</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1662">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1685">All code examples given in this chapter can be found at the GitHub
    repository at</st> [<st c="1767">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter12</st>](https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter12)<st
    c="1871">. To run the Jupyter notebooks you will need a full Python installation,
    including the</st> <st c="1958">following packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="1977">pandas</st>` <st c="1984">(>=2.0.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1994">numpy</st>` <st c="2000">(>=1.24.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="2011">scikit-learn</st>` <st c="2024">(>=1.3.0)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="2034">matplotlib</st>` <st c="2045">(>=3.7.2)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2055">The role of inner products in common learning algorithms</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2112">In</st> [*<st c="2116">Chapter 3</st>*](B19496_03.xhtml#_idTextAnchor141)<st
    c="2125">, we introduced</st> <st c="2140">the</st> **<st c="2145">Principal Component
    Analysis</st>** <st c="2173">(</st>**<st c="2175">PCA</st>**<st c="2178">) unsupervised
    learning algorithm and showed how all the calculations in PCA could be expressed
    in terms of inner products between the feature vectors of the different points
    in the</st> <st c="2361">training data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2375">In</st> [*<st c="2379">Chapter 3</st>*](B19496_03.xhtml#_idTextAnchor141)<st
    c="2388">, we also explained that the inner product</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3796.png)
    <st c="2431"><st c="2432">between vectors</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="2449"><st c="2450">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="2455"><st c="2456">gives a measure of how similar vectors</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="2496"><st c="2497">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="2502"><st c="2503">are to each other.</st> <st c="2523">Since many learning
    algorithms are based on the idea that similar datapoints behave similarly, it
    is not surprising that PCA and indeed many other learning algorithms make use
    of</st> <st c="2702">inner products.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2717">Like PCA, many</st> <st c="2733">classical statistical and machine
    learning</st> <st c="2776">algorithms can be expressed solely in</st> <st c="2814">terms
    of inner</st> <st c="2829">products, such as</st> **<st c="2847">Linear Discriminant
    Analysis</st>** <st c="2875">(</st>**<st c="2877">LDA</st>**<st c="2880">),</st>
    **<st c="2884">Fisher Discriminant Analysis</st>** <st c="2912">(</st>**<st c="2914">FDA</st>**<st
    c="2917">),</st> **<st c="2921">Canonical Correlation Analysis</st>** <st c="2951">(</st>**<st
    c="2953">CCA</st>**<st c="2956">), and</st> **<st c="2964">Support Vector Machines</st>**
    <st c="2987">(</st>**<st c="2989">SVMs</st>**<st c="2993">).</st> <st c="2997">We
    will</st> <st c="3005">refer to these types of algorithms as</st> **<st c="3043">inner-product
    based</st>** **<st c="3063">learning algorithms</st>**<st c="3082">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3083">Given the prevalence of inner-product based learning algorithms,
    it is natural to ask whether inner products between the feature vectors in a training
    dataset are all we need.</st> <st c="3260">The answer is</st> <st c="3274">more
    subtle.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3286">Sometimes we need new features in our inner products</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3339">We can only calculate inner products between datapoints using the
    features we already have.</st> <st c="3432">OK, but we already know this.</st>
    <st c="3462">What is the big deal?</st> <st c="3484">Sometimes this is OK and
    we don’t need to construct any new features.</st> <st c="3554">Sometimes it</st>
    <st c="3567">is not.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3574">Imagine that we are trying to construct a linear discriminant.</st>
    <st c="3638">A linear discriminant</st> <st c="3659">is a simple algorithm that
    constructs a decision rule in the form of a linear combination of the features.</st>
    <st c="3767">The linear discriminant is essentially a line drawn in the feature
    space.</st> <st c="3841">If a point falls on one side of the line, we classify
    it as being in one class, and if it falls on the other side of the line, we classify
    it as being in the other class.</st> *<st c="4012">Figure 12</st>**<st c="4021">.1</st>*
    <st c="4023">shows a simple example.</st> <st c="4048">We have just two features,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3801.png)
    <st c="4075"><st c="4076">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3802.png)<st
    c="4081"><st c="4082">. Our two classes are the red class and the blue class.</st>
    <st c="4138">You can easily see from</st> *<st c="4162">Figure 12</st>**<st c="4171">.1</st>*
    <st c="4173">that the class that a given point is in is completely determined
    by which side of the straight line</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>=</mo><mo>−</mo><msub><mi>x</mi><mn>1</mn></msub></mrow></mrow></math>](img/3803.png)
    <st c="4274"><st c="4275">the point is on.</st> <st c="4293">In other words, knowing
    whether the value of the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3804.png)
    <st c="4342"><st c="4350">linear combination is greater than or less than</st>
    `<st c="4398">0</st>` <st c="4399">is enough to determine the class.</st> <st
    c="4434">In this case, we would say that the classes</st> <st c="4478">are</st>
    **<st c="4482">linearly separable</st>**<st c="4500">. The existing features,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3805.png)
    <st c="4525"><st c="4526">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3806.png)<st
    c="4531"><st c="4532">, that we have on our dataset are enough to construct an
    accurate discriminant function from any training data.</st> <st c="4644">We don’t
    need to construct any</st> <st c="4675">additional fea</st><st c="4689">tures.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="4756">Figure 12.1: Existing features can linearly separate the red and
    blue classes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4833">This is not always the case.</st> <st c="4863">Take the</st> <st
    c="4872">example in</st> *<st c="4883">Figure 12</st>**<st c="4892">.2</st>*<st
    c="4894">. Clearly, the two classes, red and blue, are still separable by a simple
    boundary.</st> <st c="4978">However, that boundary is not a line.</st> <st c="5016">We
    would say that the two classes in this second example are not</st> <st c="5081">linearly
    sepa</st><st c="5094">rable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="5163">Figure 12.2: Two classes that cannot be linearly separated by
    existing features</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5242">You may be able to notice that the boundary separating the red
    and blue dots in</st> *<st c="5323">Figure 12</st>**<st c="5332">.2</st>* <st
    c="5334">is a circle and is given by the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3807.png)
    <st c="5367"><st c="5383">relation.</st> <st c="5393">So, knowing whether the
    value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/3808.png)
    <st c="5426"><st c="5433">is greater than or less than</st> `<st c="5462">0.5</st>`
    <st c="5465">is enough to tell us which class each point is in.</st> <st c="5517">We
    can do this calculation if we have the values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/3809.png)
    <st c="5569"><st c="5570">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/3810.png)
    <st c="5575"><st c="5576">in our training dataset.</st> <st c="5602">OK, calculating</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/3809.png)
    <st c="5618"><st c="5619">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math>](img/3810.png)
    <st c="5624"><st c="5625">from the values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3234.png)<st
    c="5645"><st c="5646">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3814.png)
    <st c="5650"><st c="5651">is not difficult.</st> <st c="5670">However, it does
    illustrate that to correctly separate the two classes, we would have to construct
    some new feature values.</st> <st c="5794">Moreover, the example we have shown
    in</st> <st c="5833">Figure 12</st><st c="5842">.2 is a very simple one.</st>
    <st c="5867">In real-world datasets, we will not be able to simply visualize the
    data and spot the new features required to separate the classes.</st> <st c="6000">In
    real-world datasets, we must go through a lengthy process of constructing new
    features and trying them out.</st> <st c="6111">This can be inefficient.</st>
    <st c="6136">Fortunately, using the</st> **<st c="6159">kernel trick</st>** <st
    c="6171">can</st> <st c="6175">help us perform this feature construction process
    implicitly and allows us to explore whole families of new features in a very simple
    way.</st> <st c="6315">We will learn about the kernel trick in a moment, but for
    now, let’s summarize what we have learned in</st> <st c="6418">this section.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6431">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="6447">In this section we have learned</st> <st c="6480">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6494">Many machine learning algorithms are based on computing the inner
    product between datapoints in the</st> <st c="6595">training dataset.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="6612">The existing features in a dataset may not be sufficient to construct
    an accurate learning algorithm and so new features need to</st> <st c="6742">be
    constructed.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="6757">Having highlighted the issue that we may need to construct new
    features to make our inner-product based learning algorithms work, in the next
    section, we will learn about the kernel trick and how it can allow us to do feature</st>
    <st c="6984">construction implicitly.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7008">The kernel trick</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="7025">To learn how the</st> <st c="7043">kernel trick allows us to do
    feature construction implicitly and efficiently, we will first have to learn what
    a</st> <st c="7156">kernel is.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7166">What is a kernel?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="7184">The simplest way to think</st> <st c="7210">about a kernel is to
    consider it as a mapping that takes two vectors as input and returns a scalar.</st>
    <st c="7311">It is a mapping that maps</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msup><mi
    mathvariant="double-struck">R</mi><mi>d</mi></msup><mo>×</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup><mo>→</mo><mi
    mathvariant="double-struck">R</mi></mrow></mrow></math>](img/3815.png)<st c="7337"><st
    c="7347">. This means that a kernel is a function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3816.png)<st
    c="7388"><st c="7397">, with the input vectors being</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3817.png)
    <st c="7428"><st c="7429">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>.</mml:mo></mml:math>](img/3818.png)
    <st c="7434"><st c="7435">The value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="7449"><st c="7503">is a real number.</st> <st c="7521">This means that
    the inner product</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3820.png)
    <st c="7555"><st c="7556">is an example of a</st> <st c="7576">kernel function.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7592">That is a high-level mathematical definition of what a kernel is,
    but what is the intuition behind this?</st> <st c="7698">An</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3821.png)
    <st c="7701"><st c="7702">kernel function applied to the vectors</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/2281.png)
    <st c="7742"><st c="7743">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3823.png)
    <st c="7748"><st c="7749">is typically used to measure the similarity between
    those vectors.</st> <st c="7817">Consequently, we usually want our kernel function
    to have its largest values when</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="7899"><st c="7900">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="7905"><st c="7906">are most similar and its lowest values when</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="7951"><st c="7952">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="7957"><st c="7958">are least similar.</st> <st c="7978">We want the function
    to decrease smoothly and monotonically in between those</st> <st c="8055">two
    scenarios.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8069">Commonly used kernels</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8091">You may recall from</st> [*<st c="8112">Chapter 3</st>*](B19496_03.xhtml#_idTextAnchor141)
    <st c="8121">that the simple inner product</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3796.png)
    <st c="8152"><st c="8153">between two vectors</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="8174"><st c="8175">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="8180"><st c="8181">is also called the</st> **<st c="8201">dot-product</st>**
    <st c="8212">because</st> <st c="8220">we can also write it as</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3831.png)
    <st c="8245"><st c="8249">. Consequently, the</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><msup><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mi mathvariant="normal">⊤</mi></msup><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/3832.png) <st c="8269"><st
    c="8270">function is</st> <st c="8282">an example of a</st> **<st c="8299">dot-product
    kernel</st>**<st c="8317">. More</st> <st c="8324">generally, a dot-product kernel
    is any function of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3831.png)<st
    c="8375"><st c="8379">. So, the function that follows, where</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)
    <st c="8418"><st c="8419">is any univariate function, is a</st> <st c="8453">dot-product
    kernel:</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mi>g</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mspace width="0.25em" /></mrow></mrow></math>](img/3835.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8488">Eq.1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8492">Commonly used forms of dot-product kernels are where the function</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>g</mi><mfenced
    open="(" close=")"><mi>s</mi></mfenced><mo>=</mo><msup><mfenced open="(" close=")"><mrow><mn>1</mn><mo>+</mo><mi>c</mi><mi>s</mi></mrow></mfenced><mi>p</mi></msup></mrow></mrow></math>](img/3836.png)<st
    c="8559"><st c="8576">. These are called polynomial dot-product kernels because
    the resulting kernel function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/2990.png)
    <st c="8664"><st c="8665">is a polynomial of the dot-product</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3831.png)
    <st c="8701"><st c="8705">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8706">Another commonly used class of kernel</st> <st c="8744">functions
    is</st> <st c="8757">the</st> **<st c="8762">translationally invariant</st>**
    <st c="8787">kernels.</st> <st c="8797">These are kernels of the</st> <st c="8822">following
    form:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mi>h</mi><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>−</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/3839.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8839">Eq.2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8843">They are called translationally invariant because translating the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="8910"><st c="8911">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="8916"><st c="8917">vectors by adding a constant vector</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/469.png)
    <st c="8954"><st c="8955">to both does not change the value output by the kernel
    function.</st> <st c="9021">One of the</st> <st c="9032">most used kernels of
    this type is the</st> **<st c="9070">squared-exponential kernel</st>**<st c="9096">,
    as</st> <st c="9101">seen</st> <st c="9105">her</st><st c="9109">e:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>b</mi><mn>2</mn></msup></mrow></mfrac><msup><mfenced
    open="‖" close="‖"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>−</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/3843.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="9141">Eq.3</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9145">This kernel is also</st> <st c="9166">known as</st> <st c="9174">the</st>
    **<st c="9179">Radial Basis Function</st>** <st c="9200">(</st>**<st c="9202">RBF</st>**<st
    c="9205">) kernel.</st> <st c="9216">It is also sometimes called</st> <st c="9243">the</st>
    **<st c="9248">Gaussian kernel</st>**<st c="9263">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>b</mml:mi></mml:math>](img/286.png)
    <st c="9269"><st c="9270">parameter in</st> *<st c="9284">Eq.3</st>* <st c="9288">provides
    a scale on which kernel</st> <st c="9321">function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="9331"><st c="9385">measures the differences between the</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="9422"><st c="9423">vector and the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="9439"><st c="9440">vector .</st> <st c="9450">The smaller the value of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:math>](img/3848.png)
    <st c="9475"><st c="9476">the smaller the output value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="9509"><st c="9563">for the given</st> <st c="9577">vectors</st> ![<math
    xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>≠</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mrow></math>](img/3850.png)<st c="9585"><st
    c="9586">.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9587">You’ll also realize that if in addition, we restrict the vectors</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3851.png)
    <st c="9653"><st c="9654">to have a fixed length ( say</st> `<st c="9684">1</st>`<st
    c="9685">) so that they live on the surface of the unit-sphere, then the kernel
    function in</st> *<st c="9768">Eq.3</st>* <st c="9772">will look</st> <st c="9783">as
    fo</st><st c="9788">llows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>b</mi><mn>2</mn></msup></mrow></mfrac><mfenced
    open="[" close="]"><mrow><mn>2</mn><mo>−</mo><mn>2</mn><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced><mo>=</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><msup><mi>b</mi><mn>2</mn></msup></mfrac></mrow></mfenced><mo>×</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><msup><mi>b</mi><mn>2</mn></msup></mfrac><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mrow></math>](img/3852.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="9797">Eq.4</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9801">The right-hand side of</st> *<st c="9825">Eq.4</st>* <st c="9829">is
    an example of a dot-product kernel.</st> <st c="9869">By restricting the space
    from which the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="9909"><st c="9910">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="9915"><st c="9916">vectors come, we can change the properties of the kernel
    function.</st> <st c="9984">In the example in</st> *<st c="10002">Eq.4</st>*<st
    c="10006">, we have changed a translationally invariant kernel to a dot-product
    kernel.</st> <st c="10084">In fact, using the usual Taylor series representation
    of the exponential function, we can write the right-hand side of</st> *<st c="10203">Eq.4</st>*
    <st c="10207">as fo</st><st c="10213">llows:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><msup><mi>b</mi><mn>2</mn></msup></mfrac></mrow></mfenced><mo>×</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mfrac><mn>1</mn><msup><mi>b</mi><mn>2</mn></msup></mfrac><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><msup><mi>b</mi><mn>2</mn></msup></mfrac></mrow></mfenced><mo>×</mo><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi
    mathvariant="normal">∞</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><mo>!</mo></mrow></mfrac><msup><mfenced
    open="(" close=")"><mfrac><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow><msup><mi>b</mi><mn>2</mn></msup></mfrac></mfenced><mi>k</mi></msup></mrow></mrow></mrow></mrow></math>](img/3855.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="10222">Eq.5</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10226">So, when the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3856.png)
    <st c="10240"><st c="10241">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="10246"><st c="10247">vectors lie on the surface of the unit-sphere, we
    can think of the squared-exponential kernel as being the same as a polynomial
    dot-product kernel of</st> <st c="10398">infinite order.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10413">Perhaps more interesting is the fact that the kernel in</st> *<st
    c="10470">Equations 3 and 4</st>* <st c="10487">has a parameter,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:math>](img/3848.png)
    <st c="10505"><st c="10506">that we as a user specify and can vary.</st> <st c="10547">In
    fact, our simple polynomial dot-product kernel</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><msup><mfenced open="("
    close=")"><mrow><mn>1</mn><mo>+</mo><mi>c</mi><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mi>p</mi></msup></mrow></mrow></math>](img/3859.png)
    <st c="10597"><st c="10615">had user-specified parameters.</st> <st c="10646">We
    can think of both</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>c</mml:mi></mml:math>](img/3860.png)
    <st c="10667"><st c="10668">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)
    <st c="10673"><st c="10674">as being parameters of the polynomial dot-product
    kernel that we can vary.</st> <st c="10750">It is the fact that our kernel functions
    have parameters that we can vary that allows us to explore whole classes of new
    features efficiently and implicitly.</st> <st c="10908">We will learn about this
    later in</st> <st c="10942">this section.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10955">Kernel functions for other mathematical objects</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="11003">Can we generalize</st> <st c="11022">kernels further?</st> <st
    c="11039">Yes, we can.</st> <st c="11052">We can usually reduce any mathematical
    object to a set of numbers, whether real or complex.</st> <st c="11144">For example,
    a matrix is just a set of matrix elements, that is, a set of numbers, but they
    are usually arranged in a rectangular structure.</st> <st c="11285">It is the
    rectangular arrangement of the matrix elements that gives the matrix its overall
    interesting properties, but it is still just a set of numbers.</st> <st c="11439">That
    means we can store or represent the matrix as one long vector of numbers.</st>
    <st c="11518">Consequently, we can in principle construct a kernel function that
    takes two matrices as inputs and returns a scalar value.</st> <st c="11642">The
    precise details of how that matrix kernel function is constructed are up to us.</st>
    <st c="11726">For example, if we had two</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math>](img/733.png)
    <st c="11753"><st c="11754">symmetric real matrices,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/607.png)
    <st c="11780"><st c="11781">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/573.png)<st
    c="11786"><st c="11787">, we could define a function that measures the distance
    or dissimilarity between them.</st> <st c="11874">An example dissimilarity function
    might take the</st> <st c="11923">followin</st><st c="11931">g form:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>dissimilarity</mtext><mfenced
    open="(" close=")"><mrow><munder><munder><mi>A</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder><mo>,</mo><munder><munder><mi>B</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mtext>tr</mtext><mfenced
    open="[" close="]"><mrow><msup><mfenced open="(" close=")"><mrow><munder><munder><mi>A</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder><mo>−</mo><munder><munder><mi>B</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><mfenced open="(" close=")"><mrow><munder><munder><mi>A</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder><mo>−</mo><munder><munder><mi>B</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/3865.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11983">Eq.6</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11987">The expression in</st> *<st c="12006">Eq.6</st>* <st c="12010">is
    the square of</st> <st c="12027">the</st> `<st c="12298">1</st>` <st c="12299">minus
    the dissimilarity.</st> <st c="12325">Since we tend to use kernel functions for
    measuring similarity that would give us a kernel function for matrices of the</st>
    <st c="12445">followi</st><st c="12452">ng form:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><munder><mi>A</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder><mo>,</mo><munder><munder><mi>B</mi><mo stretchy="true">_</mo></munder><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><mtext>tr</mtext><mfenced
    open="[" close="]"><mrow><msup><mfenced open="(" close=")"><mrow><munder><munder><mi>A</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder><mo>−</mo><munder><munder><mi>B</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder></mrow></mfenced><mi
    mathvariant="normal">⊤</mi></msup><mfenced open="(" close=")"><mrow><munder><munder><mi>A</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder><mo>−</mo><munder><munder><mi>B</mi><mo
    stretchy="true">_</mo></munder><mo stretchy="true">_</mo></munder></mrow></mfenced></mrow></mfenced></mrow></mrow></math>](img/3869.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12463">Eq.7</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12467">What this illustrates</st> <st c="12489">is that we can construct
    kernel functions on spaces of any mathematical objects.</st> <st c="12571">We
    can consider a kernel function to be, more generally, a function that takes two
    objects,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3870.png)
    <st c="12663"><st c="12664">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3871.png)<st
    c="12669"><st c="12670">, as input and returns and scalar value.</st> <st c="12711">The
    mathematical objects</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3872.png)
    <st c="12736"><st c="12737">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3873.png)
    <st c="12742"><st c="12743">could be two matrices, two graphs, two strings, or
    two documents, as long as we can ultimately represent them mathematically.</st>
    <st c="12870">This means that when we use the kernel trick to implicitly construct
    new features, we can use it to implicitly construct new features for matrices,
    graphs, strings, documents, and so on.</st> <st c="13057">This gives us a powerful
    tool for extending learning algorithms that we typically think of as only applying
    to vectors and getting those learning algorithms to work for more exotic classes</st>
    <st c="13246">of objects.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13257">Combining kernels</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13275">The</st> <st c="13279">kernel functions in</st> *<st c="13300">Eq.1</st>*
    <st c="13304">–</st> *<st c="13307">3</st>* <st c="13308">are very simple.</st>
    <st c="13326">This does not mean that all kernels have a mathematically simple
    form.</st> <st c="13397">In fact, sometimes we want to construct more complex
    kernel functions from simpler ones by applying mathematical operations to combine
    several simple</st> <st c="13547">kernel functions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13564">The simplest mathematical operation we can consider is to just
    combine them linearly.</st> <st c="13651">That is, given two kernel functions,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3874.png)
    <st c="13688"><st c="13694">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3875.png)<st
    c="13698"><st c="13699">, that operate on mathematical objects</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/3876.png)
    <st c="13738"><st c="13739">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="13744"><st c="13767">, we can construct a</st> <st c="13788">new function:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>f</mi><mn>3</mn></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mo>=</mo><mi>a</mi><msub><mi>f</mi><mn>1</mn></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mo>+</mo><mi>b</mi><msub><mi>f</mi><mn>2</mn></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced></mrow></mrow></math>](img/3878.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13828">Eq.8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13832">If the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3879.png)
    <st c="13840"><st c="13841">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3880.png)
    <st c="13846"><st c="13847">functions are valid kernel functions, then so is the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/3881.png)
    <st c="13901"><st c="13902">function.</st> <st c="13913">Linear combination of
    kernel functions is also usually the most complex mathematical combination operation
    we are likely</st> <st c="14034">to perform.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14045">Positive semi-definite kernels</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="14076">Having introduced what a kernel function is, we’ll now highlight
    a specific subset of kernels: the positive semi-definite kernels.</st> <st c="14208">These
    are the kernel functions that we will use in the</st> <st c="14263">kernel trick.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14276">What is a positive semi-definite kernel?</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="14317">We have already</st> <st c="14334">said that</st> <st c="14344">we
    can think of a kernel as taking mathematical objects</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="14400"><st c="14401">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="14406"><st c="14429">that live in some mathematical space and mapping them
    to a scalar value.</st> <st c="14502">We can also use a kernel function to perform
    a mapping within the space from which</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="14585"><st c="14586">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="14591"><st c="14614">come.</st> <st c="14620">Consider the calculation</st>
    <st c="14645">t</st><st c="14646">hat follows:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>∫</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>v</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>ρ</mi><mfenced open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/3886.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14672">Eq.9</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14676">The integral in</st> *<st c="14693">Eq.9</st>* <st c="14697">is
    over the space from which the mathematical objects</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="14752"><st c="14753">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="14758"><st c="14768">are drawn.</st> <st c="14779">The result of the integration
    in</st> *<st c="14812">Eq.9</st>* <st c="14816">is a function of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/3889.png)<st
    c="14834"><st c="14835">, so we can think of the integration in</st> *<st c="14875">Eq.9</st>*
    <st c="14879">as mapping the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3890.png)
    <st c="14895"><st c="14896">function to some function of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="14926"><st c="14949">. The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ρ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3892.png)
    <st c="14955"><st c="14960">function is a weighting or measure function.</st>
    <st c="15005">Often, we take</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>ρ</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3892.png)
    <st c="15020"><st c="15025">to be some probability density function that gives
    the probability of getting the object</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="15114"><st c="15115">in the mathematical space we are</st> <st c="15149">dealing
    with.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15162">Now, what if the output function of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="15199"><st c="15222">that we got from the integration in</st> *<st c="15258">Eq.9</st>*
    <st c="15262">was just proportional to the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3896.png)
    <st c="15292"><st c="15293">input function?</st> <st c="15310">We would have the
    specific result</st> <st c="15344">th</st><st c="15346">at follows:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>λ</mi><mi>v</mi><mfenced
    open="(" close=")"><mi>y</mi></mfenced><mo>=</mo><mo>∫</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>v</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>ρ</mi><mfenced open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/3897.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="15380">Eq.10</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15385">Hang on a minute!</st> <st c="15404">We</st> <st c="15407">recognize</st>
    *<st c="15417">Eq.10</st>*<st c="15422">. It is our old friend, the</st> **<st
    c="15450">eigenfunction equation</st>**<st c="15472">. This means that if we can
    find particular functions</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3898.png)
    <st c="15526"><st c="15527">that satisfy</st> *<st c="15541">Eq.10</st>*<st c="15546">,
    then</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3899.png)
    <st c="15553"><st c="15554">is said to be an</st> **<st c="15572">eigenfunction</st>**
    <st c="15585">of the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3900.png)
    <st c="15593"><st c="15601">kernel and the constant of proportionality,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>λ</mml:mi></mml:math>](img/826.png)<st
    c="15645"><st c="15646">, in</st> *<st c="15651">Eq.10</st>* <st c="15656">is
    the</st> <st c="15664">corresponding</st> **<st c="15678">eigenvalue</st>**<st
    c="15688">.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15689">How does this help us?</st> <st c="15713">The eigenfunctions and
    eigenvalues help us characterize the kernel.</st> <st c="15781">There is one characteristic
    we are interested in for the kernel trick, and that is positive definiteness.</st>
    <st c="15887">A kernel function is said to be</st> **<st c="15919">positive definite</st>**
    <st c="15936">if all its eigenvalues are</st> <st c="15963">greater than zero,
    and</st> **<st c="15987">positive semi-definite</st>** <st c="16009">if all its
    eigenvalues are non-negative.</st> <st c="16051">From the fact that all the eigenvalues
    are non-negative, we can show that the property of positive semi-definiteness
    is equivalent to</st> <st c="16185">the relationship</st> <st c="16201">that follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>∫</mo><mi>f</mi><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>v</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>v</mi><mfenced open="(" close=")"><mi>y</mi></mfenced><mi>ρ</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>ρ</mi><mfenced open="(" close=")"><mi>y</mi></mfenced><mi>d</mi><mi>x</mi><mi>d</mi><mi>y</mi><mo>≥</mo><mn>0</mn><mtext>for</mtext><mtext>any</mtext><mtext>function</mtext><mi>v</mi></mrow></mrow></math>](img/3902.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16262">Eq.11</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16267">The relationship in</st> *<st c="16288">Eq.11</st>* <st c="16293">is
    the more fundamental definition of positive semi-definiteness, but the standard
    way to prove that a kernel function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3903.png)
    <st c="16413"><st c="16421">is positive semi-definite is to determine its eigenfunctions
    and eigenvalues and see whether any of the eigenvalues</st> <st c="16537">are
    negative.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16550">Great.</st> <st c="16558">We now have all the mathematical pieces
    in place to introduce the</st> <st c="16624">kernel trick.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16637">Mercer’s theorem and the kernel trick</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="16675">The kernel trick is</st> <st c="16696">based upon</st> **<st
    c="16707">Mercer’s theorem</st>**<st c="16723">. Mercer’s theorem says the following:
    if we have a positive semi-definite kernel function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3904.png)
    <st c="16814"><st c="16822">then the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/3905.png)
    <st c="16840"><st c="16841">represents the value of the inner product between</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3906.png)
    <st c="16892"><st c="16893">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3907.png)<st
    c="16898"><st c="16899">, where the mathematical object</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/3906.png)
    <st c="16931"><st c="16932">is some mapping of the mathematical object</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="16976"><st c="16977">to a new</st> <st c="16987">mathematical space.</st></st></st></st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17006">Okay, that definition of Mercer’s theorem is a bit abstract, so
    we’ll make it more concrete by going back to using</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>d</mml:mi></mml:math>](img/596.png)<st
    c="17122"><st c="17123">-dimensional vectors</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="17144"><st c="17145">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)<st
    c="17150"><st c="17151">. If we have a positive semi-definite kernel function,</st>
    <st c="17206">f</st><st c="17207">(</st><st c="17208">x</st><st c="17209">_</st><st
    c="17210">,</st> <st c="17211">y</st><st c="17212">_</st><st c="17213">)</st><st
    c="17214">, then the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="17234"><st c="17288">represents the inner product between the</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3914.png)
    <st c="17329"><st c="17330">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3915.png)
    <st c="17335"><st c="17336">vectors, where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3914.png)
    <st c="17352"><st c="17353">is the mapping of the vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="17383"><st c="17384">to a new vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3918.png)<st
    c="17401"><st c="17402">, and likewise</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3919.png)
    <st c="17417"><st c="17418">is the mapping of the vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1569.png)
    <st c="17448"><st c="17449">to a new vector</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3921.png)<st
    c="17466"><st c="17467">. This means that from</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)<st
    c="17490"><st c="17491">, we have constructed new features</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3923.png)
    <st c="17526"><st c="17527">and calculated the inner product</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3924.png)
    <st c="17561"><st c="17562">using the</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mi mathvariant="normal">k</mi><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">n</mi><mi
    mathvariant="normal">e</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">f</mi><mi
    mathvariant="normal">u</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi
    mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi
    mathvariant="normal">n</mi></mrow></mrow></math>](img/3925.png)<st c="17573"><st
    c="17596">. In math, we are saying</st> <st c="17621">the following:</st></st></st></st></st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><munder><mi mathvariant="normal">Φ</mi><mo
    stretchy="true">_</mo></munder><mfenced open="(" close=")"><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder></mfenced><mo>∙</mo><munder><mi mathvariant="normal">Φ</mi><mo
    stretchy="true">_</mo></munder><mfenced open="(" close=")"><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mfenced></mrow></mrow></math>](img/3926.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="17637">Eq.12</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="17642">However, here is the subtlety: at no point did we say what the
    new features</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3927.png)
    <st c="17719"><st c="17720">were.</st> <st c="17727">Mercer’s theorem told us
    that we didn’t have to.</st> <st c="17776">Mercer’s theorem told us that using
    the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3928.png)
    <st c="17816"><st c="17817">kernel function is equivalent to implicitly computing
    inner products in some new feature space.</st> <st c="17914">This is the</st>
    <st c="17926">kernel trick.</st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17939">Why is this useful?</st> <st c="17960">Remember</st> <st c="17968">that
    many of the kernels that we have already introduced have some parameters in them.</st>
    <st c="18056">By varying the kernel function parameters, we are effectively varying
    the new features that we are implicitly creating.</st> <st c="18176">Varying the
    parameters is the same as varying or exploring across a whole set of new</st>
    <st c="18261">feature spaces.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18276">Mercer’s theorem – an example</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="18306">Let’s look at a simple but</st> <st c="18333">explicit example
    of Mercer’s theorem in action.</st> <st c="18382">We’ll return to our features</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3929.png)
    <st c="18411"><st c="18412">in</st> *<st c="18416">Figures 12.1</st>* <st c="18428">and</st>
    *<st c="18433">12.2</st>*<st c="18437">. If we have two points,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3930.png)
    <st c="18462"><st c="18472">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3931.png)<st
    c="18476"><st c="18480">, in our original feature space, then the inner product
    between t</st><st c="18545">hem is</st> <st c="18553">as follows:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo stretchy="true">_</mo></munder><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>2</mn></msub></mrow></mrow></math>](img/3932.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="18579">Eq.13</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18584">Now we’ll explicitly construct a new feature space,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3933.png)<st
    c="18637"><st c="18638">. From the existing</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3934.png)
    <st c="18658"><st c="18668">feature vector, we define our feature vector</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3935.png)
    <st c="18713"><st c="18714">to be</st> <st c="18721">as follows:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><mfenced
    open="(" close=")"><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>,</mo><msqrt><mn>2</mn></msqrt><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub></mrow></mfenced></mrow></mrow></math>](img/3936.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="18758">Eq.14</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18763">Similarly, the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3937.png)
    <st c="18779"><st c="18780">point in the original feature space maps to</st> <st
    c="18825">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><mfenced
    open="(" close=")"><mrow><msubsup><mi>y</mi><mn>1</mn><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>y</mi><mn>2</mn><mn>2</mn></msubsup><mo>,</mo><msqrt><mn>2</mn></msqrt><msub><mi>y</mi><mn>1</mn></msub><msub><mi>y</mi><mn>2</mn></msub></mrow></mfenced></mrow></mrow></math>](img/3938.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="18862">Eq.15</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18867">The inner product,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math>](img/3939.png)
    <st c="18887"><st c="18888">in the new feature space is then given</st> <st c="18927">by</st>
    <st c="18931">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>∙</mo><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><msubsup><mi>y</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><msubsup><mi>y</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>1</mn></msub><msub><mi>y</mi><mn>2</mn></msub><mo>=</mo><msup><mfenced
    open="(" close=")"><mrow><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>2</mn></msub></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/3940.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="18997">Eq.16</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19002">Comparing the right-hand side of</st> *<st c="19036">Eq.16</st>*
    <st c="19041">to</st> *<st c="19045">Eq.13</st>*<st c="19050">, we can see that
    it is the same as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/3941.png)<st
    c="19086"><st c="19087">. This means that, for this example, we h</st><st c="19128">ave</st>
    <st c="19133">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>x</mi><mo stretchy="true">_</mo></munder></mfenced><mo>∙</mo><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mfenced open="("
    close=")"><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mfenced><mo>=</mo><msup><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mn>2</mn></msup></mrow></mrow></math>](img/3942.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="19168">Eq.17</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19173">In this example, we have explicitly specified what the mapping
    from the original feature space</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="19269"><st c="19270">to the new feature space</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3944.png)
    <st c="19296"><st c="19297">was.</st> <st c="19303">However, we didn’t have to.</st>
    *<st c="19331">Eq.17</st>* <st c="19336">tells us that if all we want or need
    to do is compute inner products in the new feature space, then we don’t have to
    know what the mapping from the original to the new feature space is.</st> <st
    c="19523">We can just use the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/3945.png)
    <st c="19543"><st c="19544">polynomial dot-product kernel to compute inner products
    in the new feature space.</st> <st c="19627">This is the kernel trick</st> <st
    c="19652">in action.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19662">For our explicit example, the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:math>](img/3946.png)
    <st c="19693"><st c="19694">mapping only produced a finite number of features.</st>
    <st c="19746">We went from a two-dimensional feature vector to a three-dimensional
    feature vector.</st> <st c="19831">Is this always the case?</st> <st c="19856">You
    can probably guess by looking at</st> *<st c="19893">Eq.17</st>* <st c="19898">that
    the fact that using the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/3947.png)
    <st c="19928"><st c="19929">polynomial dot-product kernel is equivalent to constructing
    a finite-dimensional feature vector stems from the fact that the order of the
    polynomial is finite, that is, we just have a quadratic dot-product kernel.</st>
    <st c="20144">You’d be right.</st> <st c="20160">Using a higher-order polynomial
    dot-product, such as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math>](img/3948.png)<st
    c="20213"><st c="20228">, will lead to more implicit features, but still a finite
    number</st> <st c="20293">of</st> <st c="20295">them.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20301">Wait, I hear you say.</st> *<st c="20324">Eq.5</st>* <st c="20328">tells
    us that when</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/2281.png)
    <st c="20348"><st c="20349">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3823.png)
    <st c="20354"><st c="20355">are on the surface of the unit-sphere, then the squared-exponential
    kernel is equivalent to a polynomial dot-product kernel of infinite order.</st>
    <st c="20499">So, does using a squared-exponential kernel mean that we would implicitly
    be creating an infinite number of new features?</st> <st c="20621">Yes, it does.</st>
    <st c="20635">By using different kernel functions, we can explore a wide range
    of different new feature spaces.</st> <st c="20733">This is the power</st> <st
    c="20751">of the kernel trick – it allows us to efficiently explore new</st> <st
    c="20813">feature spaces.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20828">So, how do we use Mercer’s theorem and the kernel trick in a learning
    algorithm?</st> <st c="20910">This is what we will</st> <st c="20931">cover next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20942">Kernelized algorithms</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="20964">Mercer’s theorem tells us that we</st> <st c="20998">can take
    a positive semi-definite kernel function</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/3951.png)
    <st c="21049"><st c="21050">and use it to calculate the inner-product values in
    our inner-product based learning algorithm.</st> <st c="21147">This is often</st>
    <st c="21161">called</st> **<st c="21168">kernelizing</st>** <st c="21179">the
    algorithm.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21194">Take PCA as an example of an inner-product based learning algorithm.</st>
    <st c="21264">We know from Mercer’s theorem that computing the inner products
    using a kernel function will be equivalent to doing the PCA in some new feature
    space, even though we do not (necessarily) know what that new feature space is.</st>
    <st c="21488">The new kernelized version of the learning algorithm is called</st>
    **<st c="21550">kernel PCA</st>**<st c="21561">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21562">The great thing about using Mercer’s theorem in this way is that
    if our kernel function has some parameters, we can treat those parameters as hyper-parameters
    and vary them until we minimize some loss function, or until we achieve maximal
    prediction accuracy on a validation set.</st> <st c="21843">Each kernel function
    parameter value is equivalent to performing a PCA in some new feature space.</st>
    <st c="21941">Therefore, varying the parameters is equivalent to performing a
    whole family of PCAs across a whole family of new feature spaces until we find
    the feature space that gives us the best</st> <st c="22125">prediction accuracy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22145">In general, the approach to kernelizing any inner-product based
    learning algorithm is simple.</st> <st c="22240">It is</st> <st c="22246">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22257">Take your inner-product based</st> <st c="22288">learning algorithm.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="22307">Replace the inner product calculations with kernel</st> <st c="22359">function
    evaluations.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="22380">That’s it.</st> <st c="22392">You’re good</st> <st c="22404">to
    go.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22410">In practice, thinking about the original learning algorithm solely
    in terms of inner product calculations and learning how to express all calculations
    solely in terms of inner products can take a bit of getting used to, but once
    you do, the preceding steps are all there is</st> <st c="22685">to it.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="22691">If kernelizing an inner-product based learning algorithm is simple,
    you might ask: is there much of a difference between the original learning algorithm
    and its kernelized version?</st> <st c="22873">The answer is no.</st> <st c="22891">We
    can think of the original learning algorithm as just a special case of its</st>
    <st c="22968">kernelized version that uses a linear kernel function.</st> <st
    c="23024">For example, our original PCA algorithm, explained in</st> [*<st c="23078">Chapter
    3</st>*](B19496_03.xhtml#_idTextAnchor141)<st c="23087">, is just an example of
    kernel PCA that uses a linear dot-product kernel, and so uses a kernel function
    of the</st> <st c="23198">following form:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><munder><mi>x</mi><mo
    stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo stretchy="true">_</mo></munder></mrow></mrow></math>](img/3952.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="23215">Eq.18</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23220">That short section on how we can, in principle, use the kernel
    trick concludes this section on kernels, so let’s recap what we have learned about
    kernels and the</st> <st c="23383">kernel trick.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23396">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23412">In this section, we have learned</st> <st c="23446">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23460">A kernel function maps two mathematical objects to a</st> <st
    c="23514">scalar value</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="23526">About commonly used kernels, such as dot-product kernels and translationally
    invariant kernels, that are applied</st> <st c="23640">to vectors</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="23650">How kernel functions can be combined in</st> <st c="23691">linear
    combinations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="23710">About positive definite and positive semi-definite</st> <st c="23762">kernel
    functions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="23778">How Mercer’s theorem tells us that a positive semi-definite kernel
    function corresponds to an inner product calculation in some unknown (implicit)
    new</st> <st c="23930">feature space</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="23943">The kernel trick uses Mercer’s theorem to replace the inner products
    in an inner-product based learning algorithm with a kernel function to create
    a kernelized version of the</st> <st c="24119">learning algorithm</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="24137">Having learned about the ideas behind how to create kernelized
    learning algorithms, in the next section, we will see an example of a kernelized
    algorithm</st> <st c="24292">in action.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24302">An example of a kernelized learning algorithm</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="24348">To illustrate the simplicity of kernelized algorithms we’ll demonstrate
    with a code example for a specific inner-product based learning algorithm.</st>
    <st c="24496">The algorithm we’ll use is</st> **<st c="24523">Fisher Discriminant
    Analysis</st>** <st c="24551">(</st>**<st c="24553">FDA</st>**<st c="24556">),
    which is an algorithm for assigning points to class labels.</st> <st c="24620">The
    standard version of FDA is a form of</st> **<st c="24661">Linear Discriminant
    Analysis</st>** <st c="24689">(</st>**<st c="24691">LDA</st>**<st c="24694">).</st>
    <st c="24698">When we run the</st> <st c="24713">kernelized version of the FDA,
    we will be doing</st> **<st c="24762">Kernel Fisher Discriminant</st>** **<st
    c="24789">Analysis</st>** <st c="24797">(</st>**<st c="24799">kFDA</st>**<st c="24803">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24806">kFDA code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="24824">We will start with the example</st> <st c="24855">data in</st>
    *<st c="24864">Figure 12</st>**<st c="24873">.1</st>*<st c="24875">. The classes
    are linearly separable, so we’ll use a linear Fisher discriminant to construct
    a classifier.</st> <st c="24982">A linear discriminant for a two-class problem
    uses the orthogonal distance of a point</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="25068"><st c="25069">from a line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3954.png)
    <st c="25082"><st c="25083">to determine which class the point is in.</st> <st
    c="25126">If the point</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="25139"><st c="25140">is one side of the line, we say it is in class</st>
    `<st c="25188">1</st>`<st c="25189">, while if it is on the other side of the
    line, we say it is in class</st> `<st c="25259">2</st>`<st c="25260">. The line</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3954.png)
    <st c="25271"><st c="25272">is our predictive model that we need</st> <st c="25310">to
    train.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25319">Measuring how far a point is from the line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3954.png)
    <st c="25363"><st c="25364">is equivalent to measuring how far the point</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="25410"><st c="25411">is along the line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3959.png)<st
    c="25430"><st c="25431">, which is orthogonal to the line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3960.png)<st
    c="25465"><st c="25466">. This means that we can express the classifier as the
    mathematical condition</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>></mml:mo><mml:mi>c</mml:mi></mml:math>](img/3961.png)<st
    c="25544"><st c="25545">, where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>c</mml:mi></mml:math>](img/3860.png)
    <st c="25553"><st c="25554">is some constant.</st> <st c="25573">If the point</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="25586"><st c="25587">satisfies this condition the point is in one class,
    if it doesn’t satisfy this mathematical condition it is in the</st> <st c="25703">other
    class.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25715">Training the linear discriminant is the process of determining
    the optimal line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3954.png)
    <st c="25796"><st c="25797">which minimizes the classification error on the training
    dataset.</st> <st c="25864">Determining the optimal line</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3954.png)
    <st c="25893"><st c="25894">is equivalent to finding the optimal line</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3966.png)<st
    c="25937"><st c="25938">. Since we can see from</st> *<st c="25962">Figure 12</st>**<st
    c="25971">.1</st>* <st c="25973">that the two classes (the red and the blue points)
    can be separated by a straight line, we know that a linear discriminant using
    just the features we have,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3234.png)
    <st c="26130"><st c="26131">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3814.png)<st
    c="26136"><st c="26137">, will be sufficient to achieve a high-level</st> <st
    c="26182">of accuracy.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26194">I have written a Python class,</st> `<st c="26226">KFDA_Poly</st>`<st
    c="26235">, which allows us to do kFDA.</st> <st c="26265">I have kept things
    simple; it only allows us to do kFDA using pure polynomial dot-product kernels
    of the form</st> <st c="26375">that follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>k</mi><mfenced
    open="(" close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>,</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mo>=</mo><msup><mfenced open="("
    close=")"><mrow><munder><mi>x</mi><mo stretchy="true">_</mo></munder><mo>∙</mo><munder><mi>y</mi><mo
    stretchy="true">_</mo></munder></mrow></mfenced><mi>p</mi></msup></mrow></mrow></math>](img/3969.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26405">Eq.19</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26410">The</st> `<st c="26415">KFDA_Poly</st>` <st c="26424">class is
    defined in the</st> `<st c="26449">kernel_fda.py</st>` <st c="26462">module, which
    can be found in the Chapter12 directory of the GitHub repository.</st> <st c="26543">When
    calling the constructor for the</st> `<st c="26580">KFDA_Poly</st>` <st c="26589">class,
    we specify the degree</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="26619"><st c="26620">of the kernel that we want to use.</st> <st c="26656">To
    do linear FDA, we specify the</st> <st c="26689">degree</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>](img/3971.png)<st
    c="26696"><st c="26697">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26698">The data for</st> *<st c="26712">Figure 12</st>**<st c="26721">.1</st>*
    <st c="26723">is in the</st> `<st c="26734">lda_ex1.csv</st>` <st c="26745">file
    in the</st> `<st c="26758">Data</st>` <st c="26762">directory of the GitHub repository.</st>
    <st c="26799">We have labeled the classes</st> `<st c="26827">1</st>` <st c="26828">and</st>
    `<st c="26833">2</st>` <st c="26834">rather than</st> `<st c="26847">red</st>`
    <st c="26850">and</st> `<st c="26855">blue</st>`<st c="26859">. Class</st> `<st
    c="26867">1</st>` <st c="26868">corresponds to the blue points, while class</st>
    `<st c="26913">2</st>` <st c="26914">corresponds to the red points.</st> <st c="26946">The
    code example that follows</st> <st c="26975">can be found in the</st> `<st c="26996">Code_Examples_Chap12.ipynb</st>`
    <st c="27022">Jupyter notebook in the GitHub repository.</st> <st c="27066">First,
    we’ll read in</st> <st c="27087">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27201">Let’s look at</st> <st c="27216">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27273">This gives the table that follows, from which we can see that
    we have 1,000 datapoints, each consisting of the two features</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/3234.png)<st
    c="27398"><st c="27399">,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/3814.png)
    <st c="27401"><st c="27402">and the</st> <st c="27411">class label:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27693">Now we’ll build a linear Fisher discriminant.</st> <st c="27740">We’ll
    instantiate a</st> `<st c="27760">KFDA_Poly</st>` <st c="27769">object with a
    linear kernel (</st>`<st c="27799">degree=1</st>`<st c="27808">).</st> <st c="27812">Specifying
    a linear kernel is saying that we are going to do kFDA but with a linear kernel,
    so</st> <st c="27906">this is equivalent to</st> <st c="27929">linear FDA:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28026">Now we’ll fit the linear classifier using the training data we
    have just</st> <st c="28100">read in:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28212">We can then score the trained linear classifier on the training
    set using the built-in</st> <st c="28300">score function:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28398">This gives an output of</st> `<st c="28423">0.998</st>`<st c="28428">.
    We can see that the model scores very well on the training set.</st> <st c="28494">The
    proportion of the training points that the classifier correctly classifies is</st>
    `<st c="28576">0.998</st>`<st c="28581">, that is, nearly 100% accuracy on the
    training set.</st> <st c="28634">This is to be expected, as we know just by looking
    at</st> *<st c="28688">Figure 12</st>**<st c="28697">.1</st>* <st c="28699">that
    the two classes are separable by a straight line, so a properly trained linear
    classifier should be capable of fitting the training data nearly perfectly.</st>
    <st c="28860">We also know that this trained classifier would predict any hold-out
    datapoints accurately provided they are also drawn from the same distribution
    as the training data.</st> <st c="29029">Therefore, for the purposes of this example,
    there is no need to test our classifier on a</st> <st c="29119">holdout sample.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29134">Now we’ll repeat the process using the data from</st> *<st c="29184">Figure
    12</st>**<st c="29193">.2</st>*<st c="29195">. We know from looking at</st> *<st
    c="29221">Figure 12</st>**<st c="29230">.2</st>* <st c="29232">that a straight
    line can’t separate the two classes perfectly.</st> <st c="29296">Consequently,
    a trained linear Fisher discriminant should score poorly on the training data
    in</st> *<st c="29391">Figure 12</st>**<st c="29400">.2</st>*<st c="29402">.</st>
    <st c="29404">Let’s check.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29416">The data for</st> *<st c="29430">Figure 12</st>**<st c="29439">.2</st>*
    <st c="29441">is in the</st> `<st c="29452">lda_ex2.csv</st>` <st c="29463">file
    in the</st> `<st c="29476">Data</st>` <st c="29480">directory of the GitHub repository.</st>
    <st c="29517">It is in the same format as the previous example.</st> <st c="29567">First,
    we’ll read in</st> <st c="29588">the data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29664">Now we’ll repeat the</st> <st c="29686">process we went through
    with the first example and train a linear Fisher discriminant on this data.</st>
    <st c="29786">We will start by instantiating the</st> <st c="29821">linear classifier:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29925">Next, we’ll fit it to the training data from</st> *<st c="29971">Figure
    12</st>**<st c="29980">.2</st>*<st c="29982">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30088">Now, we’ll score the trained linear classifier on the training</st>
    <st c="30152">set data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30244">This gives an output of</st> `<st c="30269">0.502</st>`<st c="30274">.
    We can see the score on the training set is close to</st> `<st c="30329">0.5</st>`<st
    c="30332">, that is, only about 50% accuracy.</st> <st c="30368">This is a lot
    lower than in our first example.</st> <st c="30415">This is to be expected.</st>
    <st c="30439">No straight line can separate the two classes in</st> *<st c="30488">Figure
    12</st>**<st c="30497">.2</st>*<st c="30499">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30500">Can you think why the accuracy on the training set was close to</st>
    `<st c="30565">0.5</st>`<st c="30568">, even though we have trained (or rather,
    optimized) this linear classifier on the</st> <st c="30651">training data?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30665">We know that the red and blue points in</st> *<st c="30706">Figure
    12</st>**<st c="30715">.2</st>* <st c="30717">are separated by the</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3807.png)
    <st c="30739"><st c="30755">boundary.</st> <st c="30765">So, if a point</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="30780"><st c="30781">has</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>></mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3976.png)
    <st c="30786"><st c="30796">, it is in the red class, while if</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/1816.png)
    <st c="30831"><st c="30832">has</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo><</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3978.png)
    <st c="30837"><st c="30847">, it is in the blue class.</st> <st c="30874">This
    tells us that if we had a perfect classifier for this dataset, we could write
    our classifier condition as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>></mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3979.png)<st
    c="30985">![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>></mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/3980.png)<st
    c="30992"><st c="30993">. This classifier condition can also be written as</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msup><munder><mi>β</mi><mo
    stretchy="true">_</mo></munder><mi mathvariant="normal">⊤</mi></msup><munder><mi
    mathvariant="normal">Φ</mi><mo stretchy="true">_</mo></munder><mo>></mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow></mrow></math>](img/3981.png)
    <st c="31044"><st c="31045">, where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/3982.png)
    <st c="31053"><st c="31074">and</st> <st c="31078">β</st><st c="31079">_</st>
    <st c="31080">=</st> <st c="31081">(</st><st c="31082">1,1</st><st c="31085">,</st>
    <st c="31086">0</st><st c="31087">)</st><st c="31088">. This is in the form of
    a linear discriminant classifier, but one where we are using a new feature vector</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3944.png)<st
    c="31195"><st c="31196">. However, the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/3944.png)
    <st c="31211"><st c="31212">vector is precisely the new feature vector that was
    implicitly created when we used a quadratic dot-product kernel in our Mercer’s
    theorem example in the previous section.</st> <st c="31385">This suggests that
    if we train a kernel Fisher discriminant classifier using a quadratic dot-product
    kernel</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>,</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>∙</mml:mo><mml:munder
    underaccent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/3985.png)<st
    c="31493"><st c="31494">, the trained classifier should be capable of perfectly
    separating the red and the blue points in the training data shown in</st> *<st
    c="31619">Figure 12</st>**<st c="31628">.2</st>*<st c="31630">.</st> <st c="31632">Let’s
    see.</st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31642">First, we will instantiate a</st> <st c="31671">kernel classifier
    object by specifying a polynomial dot-product kernel of</st> <st c="31746">degree</st>
    `<st c="31753">2</st>`<st c="31754">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="31866">Next, we will fit the kernel Fisher discriminant to the training
    data from</st> *<st c="31942">Figure 12</st>**<st c="31951">.2</st>*<st c="31953">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32076">Finally, we’ll score the trained kernel Fisher discriminant classifier
    on the training data.</st> <st c="32170">We should get something a lot higher
    than</st> `<st c="32212">0.5</st>` <st c="32215">and much closer</st> <st c="32232">to</st>
    `<st c="32235">1</st>`<st c="32236">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32368">This gives an output of</st> `<st c="32393">0.911</st>`<st c="32398">,
    so we do get a trained classifier that fits the training data much better than
    a standard linear Fisher discriminant.</st> <st c="32518">The reason the trained
    classifier doesn’t fit the training data perfectly, that is, that the accuracy
    proportion is not</st> `<st c="32638">1</st>`<st c="32639">, is simply due to
    sampling variation.</st> <st c="32678">If we increased the size of the training
    data, we would get closer and closer to a score</st> <st c="32767">of</st> `<st
    c="32770">1</st>`<st c="32771">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32772">The code example shows how easy it is to use kernelized algorithms
    in practice.</st> <st c="32853">It is usually as simple as specifying the kernel
    that we want to use and then running the algorithm as we would normally run the
    un-kernelized version.</st> <st c="33005">The use of non-linear kernels in the
    algorithm means we can learn the non-linear structure present in the data by implicitly
    creating new features.</st> <st c="33153">Learning this non-linear structure is
    not something that the un-kernelized version of the algorithm is</st> <st c="33256">capable
    of.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33267">In the code example, we were</st> <st c="33297">able to deduce
    which kernel to use a priori.</st> <st c="33342">In real-world situations, the
    choice of kernel is something we would typically experiment with or optimize as
    a hyper-parameter of the algorithm.</st> <st c="33488">Due to the simplicity of
    using the kernelized algorithm, varying the parameters of the kernel is</st> <st
    c="33585">not difficult.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33599">That concludes the demonstration of a real kernelized algorithm.</st>
    <st c="33665">We’ll recap what we have learned from in this section and then wrap
    up the</st> <st c="33740">chapter overall.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33756">What we learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33772">In this section, we have learned</st> <st c="33806">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33820">How running a kernelized algorithm is as simple as selecting</st>
    <st c="33882">a kernel</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33890">How a kernelized algorithm can correctly learn the non-linear
    structure present in a dataset, while the standard linear version of the</st>
    <st c="34026">algorithm cannot</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34042">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="34050">This chapter has been focused on kernel methods, which are also
    called kernelized algorithms.</st> <st c="34145">The chapter has been short so
    that we can focus on the most important concepts underpinning kernel methods.</st>
    <st c="34253">Those concepts are</st> <st c="34272">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34283">Inner-product based learning algorithms are very common because
    an inner product captures the similarity between feature vectors, and learning
    by similarity is a natural basis for many machine</st> <st c="34477">learning
    algorithms.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34497">Inner products calculated from the existing features on a dataset
    may not be sufficient to learn the non-linear structure present in</st> <st c="34631">the
    dataset.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34643">Construction of new features can be necessary to make our learning</st>
    <st c="34711">algorithms accurate.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34731">Mercer’s theorem tells us that positive semi-definite kernel functions
    implicitly construct new features and calculate inner products in those new</st>
    <st c="34879">feature spaces.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34894">There are different types of</st> <st c="34924">kernel functions.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34941">We can use the kernel trick to kernelize any inner-product based</st>
    <st c="35007">learning algorithm.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="35026">Using kernelized algorithms in practice can be as simple as specifying
    a choice of kernel and</st> <st c="35121">its parameters.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="35136">By varying the parameters of a kernel, we can effectively explore
    many different new feature spaces.</st> <st c="35238">This can be an efficient
    way to learn the non-linear structure in</st> <st c="35304">a dataset.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="35314">Our next chapter is focused on another advanced topic.</st> <st
    c="35370">It is one you have probably heard of but may not be that familiar with.</st>
    <st c="35442">The topic of the next chapter is</st> **<st c="35475">information
    theory</st>**<st c="35493">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35494">Exercises</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35504">We have already given a lengthy code example of a simple kernelized
    algorithm in the main part of this chapter.</st> <st c="35617">Therefore, for
    the exercises, we will demonstrate some of the more complex aspects of kernel
    methods.</st> <st c="35719">Due to this increase in complexity, we will only ask
    a single question.</st> <st c="35791">It is intentionally challenging, so don’t
    be surprised if you don’t manage to complete it fully.</st> <st c="35888">Have
    a go at answering the exercise and then compare your answer to the one in the</st>
    `<st c="35971">Answers_to_Exercises_Chap12.ipynb</st>` <st c="36004">Jupyter notebook
    in the</st> <st c="36029">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36047">The data in the</st> `<st c="36064">kernel_PCA_matrix_data.csv</st>`
    <st c="36090">file in the</st> `<st c="36103">Data</st>` <st c="36107">directory
    of the GitHub repository contains the matrix elements of</st> `<st c="36175">N=200</st>`<st
    c="36180">, 4x4 matrices.</st> <st c="36196">Each matrix corresponds to a single
    row of the</st> `<st c="36243">.csv</st>` <st c="36247">file.</st> <st c="36254">The
    column headings are of the form</st> `<st c="36290">i_j</st>`<st c="36293">, where</st>
    `<st c="36301">i</st>` <st c="36302">and</st> `<st c="36307">j</st>` <st c="36308">are
    integers, representing the i,j matrix element.</st> <st c="36360">For example,
    the column with the</st> `<st c="36393">1_3</st>` <st c="36396">heading holds
    the</st> `<st c="36415">1,3</st>` <st c="36418">matrix elements of each matrix.</st>
    <st c="36451">Use the data to perform a kernel PCA of the matrix data, where each
    matrix is a single datapoint.</st> <st c="36549">Use the matrix kernel function
    in</st> *<st c="36583">Eq.7</st>* <st c="36587">to calculate inner products between
    any two matrices.</st> <st c="36642">You should plot the datapoints in a PCA score
    plot and comment on what</st> <st c="36713">you see.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="36721">You will find it useful to reshape the data in the</st> `<st c="36773">.csv</st>`
    <st c="36777">file into a Python list of matrices, that is,</st> <st c="36824">square
    arrays.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="36838">You can use the</st> `<st c="36855">numpy.linalg.norm</st>` <st
    c="36872">NumPy function to calculate the Frobenius norm used in</st> *<st c="36928">Eqs.</st>
    <st c="36933">6</st>* <st c="36934">and</st> *<st c="36938">7</st>*<st c="36939">.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="36940">Review the material in</st> [*<st c="36964">Chapter 3</st>*](B19496_03.xhtml#_idTextAnchor141)
    <st c="36973">on how to do PCA using just the Gram matrix of the centered data
    matrix.</st> <st c="37047">The</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:math>](img/3986.png)
    <st c="37051"><st c="37052">matrix element of the Gram matrix is the inner product
    between the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/909.png)
    <st c="37120"><st c="37132">centered datapoint and the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math>](img/1061.png)
    <st c="37159"><st c="37163">centered datapoint.</st></st></st></st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="37182">You will need to center the data, but this centering needs to
    be done in the new feature space that is implicitly created by our kernel function.</st>
    <st c="37329">Unfortunately, you don’t know what the new features are.</st> <st
    c="37386">Fortunately, you don’t have to.</st> <st c="37418">If the</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math>](img/726.png)
    <st c="37425"><st c="37441">Gram matrix of the uncentered data has matrix elements</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/3990.png)<st
    c="37496"><st c="37497">, then the matrix elements,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/3991.png)<st
    c="37525"><st c="37526">, of the Gram matrix of the centered data can be calculated
    via the</st> <st c="37594">following equation:</st></st></st></st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>G</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>j</mi><mo>′</mo></mrow></munder><mrow><msub><mi>G</mi><mrow><mi>i</mi><mrow><mi>j</mi><mo>′</mo></mrow></mrow></msub><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>i</mi><mo>′</mo></mrow></munder><msub><mi>G</mi><mrow><mrow><mi>i</mi><mo>′</mo></mrow><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>+</mo><mfrac><mn>1</mn><msup><mi>N</mi><mn>2</mn></msup></mfrac><mrow><munder><mo>∑</mo><mrow><mrow><mi>i</mi><mo>′</mo></mrow><mo>,</mo><mrow><mi>j</mi><mo>′</mo></mrow></mrow></munder><msub><mi>G</mi><mrow><mrow><mi>i</mi><mo>′</mo></mrow><mrow><mi>j</mi><mo>′</mo></mrow></mrow></msub></mrow></mrow></mrow></math>](img/3992.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37615">Eq.20</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37620">You will need to do an eigen-decomposition of the centered Gram
    matrix.</st> <st c="37693">The centered Gram matrix is real and symmetric, so
    you should use the</st> `<st c="37763">numpy.linalg.eigh</st>` <st c="37780">NumPy
    function to do</st> <st c="37802">this eigen-decomposition.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
