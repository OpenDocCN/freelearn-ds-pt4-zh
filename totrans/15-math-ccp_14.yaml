- en: <st c="0">1</st><st c="2">3</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Information Theory</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="21">Information theory and information-theoretic concepts are very useful,
    but it is unlikely that you will have to formally make use of them as a data scientist.</st>
    <st c="181">By this, we mean that you are unlikely to have to use detailed information-theoretic
    mathematical proofs or techniques in your work.</st> <st c="314">But – and it’s
    an important but – the ideas and ways of thinking that information theory introduces
    are worth understanding.</st> <st c="439">And that is what this chapter aims to
    achieve.</st> <st c="486">To do that, we will cover the</st> <st c="516">following
    topics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="533">What is information and why is it useful?</st>*<st c="575">: Here,
    we’ll define precisely what we mean by information and how we quantify</st> <st
    c="655">it mathematically</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="672">Entropy as expected information</st>*<st c="704">: Here, we’ll
    introduce the concept of the average information associated with a random variable
    and its</st> <st c="810">probability distribution</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="834">Mutual information</st>*<st c="853">: Here, we’ll extend our information
    theory concepts to multiple</st> <st c="919">random variables</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*<st c="935">The Kullback-Leibler divergence</st>*<st c="967">: Here, we’ll
    extend our information theory concepts to multiple distributions and show how
    we can use them to build optimal approximations</st> <st c="1108">of distributions</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1124">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1147">All the code examples provided in this chapter can be found in
    this book’s GitHub repository at</st> [<st c="1244">https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter13</st>](https://github.com/PacktPublishing/15-Math-Concepts-Every-Data-Scientist-Should-Know/tree/main/Chapter13)<st
    c="1348">. To run the Jupyter notebooks, you will need a full Python installation
    that includes the</st> <st c="1439">following packages:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="1458">pandas</st>` <st c="1465">(>=2.0.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1475">numpy</st>` <st c="1481">(>=1.24.3)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1492">scipy</st>` <st c="1498">(>=1.11.1)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1509">scikit-learn</st>` <st c="1522">(>=1.3.0)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="1532">matplotlib</st>` <st c="1543">(>=3.7.2)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1553">What is information and why is it useful?</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1595">As we said in the introduction, information theory and information-theoretic
    ideas are very useful.</st> <st c="1696">To understand those ideas, one of the
    first things we must address is what we mean by</st> *<st c="1782">information</st>*<st
    c="1793">. I am talking conceptually here.</st> <st c="1827">Once we have nailed
    down our conception of what information is about, writing down a mathematical
    definition will</st> <st c="1941">be easier.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1951">The concept of information</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="1978">Part of the</st> <st c="1990">difficulty in</st> <st c="2004">introducing
    information theory as a mathematical subject is that different people use the
    word “information” and apply it to different concepts.</st> <st c="2150">For example,
    the word “information” could apply to</st> <st c="2201">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2215">The semantic content or meaning of an action or event, such as
    a particular word being spoken or written.</st> <st c="2322">This is the most
    common conception that people have of what information should be about.</st> <st
    c="2411">It is the idea that the information associated with a thing should somehow
    be related to what that</st> <st c="2510">thing means.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2522">The intended effect or purpose of an action or event.</st> <st
    c="2577">When speaking a particular word, or triggering a specific event, we usually
    have an intended outcome that we would like to achieve or engineer.</st> <st c="2721">The
    association between the event and outcome may not be perfect, but we might think
    that the information associated with the event measures something about the</st>
    <st c="2882">desired outcome.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2898">How well an action or event encodes or identifies the thing it
    was intended to communicate.</st> <st c="2991">This is like the previous possible
    interpretation of “information,” but rather than concerning ourselves with how
    signal and outcome are related, in this interpretation, we focus solely on the
    efficiency and precision of the communication process.</st> <st c="3239">This
    is a narrower interpretation of what “information”</st> <st c="3295">is about.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3304">As you can</st> <st c="3315">imagine, the last of those interpretations
    of what “information” should be about is the narrowest of the three, and so also
    the easiest to define mathematically.</st> <st c="3478">Therefore, you will not
    be surprised that this is the conceptual definition of what “information” is about
    that mathematicians and scientists have settled on.</st> <st c="3637">Indeed,
    the founder of modern information theory, Claude Shannon, titled his 1948 landmark
    paper on information theory</st> *<st c="3756">A mathematical theory of communication</st>*<st
    c="3794">. For mathematicians, statisticians, scientists, engineers, and data
    scientists, information theory concerns itself with quantifying the efficiency
    of communication.</st> <st c="3960">For a longer discussion of the various possible
    interpretations of “information”, see [</st>*<st c="4047">1</st>*<st c="4049">]
    in the</st> *<st c="4058">Notes and urther reading</st>* <st c="4082">section
    at the end of</st> <st c="4105">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4118">So, we’ve nailed down conceptually what we mean when we use the
    word “information,” but how do we define it mathematically so that we can</st>
    <st c="4257">measure it?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4268">The mathematical definition of information</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="4311">In</st> <st c="4315">the UK, when I was growing up, there was a
    popular game called “Guess Who.” The game consisted of two players, who would
    each have an identical set of character playing cards, such as those shown in</st>
    *<st c="4514">Figure 13</st>**<st c="4523">.1</st>*<st c="4525">. Each player
    would choose a character in secret.</st> <st c="4575">Each player had to guess
    the character chosen by their opponent by asking questions about the attributes
    of their opponent’s chosen character – for example, “Are they wearing a hat?”
    or “Do they have black hair?” – with just “yes” or “no” answers to the questions.</st>
    <st c="4840">The winner of the game was the person who could identify their opponent’s
    chosen</st> <st c="4921">character first.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1: The game of identifying a person from their attributes](img/B19496_13_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="4939">Figure 13.1: The game of identifying a person from their attributes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5006">Now, imagine</st> <st c="5020">that you and I were playing the
    game and you asked me if my chosen character was working at a computer, and I
    said yes.</st> <st c="5140">With that answer, you could immediately identify who
    my chosen character was from the characters in</st> <st c="5240">Figure 13</st><st
    c="5249">.1\.</st> <st c="5253">The information content of the answer was very
    high because it communicated very efficiently (precisely in fact) which character
    we were talking about.</st> <st c="5405">The answer, “Yes, the person is working
    at a computer” encodes very precisely the signal about which character I selected</st>
    <st c="5527">in secret.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5537">Alternatively, if you asked the question, “Is your selected character
    a person,” I would also reply yes.</st> <st c="5643">It would have been a very
    silly question to ask, but for illustration purposes, we’ll stick with it.</st>
    <st c="5744">All the characters in</st> <st c="5766">Figure 13</st><st c="5775">.1
    are people, so my answer, “Yes, they are a person” does not help you narrow down
    the identity of my selected character.</st> <st c="5898">My answer does not efficiently
    encode the signal about who I</st> <st c="5959">have chosen.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5971">From this, we can see that the information content of my answers,
    how efficiently it communicated to you who I had chosen, was dependent on how
    rare the attribute you were asking about was – having a computer was rare (only
    one character did) while being a person was common (all the characters were people).</st>
    <st c="6281">Since the rarity of an attribute is defined by the probability distribution
    of the attribute, such as</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>Prob</mtext><mfenced
    open="(" close=")"><mrow><mtext>Has</mtext><mtext>Computer</mtext></mrow></mfenced></mrow></mrow></math>](img/3993.png)
    <st c="6383"><st c="6402">or</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>Prob</mtext><mfenced
    open="(" close=")"><mrow><mtext>Has</mtext><mtext>Black</mtext><mtext>Hair</mtext></mrow></mfenced></mrow></mrow></math>](img/3994.png)<st
    c="6405"><st c="6427">, this also tells us that the mathematical definition of
    information is related to probability and that information-theoretic concepts
    are probabilistic.</st> <st c="6581">This is another reason why we emphasized
    the importance of understanding probability in</st> [*<st c="6669">Chapter 2</st>*](B19496_02.xhtml#_idTextAnchor061)<st
    c="6678">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6679">We now know</st> <st c="6692">that the information associated with
    a thing or event is related to the probability of that thing or event occurring.</st>
    <st c="6810">We also know that the lower the probability of the thing or event
    occurring, the higher the amount of information associated</st> <st c="6935">with
    it.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6943">The formal mathematical definition of the amount of information
    associated with an event,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/3995.png)<st
    c="7034"><st c="7035">, occurring is</st> <st c="7050">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Information = </mml:mtext><mml:mo>−</mml:mo><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators=""><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:math>](img/3996.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7091">Eq.</st> <st c="7095">1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7096">We have been very formal in Eq.</st> <st c="7128">1 and used the</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/3997.png)
    <st c="7143"><st c="7152">notation to emphasize that we are dealing with a random
    variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="7218"><st c="7219">, and we are talking about the information associated with</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="7278"><st c="7279">taking a value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7298"><st c="7299">. Information itself is specifically concerned with the
    probability of a specific outcome,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7390"><st c="7391">, not the full distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4002.png)<st
    c="7420"><st c="7429">. We will encounter information-theoretic concepts such
    as entropy, which are associated with probability distributions, later.</st> <st
    c="7557">Because information increases as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="7590"><st c="7591">becomes rarer, and hence more surprising if we do see
    it, it is also said to</st> <st c="7668">represent the</st> **<st c="7683">degree
    of surprise</st>** <st c="7701">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="7705"><st c="7706">.</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7707">One of the first things you may notice about Eq.</st> <st c="7757">1
    is that we haven’t said which base we’re using when taking the logarithm.</st>
    <st c="7833">Most times, when we have a logarithm in a formula, the choice of
    base does not affect the</st> **<st c="7923">decision</st>** <st c="7931">we make
    based on the formula.</st> <st c="7962">Here, however, we are using the logarithm
    not to make a decision, but to directly quantify the amount of information associated
    with the outcome,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4005.png)<st
    c="8108"><st c="8109">. The choice of base will have an</st> <st c="8143">effect
    here.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8155">So, which base should we use?</st> <st c="8186">It is up to you,
    but there are some commonly used bases.</st> <st c="8243">The most common is base
    2, in which case we define the information associated with the outcome,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4005.png)<st
    c="8339"><st c="8340">, to be</st> <st c="8348">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Information = </mml:mtext><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators=""><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:math>](img/4007.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8390">Eq.</st> <st c="8394">2</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8395">When we use base 2 for measuring information, the amount of information
    is said to be measured in</st> **<st c="8493">bits</st>**<st c="8497">. This means
    that Eq.</st> <st c="8519">2 tells us the number of bits of information associated
    with the outcome,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4008.png)<st
    c="8593"><st c="8594">. Less frequently, you may see the term</st> **<st c="8634">Shannon</st>**
    <st c="8641">used instead of</st> **<st c="8658">bit</st>** <st c="8661">for the
    unit of information when using base 2\.</st> <st c="8709">Of course, we could
    choose a different</st> <st c="8748">base to use for our logarithm in Eq.</st>
    <st c="8785">1\.</st> <st c="8788">The other most common choices are</st> <st
    c="8822">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8833">Base</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>e</mml:mi></mml:math>](img/162.png)<st
    c="8839"><st c="8840">, so that we use the natural logarithm, and the information
    associated with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4010.png)
    <st c="8916"><st c="8917">is calculated as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/4011.png)<st
    c="8935"><st c="8949">. Unsurprisingly, when we use the natural logarithm, we
    say the information is measured</st> <st c="9037">in</st> *<st c="9040">nats</st>*<st
    c="9044">.</st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9045">Base 10, so that the information associated with</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4012.png)
    <st c="9095"><st c="9096">is calculated as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/4013.png)<st
    c="9114"><st c="9128">. In this case, we say the information is measured in</st>
    *<st c="9182">Hartleys</st>*<st c="9190">, or</st> *<st c="9195">Harts</st>*<st
    c="9200">, named after the electronics pioneer Ralph Hartley, who contributed
    to the foundations of information theory.</st> <st c="9311">You may also see the
    words</st> *<st c="9338">ban</st>* <st c="9341">and</st> *<st c="9346">dit</st>*
    <st c="9349">used when measuring information with base</st> <st c="9392">10 logarithms.</st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9406">The second thing you should notice about Eq.</st> <st c="9452">1
    is that it is monotonic in terms of probability.</st> <st c="9503">Figure 13</st><st
    c="9512">.2 shows a plot of the information (measured in bits) associated with
    an event and the probability of</st> <st c="9614">that eve</st><st c="9622">nt:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_13_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="9713">Figure 13.2: A plot of information content, measured in bits,
    of an event, and the probability of that event</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9821">The monotonic nature of the curve in</st> <st c="9859">Figure 13</st><st
    c="9868">.2 confirms what we outlined earlier, namely that the rarer an event
    is, the more insight or information it gives us, with very rare events having
    high information content.</st> <st c="10041">For example, an event that occurs
    only 1 in 100 times, and so has a probability of occurrence of 0.01, has an information
    content of</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mn>0.01</mn><mo>)</mo><mo>≈</mo><mn>6.64</mn></mrow></mrow></math>](img/4014.png)
    <st c="10174"><st c="10193">bits.</st> <st c="10199">At this point, it is also
    worth highlighting that an event that has a 50% probability of occurring has an
    information content in bits of</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>)</mo><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/4015.png)<st
    c="10336"><st c="10350">. This is a good reason for measuring information in bits
    because we can easily remember that an event that is equally likely to occur as
    not has 1 bit of information associated</st> <st c="10528">with it.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10536">Information theory applies to continuous distributions as well</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10599">We</st> <st c="10602">introduced the mathematical definition of
    information by talking about the probability of an event or outcome.</st> <st
    c="10714">Implicit in that is that we are talking about an outcome from a discrete
    random variable.</st> <st c="10804">This was also explicit in our “Guess Who”
    game example, where we spoke about the probability of a character having a particular
    attribute, such as black hair.</st> <st c="10963">Can we generalize the definition
    of information to continuous probability distributions?</st> <st c="11052">The
    answer is yes.</st> <st c="11071">For a continuous random variable,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="11105"><st c="11106">, with probability density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4017.png)<st
    c="11134"><st c="11139">, we can always define a particular event that</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="11186"><st c="11187">takes a value between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="11210"><st c="11211">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:math>](img/4020.png)<st
    c="11216"><st c="11223">. So, from the density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4021.png)<st
    c="11247"><st c="11252">, we can always create discrete events and an associated
    probability distribution.</st> <st c="11335">That means with a bit more mathematical
    work, we can extend information-theoretic concepts to continuous random variables
    as well.</st> <st c="11466">We will see this in action when we introduce the concept
    of entropy in the</st> <st c="11541">next section.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11554">It may be tempting to think that from what we just said we can
    assign an information value to the value,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="11660"><st c="11661">, of a continuous random variable.</st> <st c="11696">Surely
    we can just define information as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/4023.png)<st
    c="11737"><st c="11748">? Strictly speaking, no.</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4017.png)
    <st c="11773"><st c="11778">is a probability density, not a probability, and information
    can only be measured on probabilities.</st> <st c="11878">However, you may see
    expressions such as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/4025.png)
    <st c="11919"><st c="11928">loosely or sloppily used in informal proofs of information-theoretic
    formulae.</st> <st c="12007">In these circumstances, it is almost always the case
    that the proof can easily be made more rigorous and we arrive at the</st> <st
    c="12129">same formulae.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12143">Why we measure information on a logarithmic scale</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="12193">You</st> <st c="12197">may have looked at Eq.</st> <st c="12221">1
    and asked why we measure information on a log scale.</st> <st c="12276">Why did
    we use the logarithm function in Eq.</st> <st c="12321">1 when we could, in principle,
    have used any other monotonically increasing function</st> <st c="12406">of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4026.png)<st
    c="12409"><st c="12418">?</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12419">The simplest answer is the one Shannon gave in his original paper
    on</st> *<st c="12488">A mathematical theory of communciation</st>*<st c="12526">.
    Since we think of the information associated with an event with how much it helps
    us narrow down possibilities, a probability of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4027.png)
    <st c="12657"><st c="12658">helps us narrow down</st> <st c="12680">possibilities
    twice as much as a probability of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4028.png)<st
    c="12728"><st c="12729">. In our game of “Guess Who”, an attribute that only 25%
    of characters have, such as possessing a stethoscope, could narrow down the field
    of possible characters twice as much as an attribute that 50% of the characters
    have, such as possessing a hat.</st> <st c="12980">Likewise, a probability of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4029.png)
    <st c="13007"><st c="13008">helps us narrow down possibilities twice as much as
    a probability of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4030.png)<st
    c="13078"><st c="13079">. Therefore, it is natural for us to want to measure information
    in such a way that the difference in information between probabilities of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4027.png)
    <st c="13218"><st c="13219">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4028.png)
    <st c="13224"><st c="13225">is the same as the difference in information between
    probabilities of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4029.png)
    <st c="13296"><st c="13297">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4030.png)<st
    c="13302"><st c="13303">. This means using a monotonic function,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)<st
    c="13344"><st c="13398">, so that we have</st> <st c="13416">the following:</st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>f</mi><mfenced
    open="(" close=")"><mfrac><mn>1</mn><mn>4</mn></mfrac></mfenced><mo>−</mo><mi>f</mi><mfenced
    open="(" close=")"><mfrac><mn>1</mn><mn>2</mn></mfrac></mfenced><mo>=</mo><mi>f</mi><mfenced
    open="(" close=")"><mfrac><mn>1</mn><mn>20</mn></mfrac></mfenced><mo>−</mo><mi>f</mi><mfenced
    open="(" close=")"><mfrac><mn>1</mn><mn>10</mn></mfrac></mfenced></mrow></mrow></math>](img/4036.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13432">Eq.</st> <st c="13436">3</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13437">Or more generally, the function,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)<st
    c="13470"><st c="13524">, must satisfy the</st> <st c="13543">following equation:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>f</mml:mi><mml:mfenced separators=""><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators=""><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mfenced
    separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext> for any </mml:mtext><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext> and </mml:mtext><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:math>](img/4038.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="13630">Eq.</st> <st c="13634">4</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13635">Eq.</st> <st c="13639">4 is only satisfied by the logarithm function.</st>
    <st c="13686">So, if we want halving the probability of occurrence to always lead
    to a linear increase in information, we must measure information on a</st> <st
    c="13824">logarithmic scale.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13842">Why is quantifying information useful?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13881">In short, information theory</st> <st c="13910">is useful because
    it enables us to quantify how efficiently a signal is encoded when being transmitted.</st>
    <st c="14015">That means we can use information theory to work out better encodings
    and ultimately optimize the encoding, and in doing so minimize any signal loss
    at the receiving end.</st> <st c="14186">The practical applications of information
    theory to signal analysis and communications design</st> <st c="14280">are huge.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14289">In information theory, the person transmitting the signal is called
    the</st> **<st c="14362">transmitter</st>**<st c="14373">, and the person or</st>
    <st c="14393">process wanting to receive the signal is the</st> **<st c="14438">receiver</st>**<st
    c="14446">. The</st> <st c="14452">transmitter transmits the signal via some communication
    channel.</st> <st c="14517">When transmitting the signal via the channel, the
    signal is encoded.</st> <st c="14586">This scenario is represented schematically
    in</st> <st c="14632">Figu</st><st c="14636">re 13</st><st c="14642">.3:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_13_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="14896">Figure 13.3: Schematic of the signal transmission and receiving
    process</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14967">For a</st> <st c="14973">specific example, think about when you
    have a conversation with a friend using your cell phone.</st> <st c="15070">Your
    voice signal – changes in air pressure caused by you speaking – is encoded digitally
    and transmitted via radio waves, then decoded, and sent as electrical signals
    to the speaker in your friend’s</st> <st c="15270">cell phone.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15281">In our “Guess Who” game, I was (reluctantly) transmitting the
    signal of which character I had selected.</st> <st c="15386">I encoded that signal
    (the chosen character’s identity) by sending details of attributes the character
    had – for example, they had a stethoscope.</st> <st c="15532">The information
    associated with that single attribute, as measured by</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><mi>log</mi><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Character</mtext><mtext>has</mtext><mtext>stethoscope</mtext></mrow></mfenced></mrow></mrow></math>](img/4039.png)<st
    c="15602"><st c="15636">, quantitatively tells me how well the attribute has encoded
    the signal (which character I chose).</st> <st c="15735">In this case, the encoding
    is not perfect as it only narrows down the possibilities in</st> <st c="15822">Figure
    13</st><st c="15831">.1 to two people.</st> <st c="15849">It is lossy.</st> <st
    c="15862">I could make it better.</st> <st c="15886">For example, I could increase
    the message length and include more attributes, such as that the character has
    a stethoscope and black hair, which picks out just one person in</st> <st c="16060">Figure
    13</st><st c="16069">.1, or I could use an attribute that encodes the signal more
    efficiently, such as that they have a computer, which picks out</st> <st c="16194">one
    person.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16205">Signal loss in the communication process may also occur because
    the communication channel is itself noisy – that is, it corrupts the encoded signal
    as it is transmitted.</st> <st c="16376">Information theory also focuses on how
    to design robust encoding processes that can handle noisy imperfect communication
    channels, so information theory is a particular focus of companies building and
    running cell</st> <st c="16590">phone networks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16605">Having explained what we mean by information, and how and why
    we quantify it, this is a good place to wrap up this section with a recap of what
    we</st> <st c="16753">have learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16766">What we’ve learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16785">In this section, we’ve learned</st> <st c="16817">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16831">Information theory concerns itself with the communication of signals
    and the efficiency of encoding</st> <st c="16932">those signals.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="16946">Information-theoretic concepts are</st> <st c="16982">probabilistic
    concepts.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17005">The smaller the probability of an event or outcome occurring,
    the higher the information associated with that event</st> <st c="17122">or outcome.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17133">We measure information on a</st> <st c="17162">logarithmic scale.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17180">When we use logarithms to base 2, the resulting information is
    measured in bits.</st> <st c="17262">When we use logarithms to base</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>e</mml:mi></mml:math>](img/162.png)<st
    c="17293"><st c="17294">, the resulting information is measured in nats.</st>
    <st c="17343">When we use logarithms to base 10, the resulting information is
    measured in Hartleys, Harts, bans,</st> <st c="17442">or dits.</st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17450">When transmitting to another user (the receiver), the information
    associated with the encoding used tells us how efficient that encoding is in communicating
    the signal to</st> <st c="17622">the receiver.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="17635">Having learned that the information associated with a single outcome
    is defined from the probability of that single outcome occurring, in the next
    section, we will learn about information-theoretic concepts that relate to the
    whole probability distribution of</st> <st c="17896">possible outcomes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17914">Entropy as expected information</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="17946">For our “Guess Who” game, there</st> <st c="17978">are several
    attributes that a character can have.</st> <st c="18029">Here, I have listed the
    complete (for this purpose) set of</st> <st c="18088">possible attributes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18108">Has</st> <st c="18113">a hat</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="18118">Has</st> <st c="18123">a stethoscope</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="18136">Has</st> <st c="18141">black hair</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="18151">Has</st> <st c="18156">a computer</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="18166">Has a</st> <st c="18173">chemical flask</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="18187">From what</st> <st c="18198">we’ve learned about information theory
    so far, we’ve seen that if I tell you my chosen character has a computer, then
    you can narrow down the possibilities to a single character.</st> <st c="18377">This
    might suggest that asking if my chosen character has a computer is the most efficient
    question you can ask me.</st> <st c="18493">This isn’t quite true.</st> <st c="18516">There
    is no guarantee that my chosen character does have a computer.</st> <st c="18585">To
    identify the best question to ask, we should</st> <st c="18633">look at the</st>
    **<st c="18645">expected information</st>** <st c="18665">you’ll get by asking</st>
    <st c="18687">a question.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="18698">How do we calculate the expected amount of information you get
    from asking a single question?</st> <st c="18793">We’ll use a character having
    a stethoscope to illustrate this.</st> <st c="18856">Two out of the eight characters
    in</st> <st c="18891">Figure 13</st><st c="18900">.1 have a stethoscope, so</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Has</mtext><mtext>Stethoscope</mtext></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>4</mn></mfrac></mstyle></mrow></mrow></math>](img/4041.png)<st
    c="18926"><st c="18949">. So,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>''</mtext><mtext>t</mtext><mtext>have</mtext><mtext>stethoscope</mtext></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mn>3</mn><mn>4</mn></mfrac></mstyle></mrow></mrow></math>](img/4042.png)<st
    c="18955"><st c="18987">. If you ask me the question, “Does your chosen character
    have a stethoscope?”, there are two possible outcomes: “yes” or “no.” If the answer
    is “yes,” that narrows my chosen character down to two possibilities and gives
    you</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>4</mn></mfrac></mstyle></mrow></mrow></math>](img/4043.png)<st
    c="19212"><st c="19220">) bits of information.</st> <st c="19243">If the answer
    is “no,” that narrows my chosen character down to six possibilities and gives
    you</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>−</mo><msub><mi>log</mi><mn>2</mn></msub><mo>(</mo><mstyle
    scriptlevel="+1"><mfrac><mn>3</mn><mn>4</mn></mfrac></mstyle><mo>)</mo></mrow></mrow></math>](img/4044.png)
    <st c="19339"><st c="19349">bits</st> <st c="19354">of information.</st></st></st></st></st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19369">Now, how likely is my answer to be “yes,” and how likely is my
    answer to be “no?” We already know these probabilities.</st> <st c="19489">They
    are</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4027.png)
    <st c="19498"><st c="19499">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4046.png)<st
    c="19504"><st c="19505">, respectively.</st> <st c="19521">That means we can easily
    calculate the average or expected amount of information you are going to get from
    me in response to your question.</st> <st c="19661">It is</st> <st c="19667">as
    follows:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><msub><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mo>(</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mo>)</mo><mspace
    width="0.25em" /><mo>−</mo><mspace width="0.25em" /><mfrac><mn>3</mn><mn>4</mn></mfrac><msub><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mo>(</mo><mfrac><mn>3</mn><mn>4</mn></mfrac><mo>)</mo><mspace
    width="0.25em" /><mo>≈</mo><mn>0.811</mn><mspace width="0.25em" /><mtext>bits</mtext></mrow></mrow></math>](img/4047.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="19714">Eq.</st> <st c="19718">5</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19719">Let’s write Eq.</st> <st c="19735">5</st> <st c="19737">more symbolically:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mtext>Has stethoscope</mtext></mfenced><msub><mrow><mo>×</mo><mo>−</mo><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mi>P</mi><mfenced
    open="(" close=")"><mtext>Has stethoscope</mtext></mfenced><mo>+</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>’</mtext><mtext>t have stethoscope</mtext></mrow></mfenced><mo>×</mo><mo>−</mo><msub><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>’</mtext><mtext>t have stethoscope</mtext></mrow></mfenced></mrow></mrow></math>](img/4048.png)<st
    c="19755">![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mtext>Has stethoscope</mtext></mfenced><msub><mrow><mo>×</mo><mo>−</mo><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mi>P</mi><mfenced
    open="(" close=")"><mtext>Has stethoscope</mtext></mfenced><mo>+</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>’</mtext><mtext>t have stethoscope</mtext></mrow></mfenced><mo>×</mo><mo>−</mo><msub><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mn>2</mn></msub><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>’</mtext><mtext>t have stethoscope</mtext></mrow></mfenced></mrow></mrow></math>](img/4049.png)</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19868">Eq.</st> <st c="19872">6</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19873">Eq.</st> <st c="19877">6 makes the average information nature
    of the calculation more explicit.</st> <st c="19950">We can generalize Eq.</st>
    <st c="19972">6 to any attribute and since our answers have only two possibilities,
    “yes” or “no,” we can always write</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Doesn</mtext><mtext>'</mtext><mtext>t</mtext><mtext>have</mtext><mtext>attribute</mtext></mrow></mfenced><mo>=</mo><mn>1</mn><mo>−</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Has</mtext><mtext>attribute</mtext></mrow></mfenced></mrow></mrow></math>](img/4050.png)<st
    c="20077"><st c="20126">. For any attribute, we</st> <st c="20150">can calculate
    the expected information you’ll get back from me when you ask about that attribute.</st>
    <st c="20248">For convenience, we’ll shorten “Has attribute” to just “A.” This
    expected information is then given by the</st> <st c="20355">following equation:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Expected</mtext><mtext>information</mtext><mtext>=</mtext><mo>−</mo><mi>P</mi><mfenced
    open="(" close=")"><mtext>A</mtext></mfenced><msub><mi>log</mi><mn>2</mn></msub><mi>P</mi><mfenced
    open="(" close=")"><mtext>A</mtext></mfenced><mo>−</mo><mfenced open="(" close=")"><mrow><mn>1</mn><mo>−</mo><mi>P</mi><mfenced
    open="(" close=")"><mi>A</mi></mfenced></mrow></mfenced><msub><mi>log</mi><mn>2</mn></msub><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><mi>P</mi><mfenced open="(" close=")"><mi>A</mi></mfenced></mrow></mfenced></mrow></mrow></math>](img/4051.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="20440">Eq.</st> <st c="20444">7</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20445">Table 13.1 shows the probabilities and expected information for
    each of the five attributes we</st> <st c="20540">lis</st><st c="20543">ted earlier:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_13_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="20749">Table 13.1: Character attributes and their expected information</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20812">You’ll recall from the</st> *<st c="20836">What is information
    and why is it useful?</st>* <st c="20877">section that the higher the information
    associated with an answer, the more it allowed us to narrow down the possibilities
    of which character I had secretly selected.</st> <st c="21045">Table 13.1 shows
    us that the question that allows us to narrow down the characters most effectively
    is asking if they have a hat since it gives us the most information on average.</st>
    <st c="21225">Asking whether they have a computer is considerably less effective.</st>
    <st c="21293">This doesn’t match what we were initially thinking.</st> <st c="21345">How
    come?</st> <st c="21355">It should be clear from the formula in Eq.</st> <st c="21398">7
    what has happened.</st> <st c="21419">Although knowing a character has a computer
    narrows down the possibilities efficiently to just one person, it is unlikely
    that a person does have a computer, and this has reduced the information we get
    back on average when we ask if they have</st> <st c="21662">a computer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21673">We can re-write the equation in Eq.</st> <st c="21710">7</st>
    <st c="21712">like so:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>Expected information</mml:mtext><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mfenced
    separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced
    separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo></mml:math>](img/4052.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21774">Eq.</st> <st c="21778">8</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21779">Here, we just plug in the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>Attribute</mml:mtext></mml:mrow></mml:mfenced></mml:math>](img/4053.png)
    <st c="21814"><st c="21828">for the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/4054.png)<st
    c="21845"><st c="21846">. As a function of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/4054.png)<st
    c="21865"><st c="21866">, we can ask, “What is the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/1890.png)
    <st c="21902"><st c="21903">that gives the highest expected information?” We do
    this by finding the maximum value of the right-hand side of Eq.</st> <st c="22020">8
    with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="22038"><st c="22039">. We’ll use differential</st> <st c="22064">calculus to
    do this.</st> <st c="22085">We’ll differentiate the right-hand side of Eq.</st>
    <st c="22132">8 with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="22150"><st c="22151">, set the derivative to zero, and then solve for the resulting
    value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>p</mml:mi></mml:math>](img/2008.png)<st
    c="22223"><st c="22224">. Doing this give us</st> <st c="22245">a condition:</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><msub><mi>log</mi><mn>2</mn></msub><mi>p</mi><mo>+</mo><msub><mi>log</mi><mn>2</mn></msub><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfenced><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/4060.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="22282">Eq.</st> <st c="22286">9</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22287">Solving Eq.</st> <st c="22299">9 gives us</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>p</mi><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow></mrow></math>](img/4061.png)<st
    c="22310"><st c="22314">. This means asking about an attribute whose probability
    is</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4062.png)
    <st c="22374"><st c="22375">is the most efficient question we can ask.</st> <st
    c="22419">From Table 13.1, we can see that having a hat occurs with a probability
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math>](img/4063.png)<st
    c="22494"><st c="22497">, so asking whether a character has a hat is as efficient
    as it is possible to get in this game.</st> <st c="22594">Each time we ask a question
    where</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>Has</mtext><mtext>Attribute</mtext></mrow></mfenced><mo>=</mo><mstyle
    scriptlevel="+1"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow></mrow></math>](img/4064.png)
    <st c="22628"><st c="22649">, we remove half the remaining characters.</st> <st
    c="22692">Each time we divide the remaining characters into two groups and ask
    which group the chosen character is in, we are playing the game as efficiently</st>
    <st c="22840">as possible.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22852">Pro tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22860">Performing a task by dividing it into two parts, removing one
    part, and then repeating, is a very general technique for efficiently performing
    that task.</st> <st c="23015">For example, it can be used for efficiently sorting
    objects (merge sort) and finding the roots of an equation (the</st> <st c="23130">bisection
    method).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23148">Entropy</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23156">In Eq.</st> <st c="23164">7 and Eq.</st> <st c="23174">8, we</st>
    <st c="23179">calculated expected information.</st> <st c="23213">Expected information
    is the average information across the possible outcomes of a random variable.</st>
    <st c="23312">You may have seen a formula like that in Eq.</st> <st c="23357">7
    or Eq.</st> <st c="23366">8 before and seen it referred to as</st> **<st c="23402">entropy</st>**<st
    c="23409">. That is because entropy and expected information are two names for
    the same thing, although entropy is the more commonly used name.</st> <st c="23543">By
    now, you’ll also realize that entropy is something we calculate about</st> <st
    c="23616">random variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23633">For each attribute</st> <st c="23652">in our example, we had just
    two possible outcomes – either “yes, the character has the attribute,” or “no,
    the character doesn’t have the attribute.” The outcome is a random variable with
    two values, “yes” and “no,” which occur with probabilities</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4065.png)
    <st c="23900"><st c="23901">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4066.png)<st
    c="23906"><st c="23911">. To calculate the entropy in Eq.</st> <st c="23945">7,
    all we needed was those two probabilities.</st> <st c="23991">This is also true
    when we have a random variable with more than two outcomes.</st> <st c="24069">We
    only need its probability distribution to calculate its entropy.</st> <st c="24137">For
    a discrete random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="24169"><st c="24170">, with probability distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4068.png)<st
    c="24203"><st c="24204">, the entropy is calculated</st> <st c="24232">as follows</st><st
    c="24242">:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Entropy</mtext><mtext>=</mtext><mtext>Expected</mtext><mtext>information</mtext><mtext>=</mtext><mo>−</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow></mfenced><mi>log</mi><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow></mfenced></mrow></mrow></mrow></mrow></math>](img/4069.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="24299">Eq.</st> <st c="24303">10</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24305">Here,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="24312"><st c="24313">represents an outcome of the random variable,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="24360"><st c="24361">, so the summation in Eq.</st> <st c="24387">10 is over
    all the possible outcomes we can get for the</st> <st c="24443">random variable.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24459">Since entropy is just expected information, it is measured in
    the same units as we measure information.</st> <st c="24564">As before, those
    units are determined by the base we use for the logarithm in Eq.</st> <st c="24646">10\.</st>
    <st c="24650">The choice of base is up</st> <st c="24675">to you.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24682">We typically denote entropy by the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4072.png)<st
    c="24725"><st c="24726">. This looks like we are applying a function,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi></mml:math>](img/4073.png)<st
    c="24772"><st c="24773">, to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="24778"><st c="24779">. This is deliberate, to emphasize the fact that entropy
    is a quantity associated with random variables, so we can think of entropy as
    a function</st> <st c="24925">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="24928"><st c="24929">.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24930">The entropy of continuous random variables</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="24973">Entropy is one of</st> <st c="24992">those quantities we mentioned
    in the previous section that we can generalize from discrete random variables
    to continuous random variables.</st> <st c="25132">We can do so by considering
    discrete outcomes,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>x</mi><mo>≤</mo><mi>X</mi><mo>≤</mo><mi>x</mi><mo>+</mo><mi
    mathvariant="normal">Δ</mi><mi>x</mi></mrow></mrow></math>](img/4076.png)<st c="25179"><st
    c="25193">, using the formula for the entropy of a discrete random variable, and
    finally using the usual calculus trick of reducing the size of our intervals,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:math>](img/4077.png)<st
    c="25342"><st c="25345">, to zero.</st> <st c="25356">Doing so, for a continuous
    random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2026.png)<st
    c="25400"><st c="25401">, with probability density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4079.png)<st
    c="25429"><st c="25430">, we find the entropy is given by the</st> <st c="25468">following
    equatio</st><st c="25485">n:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mtext>Entropy</mtext><mtext>=</mtext><mi>H</mi><mo>(</mo><mi>X</mi><mo>)</mo><mtext>=</mtext><mo>−</mo><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi mathvariant="normal">x</mi></mfenced><mi>log</mi><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi><mo>+</mo><mtext>Constant</mtext></mrow></mrow></mrow></math>](img/4080.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25535">Eq.</st> <st c="25539">11</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25541">Unfortunately, the constant in Eq.</st> <st c="25577">11 becomes
    infinite when we take</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math>](img/4081.png)
    <st c="25610"><st c="25619">However, the constant is just that – a constant.</st>
    <st c="25668">It does not depend on</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4082.png)
    <st c="25690"><st c="25691">in any way, so it is the integral term in Eq.</st>
    <st c="25738">11 that contains the interesting and relevant behavior of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="25796"><st c="25797">. Because the integral term in Eq.</st> <st c="25832">11
    is the difference between the entropy and the constant, it is</st> <st c="25896">called
    the</st> **<st c="25908">differential entropy</st>**<st c="25928">. That is, for</st>
    <st c="25942">a continuous random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="25973"><st c="25974">, with density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)<st
    c="25990"><st c="25991">, we have</st> <st c="26001">the follow</st><st c="26011">ing:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Differential</mtext><mtext>Entropy</mtext><mtext>=</mtext><mo>−</mo><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>log</mi><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/4086.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26059">Eq.</st> <st c="26063">12</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26065">Because the</st> <st c="26078">expression in Eq.</st> <st c="26096">12
    is what we use when discussing the entropy of continuous random variables, you
    will frequently see the prefix</st> **<st c="26209">differential</st>** <st c="26221">dropped,
    and the integral on the right-hand side of Eq.</st> <st c="26278">12 referred
    to as the entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4087.png)<st
    c="26309"><st c="26310">, of the continuous random variable,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="26347"><st c="26348">. I will do so as well.</st> <st c="26372">When I refer
    to the entropy of the continuous random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="26435"><st c="26436">, and I use the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4087.png)<st
    c="26459"><st c="26460">, I mean its differential entropy, as defined in</st>
    <st c="26509">Eq.</st> <st c="26513">12.</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26516">The integration in Eq.</st> <st c="26540">12 is over the support
    of the random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="26587"><st c="26588">. If</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="26593"><st c="26594">is a two-dimensional real random variable, the integration
    will be over a two-dimensional real space, while if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2026.png)
    <st c="26706"><st c="26707">is one-dimensional and real, the integration will
    be over the</st> <st c="26770">real line.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26780">Calculating the entropy for some different continuous distributions
    can give us more insight into how entropy behaves and what it represents.</st>
    <st c="26923">We’ll look at two common continuous distributions – the uniform
    distribution and the Gaussian distribution.</st> <st c="27031">The formulae for
    the density functions</st> <st c="27070">and entropies of these two distributions
    are</st> <st c="27115">given here.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27126">Uniform distribution</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="normal">U</mi><mfenced open="(" close=")"><mrow><mi mathvariant="normal">a</mi><mo>,</mo><mi
    mathvariant="normal">b</mi></mrow></mfenced></mrow></mrow></math>](img/4094.png)
    <st c="27148"><st c="27156">:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Density</mtext><mo>=</mo><mfenced
    open="{" close=""><mtable columnwidth="auto" columnalign="center" rowspacing="1.0000ex"
    rowalign="baseline baseline"><mtr><mtd><mrow><mfrac><mn>1</mn><mrow><mi>b</mi><mo>−</mo><mi>a</mi></mrow></mfrac><mtext>for</mtext><mi>x</mi><mo>∈</mo><mfenced
    open="[" close="]"><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mtext>otherwise</mtext></mrow></mtd></mtr></mtable></mfenced><mo>,</mo><mtext>Entropy</mtext><mtext>=</mtext><mi>log</mi><mfenced
    open="(" close=")"><mrow><mi>b</mi><mo>−</mo><mi>a</mi></mrow></mfenced></mrow></mrow></math>](img/4095.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27222">Eq.</st> <st c="27226">13</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27228">Gaussian distribution</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi
    mathvariant="normal">N</mi><mfenced open="(" close=")"><mrow><mi mathvariant="normal">μ</mi><mo>,</mo><msup><mi
    mathvariant="normal">σ</mi><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/4096.png)
    <st c="27251"><st c="27261">:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Density</mtext><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><msup><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow></mfenced><mn>2</mn></msup></mrow></mfenced><mo>,</mo><mtext>Entropy</mtext><mtext>=</mtext><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><mi>e</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mrow></math>](img/4097.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="27339">Eq.</st> <st c="27343">14</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27345">Again, in Eq.</st> <st c="27360">13 and Eq.</st> <st c="27371">14,
    we haven’t specified which base we are taking logarithms to – it is your choice.</st>
    <st c="27456">The entropy formulae in Eq.</st> <st c="27484">13 and Eq.</st> <st
    c="27495">14 are correct for any choice</st> <st c="27525">of base.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27533">Because we</st> <st c="27544">have been able to calculate the
    entropy for these two distributions in terms of the parameters of their density
    functions, from Eq.</st> <st c="27677">13 and Eq.</st> <st c="27688">14, we can
    see how the entropy behaves as we change those parameters and hence change the
    shape of</st> <st c="27787">the distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27805">If we increase the width,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math>](img/4098.png)<st
    c="27832"><st c="27833">, of the uniform distribution, the entropy increases.</st>
    <st c="27887">Similarly, if we make our Gaussian distribution wider by increasing
    its standard deviation,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>σ</mml:mi></mml:math>](img/1545.png)<st
    c="27979"><st c="27980">, we increase its entropy.</st> <st c="28007">In both
    cases, the higher the variance of the distribution, the higher the entropy.</st>
    <st c="28091">Also, note that in both cases, the entropy is independent of the
    mean of the distribution.</st> <st c="28182">It doesn’t matter where we locate
    our distribution; the entropy is only dependent on how dispersed the</st> <st
    c="28285">distribution is.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28301">What does entropy tell us?</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="28328">Because</st> <st c="28337">entropy increases with the variance
    of a probability distribution, many people tend to think of entropy as a measure
    of</st> **<st c="28457">uncertainty</st>** <st c="28468">or</st> **<st c="28472">disorder</st>**<st
    c="28480">. This may be how you have encountered entropy before – in physics.</st>
    <st c="28548">In physics, entropy is a concept associated with thermodynamics.</st>
    <st c="28613">In thermodynamics, heating a system increases the disorder (think
    molecules moving about more rapidly with increasing temperature).</st> <st c="28745">The
    physics formulae for entropy are the same as those from information theory, namely
    the formulae in Eq.</st> <st c="28852">10 and</st> <st c="28859">Eq.</st> <st
    c="28863">12.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28866">Yet entropy is just average information, and we have tended to
    think of an increase in information as increasing the certainty with which we
    can identify the underlying cause of the information.</st> <st c="29062">How are
    these two viewpoints compatible with</st> <st c="29107">each other?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29118">A large entropy tells us that our random variable,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="29170"><st c="29171">, has a large variance and so tells us that the range
    of likely values we’d get from a single observation of that random variable is
    large.</st> <st c="29311">It tells us about the uncertainty in that single observation
    before we make it.</st> <st c="29391">It tells us about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4101.png)<st
    c="29409"><st c="29410">. In contrast, the information associated with an observation,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4102.png)<st
    c="29473"><st c="29474">, tells us how well that observation narrows down the
    underlying state of the system that gave rise to the value</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/169.png)<st
    c="29587"><st c="29588">. It tells us about</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mtext>state</mtext><mtext>|</mtext><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/4104.png)<st
    c="29608"><st c="29621">. Since entropy is just</st> <st c="29645">average information,
    it tells us how much we can narrow down possibilities on average.</st> <st c="29732">It
    tells us how much, on average, we can reduce uncertainty about our knowledge of
    a system from a single observation,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="29851"><st c="29852">. To reduce the uncertainty a lot with a single observation
    of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="29915"><st c="29916">, there must be a lot of uncertainty,</st> **<st c="29954">a
    priori</st>**<st c="29962">, in</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/4107.png)<st
    c="29967"><st c="29968">. So, entropy tells us about the</st> **<st c="30001">a
    priori</st>** <st c="30009">uncertainty in a random variable,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="30044"><st c="30045">, and the certainty with which we can identify the associated
    underlying state from observations of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="30145"><st c="30146">. The two viewpoints of entropy are compatible with each
    other.</st> <st c="30210">They are two sides of the</st> <st c="30236">same coin.</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30246">The Maximum Entropy technique</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="30276">The physics view of entropy leads to an interesting inference
    technique called</st> **<st c="30356">Maximum Entropy</st>** <st c="30371">or</st>
    **<st c="30375">MaxEnt</st>** <st c="30381">for short.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30392">The</st> <st c="30397">physics view says that entropy tells us
    about the number of underlying states compatible with a density function,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4082.png)<st
    c="30511"><st c="30512">. Let’s suppose we have</st> <st c="30536">some random
    variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="30558"><st c="30559">, but I don’t know what its distribution is.</st> <st
    c="30604">However, I do know its mean value,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/2170.png)<st
    c="30639"><st c="30640">, and its variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)
    <st c="30660"><st c="30665">(or I have good estimates of them, say from a sample
    of data).</st> <st c="30728">I want to model the distribution of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="30764"><st c="30765">and do some calculations with that distribution.</st>
    <st c="30815">What distribution should I use?</st> <st c="30847">One that is compatible
    with the values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/4115.png)
    <st c="30889"><st c="30890">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2146.png)<st
    c="30894"><st c="30899">.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30900">A reasonably logical choice is to use the most probable distribution,
    but what do we mean by that?</st> <st c="31000">We can say that the most probable
    distribution of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="31050"><st c="31051">is the one that has the highest number of possible
    underlying states compatible with its density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)<st
    c="31150"><st c="31151">. But that number is the entropy.</st> <st c="31185">So,
    it turns out that a reasonable choice for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4082.png)
    <st c="31231"><st c="31232">is one that maximizes the entropy subject to the constraints
    that the mean of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="31311"><st c="31312">is</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/2170.png)
    <st c="31316"><st c="31317">and the variance of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="31338"><st c="31339">is</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="31342"><st c="31347">.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31348">How do we do that maximization calculation?</st> <st c="31393">By
    using calculus – that is, using Lagrange multipliers as usual to impose the constraints.</st>
    <st c="31485">The objective function we must maximize is</st> <st c="31528">a</st><st
    c="31529">s follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi mathvariant="normal">x</mi></mfenced><mi>log</mi><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi><mo>+</mo><msub><mi>λ</mi><mn>1</mn></msub><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi><mo>+</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>x</mi><mi>d</mi><mi>x</mi><mo>+</mo><msub><mi>λ</mi><mn>3</mn></msub><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><msup><mi>x</mi><mn>2</mn></msup><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/4124.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="31597">Eq.</st> <st c="31601">15</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31603">The first term in our objective function in Eq.</st> <st c="31652">15
    is the entropy.</st> <st c="31671">The next three terms impose the constraints
    through the Lagrange multipliers</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/4125.png)<st
    c="31748"><st c="31761">. The constraints are that we</st> <st c="31791">must
    match the specified mean and variance, and we must have a properly normalized</st>
    <st c="31874">distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)<st
    c="31888"><st c="31889">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31890">Technically, to maximize the expression in Eq.</st> <st c="31938">15,
    we must use a technique called</st> **<st c="31973">calculus of variations</st>**<st
    c="31995">, albeit a</st> <st c="32006">relatively simple version of it in this
    instance.</st> <st c="32056">However, because of this extra complexity, we will
    just quote the answer.</st> <st c="32130">The density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4127.png)<st
    c="32143"><st c="32144">, that maximizes the entropy for a specified mean,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/4128.png)<st
    c="32195"><st c="32196">, and variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="32212"><st c="32217">, is</st> <st c="32221">as follows:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></mrow></mrow></math>](img/4130.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="32260">Eq.</st> <st c="32264">16</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32266">This is</st> <st c="32274">just the density function of the Gaussian
    distribution.</st> <st c="32331">What does this mean?</st> <st c="32352">It tells
    us that of all the distributions,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)<st
    c="32395"><st c="32396">, that have mean,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/2170.png)<st
    c="32414"><st c="32415">, and variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="32431"><st c="32436">, the Gaussian distribution is the one that has the highest
    entropy.</st> <st c="32505">It tells us that for mean,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/4134.png)<st
    c="32532"><st c="32533">, and variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2146.png)<st
    c="32549"><st c="32554">, the Gaussian distribution is the most probable, in the
    sense that it maximizes the entropy – it has the highest number of underlying
    states compatible with a mean,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>μ</mml:mi></mml:math>](img/2170.png)<st
    c="32720"><st c="32721">, and variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="32737"><st c="32742">. This is another reason for studying the Gaussian distribution.</st>
    <st c="32807">It is the most probable distribution if we only know the mean</st>
    <st c="32869">and variance.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32882">What happens if we don’t know the mean and variance?</st> <st
    c="32936">What happens if we only know the minimum and maximum values of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="32999"><st c="33000">? What is the most probable distribution then?</st> <st
    c="33047">Repeating the MaxEnt calculation, we find the properly normalized distribution
    that has the highest entropy is the uniform distribution.</st> <st c="33184">You
    can now see why we suggested looking at the entropy of both the uniform and Gaussian
    distributions – they are</st> <st c="33298">exceptional distributions.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33324">In this section, we learned about the different aspects of entropy.</st>
    <st c="33393">So,</st> <st c="33397">let’s recap.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33409">What we’ve learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33428">In this section, we learned</st> <st c="33457">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33471">The expected information tells us the average amount of information
    we get from an observation of a random variable, averaged across all the possible
    outcomes of the</st> <st c="33638">random variable</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33653">The expected information is more commonly known</st> <st c="33702">as
    entropy</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33712">Entropy is measured in the same units</st> <st c="33751">as information</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33765">Entropy can be defined and calculated for both discrete probability
    distributions and</st> <st c="33852">continuous distributions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33876">Entropy increases with an increase in the variance of</st> <st
    c="33931">a distribution</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="33945">The information theory definition of entropy is the same as the
    physics definition of entropy and they encapsulate the</st> <st c="34065">same
    concept</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34077">The MaxEnt technique can be used to determine the most probable
    distribution compatible with</st> <st c="34171">specified constraints</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34192">The Gaussian distribution has the highest entropy for a given
    specified mean</st> <st c="34270">and variance</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34282">The uniform distribution has the highest entropy for a given</st>
    <st c="34344">finite support</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="34358">Having learned about the expected information (entropy) of a single
    random variable, in the next section, we’ll learn about the information associated
    with multiple</st> <st c="34524">random variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34541">Mutual information</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="34560">In this</st> <st c="34569">section, we’re going to look at information-theoretic
    concepts relating to multiple random variables.</st> <st c="34671">We’ll focus
    on the case of just two random variables,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="34725"><st c="34726">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)<st
    c="34731"><st c="34732">, but you’ll soon realize that the new calculations and
    concepts we’ll introduce generalize easily to more than two random variables.</st>
    <st c="34866">As usual, we’ll start with the discrete case first before introducing
    the continuous</st> <st c="34951">case later.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34962">Because we’re looking at two discrete random variables,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)
    <st c="35019"><st c="35020">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)<st
    c="35025"><st c="35026">, we’ll need their joint probability distribution,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4143.png)<st
    c="35077"><st c="35085">, which we’ll use as shorthand for</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></mfenced></mrow></mrow></math>](img/4144.png)<st
    c="35120"><st c="35136">. The joint distribution is just a probability distribution
    so we can easily measure its entropy, which we’ll denote by</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4145.png)<st
    c="35256"><st c="35257">. Applying the usual rules that entropy is expected information,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4146.png)
    <st c="35322"><st c="35323">is given by the</st> <st c="35340">foll</st><st c="35344">owing
    formula:</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><munder><mo>∑</mo><mi>y</mi></munder><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>log</mi><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced></mrow></mrow></mrow></mrow></mrow></math>](img/4147.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="35388">Eq.</st> <st c="35392">17</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35394">To</st> <st c="35398">understand Eq.</st> <st c="35413">17, let’s
    look at what would happen if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="35452"><st c="35453">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="35458"><st c="35459">were independent of each other.</st> <st c="35492">The
    joint distribution would be given by</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4150.png)<st
    c="35533"><st c="35551">. Plugging this into Eq.</st> <st c="35576">17 and remembering
    the rules of logarithms of products from</st> [*<st c="35636">Chapter 1</st>*](B19496_01.xhtml#_idTextAnchor014)<st
    c="35645">, we’ll find that we have the</st> <st c="35675">following here:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>log</mi><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mrow><mo>+</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>y</mi></munder><mrow><msub><mi>P</mi><mi>Y</mi></msub><mfenced
    open="(" close=")"><mi>y</mi></mfenced></mrow></mrow><mi>log</mi><msub><mi>P</mi><mi>Y</mi></msub><mfenced
    open="(" close=")"><mi>y</mi></mfenced><mo>=</mo><mi>H</mi><mfenced open="(" close=")"><mi>X</mi></mfenced><mo>+</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>Y</mi></mfenced></mrow></mrow></math>](img/4151.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="35748">Eq.</st> <st c="35752">18</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35754">Eq.</st> <st c="35759">18 tells us that when</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="35781"><st c="35782">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="35787"><st c="35788">are independent, the entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4154.png)<st
    c="35819"><st c="35820">, is just the sum of the entropies of the separate variables,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="35882"><st c="35883">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="35888"><st c="35889">. We also know that if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="35912"><st c="35913">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="35918"><st c="35919">are independent, then knowing the value of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="35963"><st c="35964">tells us nothing about the value of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)<st
    c="36001"><st c="36002">. We gain no information about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="36033"><st c="36034">if we know</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="36046"><st c="36047">. There is no information in common between</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="36091"><st c="36092">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="36096"><st c="36097">.</st></st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36098">But what would happen if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="36124"><st c="36125">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="36130"><st c="36131">weren’t independent and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="36156"><st c="36157">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="36162"><st c="36163">did have some information in common?</st> <st c="36201">Knowing
    the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="36222"><st c="36223">would tell us information about</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="36256"><st c="36257">and vice versa.</st> <st c="36274">What would be the
    size of this common or</st> **<st c="36315">mutual information</st>**<st c="36333">?
    Well, we can take a simple approach and define this common or mutual information
    to be the difference between the average information we get from the random variables
    separately and the average information we get from the random variables together.</st>
    <st c="36584">This means that mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4171.png)<st
    c="36620"><st c="36628">, is defined as a difference between entropies and is
    given by the</st> <st c="36695">fol</st><st c="36698">lowing formula:</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced><mo>+</mo><mi>H</mi><mfenced open="(" close=")"><mi>Y</mi></mfenced><mo>−</mo><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced></mrow></mrow></math>](img/4172.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="36744">Eq.</st> <st c="36748">19</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36750">Although</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4173.png)
    <st c="36760"><st c="36768">is called mutual information, it is defined from entropies
    of the random variables,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="36852"><st c="36853">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)<st
    c="36858"><st c="36859">, not the specific outcomes,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="36888"><st c="36889">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="36894"><st c="36917">, of those random variables.</st> <st c="36946">This is
    why the symbol we use for mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4178.png)<st
    c="37000"><st c="37001">, looks like a function that’s applied to</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="37043"><st c="37044">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="37049"><st c="37050">. And being defined as a difference between entropies,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4181.png)
    <st c="37105"><st c="37106">is measured in whatever units you use for the base
    of your logarithms – bits if you’re taking logarithms to base 2, nats if you’re
    taking logarithms to base</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>e</mml:mi></mml:math>](img/162.png)<st
    c="37264"><st c="37265">, and</st> <st c="37271">so on.</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37277">Conditional entropy</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="37297">To get an</st> <st c="37307">intuition about what the mutual information,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4183.png)<st
    c="37353"><st c="37354">, tells us, we’ll use Bayes’ theorem from</st> [*<st c="37396">Chapter
    5</st>*](B19496_05.xhtml#_idTextAnchor261) <st c="37405">to re-wr</st><st c="37414">ite</st>
    <st c="37419">our formula:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></mfenced><mo>=</mo><msub><mi>P</mi><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>X</mi><mo>=</mo><mi>x</mi></mrow></mfenced><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/4184.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37472">Eq.</st> <st c="37476">20</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37478">The</st> <st c="37482">notation,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4185.png)<st
    c="37493"><st c="37497">, represents the conditional distribution of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="37542"><st c="37543">– that is, the distribution of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="37575"><st c="37576">once we know the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="37603"><st c="37604">. From Eq.</st> <st c="37615">20, we can calculate the
    entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4189.png)<st
    c="37649"><st c="37650">, and hence the mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4178.png)<st
    c="37686"><st c="37687">. By doing so, we g</st><st c="37706">et</st> <st c="37710">the
    following:</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>Y</mi></mfenced><mo>−</mo><mi>H</mi><mfenced open="(" close=")"><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></mfenced></mrow></mrow></math>](img/4191.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37749">Eq.</st> <st c="37753">21</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37755">Here,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4192.png)
    <st c="37762"><st c="37763">is the entropy defined from the conditional distribution,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4193.png)<st
    c="37822"><st c="37837">, averaged over all possible values of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="37876"><st c="37877">. So,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4195.png)
    <st c="37883"><st c="37884">is given by the</st> <st c="37901">following formula:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><munder><mo>∑</mo><mi>y</mi></munder><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mrow></mrow><mi>log</mi><msub><mi>P</mi><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mi>y</mi><mo>|</mo><mi>X</mi><mo>=</mo><mi>x</mi></mrow></mfenced></mrow></mrow></math>](img/4196.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37971">Eq.</st> <st c="37975">22</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37977">Since the entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4197.png)<st
    c="37997"><st c="38004">, is calculated from the conditional distribution,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4198.png)<st
    c="38055"><st c="38059">, it is known as the</st> **<st c="38080">conditional
    entropy</st>**<st c="38099">. You’ll recall from the previous section that entropy
    measures the uncertainty of a random variable.</st> <st c="38201">So,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4199.png)
    <st c="38205"><st c="38212">is the average uncertainty in</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)
    <st c="38242"><st c="38243">after we know</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="38258"><st c="38259">. And since</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4202.png)
    <st c="38271"><st c="38272">measures the uncertainty in</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="38301"><st c="38302">when we don’t know</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="38322"><st c="38323">, the expression in Eq.</st> <st c="38347">21 now gives
    us a way to understand the mutual information.</st> <st c="38407">Eq.</st> <st
    c="38411">21 tells us that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4173.png)
    <st c="38428"><st c="38436">is the reduction in uncertainty about</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="38474"><st c="38475">that we get on average from knowing the value</st>
    <st c="38522">of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="38525"><st c="38526">.</st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38527">We can use Bayes’ theorem to write</st> <st c="38563">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>P</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></mfenced><mo>=</mo><msub><mi>P</mi><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></mfenced><msub><mi>P</mi><mi>Y</mi></msub><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>=</mo><mi>y</mi></mrow></mfenced></mrow></mrow></math>](img/4208.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="38623">Eq.</st> <st c="38627">23</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38629">This means we can also write the mutual information,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators=""><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4209.png)<st
    c="38683"><st c="38691">,</st> <st c="38693">as follows:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced><mo>−</mo><mi>H</mi><mfenced open="(" close=")"><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></mfenced></mrow></mrow></math>](img/4210.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="38728">Eq.</st> <st c="38732">24</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38734">Here, the</st> <st c="38745">entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4211.png)<st
    c="38754"><st c="38761">, is the conditional entropy calculated from the conditional
    distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math>](img/4212.png)<st
    c="38836"><st c="38839">. Eq.</st> <st c="38845">24 means we can also</st> <st
    c="38865">interpret the mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4213.png)<st
    c="38900"><st c="38908">, as the reduction in uncertainty about</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="38948"><st c="38949">that we get on average from knowing the value of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="38999"><st c="39000">. Because we have two different ways of interpreting</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4178.png)<st
    c="39053"><st c="39054">, they must be equivalent.</st> <st c="39081">This means
    we can use the</st> <st c="39106">following formula:</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Average</mtext><mtext>reduction</mtext><mtext>in</mtext><mtext>uncertainty</mtext><mtext>about</mtext><mi>Y</mi><mtext>from</mtext><mtext>knowing</mtext><mi>X</mi><mo>=</mo><mtext>Average</mtext><mtext>reduction</mtext><mtext>in</mtext><mtext>uncertainty</mtext><mtext>about</mtext><mi>X</mi><mtext>from</mtext><mtext>knowing</mtext><mi>Y</mi></mrow></mrow></math>](img/4217.png)<st
    c="39125">![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Average</mtext><mtext>reduction</mtext><mtext>in</mtext><mtext>uncertainty</mtext><mtext>about</mtext><mi>Y</mi><mtext>from</mtext><mtext>knowing</mtext><mi>X</mi><mo>=</mo><mtext>Average</mtext><mtext>reduction</mtext><mtext>in</mtext><mtext>uncertainty</mtext><mtext>about</mtext><mi>X</mi><mtext>from</mtext><mtext>knowing</mtext><mi>Y</mi></mrow></mrow></math>](img/4218.png)</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39240">Eq.</st> <st c="39244">25</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39246">This reduction in uncertainty is always non-negative.</st> <st
    c="39301">If</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)
    <st c="39304"><st c="39305">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)
    <st c="39310"><st c="39311">are independent, then we gain no information about</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="39363"><st c="39364">from knowing the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="39391"><st c="39392">, while if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="39403"><st c="39404">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="39409"><st c="39410">are correlated, then knowing the value of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="39453"><st c="39454">does tell us something about the value</st> <st c="39494">of</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="39497"><st c="39498">:</st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>≥</mo><mn>0</mn></mrow></mrow></math>](img/4227.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="39511">Eq.</st> <st c="39515">26</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39517">By comparing Eq.</st> <st c="39535">21 and Eq.</st> <st c="39546">24,
    we can also see that we have</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4228.png)<st
    c="39579"><st c="39597">, as we would expect for a measure of the mutual information
    between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="39666"><st c="39667">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="39671"><st c="39672">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39673">Mutual information for continuous variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="39717">As we did with</st> <st c="39732">entropy, we can define mutual
    information for continuous random variables,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)
    <st c="39808"><st c="39809">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)<st
    c="39814"><st c="39815">. We can do this by constructing discrete random variables
    using small intervals,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∆</mml:mo><mml:mi>x</mml:mi></mml:math>](img/4233.png)
    <st c="39897"><st c="39901">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∆</mml:mo><mml:mi>y</mml:mi></mml:math>](img/4234.png)<st
    c="39905"><st c="39908">, of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="39913"><st c="39914">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)<st
    c="39919"><st c="39929">, and then taking the limit,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>∆</mo><mi>x</mi><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup><mo>,</mo><mo>∆</mo><mi>y</mi><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup></mrow></mrow></math>](img/4237.png)<st
    c="39958"><st c="39974">. As before, we must deal with constants that become infinite
    as</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>∆</mo><mi>x</mi><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup><mo>,</mo><mo>∆</mo><mi>y</mi><mo>→</mo><msup><mn>0</mn><mo>+</mo></msup></mrow></mrow></math>](img/4238.png)<st
    c="40039"><st c="40055">. However, once we do the formula for the (differential)
    entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math>](img/4239.png)
    <st c="40121"><st c="40129">is given in terms of the joint density,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4240.png)<st
    c="40169"><st c="40180">, and is</st> <st c="40189">as follows:</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mo>∫</mo><mo>∫</mo><msub><mi>p</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>log</mi><msub><mi>p</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>d</mi><mi>x</mi><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/4241.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="40237">Eq.</st> <st c="40241">27</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40243">Here, mutual information is still defined via the</st> <st c="40294">following
    formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced><mo>+</mo><mi>H</mi><mfenced open="(" close=")"><mi>Y</mi></mfenced><mo>−</mo><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>Y</mi></mfenced><mo>−</mo><mi>H</mi><mfenced open="(" close=")"><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></mfenced><mo>=</mo><mi>H</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced><mo>−</mo><mi>H</mi><mfenced open="(" close=")"><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></mfenced></mrow></mrow></math>](img/4242.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="40372">Eq.</st> <st c="40376">28</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40378">The</st> <st c="40382">conditional entropies,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4243.png)
    <st c="40406"><st c="40414">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4244.png)<st
    c="40418"><st c="40419">, are defined from the conditional densities,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4245.png)
    <st c="40465"><st c="40473">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4246.png)<st
    c="40477"><st c="40485">, in the way you</st> <st c="40502">would expect:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mo>∬</mo><msub><mi>p</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>log</mi><msub><mi>p</mi><mrow><mi>Y</mi><mo>|</mo><mi>X</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></mfenced><mi>d</mi><mi>x</mi><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/4247.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="40553">Eq.</st> <st c="40557">29</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>H</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></mfenced><mo>=</mo><mo>−</mo><mo>∬</mo><msub><mi>p</mi><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mi>log</mi><msub><mi>p</mi><mrow><mi>X</mi><mo>|</mo><mi>Y</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi>x</mi><mo>|</mo><mi>y</mi></mrow></mfenced><mi>d</mi><mi>x</mi><mi>d</mi><mi>y</mi></mrow></mrow></math>](img/4248.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="40596">Eq.</st> <st c="40600">30</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40602">Mutual information as a measure of correlation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="40649">Since</st> <st c="40656">the mutual information,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4178.png)<st
    c="40680"><st c="40681">, measures the information that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="40713"><st c="40714">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="40719"><st c="40720">have in common, it is a measure of the strength of
    the relationship between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)
    <st c="40797"><st c="40798">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4253.png)<st
    c="40803"><st c="40804">, just like the Pearson correlation coefficient.</st>
    <st c="40853">The advantage of the mutual information is that it is invariant
    to the monotonic transformation of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="40952"><st c="40953">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="40958"><st c="40959">. This means that we could create new random variables,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4256.png)
    <st c="41015"><st c="41026">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4257.png)<st
    c="41030"><st c="41031">, and so long as the functions,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>f</mml:mi></mml:math>](img/126.png)
    <st c="41063"><st c="41117">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>g</mml:mi></mml:math>](img/1680.png)<st
    c="41121"><st c="41122">, are monotonic,</st> <st c="41139">we have</st> <st c="41147">the
    following:</st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>I</mi><mfenced
    open="(" close=")"><mrow><mrow><mi>X</mi><mo>′</mo></mrow><mo>,</mo><mrow><mi>Y</mi><mo>′</mo></mrow></mrow></mfenced><mo>=</mo><mi>I</mi><mfenced
    open="(" close=")"><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mfenced></mrow></mrow></math>](img/4260.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="41181">Eq.</st> <st c="41185">31</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41187">This tells us that if we have a non-linear relationship between
    our random variables,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)
    <st c="41274"><st c="41275">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)<st
    c="41280"><st c="41281">, the mutual information will be just as confident at
    identifying and quantifying that relationship as when the relationship is linear.</st>
    <st c="41417">The mutual information is a sort of “non-linear correlation coefficient.”
    In contrast, the Pearson correlation coefficient value would be different for
    a non-linear relationship between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="41603"><st c="41604">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="41609"><st c="41610">compared to a linear relationship.</st> <st c="41646">The
    Pearson correlation coefficient may not spot that there is a strong relationship
    between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="41739"><st c="41740">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="41745"><st c="41746">, if that relationship</st> <st c="41769">is non-linear.</st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41783">This</st> <st c="41789">makes mutual information a popular tool
    for feature selection in machine learning, by filtering out features that have
    low mutual information scores with the target variable.</st> <st c="41964">Because
    many machine learning algorithms can build predictive models of non-linear relationships
    between features and targets, mutual information is excellent at identifying features
    a machine learning algorithm can make good</st> <st c="42190">use of.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42197">Let’s illustrate Eq.</st> <st c="42219">31 with a code example.</st>
    <st c="42243">First, we’ll introduce the data we’ll use in the code example.</st>
    <st c="42306">The plots in</st> <st c="42319">Figure 13</st><st c="42328">.4 show
    scatter plots of two variables,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="42368"><st c="42391">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)<st
    c="42395"><st c="42396">, again</st><st c="42403">st a third</st> <st c="42415">variable,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="42425"><st c="42426">:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19496_13_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="42568">Figure 13.4: Plots of y and z against x</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42607">There is a clear strong linear relationship between</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="42660"><st c="42683">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="42687"><st c="42688">, while the relationship between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)
    <st c="42721"><st c="42722">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="42727"><st c="42728">is non-linear.</st> <st c="42744">It won’t surprise
    you to learn that the</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/62.png)
    <st c="42784"><st c="42785">values have been constructed from the</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/769.png)
    <st c="42824"><st c="42834">values via</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>](img/4276.png)<st
    c="42845"><st c="42846">. If we wanted to build a model of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="42881"><st c="42904">or</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)<st
    c="42907"><st c="42908">, then</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="42915"><st c="42916">would be a good feature to use.</st> <st c="42949">Let’s
    look at a code example showing how the mutual information is just as good at identifying
    that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="43049"><st c="43050">is a good feature for predicting</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)
    <st c="43084"><st c="43085">as it is at identifying</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="43110"><st c="43111">is a good feature for</st> <st c="43134">predicting</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)<st
    c="43145"><st c="43168">.</st></st></st></st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43169">Mutual information code example</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43201">You’ll find this</st> <st c="43219">code example in the</st> `<st
    c="43239">Code_Examples_Chap13.ipynb</st>` <st c="43265">Jupyter notebook in this
    book’s</st> <st c="43298">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43316">The first problem we must address is that the definition of mutual
    information in Eq.</st> <st c="43403">19 is for random variables.</st> <st c="43431">In</st>
    <st c="43434">Figure 13</st><st c="43443">.4, we have data – that is, a sample
    from random variables.</st> <st c="43503">This means we can only estimate the
    mutual information and we must construct an estimation algorithm to do that.</st>
    <st c="43616">Fortunately, someone has already done that for us.</st> <st c="43667">We
    can use the</st> `<st c="43682">scikit-learn</st>` <st c="43694">library’s</st>
    `<st c="43705">mutual_info_regression</st>` <st c="43727">function from</st> `<st
    c="43742">sklearn.feature_selection</st>` <st c="43767">to estimate the mutual
    information from</st> <st c="43807">samples of two variables.</st> <st c="43834">We’ll
    use the data that is plotted in</st> <st c="43872">Figure 13</st><st c="43881">.4\.</st>
    <st c="43885">The data is stored in the</st> `<st c="43911">mutual_information_data.csv</st>`
    <st c="43938">file in the</st> `<st c="43951">Data</st>` <st c="43955">directory
    of this book’s GitHub repository.</st> <st c="44000">The data contains three columns
    called</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:math>](img/4284.png)
    <st c="44039"><st c="44040">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)<st
    c="44045"><st c="44046">. First, we must read in</st> <st c="44071">the data:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44327">We’ll calculate the Pearson correlation coefficient between</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="44388"><st c="44389">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>y</mml:mi></mml:math>](img/24.png)
    <st c="44394"><st c="44417">and between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)
    <st c="44429"><st c="44430">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>z</mml:mi></mml:math>](img/22.png)<st
    c="44435"><st c="44436">. We’ll use the</st> `<st c="44452">scipy.stats.pearsonr</st>`
    <st c="44472">function to calculate the Pearson correlations</st> <st c="44520">for
    us:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44713">This gives us the</st> <st c="44732">following output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="44863">There is a 26% difference between the two Pearson correlation
    coefficient values.</st> <st c="44946">Using the Pearson correlation coefficient
    to identify relationships between variables, from samples of those variables,
    works well when the relationship is linear.</st> <st c="45110">However, when the
    relationship</st> <st c="45140">between two variables is non-linear, the Pearson
    correlation coefficient is not so good at identifying that a relationship</st>
    <st c="45264">is present.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45275">Now, for comparison, we’ll estimate the mutual information between</st>
    <st c="45343">x</st> <st c="45344">and</st> <st c="45349">y</st> <st c="45350">and
    between</st> <st c="45363">x</st> <st c="45364">and</st> <st c="45368">z</st><st
    c="45369">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45625">This gives us the</st> <st c="45644">following output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45773">There is a 0.4% difference between those two estimates of mutual
    information.</st> <st c="45852">This says that mutual information is just as good
    at identifying a relationship between two variables when that relationship is
    non-linear as when the relationship</st> <st c="46016">is linear.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46026">This code example concludes this section on mutual information,
    so let’s recap what</st> <st c="46111">we’ve learned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46125">What we’ve learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="46144">In this section, we’ve learned</st> <st c="46176">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46190">The idea of entropy can be generalized to multiple</st> <st c="46242">random
    variables</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46258">The mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4290.png)<st
    c="46283"><st c="46284">, tells us how much reduction in uncertainty about</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)
    <st c="46335"><st c="46336">we get on average from knowing the value of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="46381"><st c="46382">, and</st> <st c="46388">vice versa</st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46398">Random variables that are independent of each other have zero</st>
    <st c="46461">mutual information</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46479">The conditional entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4195.png)<st
    c="46505"><st c="46506">, tells us the average entropy of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="46540"><st c="46541">when we know the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="46568"><st c="46569">, and so tells us about the average uncertainty in</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="46620"><st c="46621">after we</st> <st c="46631">know</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)</st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46637">Mutual information and conditional entropy can be calculated for
    continuous</st> <st c="46713">random variables</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46729">The mutual information between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="46761"><st c="46762">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="46767"><st c="46768">gives us a measure of the correlation between</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="46815"><st c="46816">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="46821"><st c="46822">, even when the relationship between</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="46859"><st c="46860">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="46865"><st c="46866">is non-linear</st></st></st></st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46879">Having learned how to quantify the information that is common
    between two different random variables, in the next section, we’ll learn how to
    quantify the information that is in common between two different distributions
    of the same single</st> <st c="47120">random variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47136">The Kullback-Leibler divergence</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="47168">In the</st> <st c="47176">previous section, we learned about the
    similarities between random variables from an information theory perspective.</st>
    <st c="47293">But let’s return to just a single random variable,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="47344"><st c="47345">, and ask what would happen if we had two different distributions
    for that variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="47430"><st c="47431">. As ever, we’ll start with the</st> <st c="47463">discrete
    case.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47477">Wait a moment!</st> <st c="47493">How can we have two different
    distributions for the same random variable?</st> <st c="47567">The variable is
    either random, and so has a given distribution, or it is not random.</st> <st
    c="47652">Yes, that’s correct, but imagine that we have its true distribution,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4306.png)<st
    c="47721"><st c="47722">, and an approximation to its true distribution.</st>
    <st c="47771">We’ll call the approximation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)<st
    c="47800"><st c="47801">. It may be that the true distribution is too complex
    to practically work with in a data science algorithm, so we want to replace it
    with a more tractable distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4308.png)<st
    c="47970"><st c="47971">. Ideally, we’d like some way of measuring the difference
    between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4309.png)
    <st c="48037"><st c="48038">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)
    <st c="48043"><st c="48044">so that we can tell how good an approximation</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)
    <st c="48091"><st c="48092">is.</st> <st c="48097">We need a measure of the “distance”
    between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4309.png)
    <st c="48141"><st c="48142">and</st> <st c="48147">Q</st><st c="48148">X</st><st
    c="48149">(</st><st c="48150">x</st><st c="48151">)</st><st c="48152">. Even better,
    if</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4313.png)
    <st c="48170"><st c="48171">had some parameters, such as hyperparameters, we could
    use them to minimize that distance and make</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/4314.png)
    <st c="48271"><st c="48275">look as much like</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4315.png)
    <st c="48293"><st c="48294">as we possibly can.</st> <st c="48315">That way, we’d
    get the benefit of a tractable and easy-to-use distribution,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)<st
    c="48391"><st c="48392">, that is as close as possible to the</st> <st c="48430">true
    distribution.</st></st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48448">So, how do we measure the difference between</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4309.png)
    <st c="48494"><st c="48495">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)<st
    c="48500"><st c="48501">? Using information,</st> <st c="48522">of course!</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48532">Relative entropy</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="48549">For</st> <st c="48553">any outcome value,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="48573"><st c="48574">, we can calculate the difference in information that
    we’d get according to</st> <st c="48650">P</st><st c="48651">X</st><st c="48652">(</st><st
    c="48653">x</st><st c="48654">)</st> <st c="48655">and according to</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4307.png)<st
    c="48673"><st c="48674">. This informatio</st><st c="48691">n difference is</st>
    <st c="48708">as follows:</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Information</mtext><mtext>difference</mtext><mtext>=</mtext><mo>−</mo><mi>log</mi><msub><mi>Q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>+</mo><mi>log</mi><msub><mi>P</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mi>log</mi><mfenced open="("
    close=")"><mfrac><mrow><msub><mi>P</mi><mi>X</mi></msub><mfenced open="(" close=")"><mi>x</mi></mfenced></mrow><mrow><msub><mi>Q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mfrac></mfenced></mrow></mrow></math>](img/4321.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="48780">Eq.</st> <st c="48784">32</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48786">But this is the information difference at a single value,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="48845"><st c="48846">, so to construct a difference between</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>P</mi><mi>X</mi></msub></mrow></math>](img/4323.png)<st
    c="48885"><st c="48886">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4324.png)<st
    c="48890"><st c="48891">, we must calculate the average of Eq.</st> <st c="48930">32
    over all possible values of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/169.png)<st
    c="48961"><st c="48962">, weighted with the true probabilities,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4309.png)<st
    c="49002"><st c="49003">. Doing so give</st><st c="49018">s us the</st> <st c="49028">following
    formula:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>Average</mtext><mtext>Information</mtext><mtext>Difference</mtext><mtext>=</mtext><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><msub><mi>Q</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mfenced></mrow></mrow></math>](img/4327.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49103">Eq.</st> <st c="49107">33</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49109">Because Eq.</st> <st c="49122">33 measures the difference in two
    average information values – that is, a difference in entropies – it is called</st>
    <st c="49234">the</st> **<st c="49239">relative entropy</st>**<st c="49255">.
    However, it has another more commonly used name, as we</st> <st c="49312">shall
    see.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49322">The expression on the right-hand side of Eq.</st> <st c="49368">33
    is not a measure of distance between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4328.png)
    <st c="49408"><st c="49409">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4329.png)
    <st c="49414"><st c="49415">because it is not symmetric – that is, if we swap</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4330.png)
    <st c="49466"><st c="49467">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4331.png)
    <st c="49472"><st c="49473">around in Eq.</st> <st c="49488">33, we get a slightly
    different mathematical expression.</st> <st c="49545">Instead, we refer to Eq.</st>
    <st c="49570">33 as a</st> **<st c="49578">divergence</st>**<st c="49588">. In
    this case, it is called the Kullback-Leibler divergence, or KL-divergence for
    short, and given the symbol</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4332.png)<st
    c="49699"><st c="49700">. So, for</st><st c="49709">mally we have</st> <st c="49724">the
    following:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>Q</mi><mi>X</mi></msub></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><msub><mi>Q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mfrac></mfenced></mrow></mrow></math>](img/4333.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49770">Eq.</st> <st c="49774">34</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49776">We also</st> <st c="49784">have the</st> <st c="49794">following
    formula:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>Q</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>P</mi><mi>X</mi></msub></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>x</mi></munder><mrow><msub><mi>Q</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><msub><mi>Q</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mfenced></mrow></mrow></math>](img/4334.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49814">Eq.</st> <st c="49818">35</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49820">Although</st> <st c="49829">we can see from Eq.</st> <st c="49850">34
    and Eq.</st> <st c="49861">35 that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4335.png)
    <st c="49869"><st c="49870">and so we cannot take</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4336.png)
    <st c="49893"><st c="49894">as a distance measure, we can still use it as a measure
    of the difference between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4337.png)
    <st c="49977"><st c="49978">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4338.png)<st
    c="49983"><st c="49984">. This means we can still use</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4339.png)
    <st c="50014"><st c="50015">for our original goal of finding an optimized approximation
    to the distribution,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4337.png)<st
    c="50097"><st c="50098">. In particular,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4341.png)
    <st c="50115"><st c="50116">has the following</st> <st c="50135">useful properties:</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>Q</mi><mi>X</mi></msub></mrow></mfenced><mo>=</mo><mn>0</mn><mtext>if</mtext><mtext>and</mtext><mtext>only</mtext><mtext>if</mtext><msub><mi>Q</mi><mi>X</mi></msub><mo>=</mo><msub><mi>P</mi><mi>X</mi></msub></mrow></mrow></math>](img/4342.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="50193">Eq.</st> <st c="50197">36</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>P</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>Q</mi><mi>X</mi></msub></mrow></mfenced><mo>></mo><mn>0</mn><mtext>if</mtext><msub><mi>Q</mi><mi>X</mi></msub><mo>≠</mo><msub><mi>P</mi><mi>X</mi></msub></mrow></mrow></math>](img/4343.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="50230">Eq.</st> <st c="50234">37</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50236">These properties tell us that the KL-divergence is only zero if
    our approximation is perfect.</st> <st c="50331">By adjusting any parameters in</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4344.png)
    <st c="50362"><st c="50363">to make the KL-divergence smaller, we will be making
    our approximation</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4344.png)
    <st c="50435"><st c="50436">closer</st> <st c="50444">to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4068.png)<st
    c="50447"><st c="50448">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50449">KL-divergence for continuous variables</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50488">As you</st> <st c="50495">have probably already guessed, all the
    results about KL-divergences for discrete random variables can be generalized
    to continuous random variables.</st> <st c="50645">The formulae are those you
    would guess, so we will just state them here.</st> <st c="50718">For a continuous
    random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2035.png)<st
    c="50752"><st c="50753">, the KL-divergence between probability densities</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)
    <st c="50803"><st c="50804">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4349.png)
    <st c="50809"><st c="50810">is g</st><st c="50815">iven by the</st> <st c="50828">following
    formula:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>q</mi><mi>X</mi></msub></mrow></mfenced><mo>=</mo><mo>∫</mo><mrow><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mi>log</mi><mfenced
    open="(" close=")"><mfrac><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><msub><mi>q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced></mrow></mfrac></mfenced></mrow></mrow><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/4350.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="50885">Eq.</st> <st c="50889">38</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50891">Using the KL-divergence for approximation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50933">To finish</st> <st c="50943">this chapter, we’ll give an example
    of how we can use the KL-divergence for approximating a distribution.</st> <st
    c="51050">This example is deliberately simple to help illustrate the ideas.</st>
    <st c="51116">Don’t worry – there is a more complex example that has been set
    as an exercise at the end of</st> <st c="51209">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51222">We will use a continuous random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="51265"><st c="51266">, whose true density is a Laplace distribution with density
    given by the</st> <st c="51339">following formula:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>λ</mi></mrow></mfrac><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mi>λ</mi></mfrac></mrow></mfenced></mrow></mrow></math>](img/4352.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51359">Eq.</st> <st c="51363">39</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51365">Here,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="51372"><st c="51373">has a mean of zero and a variance of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/4354.png)<st
    c="51411"><st c="51412">. We are going to approximate</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4085.png)
    <st c="51442"><st c="51443">by a Gaussian distribution with mean zero and variance,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="51500"><st c="51505">, so we’ll write</st> <st c="51522">the following:</st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mtext>exp</mtext><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><msup><mi>x</mi><mn>2</mn></msup><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow></mfenced></mrow></mrow></math>](img/4357.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51563">Eq.</st> <st c="51567">40</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51569">The variance,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="51584"><st c="51589">, is the parameter we will adjust to make</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4359.png)
    <st c="51631"><st c="51632">as close as possible to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4360.png)
    <st c="51657"><st c="51658">by minimizing the KL-divergence,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4361.png)<st
    c="51692"><st c="51697">, given in Eq.</st> <st c="51712">38\.</st> <st c="51716">We
    can simplify the calculation by first re-writi</st><st c="51765">ng</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4362.png)<st
    c="51769"><st c="51774">,</st> <st c="51776">as follows:</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>|</mo><mo>|</mo><msub><mi>q</mi><mi>X</mi></msub></mrow></mfenced><mo>=</mo><mo>−</mo><mo>∫</mo><msub><mi>p</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>log</mi><msub><mi>q</mi><mi>X</mi></msub><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mi>d</mi><mi>x</mi><mo>+</mo><mo>∫</mo><mrow><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mi>log</mi><mrow><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mi>d</mi><mi>x</mi></mrow></mrow></math>](img/4363.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="51842">Eq.</st> <st c="51846">41</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51848">The second integral on the right-hand side of Eq.</st> <st c="51899">41
    only depends on</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4364.png)<st
    c="51918"><st c="51919">, so it does not depend on our parameter,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="51961"><st c="51966">. To determine the optimal value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math>](img/4366.png)
    <st c="52002"><st c="52007">we only need minimize the first integral on the right-hand
    side of Eq.</st> <st c="52078">41\.</st> <st c="52082">Th</st><st c="52084">e
    first integral is</st> <st c="52105">as follows:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>λ</mi></mrow></mfrac><mrow><msubsup><mo>∫</mo><mrow><mo>−</mo><mi
    mathvariant="normal">∞</mi></mrow><mi mathvariant="normal">∞</mi></msubsup><msup><mi>e</mi><mrow><mo>−</mo><mstyle
    scriptlevel="+1"><mfrac><mfenced open="|" close="|"><mi>x</mi></mfenced><mi>λ</mi></mfrac></mstyle></mrow></msup></mrow><mfenced
    open="(" close=")"><mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>−</mo><mfrac><msup><mi>x</mi><mn>2</mn></msup><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow></mfenced><mi>d</mi><mi>x</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mfrac><msup><mi>λ</mi><mn>2</mn></msup><msup><mi>σ</mi><mn>2</mn></msup></mfrac></mrow></mrow></math>](img/4367.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52154">Eq.</st> <st c="52158">42</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52160">So, to determine the optimal value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/4368.png)<st
    c="52199"><st c="52202">, we must minimize the right-hand side of Eq.</st> <st
    c="52248">42 with respect to</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="52267"><st c="52272">. We can do this by differentiating with respect to</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)
    <st c="52324"><st c="52329">and setting the derivative equal to zero.</st> <st
    c="52371">By doing this</st><st c="52384">, we get the</st> <st c="52397">following
    formula:</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mfenced
    open="[" close="]"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>log</mi><mn>2</mn><mi>π</mi><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mfrac><msup><mi>λ</mi><mn>2</mn></msup><msup><mi>σ</mi><mn>2</mn></msup></mfrac></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>−</mo><mfrac><msup><mi>λ</mi><mn>2</mn></msup><msup><mi>σ</mi><mn>4</mn></msup></mfrac><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/4371.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52443">Eq.</st> <st c="52447">43</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52449">Solving Eq.</st> <st c="52462">43 for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/2090.png)<st
    c="52469"><st c="52474">, we find</st> <st c="52484">that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/4373.png)<st
    c="52489"><st c="52499">.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52500">This</st> <st c="52506">result is somewhat trivial.</st> <st c="52534">It
    says that the optimal value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/4374.png)<st
    c="52568"><st c="52571">, the variance of our Gaussian approximation, is equal
    to the variance of our true distribution.</st> <st c="52668">We could have guessed
    that.</st> <st c="52696">There are also other simpler methods, such as moment
    matching, for setting the variance parameter of an approximation.</st> <st c="52815">However,
    our main goal here was to illustrate, in as simple a way as possible, the main
    steps in using the KL-divergence to derive an optimal approximation to another
    distribution.</st> <st c="52996">There is another reason why we chose to use this
    very simple example, and why our exercises are still relatively simple at the
    end of this chapter.</st> <st c="53144">It is because, for these examples, we
    can calculate</st> <st c="53196">D</st><st c="53197">K</st><st c="53198">L</st><st
    c="53199">(</st><st c="53200">p</st><st c="53201">X</st><st c="53202">|</st><st
    c="53203">|</st> <st c="53204">q</st><st c="53205">X</st><st c="53206">)</st>
    <st c="53207">exactly and minimize it exactly.</st> <st c="53241">This is not
    always</st> <st c="53260">the case.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53269">Variational inference</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="53291">When we use</st> <st c="53303">KL-divergences in real data science
    algorithms, we are typically constructing some approximation,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:math>](img/4375.png)<st
    c="53402"><st c="53403">, to the Bayesian posterior probability,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>Prob</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:munder underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder><mml:mo>|</mml:mo><mml:mtext>Data</mml:mtext></mml:mrow></mml:mfenced></mml:math>](img/4376.png)<st
    c="53444"><st c="53462">, where</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:math>](img/4377.png)
    <st c="53470"><st c="53471">represents the parameters of a probabilistic model
    we have used to model our training data.</st> <st c="53564">The practice of constructing
    an approximation to a model posterior by varying the parameters of the approximation
    until a KL-divergence is optimal, is part of a wider field of machine learning</st>
    <st c="53757">methods known as</st> **<st c="53774">variational inference techniques</st>**<st
    c="53806">, or</st> **<st c="53811">variational inference</st>**<st c="53832">.</st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53833">In general, calculating and minimizing</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4378.png)
    <st c="53873"><st c="53883">can be hard and the resulting optimized approximation,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:math>](img/4379.png)<st
    c="53938"><st c="53939">, is not always an accurate approximation, or as accurate
    as we would like it to be.</st> <st c="54024">Instead, in variational inference,
    it is more common to work with</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4380.png)<st
    c="54090"><st c="54095">. Here, we minimize</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4380.png)
    <st c="54115"><st c="54120">with respect to the parameters of the approximate</st>
    <st c="54170">density,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:math>](img/4375.png)<st
    c="54179"><st c="54180">.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54181">Minimizing</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4383.png)
    <st c="54193"><st c="54198">has some nice properties.</st> <st c="54224">Deriving
    those properties is beyond the scope of this short section on the KL-divergence,
    so we will just state the main property at a high level.</st> <st c="54371">Minimizing
    the KL-divergence,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4384.png)<st
    c="54401"><st c="54406">, is equivalent to maximizing a lower</st> <st c="54444">bound
    on the Bayesian evidence of the model.</st> <st c="54489">You may recall from</st>
    [*<st c="54509">Chapter 8</st>*](B19496_08.xhtml#_idTextAnchor406) <st c="54518">that
    the Bayesian evidence is a quantity that measures how good our probabilistic model
    form is, given the data.</st> <st c="54632">From this, we can rigorously show
    that minimizing</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4385.png)
    <st c="54682"><st c="54687">does indeed obtain the best possible approximation,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:math>](img/4386.png)<st
    c="54739"><st c="54740">, and this motivates us to work</st> <st c="54772">with</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4387.png)<st
    c="54777"><st c="54782">.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54783">Minimizing</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4388.png)
    <st c="54795"><st c="54808">when working with real data science algorithms can
    also require using sophisticated optimization techniques, so we won’t go into
    the details here.</st> <st c="54955">However, it does explain why our introductory
    examples needed to be simple and why we focused on minimizing</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:munder
    underaccent="false"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo>_</mml:mo></mml:munder></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4389.png)
    <st c="55063"><st c="55077">instead.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55085">This section provided a concise introduction to the KL-divergence.</st>
    <st c="55153">So, let’s summarize what we’ve learned about it before wrapping
    up</st> <st c="55220">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55233">What we’ve learned</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="55252">In this section, we’ve learned</st> <st c="55284">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55298">Relative entropy measures the average difference in information
    between two different distributions for the same</st> <st c="55412">random variable</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="55427">Relative entropy is also known as the KL-divergence and can be
    used as an asymmetric measure of the difference between</st> <st c="55547">two
    distributions</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="55564">We can use the KL-divergence to optimize how well one distribution</st>
    <st c="55632">approximates another</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="55652">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="55660">This chapter was about information theory.</st> <st c="55704">Although
    you are less likely to directly use the calculations demonstrated in this chapter
    compared to the material from other chapters, the concepts and ideas behind information
    theory can be invaluable.</st> <st c="55909">Information-theoretic concepts give
    us a different way to think about probability, distributions, and what is conveyed
    when we observe a piece of data.</st> <st c="56061">Those concepts are</st> <st
    c="56080">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56091">Information theory concerns itself with the communication of signals
    and the efficiency of encoding</st> <st c="56192">those signals.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56206">The smaller the probability of an event or outcome occurring,
    the higher the information associated with that event</st> <st c="56323">or outcome.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56334">We measure information on a</st> <st c="56363">logarithmic scale.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56381">The expected information tells us the average amount of information
    we get from an observation of a random variable.</st> <st c="56499">The expected
    information is more commonly known</st> <st c="56547">as entropy.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56558">Entropy increases with the variance of a distribution, so it quantifies
    the uncertainty about an outcome before we have</st> <st c="56679">measured it.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56691">The MaxEnt technique can be used to determine the most probable
    distribution compatible with</st> <st c="56785">specified constraints.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56807">The idea of entropy can be generalized to multiple</st> <st c="56859">random
    variables.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56876">The mutual information,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4290.png)<st
    c="56901"><st c="56902">, tells us how much reduction in uncertainty about</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4142.png)
    <st c="56953"><st c="56954">we get on average from knowing the value of</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="56999"><st c="57000">, and</st> <st c="57006">vice versa.</st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57017">The conditional entropy,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math>](img/4195.png)<st
    c="57043"><st c="57044">, tells us the average entropy of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="57078"><st c="57079">when we know the value of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="57106"><st c="57107">, so it tells us about the average uncertainty in</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="57157"><st c="57158">after we</st> <st c="57168">know</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="57173"><st c="57174">.</st></st></st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57175">The mutual information between</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="57207"><st c="57208">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/4140.png)
    <st c="57213"><st c="57214">gives us a measure of the correlation between</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="57261"><st c="57262">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)<st
    c="57267"><st c="57268">, even when the relationship between</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)
    <st c="57305"><st c="57306">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>Y</mml:mi></mml:math>](img/2023.png)
    <st c="57311"><st c="57312">is non-linear.</st></st></st></st></st></st></st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57326">Relative entropy measures the average difference in information
    between two different distributions for the same</st> <st c="57440">random variable.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57456">Relative entropy is also known as the KL-divergence and can be
    used as an asymmetric measure of the difference between</st> <st c="57576">two
    distributions.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57594">We can use the KL-divergence to optimize how well one distribution</st>
    <st c="57662">approximates another.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57683">This was a very brief introduction to a very large topic.</st>
    <st c="57742">For more extensive introductions to information theory, see the
    books suggested in [</st>*<st c="57826">2</st>*<st c="57828">] of the</st> *<st
    c="57837">Notes and further</st>* *<st c="57855">reading</st>* <st c="57862">section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="57871">Like this chapter on information theory, our next chapter will
    also make use of probabilistic concepts.</st> <st c="57976">The next chapter will
    also cover an advanced topic: Bayesian</st> <st c="58037">non-parametric modeling.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58061">Exercises</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="58071">This section contains a series of exercises.</st> <st c="58117">The
    answers to all these can be found in the</st> `<st c="58162">Answers_to_Exercises_Chap13.ipynb</st>`
    <st c="58195">Jupyter notebook in this book’s</st> <st c="58228">GitHub repository.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58246">We have a composite random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="58284"><st c="58285">, that consists of three binary random variables,</st>
    ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>A</mi><mn>3</mn></msub></mrow></mrow></math>](img/4405.png)<st
    c="58335"><st c="58343">. We denote this as</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>X</mi><mo>=</mo><mfenced
    open="(" close=")"><mrow><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>A</mi><mn>3</mn></msub></mrow></mfenced></mrow></mrow></math>](img/4406.png)<st
    c="58363"><st c="58380">. We’ll use</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/4407.png)
    <st c="58392"><st c="58393">for the outcome for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/4408.png)<st
    c="58414"><st c="58415">,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/4409.png)
    <st c="58417"><st c="58418">for the outcome of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/4410.png)<st
    c="58438"><st c="58439">, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/4411.png)
    <st c="58445"><st c="58446">for the outcome of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/4412.png)<st
    c="58466"><st c="58467">. This</st> <st c="58474">means</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>a</mi><mn>3</mn></msub><mo>∈</mo><mfenced
    open="{" close="}"><mn>0,1</mn></mfenced></mrow></mrow></math>](img/4413.png)<st
    c="58480"><st c="58497">.</st></st></st></st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58498">We can write the outcome,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/311.png)<st
    c="58525"><st c="58526">, for the overall random variable,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>X</mml:mi></mml:math>](img/2022.png)<st
    c="58561"><st c="58562">, as a three-digit bit-string For example,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>010</mml:mn></mml:math>](img/4416.png)
    <st c="58605"><st c="58606">–to represent the outcome,</st> ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo>,</mo><msub><mi>a</mi><mn>3</mn></msub><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/4417.png)<st
    c="58634"><st c="58635">. There are</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:math>](img/4418.png)
    <st c="58647"><st c="58648">possible values for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>x</mml:mi></mml:math>](img/10.png)<st
    c="58669"><st c="58670">; these are 000,001,010,011,100,101,110,111\.</st> <st
    c="58715">We can also denote the true probability distribution,</st> <st c="58769">P</st><st
    c="58770">X</st><st c="58771">(</st><st c="58772">x</st><st c="58773">)</st><st
    c="58774">, as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4420.png)<st
    c="58779"><st c="58795">; it corresponds to eight numbers (between 0 and 1) that
    all add up</st> <st c="58863">to 1.</st></st></st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58868">Now, let’s introduce our approximation,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math>](img/4324.png)<st
    c="58909"><st c="58910">. We will use a product approxima</st><st c="58943">tion,
    so we’ll write</st> <st c="58965">the following:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>Q</mi><mrow><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>A</mi><mn>3</mn></msub></mrow></msub><mfenced
    open="(" close=")"><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>a</mi><mn>3</mn></msub></mrow></mfenced><mo>=</mo><msubsup><mi>P</mi><msub><mi>A</mi><mn>1</mn></msub><mrow><mo>(</mo><mtext>approx)</mtext></mrow></msubsup><mfenced
    open="(" close=")"><msub><mi>a</mi><mn>1</mn></msub></mfenced><msubsup><mi>P</mi><msub><mi>A</mi><mn>2</mn></msub><mrow><mo>(</mo><mtext>approx)</mtext></mrow></msubsup><mfenced
    open="(" close=")"><msub><mi>a</mi><mn>2</mn></msub></mfenced><msubsup><mi>P</mi><msub><mi>A</mi><mn>3</mn></msub><mrow><mo>(</mo><mtext>approx)</mtext></mrow></msubsup><mfenced
    open="(" close=")"><msub><mi>a</mi><mn>3</mn></msub></mfenced></mrow></mrow></math>](img/4422.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="59030">Eq.</st> <st c="59034">44</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59036">We’ve put the superscript “approx” on the distributions on the
    right-hand side of Eq.</st> <st c="59123">44 to emphasize that we’re constructing
    an approximation and that</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>approx)</mml:mtext></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4423.png)
    <st c="59189"><st c="59208">is not the true marginal distribution,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4424.png)<st
    c="59247"><st c="59248">, but an approximation</st> <st c="59271">to it.</st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59277">Here are the exercises concerning</st> <st c="59312">this question:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59326">What is the only form the approximation,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>approx)</mml:mtext></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4425.png)<st
    c="59368"><st c="59387">, can take?</st> <st c="59399">What are the parameters
    of</st> <st c="59426">this approximation?</st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="59445">Using this approximation for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>approx)</mml:mtext></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4426.png)<st
    c="59475"><st c="59495">, and similar approximations for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>approx)</mml:mtext></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4427.png)
    <st c="59528"><st c="59548">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>approx)</mml:mtext></mml:mrow></mml:msubsup><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4428.png)<st
    c="59552"><st c="59572">, substitute these into Eq.</st> <st c="59600">44 to write
    down the full mathematical form for the</st> <st c="59652">approximation,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4429.png)<st
    c="59667"><st c="59685">.</st></st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="59686">Using the mathematical expression for</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4430.png)
    <st c="59725"><st c="59740">that you wrote in question 2, derive an expression
    for the KL-divergence,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4431.png)<st
    c="59814"><st c="59833">, in terms of the parameters of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4432.png)
    <st c="59865"><st c="59880">and the true marginal distributions,</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4433.png)<st
    c="59917"><st c="59918">,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4434.png)<st
    c="59920"><st c="59921">,</st> <st c="59923">and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4435.png)<st
    c="59927"><st c="59928">.</st></st></st></st></st></st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="59929">Minimize the expression for the KL-divergence you derived in question
    3 concerning the parameters of</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/4430.png)<st
    c="60031"><st c="60046">. Comment on</st> <st c="60059">the solution.</st></st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="60072">Notes and further reading</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="60098">To learn more about the topics that were covered in this chapter,
    take a look at the</st> <st c="60184">following resources:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="60204">For a short but very readable discussion on the broader aspects
    of what information is, I like the book by L.</st> <st c="60315">Floridi,</st>
    *<st c="60324">Information: A Very Short Introduction</st>*<st c="60362">, 1</st><st
    c="60365">st</st> <st c="60368">Edition (2010), Oxford University Press, Oxford,
    UK.</st> <st c="60422">ISBN: 978-0199551378.</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="60443">For additional texts on the mathematical aspects of information
    theory, see</st> <st c="60520">the following:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="60534">For a modern readable introduction to the mathematical theory
    of information, I like the book by J.V.</st> <st c="60637">Stone,</st> *<st c="60644">Information
    Theory: A Tutorial Introduction</st>*<st c="60687">, 1</st><st c="60690">st</st>
    <st c="60693">Edition (2015), Sebtel Press.</st> <st c="60724">ISBN: 978-0956372857.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="60745">Another accessible and well-established account of mathematical
    information theory is the book by J.R.</st> <st c="60849">Pierce,</st> *<st c="60857">An
    Introduction to Information Theory: Symbols, Signals and Noise</st>*<st c="60922">,
    Revised 2</st><st c="60933">nd</st> <st c="60936">Edition (2003), Dover Publications,
    New York, USA.</st> <st c="60988">ISBN: 978-0486240619.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="61009">The most authoritative textbook on information theory is probably</st>
    *<st c="61076">Elements of Information Theory</st>*<st c="61106">, 2</st><st c="61109">nd</st>
    <st c="61112">Edition (2006), by T.M.</st> <st c="61137">Cover and J.A.</st> <st
    c="61152">Thomas.</st> <st c="61160">It is published by Wiley-Interscience, Hoboken,
    New Jersey, USA.</st> <st c="61225">It is a lengthy textbook (nearly</st> <st
    c="61258">800 pages).</st>
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="61269">A modern and well-known book linking aspects of information theory
    and machine learning is that by D.J.C.</st> <st c="61376">MacKay,</st> *<st c="61384">Information
    Theory, Inference and Learning Algorithms</st>*<st c="61437">, 1</st><st c="61440">st</st>
    <st c="61443">Edition (2003), Cambridge University Press, Cambridge, UK.</st>
    <st c="61503">ISBN: 978-0521642989\.</st> <st c="61525">A PDF copy of the book
    can be found online</st> <st c="61568">at</st> [<st c="61571">https://www.inference.org.uk/itila/book.html</st>](https://www.inference.org.uk/itila/book.html)<st
    c="61615">.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
