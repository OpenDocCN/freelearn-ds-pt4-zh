["```py\nimport pandas as pd\ndataset = pd.DataFrame({\n    'x': [11, 21, 28, 17, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],\n    'y': [39, 36, 30, 52, 53, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 18, 7, 24, 10]\n}) \n```", "```py\nfrom sklearn import cluster\nimport matplotlib.pyplot as plt \n```", "```py\nkmeans = cluster.KMeans(n_clusters=2) \n```", "```py\nkmeans.fit(dataset) \n```", "```py\nlabels = labels = kmeans.labels_\ncenters = kmeans.cluster_centers_\nprint(labels) \n```", "```py\n[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0] \n```", "```py\nprint(centers) \n```", "```py\n[[16.77777778 48.88888889]\n [57.09090909 15.09090909]] \n```", "```py\nplt.scatter(dataset['x'], dataset['y'], c=labels)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show() \n```", "```py\n    from sklearn.cluster import AgglomerativeClustering\n    import pandas as pd\n    import numpy as np \n    ```", "```py\n    dataset = pd.DataFrame({\n        'x': [11, 11, 20, 12, 16, 33, 24, 14, 45, 52, 51, 52, 55, 53, 55, 61, 62, 70, 72, 10],\n        'y': [39, 36, 30, 52, 53, 46, 55, 59, 12, 15, 16, 18, 11, 23, 14, 8, 18, 7, 24, 70]\n    }) \n    ```", "```py\n    cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n    cluster.fit_predict(dataset) \n    ```", "```py\n    print(cluster.labels_) \n    ```", "```py\n    [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0] \n    ```", "```py\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons \n```", "```py\nfrom matplotlib import pyplot\nfrom pandas import DataFrame\n# generate 2d classification dataset\nX, y = make_moons (n_samples=1000, noise=0.05)\n# scatter plot, dots colored by class value\ndf = DataFrame (dict (x=X[,0], y=X[,1], label=y))\ncolors = {0: 'red', 1:'blue'}\nfig, ax = pyplot.subplots()\ngrouped = df.groupby('label')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color-colors[key])\npyplot.show() \n```", "```py\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nurl = \"https://storage.googleapis.com/neurals/data/iris.csv\"\niris = pd.read_csv(url)\niris\nX = iris.drop('Species', axis=1)\npca = PCA(n_components=4)\npca.fit(X) \n```", "```py\n Sepal.Length   Sepal.Width    Petal.Length    Petal.Width    Species\n0    5.1    3.5    1.4    0.2    setosa\n1    4.9    3.0    1.4    0.2    setosa\n2    4.7    3.2    1.3    0.2    setosa\n3    4.6    3.1    1.5    0.2    setosa\n4    5.0    3.6    1.4    0.2    setosa\n...    ...    ...    ...    ...    ...\n145    6.7    3.0    5.2    2.3    virginica\n146    6.3    2.5    5.0    1.9    virginica\n147    6.5    3.0    5.2    2.0    virginica\n148    6.2    3.4    5.4    2.3    virginica\n149    5.9    3.0    5.1    1.8    virginica \n```", "```py\nX = iris.drop('Species', axis=1)\npca = PCA(n_components=4)\npca.fit(X) \n```", "```py\nPCA(n_components=4) \n```", "```py\npca_df=(pd.DataFrame(pca.components_,columns=X.columns))\npca_df \n```", "```py\nX['PC1'] = X['Sepal.Length']* pca_df['Sepal.Length'][0] + X['Sepal.Width']* pca_df['Sepal.Width'][0]+ X['Petal.Length']* pca_df['Petal.Length'][0]+X['Petal.Width']* pca_df['Petal.Width'][0]\nX['PC2'] = X['Sepal.Length']* pca_df['Sepal.Length'][1] + X['Sepal.Width']* pca_df['Sepal.Width'][1]+ X['Petal.Length']* pca_df['Petal.Length'][1]+X['Petal.Width']* pca_df['Petal.Width'][1]\nX['PC3'] = X['Sepal.Length']* pca_df['Sepal.Length'][2] + X['Sepal.Width']* pca_df['Sepal.Width'][2]+ X['Petal.Length']* pca_df['Petal.Length'][2]+X['Petal.Width']* pca_df['Petal.Width'][2]\nX['PC4'] = X['Sepal.Length']* pca_df['Sepal.Length'][3] + X['Sepal.Width']* pca_df['Sepal.Width'][3]+ X['Petal.Length']* pca_df['Petal.Length'][3]+X['Petal.Width']* pca_df['Petal.Width'][3]\nX \n```", "```py\nprint(pca.explained_variance_ratio_) \n```", "```py\n[0.92461872 0.05306648 0.01710261 0.00521218] \n```", "```py\n!pip install pyfpgrowth \n```", "```py\nimport pandas as pd\nimport numpy as np\nimport pyfpgrowth as fp \n```", "```py\ndict1 = {\n    'id':[0,1,2,3],\n    'items':[[\"wickets\",\"pads\"],\n    [\"bat\",\"wickets\",\"pads\",\"helmet\"],\n    [\"helmet\",\"pad\"],\n    [\"bat\",\"pads\",\"helmet\"]]\n }\ntransactionSet = pd.DataFrame(dict1) \n```", "```py\n id    items\n0    0    [wickets, pads]\n1    1    [bat, wickets, pads, helmet]\n2    2    [helmet, pad]\n3    3    [bat, pads, helmet] \n```", "```py\npatterns = fp.find_frequent_patterns(transactionSet['items'],1) \n```", "```py\npatterns \n```", "```py\n{('pad',): 1,\n ('helmet', 'pad'): 1,\n ('wickets',): 2,\n ('pads', 'wickets'): 2,\n ('bat', 'wickets'): 1,\n ('helmet', 'wickets'): 1,\n ('bat', 'pads', 'wickets'): 1,\n ('helmet', 'pads', 'wickets'): 1,\n ('bat', 'helmet', 'wickets'): 1,\n ('bat', 'helmet', 'pads', 'wickets'): 1,\n ('bat',): 2,\n ('bat', 'helmet'): 2,\n ('bat', 'pads'): 2,\n ('bat', 'helmet', 'pads'): 2,\n ('pads',): 3,\n ('helmet',): 3,\n ('helmet', 'pads'): 2} \n```", "```py\nrules = fp.generate_association_rules(patterns,0.3)\nrules \n```", "```py\n{('helmet',): (('pads',), 0.6666666666666666),\n ('pad',): (('helmet',), 1.0),\n ('pads',): (('helmet',), 0.6666666666666666),\n ('wickets',): (('bat', 'helmet', 'pads'), 0.5),\n ('bat',): (('helmet', 'pads'), 1.0),\n ('bat', 'pads'): (('helmet',), 1.0),\n ('bat', 'wickets'): (('helmet', 'pads'), 1.0),\n ('pads', 'wickets'): (('bat', 'helmet'), 0.5),\n ('helmet', 'pads'): (('bat',), 1.0),\n ('helmet', 'wickets'): (('bat', 'pads'), 1.0),\n ('bat', 'helmet'): (('pads',), 1.0),\n ('bat', 'helmet', 'pads'): (('wickets',), 0.5),\n ('bat', 'helmet', 'wickets'): (('pads',), 1.0),\n ('bat', 'pads', 'wickets'): (('helmet',), 1.0),\n ('helmet', 'pads', 'wickets'): (('bat',), 1.0)} \n```"]