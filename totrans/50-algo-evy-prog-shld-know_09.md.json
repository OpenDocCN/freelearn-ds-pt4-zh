["```py\nimport numpy as np\nimport sklearn,sklearn.tree\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler \n```", "```py\n# Importing the dataset\ndataset = pd.read_csv('https://storage.googleapis.com/neurals/data/Social_Network_Ads.csv') \n```", "```py\ndataset = dataset.drop(columns=['User ID']) \n```", "```py\ndataset.head(5) \n```", "```py\nenc = sklearn.preprocessing.OneHotEncoder() \n```", "```py\nenc.fit(dataset.iloc[:,[0]])\nonehotlabels = enc.transform(dataset.iloc[:,[0]]).toarray() \n```", "```py\ngenders = pd.DataFrame({'Female': onehotlabels[:, 0], 'Male': onehotlabels[:, 1]}) \n```", "```py\nresult = pd.concat([genders,dataset.iloc[:,1:]], axis=1, sort=False) \n```", "```py\nresult.head(5) \n```", "```py\ny=result['Purchased']\nX=result.drop(columns=['Purchased']) \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.25, random_state = 0) \n```", "```py\n# Feature Scaling\nsc = StandardScaler() \n```", "```py\nX_train = sc.fit_transform(X_train) \n```", "```py\nX_test = sc.transform(X_test) \n```", "```py\n    classifier = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy', random_state = 100, max_depth=2) \n    ```", "```py\n    DecisionTreeClassifier(criterion = 'entropy', random_state = 100, max_depth=2) \n    ```", "```py\n    y_pred = classifier.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred) \n    ```", "```py\n    cm \n    ```", "```py\n    array([[64, 4],\n           [2, 30]]) \n    ```", "```py\n    accuracy= metrics.accuracy_score(y_test,y_pred)\n    recall = metrics.recall_score(y_test,y_pred)\n    precision = metrics.precision_score(y_test,y_pred)\n    print(accuracy,recall,precision) \n    ```", "```py\n    0.94 0.9375 0.8823529411764706 \n    ```", "```py\n    from xgboost import XGBClassifier\n    classifier = XGBClassifier()\n    classifier.fit(X_train, y_train) \n    ```", "```py\n    XGBClassifier(base_score=None, booster=None, callbacks=None,\n                  colsample_bylevel=None, colsample_bynode=None,\n                  colsample_bytree=None, early_stopping_rounds=None,\n                  enable_categorical=False, eval_metric=None, feature_types=None,\n                  gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n                  interaction_constraints=None, learning_rate=None, max_bin=None,\n                  max_cat_threshold=None, max_cat_to_onehot=None,\n                  max_delta_step=None, max_depth=None, max_leaves=None,\n                  min_child_weight=None, missing=nan, monotone_constraints=None,\n                  n_estimators=100, n_jobs=None, num_parallel_tree=None,\n                  predictor=None, random_state=None, ...) \n    ```", "```py\n    y_pred = classifier.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred) \n    ```", "```py\n    cm \n    ```", "```py\n    array([[64, 4],\n           [4, 28]]) \n    ```", "```py\n    accuracy = metrics.accuracy_score(y_test,y_pred)\n    recall = metrics.recall_score(y_test,y_pred)\n    precision = metrics.precision_score(y_test,y_pred)\n    print(accuracy,recall,precision) \n    ```", "```py\n    0.92 0.875 0.875 \n    ```", "```py\nPf = mode (P) \n```", "```py\nclassifier = RandomForestClassifier(n_estimators = 10, max_depth = 4,\ncriterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train) \n```", "```py\nRandomForestClassifier(n_estimators = 10, max_depth = 4,criterion = 'entropy', random_state = 0) \n```", "```py\ny_pred = classifier.predict(X_test)\ncm = metrics.confusion_matrix(y_test, y_pred)\ncm \n```", "```py\narray ([[64, 4],\n        [3, 29]]) \n```", "```py\naccuracy= metrics.accuracy_score(y_test,y_pred)\nrecall = metrics.recall_score(y_test,y_pred)\nprecision = metrics.precision_score(y_test,y_pred)\nprint(accuracy,recall,precision) \n```", "```py\n0.93 0.90625 0.8787878787878788 \n```", "```py\n    from sklearn.linear_model import LogisticRegression\n    classifier = LogisticRegression(random_state = 0)\n    classifier.fit(X_train, y_train) \n    ```", "```py\n    y_pred = classifier.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    cm \n    ```", "```py\n    array ([[65, 3],\n            [6, 26]]) \n    ```", "```py\n    accuracy= metrics.accuracy_score(y_test,y_pred)\n    recall = metrics.recall_score(y_test,y_pred)\n    precision = metrics.precision_score(y_test,y_pred)\n    print(accuracy,recall,precision) \n    ```", "```py\n    0.91 0.8125 0.8996551724137931 \n    ```", "```py\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train) \n```", "```py\n    y_pred = classifier.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    cm \n    ```", "```py\n    array ([[66, 2],\n            [9, 23]]) \n    ```", "```py\n    accuracy= metrics.accuracy_score(y_test,y_pred)\n    recall = metrics.recall_score(y_test,y_pred)\n    precision = metrics.precision_score(y_test,y_pred)\n    print(accuracy,recall,precision) \n    ```", "```py\n0.89 0.71875 0.92 \n```", "```py\n    # Fitting Decision Tree Classification to the Training set\n    from sklearn.naive_bayes import GaussianNB\n    classifier = GaussianNB()\n    classifier.fit(X_train, y_train) \n    ```", "```py\n    GaussianNB() \n    ```", "```py\n    # Predicting the Test set results\n    y_pred = classifier.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred) \n    ```", "```py\n    cm \n    ```", "```py\n    array([[66, 2],\n    [6, 26]]) \n    ```", "```py\n    accuracy= metrics.accuracy_score(y_test,y_pred)\n    recall = metrics.recall_score(y_test,y_pred)\n    precision = metrics.precision_score(y_test,y_pred)\n    print(accuracy,recall,precision) \n    ```", "```py\n0.92 0.8125 0.9285714285714286 \n```", "```py\n    dataset = pd.read_csv('https://storage.googleapis.com/neurals/data/data/auto.csv') \n    ```", "```py\n    dataset.head(5) \n    ```", "```py\n    dataset=dataset.drop(columns=['NAME'])\n    dataset.head(5)\n    dataset= dataset.apply(pd.to_numeric, errors='coerce')\n    dataset.fillna(0, inplace=True) \n    ```", "```py\n    y=dataset['MPG']\n    X=dataset.drop(columns=['MPG'])\n    # Splitting the dataset into the Training set and Test set\n    from sklearn.model_selection import train_test_split\n    from sklearn.cross_validation import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) \n    ```", "```py\n    from sklearn.linear_model import LinearRegression \n    ```", "```py\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train) \n    ```", "```py\n    LinearRegression() \n    ```", "```py\n    y_pred = regressor.predict(X_test)\n    from sklearn.metrics import mean_squared_error\n    sqrt(mean_squared_error(y_test, y_pred)) \n    ```", "```py\n    19.02827669300187 \n    ```", "```py\n    from sklearn.tree import DecisionTreeRegressor\n    regressor = DecisionTreeRegressor(max_depth=3)\n    regressor.fit(X_train, y_train) \n    ```", "```py\n    DecisionTreeRegressor(max_depth=3) \n    ```", "```py\n    y_pred = regressor.predict(X_test) \n    ```", "```py\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    sqrt(mean_squared_error(y_test, y_pred)) \n    ```", "```py\n4.464255966462035 \n```", "```py\n    from sklearn import ensemble \n    ```", "```py\n    params = {'n_estimators': 500, 'max_depth': 4,          'min_samples_split': 2, 'learning_rate': 0.01,          'loss': 'squared_error'}\n    regressor = ensemble.GradientBoostingRegressor(**params)\n    regressor.fit(X_train, y_train) \n    ```", "```py\n    GradientBoostingRegressor(learning_rate=0.01, max_depth=4, n_estimators=500) \n    ```", "```py\n    y_pred = regressor.predict(X_test) \n    ```", "```py\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    sqrt(mean_squared_error(y_test, y_pred)) \n    ```", "```py\n    4.039759805419003 \n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    df = pd.read_csv(\"weather.csv\") \n    ```", "```py\n    df.columns \n    ```", "```py\n    Index(['Date', 'MinTemp', 'MaxTemp', 'Rainfall', \n           'Evaporation', 'Sunshine', 'WindGustDir', \n           'WindGustSpeed', 'WindDir9am', 'WindDir3pm', \n           'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n           'Humidity3pm', 'Pressure9am', 'Pressure3pm', \n           'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', \n           'RainToday', 'RISK_MM', 'RainTomorrow'],\n          dtype='object') \n    ```", "```py\n    df.iloc[:,0:12].head() \n    ```", "```py\n    df.iloc[:,12:25].head() \n    ```", "```py\n    x = df.drop(['Date','RainTomorrow'],axis=1) \n    ```", "```py\n    y = df['RainTomorrow'] \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train_x , train_y ,test_x , test_y = train_test_split(x,y,\n    test_size = 0.2,random_state = 2) \n    ```", "```py\n    model = LogisticRegression() \n    ```", "```py\n    model.fit(train_x , test_x) \n    ```", "```py\n    predict = model.predict(train_y) \n    ```", "```py\n    predict = model.predict(train_y)\n    from sklearn.metrics import accuracy_score\n    accuracy_score(predict , test_y) \n    ```", "```py\n    0.9696969696969697 \n    ```"]