["```py\ndef adjust_position(gradient):\n    while gradient != 0:\n        if gradient < 0:\n            print(\"Move right\")\n            # here would be your logic to move right\n        elif gradient > 0:\n            print(\"Move left\")\n            # here would be your logic to move left \n```", "```py\ndef sigmoidFunction(z):\n      return 1/ (1+np.exp(-z)) \n```", "```py\ndef relu(x):\n    if x < 0:\n        return 0\n    else:\n        return x \n```", "```py\ndef leaky_relu(x, beta=0.01):\n    if x < 0:\n        return beta * x    \n    else:        \n        return x \n```", "```py\nimport numpy as np\ndef tanh(x): \n    numerator = 1 - np.exp(-2 * x) \n    denominator = 1 + np.exp(-2 * x) \n    return numerator / denominator \n```", "```py\nimport numpy as np\ndef softmax(x): \n    return np.exp(x) / np.sum(np.exp(x), axis=0) \nnumpy library (np) to perform the mathematical operations. The softmax function takes an array of x as input, applies the exponential function to each element, and normalizes the results so that they sum up to 1, which is the total probability across all classes.\n```", "```py\n    import tensorflow as tf \n    ```", "```py\n    mnist = tf.keras.datasets.mnist \n    ```", "```py\n    (train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n    ```", "```py\n    train_images, test_images = train_images / 255.0,                             test_images / 255.0 \n    ```", "```py\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.15),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.15),\n        tf.keras.layers.Dense(10, activation='softmax'),\n    ]) \n    ```", "```py\n    # Ensure TensorFlow 2.x is being used\n    %tensorflow_version 2.x\n    import tensorflow as tf\n    from tensorflow.keras.datasets import mnist \n    ```", "```py\n    # Load MNIST dataset\n    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n    # Normalize the pixel values to be between 0 and 1\n    train_images, test_images = train_images / 255.0, test_images / 255.0 \n    ```", "```py\n    inputs = tf.keras.Input(shape=(28,28)) \n    ```", "```py\n    x = tf.keras.layers.Flatten()(inputs) \n    ```", "```py\n    x = tf.keras.layers.Dense(512, activation='relu', name='d1')(x) \n    ```", "```py\n    x = tf.keras.layers.Dropout(0.2)(x) \n    ```", "```py\n    predictions = tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='d2')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=predictions) \n    ```", "```py\n# One-hot encode the labels\ntrain_labels_one_hot = tf.keras.utils.to_categorical(train_labels, 10)\ntest_labels_one_hot = tf.keras.utils.to_categorical(test_labels, 10) \n```", "```py\noptimizer = tf.keras.optimizers.RMSprop()\nloss = 'categorical_crossentropy'\nmetrics = ['accuracy']\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics) \n```", "```py\nhistory = model.fit(train_images, train_labels_one_hot, epochs=10, validation_data=(test_images, test_labels_one_hot)) \n```", "```py\n    print(\"Define constant tensors\")\n    a = tf.constant(2)\n    print(\"a = %i\" % a)\n    b = tf.constant(3)\n    print(\"b = %i\" % b) \n    ```", "```py\n    Define constant tensors\n    a = 2\n    b = 3 \n    ```", "```py\n    print(\"Running operations, without tf.Session\")\n    c = a + b\n    print(\"a + b = %i\" % c)\n    d = a * b\n    print(\"a * b = %i\" % d) \n    ```", "```py\n    Running operations, without tf.Session\n    a + b = 5\n    a * b = 6 \n    ```", "```py\n    c = a + b\n    print(\"a + b = %s\" % c) \n    ```", "```py\n    a + b = tf.Tensor(5, shape=(), dtype=int32) \n    ```", "```py\n    d = a*b\n    print(\"a * b = %s\" % d) \n    ```", "```py\n    a * b = tf.Tensor(6, shape=(), dtype=int32) \n    ```", "```py\n    import random\n    import numpy as np\n    import tensorflow as tf \n    ```", "```py\n    def createTemplate():\n        return tf.keras.models.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(128, activation='relu'),\n            tf.keras.layers.Dropout(0.15),\n            tf.keras.layers.Dense(128, activation='relu'),\n            tf.keras.layers.Dropout(0.15),\n            tf.keras.layers.Dense(64, activation='relu'), \n        ]) \n    ```", "```py\n    def prepareData(inputs: np.ndarray, labels: np.ndarray):\n        classesNumbers = 10\n        digitalIdx = [np.where(labels == i)[0] for i in range(classesNumbers)] \n    ```", "```py\n     pairs = list()\n        labels = list()\n        n = min([len(digitalIdx[d]) for d in range(classesNumbers)]) - 1\n        for d in range(classesNumbers):\n            for i in range(n):\n                z1, z2 = digitalIdx[d][i], digitalIdx[d][i + 1]\n                pairs += [[inputs[z1], inputs[z2]]]\n                inc = random.randrange(1, classesNumbers)\n                dn = (d + inc) % classesNumbers\n                z1, z2 = digitalIdx[d][i], digitalIdx[dn][i]\n                pairs += [[inputs[z1], inputs[z2]]]\n                labels += [1, 0] \n        return np.array(pairs), np.array(labels, dtype=np.float32) \n    ```", "```py\n    input_a = tf.keras.layers.Input(shape=input_shape)\n    encoder1 = base_network(input_a)\n    input_b = tf.keras.layers.Input(shape=input_shape)\n    encoder2 = base_network(input_b) \n    ```", "```py\n    distance = tf.keras.layers.Lambda( \n        lambda embeddings: tf.keras.backend.abs(\n            embeddings[0] - embeddings[1]\n        )\n    ) ([encoder1, encoder2])\n    measureOfSimilarity = tf.keras.layers.Dense(1, activation='sigmoid') (distance) \n    ```", "```py\n# Build the model\nmodel = tf.keras.models.Model([input_a, input_b], measureOfSimilarity)\n# Train\nmodel.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\nmodel.fit([train_pairs[:, 0], train_pairs[:, 1]], tr_labels, \n          batch_size=128,epochs=10,validation_data=([test_pairs[:, 0], test_pairs[:, 1]], test_labels)) \n```", "```py\nEpoch 1/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.3459 - accuracy: 0.8500 - val_loss: 0.2652 - val_accuracy: 0.9105\nEpoch 2/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.1773 - accuracy: 0.9337 - val_loss: 0.1685 - val_accuracy: 0.9508\nEpoch 3/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.1215 - accuracy: 0.9563 - val_loss: 0.1301 - val_accuracy: 0.9610\nEpoch 4/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0956 - accuracy: 0.9665 - val_loss: 0.1087 - val_accuracy: 0.9685\nEpoch 5/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0790 - accuracy: 0.9724 - val_loss: 0.1104 - val_accuracy: 0.9669\nEpoch 6/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0949 - val_accuracy: 0.9715\nEpoch 7/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0568 - accuracy: 0.9803 - val_loss: 0.0895 - val_accuracy: 0.9722\nEpoch 8/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.0807 - val_accuracy: 0.9770\nEpoch 9/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0439 - accuracy: 0.9847 - val_loss: 0.0916 - val_accuracy: 0.9737\nEpoch 10/10\n847/847 [==============================] - 6s 7ms/step - loss: 0.0417 - accuracy: 0.9853 - val_loss: 0.0835 - val_accuracy: 0.9749\n<tensorflow.python.keras.callbacks.History at 0x7ff1218297b8> \n```"]