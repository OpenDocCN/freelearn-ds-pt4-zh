<html><head></head><body>
  <div id="_idContainer260" class="Basic-Text-Frame">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-334" class="chapterTitle">Algorithms for Natural Language Processing</h1>
    <blockquote class="packt_quote">
      <p class="quote">Language is the most important instrument of thought.</p>
      <p class="cite">—Marvin Minsky</p>
    </blockquote>
    <p class="normal">This chapter <a id="_idIndexMarker937"/>introduces algorithms for <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>). It first introduces the fundamentals of NLP. Then it presents preparing data for NLP tasks. After that, it explains the concepts of vectorizing textual data. Next, we discuss word embeddings. Finally, we present a detailed use case.</p>
    <p class="normal">This chapter is made up of the following sections:</p>
    <ul>
      <li class="bulletList">Introducing NLP</li>
      <li class="bulletList"><strong class="keyWord">Bag-of-words-based</strong> (<strong class="keyWord">BoW-based</strong>) NLP</li>
      <li class="bulletList">Introduction to word embedding</li>
      <li class="bulletList">Case study: Restaurant review sentiment analysis</li>
    </ul>
    <p class="normal">By the <a id="_idIndexMarker938"/>end of this chapter, you will understand the basic techniques that are used for NLP. You should also understand how NLP can be used to solve some interesting real-world problems.</p>
    <p class="normal">Let’s start with the basic concepts.</p>
    <h1 id="_idParaDest-335" class="heading-1">Introducing NLP</h1>
    <p class="normal">NLP is a branch of machine learning algorithms that deals with the interaction between computers and human language. It involves analyzing, processing, and understanding human language <a id="_idIndexMarker939"/>to enable machines to comprehend and respond to human communication. NLP is a comprehensive subject and involves using computer linguistic algorithms and human-computer interaction technologies and methodologies to process complex unstructured data.</p>
    <p class="normal">NLP works by processing human language and breaking it down into its constituent parts, such as words, phrases, and sentences. The goal is to enable the computer to understand the meaning of the text and respond appropriately. NLP algorithms utilize various techniques, such as statistical models, machine learning, and deep learning, to analyze and process large volumes of natural language data. For complex problems, we may need to use a combination of techniques to come up with an effective solution.</p>
    <p class="normal">One of the most significant challenges in NLP is dealing with the complexity and ambiguity of human language. Languages are quite diverse with complex grammatical structures and idiomatic expressions. Additionally, the meaning of words and phrases can vary depending on the context in which they are used. NLP algorithms must be able to handle these complexities to achieve effective language processing.</p>
    <p class="normal">Let’s start by looking at some of the terminology that is used when discussing NLP.</p>
    <h1 id="_idParaDest-336" class="heading-1">Understanding NLP terminology</h1>
    <p class="normal">NLP is a vast <a id="_idIndexMarker940"/>field of study. In this section, we will investigate some of the basic terminology related to NLP:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Corpus</strong>: A corpus is <a id="_idIndexMarker941"/>a large and structured collection of text or speech data that serves as a resource for NLP algorithms. It can consist of various types of textual data, such as written text, spoken language, transcribed conversations, and social media posts. A corpus is created by intentionally gathering and organizing data from various online and offline sources, including the internet. While the internet can be a rich source for acquiring data, deciding what data to include in a corpus requires a purposeful selection and alignment with the goals of the particular study or analysis being conducted.
    <p class="normal">Corpora, the plural of corpus, can be annotated, meaning they may contain extra details about the texts, such as part-of-speech tags and named entities. These annotated corpora offer specific information that enhances the training and evaluation of NLP algorithms, making them especially valuable resources in the field.</p></li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">Normalization</strong>: This process involves converting text into a standard form, such as <a id="_idIndexMarker942"/>converting all characters to lowercase or removing punctuation, making it more amenable to analysis.</li>
      <li class="bulletList"><strong class="keyWord">Tokenization</strong>: Tokenization <a id="_idIndexMarker943"/>breaks down text into smaller parts called tokens, usually words or subwords, enabling a more structured analysis.</li>
      <li class="bulletList"><strong class="keyWord">Named Entity Recognition</strong> (<strong class="keyWord">NER</strong>): NER identifies and classifies named entities <a id="_idIndexMarker944"/>within the text, such as people’s names, locations, organizations, etc.</li>
      <li class="bulletList"><strong class="keyWord">Stop words</strong>: These are commonly used words such as <em class="italic">and</em>, <em class="italic">the</em>, and <em class="italic">is</em>, which are often <a id="_idIndexMarker945"/>filtered out during text processing as they may not contribute significant meaning.</li>
      <li class="bulletList"><strong class="keyWord">Stemming and lemmatization</strong>: Stemming involves reducing words to their root <a id="_idIndexMarker946"/>form, while lemmatization involves converting words to their base or dictionary form. Both techniques help in analyzing the core meaning of words.</li>
    </ul>
    <p class="normal">Next, let us study different text preprocessing techniques used in NLP:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Word embeddings</strong>: This is a method used to translate words into numerical form, where <a id="_idIndexMarker947"/>each word is represented as a vector in a space that may have many dimensions. In this <a id="_idIndexMarker948"/>context, a “high-dimensional vector” refers to an array of numbers where the number of dimensions, or individual components, is quite large—often in the hundreds or even thousands. The idea behind using high-dimensional vectors is to capture the complex relationships between words, allowing words with similar meanings to be positioned closer together in this multi-dimensional space. The more dimensions the vector has, the more nuanced the relationships it can capture. Therefore, in word embeddings, semantically related words end up being closer to each other in this high-dimensional space, making it easier for algorithms to understand and process language in a way that reflects human understanding.</li>
      <li class="bulletList"><strong class="keyWord">Language modeling</strong>: Language modeling is the process of developing statistical <a id="_idIndexMarker949"/>models that can predict or generate sequences of words or characters based on the patterns and structures found in a given text corpus.</li>
      <li class="bulletList"><strong class="keyWord">Machine translation</strong>: The <a id="_idIndexMarker950"/>process of automatically translating text from one language to another using NLP techniques and models.</li>
      <li class="bulletList"><strong class="keyWord">Sentiment analysis</strong>: The process of determining the attitude or sentiment expressed <a id="_idIndexMarker951"/>in a piece of text, often by analyzing the words and phrases used and their context. </li>
    </ul>
    <h2 id="_idParaDest-337" class="heading-2">Text preprocessing in NLP</h2>
    <p class="normal">Text preprocessing is a vital stage in NLP, where raw text data undergoes a transformation <a id="_idIndexMarker952"/>to become suitable for machine learning algorithms. This transformation involves converting the unorganized and often messy text into what is known as a “structured format.” A structured format means that the data is organized into a more systematic and predictable pattern, often involving techniques like tokenization, stemming, and removing unwanted characters. These steps help in cleaning the text, reducing irrelevant information or “noise,” and arranging the data in a manner that makes it easier for the machine learning models to understand. By following this approach, the raw text, which may contain inconsistencies and irregularities, is molded into a form that enhances the accuracy, performance, and efficiency of subsequent NLP tasks. In this section, we will explore various techniques used in text preprocessing to achieve this structured format.</p>
    <h3 id="_idParaDest-338" class="heading-3">Tokenization</h3>
    <p class="normal">As a reminder, tokenization is the crucial process of dividing text into smaller units, known as tokens. These tokens <a id="_idIndexMarker953"/>can be as small as individual words or <a id="_idIndexMarker954"/>even subwords. In NLP, tokenization is often considered the first step in preparing text data for further analysis. The reason for this foundational role lies in the very nature of language, where understanding and processing text requires breaking it down into manageable parts. By transforming a continuous stream of text into individual tokens, we create a structured format that mirrors the way humans naturally read and understand language. This structuring provides the machine learning models with a clear and systematic way to analyze the text, allowing them to recognize patterns and relationships within the data. As we delve deeper into NLP techniques, this tokenized format becomes the basis upon which many other preprocessing and analysis steps are built.</p>
    <p class="normal">The following code snippet <a id="_idIndexMarker955"/>is tokenizing the given text using the <strong class="keyWord">Natural Language Toolkit</strong> (<code class="inlineCode">nltk</code>) library in Python. The <code class="inlineCode">nltk</code> is a widely used library in Python, specifically designed <a id="_idIndexMarker956"/>for working with human language data. It provides easy-to-use interfaces and tools for tasks such as classification, tokenization, stemming, tagging, parsing, and more, making it a valuable asset for NLP. For those who wish to leverage these capabilities in their Python projects, the <code class="inlineCode">nltk </code>library can be downloaded and installed directly from the <strong class="keyWord">Python Package Index</strong> (<code class="inlineCode">PyPI</code>) by using the command <code class="inlineCode">pip install</code> <code class="inlineCode">nltk</code>. By incorporating the nltk library into your code, you can access a rich set of functions and resources that streamline the development and execution of various NLP tasks, making it a popular choice among researchers, educators, and developers in the field of computational linguistics. Let us start by importing relevant functions and using them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize
corpus = <span class="hljs-string">'This is a book about algorithms.'</span>
tokens = word_tokenize(corpus)
<span class="hljs-built_in">print</span>(tokens)
</code></pre>
    <p class="normal">The output will be a list that looks like this:</p>
    <pre class="programlisting con"><code class="hljs-con">['This', 'is', 'a', 'book', 'about', 'algorithms', '.']
</code></pre>
    <p class="normal">In this example, each token is a word. The granularity of the resulting tokens will vary based on the objective—for example, each token can consist of a word, a sentence, or a paragraph.</p>
    <p class="normal">To tokenize text based on sentences, you can use the <code class="inlineCode">sent_tokenize</code> function from the <code class="inlineCode">nltk.tokenize</code> module:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> sent_tokenize
corpus = <span class="hljs-string">'This is a book about algorithms. It covers various topics in depth.'</span>
</code></pre>
    <p class="normal">In this example, the <code class="inlineCode">corpus</code> variable contains two sentences. The <code class="inlineCode">sent_tokenize</code> function takes the corpus as input and returns a list of sentences. When you run the modified code, you will get the following output:</p>
    <pre class="programlisting code"><code class="hljs-code">sentences = sent_tokenize(corpus)
<span class="hljs-built_in">print</span>(sentences)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">['This is a book about algorithms.', 'It covers various topics in depth.']
</code></pre>
    <p class="normal">Sometimes we <a id="_idIndexMarker957"/>may need to break down large texts into paragraph-level chunks. <code class="inlineCode">nltk</code> can help with that task. It’s a feature that could be particularly <a id="_idIndexMarker958"/>useful in applications such as document summarization, where understanding the structure at the paragraph level may be crucial. Tokenizing text into paragraphs might seem straightforward, but it can be complex depending on the structure and format of the text. A simple approach is to split the text into two newline characters, which often separate paragraphs in plain text documents.</p>
    <p class="normal">Here’s a basic example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_paragraphs</span>(<span class="hljs-params">text</span>):
    <span class="hljs-comment"># Split by two newline characters</span>
    paragraphs = text.split(<span class="hljs-string">'\n\n'</span>) 
    <span class="hljs-keyword">return</span> [p.strip() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> paragraphs <span class="hljs-keyword">if</span> p]
</code></pre>
    <p class="normal">Next, let us look into how we can clean the data.</p>
    <h3 id="_idParaDest-339" class="heading-3">Cleaning data</h3>
    <p class="normal">Cleaning data is an essential step in NLP, as raw text data often contains noise and irrelevant <a id="_idIndexMarker959"/>information that can hinder the performance of NLP models. The goal of <a id="_idIndexMarker960"/>cleaning data for NLP is to preprocess the text data to remove noise and irrelevant information, and to transform it into a format that is suitable for analysis using NLP techniques. Note that data cleaning is done after it is tokenized. The reason is that cleaning might involve operations that depend on the structure revealed by tokenization. For instance, removing specific words or altering word forms might be done more accurately after the text is tokenized into individual terms.</p>
    <p class="normal">Let us study some techniques used to clean data and prepare it for machine learning tasks:</p>
    <h4 class="heading-4">Case conversion</h4>
    <p class="normal">Case conversion <a id="_idIndexMarker961"/>is a technique in NLP where text is transformed from one case format to another, such as from uppercase to lowercase, or from title case to uppercase.</p>
    <p class="normal">For example, the text “Natural Language Processing” in title case could be converted to lowercase <a id="_idIndexMarker962"/>to be “natural language processing.”</p>
    <p class="normal">This simple yet effective step helps in standardizing the text, which in turn simplifies its processing for various NLP algorithms. By ensuring that the text is in a uniform case, it aids in eliminating inconsistencies that might otherwise arise from variations in capitalization.</p>
    <h4 class="heading-4">Punctuation removal</h4>
    <p class="normal">Punctuation removal in NLP refers to the process of removing punctuation marks from raw text <a id="_idIndexMarker963"/>data before analysis. Punctuation marks are symbols such as periods (<code class="inlineCode">.</code>), commas (<code class="inlineCode">,</code>), question marks (<code class="inlineCode">?</code>), and exclamation marks (<code class="inlineCode">!</code>) that are used <a id="_idIndexMarker964"/>in written language to indicate pauses, emphasis, or intonation. While they are essential in written language, they can add noise and complexity to raw text data, which can hinder the performance of NLP models.</p>
    <p class="normal">It’s a reasonable concern to wonder how the removal of punctuation might affect the meaning of sentences. Consider the following examples:</p>
    <p class="normal"><code class="inlineCode">"She's a cat."</code></p>
    <p class="normal"><code class="inlineCode">"She's a cat??"</code></p>
    <p class="normal">Without punctuation, both lines become “She’s a cat,” potentially losing the distinct emphasis conveyed by the question marks.</p>
    <p class="normal">However, it’s worth noting that in many NLP tasks, such as topic classification or sentiment analysis, punctuation might not significantly impact the overall understanding. Additionally, models can rely on other cues from the text’s structure, content, or context to derive meaning. In cases where the nuances of punctuation are critical, specialized models and preprocessing techniques may be employed to retain the required information.</p>
    <h4 class="heading-4">Handling numbers in NLP</h4>
    <p class="normal">Numbers within text data can pose challenges in NLP. Here’s a look at two main strategies <a id="_idIndexMarker965"/>for handling numbers in text analysis, considering both the traditional approach of removal and an alternative option of standardization.</p>
    <p class="normal">In some NLP tasks, numbers may be considered noise, particularly when the focus is on aspects likeyWord frequency or sentiment analysis. Here’s why some analysts might choose to remove numbers:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Lack of relevance</strong>: Numeric characters may not carry significant meaning in specific text analysis scenarios.</li>
      <li class="bulletList"><strong class="keyWord">Skewing frequency counts</strong>: Numbers can distort word frequency counts, especially in models like topic modeling.</li>
      <li class="bulletList"><strong class="keyWord">Reducing complexity</strong>: Removing numbers may simplify the text data, potentially enhancing the performance of NLP models.</li>
    </ul>
    <p class="normal">However, an alternative approach is to convert all numbers to a standard representation rather than discarding them. This method acknowledges that numbers can carry essential information and ensures that their value is retained in a consistent format. It can be particularly useful in contexts where numerical data plays a vital role in the meaning of the text.</p>
    <p class="normal">Deciding whether to remove or retain numbers requires an understanding of the problem being solved. An algorithm may need customization to distinguish whether a number is significant based on the context of the text and the specific NLP task. Analyzing the role of numbers within the domain of the text and the goals of the analysis can guide this decision-making process.</p>
    <p class="normal">Handling numbers in NLP is not a one-size-fits-all approach. Whether to remove, standardize, or carefully analyze numbers depends on the unique requirements of the task at hand. Understanding these options and their implications helps in making informed decisions that align with the goals of the text analysis.</p>
    <h4 class="heading-4">White space removal</h4>
    <p class="normal">White space <a id="_idIndexMarker966"/>removal in NLP refers to <a id="_idIndexMarker967"/>the process of removing unnecessary white spaces, such as multiple spaces and tab characters. White space in the context of text data is not merely the space between words but includes other “invisible” characters that create spacing within text. In NLP, white space removal refers to the process of eliminating these unnecessary white space characters. Removing unnecessary white spaces can reduce the size of the text data and make it easier to process and analyze. </p>
    <p class="normal">Here’s a <a id="_idIndexMarker968"/>simple example to illustrate <a id="_idIndexMarker969"/>white space removal:</p>
    <ul>
      <li class="bulletList">Input text: <code class="inlineCode">"The quick brown fox \tjumps over the lazy dog."</code></li>
      <li class="bulletList">Processed text: <code class="inlineCode">"The quick brown fox jumps over the lazy dog."</code></li>
    </ul>
    <p class="normal">In the above example, extra spaces and a tab character (denoted by <code class="inlineCode">\t</code>) are removed to create a cleaner and more standardized text string.</p>
    <h4 class="heading-4">Stop word removal</h4>
    <p class="normal">Stop word removal is the process of eliminating common words, known as stop words, from a text <a id="_idIndexMarker970"/>corpus. stop words are words that occur frequently in a language but do not carry significant meaning or contribute to the overall understanding of the text. Examples of stop words in English include <em class="italic">the, </em>and<em class="italic">, is, in </em>and<em class="italic"> for</em>. Stop word removal helps reduce the dimensionality of the data and improve the efficiency of the algorithms. By removing words that don’t contribute meaningfully to the analysis, computational resources can be focused on the words that do matter, improving the efficiency of various NLP algorithms.</p>
    <p class="normal">Note that stop word <a id="_idIndexMarker971"/>removal is more than a mere reduction in text size; it’s about focusing on the words that truly matter for the analysis at hand. While stop words play a vital role in language structure, their removal in NLP can enhance the efficiency and focus of the analysis, particularly in tasks like sentiment analysis where the primary concern is understanding the underlying emotion or opinion.</p>
    <h4 class="heading-4">Stemming and lemmatization</h4>
    <p class="normal">In textual <a id="_idIndexMarker972"/>data, most words are likely to be present in <a id="_idIndexMarker973"/>slightly different forms. Reducing each word to its origin or stem in <a id="_idIndexMarker974"/>a family of words is called <strong class="keyWord">stemming</strong>. It is used to group words based <a id="_idIndexMarker975"/>on their similar meanings to reduce the total number of words that need to be analyzed. Essentially, stemming reduces the overall conditionality of the problem. The most common algorithm for stemming English is the Porter algorithm.</p>
    <p class="normal">For example, let us look into a couple of examples:</p>
    <ul>
      <li class="bulletList">Example 1: <code class="inlineCode">{use, used, using, uses} =&gt; use</code></li>
      <li class="bulletList">Example 2: <code class="inlineCode">{easily, easier, easiest} =&gt; easi</code></li>
    </ul>
    <p class="normal">It’s important to note that stemming can sometimes result in misspelled words, as seen in example 2 where <code class="inlineCode">easi</code> was produced.</p>
    <p class="normal">Stemming is a simple and quick process, but it may not always produce correct results. For cases <a id="_idIndexMarker976"/>where correct spelling is required, lemmatization <a id="_idIndexMarker977"/>is a more appropriate method. Lemmatization <a id="_idIndexMarker978"/>considers the context and reduces <a id="_idIndexMarker979"/>words to their base form. The base form of a word, also known as the lemma, is its most simple and meaningful version. It represents the way a word would appear in the dictionary, devoid of any inflectional endings, which will be a correct English word, resulting in more accurate and meaningful word roots.</p>
    <div class="note">
      <p class="normal">The process of guiding algorithms to recognize similarities is a precise and thoughtful task. Unlike humans, algorithms need explicit rules and criteria to make connections that might seem obvious to us. Understanding this distinction and knowing how to provide the necessary guidance is a vital skill in the development and tuning of algorithms for various applications.</p>
    </div>
    <h1 id="_idParaDest-340" class="heading-1">Cleaning data using Python</h1>
    <p class="normal">Let us <a id="_idIndexMarker980"/>look into how we can clean text using Python.</p>
    <p class="normal">First, let’s <a id="_idIndexMarker981"/>import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> string
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords
<span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> PorterStemmer
<span class="hljs-comment"># Make sure to download the NLTK resources</span>
nltk.download(<span class="hljs-string">'punkt'</span>)
nltk.download(<span class="hljs-string">'stopwords'</span>)
</code></pre>
    <p class="normal">Next, here is the main function to perform text cleaning:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">clean_text</span>(<span class="hljs-params">text</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Cleans input text by converting case, removing punctuation, numbers,</span>
<span class="hljs-string">    white spaces, stop words and stemming</span>
<span class="hljs-string">    """</span>
    <span class="hljs-comment"># Convert to lowercase</span>
    text = text.lower()
    
    <span class="hljs-comment"># Remove punctuation</span>
    text = text.translate(<span class="hljs-built_in">str</span>.maketrans(<span class="hljs-string">''</span>, <span class="hljs-string">''</span>, string.punctuation))
    
    <span class="hljs-comment"># Remove numbers</span>
    text = re.sub(<span class="hljs-string">r'\d+'</span>, <span class="hljs-string">''</span>, text)
    
    <span class="hljs-comment"># Remove white spaces</span>
    text = text.strip()
    
    <span class="hljs-comment"># Remove stop words</span>
    stop_words = <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">'english'</span>))
    tokens = nltk.word_tokenize(text)
    filtered_text = [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stop_words]
    text = <span class="hljs-string">' '</span>.join(filtered_text)
    
    <span class="hljs-comment"># Stemming</span>
    ps = PorterStemmer()
    tokens = nltk.word_tokenize(text)
    stemmed_text = [ps.stem(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tokens]
    text = <span class="hljs-string">' '</span>.join(stemmed_text)
    
    <span class="hljs-keyword">return</span> text
</code></pre>
    <p class="normal">Let us <a id="_idIndexMarker982"/>test the <a id="_idIndexMarker983"/>function <code class="inlineCode">clean_text()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">corpus=<span class="hljs-string">"7- Today, Ottawa is becoming cold again "</span>
clean_text(corpus)
</code></pre>
    <p class="normal">The result will be:</p>
    <pre class="programlisting con"><code class="hljs-con">today ottawa becom cold
</code></pre>
    <p class="normal">Note the word <code class="inlineCode">becom</code> in the output. As we are using stemming, not all the words in the output are expected to be correct English words.</p>
    <p class="normal">All the preceding processing steps are typically needed; the actual processing steps depend on the problem that we want to solve. They will vary from use case to use case—for example, if the numbers in the text represent something that may have some value in the <a id="_idIndexMarker984"/>context of the problem that we are trying to solve, then we <a id="_idIndexMarker985"/>may not need to remove the numbers from the text in the normalization phase.</p>
    <p class="normal">Once the data is cleaned, we need to store the results in a data structure tailored for this purpose. This data structure is called the <strong class="keyWord">Term Document Matrix</strong> (<strong class="keyWord">TDM</strong>) and is explained next.</p>
    <h1 id="_idParaDest-341" class="heading-1">Understanding the Term Document Matrix</h1>
    <p class="normal">A TDM is a <a id="_idIndexMarker986"/>mathematical structure used in NLP. It’s a table that counts the frequency of terms (words) in a collection of documents. Each row represents a unique term, and each column represents a specific document. It’s an essential tool for text analysis, where you can see how often each word occurs in various texts.</p>
    <p class="normal">For documents containing the words <code class="inlineCode">cat</code> and <code class="inlineCode">dog</code>:</p>
    <ul>
      <li class="bulletList">Document 1: <code class="inlineCode">cat cat dog</code></li>
      <li class="bulletList">Document 2: <code class="inlineCode">dog dog cat</code></li>
    </ul>
    <table id="table001-7" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell"/>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Document 1</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Document 2</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">cat</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">dog</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">1</p>
          </td>
          <td class="table-cell">
            <p class="normal">2</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">This matrix structure allows the efficient storage, organization, and analysis of large text datasets. In Python, the <code class="inlineCode">CountVectorizer</code> module from the <code class="inlineCode">sklearn</code> library can be used to create a TDM as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer
<span class="hljs-comment"># Define a list of documents</span>
documents = [<span class="hljs-string">"Machine Learning is useful"</span>, <span class="hljs-string">"Machine Learning is fun"</span>, <span class="hljs-string">"Machine Learning is AI"</span>]
<span class="hljs-comment"># Create an instance of CountVectorizer</span>
vectorizer = CountVectorizer()
<span class="hljs-comment"># Fit and transform the documents into a TDM</span>
tdm = vectorizer.fit_transform(documents)
<span class="hljs-comment"># Print the TDM</span>
<span class="hljs-built_in">print</span>(tdm.toarray())
</code></pre>
    <p class="normal">The output looks as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">[[0 0 1 1 1 1]
 [0 1 1 1 1 0]
 [1 0 1 1 1 0]]
</code></pre>
    <p class="normal">Note that corresponding to each document, there is a row, and corresponding to each distinct word, there is a column. There are three documents and there are six distinct words, resulting in a matrix with dimensions 3x6.</p>
    <p class="normal">In this matrix, the numbers represent the frequency with which each word (column) appears in the corresponding document (row). So, for example, if the number in the first row and first column is 1, this means that the first word appears once in the first document.</p>
    <p class="normal">TDM uses the frequency of each term by default, which is a simple way to quantify the importance of each word in the context of each individual document. A more sophisticated way to quantify the importance of each word is TF-IDF, which is explained in the next section.</p>
    <h2 id="_idParaDest-342" class="heading-2">Using TF-IDF</h2>
    <p class="normal"><strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>) is a method used to calculate the significance <a id="_idIndexMarker987"/>of words in a <a id="_idIndexMarker988"/>document. It considers two <a id="_idIndexMarker989"/>main components to determine the weight of each <a id="_idIndexMarker990"/>term: the <strong class="keyWord">Term Frequency</strong> (<strong class="keyWord">TF</strong>) and the <strong class="keyWord">Inverse Document Frequency</strong> (<strong class="keyWord">IDF</strong>). The TF looks at how often a word appears in a specific document, while the IDF examines how rare the word is across a collection of documents, known as a corpus. In the context of TF-IDF, the corpus refers to the entire set of documents that you are analyzing. If we are working with a collection of book reviews, for example, the corpus would include all the reviews:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TF</strong>: TF measures the number of times a term appears in a document. It is calculated as the ratio of the number of occurrences of a term in a document to the total number of terms in the document. The more frequent the term, the higher its TF value.</li>
      <li class="bulletList"><strong class="keyWord">IDF</strong>: IDF measures the importance of a term across the entire corpus of documents. It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term. The rarer the term across the corpus, the higher its IDF value.</li>
    </ul>
    <p class="normal">To compute <a id="_idIndexMarker991"/>TF-IDF using Python, do the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-comment"># Define a list of documents</span>
documents = [<span class="hljs-string">"Machine Learning enables learning"</span>, <span class="hljs-string">"Machine Learning is fun"</span>, <span class="hljs-string">"Machine Learning is useful"</span>]
<span class="hljs-comment"># Create an instance of TfidfVectorizer</span>
vectorizer = TfidfVectorizer()
<span class="hljs-comment"># Fit and transform the documents into a TF-IDF matrix</span>
tfidf_matrix = vectorizer.fit_transform(documents)
<span class="hljs-comment"># Get the feature names</span>
feature_names = vectorizer.get_feature_names_out()
<span class="hljs-comment"># Loop over the feature names and print the TF-IDF score for each term</span>
<span class="hljs-keyword">for</span> i, term <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(feature_names):
    tfidf = tfidf_matrix[:, i].toarray().flatten()
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{term}</span><span class="hljs-string">: </span><span class="hljs-subst">{tfidf}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">This will print:</p>
    <pre class="programlisting con"><code class="hljs-con">enables:   [0.60366655 0.         0.        ]
fun:       [0.         0.66283998 0.        ]
is:        [0.         0.50410689 0.50410689]
learning:  [0.71307037 0.39148397 0.39148397]
machine:   [0.35653519 0.39148397 0.39148397]
useful:    [0.         0.         0.66283998]
</code></pre>
    <p class="normal">Each column in the output corresponds to a document, and the rows represent the TF-IDF values <a id="_idIndexMarker992"/>for the terms across the documents. For example, the term <code class="inlineCode">kids</code> has a non-zero TF-IDF value only in the second document, which is in line with our expectations.</p>
    <h2 id="_idParaDest-343" class="heading-2">Summary and discussion of results</h2>
    <p class="normal">The TF-IDF method provides a valuable way to weigh the importance of terms within individual <a id="_idIndexMarker993"/>documents and across an entire corpus. The resulting TF-IDF values reveal the relevance of specific terms within each document, taking into account both their frequency in a given document and their rarity across the entire collection. In the provided example, the varying TF-IDF scores for different terms demonstrate the model’s ability to distinguish words that are unique to specific documents from those that are more commonly used. This ability can be leveraged in various applications, such as text classification, information retrieval, and feature selection, to enhance the understanding and processing of text data.</p>
    <h1 id="_idParaDest-344" class="heading-1">Introduction to word embedding</h1>
    <p class="normal">One of the major advancements in NLP is our ability to create a meaningful numeric representation <a id="_idIndexMarker994"/>of words in the form of dense vectors. This technique is called word embedding. So, what exactly is a dense vector? Imagine you have a word like <code class="inlineCode">apple</code>. In word embedding, <code class="inlineCode">apple</code> might be represented as a series of numbers, such as <code class="inlineCode">[0.5, 0.8, 0.2]</code>, where each number is a coordinate in a continuous, multi-dimensional space. The term “dense” means that most or all of these numbers are non-zero, unlike sparse vectors where many elements might be zero. In simple terms, word embedding takes each word in a text and turns it into a unique, multi-dimensional point in space. This way, words with similar meanings will end up closer to each other in this space, allowing algorithms to understand the relationships between words. Yoshua Bengio first introduced the term in his paper <em class="italic">A Neural Probabilistic Language Model</em>. Each word in an NLP problem can be thought of as a categorical object.</p>
    <p class="normal">In word embedding, try to establish the neighborhood of each word and use it to quantify its meaning and importance. The neighborhood of a word is the set of words that surround a particular word.</p>
    <p class="normal">To truly grasp the concept of word embedding, let’s look at a tangible example involving a vocabulary of four familiar fruits: <code class="inlineCode">apple</code>, <code class="inlineCode">banana</code>, <code class="inlineCode">orange</code>, and <code class="inlineCode">pear</code>. The goal here is to represent these words as dense vectors, numerical arrays where each number captures a specific characteristic or feature of the word.</p>
    <p class="normal">Why represent <a id="_idIndexMarker995"/>words this way? In NLP, converting words into dense vectors enables algorithms to quantify the relationships between different words. Essentially, we’re turning abstract language into something that is mathematically measurable.</p>
    <p class="normal">Consider the features of sweetness, acidity, and juiciness for our fruit words. We could rate these features on a scale from 0 to 1 for each fruit, where 0 means the feature is entirely absent, and 1 means the feature is strongly present. This rating could look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">"apple": [0.5, 0.8, 0.2] – moderately sweet, quite acidic, not very juicy
"banana": [0.2, 0.3, 0.1] – not very sweet, moderately acidic, not juicy
"orange": [0.9, 0.6, 0.9] – very sweet, somewhat acidic, very juicy
"pear": [0.4, 0.1, 0.7] – moderately sweet, barely acidic, quite juicy
</code></pre>
    <p class="normal">The numbers are subjective and can be derived from taste tests, expert opinions, or other methods, but they serve to transform the words into a format that an algorithm can understand and work with.</p>
    <p class="normal">Visualizing this, you can imagine a 3D space where each axis represents one of the features (sweetness, acidity, or juiciness), and each fruit’s vector places it at a specific point in this space. Words (fruits) with similar tastes would be closer to each other in this space.</p>
    <p class="normal">So, why the choice of dense vectors with a length of 3? This is based on the specific features we have chosen to represent. In other applications, the vector length might be different, based on the number of features you want to capture.</p>
    <p class="normal">This example illustrates how word embedding takes a word and turns it into a numerical vector that holds real-world meaning. It’s a crucial step in enabling machines to “understand” and process human language.</p>
    <h1 id="_idParaDest-345" class="heading-1">Implementing word embedding with Word2Vec</h1>
    <p class="normal">Word2Vec is a prominent method used for obtaining vector representations of words, commonly <a id="_idIndexMarker996"/>referred to as word embeddings. Rather than “generating words,” this algorithm creates numerical vectors that <a id="_idIndexMarker997"/>represent the semantic meaning of each word in the language.</p>
    <p class="normal">The basic idea behind Word2Vec is to use a neural network to predict the context of each word in a given text corpus. The neural network is trained by inputting the word and its surrounding context words, and the network learns to output the probability distribution of the context words given the input word. The weights of the neural network are then used as the word embeddings, which can be used for various NLP tasks:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gensim
<span class="hljs-comment"># Define a text corpus</span>
corpus = [[<span class="hljs-string">'apple'</span>, <span class="hljs-string">'banana'</span>, <span class="hljs-string">'orange'</span>, <span class="hljs-string">'pear'</span>],
          [<span class="hljs-string">'car'</span>, <span class="hljs-string">'bus'</span>, <span class="hljs-string">'train'</span>, <span class="hljs-string">'plane'</span>],
          [<span class="hljs-string">'dog'</span>, <span class="hljs-string">'cat'</span>, <span class="hljs-string">'fox'</span>, <span class="hljs-string">'fish'</span>]]
<span class="hljs-comment"># Train a word2vec model on the corpus</span>
model = gensim.models.Word2Vec(corpus, window=<span class="hljs-number">5</span>, min_count=<span class="hljs-number">1</span>, workers=<span class="hljs-number">4</span>)
</code></pre>
    <p class="normal">Let us break down the important parameters of <code class="inlineCode">Word2Vec()</code> function:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">sentences</strong>: This <a id="_idIndexMarker998"/>is the input data for the model. It should be a collection of sentences, where each sentence is a list of words. Essentially, it’s a list of lists of words that represents your entire text corpus.</li>
      <li class="bulletList"><strong class="keyWord">size</strong>: This defines <a id="_idIndexMarker999"/>the dimensionality of the word embeddings. In other words, it sets the number of features or numerical values in the vectors that represent the words. A typical value might be <code class="inlineCode">100</code> or <code class="inlineCode">300</code>, depending on the complexity of the vocabulary.</li>
      <li class="bulletList"><strong class="keyWord">window</strong>: This <a id="_idIndexMarker1000"/>parameter sets the maximum distance between the target word and the context words used for prediction within a sentence. For example, if you set the window size to <code class="inlineCode">5</code>, the algorithm will consider the five words immediately before and after the target word in the training process.</li>
      <li class="bulletList"><strong class="keyWord">min_count</strong>: Words that appear infrequently in the corpus may be excluded from the <a id="_idIndexMarker1001"/>model by setting this parameter. If you set <code class="inlineCode">min_count</code> to <code class="inlineCode">2</code>, for example, any word that appears fewer than two times across all the sentences will be ignored during training.</li>
      <li class="bulletList"><strong class="keyWord">workers</strong>: This <a id="_idIndexMarker1002"/>refers to the number of processing threads used during training. Increasing this value can speed up training on multi-core machines by enabling parallel processing.</li>
    </ul>
    <p class="normal">Once the <a id="_idIndexMarker1003"/>Word2Vec model is trained, one of the powerful ways to use it is to measure the similarity or “distance” between <a id="_idIndexMarker1004"/>words in the embedding space. This similarity score can give us insight into how the model perceives relationships between different words. Now let us check the model by looking at the distance between <code class="inlineCode">car</code> and <code class="inlineCode">train</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(model.wv.similarity(<span class="hljs-string">'car'</span>, <span class="hljs-string">'train'</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">-0.057745814
</code></pre>
    <p class="normal">Now let’s look into the similarity of <code class="inlineCode">car</code> and <code class="inlineCode">apple</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(model.wv.similarity(<span class="hljs-string">'car'</span>, <span class="hljs-string">'apple'</span>))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">0.11117952
</code></pre>
    <p class="normal">Thus, the output gives us the similarity score between individual terms based on the word embeddings learned by the model.</p>
    <h2 id="_idParaDest-346" class="heading-2">Interpreting similarity scores</h2>
    <p class="normal">The following <a id="_idIndexMarker1005"/>details help with interpreting <a id="_idIndexMarker1006"/>similarity scores:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Very similar</strong>: Scores close to 1 signify strong similarity. Words with this score often share contextual or semantic meanings.</li>
      <li class="bulletList"><strong class="keyWord">Moderately similar</strong>: Scores around 0.5 indicate some level of similarity, possibly due to shared attributes or themes.</li>
      <li class="bulletList"><strong class="keyWord">Weak or no similarity</strong>: Scores close to 0 or negative imply little to no similarity or even contrast in meanings.</li>
    </ul>
    <p class="normal">Thus, these similarity scores provide quantitative insights into word relationships. By understanding <a id="_idIndexMarker1007"/>these scores, you can better analyze the semantic <a id="_idIndexMarker1008"/>structure of your text corpus and leverage it for various NLP tasks.</p>
    <p class="normal">Word2Vec provides a powerful and efficient way to represent textual data in a way that captures semantic relationships between words, reduces dimensionality, and improves accuracy in downstream NLP tasks. Let us look into the advantages and disadvantages of Word2Vec.</p>
    <h2 id="_idParaDest-347" class="heading-2">Advantages and disadvantages of Word2Vec</h2>
    <p class="normal">The following <a id="_idIndexMarker1009"/>are the advantages of using Word2Vec:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Capturing semantic relationships</strong>: Word2Vec’s embeddings are positioned in the vector space in such a way that semantically related words are located near each other. This spatial arrangement captures syntactic and semantic relationships like synonyms, analogies, and more, enabling better performance in tasks like information retrieval and semantic analysis.</li>
      <li class="bulletList"><strong class="keyWord">Reducing dimensionality</strong>: Traditional one-hot encoding of words can create a sparse and high-dimensional space, especially with large vocabularies. Word2Vec compresses this into a denser and lower-dimensional continuous vector space (typically ranging from 100 to 300 dimensions). This condensed representation preserves essential linguistic patterns while being computationally more efficient.</li>
      <li class="bulletList"><strong class="keyWord">Handling out-of-vocabulary words</strong>: Word2Vec can infer embeddings for words that didn’t appear in the training corpus by leveraging the surrounding context words. This property aids in generalizing better to unseen or new text data, enhancing robustness.</li>
    </ul>
    <p class="normal">Now let us <a id="_idIndexMarker1010"/>look into some of the disadvantages of using Word2Vec:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Training complexity</strong>: Word2Vec models can be computationally demanding to train, particularly with vast vocabularies and higher-dimensional vectors. They require significant computing resources and may necessitate optimization techniques, such as negative sampling or hierarchical softmax, to scale efficiently.</li>
      <li class="bulletList"><strong class="keyWord">Lack of interpretability</strong>: The continuous and dense nature of Word2Vec embeddings makes them challenging to interpret by humans. Unlike carefully crafted linguistic features, the dimensions in Word2Vec don’t correspond <a id="_idIndexMarker1011"/>to intuitive characteristics, making it difficult to understand what specific aspects of the words are being captured.</li>
      <li class="bulletList"><strong class="keyWord">Sensitive to text preprocessing</strong>: The quality and effectiveness of Word2Vec embeddings can vary significantly based on the preprocessing steps applied to the text data. Factors such as tokenization, stemming, and lemmatization, or the removal of stopwords, must be carefully considered. The choice of preprocessing can impact the spatial relationships within the vector space, potentially affecting the model’s performance on downstream tasks.</li>
    </ul>
    <p class="normal">Next, let us look into a case study about restaurant reviews that combines all the concepts presented in this chapter.</p>
    <h1 id="_idParaDest-348" class="heading-1">Case study: Restaurant review sentiment analysis</h1>
    <p class="normal">We will <a id="_idIndexMarker1012"/>use the Yelp Reviews dataset, which contains labeled reviews as positive (5 stars) or negative (1 star). We will train a model that can classify the reviews of a restaurant as negative or positive.</p>
    <p class="normal">Let’s implement this processing pipeline by going through the following steps.</p>
    <h2 id="_idParaDest-349" class="heading-2">Importing required libraries and loading the dataset</h2>
    <p class="normal">First, we <a id="_idIndexMarker1013"/>import <a id="_idIndexMarker1014"/>the packages that we need:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> PorterStemmer
<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords
</code></pre>
    <p class="normal">Then <a id="_idIndexMarker1015"/>we import <a id="_idIndexMarker1016"/>the dataset from a .<code class="inlineCode">csv</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code">url = <span class="hljs-string">'https://storage.googleapis.com/neurals/data/2023/Restaurant_Reviews.tsv'</span>
dataset = pd.read_csv(url, delimiter=<span class="hljs-string">'\t'</span>, quoting=<span class="hljs-number">3</span>)
dataset.head()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">                                             Review     Liked
0                           Wow... Loved this place.        1
1                                 Crust is not good.        0
2          Not tasty and the texture was just nasty.        0
3     Stopped by during the late May bank holiday of...     1
4      The selection on the menu was great and so wer...    1
</code></pre>
    <h2 id="_idParaDest-350" class="heading-2">Building a clean corpus: Preprocessing text data</h2>
    <p class="normal">Next, we <a id="_idIndexMarker1017"/>clean the data by performing text preprocessing on each of the reviews of the dataset using stemming and stopword removal techniques:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">clean_text</span>(<span class="hljs-params">text</span>):
    text = re.sub(<span class="hljs-string">'[^a-zA-Z]'</span>, <span class="hljs-string">' '</span>, text)
    text = text.lower()
    text = text.split()
    ps = PorterStemmer()
    text = [
        ps.stem(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> text 
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> word <span class="hljs-keyword">in</span> <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">'english'</span>))]
    text = <span class="hljs-string">'</span><span class="hljs-string"> '</span>.join(text)
    <span class="hljs-keyword">return</span> text
corpus = [clean_text(review) <span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">'Review'</span>]]
</code></pre>
    <p class="normal">The code iterates through each review in the dataset (in this case, the <code class="inlineCode">'Review'</code> column) and applies the <code class="inlineCode">clean_text</code> function to preprocess and clean each review. The code creates a new list called <code class="inlineCode">corpus</code>. The result is a list of cleaned and preprocessed reviews stored in the <code class="inlineCode">corpus</code> variable.</p>
    <h2 id="_idParaDest-351" class="heading-2">Converting text data into numerical features</h2>
    <p class="normal">Now let’s define the features (represented by <code class="inlineCode">y</code>) and the label (represented by <code class="inlineCode">X</code>). Remember <a id="_idIndexMarker1018"/>that <strong class="keyWord">features</strong> are the independent variables or attributes that describe the characteristics of the data, used as input for predictions. </p>
    <p class="normal">And <strong class="keyWord">labels</strong> are the dependent variables or target values that the model is trained to predict, representing the outcomes corresponding to the features:</p>
    <pre class="programlisting code"><code class="hljs-code">vectorizer = CountVectorizer(max_features=<span class="hljs-number">1500</span>)
X = vectorizer.fit_transform(corpus).toarray()
y = dataset.iloc[:, <span class="hljs-number">1</span>].values
</code></pre>
    <p class="normal">Let’s divide the data into testing and training data:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number">0.20</span>, random_state=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">To train the model, we are using the Naive Bayes algorithm that we studied in <em class="chapterRef">Chapter 7</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">classifier = GaussianNB()
classifier.fit(X_train, y_train)
</code></pre>
    <p class="normal">Let’s predict the test set results:</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred = classifier.predict(X_test)
</code></pre>
    <p class="normal">Next, let us print the confusion matrix. Remember that the confusion matrix is a table that helps visualize the performance of the classification model: </p>
    <pre class="programlisting code"><code class="hljs-code">cm = confusion_matrix(y_test, y_pred)
<span class="hljs-built_in">print</span>(cm)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[[55 42]
 [12 91]]
</code></pre>
    <p class="normal">Looking at the confusion matrix, we can estimate the misclassification.</p>
    <h2 id="_idParaDest-352" class="heading-2">Analyzing the results</h2>
    <p class="normal">The <a id="_idIndexMarker1019"/>confusion matrix gives us a glimpse into the misclassifications made by our model. In this context, there are:</p>
    <ul>
      <li class="bulletList">55 true positives (correctly predicted positive reviews)</li>
      <li class="bulletList">42 false positives (incorrectly predicted positive reviews)</li>
      <li class="bulletList">12 false negatives (incorrectly predicted negative reviews)</li>
      <li class="bulletList">91 true negatives (correctly predicted negative reviews)</li>
    </ul>
    <p class="normal">The 55 true positives and 91 true negatives show that our model has a reasonable ability to distinguish between positive and negative reviews. However, the 42 false positives and 12 false negatives highlight areas for potential improvement.</p>
    <p class="normal">In the context of restaurant reviews, understanding these numbers helps business owners and customers alike gauge the general sentiment. A high rate of true positives and true negatives indicates that the model can be trusted to give an accurate sentiment overview. This information could be invaluable for restaurants aiming to improve service or for potential customers seeking honest reviews. On the other hand, the presence of false positives and negatives suggests areas where the model might need fine-tuning to avoid misclassification and provide more accurate insights.</p>
    <h1 id="_idParaDest-353" class="heading-1">Applications of NLP</h1>
    <p class="normal">The continued advancement of NLP technology has revolutionized the way we interact with computers <a id="_idIndexMarker1020"/>and other digital devices. It has made significant progress in recent years, with impressive achievements in many tasks, including:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Topic identification</strong>: To discover topics in a text repository and then classify the documents in the repository according to the topics discovered.</li>
      <li class="bulletList"><strong class="keyWord">Sentiment analysis</strong>: To classify the text according to the positive or negative sentiments that it contains.</li>
      <li class="bulletList"><strong class="keyWord">Machine translation</strong>: To translate between different languages.</li>
      <li class="bulletList"><strong class="keyWord">Text to speech</strong>: To convert spoken words into text.</li>
      <li class="bulletList"><strong class="keyWord">Question answering</strong>: This is a process of understanding and responding to a query using the information that is available. It involves intelligently interpreting the question and providing a relevant answer based on the existing knowledge or data.</li>
      <li class="bulletList"><strong class="keyWord">Entity recognition</strong>: To identify <a id="_idIndexMarker1021"/>entities (such as a person, place, or thing) from text.</li>
      <li class="bulletList"><strong class="keyWord">Fake news detection</strong>: To flag fake news based on the content.</li>
    </ul>
    <h1 id="_idParaDest-354" class="heading-1">Summary</h1>
    <p class="normal">The chapter discussed the basic terminology related to NLP, such as corpus, word embeddings, language modeling, machine translation, and sentiment analysis. In addition, the chapter covered various text preprocessing techniques that are essential in NLP, including tokenization, which involves breaking down text into smaller units called tokens, and other techniques such as stemming and stop word removal. </p>
    <p class="normal">The chapter also discussed word embeddings and then presented a use case on restaurant review sentiment analysis. Now, readers should have a better understanding of the fundamental techniques used in NLP and their potential applications to real-world problems.</p>
    <p class="normal">In the next chapter, we will look at training neural networks for sequential data. We will also investigate how the use of deep learning can further improve NLP techniques and the methodologies discussed in this chapter.</p>
    <h1 id="_idParaDest-355" class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/WHLel"><span class="url">https://packt.link/WHLel</span></a></p>
    <p class="normal"><img src="../Images/QR_Code1955211820597889031.png" alt="" role="presentation"/></p>
  </div>
</body></html>