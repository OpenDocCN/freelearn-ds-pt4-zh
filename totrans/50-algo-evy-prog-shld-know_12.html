<html><head></head><body>
  <div id="_idContainer409" class="Basic-Text-Frame">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-356" class="chapterTitle">Understanding Sequential Models</h1>
    <blockquote class="packt_quote">
      <p class="quote">A sequence works in a way a collection never can.</p>
      <p class="cite">—George Murray</p>
    </blockquote>
    <p class="normal">This chapter covers an important class of machine learning models, the sequential models. A defining characteristic of such models is that the processing layers are arranged in such a way that the output of one layer is the input to the other. This architecture makes them perfect to process sequential data. Sequential data is the type of data that consists of ordered series of elements such as a sentence in a document or a time series of stock market prices. </p>
    <p class="normal">In this chapter, we will start with understanding the characteristics of sequential data. Then, we will present the working of RNNs and how they can be used to process sequential data. Next, we will learn how we can address the limitations of RNN through GRU without scarifying accuracy. Then, we will discuss the architecture of LSTM. Finally, we will compare different sequential modeling architectures with a recommendation on when to use which one.</p>
    <p class="normal">In this chapter, we will go through the following concepts:</p>
    <ul>
      <li class="bulletList">Understanding sequential data</li>
      <li class="bulletList">How RNNs can process sequential data</li>
      <li class="bulletList">Addressing the limitations of RNNs through GRUs</li>
      <li class="bulletList">Understanding LSTM</li>
    </ul>
    <p class="normal">Let us start by first looking into the characteristics of sequential data.</p>
    <h1 id="_idParaDest-357" class="heading-1">Understanding sequential data</h1>
    <p class="normal">Sequential data is a<a id="_idIndexMarker1022"/> specific type of data structure where the order of the elements matters, and each element has a relational dependency on its predecessors. This “sequential behavior” is distinct because it conveys information not just in the individual elements but also in the pattern or sequence in which they occur. In sequential data, the current observation is not only influenced by external factors but also by previous observations in the sequence. This dependency forms the core characteristic of sequential data.</p>
    <p class="normal">Understanding the different types of sequential data is essential to appreciate its broad applications. Here are <a id="_idIndexMarker1023"/>the primary categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Time series data</strong>: This is a <a id="_idIndexMarker1024"/>series of data<a id="_idIndexMarker1025"/> points indexed or listed in time order. The value at any point in time is dependent on the past values. Time series data is widely used in various fields, including economics, finance, and healthcare.</li>
      <li class="bulletList"><strong class="keyWord">Textual data</strong>: Text <a id="_idIndexMarker1026"/>data <a id="_idIndexMarker1027"/>is also sequential in nature, where the order of words, sentences, or paragraphs can convey <a id="_idIndexMarker1028"/>meaning. <strong class="keyWord">Natural language processing</strong> (<strong class="keyWord">NLP</strong>) leverages this sequential property to analyze and interpret human languages.</li>
      <li class="bulletList"><strong class="keyWord">Spatial-temporal data</strong>: This <a id="_idIndexMarker1029"/>involves<a id="_idIndexMarker1030"/> data that captures both spatial and temporal relationships, such as weather patterns or traffic flow over time in a specific geographical area.</li>
    </ul>
    <p class="normal">Here’s how these types of sequential data manifest in real-world scenarios:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Time series data</strong>: This <a id="_idIndexMarker1031"/>type of data is clearly illustrated through financial market trends, where stock prices constantly vary in response to ongoing market dynamics. Similarly, sociological studies might analyze birth rates, reflecting year-to-year changes influenced by factors like economic conditions and social policies.</li>
      <li class="bulletList"><strong class="keyWord">Textual data</strong>: The sequential nature of text is paramount in literary and journalistic works. In novels, news articles, or essays, the specific ordering of words, sentences, and paragraphs constructs narratives and arguments, giving the text meaning beyond individual words.</li>
      <li class="bulletList"><strong class="keyWord">Spatial-temporal data</strong>: Areas in which this data type is vital are urban development and environmental studies. For instance, housing prices across different regions might be tracked over time to identify economic trends, while meteorological<a id="_idIndexMarker1032"/> studies might monitor weather changes at specific geographical locations to forecast patterns and natural events.</li>
    </ul>
    <p class="normal">These real-world examples demonstrate how the inherent sequential behavior in different types of data can be leveraged to provide insights and drive decisions across various domains.</p>
    <p class="normal">In deep learning, handling sequential data requires specialized neural network architectures like sequential models. These models are designed to capture and exploit the temporal dependencies that inherently exist among the elements of sequential data. By recognizing these dependencies, sequential models provide a robust framework for creating more nuanced and effective machine learning models.</p>
    <p class="normal">In summary, sequential data is a rich and complex type of data that finds applications across diverse domains. Recognizing its sequential nature, understanding its types, and leveraging specialized <a id="_idIndexMarker1033"/>models enable data scientists to draw deeper insights and build more powerful predictive tools. Before we study the technical details, let us start by looking at the history of sequential modeling techniques.</p>
    <p class="normal">Let us study different types of sequential models.</p>
    <h2 id="_idParaDest-358" class="heading-2">Types of sequence models</h2>
    <p class="normal">Sequential models are <a id="_idIndexMarker1034"/>classified into various categories by examining the kind of data they handle, both in terms of input and output. This classification takes into account the specific nature of the data being used (like textual information, numerical data, or time-based patterns), and also how this data evolves or transforms from the beginning of the process to the end. By delving into these characteristics, we can identify three principal types of sequence models.</p>
    <h3 id="_idParaDest-359" class="heading-3">One-to-many</h3>
    <p class="normal">In one-to-many<a id="_idIndexMarker1035"/> sequence <a id="_idIndexMarker1036"/>models, a singular event or input can initiate the generation of an entire sequence. This unique attribute opens doors to a wide range of applications, but it also leads to complexities in training and implementation. The one-to-many sequence models offer exciting opportunities but come with inherent complexities in training and execution. As generative AI continues to advance, these models are likely to play a pivotal role in shaping creative and customized solutions across various domains. </p>
    <p class="normal">The key to harnessing their potential lies in understanding their capabilities and recognizing the intricacies of training<a id="_idIndexMarker1037"/> and implementation. The one-to-many <a id="_idIndexMarker1038"/>sequence model is shown in <em class="italic">Figure 10.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_01.png" alt="A diagram of a flowchart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.1: One-to-many sequential model </p>
    <p class="normal">Let’s delve into the characteristics, capabilities, and challenges of one-to-many models:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Wide range of applications</strong>: The ability <a id="_idIndexMarker1039"/>to translate a single input into a meaningful sequence makes one-to-many models versatile and powerful. They can be employed to write poetry, create art such as drawings and paintings, and even craft personalized cover letters for job applications.</li>
      <li class="bulletList"><strong class="keyWord">Part of generative AI</strong>: These models fall under the umbrella of generative AI, a burgeoning field that aims to create new content that is both coherent and contextually relevant. This is what allows them to perform such varied tasks as mentioned above.</li>
      <li class="bulletList"><strong class="keyWord">Intensive training process</strong>: Training one-to-many models is typically more time-consuming and computationally expensive compared to other sequence models. The reason for this lies in the complexity of translating a single input into a wide array of potential outputs. The model must learn not only the relationship between the input and the output but also the intricate patterns<a id="_idIndexMarker1040"/> and structures inherent in the generated sequence.</li>
    </ul>
    <p class="normal">Note that unlike one-to-one models, where a single input corresponds to a single output, or many-to-many models, where a sequence of inputs is mapped to a sequence of outputs, the one-to-many paradigm must learn to extrapolate a rich and structured sequence from a singular starting point. This requires a deeper understanding of the underlying patterns and can often necessitate more sophisticated training algorithms.</p>
    <p class="normal">The one-to-many approach isn’t without its challenges. Ensuring that the generated sequence <a id="_idIndexMarker1041"/>maintains coherence, relevance, and<a id="_idIndexMarker1042"/> creativity requires careful design and fine-tuning. It often demands a more extensive dataset and expert knowledge in the specific domain to guide the model’s training.</p>
    <h3 id="_idParaDest-360" class="heading-3">Many-to-one</h3>
    <p class="normal">Many-to-one sequential <a id="_idIndexMarker1043"/>models <a id="_idIndexMarker1044"/>are specialized tools in data analysis that take a sequence of inputs and convert them into a single output. This process of synthesizing multiple inputs into one concise output forms the core of the many-to-one model, allowing it to distill the essential characteristics of the data.</p>
    <p class="normal">These models have diverse applications, such as in sentiment analysis, where a sequence of words like a review or a post is analyzed to determine an overall sentiment such as positive, negative, or neutral. The many-to-one sequential model is shown in <em class="italic">Figure 10.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_02.png" alt="A diagram of a flowchart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.2: Many-to-one sequential model</p>
    <p class="normal">The training process of many-to-one models is a complex yet integral part of their functionality. It distinguishes them from one-to-many models, whose focus is on creating a sequence from a single input. In contrast, many-to-one models must efficiently compress information, demanding careful selection of algorithms and precise tuning of parameters.</p>
    <p class="normal">Training a many-to-one model involves teaching it to identify the vital features of the input sequence and to represent them accurately in the output. This involves discarding irrelevant information, a task that requires intricate balancing. The training process also often necessitates specialized pre-processing and feature engineering, tailored to the specific nature of the input data.</p>
    <p class="normal">As discussed in the prior subsection, the training of many-to-one models may be more challenging than other types, requiring a deeper understanding of the underlying relationships in the data. Continuous monitoring of the model’s performance during training, along with a methodical selection of data and hyperparameters, is essential for the success of the model.</p>
    <p class="normal">Many-to-one models are noteworthy for their ability to simplify complex data into understandable insights, finding applications in various industries for tasks such as summarization, classification, and prediction. Although their design and training can be intricate, their unique ability to interpret sequential data provides inventive solutions to complex data analysis challenges.</p>
    <p class="normal">Thus, many-to-one sequential models are vital instruments in contemporary data analysis, and understanding their particular training process is crucial for leveraging their capabilities fully. The training process, characterized by meticulous algorithm selection, parameter<a id="_idIndexMarker1045"/> tuning, and domain<a id="_idIndexMarker1046"/> expertise, sets these models apart. As the field progresses, many-to-one models will continue to offer valuable contributions to data interpretation and application.</p>
    <h3 id="_idParaDest-361" class="heading-3">Many-to-many</h3>
    <p class="normal">This is a type of<a id="_idIndexMarker1047"/> sequential<a id="_idIndexMarker1048"/> model that takes sequential data as the input, processes it in some way, and then generates sequential data as the output. An example of many-to-many models is machine translation, where a sequence of words in one language is translated into a corresponding sequence in another language. An illustrative example of this would be the translation of English text into French. While there are numerous machine translation models that fall into this category, a prominent approach is the use of <strong class="keyWord">Sequence-to-Sequence</strong> (<strong class="keyWord">Seq2Seq</strong>) models, particularly with STM networks. Seq2Seq <a id="_idIndexMarker1049"/>models with LSTM have become a standard method for tasks such as English-to-French translation and have been implemented in various NLP frameworks and tools. The many-to-many sequential model is shown in <em class="italic">Figure 10.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_03.png" alt="A diagram of a flowchart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.3: Many-to-many sequential model</p>
    <p class="normal">Over the years, many algorithms have been developed to process and train machine learning models using <a id="_idIndexMarker1050"/>sequential <a id="_idIndexMarker1051"/>data. Let us start with studying how to represent sequential data with 3-dimensional data structures.</p>
    <h1 id="_idParaDest-362" class="heading-1">Data representation for sequential models</h1>
    <p class="normal">Timesteps add depth <a id="_idIndexMarker1052"/>to the data, making it a 3D structure. In<a id="_idIndexMarker1053"/> the context of sequential data, each “unit” or instance of this dimension is termed a “timestep.” This is crucial to remember: while the dimension is called “timesteps,” each individual data point in this dimension is a “timestep.” <em class="italic">Figure 10.4</em> illustrates the three dimensions in data used for training RNNs, emphasizing the addition of timesteps:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_04.png" alt="A picture containing text, linedrawing  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.4: The 3D data structures used in RNN training</p>
    <p class="normal">Given that the concept of timesteps is a new addition to our exploration, a special notation is introduced to represent it effectively. A superscript enclosing a timestep in angle brackets is paired with the variable in question. For example, using this notation, <img src="../Images/B18046_10_001.png" alt="" role="presentation"/> and <img src="../Images/B18046_10_002.png" alt="" role="presentation"/> represent the value of the variable <code class="inlineCode">stock_price</code> at timestep <em class="italic">t1</em> and timestep <em class="italic">t2</em>, respectively.</p>
    <p class="normal">The choice of dividing data into batches, essentially deciding the “length,” can be both an intentional design decision and influenced by external tools and libraries. Often, machine learning<a id="_idIndexMarker1054"/> frameworks<a id="_idIndexMarker1055"/> provide utilities to automatically batch data, but choosing an optimal batch size can be a combination of experimentation and domain knowledge.</p>
    <p class="normal">Let us start the discussion on sequential modeling techniques with RNNs first.</p>
    <h1 id="_idParaDest-363" class="heading-1">Introducing RNNs</h1>
    <p class="normal">RNNs, are a special<a id="_idIndexMarker1056"/> breed of neural networks designed specifically for sequential data. Here’s a breakdown of their key attributes.</p>
    <p class="normal">The term “recurrent” stems from the unique feedback loop RNNs possess. Unlike traditional neural networks, which are essentially stateless and produce outputs solely based on the current inputs, RNNs carry forward a “state” from one step in the sequence to the next.</p>
    <p class="normal">When we talk about a “run” in the context of RNNs, we’re referring to a single pass or processing of an element in the sequence. So, as the RNN processes each element, or each “run,” it retains some information from the previous steps.</p>
    <p class="normal">The magic of RNNs lies in their ability to maintain a memory of previous runs or steps. They achieve this by incorporating an additional input, which is essentially the state or memory from the previous run. This mechanism allows RNNs to recognize and learn the dependencies between <a id="_idIndexMarker1057"/>elements in a sequence, such as the relationships between consecutive words in a sentence.</p>
    <p class="normal">Let us study the architecture of RNNs in detail.</p>
    <h2 id="_idParaDest-364" class="heading-2">Understanding the architecture of RNNs</h2>
    <p class="normal">First, let us <a id="_idIndexMarker1058"/>define some variables:</p>
    <ul>
      <li class="bulletList"><img src="../Images/B18046_10_003.png" alt="" role="presentation"/> : the input at timestep <em class="italic">t</em></li>
      <li class="bulletList"><img src="../Images/B18046_10_004.png" alt="" role="presentation"/> : actual output (ground truth) at timestep <em class="italic">t</em></li>
      <li class="bulletList"><img src="../Images/B18046_10_005.png" alt="" role="presentation"/> : predicted output at timestep <em class="italic">t</em></li>
    </ul>
    <h3 id="_idParaDest-365" class="heading-3">Understanding the memory cell and hidden state</h3>
    <p class="normal"><strong class="keyWord">RNNs</strong> stand out <a id="_idIndexMarker1059"/>because of their inherent ability to remember and maintain context as they progress through different timesteps. This state at a certain timestep <em class="italic">t</em> is represented by <img src="../Images/B18046_10_006.png" alt="" role="presentation"/>, where <em class="italic">h</em> stands for hidden. It is the summary of the information learned up to a particular timestep. As shown in <em class="italic">Figure 10.5</em>, the RNN keeps on learning by updating its hidden state at each timestep. The RNN uses this hidden state at each timestep to keep a context. At its core, “context” refers to the collective information or knowledge an RNN retains from previous timesteps. It allows RNNs to memorize the state at each timestep and pass this information to the next timestep as it progresses along through the sequence. This hidden state makes the RNN stateful:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_05.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.5: Hidden state in RNN</p>
    <p class="normal">For example, if we use an RNN to translate a sentence from English to French, each input is a sentence that needs to be defined as sequential data. To get it right, the RNN cannot translate each word in isolation. It needs to capture the context of the words that have been translated so far, allowing the RNN to correctly translate the entire sentence. This is achieved through the hidden state that is calculated and stored at each timestep and <a id="_idIndexMarker1060"/>passed on to the later ones.</p>
    <div class="note">
      <p class="normal">The RNN’s strategy of memorizing the state with the intention of using it for future timesteps brings new research questions that need to be addressed. For example, <em class="italic">what</em> to remember and <em class="italic">what</em> to forget. And, perhaps the trickiest one, <em class="italic">when</em> to forget. The variants of RNN, like GRUs and LSTM, attempt to answer these questions in different ways.</p>
    </div>
    <h3 id="_idParaDest-366" class="heading-3">Understanding the characteristics of the input variable</h3>
    <p class="normal">Let’s get a <a id="_idIndexMarker1061"/>deeper understanding of the input variable, <img src="../Images/B18046_10_007.png" alt="" role="presentation"/>, and the methodology behind encoding it when working with RNNs. One of the pivotal applications for RNNs lies in the realm of NLP. Here, the sequential data we deal with comprises sentences. Think of each sentence as a sequence of words, such that a sentence can be delineated as:</p>
    <p class="center"><img src="../Images/B18046_10_008.png" alt="" role="presentation"/></p>
    <p class="normal">In this representation, <img src="../Images/B18046_10_009.png" alt="" role="presentation"/> denotes an individual word within the sentence. To avoid confusion: each <img src="../Images/B18046_10_010.png" alt="" role="presentation"/> is not an entire sentence but rather an individual word within it.</p>
    <p class="normal">Each word, <img src="../Images/B18046_10_011.png" alt="" role="presentation"/>, is encoded using a one-hot vector. The length of this vector is defined by |V|, where:</p>
    <ul>
      <li class="bulletList">V signifies our vocabulary set, which is a collection of distinct words.</li>
      <li class="bulletList">|V| quantifies the total number of entries in V.</li>
    </ul>
    <p class="normal">In the context of widely-used applications, one could envision V as comprising the entire set of words found <a id="_idIndexMarker1062"/>in a standard English dictionary, which may contain roughly 150,000 words. However, for specific NLP tasks, only a subset of this vast vocabulary is necessary.</p>
    <p class="normal"><strong class="keyWord">Note</strong>: It’s essential to differentiate between V and |V|. While V stands for the vocabulary itself, |V| represents the size of this vocabulary.</p>
    <p class="normal">When referring to the “dictionary,” we’re drawing from a general notion of standard English dictionaries. However, there are more exhaustive corpora available, like the Common Crawl, which can contain word sets stretching into the tens of millions.</p>
    <p class="normal">For many <a id="_idIndexMarker1063"/>applications, a subset of this vocabulary should be enough. Formally,</p>
    <p class="center"><img src="../Images/B18046_10_012.png" alt="" role="presentation"/></p>
    <p class="normal">To understand the working of RNNs, let us examine the first timestep, <em class="italic">t1</em>.</p>
    <h2 id="_idParaDest-367" class="heading-2">Training the RNN at the first timestep</h2>
    <p class="normal">RNNs operate by <a id="_idIndexMarker1064"/>analyzing sequences one timestep at a time. Let’s dive into the initial phase of this process. For the timestep <em class="italic">t1</em>, the network receives an input represented as <img src="../Images/B18046_10_013.png" alt="" role="presentation"/>. Based on this input, the RNN makes an initial prediction, which we denote as <img src="../Images/B18046_10_014.png" alt="" role="presentation"/>. At every timestep, <em class="italic">tt</em>, the RNN leverages the hidden state from the previous timestep, <img src="../Images/B18046_10_015.png" alt="" role="presentation"/>, to provide contextual information.</p>
    <p class="normal">However, at <em class="italic">t1</em>, since we’re just beginning, there’s no prior hidden state to reference. Therefore, the hidden state <img src="../Images/B18046_10_016.png" alt="" role="presentation"/> is initialized to zero.</p>
    <h3 id="_idParaDest-368" class="heading-3">The activation function in action</h3>
    <p class="normal">Referencing <em class="italic">Figure 10.6</em>, you’ll <a id="_idIndexMarker1065"/>notice an element marked by <strong class="keyWord">A</strong>. This represents the activation function, a crucial component in neural networks. Essentially, the activation function determines how much signal to pass onto the next layer. For this timestep, the activation function receives both the input <img src="../Images/B18046_10_017.png" alt="" role="presentation"/> and the previous hidden state <img src="../Images/B18046_10_018.png" alt="" role="presentation"/>.</p>
    <p class="normal">As discussed in <em class="chapterRef">Chapter 8</em>, an activation function in neural networks is a mathematical equation that determines the output of a neuron based on its input. Its primary role is to introduce non-linearity into the network, enabling it to learn from errors and make adjustments, which is essential for learning complex patterns.</p>
    <p class="normal">A recurring choice for the activation function in many neural networks is “<code class="inlineCode">tanh</code>.” But what’s the reasoning behind this preference?</p>
    <p class="normal">The world of neural networks isn’t without its challenges, and one such obstacle is the vanishing gradient problem. To put it plainly, as we keep training our model, occasionally, the gradient values, which guide our weight adjustments, diminish to tiny numbers. This drop means the changes we make to our network’s weights become almost negligible. Such minute tweaks result in an excruciatingly slow learning process, sometimes even coming to a standstill. Here’s where the “<code class="inlineCode">tanh</code>" function shines. It’s chosen because it acts as a buffer against this vanishing gradient issue, steering the training process<a id="_idIndexMarker1066"/> toward consistency and efficiency:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_06.png" alt="Diagram  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 10.6: RNN training at timestep t1</p>
    <p class="normal">As we zero in on the outcome of the activation function, we arrive at the value for the hidden state, <img src="../Images/B18046_10_019.png" alt="" role="presentation"/>. In mathematical terms, this relationship can be expressed as:</p>
    <p class="center"><img src="../Images/B18046_10_020.png" alt="" role="presentation"/></p>
    <p class="normal">This hidden state is not just a passing phase. It holds value as we step into the next timestep, <em class="italic">t2</em>. Think of it as a relay racer passing on the baton, or in this case, context, from one timestep to its successor, ensuring continuity in the sequence.</p>
    <p class="normal">The second activation function (represented by <strong class="keyWord">B</strong> in <em class="italic">Figure 10.7</em>) is used to generate the predicted output <img src="../Images/B18046_10_021.png" alt="" role="presentation"/> at timestep <em class="italic">t1</em>. The choice of this activation function will depend on the type of the output variable. For instance, if an RNN is employed to predict stock market prices, the ReLU function can be adopted as the output variable is continuous. On the other hand, if we are doing sentiment analysis on a bunch of posts, it can be a sigmoid activation function. In <em class="italic">Figure 10.7</em>, assuming that it is a multiclass output variable, we are using the softmax activation function. Remember that a multiclass output variable refers to a situation where the output or the prediction can fall into one of several distinct classes. In machine learning, this is common in classification problems where the aim is to categorize an input into one of several predefined categories. For example, if we are categorizing objects as a car, bike, or bus, the output variable has multiple classes, thus is termed as “multiclass.” Mathematically, we can represent it as:</p>
    <p class="center"><img src="../Images/B18046_10_022.png" alt="" role="presentation"/></p>
    <p class="normal">From <em class="italic">Eq. 10.1</em> and <em class="italic">Eq. 10.2</em>, It should be obvious that the objective of training the RNN is to find the optimal values<a id="_idIndexMarker1067"/> of three sets of weight matrices (<em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">yh</sub>) and two sets of biases (<em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">h</sub> and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">y</sub>). As we progress, it becomes evident that these weights and biases maintain consistency across all timesteps.</p>
    <h3 id="_idParaDest-369" class="heading-3">Training the RNN for a whole sequence</h3>
    <p class="normal">Previously, we developed<a id="_idIndexMarker1068"/> the mathematical formulation for the hidden state for the first timestep, <em class="italic">t1</em>. Let us now study the working of the RNN through more than one timestep to train a complete sequence, as shown in <em class="italic">Figure 10.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_07.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.7: Sequential processing in RNN</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Info</strong>: In <em class="italic">Figure 10.7</em>, it can be observed that the hidden state travels from left to right carrying the context forward shown by the arrow <strong class="keyWord">A</strong>. The ability of RNNs and their variants to create this “information highway” propagating through time is the defining feature of RNNs.</p>
    </div>
    <p class="normal">We calculated <em class="italic">Eq. 10.1</em> for the timestep <em class="italic">t1</em>. For any timestep t, we can generalize <em class="italic">Eq. 10.1</em> as:</p>
    <p class="center"><img src="../Images/B18046_10_023.png" alt="" role="presentation"/></p>
    <p class="normal">For NLP applications, <img src="../Images/B18046_10_024.png" alt="" role="presentation"/>is encoded as a one-hot vector. In this case, the dimension of <img src="../Images/B18046_10_025.png" alt="" role="presentation"/> will be <a id="_idIndexMarker1069"/>equal to |V|, where V is the vector representing the vocabulary. The hidden variable <img src="../Images/B18046_10_026.png" alt="" role="presentation"/> will be a lower-dimensional representation of the original input, <img src="../Images/B18046_10_025.png" alt="" role="presentation"/>. By lowering the dimension of the input variable <img src="../Images/B18046_10_028.png" alt="" role="presentation"/> by many folds, we intend the hidden layer to capture only the important information of the input variable <img src="../Images/B18046_10_025.png" alt="" role="presentation"/>. The dimension of <img src="../Images/B18046_10_006.png" alt="" role="presentation"/> is represented by <em class="italic">D</em><sub class="subscript-italic" style="font-style: italic;">h</sub>.</p>
    <p class="normal">It is not unusual for <img src="../Images/B18046_10_006.png" alt="" role="presentation"/> to have dimensions 500 times lower than <img src="../Images/B18046_10_032.png" alt="" role="presentation"/>.</p>
    <p class="normal">So, typically:</p>
    <p class="center"><img src="../Images/B18046_10_033.png" alt="" role="presentation"/></p>
    <p class="normal">Because of the<a id="_idIndexMarker1070"/> lower dimensions of <img src="../Images/B18046_10_034.png" alt="" role="presentation"/> , weight matrix <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> is a comparatively small data structure as <img src="../Images/B18046_10_035.png" alt="" role="presentation"/>. <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> on the other hand, will be as wide as <img src="../Images/B18046_10_036.png" alt="" role="presentation"/>.</p>
    <h4 class="heading-4">Combining weight matrices</h4>
    <p class="normal">In <em class="italic">Eq. 10.3</em>, both <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> are<a id="_idIndexMarker1071"/> used in the calculation of <img src="../Images/B18046_10_037.png" alt="" role="presentation"/>. To simplify the analysis, it helps to combine <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> into one weight parameter matrix, <img src="../Images/B18046_10_038.png" alt="" role="presentation"/>. This simplified representation will be quite useful for the discussion of more complex variants of RNNs that are discussed later in this chapter.</p>
    <p class="normal">To create one combined weight matrix, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">h</sub>, we simply horizontally concatenate <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> horizontally to create a combined weight matrix, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">h</sub>:</p>
    <p class="center"><img src="../Images/B18046_10_039.png" alt="" role="presentation"/></p>
    <p class="normal">As we are simply horizontally concatenating, the dimensions of the <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">h</sub> will have the same number of rows and total number of columns, i.e.,</p>
    <p class="center"><img src="../Images/B18046_10_040.png" alt="" role="presentation"/></p>
    <p class="normal">Using <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">h</sub> in <em class="italic">Eq. 10.3</em>:</p>
    <p class="center"><img src="../Images/B18046_10_041.png" alt="" role="presentation"/></p>
    <p class="normal">Where <img src="../Images/B18046_10_042.png" alt="" role="presentation"/> indicates<a id="_idIndexMarker1072"/> the vertical stacking of two vectors together.</p>
    <p class="center"><img src="../Images/B18046_10_043.png" alt="" role="presentation"/></p>
    <p class="normal">Where <img src="../Images/B18046_10_044.png" alt="" role="presentation"/> and <img src="../Images/B18046_10_045.png" alt="" role="presentation"/> are the respective transposed vectors.</p>
    <p class="normal">Let us look at a specific example.</p>
    <p class="normal">Let us assume that we are using RNNs for an NLP application. The size of the vocabulary is 50,000 words. It means that each input <img src="../Images/B18046_10_025.png" alt="" role="presentation"/> will be encoded as a hot vector having a dimension of 50,000. Let assume that <img src="../Images/B18046_10_047.png" alt="" role="presentation"/> has a dimension of 50. It will be the lower-dimension representation of <img src="../Images/B18046_10_025.png" alt="" role="presentation"/>.</p>
    <p class="normal">Now, it should be obvious that <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> will have dimensions of (50<img src="../Images/B18046_10_049.png" alt="" role="presentation"/>50). <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> will have dimensions of (50<img src="../Images/B18046_10_049.png" alt="" role="presentation"/>50,000).</p>
    <p class="normal">Going back to the<a id="_idIndexMarker1073"/> above example, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">h</sub> will have dimensions of (50x50,000+50) = 50<img src="../Images/B18046_10_051.png" alt="" role="presentation"/>50,050, i.e.,:</p>
    <p class="center"><img src="../Images/B18046_10_052.png" alt="" role="presentation"/></p>
    <h3 id="_idParaDest-370" class="heading-3">Calculating the output for each timestep</h3>
    <p class="normal">In our model, the<a id="_idIndexMarker1074"/> output generated for a given timestep, such as <em class="italic">t1</em>, is denoted by <img src="../Images/B18046_10_053.png" alt="" role="presentation"/>. Since we are employing the softmax function for normalization in our model, the output for any timestep, <em class="italic">tt</em>, can be generalized using the following equation:</p>
    <p class="center"><img src="../Images/B18046_10_054.png" alt="" role="presentation"/></p>
    <p class="normal">Understanding how the output is calculated at each timestep lays the foundation for the subsequent stage of training, where we need to evaluate how well the model is performing.</p>
    <p class="normal">Now that we have a grasp of how the outputs are generated at each timestep, it becomes essential to determine the discrepancy between these predicted outputs and the actual target values. This discrepancy, referred to as “loss,” gives us a measure of the model’s error. In the next section, we will delve into the methods of computing RNN loss, allowing us to gauge the model’s accuracy and make necessary adjustments to the weights and<a id="_idIndexMarker1075"/> biases. This process is vital in training the model to make more accurate predictions, thereby enhancing its overall performance.</p>
    <h4 class="heading-4">Computing RNN loss</h4>
    <p class="normal">As mentioned, the objective of <a id="_idIndexMarker1076"/>training RNNs is to find the right values of three sets of weights (<em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub>, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub>, and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">yh</sub>) and two sets of biases (<em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">h</sub> and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">y</sub>). Initially, at timestep <em class="italic">t1</em>, these values are initialized randomly. </p>
    <p class="normal">As the training process progresses, these values are changed as the gradient descent algorithm kicks in. We need to compute loss at each timestep of the forward propagation in RNNs. Let us break down the process of computing the loss:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Compute loss for individual timestep</strong>:
    <p class="normal">At timestep <em class="italic">t1</em>, the predicted output is <img src="../Images/B18046_10_055.png" alt="" role="presentation"/>. The expected output is <img src="../Images/B18046_10_056.png" alt="" role="presentation"/>. The actual loss function used will depend on the type of model we are training. For example, if we are training a classifier, then this loss at timestep <em class="italic">t1</em> will be:</p>
    <p class="center"><img src="../Images/B18046_10_057.png" alt="" role="presentation"/></p></li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="2"><strong class="keyWord">Aggregate loss for complete sequence</strong>:
    <p class="normal">For a complete sequence consisting of multiple timesteps, we will compute the individual losses for each of the timesteps, {<em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">1</sub>,<em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">2</sub>,…<em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">T</sub>). The loss for one sequence with <em class="italic">T</em> timesteps will be the aggregate of the loss of each timestep, as calculated by the following equation:</p>
    <p class="center"><img src="../Images/B18046_10_058.png" alt="" role="presentation"/></p></li>
    </ol>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="3"><strong class="keyWord">Compute loss for multiple sequences in a batch</strong>:
    <p class="normal">If there is more than one sequence in the batch, then, first, the loss is calculated for each individual sequence. We then compute the cost across all the sequences in a particular batch and use it for backpropagation.</p>
    <p class="normal">By calculating the loss in this structured manner, we guide the model in adjusting its weights and biases to better align with the desired output. This iterative process, repeated over <a id="_idIndexMarker1077"/>many batches and epochs, allows the model to learn from the data and make more accurate predictions.</p></li>
    </ol>
    <h2 id="_idParaDest-371" class="heading-2">Backpropagation through time</h2>
    <p class="normal">Backpropagation, a<a id="_idIndexMarker1078"/>s explained in <em class="chapterRef">Chapter 8</em>, is used in neural networks to progressively learn from the examples of training datasets. RNNs add another dimension to the training data, that is, the timesteps. <strong class="keyWord">Backpropagation through time</strong> (<strong class="keyWord">BPTT</strong>) is designed<a id="_idIndexMarker1079"/> to handle the sequential data as the training process is going through the timesteps. </p>
    <p class="normal">Backpropagation is triggered when the forward feed process calculates the loss of the last timestep of a batch. We then apply this derivative to adjust the weights and biases for the RNN model. RNNs have three sets of weights, <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hh</sub> ,<em class="italic"> W</em><sub class="subscript-italic" style="font-style: italic;">hx</sub> and <em class="italic">W</em><sub class="subscript-italic" style="font-style: italic;">hy</sub>, and two sets of biases (<em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">h</sub> and <em class="italic">b</em><sub class="subscript-italic" style="font-style: italic;">y</sub>). Once the weights and biases are adjusted, we will continue with gradient descent for model training.</p>
    <div class="note">
      <p class="normal">The name of this section, <em class="italic">Backpropagation through time</em>, does not hint toward any time machine that takes us back to some medieval era. Instead, it stems from the fact that once the cost has been calculated through forward-feed, it needs to run backward through each of the timesteps and update weights and biases.</p>
    </div>
    <p class="normal">The backpropagation process is crucial for tuning the model’s parameters, but once the model is trained, what’s next? After we’ve used backpropagation to minimize the loss, we have a model that’s ready to make predictions. In the next section, we’ll explore how to use the trained RNN model to make predictions on new data. We’ll find that predicting with RNNs is similar to the process used with fully connected neural networks, where the input data is processed by the trained RNN to produce the predictions. This <a id="_idIndexMarker1080"/>shift from training to prediction <a id="_idIndexMarker1081"/>forms a natural progression in understanding how RNNs can be applied to real-world problems.</p>
    <h3 id="_idParaDest-372" class="heading-3">Predicting with RNNs</h3>
    <p class="normal">Once the model is<a id="_idIndexMarker1082"/> trained, predicting with RNNs is similar to with fully connected neural networks. The input data is given as input to the trained RNN model and predictions are obtained. Here’s how it functions:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Input preparation</strong>: Just like in a standard neural network, you begin by preparing the input data. In the case of an RNN, this input data is typically sequential, representing timesteps in a process or series.</li>
      <li class="numberedList"><strong class="keyWord">Model utilization</strong>: You then feed this input data into the trained RNN model. The model’s learned weights and biases, optimized during the training phase, are used to process the input through each layer of the network. In an RNN, this includes passing the data through the recurrent connections that handle the sequential aspects of the data.</li>
      <li class="numberedList"><strong class="keyWord">Activation functions</strong>: As in other neural networks, activation functions within the RNN transform the data as it moves through the layers. Depending on the specific design of the RNN, different activation functions might be used at different stages.</li>
      <li class="numberedList"><strong class="keyWord">Generating predictions</strong>: The penultimate step is generating the predictions. The output of the RNN is processed through a final layer, often using a softmax activation function for classification tasks, to produce the final prediction for each input sequence.</li>
      <li class="numberedList"><strong class="keyWord">Interpretation</strong>: The predictions are then interpreted based on the specific task at hand. This could be classifying a sequence of text, predicting the next value in a time series, or any other task that relies on sequential data.</li>
    </ol>
    <p class="normal">Thus, predicting with an RNN follows a process similar to that of fully connected neural networks, with the main distinction being the handling of sequential data. The RNN’s ability to capture temporal relationships within the data allows it to provide unique insights and <a id="_idIndexMarker1083"/>predictions that other neural network architectures might struggle with.</p>
    <h2 id="_idParaDest-373" class="heading-2">Limitations of basic RNNs</h2>
    <p class="normal">Earlier in the<a id="_idIndexMarker1084"/> chapter, we introduced basic RNNs. Sometimes we refer to basic RNNs as “plain vanilla” RNNs. This term refers to their fundamental, unadorned structure. While they serve as a solid introduction to recurrent neural networks, these basic RNNs do have notable limitations:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Vanishing gradient problem</strong>: This issue makes it challenging for the RNN to learn and retain long-term dependencies in the data.</li>
      <li class="numberedList"><strong class="keyWord">Inability to look ahead in the sequence</strong>: Traditional RNNs process sequences from the beginning to the end, which limits their capability to understand the future context in a sequence.</li>
    </ol>
    <p class="normal">Let us investigate them one by one.</p>
    <h3 id="_idParaDest-374" class="heading-3">Vanishing gradient problem</h3>
    <p class="normal">RNNs iteratively process the input data one timestep at a time. This means that as the input sequences become longer, RNNs find it hard to capture long-term dependencies. Long-term dependencies refer to relationships between elements in a sequence that are far apart from each other. Imagine analyzing a lengthy piece of text, such as a novel. If a character’s actions in the first chapter influence events in the last chapter, that’s a long-term dependency. The information from the beginning of the text has to be “remembered” all the way to the end for full understanding.</p>
    <p class="normal">RNNs often struggle with such long-range connections. The hidden state mechanism of RNNs, designed to retain information from previous timesteps, can be too simplistic to capture these intricate relationships. As the distance between related elements grows, the RNN may lose track of the connection. There is not much intelligence on when and what to keep in memory and when and what to forget.</p>
    <p class="normal">For many use cases in sequential data, only the most recent information is important. For example, consider a predictive text application trying to assist a person typing an email by suggesting the next word to type. </p>
    <p class="normal">As we know, such functionality is now standard in modern word processors. If the user is typing:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_08.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 10.8: Predictive text example</p>
    <p class="normal">the predictive text application can easily suggest the next word “hard”. It does not need to bring the context from the prior sentences to predict the next word. For such applications, where long-term memory is not required, RNNs are the best choice. RNNs will not over-complicate the architecture without compromising on accuracy.</p>
    <p class="normal">But for other applications, keeping the long-term dependencies is important. RNNs struggle with managing long-term dependencies. Let us look at an example:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_09.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 10.9: Predictive text example with a long-term dependency</p>
    <p class="normal">As we read this sentence from left to right, we can observe that “was” (used later in the sentence) is referring to the “man.” RNNs in their original form will struggle to carry the hidden state forward for multiple timesteps. The reason is that, in RNNs, the hidden state is calculated for each timestep and carried forward.</p>
    <p class="normal">Due to the recursive nature of this operation, we are always concerned about the signal prematurely fading while progressing from element to element in different timesteps. This behavior of RNNs is identified as the vanishing gradient problem. To combat this vanishing gradient <a id="_idIndexMarker1085"/>problem, we prefer to choose tanh as the activation function. As the second derivative of tanh decays very slowly to zero, the choice of tanh helps manage the vanishing gradient problem to some extent. But we need more sophisticated architecture, like GRUs and LSTM, to better manage the vanishing gradient problem, which we will discuss in the next section.</p>
    <h3 id="_idParaDest-375" class="heading-3">Inability to look ahead in the sequence</h3>
    <p class="normal">RNNs can be categorized based on the direction of information flow through the sequence. The two primary types are unidirectional RNNs and bidirectional RNNs.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Unidirectional RNNs</strong>: These<a id="_idIndexMarker1086"/> networks process the input data in one direction, usually from the beginning of the sequence to the end. They carry the context forward, building understanding step by step as they iterate through the elements of a sequence, such as words in a sentence. Here’s the limitation: unidirectional RNNs cannot “look ahead” in the sequence. 
    <p class="normal">They only have access to the information they’ve seen so far, meaning they can’t incorporate future elements to build a more accurate or nuanced context. Imagine reading a <a id="_idIndexMarker1087"/>complex sentence one word at a time, without being able to glance ahead and see what’s coming. You might miss subtleties or misunderstand the overall meaning.</p></li>
    </ul>
    <ul>
      <li class="bulletList"><strong class="keyWord">Bidirectional RNNs</strong>: In contrast, bidirectional RNNs process the sequence in both directions <a id="_idIndexMarker1088"/>simultaneously. They combine insights from both the past and the future elements, allowing for a richer understanding of the context.</li>
    </ul>
    <p class="normal">Let us consider the following two sentences:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_10.png" alt="" role="presentation"/> </figure>
    <p class="packt_figref">Figure 10.10: Examples where an RNN must look ahead in the sentence</p>
    <p class="normal">Both of these sentences use the word “cricket.” If the context is built only from left to right, as done in unidirectional RNNs, we cannot contextualize “cricket” properly as its relevant<a id="_idIndexMarker1089"/> information will be in a future timestep. To solve this problem, we will look into bidirectional RNNs, which are discussed in <em class="chapterRef">Chapter 11</em><em class="chapterRef">.11</em></p>
    <p class="normal">Now let us study GRUs and their detailed working and architecture.</p>
    <h1 id="_idParaDest-376" class="heading-1">GRU</h1>
    <p class="normal">GRUs represent an <a id="_idIndexMarker1090"/>evolution of the basic RNN structure, specifically designed to address some of the challenges encountered with traditional RNNs, such as the vanishing gradient problem. The architecture of a GRU is illustrated in <em class="italic">Figure 10.8</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_11.png" alt="Diagram, schematic  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.11: GRU</p>
    <p class="normal">Let us start discussing GRU with the first activation function, annotated as <strong class="keyWord">A</strong>. At each timestep t, GRU first calculates the hidden state using the tanh activation function and utilizing <img src="../Images/B18046_10_059.png" alt="" role="presentation"/> and <img src="../Images/B18046_10_060.png" alt="" role="presentation"/> as inputs. The calculation is no different than how the hidden state is determined in<a id="_idIndexMarker1091"/> the original RNNs presented in the previous section. But there is an important difference. The output is a <em class="italic">candidate</em> hidden state, which is calculated using <em class="italic">Eq. 10.6</em>:</p>
    <p class="center"><img src="../Images/B18046_10_061.png" alt="" role="presentation"/></p>
    <p class="normal">where <img src="../Images/B18046_10_062.png" alt="" role="presentation"/> is the candidate value of the hidden layer.</p>
    <p class="normal">Now, instead of using the candidate hidden state straight away, the GRU takes a moment to decide whether to use it. Imagine it like someone pausing to think before making a decision. This pause-and-think step is what we call<a id="_idIndexMarker1092"/> the <strong class="keyWord">gating mechanism</strong>. It checks out the information and then selects what details to remember and what to forget for the next step. It’s kind of like filtering out the noise and focusing on the important stuff. By blending the old information (from the previous hidden state) and the new draft (the candidate), GRUs are better at following long stories or sequences without getting lost. By introducing a candidate hidden state, GRUs bring an added layer of flexibility. They can judiciously decide the portion of the candidate state to incorporate. This distinction equips GRUs to adeptly tackle challenges, such as the vanishing gradient, with a finesse that traditional RNNs often lack. In simpler terms, while the classic RNNs might struggle to remember long stories, GRUs, with their special features, are better<a id="_idIndexMarker1093"/> listeners and retainers.</p>
    <div class="note">
      <p class="normal">LSTM was proposed in 1997 and GRUs in 2014. Most books on this topic prefer the chronological order and present LSTMs first. I have chosen to present these algorithms ordered by complexity. As the motivation behind GRUs was to simplify LSTMs, it may be useful to study the simpler algorithm first.</p>
    </div>
    <h2 id="_idParaDest-377" class="heading-2">Introducing the update gate</h2>
    <p class="normal">In a standard <a id="_idIndexMarker1094"/>RNN, the <a id="_idIndexMarker1095"/>hidden value at each timestep is calculated and automatically becomes the new state of the memory cell. In contrast, GRUs introduce a more nuanced approach. The GRU model brings more flexibility to the process by allowing control over when to update the state of the memory cell. This added flexibility is implemented through a mechanism called the “update gate,” sometimes referred to as the “reset gate.”</p>
    <p class="normal">The update gate’s function is to evaluate whether the information in the candidate hidden state, <img src="../Images/B18046_10_063.png" alt="" role="presentation"/>, is significant enough to update the memory cell’s hidden state or if the memory cell should retain the old hidden value from previous timesteps.</p>
    <p class="normal">In mathematical terms, this decision-making process helps the model to manage information more selectively, determining whether to integrate new insights or continue relying on previously acquired knowledge. If the model deems that the candidate hidden state’s information is not significant enough to alter the memory cell’s existing state, the previous hidden value will be retained. Conversely, if the new information is considered relevant, it can overwrite the memory cell’s state, thus adjusting the model’s internal representation as it processes the sequence.</p>
    <p class="normal">This unique gating mechanism sets GRUs apart from traditional RNNs and allows for more effective learning<a id="_idIndexMarker1096"/> from sequential data with complex temporal <a id="_idIndexMarker1097"/>relationships.</p>
    <h2 id="_idParaDest-378" class="heading-2">Implementing the update gate</h2>
    <p class="normal">This intelligence that <a id="_idIndexMarker1098"/>we add to how the state is updated in the <a id="_idIndexMarker1099"/>memory cell is the defining feature of a GRU. The decision will be taken soon of whether we should update the current hidden state with the candidate hidden state. To make this decision, we use the second activation function shown in <em class="italic">Figure 10.</em><em class="chapterRef">11</em>, annotated as <strong class="keyWord">B</strong>. This activation function is implementing the update gate.</p>
    <p class="normal">It is implemented as a sigmoid layer that takes as input the current input and the previous hidden state. The output of the sigmoid layer is a value between 0 and 1 represented by the variable <img src="../Images/B18046_10_064.png" alt="" role="presentation"/> The output of the update gate is the variable <img src="../Images/B18046_10_065.png" alt="" role="presentation"/>, which is governed by the following sigmoid function:</p>
    <p class="center"><img src="../Images/B18046_10_066.png" alt="" role="presentation"/></p>
    <p class="normal">As <img src="../Images/B18046_10_067.png" alt="" role="presentation"/> is the output of a sigmoid function, it is close to either 1 or 0, which determines whether the update gate is open or closed. If the update gate is open, <img src="../Images/B18046_10_068.png" alt="" role="presentation"/> will be chosen as the new hidden<code class="inlineCode"> </code>state. In the training process, the GRU will learn when to open the gate and when to close it.</p>
    <h2 id="_idParaDest-379" class="heading-2">Updating the hidden cell</h2>
    <p class="normal">For a certain <a id="_idIndexMarker1100"/>timestep, the <a id="_idIndexMarker1101"/>next hidden state is determined using the calculation from the following equation:</p>
    <p class="center"><img src="../Images/B18046_10_069.png" alt="" role="presentation"/></p>
    <p class="normal"><em class="italic">Eq. 10.8</em> consists of two terms, annotated as <strong class="keyWord">1</strong> and <strong class="keyWord">2</strong>. Being an output of a sigmoid function, <img src="../Images/B18046_10_070.png" alt="" role="presentation"/> can either be 0 or 1. It means:</p>
    <p class="center"><img src="../Images/B18046_10_071.png" alt="" role="presentation"/></p>
    <p class="center"><img src="../Images/B18046_10_072.png" alt="" role="presentation"/></p>
    <p class="normal">In other words, if <a id="_idIndexMarker1102"/>the <a id="_idIndexMarker1103"/>gate is open, update the value of <img src="../Images/B18046_10_073.png" alt="" role="presentation"/>. Otherwise, just retain the old state.</p>
    <p class="normal">Let us now look into how we can run GRUs for multiple timesteps.</p>
    <h3 id="_idParaDest-380" class="heading-3">Running GRUs for multiple timesteps</h3>
    <p class="normal">When deploying<a id="_idIndexMarker1104"/> GRUs across several timesteps, we can visualize this process as depicted in <em class="italic">Figure 10.12</em>. Much like the foundational RNNs we discussed in the prior segment, GRUs create what can be thought of as an “information highway.” This pathway effectively transfers context from the beginning to the end of a sequence, visualized as <img src="../Images/B18046_10_074.png" alt="" role="presentation"/> in <em class="italic">Figure 10.12 </em>and annotated as <strong class="keyWord">A</strong>.</p>
    <p class="normal">What differentiates GRUs from traditional RNNs is the decision-making process about how information flows on this highway. Instead of transferring information blindly at each timestep, a GRU pauses to evaluate its relevance.</p>
    <p class="normal">Let’s illustrate this with a basic example. Imagine reading a book where each sentence is a piece of information. However, instead of remembering every detail about every sentence, your mind (acting like a GRU) selectively recalls the most impactful or emotional sentences. This selective memory is akin to how the update gate in a GRU works.</p>
    <p class="normal">The update gate serves a crucial role here. It’s a mechanism that determines which portions of the prior information, or the prior “hidden state,” should be retained or discarded. Essentially, the gate helps the network zoom in on and retain the most pertinent details, ensuring<a id="_idIndexMarker1105"/> that the carried context remains as relevant as possible.</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_12.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.12: Sequential processing in RNN</p>
    <h1 id="_idParaDest-381" class="heading-1">Introducing LSTM</h1>
    <p class="normal">RNNs are widely <a id="_idIndexMarker1106"/>used for sequence modeling tasks, but they suffer from limitations in capturing long-term dependencies in the data. An advanced version of RNNs, known as LSTM, was developed to address these limitations. Unlike simple RNNs, LSTMs have a more complex mechanism to manage context, enabling them to better capture patterns in sequences.</p>
    <p class="normal">In the previous section, we discussed GRUs, where hidden state <img src="../Images/B18046_10_006.png" alt="" role="presentation"/> is used to carry the context from timestep to timestep. LSTM has a much more complex mechanism for managing the context. It has two variables that carry the context from timestep to timestep: the cell state and the hidden state. They are explained as follows:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">The cell state</strong> (represented as <img src="../Images/B18046_10_076.png" alt="" role="presentation"/>): This is responsible for maintaining the long-term dependencies<a id="_idIndexMarker1107"/> of the input data. It is passed from one timestep to the next and is used to maintain information across a longer period. As we will learn later in this section, it is carefully determined by the forget gate and the update gate what should be included in the cell state. It can be considered as the “persistence layer” or “memory” of the LSTM as it maintains the information over a long period of time.</li>
      <li class="numberedList"><strong class="keyWord">The hidden state</strong> (represented as <img src="../Images/B18046_10_077.png" alt="" role="presentation"/>): This context is focused on the current<a id="_idIndexMarker1108"/> timestep, which may or may not be important for the long-term dependencies. It is the output of the LSTM unit for a particular timestep and is passed as input to the next time step. As indicated in <em class="italic">Figure 10.23</em>, the hidden state, <img src="../Images/B18046_10_078.png" alt="" role="presentation"/>, is used to generate the output <img src="../Images/B18046_10_079.png" alt="" role="presentation"/>at timestep <em class="italic">t</em>.</li>
    </ol>
    <p class="normal">Let us now study <a id="_idIndexMarker1109"/>these mechanisms in more detail, starting with how the current cell state is updated.</p>
    <h2 id="_idParaDest-382" class="heading-2">Introducing the forget gate</h2>
    <p class="normal">The forget gate in an <a id="_idIndexMarker1110"/>LSTM network is responsible for determining which<a id="_idIndexMarker1111"/> information to discard from the previous state, and which information to keep. It is annotated as <strong class="keyWord">A</strong> in <em class="italic">Figure 10.3</em>. It is implemented as a sigmoid layer that takes as input the current input and the previous hidden state. The output of the sigmoid layer is a vector of values between 0 and 1, where each value corresponds to a single cell in the LSTM’s memory.</p>
    <p class="center"><img src="../Images/B18046_10_080.png" alt="" role="presentation"/></p>
    <p class="normal">As it is a sigmoid function, it means that <img src="../Images/B18046_10_081.png" alt="" role="presentation"/> can be either close to 0 or 1.</p>
    <p class="normal">If <img src="../Images/B18046_10_082.png" alt="" role="presentation"/> is 1, then it means that the value from the previous state <img src="../Images/B18046_10_083.png" alt="" role="presentation"/> should be used to calculate <img src="../Images/B18046_10_084.png" alt="" role="presentation"/>. If <img src="../Images/B18046_10_085.png" alt="" role="presentation"/> is 0, then<a id="_idIndexMarker1112"/> it means that the value from the previous state <img src="../Images/B18046_10_086.png" alt="" role="presentation"/>should <a id="_idIndexMarker1113"/>be forgotten.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Info</strong>: Usually, binary variables are considered active when their logic is 1. It may feel counter-intuitive that the “forget gate” forgets the previous state when <img src="../Images/B18046_10_087.png" alt="" role="presentation"/> = 0, but this is how logic was presented in the original paper and is followed by the researchers for consistency.</p>
    </div>
    <figure class="mediaobject"><img src="../Images/B18046_10_13.png" alt="Diagram, schematic  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.13: LSTM architecture</p>
    <h2 id="_idParaDest-383" class="heading-2">The candidate cell state</h2>
    <p class="normal">In LSTM, at <a id="_idIndexMarker1114"/>each<a id="_idIndexMarker1115"/> timestep, a candidate cell state, <img src="../Images/B18046_10_088.png" alt="" role="presentation"/>, is calculated, which is annotated as <strong class="keyWord">Y</strong> in <em class="italic">Figure 10.13</em>, and is the proposed new state for the memory cell. It is calculated using the current input <img src="../Images/B18046_10_025.png" alt="" role="presentation"/> and the previous hidden state <img src="../Images/B18046_10_090.png" alt="" role="presentation"/> as follows:</p>
    <p class="center"><img src="../Images/B18046_10_091.png" alt="" role="presentation"/></p>
    <h2 id="_idParaDest-384" class="heading-2">The update gate</h2>
    <p class="normal">The update <a id="_idIndexMarker1116"/>gate is <a id="_idIndexMarker1117"/>also called the input gate. The update gate in LSTM networks is a mechanism that allows the network to selectively incorporate new information into the current state so that the memory can focus on the most relevant information. It is annotated as <strong class="keyWord">B</strong> in <em class="italic">Figure 10.13</em>.</p>
    <p class="normal">It is responsible for determining whether the candidate cell state <img src="../Images/B18046_10_088.png" alt="" role="presentation"/>should be added to <img src="../Images/B18046_10_093.png" alt="" role="presentation"/>. It is implemented as a sigmoid layer that takes as input the current input <img src="../Images/B18046_10_025.png" alt="" role="presentation"/> and the previous hidden state:</p>
    <p class="center"><img src="../Images/B18046_10_095.png" alt="" role="presentation"/></p>
    <p class="normal">The output of the sigmoid layer, <img src="../Images/B18046_10_096.png" alt="" role="presentation"/>, is a vector of values between 0 and 1, where each value corresponds to a single cell in the LSTM’s memory. A value of 0 indicates that the calculated <img src="../Images/B18046_10_097.png" alt="" role="presentation"/> should be ignored, while a value of 1 indicates that <img src="../Images/B18046_10_098.png" alt="" role="presentation"/> is significant enough to be incorporated in <img src="../Images/B18046_10_099.png" alt="" role="presentation"/>. Being a sigmoid function, it can have any value between 0 and 1, which indicates that some of the information from <img src="../Images/B18046_10_097.png" alt="" role="presentation"/> should be incorporated in <img src="../Images/B18046_10_093.png" alt="" role="presentation"/>, but not all.</p>
    <p class="normal">The update gate allows the LSTM to selectively incorporate new information into the current state and prevent the memory from becoming flooded with irrelevant data. By controlling the amount of new information that is added to the memory state, the update gate<a id="_idIndexMarker1118"/> helps <a id="_idIndexMarker1119"/>the LSTM to maintain a balance between preserving the previous state and incorporating new information.</p>
    <h2 id="_idParaDest-385" class="heading-2">Calculating memory state</h2>
    <p class="normal">As compared to <a id="_idIndexMarker1120"/>GRU, the main difference in LSTM is that <a id="_idIndexMarker1121"/>instead of having a single update gate (as we have in GRU), we have separate gates for the update and forget mechanisms for hidden state management. Each gate determines what is the right mix of various states to optimally calculate both the long-term memory <img src="../Images/B18046_10_093.png" alt="" role="presentation"/> current cell state and the current hidden state, <img src="../Images/B18046_10_078.png" alt="" role="presentation"/>. The memory state is calculated by:</p>
    <p class="center"><img src="../Images/B18046_10_104.png" alt="" role="presentation"/></p>
    <p class="normal"><em class="italic">Eq. 10.12</em> consists of two terms annotated as <strong class="keyWord">1</strong> and <strong class="keyWord">2</strong>. Being an output of a sigmoid function, <img src="../Images/B18046_10_105.png" alt="" role="presentation"/>and <img src="../Images/B18046_10_106.png" alt="" role="presentation"/> can either be 0 or 1. It means:</p>
    <p class="center"><img src="../Images/B18046_10_071.png" alt="" role="presentation"/></p>
    <p class="center"><img src="../Images/B18046_10_072.png" alt="" role="presentation"/></p>
    <p class="normal">In other words, if the gate is open, update the value of <img src="../Images/B18046_10_037.png" alt="" role="presentation"/>. Otherwise, just retain the old state.</p>
    <p class="normal">Thus, the update gate in a GRU is a mechanism that allows the network to selectively discard information from the previous hidden state so that the hidden state can focus on the most <a id="_idIndexMarker1122"/>relevant information. This is shown in <em class="italic">Figure 10.13</em>, which <a id="_idIndexMarker1123"/>shows how the state travels from left to right.</p>
    <h2 id="_idParaDest-386" class="heading-2">The output gate</h2>
    <p class="normal">The output gate in <a id="_idIndexMarker1124"/>an LSTM network is annotated as <strong class="keyWord">C</strong> in <em class="italic">Figure 10.13</em>. It is<a id="_idIndexMarker1125"/> responsible for determining which information from the current memory state should be passed on as the output of the LSTM. It is implemented as a sigmoid layer that takes as input the current input and the previous hidden state. The output of the sigmoid layer is a vector of values between 0 and 1, where each value corresponds to a single cell in the LSTM’s memory.</p>
    <p class="normal">As it is a sigmoid function, it means that <img src="../Images/B18046_10_110.png" alt="" role="presentation"/> can be either close to 0 or 1.</p>
    <p class="normal">If <img src="../Images/B18046_10_111.png" alt="" role="presentation"/> is 1, then it means that the value from the previous state <img src="../Images/B18046_10_099.png" alt="" role="presentation"/> should be used to calculate. <img src="../Images/B18046_10_076.png" alt="" role="presentation"/> If <img src="../Images/B18046_10_081.png" alt="" role="presentation"/> is 0, then it means that the value from the previous state <img src="../Images/B18046_10_099.png" alt="" role="presentation"/>should be forgotten.</p>
    <p class="center"><img src="../Images/B18046_10_116.png" alt="" role="presentation"/></p>
    <p class="normal">A value of 0 indicates that the corresponding cell should not contribute to the output, while a value of 1 indicates that the cell should fully contribute to the output. Values between 0 and 1 indicate that the cell should contribute some, but not all, of its value to the output.</p>
    <p class="normal">In LSTMs, after processing the output gate, the current state is passed through a <code class="inlineCode">tanh</code> function. This function adjusts the values such that they fall within a range between -1 and 1. Why is this scaling necessary? The <code class="inlineCode">tanh</code> function ensures that the LSTM’s output remains normalized and prevents values from becoming too large, which can be problematic during training due to potential issues like exploding gradients.</p>
    <p class="normal">After scaling, the result from the output gate is multiplied by this normalized state. This combined value represents the final output of the LSTM at that specific timestep.</p>
    <p class="normal">To provide a simple analogy: imagine adjusting the volume of your music so it’s neither too loud nor too soft, but just right for your environment. The <code class="inlineCode">tanh</code> function acts similarly, ensuring the output is optimized and suitable for further processing.</p>
    <p class="normal">The output gate is important because it allows the LSTM to selectively pass on relevant information<a id="_idIndexMarker1126"/> from<a id="_idIndexMarker1127"/> the current memory state as the output. It also helps to prevent irrelevant information from being passed on as the output.</p>
    <p class="normal">This output gate generates the variable <img src="../Images/B18046_10_117.png" alt="" role="presentation"/>which determines that the contribution of the cell state is output to the hidden state:</p>
    <p class="center"><img src="../Images/B18046_10_118.png" alt="" role="presentation"/></p>
    <p class="normal">In LSTM, <img src="../Images/B18046_10_119.png" alt="" role="presentation"/> is used as input to the gates, whereas <img src="../Images/B18046_10_093.png" alt="" role="presentation"/> is the hidden state.</p>
    <p class="normal">In summary, the output gate in LSTM networks is a mechanism that allows the network to selectively pass on relevant information from the current memory state as the output so that the LSTM can generate appropriate output based on the relevant information it has <a id="_idIndexMarker1128"/>stored in<a id="_idIndexMarker1129"/> its memory.</p>
    <h2 id="_idParaDest-387" class="heading-2">Putting everything together</h2>
    <p class="normal">Let’s delve into the <a id="_idIndexMarker1130"/>workings of the LSTM across multiple timesteps, as depicted by <strong class="keyWord">A</strong> in <em class="italic">Figure 10.14</em>.</p>
    <p class="normal">Just like GRUs, LSTMs create a conduit – often referred to as an “information highway” – which helps ferry context across successive timesteps. This is illustrated in <em class="italic">Figure 10.14</em>. What’s fascinating about LSTMs is their ability to use long-term memory to transport this context.</p>
    <p class="normal">As we traverse from one timestep to the next, the LSTM learns what should be retained in its long-term memory, denoted as <img src="../Images/B18046_10_076.png" alt="" role="presentation"/>. At the start of every timestep, <img src="../Images/B18046_10_093.png" alt="" role="presentation"/> interacts with the “forget gate,” allowing some pieces of information to be discarded. Subsequently, it encounters the “update gate,” where new data is infused. This allows <img src="../Images/B18046_10_093.png" alt="" role="presentation"/> to transition between timesteps, continually gaining and shedding information as dictated by the two gates.</p>
    <p class="normal">Now, here’s where it gets intricate. At the close of every timestep, a copy of the long-term memory, <img src="../Images/B18046_10_093.png" alt="" role="presentation"/>, undergoes transformation via the tanh function. This processed data is then sieved by the output gate, culminating in what we term short-term memory, <img src="../Images/B18046_10_077.png" alt="" role="presentation"/>. This short-term memory serves a dual purpose: it determines the output at that specific timestep and lays the foundation for the subsequent timestep, as portrayed in <em class="italic">Figure 10.14</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18046_10_14.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 10.14: LSTM with multiple timesteps</p>
    <p class="normal">Let us now<a id="_idIndexMarker1131"/> look into how we can code RNNs.</p>
    <h2 id="_idParaDest-388" class="heading-2">Coding sequential models</h2>
    <p class="normal">For our exploration<a id="_idIndexMarker1132"/> into LSTM, we’ll be diving into sentiment <a id="_idIndexMarker1133"/>analysis using the well-known IMDb movie reviews dataset. Here, every review is tagged with a sentiment, positive or negative, encoded as binary values (<code class="inlineCode">True</code> for positive, and <code class="inlineCode">False</code> for negative). Our aim is to craft a binary classifier capable of predicting these sentiments based solely on the text content of the review.</p>
    <p class="normal">In total, the dataset boasts 50,000 movie reviews. For our purposes, we’ll be dividing this equally: 25,000 reviews for training our model, and the remaining 25,000 for evaluating its performance.</p>
    <p class="normal">For those seeking a deeper dive into the dataset, more information is available at Stanford’s IMDB Dataset.</p>
    <h3 id="_idParaDest-389" class="heading-3">Loading the dataset</h3>
    <p class="normal">First, we need to<a id="_idIndexMarker1134"/> load the dataset. We will import this dataset through <code class="inlineCode">keras.datasets</code>. The advantage of importing this dataset through <code class="inlineCode">keras.datasets</code> is that it has been processed to be used for machine learning. For example, the reviews have been individually encoded as a list of word indexes. The overall frequency of a particular word has been chosen as the index. So, if the index of the word is “7,” it means that it is the 7<sup class="superscript">th</sup> most frequent word. The use of pre-prepared data allows us to focus on the RNN algorithm instead of data preparation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.datasets <span class="hljs-keyword">import</span> imdb
vocab_size = <span class="hljs-number">50000</span>
(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.imdb.load_data(num_words= vocab_size)
</code></pre>
    <p class="normal">Note that the argument <code class="inlineCode">num_words=50000</code> is used to select only the top 50000 words. As the frequency of a word is used as the index, it means all the words with indexes less than 50000 are filtered out:</p>
    <pre class="programlisting code"><code class="hljs-code">"I watched the movie in a cinema and I really like it" 
[13, 296, 4, 20, 11, 6, 4435, 5, 13, 66, 447,12]
</code></pre>
    <p class="normal">When working with sequences of varying lengths, it’s often beneficial to ensure that they all have a uniform length. This is particularly crucial when feeding them into neural networks, which often expect consistent input sizes. To achieve this, we use padding—adding zeros at the beginning or end of sequences until they reach a specified length. </p>
    <p class="normal">Here’s how you can implement this with TensorFlow: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Pad the sequences</span>
max_review_length = <span class="hljs-number">500</span>
x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)
</code></pre>
    <p class="normal">Indexes are great for the consumption of algorithms. For human readability, we can convert these indexes back to words:</p>
    <pre class="programlisting code"><code class="hljs-code">word_index = tf.keras.datasets.imdb.get_word_index()
reverse_word_index = <span class="hljs-built_in">dict</span>([(value, key) <span class="hljs-keyword">for</span> (key, value) <span class="hljs-keyword">in</span> word_index.items()])
<span class="hljs-keyword">def</span> <span class="hljs-title">decode_review</span>(<span class="hljs-params">padded_sequence</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">" "</span>.join([reverse_word_index.get(i - <span class="hljs-number">3</span>, <span class="hljs-string">"</span><span class="hljs-string">?"</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> padded_sequence])
</code></pre>
    <p class="normal">Note that word<a id="_idIndexMarker1135"/> indexes start from 3 instead of 0 or 1. The reason is that the first three indexes are reserved.</p>
    <p class="normal">Next, let us look into how we can prepare the data.</p>
    <h3 id="_idParaDest-390" class="heading-3">Preparing the data</h3>
    <p class="normal">In our example, we are <a id="_idIndexMarker1136"/>considering a vocabulary of 50,000 words. This means that each word in the input sequence <img src="../Images/B18046_10_025.png" alt="" role="presentation"/> will be encoded using a one-hot vector representation, where the dimension of each vector is 50,000. A one-hot vector is a binary vector that has 0s in all positions except for the index corresponding to the word, where it has a 1. Here’s we can load the IMDb dataset in TensorFlow, specifying the vocabulary size:</p>
    <pre class="programlisting code"><code class="hljs-code">vocab_size = <span class="hljs-number">50</span><span class="hljs-number">000</span>
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)
</code></pre>
    <p class="normal">Note that as <code class="inlineCode">vocab_size</code> is set to <code class="inlineCode">50,000</code>, so the data will be loaded with the <code class="inlineCode">50,000</code> most frequently occurring words. The remaining words will be discarded or replaced with a special token (often denoted as <code class="inlineCode">&lt;UNK&gt;</code> for “unknown”). This ensures that our input data is manageable and only includes the most relevant information for our model. The<a id="_idIndexMarker1137"/> variables <code class="inlineCode">x_train</code> and <code class="inlineCode">x_test</code> will contain the training and testing input data, respectively, while <code class="inlineCode">y_train</code> and <code class="inlineCode">y_test</code> will contain the corresponding labels.</p>
    <h3 id="_idParaDest-391" class="heading-3">Creating the model</h3>
    <p class="normal">We begin by defining <a id="_idIndexMarker1138"/>an empty stack. We’ll use this for building our network, layer by layer:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.models.Sequential()
</code></pre>
    <p class="normal">Next, we’ll add an <code class="inlineCode">Embedding</code> layer to our model. If you recall our discussion about word embeddings in <em class="chapterRef">Chapter 9</em>, we used them to represent words in a continuous vector space. The <code class="inlineCode">Embedding</code> layer serves a similar purpose but within the neural network. It provides a way to map each word in our vocabulary to a continuous vector. Words that are close to one another in this vector space are likely to share context or meaning. </p>
    <p class="normal">Let us define the <code class="inlineCode">Embedding</code> layer, considering the vocabulary size we chose earlier and mapping each word to a 50-dimensional vector, corresponding to the dimension of <img src="../Images/B18046_10_026.png" alt="" role="presentation"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(
    tf.keras.layers.Embedding(
        input_dim = vocab_size, 
        output_dim = <span class="hljs-number">50</span>, 
        input_length = review_length 
    )
)
</code></pre>
    <p class="normal"><code class="inlineCode">Dropout</code> layers prevents overfitting and force the model to learn multiple representations of the same data by randomly disabling neurons in the learning phase. Let us randomly disable 25% of the neurons to deal with overfitting:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(
    tf.keras.layers.Dropout(
        rate=<span class="hljs-number">0.25</span>
    )
)
</code></pre>
    <p class="normal">Next, we’ll add an LSTM layer, which is a specialized form of RNN. While basic RNNs have issues in learning long-term dependencies, LSTMs are designed to remember such dependencies, making them suitable for our task. This LSTM layer will analyze the sequence of words in the review along with their embeddings, using this information to determine the sentiment of a given review. We’ll use 32 units in this layer:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(
    tf.keras.layers.LSTM(
        units=<span class="hljs-number">32</span> 
    )
)
</code></pre>
    <p class="normal">Add a second <code class="inlineCode">Dropout</code> layer to drop 25% of neurons to reduce overfitting:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(
    tf.keras.layers.Dropout(
        rate=<span class="hljs-number">0.25</span>
    )
)
</code></pre>
    <p class="normal">All LSTM units are <a id="_idIndexMarker1139"/>connected to a single node in the <code class="inlineCode">Dense</code> layer. A sigmoid activation function determines the output from this node – a value between 0 and 1. Closer to 0 indicates a negative review. Closer to 1 indicates a positive review:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(
    tf.keras.layers.Dense(
        units=<span class="hljs-number">1</span>, 
        activation=<span class="hljs-string">'sigmoid'</span> 
    )
)
</code></pre>
    <p class="normal">Now, let us compile the model. We will use <code class="inlineCode">binary_crossentropy</code> as the loss function and <code class="inlineCode">Adam</code> as the optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(
    loss=tf.keras.losses.binary_crossentropy, 
    optimizer=tf.keras.optimizers.Adam(), 
    metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Display <a id="_idIndexMarker1140"/>a summary of the model’s structure:</p>
    <pre class="programlisting code"><code class="hljs-code">model.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">__________________________________________________________________________
Layer (type)               Output Shape               Param #
=========================================================================
embedding (Embedding)      (None, 500, 32)            320000
dropout (Dropout)          (None, 500, 32)            0
lstm (LSTM)                (None, 32)                 8320
dropout_1 (Dropout)        (None, 32)                 0
dense (Dense)              (None, 1)                  33
=========================================================================
Total params: 328,353
Trainable params: 328,353
Non—trainable params: 0
</code></pre>
    <h3 id="_idParaDest-392" class="heading-3">Training the model</h3>
    <p class="normal">We’ll now train the <a id="_idIndexMarker1141"/>LSTM model on our training data. Training the model involves several key components, each of which is described below:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Training Data</strong>: These are the features (reviews) and labels (positive or negative sentiments) that our model will learn from.</li>
      <li class="bulletList"><strong class="keyWord">Batch Size</strong>: This determines the number of samples that will be used in each update of the model parameters. A higher batch size might require more memory.</li>
      <li class="bulletList"><strong class="keyWord">Epochs</strong>: An epoch is a complete iteration over the entire training data. The more epochs, the more times the learning algorithm will work through the entire training dataset.</li>
      <li class="bulletList"><strong class="keyWord">Validation Split</strong>: This fraction of the training data will be set aside for validation and not be used for training. It helps us evaluate how well the model is performing.</li>
      <li class="bulletList"><strong class="keyWord">Verbose</strong>: This parameter controls how much output the model will produce during training. A value of 1 means that progress bars will be displayed:
        <pre class="programlisting code"><code class="hljs-code">history = model.fit(
    x_train, y_train,    <span class="hljs-comment"># Training data</span>
    batch_size=<span class="hljs-number">256</span>,      
    epochs=<span class="hljs-number">3</span>,            
    validation_split=<span class="hljs-number">0.2</span>,
    verbose=<span class="hljs-number">1</span>            
)
</code></pre>
        <pre class="programlisting con"><code class="hljs-con">Epoch 1/3
79/79 [==============================] - 75s 924ms/step - loss: 0.5757 - accuracy: 0.7060 - val_loss: 0.4365 - val_accuracy: 0.8222
Epoch 2/3
79/79 [==============================] - 79s 1s/step - loss: 0.2958 - accuracy: 0.8900 - val_loss: 0.3040 - val_accuracy: 0.8812
Epoch 3/3
79/79 [==============================] - 73s 928ms/step - loss: 0.1739 - accuracy: 0.9437 - val_loss: 0.2768 - val_accuracy: 0.8884
</code></pre>
      </li>
    </ul>
    <h3 id="_idParaDest-393" class="heading-3">Viewing some incorrect predictions</h3>
    <p class="normal">Let’s have a look <a id="_idIndexMarker1142"/>at some of the incorrectly classified reviews:</p>
    <pre class="programlisting code"><code class="hljs-code">predicted_probs = model.predict(x_test)
predicted_classes_reshaped = (predicted_probs &gt; <span class="hljs-number">0.5</span>).astype(<span class="hljs-string">"int32"</span>).reshape(-<span class="hljs-number">1</span>)
incorrect = np.nonzero(predicted_classes_reshaped != y_test)[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">We select the first 20 incorrectly classified reviews:</p>
    <pre class="programlisting code"><code class="hljs-code">class_names = [<span class="hljs-string">"</span><span class="hljs-string">Negative"</span>, <span class="hljs-string">"Positive"</span>]
<span class="hljs-keyword">for</span> j, incorrect_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(incorrect[<span class="hljs-number">0</span>:<span class="hljs-number">20</span>]):
    predicted = class_names[predicted_classes_reshaped[incorrect_index]]
    actual = class_names[y_test[incorrect_index]]
    human_readable_review = decode_review(x_test[incorrect_index])
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Incorrectly classified Test Review [</span><span class="hljs-subst">{j+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">]"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Test Review #</span><span class="hljs-subst">{incorrect_index}</span><span class="hljs-string">: Predicted [</span><span class="hljs-subst">{predicted}</span><span class="hljs-string">] Actual [</span><span class="hljs-subst">{actual}</span><span class="hljs-string">]"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Test Review Text: </span><span class="hljs-subst">{human_readable_review.replace(</span><span class="hljs-string">'&lt;PAD&gt; '</span><span class="hljs-subst">, </span><span class="hljs-string">''</span><span class="hljs-subst">)}</span><span class="hljs-string">\n"</span>)
</code></pre>
    <h1 id="_idParaDest-394" class="heading-1">Summary</h1>
    <p class="normal">The foundational concepts of sequential models were explained in this chapter, which aimed to give you a basic understanding of the techniques and methodologies of such techniques. In this chapter, we presented RNNs, which are great for handling sequential data. A GRU is a type of RNN that was introduced by Cho et al. in 2014 as a simpler alternative to LSTM networks.</p>
    <p class="normal">Like LSTMs, GRUs are designed to learn long-term dependencies in sequential data, but they do so using a different approach. GRUs use a single gating mechanism to control the flow of information into and out of the hidden state, rather than the three gates used by LSTMs. This makes them easier to train and requires fewer parameters, making them more efficient to use.</p>
    <p class="normal">The next chapter introduces some advanced techniques related to sequential models.</p>
    <h1 id="_idParaDest-395" class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/WHLel"><span class="url">https://packt.link/WHLel</span></a></p>
    <p class="normal"><img src="../Images/QR_Code1955211820597889031.png" alt="" role="presentation"/></p>
  </div>
</body></html>