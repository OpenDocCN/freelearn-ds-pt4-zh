["```py\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import imdb\nvocab_size = 50000\n(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.imdb.load_data(num_words= vocab_size) \n```", "```py\n\"I watched the movie in a cinema and I really like it\" \n[13, 296, 4, 20, 11, 6, 4435, 5, 13, 66, 447,12] \n```", "```py\n# Pad the sequences\nmax_review_length = 500\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length) \n```", "```py\nword_index = tf.keras.datasets.imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndef decode_review(padded_sequence):\n    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in padded_sequence]) \n```", "```py\nvocab_size = 50000\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size) \n```", "```py\nmodel = tf.keras.models.Sequential() \n```", "```py\nmodel.add(\n    tf.keras.layers.Embedding(\n        input_dim = vocab_size, \n        output_dim = 50, \n        input_length = review_length \n    )\n) \n```", "```py\nmodel.add(\n    tf.keras.layers.Dropout(\n        rate=0.25\n    )\n) \n```", "```py\nmodel.add(\n    tf.keras.layers.LSTM(\n        units=32 \n    )\n) \n```", "```py\nmodel.add(\n    tf.keras.layers.Dropout(\n        rate=0.25\n    )\n) \n```", "```py\nmodel.add(\n    tf.keras.layers.Dense(\n        units=1, \n        activation='sigmoid' \n    )\n) \n```", "```py\nmodel.compile(\n    loss=tf.keras.losses.binary_crossentropy, \n    optimizer=tf.keras.optimizers.Adam(), \n    metrics=['accuracy']) \n```", "```py\nmodel.summary() \n```", "```py\n__________________________________________________________________________\nLayer (type)               Output Shape               Param #\n=========================================================================\nembedding (Embedding)      (None, 500, 32)            320000\ndropout (Dropout)          (None, 500, 32)            0\nlstm (LSTM)                (None, 32)                 8320\ndropout_1 (Dropout)        (None, 32)                 0\ndense (Dense)              (None, 1)                  33\n=========================================================================\nTotal params: 328,353\nTrainable params: 328,353\nNonâ€”trainable params: 0 \n```", "```py\n    history = model.fit(\n        x_train, y_train,    # Training data\n        batch_size=256,      \n        epochs=3,            \n        validation_split=0.2,\n        verbose=1            \n    ) \n    ```", "```py\n    Epoch 1/3\n    79/79 [==============================] - 75s 924ms/step - loss: 0.5757 - accuracy: 0.7060 - val_loss: 0.4365 - val_accuracy: 0.8222\n    Epoch 2/3\n    79/79 [==============================] - 79s 1s/step - loss: 0.2958 - accuracy: 0.8900 - val_loss: 0.3040 - val_accuracy: 0.8812\n    Epoch 3/3\n    79/79 [==============================] - 73s 928ms/step - loss: 0.1739 - accuracy: 0.9437 - val_loss: 0.2768 - val_accuracy: 0.8884 \n    ```", "```py\npredicted_probs = model.predict(x_test)\npredicted_classes_reshaped = (predicted_probs > 0.5).astype(\"int32\").reshape(-1)\nincorrect = np.nonzero(predicted_classes_reshaped != y_test)[0] \n```", "```py\nclass_names = [\"Negative\", \"Positive\"]\nfor j, incorrect_index in enumerate(incorrect[0:20]):\n    predicted = class_names[predicted_classes_reshaped[incorrect_index]]\n    actual = class_names[y_test[incorrect_index]]\n    human_readable_review = decode_review(x_test[incorrect_index])\n    print(f\"Incorrectly classified Test Review [{j+1}]\")\n    print(f\"Test Review #{incorrect_index}: Predicted [{predicted}] Actual [{actual}]\")\n    print(f\"Test Review Text: {human_readable_review.replace('<PAD> ', '')}\\n\") \n```"]