<html><head></head><body>
  <div id="_idContainer440" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-396" class="chapterTitle">Advanced Sequential Modeling Algorithms</h1>
    <blockquote class="packt_quote">
      <p class="quote">An algorithm is a sequence of instructions that, if followed, will solve a problem.</p>
      <p class="cite">—Unknown</p>
    </blockquote>
    <p class="normal">In the last chapter we looked into the core principles of sequential models. It provided an introductory overview of their techniques and methodologies. The sequential modeling algorithms discussed in the last chapter had two basic restrictions. First, the output sequence was required to have the same number of elements as the input sequence. Second, those algorithms can process only one element of an input sequence at a time. If the input sequence is a sentence, it means that the sequential algorithms discussed so far can “<em class="italic">attend</em>,” or process, only one word at a time. To be able to better mimic the processing capabilities of the human brain, we need much more than that. We need complex sequential models that process an output with different lengths to the input, and which can attend to more than one word of a sentence at the same time, removing this information bottleneck.</p>
    <p class="normal">In this chapter, we will delve deeper into the advanced aspects of sequential models to understand the creation of complex configurations. We’ll start by breaking down key elements, such as <a id="_idIndexMarker1143"/>autoencoders and <strong class="keyWord">Sequence-to-Sequence</strong> (<strong class="keyWord">Seq2Seq</strong>) models. Next, we will look at attention mechanisms and transformers, which <a id="_idIndexMarker1144"/>are pivotal in the development of <strong class="keyWord">Large Language Models</strong> (<strong class="keyWord">LLMs</strong>), which we will then study.</p>
    <p class="normal">By the end of this chapter, you will have gained a comprehensive understanding of these advanced structures and their significance in the realm of machine learning. We will also provide insights into the practical applications of these models.</p>
    <p class="normal">The following topics are covered in this chapter:</p>
    <ul>
      <li class="bulletList">Introduction to autoencoders</li>
      <li class="bulletList">Seq2Seq models</li>
      <li class="bulletList">Attention mechanisms</li>
      <li class="bulletList">­Transformers</li>
      <li class="bulletList">LLMs</li>
      <li class="bulletList">Deep and wide architectures</li>
    </ul>
    <p class="normal">First, let’s explore an overview of advanced sequential models.</p>
    <h1 id="_idParaDest-397" class="heading-1">The evolution of advanced sequential modeling techniques</h1>
    <p class="normal">In <em class="chapterRef">Chapter 10</em>, <em class="italic">Understanding Sequential Models</em>, we touched upon the foundational aspects <a id="_idIndexMarker1145"/>of sequential models. While they serve numerous use cases, they face challenges in grasping and producing the complex intricacies of human language.</p>
    <p class="normal">We’ll begin our <a id="_idIndexMarker1146"/>journey by discussing <strong class="keyWord">autoencoders</strong>. Introduced in the early 2010s, autoencoders provided a refreshing approach to data representation. They marked a <a id="_idIndexMarker1147"/>significant evolution in <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>), transforming how we thought about data encoding and decoding. But the momentum in NLP didn’t stop there. By the mid-2010s, <strong class="keyWord">Seq2Seq</strong> models entered <a id="_idIndexMarker1148"/>the scene, bringing forth innovative methodologies for tasks such as language translation. These models could adeptly transform one sequence form into another, heralding an era of advanced sequence processing.</p>
    <p class="normal">However, with the rise in data complexity, the NLP community felt the need for more sophisticated tools. This led <a id="_idIndexMarker1149"/>to the 2015 unveiling of the <strong class="keyWord">attention mechanism</strong>. This elegant solution provided models the ability to selectively concentrate on specific portions of input data, enabling them to manage longer sequences more efficiently. Essentially, it allowed models to weigh the importance of different data segments, amplifying the relevant and diminishing the less pertinent.</p>
    <p class="normal">Building on this foundation, 2017 saw the advent of the <strong class="keyWord">transformer</strong> architecture. Fully leveraging the capabilities <a id="_idIndexMarker1150"/>of attention mechanisms, transformers <a id="_idIndexMarker1151"/>set new benchmarks in NLP. </p>
    <p class="normal">These advancements culminated in the development of <strong class="keyWord">Large Language Models</strong> (<strong class="keyWord">LLMs</strong>). Trained on vast and diverse textual data, LLMs can both understand and generate nuanced human language expressions. Their unparalleled prowess is evident in their widespread applications, from healthcare diagnostics to algorithmic trading in finance.</p>
    <p class="normal">In the <a id="_idIndexMarker1152"/>sections that follow, we’ll unpack the intricacies of autoencoders—from their early beginnings to their central role in today’s advanced sequential models. Prepare to deep dive into the mechanisms, applications, and evolutions of these transformative tools.</p>
    <h1 id="_idParaDest-398" class="heading-1">Exploring autoencoders</h1>
    <p class="normal">Autoencoders occupy a unique niche in the landscape of neural network architectures, playing a pivotal <a id="_idIndexMarker1153"/>role in the narrative of advanced sequential models. Essentially, an autoencoder is designed to create a network where the output mirrors its input, implying a compression of the input data into a more succinct, lower-dimensional latent representation.</p>
    <p class="normal">The autoencoder <a id="_idIndexMarker1154"/>structure can be conceptualized as a dual-phase <a id="_idIndexMarker1155"/>process: the <strong class="keyWord">encoding</strong> phase and the <strong class="keyWord">decoding</strong> phase.</p>
    <p class="normal">Consider the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_01.png" alt="A picture containing text, clock  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.1: Autoencoder architecture</p>
    <p class="normal">In this diagram we make the following assumptions:</p>
    <ul>
      <li class="bulletList"><em class="italic">x</em> corresponds to the input data</li>
      <li class="bulletList"><em class="italic">h</em> is the compressed form of our data</li>
      <li class="bulletList"><em class="italic">r</em> denotes the output, a recreation or approximation of <em class="italic">x</em></li>
    </ul>
    <p class="normal">We can see that the two phases are represented by <em class="italic">f</em> and <em class="italic">g</em>. Let’s look at them in more detail:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Encoding</strong> (<em class="italic">f</em>): Described <a id="_idIndexMarker1156"/>mathematically as <em class="italic">h</em> = <em class="italic">f</em>(<em class="italic">x</em>). In this stage, the input, represented as <em class="italic">x</em>, transforms into a condensed, hidden representation labeled <em class="italic">h</em>.</li>
      <li class="bulletList"><strong class="keyWord">Decoding</strong> (<em class="italic">g</em>): During <a id="_idIndexMarker1157"/>this phase, represented as <em class="italic">r</em> = <em class="italic">g</em>(<em class="italic">h</em>), the compacted <em class="italic">h</em> is unraveled, aiming to reproduce the initial input.</li>
    </ul>
    <p class="normal">When training <a id="_idIndexMarker1158"/>an autoencoder, the goal is to perfect <em class="italic">h</em>, ensuring it encapsulates the essence of the input data. In achieving a high-quality <em class="italic">h</em>, we ensure the recreated output <em class="italic">r</em> mirrors the original <em class="italic">x</em> with minimal loss. The objective is not just to reproduce but also to train an <em class="italic">h</em> that’s streamlined and efficient in this reproduction task.</p>
    <h2 id="_idParaDest-399" class="heading-2">Coding an autoencoder</h2>
    <p class="normal">The <strong class="keyWord">Modified National Institute of Standards and Technology</strong> (<strong class="keyWord">MNIST</strong>) dataset is a renowned <a id="_idIndexMarker1159"/>database of handwritten digits, consisting of 28x28 pixel grayscale images representing numbers <a id="_idIndexMarker1160"/>from 0 to 9. It has been widely used as a benchmark for machine learning algorithms. More information and access to the dataset <a id="_idIndexMarker1161"/>can be found at the official MNIST website. For those interested in accessing the dataset, it’s available at the official MNIST repository hosted by Yann LeCun: <a href="http://yann.lecun.com/exdb/mnist/"><span class="url">yann.lecun.com/exdb/mnist/</span></a>. Please note that an account may be required to download the dataset.</p>
    <p class="normal">In this section, we’ll employ an autoencoder to reproduce these handwritten digits. The unique feature of autoencoders is their training mechanism: the <em class="italic">input</em> and the <em class="italic">target output</em> are the same image. Let’s break this down.</p>
    <p class="normal">First, there <a id="_idIndexMarker1162"/>is the <strong class="keyWord">training</strong> <strong class="keyWord">phase</strong>, where the following steps occur:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The MNIST images are provided to the autoencoder.</li>
      <li class="numberedList">The encoder segment compresses these images into a condensed latent representation.</li>
      <li class="numberedList">The decoder segment then tries to restore the original image from this representation. By iterating over this process, the autoencoder acquires the nuances of compressing and reconstructing, capturing the core patterns of the handwritten digits.</li>
    </ol>
    <p class="normal">Second, there <a id="_idIndexMarker1163"/>is the <strong class="keyWord">reconstruction phase</strong>:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">With the model trained, when we feed it new images of handwritten digits, the autoencoder will first encode them into its internal representation.</li>
      <li class="numberedList">Then, decoding this representation will yield a reconstructed image, which, if the training was successful, should closely match the original.</li>
    </ol>
    <p class="normal">With the autoencoder effectively trained on the MNIST dataset, it becomes a powerful tool to process and reconstruct handwritten digit images.</p>
    <h2 id="_idParaDest-400" class="heading-2">Setting up the environment</h2>
    <p class="normal">Before diving <a id="_idIndexMarker1164"/>into the code, essential libraries must be imported. TensorFlow will be our primary tool, but for data handling, libraries like NumPy may be pivotal:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
    <h3 id="_idParaDest-401" class="heading-3">Data preparation</h3>
    <p class="normal">Next, we’ll segregate <a id="_idIndexMarker1165"/>the dataset into training <a id="_idIndexMarker1166"/>and test segments and then normalize them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Load dataset</span>
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
<span class="hljs-comment"># Normalize data to range [0, 1]</span>
x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span>
</code></pre>
    <p class="normal">Note that the division by <code class="inlineCode">255.0</code> is to normalize our grayscale image data, a step that optimizes the learning process.</p>
    <h3 id="_idParaDest-402" class="heading-3">Model architecture</h3>
    <p class="normal">Designing the <a id="_idIndexMarker1167"/>autoencoder involves <a id="_idIndexMarker1168"/>making decisions about the layers, their sizes, and activation functions. Here, the model is defined using TensorFlow’s <code class="inlineCode">Sequential</code> and <code class="inlineCode">Dense</code> classes:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
    tf.keras.layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.Dense(<span class="hljs-number">784</span>, activation=<span class="hljs-string">'sigmoid'</span>),
    tf.keras.layers.Reshape((<span class="hljs-number">28</span>, <span class="hljs-number">28</span>))
])
</code></pre>
    <p class="normal">Flattening the 28x28 images gives us a 1D array of 784 elements, hence the input shape.</p>
    <h3 id="_idParaDest-403" class="heading-3">Compilation</h3>
    <p class="normal">After the <a id="_idIndexMarker1169"/>model is defined, it’s compiled with a <a id="_idIndexMarker1170"/>specified loss function and optimizer. Binary cross-entropy is chosen due to the binary nature of our grayscale images:</p>
    <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
</code></pre>
    <h3 id="_idParaDest-404" class="heading-3">Training</h3>
    <p class="normal">The training <a id="_idIndexMarker1171"/>phase is initiated with the <code class="inlineCode">fit</code> method. Here, the model learns the nuances of the MNIST handwritten digits:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(x_train, x_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">128</span>,
          validation_data=(x_test, x_test))
</code></pre>
    <h3 id="_idParaDest-405" class="heading-3">Prediction</h3>
    <p class="normal">With a <a id="_idIndexMarker1172"/>trained model, predictions (both encoding and decoding) can be executed as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">encoded_data = model.predict(x_test)
decoded_data = model.predict(encoded_data)
</code></pre>
    <h3 id="_idParaDest-406" class="heading-3">Visualization</h3>
    <p class="normal">Let us now <a id="_idIndexMarker1173"/>visually compare the original images with their reconstructed counterparts. The following script showcases a visualization procedure that displays two rows of images:</p>
    <pre class="programlisting code"><code class="hljs-code">n = <span class="hljs-number">10</span>  <span class="hljs-comment"># number of images to display</span>
plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):
    <span class="hljs-comment"># Original images</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, n, i + <span class="hljs-number">1</span>)
    plt.imshow(x_test[i].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>) , cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
    <span class="hljs-comment"># Reconstructed images</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, n, i + <span class="hljs-number">1</span> + n)
    plt.imshow(decoded_data[i].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>) , cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
plt.show()
</code></pre>
    <p class="normal">The following <a id="_idIndexMarker1174"/>screenshot shows the outputted reconstructed images:</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_02.png" alt="A black square with white numbers  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.2: The original test images (top row) and the post-reconstruction by the autoencoder (bottom row)</p>
    <p class="normal">The top row presents the original test images, while the bottom row exhibits the post-reconstruction images made by the autoencoder. Through this side-by-side comparison, we can discern the efficacy of our model in preserving the intrinsic features of the input.</p>
    <p class="normal">Let us now discuss Seq2Seq models.</p>
    <h1 id="_idParaDest-407" class="heading-1">Understanding the Seq2Seq model</h1>
    <p class="normal">Following our exploration of autoencoders, another groundbreaking architecture in the realm of <a id="_idIndexMarker1175"/>advanced sequential models is the <strong class="keyWord">Seq2Seq</strong> model. Central to many state-of-the-art natural language processing tasks, the Seq2Seq model exhibits a unique capability: transforming an input sequence into an output sequence that may differ in length. This flexibility allows it to excel in challenges like machine translation, where the source and target sentences can naturally differ in size.</p>
    <p class="normal">Refer to <em class="italic">Figure 11.3</em>, which visualizes the core components of a Seq2Seq model:</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_03.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.3: Illustration of the Seq2Seq model architecture</p>
    <p class="normal">Broadly, there <a id="_idIndexMarker1176"/>are three main elements:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Encoder</strong>: Processes the input sequence</li>
      <li class="bulletList"><strong class="keyWord">Thought vector</strong>: A bridge between the encoder and decoder</li>
      <li class="bulletList"><strong class="keyWord">Decoder</strong>: Generates the output sequence</li>
    </ul>
    <p class="normal">Let us explore them one by one.</p>
    <h2 id="_idParaDest-408" class="heading-2">Encoder</h2>
    <p class="normal">The encoder <a id="_idIndexMarker1177"/>is shown as <img src="../Images/B18046_11_09.png" alt="Icon  Description automatically generated"/> in <em class="italic">Figure 11.3</em>. As we can observe, it is <a id="_idIndexMarker1178"/>an input <strong class="keyWord">Recurrent Neural Network</strong> (<strong class="keyWord">RNN</strong>) that processes the input sequence. The input sentence in this case is a three-word sentence: <em class="italic">Is Ottawa cold? </em>It can be represented as:</p>
    <p class="center">X = {x<sup class="superscript">&lt;1&gt;</sup>, x<sup class="superscript">&lt;2&gt;</sup>,… …., x<sup class="superscript">&lt;L1&gt;</sup>}</p>
    <p class="normal">The encoder <a id="_idIndexMarker1179"/>traverses through this sequence until it encounters an <strong class="keyWord">End-Of-Sentence</strong> (&lt;<strong class="keyWord">EOS</strong>&gt;) token, indicating the conclusion of the input. It will be positioned at timestep <em class="italic">L1</em>.</p>
    <h2 id="_idParaDest-409" class="heading-2">Thought vector</h2>
    <p class="normal">Throughout <a id="_idIndexMarker1180"/>the encoding phase, the RNN updates <a id="_idIndexMarker1181"/>its hidden state, denoted by h<sup class="superscript">&lt;t&gt;</sup>. The final hidden state, captured at the end of the sequence h<sup class="superscript">&lt;L1&gt;</sup>, is relayed to the decoder. This final state is termed the <strong class="keyWord">thought vector</strong>, coined by Geoffrey Hinton in 2015. This compact representation captures the essence of the input sequence. The thought vector is shown as <img src="../Images/B18046_11_10.png" alt="Icon  Description automatically generated"/> in <em class="italic">Figure 11.3</em>.</p>
    <h2 id="_idParaDest-410" class="heading-2">Decoder or writer</h2>
    <p class="normal">Upon completion <a id="_idIndexMarker1182"/>of the encoding process, a <code class="inlineCode">&lt;GO&gt;</code> token signals the decoder to <a id="_idIndexMarker1183"/>commence. Using the encoder’s final hidden state h<sup class="superscript">&lt;L1&gt;</sup> as its initial input, the <a id="_idIndexMarker1184"/>decoder, an output RNN, begins constructing the output sequence, Y = {y<sup class="superscript">&lt;1&gt;</sup>, y<sup class="superscript">&lt;2&gt;</sup>,… …., y<sup class="superscript">&lt;L2&gt;</sup>}. In the context of <em class="italic">Figure 11.3</em>, this output sequence translates to the sentence: <em class="italic">Yes</em>, <em class="italic">it is</em>.</p>
    <h2 id="_idParaDest-411" class="heading-2">Special tokens in Seq2Seq</h2>
    <p class="normal">While <code class="inlineCode">&lt;EOS&gt;</code> and <code class="inlineCode">&lt;GO&gt;</code> are essential tokens within the Seq2Seq paradigm, there are others <a id="_idIndexMarker1185"/>worth noting:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">&lt;UNK&gt;</code>: Standing for <em class="italic">unknown</em>, this token replaces infrequent words, ensuring the vocabulary remains manageable.</li>
      <li class="bulletList"><code class="inlineCode">&lt;PAD&gt;</code>: Used for padding shorter sequences, this token standardizes sequence lengths during training, enhancing the model’s efficacy.</li>
    </ul>
    <p class="normal">A salient feature of the Seq2Seq model is its ability to handle variable sequence lengths, meaning input and output sequences can inherently differ in size. This flexibility, combined with its sequential nature, makes Seq2Seq a pivotal architecture in the advanced modeling landscape, bridging our journey from autoencoders to more complex, nuanced sequential processing systems.</p>
    <p class="normal">Having traversed the foundational realms of autoencoders and delved deep into the Seq2Seq models, we now need to understand the limitations of the encoder-decoder framework. </p>
    <h2 id="_idParaDest-412" class="heading-2">The information bottleneck dilemma</h2>
    <p class="normal">As we have learned, the heart of traditional Seq2Seq models is the thought vector, h<sup class="superscript">&lt;L1&gt;</sup> . This is the <a id="_idIndexMarker1186"/>last hidden state from the encoder, which serves as the bridge to the decoder. This vector is tasked with encapsulating the entirety of the input sequence, <em class="italic">X</em>. The simplicity of this mechanism is both its strength and its weakness. This weakness is highlighted when sequences grow longer; compressing vast amounts of information into a fixed-size representation becomes increasingly <a id="_idIndexMarker1187"/>formidable. This is termed the <strong class="keyWord">information bottleneck</strong>. No matter the richness or complexity of the input, the fixed-length memory constraint means only so much can be relayed from the encoder to the decoder.</p>
    <p class="normal">To learn how <a id="_idIndexMarker1188"/>this problem has been addressed, we need to shift our focus from Seq2Seq models to the attention mechanism.</p>
    <h1 id="_idParaDest-413" class="heading-1">Understanding the attention mechanism</h1>
    <p class="normal">Following the challenges presented by the fixed-length memory in traditional Seq2Seq models, 2014 marked a revolutionary step forward. Dzmitry Bahdanau, KyungHyun Cho, and Yoshua <a id="_idIndexMarker1189"/>Bengio proposed a transformative solution: the <strong class="keyWord">attention mechanism</strong>. Unlike earlier models that tried (often in vain) to condense entire sequences into limited memory spaces, attention mechanisms enabled models to hone in on specific, relevant parts of the input sequence. Picture it as a magnifying glass over only the most critical data at each decoding step.</p>
    <h2 id="_idParaDest-414" class="heading-2">What is attention in neural networks?</h2>
    <p class="normal">Attention, as the adage goes, is where focus goes. In the realm of NLP and particularly in the training <a id="_idIndexMarker1190"/>of LLMs, attention has garnered <a id="_idIndexMarker1191"/>significant emphasis. Traditionally, neural networks processed input data in a fixed sequence, potentially missing out on the relevance of context. Enter attention—a mechanism that weighs the importance of different input data, focusing more on what’s relevant.</p>
    <h3 id="_idParaDest-415" class="heading-3">Basic idea</h3>
    <p class="normal">Just as <a id="_idIndexMarker1192"/>humans pay more attention to salient parts of an image or text, attention mechanisms allow neural models to focus on more relevant parts of the input data. It effectively tells the model where to “look” next.</p>
    <h3 id="_idParaDest-416" class="heading-3">Example</h3>
    <p class="normal">Inspired by <a id="_idIndexMarker1193"/>my recent journey to Egypt, which felt like a voyage back in time, consider the expressive and symbolic language of ancient Egypt: hieroglyphs.</p>
    <p class="normal">Hieroglyphs were much more than mere symbols; they were an intricate fusion of art and language, representing multifaceted meanings. This system, with its vast array of symbols, exemplifies the foundational principles of attention mechanisms in neural networks.</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_04.png" alt="A group of pyramids in a desert  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.4: Giza’s prominent pyramids - Khufu and Khafre, accompanied by inscriptions in the age-old Egyptian script, “hieroglyphs” (photographs taken by the author)</p>
    <p class="normal">For instance, an Egyptian scribe wishes to convey news about an anticipated grand festival by <a id="_idIndexMarker1194"/>the Nile. Out of the thousands of hieroglyphs available:</p>
    <ul>
      <li class="bulletList"><img src="../Images/B18046_11_11.png" alt="A white symbol on a black background  Description automatically generated"/> The <em class="italic">Ankh</em> hieroglyph, symbolizing life, captures the festival’s vibrancy and celebratory spirit.</li>
      <li class="bulletList"><img src="../Images/B18046_11_12.png" alt="A black pole with a number one  Description automatically generated"/> The <em class="italic">Was</em> symbol, resembling a staff, hints at authority or the Pharaoh’s pivotal role in the celebrations.</li>
      <li class="bulletList"><img src="../Images/B18046_11_13.png" alt="A black zigzag lines on a white background  Description automatically generated"/> An illustration of the <em class="italic">Nile</em>, central to Egyptian culture, pinpoints the festival’s venue.</li>
    </ul>
    <p class="normal">However, to communicate the festival’s grandeur and importance, not all symbols would hold equal weight. The scribe would have to emphasize or repeat specific hieroglyphs to <a id="_idIndexMarker1195"/>draw attention to the most crucial aspects of the message.</p>
    <p class="normal">This selective emphasis is parallel to neural network attention mechanisms.</p>
    <h2 id="_idParaDest-417" class="heading-2">Three key aspects of attention mechanisms</h2>
    <p class="normal">In neural networks, especially within NLP tasks, attention mechanisms play a crucial role in filtering and <a id="_idIndexMarker1196"/>focusing on relevant information. Here, we’ll distill the primary facets of attention into three main components: contextual relevance, symbol efficiency, and prioritized focus:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Contextual relevance</strong>:<ul>
          <li class="bulletList"><strong class="keyWord">Overview</strong>: At its <a id="_idIndexMarker1197"/>core, attention aims to allocate more importance to certain parts <a id="_idIndexMarker1198"/>of the input data that are deemed more relevant to the task at hand.</li>
          <li class="bulletList"><strong class="keyWord">Deep dive</strong>: Take a simple input like <em class="italic">“The grand Nile festival.”</em> In this context, attention mechanisms might assign higher weights to the words <em class="italic">“Nile”</em> and <em class="italic">“grand.”</em> This isn’t because of their general significance but due to their task-specific importance. Instead of treating every word or input with uniform importance, attention differentiates and adjusts the model’s focus based on context.</li>
          <li class="bulletList"><strong class="keyWord">In practice</strong>: Think of this as a spotlight. Just as a spotlight on a stage illuminates specific actors during crucial moments while dimming others, attention shines a light on specific input data that holds more contextual value.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Symbol efficiency</strong>:<ul>
          <li class="bulletList"><strong class="keyWord">Overview</strong>: The ability <a id="_idIndexMarker1199"/>of attention to condense <a id="_idIndexMarker1200"/>vast amounts of information into digestible, critical segments.</li>
          <li class="bulletList"><strong class="keyWord">Deep dive</strong>: Hieroglyphs can encapsulate complex narratives or ideas within singular symbols. Analogously, attention mechanisms, by assigning varied weights, can determine which segments of the data contain maximal information and should be processed preferentially.</li>
          <li class="bulletList"><strong class="keyWord">In practice</strong>: Consider compressing a large document into a succinct summary. The summary retains only the most critical information, mirroring the function of attention mechanisms that extract and prioritize the most pertinent data from a larger input.</li>
        </ul>
      </li>
      <li class="bulletList"><strong class="keyWord">Prioritized focus</strong>:<ul>
          <li class="bulletList"><strong class="keyWord">Overview</strong>: Attention mechanisms don’t distribute their focus uniformly. They are designed to prioritize segments of input data based <a id="_idIndexMarker1201"/>on their perceived relevance to the task.</li>
          <li class="bulletList"><strong class="keyWord">Deep dive</strong>: Drawing inspiration from our hieroglyph example, just as an Egyptian scribe might emphasize the “<em class="italic">Ankh</em>” symbol when wanting to convey the idea of life or celebration, attention mechanisms will adjust their focus (or weights) to specific parts of the input that are more relevant.</li>
          <li class="bulletList"><strong class="keyWord">In practice</strong>: It’s akin to reading a research paper. While the entire document holds value, one might focus more on the abstract, conclusion, or specific data points that align with their current research needs.</li>
        </ul>
      </li>
    </ul>
    <p class="normal">Thus, the attention mechanism in neural networks emulates the selective focus humans naturally employ when processing information. By understanding the nuances of how attention prioritizes and processes data, we can better design and interpret neural models.</p>
    <h2 id="_idParaDest-418" class="heading-2">A deeper dive into attention mechanisms</h2>
    <p class="normal">Attention mechanisms can be thought of as an evolved form of communication, much like hieroglyphs <a id="_idIndexMarker1202"/>were in ancient times. Traditionally, an encoder sought to distill an entire input sequence into one encapsulating hidden state. This is analogous to an Egyptian scribe trying to convey an entire event using a single hieroglyph. While possible, it’s challenging and may not capture the event’s full essence.</p>
    <p class="normal">Now, with the enhanced encoder-decoder approach, we have the luxury of generating a hidden state for every step, offering a richer tapestry of data for the decoder. But referencing every single hieroglyph (or state) at once would be chaotic, like a scribe using every symbol available to describe a single event by the Nile. That’s where attention comes in.</p>
    <p class="normal">Attention allows the decoder to prioritize. Just as a scribe might focus on the “Ankh” hieroglyph to signify life and vitality, or the “Was” staff to represent power, or even depict the Nile <a id="_idIndexMarker1203"/>itself to pinpoint a location, the decoder assigns varying weightage to each encoder state. It decides which parts of the input sequence (or which hieroglyphs) deserve more emphasis. Using our translation example, when converting “<em class="italic">Transformers are great!</em>” to “<em class="italic">Transformatoren sind grossartig!</em>”, the mechanism emphasizes aligning “<em class="italic">great</em>” with “<em class="italic">grossartig</em>,” ensuring the core sentiment remains intact.</p>
    <p class="normal">This selective focus, whether in neural network attention mechanisms or hieroglyphic storytelling, ensures precision and clarity in the conveyed message.</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_05.png" alt="enc-dec-attn"/></figure>
    <p class="packt_figref">Figure 11.5: RNNs employing an encoder-decoder structure enhanced with an attention mechanism</p>
    <h2 id="_idParaDest-419" class="heading-2">The challenges of attention mechanisms</h2>
    <p class="normal">While incorporating attention with RNNs offers notable improvements, it’s not a silver bullet. One <a id="_idIndexMarker1204"/>significant hurdle is computational cost. The act of transferring multiple hidden states from encoder to decoder demands substantial processing power.</p>
    <p class="normal">However, as with all technological progress, solutions continually emerge. One such advancement is the introduction of <strong class="keyWord">self-attention</strong>, a cornerstone of transformer architectures. This innovative variant refines the attention process, making it more efficient and scalable.</p>
    <h1 id="_idParaDest-420" class="heading-1">Delving into self-attention</h1>
    <p class="normal">Let’s consider again the ancient art of hieroglyphs, where symbols were chosen intentionally to <a id="_idIndexMarker1205"/>convey complex messages. Self-attention operates in a similar manner, determining which parts of a sequence are vital and should be emphasized.</p>
    <p class="normal">Illustrated in <em class="italic">Figure 11.6</em> is the beauty of integrating self-attention within sequential models. Think of the bottom layer, churning with bidirectional RNNs, as the foundational <a id="_idIndexMarker1206"/>stones of a pyramid. They generate what we call the <strong class="keyWord">context vector</strong> (<strong class="keyWord">c2</strong>), a summary, much like a hieroglyph would for an event.</p>
    <p class="normal">Each step or word in a sequence has its <strong class="keyWord">weightage</strong>, symbolized as <em class="italic">α</em>. These weights interact with the context vector, emphasizing the importance of certain elements over others.</p>
    <p class="normal">Imagine a scenario wherein the input <em class="italic">X</em><sub class="subscript-italic" style="font-style: italic;">k</sub> represents a distinct sentence, denoted as <em class="italic">k</em>, which spans a length of <em class="italic">L1</em>. This can be mathematically articulated as:</p>
    <p class="center"><img src="../Images/B18046_11_002.png" alt="" role="presentation"/></p>
    <p class="normal">Here, every element, <img src="../Images/B18046_11_003.png" alt="" role="presentation"/>, represents a word or token from sentence <em class="italic">k</em>: the superscript &lt;t&gt; indicates its specific position or timestep within that sentence.</p>
    <h2 id="_idParaDest-421" class="heading-2">Attention weights</h2>
    <p class="normal">In the realm of self-attention, attention weights play a pivotal role, acting like a compass pointing to <a id="_idIndexMarker1207"/>which words are essential. They assign an “importance score” to each word when generating the context vector.</p>
    <p class="normal">To bring this into perspective, consider our earlier translation example: “<em class="italic">Transformers are great!</em>” translated to “<em class="italic">Transformatoren sind grossartig!</em>”. When focusing on the word “<em class="italic">Transformers</em>”, the attention weights might break down like this:</p>
    <ul>
      <li class="bulletList">α<sub class="subscript">2,1</sub>: Measures the relationship between “<em class="italic">Transformers</em>” and the beginning of the sentence. A high value here indicates that the word “<em class="italic">Transformers</em>” significantly relies on the beginning for its context. </li>
      <li class="bulletList">α<sub class="subscript">2,2</sub>: Reflects how much “<em class="italic">Transformers</em>” emphasizes its inherent meaning. </li>
      <li class="bulletList">α<sub class="subscript">2,3</sub> and α<sub class="subscript">2,4</sub>: These weigh how much “<em class="italic">Transformers</em>” takes into context the words “<em class="italic">are</em>” and “<em class="italic">great!</em>”, respectively. High scores here mean that “<em class="italic">Transformers</em>” is deeply influenced by these surrounding words.</li>
    </ul>
    <p class="normal">During training, these attention weights are constantly adjusted and fine-tuned. This ongoing <a id="_idIndexMarker1208"/>refinement ensures our model understands the intricate dance between words in a sentence, capturing both the explicit and subtle connections.</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_06.png" alt="A diagram of a complex structure  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 11.6: Integrating self-attention in sequential models</p>
    <p class="normal">Before we delve deeper into the mechanisms of self-attention, it’s vital to understand the key pieces that come together in <em class="italic">Figure 11.6</em>.</p>
    <h2 id="_idParaDest-422" class="heading-2">Encoder: bidirectional RNNs</h2>
    <p class="normal">In the last chapter we investigated the main architectural building blocks of unidirectional <a id="_idIndexMarker1209"/>RNN and its variants. <strong class="keyWord">Bidirectional RNNs</strong> were invented to address that need (Schuster and Paliwal, 1997). We also <a id="_idIndexMarker1210"/>identified a deficiency in unidirectional RNNs, as they are only capable of carrying the context in one direction.</p>
    <p class="normal">For an input sequence, say <em class="italic">X</em>, the bidirectional RNN first reads it from the start to the end, and then from the end back to the start. This dual approach helps capture information based on preceding and succeeding elements. For each timestep, we get two hidden states: <img src="../Images/B18046_11_004.png" alt="" role="presentation"/> for the forward direction and <img src="../Images/B18046_11_005.png" alt="" role="presentation"/>for the backward one. These states are merged into a single one for that timestep, represented by:</p>
    <p class="center"><img src="../Images/B18046_11_006.png" alt="" role="presentation"/></p>
    <p class="normal">For instance, if <img src="../Images/B18046_11_007.png" alt="" role="presentation"/> and <img src="../Images/B18046_11_008.png" alt="" role="presentation"/>are 64-dimensional vectors, the resulting h<sup class="superscript">&lt;t2&gt;</sup> is 128-dimensional. This combined state is a detailed representation of the sequence context from both directions.</p>
    <h2 id="_idParaDest-423" class="heading-2">Thought vector</h2>
    <p class="normal">The thought <a id="_idIndexMarker1211"/>vector, here symbolized as C<sub class="subscript">k</sub>, is a representation of the input X<sub class="subscript">k</sub>. As we <a id="_idIndexMarker1212"/>have learned, its creation is an attempt to capture the sequencing patterns, context, and state of each element in X<sub class="subscript">k</sub>.</p>
    <p class="normal">In our preceding diagram it is defined as:</p>
    <p class="center"><img src="../Images/B18046_11_009.png" alt="" role="presentation"/></p>
    <p class="normal">Where <img src="../Images/B18046_11_010.png" alt="" role="presentation"/> are attention weights for timestep <em class="italic">t</em> that are refined during training.</p>
    <p class="normal">Using the <a id="_idIndexMarker1213"/>summation <a id="_idIndexMarker1214"/>notation, it can be expressed as:</p>
    <p class="center"><img src="../Images/B18046_11_011.png" alt="" role="presentation"/></p>
    <h2 id="_idParaDest-424" class="heading-2">Decoder: regular RNNs</h2>
    <p class="normal"><em class="italic">Figure 11.5</em> shows the decoder connected through the thought vector to the encoder.</p>
    <p class="normal">The output <a id="_idIndexMarker1215"/>of the decoder for a certain sentence k is represented by:</p>
    <p class="center"><img src="../Images/B18046_11_012.png" alt="" role="presentation"/></p>
    <p class="normal">Note that <a id="_idIndexMarker1216"/>the output has a length of <em class="italic">L2</em>, which is different from the length of the input sequence, which was <em class="italic">L1</em>.</p>
    <h2 id="_idParaDest-425" class="heading-2">Training versus inference</h2>
    <p class="normal">In the training data for a certain input sequence <em class="italic">k</em>, we have the expected output vector representing <a id="_idIndexMarker1217"/>the ground truth, which is represented by a vector Y<sub class="subscript">k</sub>. This is:</p>
    <p class="center"><img src="../Images/B18046_11_013.png" alt="" role="presentation"/></p>
    <p class="normal">At each timestep, the decoder’s RNN gets three inputs:</p>
    <ul>
      <li class="bulletList"><img src="../Images/B18046_11_014.png" alt="" role="presentation"/>: The previous hidden state</li>
      <li class="bulletList">C<sub class="subscript">k</sub>: The thought vector for sequence <em class="italic">k</em></li>
      <li class="bulletList"><img src="../Images/B18046_11_015.png" alt="" role="presentation"/>: The previous word in the ground truth vector Y<sub class="subscript">k</sub></li>
    </ul>
    <p class="normal">However, during inference, as there’s no prior ground truth available, the decoder’s RNN uses the prior output word, <img src="../Images/B18046_11_016.png" alt="" role="presentation"/>, instead.</p>
    <p class="normal">Now that <a id="_idIndexMarker1218"/>we have learned how self-attention addresses the challenges faced by attention mechanisms and the basic operations it involves, we can move our attention to the next major advancement in sequential modeling: transformers.</p>
    <h1 id="_idParaDest-426" class="heading-1">Transformers: the evolution in neural networks after self-attention</h1>
    <p class="normal">Our exploration <a id="_idIndexMarker1219"/>into self-attention revealed its powerful capability to reinterpret sequence data, providing each word with a contextual understanding based on its relationships with other words. This principle set the stage for an evolutionary leap in neural <a id="_idIndexMarker1220"/>network designs: the <strong class="keyWord">transformer</strong> architecture.</p>
    <p class="normal">Introduced by the Google Brain team in their 2017 paper, <em class="italic">Attention is All You Need</em> (<a href="https://arxiv.org/abs/1706.03762"><span class="url">https://arxiv.org/abs/1706.03762</span></a>), the transformer architecture is built upon the very essence of self-attention. Before its advent, RNNs were the go-to. Picture RNNs as diligent librarians reading an English sentence to translate it into German, word by word, ensuring the context is relayed from one word to the next. They’re reliable for short texts but can stumble when sentences get too long, misplacing the essence of earlier words.</p>
    <figure class="mediaobject"><img src="../Images/B18046_11_07.png" alt="transformer-self-attn"/></figure>
    <p class="packt_figref">Figure 11.7: Encoder-decoder architecture of the original transformer</p>
    <p class="normal">Transformers are a fresh approach to sequence data. Instead of a linear, word-by-word progression, transformers, armed with advanced attention mechanisms, comprehend an entire sequence in a single glance. It’s like instantly grasping the sentiment of a whole paragraph rather than piecing it together word by word. This holistic view ensures a richer, all-encompassing understanding, celebrating the nuanced interplay between words.</p>
    <p class="normal">Self-attention is <a id="_idIndexMarker1221"/>central to the transformer’s efficiency. While we touched upon this in the previous section, it’s worth noting how pivotal it is here. Each layer of the network, through self-attention, can resonate with every other part of the input data. As depicted in <em class="italic">Figure 11.7</em>, the transformer architecture employs self-attention for both its encoder and decoder segments, which then feed into neural networks (also <a id="_idIndexMarker1222"/>known as <strong class="keyWord">feedforward neural networks (FFNNs)</strong>). Beyond being more trainable, this setup has catalyzed many of the recent breakthroughs in NLP.</p>
    <p class="normal">To illustrate, consider <em class="italic">Ancient Egypt: An Enthralling Overview of Egyptian History</em>, by Billy Wellman. Within it, the relationships between early pharaohs like Ramses and Cleopatra and pyramid construction are vast and intricate. Traditional models might stumble with such expansive content.</p>
    <h2 id="_idParaDest-427" class="heading-2">Why transformers shine</h2>
    <p class="normal">The transformer architecture, with its self-attention mechanism, emerges as a promising solution. When <a id="_idIndexMarker1223"/>encountering a term like “<em class="italic">pyramids</em>,” the model can, using self-attention, assess its relevance to terms like “<em class="italic">Ramses</em>” or “<em class="italic">Cleopatra</em>,” irrespective of their position. This ability to attend to various input parts demonstrates why transformers are pivotal in modern NLP.</p>
    <h2 id="_idParaDest-428" class="heading-2">A Python code breakdown</h2>
    <p class="normal">Here’s a <a id="_idIndexMarker1224"/>simplified version of how the self-attention mechanism can be implemented:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">def</span> <span class="hljs-title">self_attention</span>(<span class="hljs-params">Q, K, V</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Q: Query matrix</span>
<span class="hljs-string">    K: Key matrix</span>
<span class="hljs-string">    V: Value matrix</span>
<span class="hljs-string">    """</span>
    
    <span class="hljs-comment"># Calculate the attention weights</span>
    attention_weights = np.matmul(Q, K.T)
    
    <span class="hljs-comment"># Apply the softmax to get probabilities</span>
    attention_probs = np.exp(attention_weights) / np.<span class="hljs-built_in">sum</span>(np.exp(attention_weights), axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)
    
    <span class="hljs-comment"># Multiply the probabilities with the value matrix to get the output</span>
    output = np.matmul(attention_probs, V)
    
    <span class="hljs-keyword">return</span> output
<span class="hljs-comment"># Example</span>
Q = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])  <span class="hljs-comment"># Example Query</span>
K = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])  <span class="hljs-comment"># Key matrix</span>
V = np.array([[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]])  <span class="hljs-comment"># Value matrix</span>
output = self_attention(Q, K, V)
<span class="hljs-built_in">print</span>(output)
</code></pre>
    <p class="normal">Output:</p>
    <pre class="programlisting con"><code class="hljs-con">[[0.09003057 1.57521038 0.57948752]
 [0.86681333 0.14906291 1.10143419]
 [0.4223188  0.73304361 1.26695639]]
</code></pre>
    <p class="normal">This code is <a id="_idIndexMarker1225"/>a basic representation, and the real transformer model uses a more optimized and detailed approach, especially when scaling for larger sequences. But the essence is the dynamic weighting of different words in the sequence, allowing the model to bring in contextual understanding.</p>
    <h2 id="_idParaDest-429" class="heading-2">Understanding the output</h2>
    <ul>
      <li class="bulletList">The first row, <code class="inlineCode">[0.09003057 1.57521038 0.57948752]</code>, corresponds to the weighted <a id="_idIndexMarker1226"/>combination of the V matrix for the first word in the query (in this case, represented by the first row of the Q matrix). This means when our model encounters the word represented by this query, it will focus 9% on the first word, 57.5% on the second word, and 57.9% on the third word from the V matrix to derive contextual understanding.</li>
      <li class="bulletList">The second row, <code class="inlineCode">[0.86681333 0.14906291 1.10143419]</code>, is the attention result for the second word in the query. It focuses 86.6% on the first word, 14.9% on the second, and 110.1% on the third from the V matrix.</li>
      <li class="bulletList">The third row, <code class="inlineCode">[0.4223188 0.73304361 1.26695639]</code>, is for the third word in the query. It has attention weights of 42.2%, 73.3%, and 126.7%, respectively, for the words in the V matrix.</li>
    </ul>
    <p class="normal">Having reviewed transformers, their place in sequential modeling, their code, and their output, we can consider the next major development in NLP. Next, let us look at LLMs.</p>
    <h1 id="_idParaDest-430" class="heading-1">LLMs</h1>
    <p class="normal"><strong class="keyWord">LLMs</strong> are the <a id="_idIndexMarker1227"/>next evolutionary step after transformers in the world of NLP. They’re not just beefed-up older models; they represent a quantum leap. These models can handle vast amounts of text data and perform tasks previously thought to be reserved for human minds.</p>
    <p class="normal">Simply put, LLMs can produce text, answer questions, and even code. Picture chatting with software and it replying just like a human, catching subtle hints and recalling earlier parts of the conversation. That’s what LLMs offer.</p>
    <p class="normal"><strong class="keyWord">Language models</strong> (<strong class="keyWord">LMs</strong>) have always <a id="_idIndexMarker1228"/>been the backbone of NLP, helping in tasks ranging from machine translation to more modern tasks like text classification. While <a id="_idIndexMarker1229"/>the early LMs relied on RNNs and <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) structures, today’s NLP achievements are primarily due to deep learning techniques, especially transformers.</p>
    <p class="normal">The hallmark of LLMs? Their capacity to read and learn from vast quantities of text. Training one from scratch is a serious undertaking, requiring powerful computers and lots of time. Depending on factors like the model’s size and the amount of training data—say, from giant sources like Wikipedia or the Common Crawl dataset—it could take weeks or even months to train an LLM.</p>
    <p class="normal">Dealing with long sequences is a known challenge for LLMs. Earlier models, built on RNNs and LSTMs, faced issues with lengthy sequences, often losing vital details, which hampered their performance. This is where we start to see the role of <strong class="keyWord">attention</strong> come into play. Attention mechanisms act as a torch, illuminating essential sections of long inputs. For example, in a text about car advancements, attention makes sure the model recognizes and focuses on the major breakthroughs, no matter where they appear in the text.</p>
    <h2 id="_idParaDest-431" class="heading-2">Understanding attention in LLMs</h2>
    <p class="normal">Attention mechanisms have become foundational in the neural network domain, particularly evident in LLMs. Training these mammoth models, laden with millions or even billions of parameters, is not a walk in the park. At their core, attention mechanisms <a id="_idIndexMarker1230"/>are like highlighters, emphasizing key details. For instance, when processing a lengthy text on NLP’s evolution, LLMs can understand the overall theme, but attention ensures they don’t miss the critical milestones. Transformers utilize this attention feature, aiding LLMs in handling vast text stretches and ensuring <strong class="keyWord">contextual</strong> consistency.</p>
    <p class="normal">For LLMs, context is everything. For example, if an LLM crafts a story starting with a cat, attention ensures that as the tale unfolds, the context remains. So instead of introducing unrelated sounds like “<em class="italic">barking</em>,” the story would naturally lean toward “<em class="italic">purring</em>” or “<em class="italic">meowing</em>.”</p>
    <p class="normal">Training an LLM resembles running a supercomputer continuously for months, purely to process vast text quantities. And when the initial training is done, it’s only the beginning. Think of it like owning a high-end vehicle—you’d need periodic maintenance. Similarly, LLMs need frequent updates and refinements based on new data.</p>
    <p class="normal">Even after training an LLM, the work isn’t over. For these models to remain effective, they need to keep learning. Imagine teaching someone English grammar rules and then throwing in slang or idioms—they need to adapt to these irregularities for a complete understanding.</p>
    <p class="normal">Highlighting a historical shift, between 2017 and 2018, there was a notable change in the LLM landscape. Firms, including OpenAI, began leveraging unsupervised pretraining, paving the way for more streamlined models for tasks, like sentiment analysis.</p>
    <h2 id="_idParaDest-432" class="heading-2">Exploring the powerhouses of NLP: GPT and BERT</h2>
    <p class="normal"><strong class="keyWord">Universal Language Model Fine-Tuning</strong> (<strong class="keyWord">ULMFiT</strong>) set the stage for a new era in NLP. This <a id="_idIndexMarker1231"/>method pioneered the reuse of <a id="_idIndexMarker1232"/>pre-trained LSTM models, adapting them to a variety of NLP tasks, which saved both computational resources and time. Let’s break down its process:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Pretraining</strong>: This is akin to teaching a child the basics of a language. Using extensive datasets like Wikipedia, the model learns the foundational structures and grammar of the language. Imagine this as equipping a student with general knowledge textbooks.</li>
      <li class="numberedList"><strong class="keyWord">Domain adaptation</strong>: The model then delves into specialized areas or genres. If the first step was about learning grammar, this step is like introducing the model to different genres of literature - from thrillers to scientific journals. It still predicts words, but now within specific contexts.</li>
      <li class="numberedList"><strong class="keyWord">Fine-tuning</strong>: Finally, the model is honed for specific tasks, such as detecting emotions or sentiments in a given text. This is comparable to training a student to write essays or analyze texts in depth.</li>
    </ol>
    <h3 id="_idParaDest-433" class="heading-3">2018’s LLM pioneers: GPT and BERT</h3>
    <p class="normal">2018 saw the rise of two standout models: GPT and BERT. Let us look into them in more detail.</p>
    <h4 class="heading-4">Generative Pre-trained Transformer (GPT)</h4>
    <p class="normal">Inspired by <a id="_idIndexMarker1233"/>ULMFiT, GPT is a model that leans on the decoder side of the transformer architecture. Visualize the vastness of human literature. If traditional models are trained with a fixed set of books, GPT is like giving a scholar access to an entire library, including the BookCorpus - a rich dataset with diverse, unpublished books. This allows GPT to draw insights from genres ranging from fiction to history.</p>
    <p class="normal">Here’s an analogy: traditional models might know the plots of Shakespeare’s plays. GPT, with its extensive learning, would understand not only the plots but also the cultural context, character nuances, and the evolution of Shakespeare’s writing style over time.</p>
    <p class="normal">Its focus on the decoder makes GPT a master of generating text that’s both relevant and coherent, like a seasoned author drafting a novel.</p>
    <h4 class="heading-4">BERT (Bidirectional Encoder Representations from Transformers)</h4>
    <p class="normal">BERT revamped traditional language modeling with its “masked language modeling” technique. Unlike models that just predict the next word in a sentence, BERT fills in intentionally blanked-out or “masked” words, enhancing its contextual understanding.</p>
    <p class="normal">Let’s <a id="_idIndexMarker1234"/>put this into perspective. In a sentence like “<em class="italic">She went to Paris to visit the ___</em>,” conventional models might predict words that fit after “<em class="italic">the</em>,” such as “<em class="italic">museum</em>.” BERT, given “<em class="italic">She went to Paris to visit the masked</em>,” would aim to deduce that “<em class="italic">masked</em>” could be replaced by “<em class="italic">Eiffel Tower</em>,” understanding the broader context of a trip to Paris.</p>
    <p class="normal">BERT’s approach offers a more rounded view of language, capturing the essence of words based on what precedes and follows them, elevating its language comprehension prowess.</p>
    <p class="normal">The key to success when training an LLM lies in combining ‘deep’ and ‘wide’ learning architectures. Think of the ‘deep’ part as a specialist deeply focused on a subject, while the ‘wide’ approach is like a jack of all trades, understanding a bit of everything.</p>
    <h2 id="_idParaDest-434" class="heading-2">Using deep and wide models to create powerful LLMs</h2>
    <p class="normal">LLMs are intricately designed to excel at a rather specific task: predicting the next word in a <a id="_idIndexMarker1235"/>sequence. It might seem simple at first, but to achieve this with high accuracy, models often draw inspiration from certain aspects <a id="_idIndexMarker1236"/>of human learning.</p>
    <p class="normal">The human brain, a marvel of nature, processes information by recognizing and abstracting <a id="_idIndexMarker1237"/>common patterns from the surrounding environment. On top of this foundational understanding, humans then enhance <a id="_idIndexMarker1238"/>their knowledge by memorizing specific instances or exceptions that don’t fit the usual patterns. Think of it as understanding a rule and then learning the outliers to that rule.</p>
    <p class="normal">To infuse machines with this dual-layered learning approach, we need thoughtful machine learning architectures. A rudimentary method might involve training models solely on generalized patterns, sidelining the exceptions. However, to truly excel, especially in tasks like predicting the next word, models must be adept at grasping both the common patterns and the unique exceptions that punctuate a language.</p>
    <p class="normal">While LLMs are not designed to fully emulate human intelligence (which is multifaceted and not solely about predicting sequences), they do borrow from human learning strategies to become proficient at their specific tasks.</p>
    <p class="normal">LLMs are <a id="_idIndexMarker1239"/>designed to understand and generate <a id="_idIndexMarker1240"/>language by detecting <a id="_idIndexMarker1241"/>patterns in vast amounts <a id="_idIndexMarker1242"/>of text data. Consider the following basic linguistic guidelines:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Ancient Egyptian hieroglyphs provide a fascinating example. In this early writing system, a symbol might represent a word, sound, or even a concept. For instance, while a single hieroglyph could denote the word “<em class="italic">river</em>,” a combination of hieroglyphs could convey a deeper meaning like “<em class="italic">the life-giving Nile River</em>.”</li>
      <li class="numberedList">Now, consider how questions are formed. Typically, they begin with auxiliary verbs. However, indirect inquiries, such as “<em class="italic">I wonder if the Nile will flood this year</em>” diverge from this conventional pattern.</li>
    </ol>
    <p class="normal">To effectively predict the next word or phrase in a sequence, LLMs must master both the prevailing language norms and their occasional outliers.</p>
    <h1 id="_idParaDest-435" class="heading-1">Bottom of Form</h1>
    <p class="normal">Thus, combining deep and wide models (<em class="italic">Figure 11.8</em>) has been shown to improve the performance <a id="_idIndexMarker1243"/>of models on a wide range of tasks. Deep models are <a id="_idIndexMarker1244"/>characterized by having many hidden layers and are adept at learning complex relationships between input and output. </p>
    <p class="normal">In contrast, wide models are designed to learn simple patterns in the data. By combining the two, it is possible to capture both the complex relationships and the simple patterns, leading to more robust and flexible models. </p>
    <p class="normal"><img src="../Images/B18046_11_08.png" alt="A diagram of a network  Description automatically generated"/></p>
    <p class="packt_figref">Figure 11.8: Architecture of deep and wide models</p>
    <p class="normal">Incorporating exceptions into the training process is crucial for better generalization of models to new <a id="_idIndexMarker1245"/>and unseen data. For example, a language model that is trained only on data that includes one meaning of a word may struggle to recognize <a id="_idIndexMarker1246"/>other meanings when it encounters them in new data. By incorporating exceptions, the model can learn to recognize multiple meanings of a word, which can improve its performance on a variety of NLP tasks.</p>
    <p class="normal">Deep architectures are typically used for tasks that require learning complex, hierarchical abstract representations of data. The features that exhibit generalizable patterns are called dense features. When we use deep architectures to formulate the rules, we call it learning by generalization. To build a wide and deep network, we connect the sparse features directly to the output node.</p>
    <p class="normal">In the field of machine learning, combining deep and wide models has been identified as an important approach to building more flexible and robust models that can capture both complex relationships and simple patterns in data. Deep models excel at learning complex, hierarchical abstract representations of data, by having many hidden layers, where each layer processes the data and learns different features at different levels of abstraction. In contrast, wide models have a minimum number of hidden layers and are typically used for tasks that require learning simple, non-linear relationships in the data without creating any layer of abstraction. </p>
    <p class="normal">Such patterns are represented through sparse features. When the wide part of the model has one or zero hidden layers, it can be used to memorize the examples and formulate exceptions. Thus, when wide architectures are used to <a id="_idIndexMarker1247"/>formulate rules, we call it learning by <strong class="keyWord">memorization</strong>.</p>
    <p class="normal">The deep and <a id="_idIndexMarker1248"/>wide models can use the deep neural network to <a id="_idIndexMarker1249"/>generalize patterns. Typically, this portion of the model will take lots of time to train. The wide partition and efforts to capture all the exceptions to these generalizations in real time are a part of the constant algorithmic learning process.</p>
    <h1 id="_idParaDest-436" class="heading-1">Summary</h1>
    <p class="normal">In this chapter we discussed advanced sequential models, which are advanced techniques designed to process input sequences, especially when the length of output sequences may differ from that of the input. Autoencoders, a type of neural network architecture, are particularly adept at compressing data. They work by encoding input data into a smaller representation and then decoding it back to resemble the original input. This process can be useful in tasks like image denoising, where noise from an image is filtered out to produce a clearer version.</p>
    <p class="normal">Another influential model is the Seq2Seq model. It’s designed to handle tasks where input and output sequences have varying lengths, making it ideal for applications like machine translation. However, traditional Seq2Seq models face the <strong class="keyWord">information bottleneck</strong> challenge, wherein the entire context of an input sequence needs to be captured in a single, fixed-size representation. Addressing this, the attention mechanism was introduced, allowing models to focus on different parts of the input sequence dynamically. The transformer architecture, introduced in the paper <em class="italic">Attention is All You Need</em>, utilizes this mechanism, revolutionizing the processing of sequence data. Transformers, unlike their predecessors, can attend to all positions in a sequence simultaneously, capturing intricate relationships within the data. This innovation paved the way for LLMs, which have gained prominence for their human-like text-generation capabilities.</p>
    <p class="normal">In the next chapter we will look into how to use recommendation engines.</p>
    <h1 id="_idParaDest-437" class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/WHLel"><span class="url">https://packt.link/WHLel</span></a></p>
    <p class="normal"><img src="../Images/QR_Code1955211820597889031.png" alt="" role="presentation"/></p>
  </div>
</body></html>