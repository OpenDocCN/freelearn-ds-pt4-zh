- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic Strategies for Data Handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the New Oil of the Digital Economy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Wired Magazine
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this data-driven era, the ability to extract meaningful information from
    large data sets is fundamentally shaping our decision-making processes. The algorithms
    we delve into throughout this book lean heavily on this reliance on data. Therefore,
    it becomes important to develop tools, methodologies, and strategic plans aimed
    at creating robust and efficient infrastructures for data storage.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this chapter is data-centric algorithms to efficiently manage data.
    Integral to these algorithms are operations such as efficient storage and data
    compression. By employing such methodologies, data-centric architectures enable
    data management and efficient resource utilization. By the end of this chapter,
    you should be well-equipped to understand the concepts and trade-offs involved
    in designing and implementing various data-centric algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter discusses the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data compression algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first introduce the basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data algorithms are specialized for managing and optimizing data storage. Beyond
    storage, they handle tasks like data compression, ensuring efficient storage space
    utilization, and streamline rapid data retrieval, critical in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'A critical facet in understanding data algorithms, especially in distributed
    systems, is the CAP theorem. Here’s where its significance lies: this theorem
    elucidates the balance among consistency, availability, and partition tolerance.
    In any distributed system, achieving two out of these three guarantees simultaneously
    is all we can hope for. Comprehending CAP’s subtleties aids in discerning the
    challenges and design decisions in modern data algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: In the scope of data governance, these algorithms are invaluable. They assure
    data consistency across all distributed system nodes, ensuring data integrity.
    They also assure efficient data availability and manage data partition tolerance,
    enhancing the system’s resilience and security.
  prefs: []
  type: TYPE_NORMAL
- en: Significance of CAP theorem in context of data algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CAP theorem doesn’t just set theoretical limits; it has practical implications
    in real-world scenarios where data is manipulated, stored, and retrieved. Imagine,
    for instance, a scenario where an algorithm must retrieve data from a distributed
    system. The choices made around consistency, availability, and partition tolerance
    directly impact the efficiency and reliability of that algorithm. If a system
    prioritizes availability, the data might be easily retrievable but may not be
    the most up-to-date version. Conversely, a system prioritizing consistency might
    sometimes delay data retrieval to ensure that only the most recent data is accessed.
  prefs: []
  type: TYPE_NORMAL
- en: The data-centric algorithms we discuss here are, in many ways, influenced by
    these CAP constraints. By intertwining our understanding of CAP theorem with data
    algorithms, we can make more informed decisions when dealing with data challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Storage in distributed environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Single-node architecture is effective for smaller data sets. However, with the
    surge in dataset sizes, distributed environment storage has become standard for
    large scale problems. Identifying the right strategy for data storage in such
    environments depends on various factors, including the nature of the data and
    anticipated usage patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The CAP theorem provides a foundational principle for developing these storage
    strategies, helping us tackle challenges linked with managing expansive data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting CAP theorem and data compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might initially seem there’s little overlap between the CAP theorem and data
    compression. But consider the practical implications. If we prioritize consistency
    in our system (as per CAP considerations), our data compression methods would
    need to ensure that data remains consistently compressed across all nodes. In
    a system where availability takes precedence, the compression method might be
    optimized for speed, even if it leads to minor inconsistencies. This interplay
    highlights that our choices around CAP influence even how we compress and retrieve
    our data, demonstrating the theorem’s pervasive influence in data-centric algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the CAP theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 1998, Eric Brewer proposed a theorem that later became famous as the CAP
    theorem. It highlights the various trade-offs involved in designing a distributed
    service system. To understand the CAP theorem, first, let’s define the following
    three characteristics of distributed service systems: consistency, availability,
    and partition tolerance. CAP is, in fact, an acronym made up of these three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency** (or simply **C**): The distributed service consists of various
    nodes. Any of these nodes can be used to read, write, or update records in the
    data repository. Consistency guarantees that at a certain time, t1, independent
    of which node we use to read the data, we will get the same result. Every read
    operation either returns the latest data that is consistent across the distributed
    repository or gives an error message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability** (or simply **A**): In the area of distributed systems, availability
    means that the system as a whole always responds to requests. This ensures that
    users get a reply every time they query the system, even if it might not always
    be the latest piece of data. So, instead of focusing on every single node being
    up-to-date, the emphasis is on the entire system being responsive. It’s about
    guaranteeing that a user’s request never goes unanswered, even if some parts of
    the system have outdated information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition Tolerance** (or simply **P**): In a distributed system, multiple
    nodes are connected via a communication network. Partition tolerance guarantees
    that, in the event of communication failure between a small subset of nodes (one
    or more), the system remains operational. Note that to guarantee partition tolerance,
    data needs to be replicated across a sufficient number of nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these three characteristics, the CAP theorem carefully summarizes the
    trade-offs involved in the architecture and design of a distributed service system.
    Specifically, the CAP theorem states that, in a distributed storage system, we
    can only have two of the following characteristics: consistency or **C**, availability
    or **A**, and partition tolerance or **P**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, venn diagram  Description automatically generated](img/B18046_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Visualizing the choices in distributed systems: the CAP theorem'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed data storage is increasingly becoming an essential component of
    modern IT infrastructure. Designing distributed data storage should be carefully
    considered, based on the characteristics of the data and the requirements of the
    problem we want to solve. When applied to distributed databases, the CAP theorem
    helps to guide the design and decision-making process by ensuring that developers
    and architects understand the fundamental trade-offs and limitations involved
    in creating distributed database systems. Balancing these three characteristics
    is crucial to achieve the desired performance, reliability, and scalability of
    the distributed database system. When applied to distributed databases, the CAP
    theorem helps to guide the design and decision-making process by ensuring that
    developers and architects understand the fundamental trade-offs. Balancing these
    three characteristics is crucial in order to achieve the desired performance,
    reliability, and scalability of the distributed database system. In the context
    of the CAP theorem, we can assume that there are three types of distributed storage
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: A **CA** system (implementing Consistency-Availability)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **AP** system (implementing Availability-Partition Tolerance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **CP** system (implementing Consistency-Partition Tolerance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data storage into **CA**, **AP**, and **CP** systems helps us to
    understand the various trade-offs involved when designing data storage systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look into them, one by one.
  prefs: []
  type: TYPE_NORMAL
- en: CA systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional single-node systems are CA systems. In non-distributed systems,
    partition tolerance is not a concern as there is no need to manage communication
    between multiple nodes. As a result, these systems can focus on maintaining both
    consistency and availability. In other words, they are CA systems.
  prefs: []
  type: TYPE_NORMAL
- en: A system can function without partition tolerance by storing and processing
    data on a single node or server. While this approach may not be suitable for handling
    large-scale data sets or high-velocity data streams, it can be effective for smaller
    data sizes or applications with less demanding performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional single-node databases, such as Oracle or MySQL, are prime examples
    of CA systems. These systems are well-suited for use cases where data volume and
    velocity are relatively low, and partition tolerance is not a critical factor.
    Examples include small to medium-sized businesses, academic projects, or applications
    with a limited number of users and data sources.
  prefs: []
  type: TYPE_NORMAL
- en: AP systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AP systems are distributed storage systems designed to prioritize availability
    and partition tolerance, even at the expense of consistency. These highly responsive
    systems can sacrifice consistency, if necessary, to accommodate high-velocity
    data. In doing so, these distributed storage systems can handle user requests
    immediately, even if it results in temporarily serving slightly outdated or inconsistent
    data across different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: When consistency is sacrificed in AP systems, users might occasionally may get
    slightly outdated information. In some cases, this temporary inconsistency is
    an acceptable trade-off, as the ability to quickly process user requests and maintain
    high availability is deemed more critical than strict data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Typical AP systems are used in real-time monitoring systems, such as sensor
    networks. High-velocity distributed databases, like Cassandra, are prime examples
    of AP systems.
  prefs: []
  type: TYPE_NORMAL
- en: An AP system is recommended for implementing distributed data storage in scenarios
    where high availability, responsiveness, and partition tolerance are essential.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if Transport Canada wants to monitor traffic on one of the highways
    in Ottawa through a network of sensors installed at different locations on the
    highway, an AP system would be the preferred choice. In this context, prioritizing
    real-time data processing and availability is crucial to ensuring that traffic
    monitoring can function effectively, even in the presence of network partitions
    or temporary inconsistencies. This is why an AP system is often the recommended
    choice for such applications, despite the potential trade-off of sacrificing consistency.
  prefs: []
  type: TYPE_NORMAL
- en: CP systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CP systems prioritize both consistency and partition tolerance, ensuring that
    distributed storage systems guarantee consistency before a read process retrieves
    a value. These systems are specifically designed to maintain data consistency
    and continue operating effectively even in the presence of network partitions.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal data type for CP systems is data that requires strict consistency
    and accuracy, even if it means sacrificing the immediate availability of the system.
    Examples include financial transactions, inventory management, and critical business
    operations data. In these cases, ensuring that the data remains consistent and
    accurate across the distributed environment is of paramount importance.
  prefs: []
  type: TYPE_NORMAL
- en: A typical use case for CP systems is when we want to store document files in
    JSON format. Document datastores, such as MongoDB, are CP systems tuned for consistency
    in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: With an understanding of the different types of distributed storage systems,
    we can now move on to exploring data compression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding data compression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data compression is an essential methodology used for data storage. It not only
    enhances storage efficiency and minimizes data transmission times, but it also
    has significant implications for cost savings and performance improvements, particularly
    in the realm of big data and cloud computing. This section presents the details
    data compression techniques, with a special focus on the lossless algorithms Huffman
    and LZ77, and their influence on modern compression schemes, such as Gzip, LZO,
    and Snappy.
  prefs: []
  type: TYPE_NORMAL
- en: Lossless compression techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lossless compression revolves around eliminating redundancy in data to minimize
    storage needs while ensuring perfect reversibility. Huffman and LZ77 are two foundational
    algorithms that have strongly influenced the field.
  prefs: []
  type: TYPE_NORMAL
- en: Huffman coding focuses on variable-length coding, representing frequent characters
    with fewer bits, while LZ77, a dictionary-based algorithm, exploits repeated data
    sequences and represents them with shorter references. Let us look into them one
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Huffman coding: Implementing variable-length coding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Huffman coding, a form of entropy encoding, is used widely in lossless data
    compression. The key principle underlying Huffman coding is to assign shorter
    codes to more frequently occurring characters in a dataset, thereby reducing the
    overall data size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm uses a specific type of binary tree known as a Huffman tree,
    where each leaf node corresponds to a data element. The frequency of occurrence
    of the element determines the placement in the tree: more frequent elements are
    placed closer to the root. This strategy ensures that the most common elements
    have the shortest codes.'
  prefs: []
  type: TYPE_NORMAL
- en: A quick example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine we have data containing letters **A**, **B**, and **C** with frequencies
    `5`, `9`, and `12` respectively. In Huffman coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**C**, being the most frequent, might get a short code like `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B**, the next frequent, could get `10`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A**, the least frequent, might have `11`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand it fully, let us go through an example in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Huffman coding in Python
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We start by creating a node for each character, where the node contains the
    character and its frequency. These nodes are then added to a priority queue, with
    the least frequent elements having the highest priority. For this, we create a
    `Node` class to represent each character in the Huffman tree. Each `Node` object
    contains the character, its frequency, and pointers to its left and right children.
    The `__lt__` method is defined to compare two `Node` objects based on their frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build the Huffman tree. The construction of a Huffman tree involves
    a series of insertions and deletions in a priority queue, typically implemented
    as a binary heap. To build the Huffman tree, we create a min-heap of Node objects.
    A min-heap is a specialized tree-based structure that satisfies a simple but important
    condition: the parent node has a value less than or equal to its children. This
    property ensures that the smallest element is always at the root, making it efficient
    for priority operations. We repeatedly pop the two nodes with the lowest frequencies,
    merge them, and push the merged node back into the heap. This process continues
    until there is only one node left, which becomes the root of the Huffman tree.
    The tree can be built by `build_tree` function, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the Huffman tree is constructed, we can generate the Huffman codes by traversing
    the tree. Starting from the root, we append a `0` for every left branch we follow
    and a `1` for every right branch. When we reach a leaf node, the sequence of `0`s
    and `1`s accumulated along the path from the root forms the Huffman code for the
    character at that leaf node. This functionality is achieved by creating `generate_codes`
    function as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now let us use the Huffman tree. Let us first define data that we will use for
    Huffman’s encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then, we print out the Huffman codes for each character.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can infer the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed length code**: The fixed-length code for this table is `3`. This is
    because, with six characters, a fixed-length binary representation would need
    a maximum of three bits (2³ = 8 possible combinations, which can cover our 6 characters).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable length code**: The variable-length code for this table is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`45(1) + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following diagram shows the Huffman tree created from the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18046_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: The Huffman tree: visualizing the vompression process'
  prefs: []
  type: TYPE_NORMAL
- en: Note that Huffman encoding is about converting data into a Huffman tree that
    enables compression. Decoding or decompression brings the data back to the original
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at Huffman’s encoding, let us now explore another lossless compression
    technique based on dictionary-based compression.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us discuss dictionary-based compression.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dictionary-based compression LZ77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LZ77 belongs to a family of compression algorithms known as dictionary coders.
    Rather than maintaining a static dictionary of code words, as in Huffman coding,
    LZ77 dynamically builds a dictionary of substrings seen in the input data. This
    dictionary isn’t stored separately but is implicitly referred to as a sliding
    window over the already-encoded input, facilitating an elegant and efficient method
    of representing repeating sequences.
  prefs: []
  type: TYPE_NORMAL
- en: The LZ77 algorithm operates on the principle of replacing repeated occurrences
    of data with references to a single copy. It maintains a “sliding window” of recently
    processed data. When it encounters a substring that has occurred before, it doesn’t
    store the actual substring; instead, it stores a pair of values – the distance
    to the start of the repeated substring in the sliding window, and the length of
    the repeated substring.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding with an example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine a scenario where you’re reading the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data_string = "ABABCABABD"`'
  prefs: []
  type: TYPE_NORMAL
- en: Now, as you process this string left to right, when you encounter the substring
    "`CABAB`", you’ll notice that "`ABAB`" has appeared before, right after the initial
    "`AB`". LZ77 takes advantage of such repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of writing "`ABAB`" again, LZ77 would suggest: “Hey, look back two
    characters and copy the next two characters!” In technical terms, this is a reference
    back of two characters with a length of two characters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, compressing our `data_string` using LZ77, it might look something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ABABC<2,2>D`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `<2,2>` is the LZ77 notation, indicating “go back by two characters and
    copy the next two.”
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Huffman
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To appreciate the power and differences between LZ77 and Huffman, it’s helpful
    to use the same data. Let’s stick with our `data_string = "ABABCABABD"`.
  prefs: []
  type: TYPE_NORMAL
- en: While LZ77 identifies repeated sequences in data and references them, Huffman
    encoding is more about representing frequent characters with shorter codes.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you were to compress our `data_string` using Huffman, you might
    see certain characters, say '`A`' and '`B`', that are more frequent, represented
    with shorter binary codes than the less frequent '`C`' and '`D`'.
  prefs: []
  type: TYPE_NORMAL
- en: This comparison showcases that while Huffman is all about frequency-based representation,
    LZ77 is about spotting and referencing patterns. Depending on the data type and
    structure, one might be more efficient than the other.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced lossless compression formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principles laid out by Huffman and LZ77 have given rise to advanced compression
    formats. We will look into three advanced formats in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: LZO
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Snappy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: gzip
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us look into them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'LZO compression: Prioritizing speed'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LZO is a lossless data compression algorithm that emphasizes rapid compression
    and decompression. It replaces repeated occurrences of data with references to
    a single copy. After this initial pass of LZ77 compression, the data is then passed
    through a Huffman coding stage.
  prefs: []
  type: TYPE_NORMAL
- en: While its compression ratio might not be the highest, its processing speed is
    significantly faster than many other algorithms. This makes LZO an excellent choice
    for situations where quick data access is a priority, such as real-time data processing
    and streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Snappy compression: Striking a balance'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Snappy is another fast compression and decompression library originally developed
    by Google. The primary focus of Snappy is to achieve high speeds and reasonable
    compression, but not necessarily the maximum compression.
  prefs: []
  type: TYPE_NORMAL
- en: Snappy’s compression method is based on LZ77 but with a focus on speed and without
    an additional entropy encoding step like Huffman coding. Instead, Snappy utilizes
    a much simpler encoding algorithm that ensures speedy compressions and decompressions.
    The algorithm uses a copy-based strategy where it searches for repeated sequences
    in the data and then encodes them as a length and a reference to the previous
    location.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that due to this tradeoff for speed, Snappy does not compress
    data as efficiently as algorithms that use Huffman coding or other forms of entropy
    encoding. However, in use-cases where speed is more critical than the compression
    ratio, Snappy can be a very effective choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'GZIP compression: Maximizing storage efficiency'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`GZIP` is a file format and a software application used for file compression
    and decompression. The `GZIP` data format uses a combination of the LZ77 algorithm
    and Huffman coding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practical example: Data management in AWS: A focus on CAP theorem and compression
    algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us consider an example of a global e-commerce platform that runs on multiple
    cloud servers across the world. This platform handles thousands of transactions
    every second, and the data generated from these transactions needs to be stored
    and processed efficiently. We’ll see how the CAP theorem and compression algorithms
    can guide the design of the platform’s data management system.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Applying the CAP theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CAP theorem states that a distributed data store cannot simultaneously
    provide more than two out of the following three guarantees: consistency, availability,
    and partition tolerance.'
  prefs: []
  type: TYPE_NORMAL
- en: In our e-commerce platform scenario, availability and partition tolerance might
    be prioritized. High availability ensures that the system can continue processing
    transactions even if a few servers fail. Partition tolerance means the system
    can still function even if network failures cause some of the servers to be isolated.
  prefs: []
  type: TYPE_NORMAL
- en: While this means the system may not always provide strong consistency (every
    read receives the most recent write), it could use eventual consistency (updates
    propagate through the system and eventually all replicas show the same value)
    to ensure a good user experience. In practice, slight inconsistencies might be
    acceptable, for example, when it takes a few seconds for a user’s shopping cart
    to update across all devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the AWS ecosystem, we have a variety of data storage services that can be
    chosen based on the needs defined by the CAP theorem. For our e-commerce platform,
    we would prefer availability and partition tolerance over consistency. Amazon
    DynamoDB, a key-value NoSQL database, would be an excellent fit. It offers built-in
    support for multi-region replication and automatic sharding, ensuring high availability
    and partition tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: For consistency, DynamoDB offers “eventual consistency” and “strong consistency”
    options. In our case, we would opt for eventual consistency to prioritize availability
    and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Using compression algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The platform would generate vast amounts of data, including transaction details,
    user behavior logs, and product information. Storing and transferring this data
    could be costly and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Here, compression algorithms like gzip, Snappy, or LZO can help. For instance,
    the platform might use gzip to compress transaction logs that are archived for
    long-term storage. Given that gzip can typically compress text files to about
    30% of their original size, this could reduce storage costs significantly.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, for real-time analytics on user behavior data, the platform
    might use Snappy or LZO. While these algorithms may not compress data as much
    as gzip, they are faster and would allow the analytics system to process data
    more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: AWS provides various ways to implement compression depending on the type and
    use of data. For compressing transaction logs for long-term storage, we could
    use Amazon S3 (Simple Storage Service) coupled with gzip compression. S3 supports
    automatic gzip compression for files being uploaded, which can significantly reduce
    storage costs. For real-time analytics on user behavior data, we could use Amazon
    Kinesis Data Streams with Snappy or LZO compression. Kinesis can capture, process,
    and store data streams for real-time analytics, and supports compression to handle
    high-volume data.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Quantifying the benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The benefits can be quantified similarly as described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a practical example to demonstrate potential cost savings. Imagine
    our platform produces 1 TB of transaction logs daily. By leveraging gzip compression
    with S3, we can potentially shrink the storage requirement to roughly 300 GB.
    As of August 2023, S3 charges around $0.023 for every GB up to the initial 50
    TB monthly. Doing the math, this equates to a saving of about $485 each month,
    or a significant $5,820 annually, just from the log storage. It’s worth noting
    that the cited AWS pricing is illustrative and specific to August 2023; be sure
    to check current rates as they might vary.
  prefs: []
  type: TYPE_NORMAL
- en: Using Snappy or LZO with Kinesis for real-time analytics could improve data
    processing speed. This could lead to more timely and personalized user recommendations,
    potentially increasing sales. The financial gain could be calculated based on
    the increase in conversion rate attributed to improved recommendation speed.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, by using DynamoDB and adhering to the CAP theorem, we ensure a smooth
    shopping experience for our users even in the event of network partitions or individual
    server failures. The value of this choice could be reflected in the platform’s
    user retention rate and overall customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we examined the design of data-centric algorithms, concentrating
    on three key components: data storage, data governance and data compression. We
    investigated the various issues related to data governance. We analyzed how the
    distinct attributes of data influence the architectural decisions for data storage.
    We investigated different data compression algorithms, each providing specific
    advantages in terms of efficiency and performance. In the next chapter, we will
    look at cryptographic algorithms. We will learn how we can use the power of these
    algorithms to secure exchanged and stored messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask questions to the author, and learn about new releases – follow the QR code
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/WHLel](https://packt.link/WHLel)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1955211820597889031.png)'
  prefs: []
  type: TYPE_IMG
