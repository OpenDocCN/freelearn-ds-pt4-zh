<html><head></head><body>
<div id="_idContainer062">
<h1 class="chapter-number" id="_idParaDest-23"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-24"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.2.1">Using Machine Learning in Business Operations</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Machine learning</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.4.1"> is an area of research focused on comprehending and developing “learning” processes, or processes that use data to enhance performance on a given set of tasks. </span><span class="koboSpan" id="kobo.4.2">It is considered to be a component of artificial intelligence. </span><span class="koboSpan" id="kobo.4.3">Among them, machine learning is a technology that enables companies to efficiently extract knowledge from unstructured data. </span><span class="koboSpan" id="kobo.4.4">With little to no programming, machine learning—and more precisely, machine learning algorithms—can be used to iteratively learn from a given dataset and comprehend patterns, behaviors, and </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">In this chapter, we will learn how to do </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.8.1">Validate the difference of observed effects with </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">statistical analysis</span></span></li>
<li><span class="koboSpan" id="kobo.10.1">Analyze the correlation and causation as well as model relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">between variables</span></span></li>
<li><span class="koboSpan" id="kobo.12.1">Prepare the data for clustering and machine </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">learning models</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Develop machine learning models for regression </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">and classification</span></span><a id="_idTextAnchor129"/></li>
</ul>
<h1 id="_idParaDest-25"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.16.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.17.1">In order to be able to follow the steps in this chapter, you will need to meet the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">next requirements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.19.1">Have a Jupyter notebook instance running Python 3.7 and above. </span><span class="koboSpan" id="kobo.19.2">You can use the Google Colab notebook to run the steps as well if you have a Google </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">Drive account.</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">Have an understanding of basic math and </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">statistical concepts.</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Download the example datasets provided in the book’s GitHub page, and the original source </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">is </span></span><a href="https://python.cogsci.nl/numerical/statistics/"><span class="No-Break"><span class="koboSpan" id="kobo.25.1">https://python.cogsci.nl/numerical/statistics/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.26.1">.</span></span><a id="_idTextAnchor131"/></li>
</ul>
<h1 id="_idParaDest-26"><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.27.1">Validating the effect of changes with the t-test</span></h1>
<p><span class="koboSpan" id="kobo.28.1">When measuring the effects of certain actions applied to a given population of users, we need to validate that these actions have actually affected the target groups in a significant manner. </span><span class="koboSpan" id="kobo.28.2">To be able to do this, we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">the t-test.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">A t-test</span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.31.1"> is a statistical test that is used to compare the means of two groups to ascertain whether a method or treatment has an impact on the population of interest or whether two groups differ from one another; it is frequently employed in </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">hypothesis testing.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">When the </span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.34.1">datasets in the two groups don’t relate to identical values, separate t-test samples are chosen independently of one another. </span><span class="koboSpan" id="kobo.34.2">They might consist of two groups of randomly selected, unrelated patients to study the effects of a medication, for example. </span><span class="koboSpan" id="kobo.34.3">While the other group receives the prescribed treatment, one of the groups serves as the control group and is given a placebo. </span><span class="koboSpan" id="kobo.34.4">This results in two separate sample sets that are unpaired and unconnected from one another. </span><span class="koboSpan" id="kobo.34.5">Simply put, the t-test is employed to compare the means of two groups. </span><span class="koboSpan" id="kobo.34.6">It is frequently employed in hypothesis testing to establish whether a procedure or treatment truly affects the population of interest or whether two groups differ from </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">one another.</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">The t-test is used in the context of businesses to compare two different means and determine whether they represent the exact same population, and it’s especially useful in validating the effects of promotions applied in the uplift of sales. </span><span class="koboSpan" id="kobo.36.2">Additionally, it enables firms to comprehend the likelihood that their outcomes are the product </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">of chance.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">We will learn how to make an independent-samples t-test using the SciPy package and the Matzke et al. </span><span class="koboSpan" id="kobo.38.2">dataset (2015). </span><span class="koboSpan" id="kobo.38.3">Participants in this dataset underwent a memory challenge in which they had to recollect a list of words. </span><span class="koboSpan" id="kobo.38.4">One group of participants focused on a central fixation dot on a display during the retention interval. </span><span class="koboSpan" id="kobo.38.5">Another group of volunteers continuously moved their eyes horizontally, which some people think helps </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">with memory.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">To determine whether memory performance (</span><strong class="source-inline"><span class="koboSpan" id="kobo.41.1">CriticalRecall</span></strong><span class="koboSpan" id="kobo.42.1">) was better for the horizontal eye movement group than the fixation group, we can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.43.1">ttest_ind</span></strong><span class="koboSpan" id="kobo.44.1"> function from the </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">SciPy librar</span><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.46.1">y:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.47.1">
from scipy.stats import ttest_ind
import pandas as </span><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.48.1">pd
dm = pd.read_csv('matzke_et_al.csv</span><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.49.1">')
dm_horizontal = dm[dm.Condition=='Horizontal</span><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.50.1">']
dm_fixation = dm[dm.Condition=='Fixation</span><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.51.1">']
t, p = ttest_ind(dm_horizontal.CriticalRecall, dm_fixation.CriticalRecal</span><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.52.1">l)
print('t = {:.3f}, p = {:.3f}'.format(t, p))</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.53.1"><img alt="Figure 2.1: T-test result " src="image/B19026_02_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.54.1">Figure 2.1: T-test result</span></p>
<p><span class="koboSpan" id="kobo.55.1">The test’s p-value, which</span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.56.1"> may be</span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.57.1"> found on the output, is all you need to evaluate the t-test findings. </span><span class="koboSpan" id="kobo.57.2">Simply compare the output’s p-value to the selected alpha level to conduct a hypothesis test at the desired alpha (</span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">significance) level:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.59.1">
import seaborn as sns
import matplotlib.pyplot as plt # visualization
sns.barplot(x='Condition', y='CriticalRecall', data=dm)
plt.xlabel('Condition')
plt.ylabel('Memory performance')
plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.60.1">You can reject the null hypothesis if the p-value is less than your threshold for significance (for example, 0.05). </span><span class="koboSpan" id="kobo.60.2">The two means’ difference is statistically significant. </span><span class="koboSpan" id="kobo.60.3">The data from your sample is convincing enough to support the conclusion that the two population means are </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">not equal:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.62.1"><img alt="Figure 2.2: Population distribution " src="image/B19026_02_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.63.1">Figure 2.2: Population distribution</span></p>
<p><span class="koboSpan" id="kobo.64.1">A high t-score, also known as a </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.65.1">t-value, denotes that the groups are distinct, whereas a low t-score denotes similarity. </span><span class="koboSpan" id="kobo.65.2">Degrees of freedom, or the values in a study that can fluctuate, are crucial for determining the significance and veracity of the </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">null hypothesis.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">In our example, the results indicate a noteworthy difference (p =.0066). </span><span class="koboSpan" id="kobo.67.2">The fixation group, however, outperformed the other groups, where the effect is in the opposite direction from what </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">was anticipated.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">Another</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.70.1"> way to test the difference between two populations is using the paired-samples t-test, which compares a single group’s means for two variables. </span><span class="koboSpan" id="kobo.70.2">To determine whether the average deviates from 0, the process computes the differences between the values of the two variables for each occurrence. </span><span class="koboSpan" id="kobo.70.3">The means of two independent or unrelated groups are compared using an </span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.71.1">unpaired t-test. </span><span class="koboSpan" id="kobo.71.2">An unpaired t-test makes the assumption that the variance in the groups is equal. </span><span class="koboSpan" id="kobo.71.3">The variance is not expected to be equal in a</span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.72.1"> paired t-test. </span><span class="koboSpan" id="kobo.72.2">The process also automates the calculation of the t-test effect size. </span><span class="koboSpan" id="kobo.72.3">The paired t-test is used when data are in the form of matched pairs, while the two-sample t-test is used when data from two samples are </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">statistically independent.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">Let’s use the Moore, McCabe, and Craig datasets. </span><span class="koboSpan" id="kobo.74.2">Here, aggressive conduct in dementia patients was assessed during the full moon and another lunar phase. </span><span class="koboSpan" id="kobo.74.3">This was a within-subject design because measurements were taken from every participant at </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">both times.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">You can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.77.1">ttest_rel</span></strong><span class="koboSpan" id="kobo.78.1"> SciPy function to test whether aggression differed between the full moon and the other </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">lunar </span><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.80.1">phase:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.81.1">
from scipy.stats import tte</span><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.82.1">st_rel
dm = pd.read_csv('moon-aggression</span><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.83.1">.csv')
t, p = ttest_rel(dm.Moon, dm.</span><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.84.1">Other)
print('t = {:.3f}, p = {:.3f}'.format(t, p))</span></pre>
<p><span class="koboSpan" id="kobo.85.1">As you</span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.86.1"> can see in the figure below, there was an interesting effect that was substantial, as the p values are never 0 as the output implies. </span><span class="koboSpan" id="kobo.86.2">This effect was such that people were indeed most violent during </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">full </span><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.88.1">moons:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.89.1"><img alt="Figure 2.3: T-test result of the aggression dataset " src="image/B19026_02_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.90.1">Figure 2.3: T-test result of the aggression dataset</span></p>
<p><span class="koboSpan" id="kobo.91.1">Another way in which we can compare the difference between two groups is the statistical method known as</span><a id="_idIndexMarker070"/> <strong class="bold"><span class="koboSpan" id="kobo.92.1">analysis of variance</span></strong><span class="koboSpan" id="kobo.93.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.94.1">ANOVA</span></strong><span class="koboSpan" id="kobo.95.1">), which is used to examine how different means differ from one another. </span><span class="koboSpan" id="kobo.95.2">Ronald Fisher created this statistical test in 1918, and it has been in use ever since. </span><span class="koboSpan" id="kobo.95.3">Simply put, an ANOVA analysis determines whether the means of three or more independent groups differ statistically. </span><span class="koboSpan" id="kobo.95.4">So does ANOVA replace the t-test, then? </span><span class="koboSpan" id="kobo.95.5">Not really. </span><span class="koboSpan" id="kobo.95.6">ANOVA is used to compare the means among three or more groups, while the t-test is used to compare the means between </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">two groups.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">When employed in a business setting, ANOVA</span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.98.1"> can be used to manage budgets by, for instance, comparing your budget against costs to manage revenue and inventories. </span><span class="koboSpan" id="kobo.98.2">ANOVA can also be used to manage budgets by, for instance, comparing your budget against costs to manage revenue and inventories. </span><span class="koboSpan" id="kobo.98.3">For example, in order to better understand how sales will perform in the future, ANOVA can also be used to forecast trends by examining data patterns. </span><span class="koboSpan" id="kobo.98.4">When assessing the multi-item scales used frequently in market research, ANOVA is </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.99.1">especially helpful. </span><span class="koboSpan" id="kobo.99.2">Using ANOVA might assist you as a market researcher in comprehending how various groups react. </span><span class="koboSpan" id="kobo.99.3">You can start the test by accepting the null hypothesis, or that the means of all the groups that were observed </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">are equal.</span></span></p>
<p><span class="koboSpan" id="kobo.101.1">For our next example, let’s revisit the heart rate information provided by Moore, McCabe, and Craig. </span><span class="koboSpan" id="kobo.101.2">Gender and group are two subject-specific factors in this dataset, along with one dependent variable (heart rate). </span><span class="koboSpan" id="kobo.101.3">You need the following code to see if gender, group, or their interactions have an impact on </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">heart rate.</span></span></p>
<p><span class="koboSpan" id="kobo.103.1">We will use a combination of </span><strong class="bold"><span class="koboSpan" id="kobo.104.1">ordinary least squares</span></strong><span class="koboSpan" id="kobo.105.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.106.1">OLS</span></strong><span class="koboSpan" id="kobo.107.1">) and an ANOVA test (</span><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">anova_lm</span></strong><span class="koboSpan" id="kobo.109.1">), which isn’t very elegant, but </span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.110.1">the important part is </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">th</span><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.112.1">e formula:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.113.1">
from statsmodels.stats.anova impor</span><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.114.1">t anova_lm
dm = pd.read_csv('heart</span><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.115.1">rate.csv')
dm = dm.rename({'Heart Rate':'HeartRate'},axis=1)  # statsmodels doesn't l</span><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.116.1">ike spaces
df = anova_lm(ols('HeartRate ~ Gender * Group', data=</span><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.117.1">dm).fit())
print(df)</span></pre>
<p><span class="koboSpan" id="kobo.118.1">The results</span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.119.1"> show us that heart rate is related to all factors: gender (F = 185.980, p &lt; .001), group (F = 695.647, p &lt; .001), and the gender-by-group interaction (F = 7.409, p = .</span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">006).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.121.1"><img alt="Figure 2.4: ANOVA test results " src="image/B19026_02_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.122.1">Figure 2.4: ANOVA test results</span></p>
<p><span class="koboSpan" id="kobo.123.1">Now that we have validated that there is in fact difference between multiple groups, we can start to model </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">these rela</span><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.125.1">tionships.</span></span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.126.1">Modeling relationships with multiple linear regression</span></h2>
<p><span class="koboSpan" id="kobo.127.1">The</span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.128.1"> statistical method </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.129.1">known as multiple linear regression employs two or more independent variables to forecast the results of a dependent variable. </span><span class="koboSpan" id="kobo.129.2">Using this method, analysts may calculate the model’s variance and the relative contributions of each independent variable to the overall variance. </span><span class="koboSpan" id="kobo.129.3">Regressions involving numerous explanatory variables, both linear and nonlinear, fall under the category of </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">multiple regression.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">The purpose of multiple regression analysis is so that researchers can evaluate the strength of the relationship between an outcome (the dependent variable) and a number of predictor variables, as well as the significance of each predictor to the relationship using multiple regression analysis frequently with the effect of other predictors </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">statistically eliminated.</span></span></p>
<p><span class="koboSpan" id="kobo.133.1">Multiple regression includes multiple independent variables, whereas linear regression only takes into account one independent variable to affect the </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">relationship’s slope.</span></span></p>
<p><span class="koboSpan" id="kobo.135.1">Businesses can use linear regressions to analyze trends and generate estimates or forecasts. </span><span class="koboSpan" id="kobo.135.2">For instance, if a firm’s sales have been rising gradually each month for the previous several years, the corporation may anticipate sales in the months to come by doing a linear analysis of the sales data with </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">monthly sales.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">Let’s use the dataset from Moore, McCabe, and Craig, which contains grade point averages and SAT scores for mathematics and verbal knowledge for high-school students. </span><span class="koboSpan" id="kobo.137.2">We can use the following code to test whether </span><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">satm</span></strong><span class="koboSpan" id="kobo.139.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">satv</span></strong><span class="koboSpan" id="kobo.141.1"> are (uniquely) related </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">gpa</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">OLS</span></strong><span class="koboSpan" id="kobo.147.1"> SciPy function to evaluate this relationship, which is passed as a combination of the variables in question, and then fitted </span><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.148.1">to </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.150.1">
from statsmodels.formula.ap</span><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.151.1">i import ols
dm = pd.read_cs</span><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.152.1">v('gpa.csv')
model = ols('gpa ~ satm + satv', da</span><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.153.1">ta=dm).fit()
print(model.su</span><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.154.1">mmary())</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.155.1"><img alt="Figure 2.5: OLS results " src="image/B19026_02_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.156.1">Figure 2.5: OLS results</span></p>
<p><span class="koboSpan" id="kobo.157.1">The result </span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.158.1">shows us that only</span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.159.1"> SAT scores for mathematics, but not for verbal knowledge, are uniquely related to the grade </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">point average.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">In the next section, we will look at the concepts of correlation, which is when variables behave in a similar manner, and causation, which is when a variable affects</span><a id="_idTextAnchor156"/> <span class="No-Break"><span class="koboSpan" id="kobo.162.1">another one.</span></span></p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.163.1">Establishing correlation and causation</span></h1>
<p><span class="koboSpan" id="kobo.164.1">The statistical measure known as correlation</span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.165.1"> expresses how closely two variables are related linearly, which can be understood graphically as how close two curves overlap. </span><span class="koboSpan" id="kobo.165.2">It’s a typical technique for describing straightforward connections without explicitly stating cause </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">and consequence.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">The correlation matrix</span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.168.1"> displays the correlation values, which quantify how closely each pair of variables is related linearly. </span><span class="koboSpan" id="kobo.168.2">The correlation coefficients have a range of -1 to +1. </span><span class="koboSpan" id="kobo.168.3">The correlation value is positive if the two variables tend to rise and </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">fall together.</span></span></p>
<p><span class="koboSpan" id="kobo.170.1">The four types of correlations that are typically measured in statistics are the Spearman correlation, Pearson correlation, Kendall rank correlation, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">point-biserial correlation.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">In order for organizations to make data-driven decisions based on forecasting the result of events, correlation and regression analysis are used to foresee future outcomes. </span><span class="koboSpan" id="kobo.172.2">The two main advantages of correlation analysis are that it enables quick hypothesis testing and assists businesses in deciding which variables they wish to look into further. </span><span class="koboSpan" id="kobo.172.3">To determine the strength of the linear relationship between two variables, the primary type of correlation analysis applies Pearson’s </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">r</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.174.1"> formula.</span></span></p>
<p><span class="koboSpan" id="kobo.175.1">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">corr</span></strong><span class="koboSpan" id="kobo.177.1"> method</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.178.1"> in a pandas data frame, we can calculate the pairwise correlation of columns while removing NA/null values. </span><span class="koboSpan" id="kobo.178.2">The technique can be passed as a parameter with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">pearson</span></strong><span class="koboSpan" id="kobo.180.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.181.1">kendall</span></strong><span class="koboSpan" id="kobo.182.1"> values for the standard correlation coefficient, </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">spearman</span></strong><span class="koboSpan" id="kobo.184.1"> for the Spearman rank correlation, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">kendall</span></strong><span class="koboSpan" id="kobo.186.1"> for the Kendall Tau </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">correlation coefficient.</span></span></p>
<p><span class="koboSpan" id="kobo.188.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">corr</span></strong><span class="koboSpan" id="kobo.190.1"> method in a pandas data frame returns a matrix of floats from 1 along the diagonals and symmetric regardless of the </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">call</span><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.192.1">able’s behavior:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.193.1">
im</span><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.194.1">port numpy as np
imp</span><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.195.1">ort pandas as pd
df = pd.DataFrame([(.2, .3,.8), (.0, .6,.9), (.6, .0,.</span><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.196.1">4), (.2, .1,.9),(.1, .3,.7), (.1, .5,.6), (.7, .1,.5), (.3, .0,.8),],columns=['dogs', </span><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.197.1">'cats','birds'])
corr</span><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.198.1">_mat = df.corr()</span></pre>
<p><span class="koboSpan" id="kobo.199.1">We can plot the results correlation</span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.200.1"> matrix using a</span><a id="_idTextAnchor164"/> <span class="No-Break"><span class="koboSpan" id="kobo.201.1">seaborn heatmap:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.202.1">
imp</span><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.203.1">ort seaborn as sn
sn.heatmap(corr</span><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.204.1">_mat</span><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.205.1">, annot=True)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.206.1"><img alt="Figure 2.6: Correlation matrix " src="image/B19026_02_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.207.1">Figure 2.6: Correlation matrix</span></p>
<p><span class="koboSpan" id="kobo.208.1">Finding groups of highly correlated features and only maintaining one of them is the main goal of employing pairwise correlation for feature selection, which aims to maximize the predictive value of your model by using the fewest number of </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">features possible.</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">Pairwise correlation is calculated between rows or columns of a DataFrame and rows or columns of a Series or DataFrame. </span><span class="koboSpan" id="kobo.210.2">The correlations are calculated after DataFrames have been aligned along both axes. </span><span class="koboSpan" id="kobo.210.3">Next, we can see an example that might mak</span><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.211.1">e it </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">more clear:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.213.1">
df1=pd.DataFrame( np.random.randn(3,2), col</span><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.214.1">umns=['a','b'] )
df2=pd.DataFrame( np.random.randn(3,2), columns=['a','b'] )</span></pre>
<p><span class="koboSpan" id="kobo.215.1">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">corr</span></strong><span class="koboSpan" id="kobo.217.1"> to compare</span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.218.1"> numerical columns within the same data frame. </span><span class="koboSpan" id="kobo.218.2">Non-numerical columns will automati</span><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.219.1">cally </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">be skipped:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.221.1">
corr</span><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.222.1">_mat = df1.corr()
sn.heatmap(corr</span><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.223.1">_mat, annot=Tru</span><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.224.1">e)
plt.show()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.225.1"><img alt="Figure 2.7: Correlation matrix " src="image/B19026_02_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.226.1">Figure 2.7: Correlation matrix</span></p>
<p><span class="koboSpan" id="kobo.227.1">We can also compare the columns of </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">df1</span></strong><span class="koboSpan" id="kobo.229.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.230.1">df2</span></strong><span class="koboSpan" id="kobo.231.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">corrwith</span></strong><span class="koboSpan" id="kobo.233.1">. </span><span class="koboSpan" id="kobo.233.2">Note that only columns with the same nam</span><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.234.1">es </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">are compared:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.236.1">
df1.corrwith(df2)</span></pre>
<p><span class="koboSpan" id="kobo.237.1">To make things easier, we can rename the columns of </span><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">df2</span></strong><span class="koboSpan" id="kobo.239.1"> to match the columns of </span><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">df1</span></strong><span class="koboSpan" id="kobo.241.1"> if we would like for pandas to disregard the column names and only compare the first row of </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">df1</span></strong><span class="koboSpan" id="kobo.243.1"> to the f</span><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.244.1">irst row </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">df2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.248.1">
df1.corrwith(df2.set_axis( df1.columns, axis='columns', inplace=False))</span></pre>
<p><span class="koboSpan" id="kobo.249.1">It’s important to note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">df1</span></strong><span class="koboSpan" id="kobo.251.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">df2</span></strong><span class="koboSpan" id="kobo.253.1"> need to have the same number of columns in </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">that case.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Last but not least, you could also just horizontally combine the two datasets and utilize </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">corr</span></strong><span class="koboSpan" id="kobo.257.1">. </span><span class="koboSpan" id="kobo.257.2">The benefit is that this essentially functions independently of the quantity and naming conventions of the columns, but the drawback is that you can receive more output</span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.258.1"> than you</span><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.259.1"> require </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">or want:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.261.1">
corr_mat = pd.concat([df1,df</span><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.262.1">2],axis=1).corr()
sn.heatmap(corr</span><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.263.1">_mat, annot=Tr</span><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.264.1">ue)
plt.show()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.265.1"><img alt="Figure 2.8: Correlation heatmap " src="image/B19026_02_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.266.1">Figure 2.8: Correlation heatmap</span></p>
<p><span class="koboSpan" id="kobo.267.1">Now that we have established the fact that two variables can be correlated using correlation analysis, we can seek to validate whether the variables are actually impacting one another using </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">causation analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.269.1">The ability of one variable to impact another is known as causality. </span><span class="koboSpan" id="kobo.269.2">The first variable might create the second or might change the incidence of the </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">second variable.</span></span></p>
<p><span class="koboSpan" id="kobo.271.1">Causality</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.272.1"> is the process by which one event, process, state, or object influences the development of another event, process, condition, or object, where the cause and effect are both partially reliant on each other. </span><span class="koboSpan" id="kobo.272.2">So what distinguishes correlation from causation? </span><span class="koboSpan" id="kobo.272.3">Correlation does not automatically imply causation, even if causality and correlation might coexist. </span><span class="koboSpan" id="kobo.272.4">In situations where action A results in outcome B, causation</span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.273.1"> is expressly applicable. </span><span class="koboSpan" id="kobo.273.2">Correlation, on the other hand, is just </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">a relationship.</span></span></p>
<p><span class="koboSpan" id="kobo.275.1">We can use the next dataset to study the causation </span><a id="_idTextAnchor180"/><span class="No-Break"><span class="koboSpan" id="kobo.276.1">between variables:</span></span></p>
<pre class="source-code">
<a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.277.1">import numpy as np
import pandas a</span><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.278.1">s pd
import random
ds = pd.DataFrame(columns = ['x','y'])
ds['x'] = [int(n&gt;500) for n in random.sample(range(0, 1000), 
      100)]
ds['y'] = [int(n&gt;500) for n in random.sample(range(0, 100</span><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.279.1">0), 
      100)]
ds.head()</span></pre>
<p><span class="koboSpan" id="kobo.280.1">To study the causation, we can </span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.281.1">seek to estimate the difference in means between two groups. </span><span class="koboSpan" id="kobo.281.2">The absolute difference between the mean values in two different groups is measured by the mean difference, often known as the difference in means. </span><span class="koboSpan" id="kobo.281.3">It offers you a sense of how much the averages of the experimental group and control groups differ from one another in </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">clinical studies.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">In the next example, we will estimate the uplift as a quantified difference in means along with the determined standard error. </span><span class="koboSpan" id="kobo.283.2">We will use 90 as the confidence interval in the range of the normal, which yields</span><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.284.1"> a z-score </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">of 1.96:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.286.1">
base,var  = ds[ds.x =</span><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.287.1">= 0], ds[ds.x == 1]
delta = var.y.mea</span><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.288.1">n() - base.y.mean()
delta_dev = 1.96 * np.sqrt(var.y.var() / var.shape[0] 
      +base.y.var</span><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.289.1">() / base.shape[0])
print("estimated_effect":,delta, "standard_error": delta_dev)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.290.1"><img alt="Figure 2.9: Estimated differences between populations " src="image/B19026_02_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.291.1">Figure 2.9: Estimated differences between populations</span></p>
<p><span class="koboSpan" id="kobo.292.1">We can also use the contingency </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">chi-square</span></strong><span class="koboSpan" id="kobo.294.1"> for the comparison of two groups with a dichotomous dependent variable. </span><span class="koboSpan" id="kobo.294.2">For example, we might contrast males and females using a yes/no response scale. </span><span class="koboSpan" id="kobo.294.3">The contingency chi-square is built on the same ideas as the straightforward chi-square analysis, which compares the anticipated and </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">actual outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">This statistical technique is </span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.297.1">used to compare actual outcomes with predictions. </span><span class="koboSpan" id="kobo.297.2">The goal of this test is to establish whether a discrepancy between observed and expected data is the result of chance or a correlation between the variables you are researching. </span><span class="koboSpan" id="kobo.297.3">The results create a contingency matrix from which we can infer that your variables are independent of one another and have no association with one another if C is close to zero (or equal to zero). </span><span class="koboSpan" id="kobo.297.4">There is a relationship if C is not zero; C can only take </span><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.298.1">on </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">positive values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.300.1">
from scipy.stats impo</span><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.301.1">rt chi2_contingency
co</span><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.302.1">ntingency_table = (
         ds 
         .assign(placeholder=1) 
         .pivot_table(index="x", columns="y", 
              values="placeholder", aggf</span><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.303.1">unc="sum") 
         .values)
_, p, _, _ = chi2_contingency(contingency_table, 
     lambda_="log-likelihood")</span></pre>
<p><span class="koboSpan" id="kobo.304.1">Here, we will just seek to inte</span><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.305.1">rpret </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">the p-values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.307.1">
print("P-value:",p)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.308.1"><img alt="Figure 2.10: Resulting p-value " src="image/B19026_02_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.309.1">Figure 2.10: Resulting p-value</span></p>
<p><span class="koboSpan" id="kobo.310.1">Now we will use a set of datasets that were </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">synthe</span><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.312.1">tically generated:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.313.1">
data_1 = pd.read_csv('obse</span><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.314.1">rved_data_1.csv' )
data_1.plot.scatter(x="z", y="y", c="x", cmap="rainbow", </span><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.315.1">colorbar=False)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.316.1"><img alt="Figure 2.11: Plot of data distribution " src="image/B19026_02_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.317.1">Figure 2.11: Plot of data distribution</span></p>
<p><span class="koboSpan" id="kobo.318.1">The probability density function of a </span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.319.1">continuous random variable can be estimated using the </span><strong class="bold"><span class="koboSpan" id="kobo.320.1">kernel density estimation</span></strong><span class="koboSpan" id="kobo.321.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.322.1">KDE</span></strong><span class="koboSpan" id="kobo.323.1">) seaborn</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.324.1"> method. </span><span class="koboSpan" id="kobo.324.2">The area under the depicted curve serves as a representation of the probability distribution </span><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.325.1">of the </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">data values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.327.1">
im</span><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.328.1">port seaborn as sns
sns.kdeplot(data_1.loc[lambda df: df.x == 0].y,</span><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.329.1"> label="untreated")
sns.kdeplot(data_1.loc[lambda df: df.x == 1].y, </span><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.330.1">label="treated")</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.331.1"><img alt="Figure 2.12: Density graph " src="image/B19026_02_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.332.1">Figure 2.12: Density graph</span></p>
<p><span class="koboSpan" id="kobo.333.1">To study causation, the researcher must build a model to describe the connections between ideas connected to a particular phenomenon in </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">causal modeling.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">Multiple causality—the idea that any given outcome may have more than one cause—is incorporated into causal models. </span><span class="koboSpan" id="kobo.335.2">For instance, social status, age, sex, ethnicity, and other factors may influence someone’s voting behavior. </span><span class="koboSpan" id="kobo.335.3">In addition, some of the independent or explanatory factors might </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">be connected.</span></span></p>
<p><span class="koboSpan" id="kobo.337.1">External validity can be addressed using causal models (whether results from one study apply to unstudied populations). </span><span class="koboSpan" id="kobo.337.2">In some cases, causal models can combine data to provide answers to questions that no single dataset alone is able </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">to address.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">est_via_ols</span></strong><span class="koboSpan" id="kobo.341.1"> function of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">causalinference</span></strong><span class="koboSpan" id="kobo.343.1"> package to estimate average treatment effects using </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">least squares.</span></span></p>
<p><span class="koboSpan" id="kobo.345.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.346.1">y</span></em><span class="koboSpan" id="kobo.347.1"> is the </span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.348.1">potential outcome when treated, D is the treatment status, and X is a vector of covariates or </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">individual characteristics.</span></span></p>
<p><span class="koboSpan" id="kobo.350.1">The parameter to control is </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">adj</span></strong><span class="koboSpan" id="kobo.352.1">, an int which can be either 0, 1, or 2. </span><span class="koboSpan" id="kobo.352.2">This parameter indicates how covariate adjustments are to be performed. </span><span class="koboSpan" id="kobo.352.3">Setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">adj</span></strong><span class="koboSpan" id="kobo.354.1"> to 0 will not include any covariates. </span><span class="koboSpan" id="kobo.354.2">Set </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">adj</span></strong><span class="koboSpan" id="kobo.356.1"> to 1 to include treatment indicator D and covariates X separately, or set </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">adj</span></strong><span class="koboSpan" id="kobo.358.1"> to 2 to additionally include interaction terms between D and X.</span><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.359.1"> The default </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">is 2.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.361.1">
!pip inst</span><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.362.1">all causalinference
from causalinference</span><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.363.1"> import CausalMode</span><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.364.1">l
cm = CausalModel(
    </span><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.365.1">Y=data_1.y.values,
    </span><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.366.1">D=data_1.x.values,
    X=data_1.z.values)
cm</span><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.367.1">.est_via_ols(adj=1)
print(cm.estimates)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.368.1"><img alt="Figure 2.13: Causal model results " src="image/B19026_02_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.369.1">Figure 2.13: Causal model results</span></p>
<p><span class="koboSpan" id="kobo.370.1">The estimates show us that there is a negative relationship between the variables. </span><span class="koboSpan" id="kobo.370.2">The negative estimate might be an indication that the application of D reduces the probability of Y by 48%. </span><span class="koboSpan" id="kobo.370.3">It’s really important to look at the entire set of estimate distributions to dra</span><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.371.1">w </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">any conclusions.</span></span></p>
<p><span class="koboSpan" id="kobo.373.1">The analysis of a </span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.374.1">hypothetical or counterfactual reality is causal analysis, because we must make claims about the counterfactual result that we did not witness in order to assess th</span><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.375.1">e </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">treatment effect:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.377.1">
data_2 = pd.read_csv('ob</span><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.378.1">served_data_2.csv')
data_2.plot.scatter(x="z", y="y", c="x", cmap="rainbow", colorbar=False)</span></pre>
<p><span class="koboSpan" id="kobo.379.1">The data previously loaded will show us different values in t</span><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.380.1">he </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">causal model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.382.1"><img alt="Figure 2.14: Data distribution " src="image/B19026_02_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.383.1">Figure 2.14: Data distribution</span></p>
<p><span class="koboSpan" id="kobo.384.1">We will build the new causal model using the new </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">loaded values:</span></span><a id="_idTextAnchor211"/></p>
<pre class="source-code"><span class="koboSpan" id="kobo.386.1">
cm = CausalModel(
    Y=</span><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.387.1">data_2 .y.values,
    D=</span><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.388.1">data_2 .x.values,
    X</span><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.389.1">=data_2 .z.values)
cm.est_via_ols(adj=1)</span></pre>
<p><span class="koboSpan" id="kobo.390.1">We can print the </span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.391.1">treatment effect estimates to validate whether our cau</span><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.392.1">sal model </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">is valid:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.394.1">
pri</span><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.395.1">nt(cm.estimates)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.396.1"><img alt="Figure 2.15: Causal model results with new data " src="image/B19026_02_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.397.1">Figure 2.15: Causal model results with new data</span></p>
<p><span class="koboSpan" id="kobo.398.1">The estimates inform us that the relationship has </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">become positive.</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">Causal models</span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.401.1"> are a great way to validate the modeling and direction of relationships between the variables </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">in data.</span></span></p>
<p><span class="koboSpan" id="kobo.403.1">In the next section, we will dive into how we can use scaling to prepare our data for machine learning, depending on the distri</span><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.404.1">bution that </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">it has.</span></span></p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.406.1">Scaling features to a range</span></h1>
<p><span class="koboSpan" id="kobo.407.1">When working</span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.408.1"> with machine learning models, it is important to</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.409.1"> preprocess data so certain problems such as an explosion of gradients or lack of proper distribution representation can </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">be solved.</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">To transform raw feature vectors into a representation that is better suited for the downstream estimators, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">sklearn.preprocessing</span></strong><span class="koboSpan" id="kobo.413.1"> package offers a number of common utility functions and </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">transformer classes.</span></span></p>
<p><span class="koboSpan" id="kobo.415.1">Many machine learning estimators used in </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.417.1"> frequently require dataset standardization; if the individual features do not more or less resemble standard normally distributed data, they may behave poorly: Gaussian with a mean of 0 and a variation </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">of 1.</span></span></p>
<p><span class="koboSpan" id="kobo.419.1">In general, standardizing the dataset is advantageous for learning algorithms. </span><span class="koboSpan" id="kobo.419.2">Robust scalers or transformers are preferable if there are any outliers in the collection. </span><span class="koboSpan" id="kobo.419.3">On a dataset with marginal outliers, the actions of several scalers, transformers, and normalizers are highlighted in the analysis of the impact of various scalers on data </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">containing outliers.</span></span></p>
<p><span class="koboSpan" id="kobo.421.1">In reality, we frequently ignore the distribution’s shape and simply adapt the data to scale by dividing non-constant features by their standard deviation and centering it by subtracting each feature’s </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">mean value.</span></span></p>
<p><span class="koboSpan" id="kobo.423.1">For instance, several components of a learning algorithm’s objective function (such as the RBF kernel of SVMs or the l1 and l2 regularizers of linear models) may make the assumption that all features are centered around zero or have variance in the same order. </span><span class="koboSpan" id="kobo.423.2">A feature may dominate the objective function and prevent the estimator from successfully inferring from other features as expected if its variance is orders of magnitude greater than that of </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">other features.</span></span></p>
<p><span class="koboSpan" id="kobo.425.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.427.1"> utility class, which the preprocessing module offers, makes it quick and simple to carry out the following operation on a</span><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.428.1">n </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">array-like dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.430.1">
from sklearn</span><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.431.1"> import preprocessing
x_train = pd.DataFr</span><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.432.1">ame([[ 1., -1.,  2.],
                    [ 2.,  0.,  0.],
                    [ 0.,  1., -1.]],c</span><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.433.1">olumns=['x','y','z'])
scaler = preprocessing.StandardScaler().fit(x_train)</span></pre>
<p><span class="koboSpan" id="kobo.434.1">The following code will fit the scaler to the data, assuming that our dist</span><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.435.1">ribution </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">is standard:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.437.1">
scaler.mean_</span></pre>
<p><span class="koboSpan" id="kobo.438.1">We can visualize now the mean of </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">the data:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.440.1"><img alt="Figure 2.16: Mean of the data " src="image/B19026_02_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.441.1">Figure 2.16: Mean of the data</span></p>
<p><span class="koboSpan" id="kobo.442.1">We can visualiz</span><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.443.1">e the scale </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">as well:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.445.1">
scaler.scale_</span></pre>
<p><span class="koboSpan" id="kobo.446.1">The data is </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.447.1">shown</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.448.1"> as an array </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">of values:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.450.1"><img alt="Figure 2.17: Scale of the columns " src="image/B19026_02_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.451.1">Figure 2.17: Scale of the columns</span></p>
<p><span class="koboSpan" id="kobo.452.1">Finally, we can scale the data using </span><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.453.1">the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">transform</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.455.1"> method:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.456.1">
x_scaled = sca</span><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.457.1">ler.transform(x_train)</span></pre>
<p><span class="koboSpan" id="kobo.458.1">A different method of standardization is to scale each feature’s maximum absolute value to one unit, or to a value between a predetermined minimum and maximum value, usually zero and one. </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">MaxAbsScaler</span></strong><span class="koboSpan" id="kobo.460.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">MinMaxScaler</span></strong><span class="koboSpan" id="kobo.462.1"> can be used to </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">do this.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">The robustness to very small standard deviations of features and the preservation of zero entries in sparse data are two reasons to employ </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">this scaling.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">To scale a toy data matrix to the [0, 1] range, consider t</span><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.467.1">he </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">following example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
min_max_scaler = preproc</span><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.470.1">essing.MinMaxScaler()
x_train_minmax = min_max_scaler.</span><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.471.1">fit_transform(x_train)</span></pre>
<p><span class="koboSpan" id="kobo.472.1">In case our distribution differs from the standard Gaussian, we can use non-linear transformations. </span><span class="koboSpan" id="kobo.472.2">There are two different kinds of transformations: power and quantile transform. </span><span class="koboSpan" id="kobo.472.3">The rank of the values along each feature is preserved by both quantile and power transforms because they are based on monotonic transformations of </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">the features.</span></span></p>
<p><span class="koboSpan" id="kobo.474.1">Based on the formula, which is the cumulative distribution function of the feature and the quantile function of the desired output distribution, quantile transformations place all features into the same desired distribution. </span><span class="koboSpan" id="kobo.474.2">These two facts are used in this formula: it is uniformly distributed if it is a random variable with a continuous cumulative distribution function, and it has distribution if it is a random variable with a uniform distribution on. </span><span class="koboSpan" id="kobo.474.3">A quantile transform smoothes out atypical distributions using a rank transformation and is less susceptible to outliers than scaling techniques. </span><span class="koboSpan" id="kobo.474.4">Correlations and distances within and between features are, however, distorted </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">by it.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">Sklearn provides </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.477.1">a series of parametric transformations </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.478.1">called power transforms that aim to translate data from any distribution to one that resembles a Gaussian distribution as closely </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">as possible.</span></span></p>
<p><span class="koboSpan" id="kobo.480.1">We can map our data to a uniform distribution using </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">QuantileTransformer</span></strong><span class="koboSpan" id="kobo.482.1">, which provides a non-parametric transformation to map the data to a uniform distribution with </span><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.483.1">values between 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">and 1:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.485.1">
from sklearn.da</span><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.486.1">tasets import load_</span><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.487.1">iris
data = load_iris()
x, y = data</span><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.488.1">['data'],data['target']
quantile_transformer = preprocessing.QuantileTra</span><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.489.1">nsformer(
n_quantiles=5)
x_train_qt = quantile_trans</span><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.490.1">former.fit_transform(x)
x_train_qt[:5]</span></pre>
<p><span class="koboSpan" id="kobo.491.1">We can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">resulting array:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.493.1"><img alt="Figure 2.18: Transformed data " src="image/B19026_02_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.494.1">Figure 2.18: Transformed data</span></p>
<p><span class="koboSpan" id="kobo.495.1">It is also possible to map data to a normal distribution using </span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">QuantileTransformer</span></strong><span class="koboSpan" id="kobo.497.1"> by setting </span><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">output_distribution='normal'</span></strong><span class="koboSpan" id="kobo.499.1">. </span><span class="koboSpan" id="kobo.499.2">The following example uses the earlier example </span><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.500.1">with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">iris</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.502.1"> dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.503.1">
quantile_transformer = preprocessing.QuantileTransformer(
n_quantiles=5,output_</span><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.504.1">distribution='normal')
x_trans_qt = quantile_transf</span><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.505.1">ormer.fit_transform(x)
quantile_transformer.quantiles_</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.506.1"><img alt="Figure 2.19: Transformed data through the quantiles method " src="image/B19026_02_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.507.1">Figure 2.19: Transformed data through the quantiles method</span></p>
<p><span class="koboSpan" id="kobo.508.1">The</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.509.1"> preceding code will scale the data using a quantile </span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.510.1">transformation, defining five quantiles and looking to have a normal distribution in </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">the output.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">To determine the proper distribution to be utilized, we need to analyze in depth the distribution of our variables, as the wrong transformation can make us lose details that might be important to take </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">into account.</span></span></p>
<p><span class="koboSpan" id="kobo.514.1">In the next section, we will dive into unsupervised learning by looking at clustering algorit</span><a id="_idTextAnchor239"/><span class="koboSpan" id="kobo.515.1">hms </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">using scikit-learn.</span></span></p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor240"/><span class="koboSpan" id="kobo.517.1">Clustering data and reducing the dimensionality</span></h1>
<p><span class="koboSpan" id="kobo.518.1">The process of clustering</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.519.1"> involves grouping the population or data points into a number of groups so that the data points within each group are more similar to one another than the data points within other groups. </span><span class="koboSpan" id="kobo.519.2">Simply said, the goal is to sort any groups of people who share similar characteristics into clusters. </span><span class="koboSpan" id="kobo.519.3">It is frequently used in business analytics. </span><span class="koboSpan" id="kobo.519.4">How to arrange the enormous volumes of available data into useful structures is one of the issues that organizations are </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">currently confronting.</span></span></p>
<p><span class="koboSpan" id="kobo.521.1">Image segmentation, grouping web pages, market segmentation, and information retrieval are four examples of how clustering can help firms better manage their data. </span><span class="koboSpan" id="kobo.521.2">Data clustering is beneficial for retail firms since it influences sales efforts, customer retention, and customer </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">shopping behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.523.1">The goal of the vector quantization technique known as “K-means clustering,” which has its roots in signal processing, is to divide a set of n </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.524.1">observations into k clusters, each of which has as its prototype in the observation with the closest mean. </span><span class="koboSpan" id="kobo.524.2">K-means clustering is an unsupervised technique that uses the input data as is and doesn’t require a labeled response. </span><span class="koboSpan" id="kobo.524.3">A popular method for clustering is K-means clustering. </span><span class="koboSpan" id="kobo.524.4">Typically, practitioners start by studying the dataset’s architecture. </span><span class="koboSpan" id="kobo.524.5">Data points are grouped by K-means into distinct, </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">non-overlapping groups.</span></span></p>
<p><span class="koboSpan" id="kobo.526.1">In the next code, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">KMeans</span></strong><span class="koboSpan" id="kobo.528.1"> to fit the data in order to label each data </span><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.529.1">point to a </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">given cluster:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.531.1">
from skle</span><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.532.1">arn.cluster import KMeans
kmeans = KMeans(n_clusters=len(set(y)</span><a id="_idTextAnchor243"/><span class="koboSpan" id="kobo.533.1">), random_state=0).fit(x)
kmeans.labels_</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.534.1"><img alt="Figure 2.20: Cluster data " src="image/B19026_02_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.535.1">Figure 2.20: Cluster data</span></p>
<p><span class="koboSpan" id="kobo.536.1">We can predict to which</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.537.1"> cluster each new </span><a id="_idTextAnchor244"/><span class="koboSpan" id="kobo.538.1">instance of </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">data belongs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
kmeans.predict(x[0].reshape(1,-1))</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.541.1"><img alt="Figure 2.21: Predicted data " src="image/B19026_02_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.542.1">Figure 2.21: Predicted data</span></p>
<p><span class="koboSpan" id="kobo.543.1">We can also visual</span><a id="_idTextAnchor245"/><span class="koboSpan" id="kobo.544.1">ize the </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">cluster centers:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.546.1">
kmeans.cluster_centers_</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.547.1"><img alt="Figure 2.22: Cluster centers " src="image/B19026_02_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.548.1">Figure 2.22: Cluster centers</span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">KMeans</span></strong><span class="koboSpan" id="kobo.550.1"> allows us to</span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.551.1"> find the characteristics in common data when the number of variables is too high and it’s useful for segmentation. </span><span class="koboSpan" id="kobo.551.2">But sometimes, there is the need to reduce the number of dimensions to a set of grouped variables with </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">common traits.</span></span></p>
<p><span class="koboSpan" id="kobo.553.1">In order to project the data into a lower dimensional environment, we can use </span><strong class="bold"><span class="koboSpan" id="kobo.554.1">principal component analysis</span></strong><span class="koboSpan" id="kobo.555.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.556.1">PCA</span></strong><span class="koboSpan" id="kobo.557.1">), a linear</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.558.1"> dimensionality reduction</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.559.1"> technique. </span><span class="koboSpan" id="kobo.559.2">Before using the SVD, the input data is scaled but not centered for each feature. </span><span class="koboSpan" id="kobo.559.3">Depending on the structure of the input data and the number of components to be extracted, it employs either a randomized truncated SVD or the complete SVD implementation as implemented by LAPACK. </span><span class="koboSpan" id="kobo.559.4">You should be aware that this class does not accept sparse input. </span><span class="koboSpan" id="kobo.559.5">For a sparse data alternative, use the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">TruncatedSVD</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.561.1"> class.</span></span></p>
<p><span class="koboSpan" id="kobo.562.1">Up next, we fit data into two components in order to </span><a id="_idTextAnchor246"/><span class="koboSpan" id="kobo.563.1">reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">the dimensionality:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.565.1">
from sklear</span><a id="_idTextAnchor247"/><span class="koboSpan" id="kobo.566.1">n.decomposition import PCA</span><a id="_idTextAnchor248"/><span class="koboSpan" id="kobo.567.1">
pca = PCA(n_components=2)
pca.fit(x)</span></pre>
<p><span class="koboSpan" id="kobo.568.1">We should strive to account for the maximum amount of variance possible, which in simple terms can be understood as the degree to which our model can </span><a id="_idTextAnchor249"/><span class="koboSpan" id="kobo.569.1">explain the </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">whole dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.571.1">
print(pca.</span><a id="_idTextAnchor250"/><span class="koboSpan" id="kobo.572.1">explained_variance_ratio_)
print(pca.singular_values_)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.573.1"><img alt="Figure 2.23: PCA singular values " src="image/B19026_02_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.574.1">Figure 2.23: PCA singular values</span></p>
<p><span class="koboSpan" id="kobo.575.1">After we have worked </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.576.1">our data to preprocess it, reducing the number of dimensions and clustering, we can now build machine learning models to make predictions of </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">future behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.578.1">In the next section, we will build machine learning models that we can use to predict new data labels for regression</span><a id="_idTextAnchor251"/><span class="koboSpan" id="kobo.579.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">classification tasks.</span></span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor252"/><span class="koboSpan" id="kobo.581.1">Building machine learning models</span></h1>
<p><span class="koboSpan" id="kobo.582.1">One of the </span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.583.1">most simple machine learning models we can construct to make a forecast of future behaviors is linear regression, which reduces the residual sum of squares between the targets observed in the dataset and the targets anticipated by the linear approximation, fitting a linear model </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">using coefficients.</span></span></p>
<p><span class="koboSpan" id="kobo.585.1">This is simply ordinary least squares or non-negative least squares wrapped in a predictor object from the </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">implementation perspective.</span></span></p>
<p><span class="koboSpan" id="kobo.587.1">We can implement this really simply by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">LinearR</span><a id="_idTextAnchor253"/><span class="koboSpan" id="kobo.589.1">egression</span></strong><span class="koboSpan" id="kobo.590.1"> class </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">in Sklearn:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.592.1">
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_diabetes</span><a id="_idTextAnchor254"/><span class="koboSpan" id="kobo.593.1">
data_reg = load_diabetes()
x,y = data_reg</span><a id="_idTextAnchor255"/><span class="koboSpan" id="kobo.594.1">['data'],data_reg['target']
reg = L</span><a id="_idTextAnchor256"/><span class="koboSpan" id="kobo.595.1">inearRegression().fit(x, y)
reg.score(x, y)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.596.1"><img alt="Figure 2.24: Model regression score " src="image/B19026_02_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.597.1">Figure 2.24: Model regression score</span></p>
<p><span class="koboSpan" id="kobo.598.1">The preceding code will fit a linear regression model to our data and print the score of </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">our data.</span></span></p>
<p><span class="koboSpan" id="kobo.600.1">We can also print the coefficients, which give us a great estimation of the contribution of each </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.601.1">variable to explain the variabl</span><a id="_idTextAnchor257"/><span class="koboSpan" id="kobo.602.1">e we are trying </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">to predict:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.604.1">
reg.coef_</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.605.1"><img alt="Figure 2.25: Regression coefficients " src="image/B19026_02_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.606.1">Figure 2.25: Regression coefficients</span></p>
<p><span class="koboSpan" id="kobo.607.1">We can also prin</span><a id="_idTextAnchor258"/><span class="koboSpan" id="kobo.608.1">t the </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">intercept variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.610.1">
reg.intercept_</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.611.1"><img alt="Figure 2.26: Regression intercepts " src="image/B19026_02_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.612.1">Figure 2.26: Regression intercepts</span></p>
<p><span class="koboSpan" id="kobo.613.1">Finally, we can use the m</span><a id="_idTextAnchor259"/><span class="koboSpan" id="kobo.614.1">odel to </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">make predictions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.616.1">
print('Predicted:',reg.predict(x[10].reshape(
1,-1)),'Actual:',y[10])</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.617.1"><img alt="Figure 2.27: Predicted regression values " src="image/B19026_02_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.618.1">Figure 2.27: Predicted regression values</span></p>
<p><span class="koboSpan" id="kobo.619.1">Here we are predicting a continuous variable, but we can also predict categorical variables using a classifier instead of </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">a regression.</span></span></p>
<p><span class="koboSpan" id="kobo.621.1">Sklearn </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.622.1">gives us the option of using the logistic regression (</span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">logit</span></strong><span class="koboSpan" id="kobo.624.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.625.1">MaxEnt</span></strong><span class="koboSpan" id="kobo.626.1">) classifier, in which in the multiclass case, the training algorithm uses the one-vs-rest (</span><strong class="source-inline"><span class="koboSpan" id="kobo.627.1">OvR</span></strong><span class="koboSpan" id="kobo.628.1">) scheme if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.629.1">'multi_class'</span></strong><span class="koboSpan" id="kobo.630.1"> option is set to ‘</span><strong class="source-inline"><span class="koboSpan" id="kobo.631.1">ovr'</span></strong><span class="koboSpan" id="kobo.632.1"> and uses the cross-entropy loss if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.633.1">'multi_class'</span></strong><span class="koboSpan" id="kobo.634.1"> option is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.635.1">'multinomial'</span></strong><span class="koboSpan" id="kobo.636.1">. </span><span class="koboSpan" id="kobo.636.2">This class uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">'liblinear'</span></strong><span class="koboSpan" id="kobo.638.1"> library, </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">'newton-cg','sag','saga'</span></strong><span class="koboSpan" id="kobo.640.1">, and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">'lbfgs'</span></strong><span class="koboSpan" id="kobo.642.1"> solvers to implement regularized logistic regression. </span><span class="koboSpan" id="kobo.642.2">Keep in mind that regularization is used by default. </span><span class="koboSpan" id="kobo.642.3">Both dense and sparse input can be handled by it. </span><span class="koboSpan" id="kobo.642.4">For best speed, only use matrices with 64-bit floats; all other input formats will </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">be transformed.</span></span></p>
<p><span class="koboSpan" id="kobo.644.1">The sole regularization supported by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">"newton-cg,"</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.646.1">"sag,"</span></strong><span class="koboSpan" id="kobo.647.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">"lbfgs"</span></strong><span class="koboSpan" id="kobo.649.1"> solvers is the L2 regularization with the primal formulation. </span><span class="koboSpan" id="kobo.649.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">"liblinear"</span></strong><span class="koboSpan" id="kobo.651.1"> solver supports both the L1 and L2 regularizations, however, only the L2 penalty has a dual formulation. </span><span class="koboSpan" id="kobo.651.2">The only solver that supports the elastic net regularization is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">"</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">saga"</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.654.1"> solver.</span></span></p>
<p><span class="koboSpan" id="kobo.655.1">When fitting the model, the underlying C program chooses features using a random number generator. </span><span class="koboSpan" id="kobo.655.2">Thus, slightly varied outputs for the same input data are common. </span><span class="koboSpan" id="kobo.655.3">Try using a smaller </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">to</span><a id="_idTextAnchor260"/><span class="koboSpan" id="kobo.657.1">l</span></strong><span class="koboSpan" id="kobo.658.1"> parameter if </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">it occurs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.660.1">
from sklearn.pipe</span><a id="_idTextAnchor261"/><span class="koboSpan" id="kobo.661.1">line import make_pipeline
from sklearn.preprocess</span><a id="_idTextAnchor262"/><span class="koboSpan" id="kobo.662.1">ing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_digits
d</span><a id="_idTextAnchor263"/><span class="koboSpan" id="kobo.663.1">ata_class = load_digits()
x,y = data_class['da</span><a id="_idTextAnchor264"/><span class="koboSpan" id="kobo.664.1">ta'],data_class['target']
clf = make_pipeline(StandardScaler(), 
      LogisticRegre</span><a id="_idTextAnchor265"/><span class="koboSpan" id="kobo.665.1">ssion(penalty=</span><a id="_idTextAnchor266"/><span class="koboSpan" id="kobo.666.1">'l2',C=.1))
clf.fit(x, y)
clf.predict(x[:2, :])</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.667.1"><img alt="Figure 2.28: Logistic regression results " src="image/B19026_02_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.668.1">Figure 2.28: Logistic regression results</span></p>
<p><span class="koboSpan" id="kobo.669.1">We can also score the model to assess the preci</span><a id="_idTextAnchor267"/><span class="koboSpan" id="kobo.670.1">sion of </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">our predictions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.672.1">
clf.score(x, y)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.673.1"><img alt="Figure 2.29: User data " src="image/B19026_02_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.674.1">Figure 2.29: User data</span></p>
<p><span class="koboSpan" id="kobo.675.1">In order to validate the model, we can use cross-validation, which allows us to evaluate the estimator’s performance. </span><span class="koboSpan" id="kobo.675.2">This is a methodological error in learning the parameters of a prediction function and evaluating it on the same set of data. </span><span class="koboSpan" id="kobo.675.3">A model that simply repeats the labels of the samples it has just seen would score well but be unable to make any predictions about data that has not yet been seen. </span><span class="koboSpan" id="kobo.675.4">Overfitting is the term for this circumstance. </span><span class="koboSpan" id="kobo.675.5">It is customary to reserve a portion of the available data as a test set (x test, y test) when conducting a (supervised) machine learning experiment in order to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">this problem.</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">It should be noted that the term “experiment” does not just refer to academic purposes because machine learning experiments sometimes begin in commercial contexts as well. </span><span class="koboSpan" id="kobo.677.2">Grid search methods can be used to find the </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">optimal parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.679.1">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.681.1">, a random split into training and test sets can be quickly computed with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.682.1">train_test_split</span></strong><span class="koboSpan" id="kobo.683.1"> helper function. </span><span class="koboSpan" id="kobo.683.2">Let’s load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">iris</span></strong><span class="koboSpan" id="kobo.685.1"> dataset to </span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.686.1">fit a linear suppo</span><a id="_idTextAnchor268"/><span class="koboSpan" id="kobo.687.1">rt vector machine </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">on it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.689.1">
x, y = lo</span><a id="_idTextAnchor269"/><span class="koboSpan" id="kobo.690.1">ad_iris(return_X_y=True)
x.shape, y.shape</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.691.1"><img alt="Figure 2.30: Data shape " src="image/B19026_02_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.692.1">Figure 2.30: Data shape</span></p>
<p><span class="koboSpan" id="kobo.693.1">We can now quickly sample a training set while holding out 40% of the data for testing (eval</span><a id="_idTextAnchor270"/><span class="koboSpan" id="kobo.694.1">uating) </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">our classifier:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.696.1">
from sklearn.model_selection </span><a id="_idTextAnchor271"/><span class="koboSpan" id="kobo.697.1">import train_test_split
</span><a id="_idTextAnchor272"/><span class="koboSpan" id="kobo.698.1">from sklearn import svm
x_train, x_test, y_train, y_test = train_test_split(x, y, 
test_size=0.4, random_state=0)</span></pre>
<p><span class="koboSpan" id="kobo.699.1">We can validate the shape of the generated train dataset by looking at</span><a id="_idTextAnchor273"/><span class="koboSpan" id="kobo.700.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.701.1">numpy</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.702.1">array shape:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.703.1">
x_train.shape, y_train.shape</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.704.1"><img alt="Figure 2.31: Train data shape " src="image/B19026_02_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.705.1">Figure 2.31: Train data shape</span></p>
<p><span class="koboSpan" id="kobo.706.1">We can </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.707.1">repeat the same</span><a id="_idTextAnchor274"/><span class="koboSpan" id="kobo.708.1"> with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.709.1">test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.710.1"> dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.711.1">
x_test.shape, y_test.shape</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.712.1"><img alt="Figure 2.32: Test data shape " src="image/B19026_02_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.713.1">Figure 2.32: Test data shape</span></p>
<p><span class="koboSpan" id="kobo.714.1">Finally, we can train our machine learning model on the training data and score it using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.715.1">test</span></strong><span class="koboSpan" id="kobo.716.1"> dataset, which holds data points not seen by the </span><a id="_idTextAnchor275"/><span class="koboSpan" id="kobo.717.1">model </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.719.1">
clf = svm.SVC(kernel='linear', C=.7)</span><a id="_idTextAnchor276"/><span class="koboSpan" id="kobo.720.1">.fit(x_train, y_train)
clf.score(x_test, y_test)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.721.1"><img alt="Figure 2.33: Logistic regression scores " src="image/B19026_02_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.722.1">Figure 2.33: Logistic regression scores</span></p>
<p><span class="koboSpan" id="kobo.723.1">There is still a chance of overfitting on the test set when comparing various settings of hyperparameters for estimators, such as the C setting that must be manually selected for an SVM. </span><span class="koboSpan" id="kobo.723.2">This is because the parameters can be adjusted until the estimator performs at its best. </span><span class="koboSpan" id="kobo.723.3">In this method, the model may “leak” information about the test set, and evaluation measures may no longer reflect generalization performance. </span><span class="koboSpan" id="kobo.723.4">This issue can be resolved by holding out a further portion of the dataset as a “validation set”: training is conducted on the training set, followed by evaluation on the validation set, and when it appears that the experiment has succeeded, a final evaluation can be conducted on the </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.725.1">However, by dividing the available data into three sets, we dramatically cut down on the number of samples that can be used to train the model, and the outcomes can vary depending on the randomization of the pair of (train and </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">validation) sets.</span></span></p>
<p><span class="koboSpan" id="kobo.727.1">Cross-validation</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.728.1"> is an approach that can be used to address this issue (CV for short). </span><span class="koboSpan" id="kobo.728.2">When doing CV, the validation set is no longer required, but a test set should still be kept aside for final assessment. </span><span class="koboSpan" id="kobo.728.3">The fundamental strategy, known as a k-fold CV, divides the training set into k smaller sets (other approaches are described below, but generally follow the same principles). </span><span class="koboSpan" id="kobo.728.4">Every single one of the k “folds” is done </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">as follows:</span></span></p>
<p><span class="koboSpan" id="kobo.730.1">The folds are used as training data for a model, and the resulting model is validated using the remaining portion of the data (as in, it is used as a test set to compute a performance measure such </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">as accuracy).</span></span></p>
<p><span class="koboSpan" id="kobo.732.1">The average of the </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.733.1">numbers calculated in the loop is then the performance indicator supplied by k-fold cross-validation. </span><span class="koboSpan" id="kobo.733.2">Although this method can be computationally expensive, it does not waste a lot of data (unlike fixing an arbitrary validation set), which is a significant benefit in applications such as inverse inference where there are </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">few samples.</span></span></p>
<p><span class="koboSpan" id="kobo.735.1">We can compute the cross-validated metrics by calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.736.1">cross_val</span></strong><span class="koboSpan" id="kobo.737.1"> score helper function on the estimator and the dataset is the simplest approach to apply cross-validation. </span><span class="koboSpan" id="kobo.737.2">The example that follows shows how to split the data, develop a model, and calculate the score five times in a row (using various splits each time) to measure the accuracy of a linear kernel support vector machi</span><a id="_idTextAnchor277"/><span class="koboSpan" id="kobo.738.1">ne on the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">iris</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.740.1"> dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.741.1">
from sklearn.model_selection</span><a id="_idTextAnchor278"/><span class="koboSpan" id="kobo.742.1"> import cross_val_score
scores = cross_val</span><a id="_idTextAnchor279"/><span class="koboSpan" id="kobo.743.1">_score(clf, x, y, cv=5)</span></pre>
<p><span class="koboSpan" id="kobo.744.1">The mean score and the standard deviat</span><a id="_idTextAnchor280"/><span class="koboSpan" id="kobo.745.1">ion are hence given by </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.747.1">
print('Mean:',scores.mean(),'Standard Deviation:', 
scores.std())</span></pre>
<p><span class="koboSpan" id="kobo.748.1">The estimator’s scoring technique is by default used to calculate the score at each </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">CV iteration:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.750.1"><img alt="Figure 2.34: CV mean scores " src="image/B19026_02_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.751.1">Figure 2.34: CV mean scores</span></p>
<p><span class="koboSpan" id="kobo.752.1">This can be altered by applying </span><a id="_idTextAnchor281"/><span class="koboSpan" id="kobo.753.1">the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">scoring</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.755.1"> parameter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.756.1">
from </span><a id="_idTextAnchor282"/><span class="koboSpan" id="kobo.757.1">sklearn import metrics
scores = cross_val_score(clf, x, y, cv=</span><a id="_idTextAnchor283"/><span class="koboSpan" id="kobo.758.1">5, scoring='f1_macro')
scores</span></pre>
<p><span class="koboSpan" id="kobo.759.1">Since the </span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.760.1">samples in the iris dataset are distributed evenly throughout the target classes, the accuracy and F1 score are </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">nearly equal:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.762.1"><img alt="Figure 2.35: CV scores " src="image/B19026_02_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.763.1">Figure 2.35: CV scores</span></p>
<p><span class="koboSpan" id="kobo.764.1">The CV score defaults to using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.765.1">KFold</span></strong><span class="koboSpan" id="kobo.766.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.767.1">StratifiedKFold</span></strong><span class="koboSpan" id="kobo.768.1"> strategies when the </span><strong class="source-inline"><span class="koboSpan" id="kobo.769.1">cv</span></strong><span class="koboSpan" id="kobo.770.1"> parameter is an integer, with the latter being utilized if the estimator comes </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.772.1">ClassifierMixin</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.774.1">It is also possible to use other CV strategies by passing a CV iterator using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.775.1">ShuffleSplit</span></strong> <a id="_idTextAnchor284"/><span class="koboSpan" id="kobo.776.1">Sklearn </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">class instead:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.778.1">
from sklearn.model_selecti</span><a id="_idTextAnchor285"/><span class="koboSpan" id="kobo.779.1">on import ShuffleSplit
</span><a id="_idTextAnchor286"/><span class="koboSpan" id="kobo.780.1">n_samples = x.shape[0]
cv = ShuffleSplit(n_splits=5, test_siz</span><a id="_idTextAnchor287"/><span class="koboSpan" id="kobo.781.1">e=0.3, random_state=0)
cross_val_score(clf, x, y, cv=cv)</span></pre>
<p><span class="koboSpan" id="kobo.782.1">The</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.783.1"> preceding code will show us the CV scores on multiple folds of test samples, which can be used to prevent the </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">overfitting problem:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.785.1"><img alt="Figure 2.36: Results using shuffle split " src="image/B19026_02_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.786.1">Figure 2.36: Results using shuffle split</span></p>
<p><span class="koboSpan" id="kobo.787.1">The preceding results show us the re</span><a id="_idTextAnchor288"/><span class="koboSpan" id="kobo.788.1">sults of the </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">CV score.</span></span></p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor289"/><span class="koboSpan" id="kobo.790.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.791.1">In this chapter, we have learned how descriptive statistics and machine learning models can be used to quantify the difference between populations, which can later be used to validate business hypotheses as well as to assess the lift of certain marketing activities. </span><span class="koboSpan" id="kobo.791.2">We have also learned how to study the relationship of variables with the use of correlation and causation analysis, and how to model these relationships with linear models. </span><span class="koboSpan" id="kobo.791.3">Finally, we have built machine learning models to predict and </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">classify variables.</span></span></p>
<p><span class="koboSpan" id="kobo.793.1">In the next chapter, we will learn how to use results from web searches and how to apply this in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">market research.</span></span></p>
</div>


<div class="Content" id="_idContainer063">
<h1 id="_idParaDest-33"><a id="_idTextAnchor290"/><span class="koboSpan" id="kobo.1.1">Part 2: Market and Customer Insights</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part, you will learn how to obtain and analyze market data by leveraging certain tools. </span><span class="koboSpan" id="kobo.2.2">This part will teach you how to obtain search trends, enrich trends with similar queries, use scraping tools to obtain data, and structure the results to drive better decisions using effective visualizations to monitor key business </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">performance KPIs.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B19026_03.xhtml#_idTextAnchor291"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 3</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Finding Business Opportunities with Market Insights</span></em></li>
<li><a href="B19026_04.xhtml#_idTextAnchor442"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Understanding Customer Preferences with Conjoint Analysis</span></em></li>
<li><a href="B19026_05.xhtml#_idTextAnchor574"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Selecting the Optimal Price with Price Demand Elasticity</span></em></li>
<li><a href="B19026_06.xhtml#_idTextAnchor703"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Product Recommendation </span></em></li>
</ul>
</div>
<div>
<div id="_idContainer064">
</div>
</div>
<div>
<div id="_idContainer065">
</div>
</div>
</body></html>