["```py\nfrom scipy.stats import ttest_ind\nimport pandas as pd\ndm = pd.read_csv('matzke_et_al.csv')\ndm_horizontal = dm[dm.Condition=='Horizontal']\ndm_fixation = dm[dm.Condition=='Fixation']\nt, p = ttest_ind(dm_horizontal.CriticalRecall, dm_fixation.CriticalRecall)\nprint('t = {:.3f}, p = {:.3f}'.format(t, p))\n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt # visualization\nsns.barplot(x='Condition', y='CriticalRecall', data=dm)\nplt.xlabel('Condition')\nplt.ylabel('Memory performance')\nplt.show()\n```", "```py\nfrom scipy.stats import ttest_rel\ndm = pd.read_csv('moon-aggression.csv')\nt, p = ttest_rel(dm.Moon, dm.Other)\nprint('t = {:.3f}, p = {:.3f}'.format(t, p))\n```", "```py\nfrom statsmodels.stats.anova import anova_lm\ndm = pd.read_csv('heartrate.csv')\ndm = dm.rename({'Heart Rate':'HeartRate'},axis=1)  # statsmodels doesn't like spaces\ndf = anova_lm(ols('HeartRate ~ Gender * Group', data=dm).fit())\nprint(df)\n```", "```py\nfrom statsmodels.formula.api import ols\ndm = pd.read_csv('gpa.csv')\nmodel = ols('gpa ~ satm + satv', data=dm).fit()\nprint(model.summary())\n```", "```py\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame([(.2, .3,.8), (.0, .6,.9), (.6, .0,.4), (.2, .1,.9),(.1, .3,.7), (.1, .5,.6), (.7, .1,.5), (.3, .0,.8),],columns=['dogs', 'cats','birds'])\ncorr_mat = df.corr()\n```", "```py\nimport seaborn as sn\nsn.heatmap(corr_mat, annot=True)\n```", "```py\ndf1=pd.DataFrame( np.random.randn(3,2), columns=['a','b'] )\ndf2=pd.DataFrame( np.random.randn(3,2), columns=['a','b'] )\n```", "```py\ncorr_mat = df1.corr()\nsn.heatmap(corr_mat, annot=True)\nplt.show()\n```", "```py\ndf1.corrwith(df2)\n```", "```py\ndf1.corrwith(df2.set_axis( df1.columns, axis='columns', inplace=False))\n```", "```py\ncorr_mat = pd.concat([df1,df2],axis=1).corr()\nsn.heatmap(corr_mat, annot=True)\nplt.show()\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport random\nds = pd.DataFrame(columns = ['x','y'])\nds['x'] = [int(n>500) for n in random.sample(range(0, 1000), \n      100)]\nds['y'] = [int(n>500) for n in random.sample(range(0, 1000), \n      100)]\nds.head()\n```", "```py\nbase,var  = ds[ds.x == 0], ds[ds.x == 1]\ndelta = var.y.mean() - base.y.mean()\ndelta_dev = 1.96 * np.sqrt(var.y.var() / var.shape[0] \n      +base.y.var() / base.shape[0])\nprint(\"estimated_effect\":,delta, \"standard_error\": delta_dev)\n```", "```py\nfrom scipy.stats import chi2_contingency\ncontingency_table = (\n         ds \n         .assign(placeholder=1) \n         .pivot_table(index=\"x\", columns=\"y\", \n              values=\"placeholder\", aggfunc=\"sum\") \n         .values)\n_, p, _, _ = chi2_contingency(contingency_table, \n     lambda_=\"log-likelihood\")\n```", "```py\nprint(\"P-value:\",p)\n```", "```py\ndata_1 = pd.read_csv('observed_data_1.csv' )\ndata_1.plot.scatter(x=\"z\", y=\"y\", c=\"x\", cmap=\"rainbow\", colorbar=False)\n```", "```py\nimport seaborn as sns\nsns.kdeplot(data_1.loc[lambda df: df.x == 0].y, label=\"untreated\")\nsns.kdeplot(data_1.loc[lambda df: df.x == 1].y, label=\"treated\")\n```", "```py\n!pip install causalinference\nfrom causalinference import CausalModel\ncm = CausalModel(\n    Y=data_1.y.values,\n    D=data_1.x.values,\n    X=data_1.z.values)\ncm.est_via_ols(adj=1)\nprint(cm.estimates)\n```", "```py\ndata_2 = pd.read_csv('observed_data_2.csv')\ndata_2.plot.scatter(x=\"z\", y=\"y\", c=\"x\", cmap=\"rainbow\", colorbar=False)\n```", "```py\ncm = CausalModel(\n    Y=data_2 .y.values,\n    D=data_2 .x.values,\n    X=data_2 .z.values)\ncm.est_via_ols(adj=1)\n```", "```py\nprint(cm.estimates)\n```", "```py\nfrom sklearn import preprocessing\nx_train = pd.DataFrame([[ 1., -1.,  2.],\n                    [ 2.,  0.,  0.],\n                    [ 0.,  1., -1.]],columns=['x','y','z'])\nscaler = preprocessing.StandardScaler().fit(x_train)\n```", "```py\nscaler.mean_\n```", "```py\nscaler.scale_\n```", "```py\nx_scaled = scaler.transform(x_train)\n```", "```py\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_train_minmax = min_max_scaler.fit_transform(x_train)\n```", "```py\nfrom sklearn.datasets import load_iris\ndata = load_iris()\nx, y = data['data'],data['target']\nquantile_transformer = preprocessing.QuantileTransformer(\nn_quantiles=5)\nx_train_qt = quantile_transformer.fit_transform(x)\nx_train_qt[:5]\n```", "```py\nquantile_transformer = preprocessing.QuantileTransformer(\nn_quantiles=5,output_distribution='normal')\nx_trans_qt = quantile_transformer.fit_transform(x)\nquantile_transformer.quantiles_\n```", "```py\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=len(set(y)), random_state=0).fit(x)\nkmeans.labels_\n```", "```py\nkmeans.predict(x[0].reshape(1,-1))\n```", "```py\nkmeans.cluster_centers_\n```", "```py\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(x)\n```", "```py\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n```", "```py\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\ndata_reg = load_diabetes()\nx,y = data_reg['data'],data_reg['target']\nreg = LinearRegression().fit(x, y)\nreg.score(x, y)\n```", "```py\nreg.coef_\n```", "```py\nreg.intercept_\n```", "```py\nprint('Predicted:',reg.predict(x[10].reshape(\n1,-1)),'Actual:',y[10])\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_digits\ndata_class = load_digits()\nx,y = data_class['data'],data_class['target']\nclf = make_pipeline(StandardScaler(), \n      LogisticRegression(penalty='l2',C=.1))\nclf.fit(x, y)\nclf.predict(x[:2, :])\n```", "```py\nclf.score(x, y)\n```", "```py\nx, y = load_iris(return_X_y=True)\nx.shape, y.shape\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nx_train, x_test, y_train, y_test = train_test_split(x, y, \ntest_size=0.4, random_state=0)\n```", "```py\nx_train.shape, y_train.shape\n```", "```py\nx_test.shape, y_test.shape\n```", "```py\nclf = svm.SVC(kernel='linear', C=.7).fit(x_train, y_train)\nclf.score(x_test, y_test)\n```", "```py\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, x, y, cv=5)\n```", "```py\nprint('Mean:',scores.mean(),'Standard Deviation:', \nscores.std())\n```", "```py\nfrom sklearn import metrics\nscores = cross_val_score(clf, x, y, cv=5, scoring='f1_macro')\nscores\n```", "```py\nfrom sklearn.model_selection import ShuffleSplit\nn_samples = x.shape[0]\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(clf, x, y, cv=cv)\n```"]