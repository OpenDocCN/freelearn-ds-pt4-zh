- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Machine Learning in Business Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is an area of research focused on comprehending and developing
    “learning” processes, or processes that use data to enhance performance on a given
    set of tasks. It is considered to be a component of artificial intelligence. Among
    them, machine learning is a technology that enables companies to efficiently extract
    knowledge from unstructured data. With little to no programming, machine learning—and
    more precisely, machine learning algorithms—can be used to iteratively learn from
    a given dataset and comprehend patterns, behaviors, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Validate the difference of observed effects with statistical analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the correlation and causation as well as model relationships between
    variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the data for clustering and machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop machine learning models for regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to be able to follow the steps in this chapter, you will need to meet
    the next requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a Jupyter notebook instance running Python 3.7 and above. You can use the
    Google Colab notebook to run the steps as well if you have a Google Drive account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an understanding of basic math and statistical concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the example datasets provided in the book’s GitHub page, and the original
    source is [https://python.cogsci.nl/numerical/statistics/](https://python.cogsci.nl/numerical/statistics/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the effect of changes with the t-test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When measuring the effects of certain actions applied to a given population
    of users, we need to validate that these actions have actually affected the target
    groups in a significant manner. To be able to do this, we can use the t-test.
  prefs: []
  type: TYPE_NORMAL
- en: A t-test is a statistical test that is used to compare the means of two groups
    to ascertain whether a method or treatment has an impact on the population of
    interest or whether two groups differ from one another; it is frequently employed
    in hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: When the datasets in the two groups don’t relate to identical values, separate
    t-test samples are chosen independently of one another. They might consist of
    two groups of randomly selected, unrelated patients to study the effects of a
    medication, for example. While the other group receives the prescribed treatment,
    one of the groups serves as the control group and is given a placebo. This results
    in two separate sample sets that are unpaired and unconnected from one another.
    Simply put, the t-test is employed to compare the means of two groups. It is frequently
    employed in hypothesis testing to establish whether a procedure or treatment truly
    affects the population of interest or whether two groups differ from one another.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test is used in the context of businesses to compare two different means
    and determine whether they represent the exact same population, and it’s especially
    useful in validating the effects of promotions applied in the uplift of sales.
    Additionally, it enables firms to comprehend the likelihood that their outcomes
    are the product of chance.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to make an independent-samples t-test using the SciPy package
    and the Matzke et al. dataset (2015). Participants in this dataset underwent a
    memory challenge in which they had to recollect a list of words. One group of
    participants focused on a central fixation dot on a display during the retention
    interval. Another group of volunteers continuously moved their eyes horizontally,
    which some people think helps with memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine whether memory performance (`CriticalRecall`) was better for the
    horizontal eye movement group than the fixation group, we can utilize the `ttest_ind`
    function from the SciPy library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.1: T-test result ](img/B19026_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: T-test result'
  prefs: []
  type: TYPE_NORMAL
- en: 'The test’s p-value, which may be found on the output, is all you need to evaluate
    the t-test findings. Simply compare the output’s p-value to the selected alpha
    level to conduct a hypothesis test at the desired alpha (significance) level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can reject the null hypothesis if the p-value is less than your threshold
    for significance (for example, 0.05). The two means’ difference is statistically
    significant. The data from your sample is convincing enough to support the conclusion
    that the two population means are not equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Population distribution ](img/B19026_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Population distribution'
  prefs: []
  type: TYPE_NORMAL
- en: A high t-score, also known as a t-value, denotes that the groups are distinct,
    whereas a low t-score denotes similarity. Degrees of freedom, or the values in
    a study that can fluctuate, are crucial for determining the significance and veracity
    of the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the results indicate a noteworthy difference (p =.0066). The
    fixation group, however, outperformed the other groups, where the effect is in
    the opposite direction from what was anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to test the difference between two populations is using the paired-samples
    t-test, which compares a single group’s means for two variables. To determine
    whether the average deviates from 0, the process computes the differences between
    the values of the two variables for each occurrence. The means of two independent
    or unrelated groups are compared using an unpaired t-test. An unpaired t-test
    makes the assumption that the variance in the groups is equal. The variance is
    not expected to be equal in a paired t-test. The process also automates the calculation
    of the t-test effect size. The paired t-test is used when data are in the form
    of matched pairs, while the two-sample t-test is used when data from two samples
    are statistically independent.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the Moore, McCabe, and Craig datasets. Here, aggressive conduct in
    dementia patients was assessed during the full moon and another lunar phase. This
    was a within-subject design because measurements were taken from every participant
    at both times.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `ttest_rel` SciPy function to test whether aggression differed
    between the full moon and the other lunar phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the figure below, there was an interesting effect that was
    substantial, as the p values are never 0 as the output implies. This effect was
    such that people were indeed most violent during full moons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: T-test result of the aggression dataset ](img/B19026_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: T-test result of the aggression dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Another way in which we can compare the difference between two groups is the
    statistical method known as **analysis of variance** (**ANOVA**), which is used
    to examine how different means differ from one another. Ronald Fisher created
    this statistical test in 1918, and it has been in use ever since. Simply put,
    an ANOVA analysis determines whether the means of three or more independent groups
    differ statistically. So does ANOVA replace the t-test, then? Not really. ANOVA
    is used to compare the means among three or more groups, while the t-test is used
    to compare the means between two groups.
  prefs: []
  type: TYPE_NORMAL
- en: When employed in a business setting, ANOVA can be used to manage budgets by,
    for instance, comparing your budget against costs to manage revenue and inventories.
    ANOVA can also be used to manage budgets by, for instance, comparing your budget
    against costs to manage revenue and inventories. For example, in order to better
    understand how sales will perform in the future, ANOVA can also be used to forecast
    trends by examining data patterns. When assessing the multi-item scales used frequently
    in market research, ANOVA is especially helpful. Using ANOVA might assist you
    as a market researcher in comprehending how various groups react. You can start
    the test by accepting the null hypothesis, or that the means of all the groups
    that were observed are equal.
  prefs: []
  type: TYPE_NORMAL
- en: For our next example, let’s revisit the heart rate information provided by Moore,
    McCabe, and Craig. Gender and group are two subject-specific factors in this dataset,
    along with one dependent variable (heart rate). You need the following code to
    see if gender, group, or their interactions have an impact on heart rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a combination of `anova_lm`), which isn’t very elegant, but the
    important part is the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show us that heart rate is related to all factors: gender (F =
    185.980, p < .001), group (F = 695.647, p < .001), and the gender-by-group interaction
    (F = 7.409, p = .006).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: ANOVA test results ](img/B19026_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: ANOVA test results'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have validated that there is in fact difference between multiple
    groups, we can start to model these relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling relationships with multiple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The statistical method known as multiple linear regression employs two or more
    independent variables to forecast the results of a dependent variable. Using this
    method, analysts may calculate the model’s variance and the relative contributions
    of each independent variable to the overall variance. Regressions involving numerous
    explanatory variables, both linear and nonlinear, fall under the category of multiple
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of multiple regression analysis is so that researchers can evaluate
    the strength of the relationship between an outcome (the dependent variable) and
    a number of predictor variables, as well as the significance of each predictor
    to the relationship using multiple regression analysis frequently with the effect
    of other predictors statistically eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple regression includes multiple independent variables, whereas linear
    regression only takes into account one independent variable to affect the relationship’s
    slope.
  prefs: []
  type: TYPE_NORMAL
- en: Businesses can use linear regressions to analyze trends and generate estimates
    or forecasts. For instance, if a firm’s sales have been rising gradually each
    month for the previous several years, the corporation may anticipate sales in
    the months to come by doing a linear analysis of the sales data with monthly sales.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the dataset from Moore, McCabe, and Craig, which contains grade point
    averages and SAT scores for mathematics and verbal knowledge for high-school students.
    We can use the following code to test whether `satm` and `satv` are (uniquely)
    related to `gpa`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `OLS` SciPy function to evaluate this relationship, which is
    passed as a combination of the variables in question, and then fitted to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.5: OLS results ](img/B19026_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: OLS results'
  prefs: []
  type: TYPE_NORMAL
- en: The result shows us that only SAT scores for mathematics, but not for verbal
    knowledge, are uniquely related to the grade point average.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the concepts of correlation, which is when
    variables behave in a similar manner, and causation, which is when a variable
    affects another one.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing correlation and causation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The statistical measure known as correlation expresses how closely two variables
    are related linearly, which can be understood graphically as how close two curves
    overlap. It’s a typical technique for describing straightforward connections without
    explicitly stating cause and consequence.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation matrix displays the correlation values, which quantify how closely
    each pair of variables is related linearly. The correlation coefficients have
    a range of -1 to +1\. The correlation value is positive if the two variables tend
    to rise and fall together.
  prefs: []
  type: TYPE_NORMAL
- en: The four types of correlations that are typically measured in statistics are
    the Spearman correlation, Pearson correlation, Kendall rank correlation, and the
    point-biserial correlation.
  prefs: []
  type: TYPE_NORMAL
- en: In order for organizations to make data-driven decisions based on forecasting
    the result of events, correlation and regression analysis are used to foresee
    future outcomes. The two main advantages of correlation analysis are that it enables
    quick hypothesis testing and assists businesses in deciding which variables they
    wish to look into further. To determine the strength of the linear relationship
    between two variables, the primary type of correlation analysis applies Pearson’s
    `r` formula.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `corr` method in a pandas data frame, we can calculate the pairwise
    correlation of columns while removing NA/null values. The technique can be passed
    as a parameter with the `pearson` or `kendall` values for the standard correlation
    coefficient, `spearman` for the Spearman rank correlation, or `kendall` for the
    Kendall Tau correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `corr` method in a pandas data frame returns a matrix of floats from 1
    along the diagonals and symmetric regardless of the callable’s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the results correlation matrix using a seaborn heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.6: Correlation matrix ](img/B19026_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Correlation matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Finding groups of highly correlated features and only maintaining one of them
    is the main goal of employing pairwise correlation for feature selection, which
    aims to maximize the predictive value of your model by using the fewest number
    of features possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pairwise correlation is calculated between rows or columns of a DataFrame and
    rows or columns of a Series or DataFrame. The correlations are calculated after
    DataFrames have been aligned along both axes. Next, we can see an example that
    might make it more clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `corr` to compare numerical columns within the same data frame. Non-numerical
    columns will automatically be skipped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.7: Correlation matrix ](img/B19026_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Correlation matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also compare the columns of `df1` and `df2` with `corrwith`. Note that
    only columns with the same names are compared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To make things easier, we can rename the columns of `df2` to match the columns
    of `df1` if we would like for pandas to disregard the column names and only compare
    the first row of `df1` to the first row of `df2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to note that `df1` and `df2` need to have the same number of
    columns in that case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last but not least, you could also just horizontally combine the two datasets
    and utilize `corr`. The benefit is that this essentially functions independently
    of the quantity and naming conventions of the columns, but the drawback is that
    you can receive more output than you require or want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.8: Correlation heatmap ](img/B19026_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Correlation heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established the fact that two variables can be correlated using
    correlation analysis, we can seek to validate whether the variables are actually
    impacting one another using causation analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of one variable to impact another is known as causality. The first
    variable might create the second or might change the incidence of the second variable.
  prefs: []
  type: TYPE_NORMAL
- en: Causality is the process by which one event, process, state, or object influences
    the development of another event, process, condition, or object, where the cause
    and effect are both partially reliant on each other. So what distinguishes correlation
    from causation? Correlation does not automatically imply causation, even if causality
    and correlation might coexist. In situations where action A results in outcome
    B, causation is expressly applicable. Correlation, on the other hand, is just
    a relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the next dataset to study the causation between variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To study the causation, we can seek to estimate the difference in means between
    two groups. The absolute difference between the mean values in two different groups
    is measured by the mean difference, often known as the difference in means. It
    offers you a sense of how much the averages of the experimental group and control
    groups differ from one another in clinical studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we will estimate the uplift as a quantified difference
    in means along with the determined standard error. We will use 90 as the confidence
    interval in the range of the normal, which yields a z-score of 1.96:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.9: Estimated differences between populations ](img/B19026_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Estimated differences between populations'
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the contingency `chi-square` for the comparison of two groups
    with a dichotomous dependent variable. For example, we might contrast males and
    females using a yes/no response scale. The contingency chi-square is built on
    the same ideas as the straightforward chi-square analysis, which compares the
    anticipated and actual outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This statistical technique is used to compare actual outcomes with predictions.
    The goal of this test is to establish whether a discrepancy between observed and
    expected data is the result of chance or a correlation between the variables you
    are researching. The results create a contingency matrix from which we can infer
    that your variables are independent of one another and have no association with
    one another if C is close to zero (or equal to zero). There is a relationship
    if C is not zero; C can only take on positive values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will just seek to interpret the p-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.10: Resulting p-value ](img/B19026_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Resulting p-value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use a set of datasets that were synthetically generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.11: Plot of data distribution ](img/B19026_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Plot of data distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability density function of a continuous random variable can be estimated
    using the **kernel density estimation** (**KDE**) seaborn method. The area under
    the depicted curve serves as a representation of the probability distribution
    of the data values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.12: Density graph ](img/B19026_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Density graph'
  prefs: []
  type: TYPE_NORMAL
- en: To study causation, the researcher must build a model to describe the connections
    between ideas connected to a particular phenomenon in causal modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple causality—the idea that any given outcome may have more than one cause—is
    incorporated into causal models. For instance, social status, age, sex, ethnicity,
    and other factors may influence someone’s voting behavior. In addition, some of
    the independent or explanatory factors might be connected.
  prefs: []
  type: TYPE_NORMAL
- en: External validity can be addressed using causal models (whether results from
    one study apply to unstudied populations). In some cases, causal models can combine
    data to provide answers to questions that no single dataset alone is able to address.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `est_via_ols` function of the `causalinference` package to estimate
    average treatment effects using least squares.
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* is the potential outcome when treated, D is the treatment status,
    and X is a vector of covariates or individual characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter to control is `adj`, an int which can be either 0, 1, or 2\. This
    parameter indicates how covariate adjustments are to be performed. Setting `adj`
    to 0 will not include any covariates. Set `adj` to 1 to include treatment indicator
    D and covariates X separately, or set `adj` to 2 to additionally include interaction
    terms between D and X. The default is 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.13: Causal model results ](img/B19026_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.13: Causal model results'
  prefs: []
  type: TYPE_NORMAL
- en: The estimates show us that there is a negative relationship between the variables.
    The negative estimate might be an indication that the application of D reduces
    the probability of Y by 48%. It’s really important to look at the entire set of
    estimate distributions to draw any conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis of a hypothetical or counterfactual reality is causal analysis,
    because we must make claims about the counterfactual result that we did not witness
    in order to assess the treatment effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The data previously loaded will show us different values in the causal model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14: Data distribution ](img/B19026_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Data distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build the new causal model using the new loaded values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the treatment effect estimates to validate whether our causal
    model is valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.15: Causal model results with new data ](img/B19026_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: Causal model results with new data'
  prefs: []
  type: TYPE_NORMAL
- en: The estimates inform us that the relationship has become positive.
  prefs: []
  type: TYPE_NORMAL
- en: Causal models are a great way to validate the modeling and direction of relationships
    between the variables in data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into how we can use scaling to prepare our
    data for machine learning, depending on the distribution that it has.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling features to a range
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with machine learning models, it is important to preprocess data
    so certain problems such as an explosion of gradients or lack of proper distribution
    representation can be solved.
  prefs: []
  type: TYPE_NORMAL
- en: To transform raw feature vectors into a representation that is better suited
    for the downstream estimators, the `sklearn.preprocessing` package offers a number
    of common utility functions and transformer classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many machine learning estimators used in `scikit-learn` frequently require
    dataset standardization; if the individual features do not more or less resemble
    standard normally distributed data, they may behave poorly: Gaussian with a mean
    of 0 and a variation of 1.'
  prefs: []
  type: TYPE_NORMAL
- en: In general, standardizing the dataset is advantageous for learning algorithms.
    Robust scalers or transformers are preferable if there are any outliers in the
    collection. On a dataset with marginal outliers, the actions of several scalers,
    transformers, and normalizers are highlighted in the analysis of the impact of
    various scalers on data containing outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, we frequently ignore the distribution’s shape and simply adapt the
    data to scale by dividing non-constant features by their standard deviation and
    centering it by subtracting each feature’s mean value.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, several components of a learning algorithm’s objective function
    (such as the RBF kernel of SVMs or the l1 and l2 regularizers of linear models)
    may make the assumption that all features are centered around zero or have variance
    in the same order. A feature may dominate the objective function and prevent the
    estimator from successfully inferring from other features as expected if its variance
    is orders of magnitude greater than that of other features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StandardScaler` utility class, which the preprocessing module offers,
    makes it quick and simple to carry out the following operation on an array-like
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will fit the scaler to the data, assuming that our distribution
    is standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize now the mean of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Mean of the data ](img/B19026_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Mean of the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the scale as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is shown as an array of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17: Scale of the columns ](img/B19026_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Scale of the columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can scale the data using the `transform` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: A different method of standardization is to scale each feature’s maximum absolute
    value to one unit, or to a value between a predetermined minimum and maximum value,
    usually zero and one. `MaxAbsScaler` or `MinMaxScaler` can be used to do this.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness to very small standard deviations of features and the preservation
    of zero entries in sparse data are two reasons to employ this scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'To scale a toy data matrix to the [0, 1] range, consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In case our distribution differs from the standard Gaussian, we can use non-linear
    transformations. There are two different kinds of transformations: power and quantile
    transform. The rank of the values along each feature is preserved by both quantile
    and power transforms because they are based on monotonic transformations of the
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the formula, which is the cumulative distribution function of the
    feature and the quantile function of the desired output distribution, quantile
    transformations place all features into the same desired distribution. These two
    facts are used in this formula: it is uniformly distributed if it is a random
    variable with a continuous cumulative distribution function, and it has distribution
    if it is a random variable with a uniform distribution on. A quantile transform
    smoothes out atypical distributions using a rank transformation and is less susceptible
    to outliers than scaling techniques. Correlations and distances within and between
    features are, however, distorted by it.'
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn provides a series of parametric transformations called power transforms
    that aim to translate data from any distribution to one that resembles a Gaussian
    distribution as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can map our data to a uniform distribution using `QuantileTransformer`,
    which provides a non-parametric transformation to map the data to a uniform distribution
    with values between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the resulting array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18: Transformed data ](img/B19026_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Transformed data'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to map data to a normal distribution using `QuantileTransformer`
    by setting `output_distribution=''normal''`. The following example uses the earlier
    example with the `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.19: Transformed data through the quantiles method ](img/B19026_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Transformed data through the quantiles method'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code will scale the data using a quantile transformation, defining
    five quantiles and looking to have a normal distribution in the output.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the proper distribution to be utilized, we need to analyze in depth
    the distribution of our variables, as the wrong transformation can make us lose
    details that might be important to take into account.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into unsupervised learning by looking at clustering
    algorithms using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data and reducing the dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of clustering involves grouping the population or data points into
    a number of groups so that the data points within each group are more similar
    to one another than the data points within other groups. Simply said, the goal
    is to sort any groups of people who share similar characteristics into clusters.
    It is frequently used in business analytics. How to arrange the enormous volumes
    of available data into useful structures is one of the issues that organizations
    are currently confronting.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation, grouping web pages, market segmentation, and information
    retrieval are four examples of how clustering can help firms better manage their
    data. Data clustering is beneficial for retail firms since it influences sales
    efforts, customer retention, and customer shopping behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the vector quantization technique known as “K-means clustering,”
    which has its roots in signal processing, is to divide a set of n observations
    into k clusters, each of which has as its prototype in the observation with the
    closest mean. K-means clustering is an unsupervised technique that uses the input
    data as is and doesn’t require a labeled response. A popular method for clustering
    is K-means clustering. Typically, practitioners start by studying the dataset’s
    architecture. Data points are grouped by K-means into distinct, non-overlapping
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code, we can use `KMeans` to fit the data in order to label each
    data point to a given cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.20: Cluster data ](img/B19026_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: Cluster data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can predict to which cluster each new instance of data belongs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.21: Predicted data ](img/B19026_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: Predicted data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also visualize the cluster centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.22: Cluster centers ](img/B19026_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: Cluster centers'
  prefs: []
  type: TYPE_NORMAL
- en: '`KMeans` allows us to find the characteristics in common data when the number
    of variables is too high and it’s useful for segmentation. But sometimes, there
    is the need to reduce the number of dimensions to a set of grouped variables with
    common traits.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to project the data into a lower dimensional environment, we can use
    `TruncatedSVD` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up next, we fit data into two components in order to reduce the dimensionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We should strive to account for the maximum amount of variance possible, which
    in simple terms can be understood as the degree to which our model can explain
    the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.23: PCA singular values ](img/B19026_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: PCA singular values'
  prefs: []
  type: TYPE_NORMAL
- en: After we have worked our data to preprocess it, reducing the number of dimensions
    and clustering, we can now build machine learning models to make predictions of
    future behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will build machine learning models that we can use to
    predict new data labels for regression and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Building machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most simple machine learning models we can construct to make a forecast
    of future behaviors is linear regression, which reduces the residual sum of squares
    between the targets observed in the dataset and the targets anticipated by the
    linear approximation, fitting a linear model using coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: This is simply ordinary least squares or non-negative least squares wrapped
    in a predictor object from the implementation perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this really simply by using the `LinearRegression` class in
    Sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.24: Model regression score ](img/B19026_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.24: Model regression score'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code will fit a linear regression model to our data and print
    the score of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print the coefficients, which give us a great estimation of the
    contribution of each variable to explain the variable we are trying to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.25: Regression coefficients ](img/B19026_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.25: Regression coefficients'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print the intercept variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.26: Regression intercepts ](img/B19026_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.26: Regression intercepts'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use the model to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.27: Predicted regression values ](img/B19026_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.27: Predicted regression values'
  prefs: []
  type: TYPE_NORMAL
- en: Here we are predicting a continuous variable, but we can also predict categorical
    variables using a classifier instead of a regression.
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn gives us the option of using the logistic regression (`logit` and `MaxEnt`)
    classifier, in which in the multiclass case, the training algorithm uses the one-vs-rest
    (`OvR`) scheme if the `'multi_class'` option is set to ‘`ovr'` and uses the cross-entropy
    loss if the `'multi_class'` option is set to `'multinomial'`. This class uses
    the `'liblinear'` library, `'newton-cg','sag','saga'`, and the `'lbfgs'` solvers
    to implement regularized logistic regression. Keep in mind that regularization
    is used by default. Both dense and sparse input can be handled by it. For best
    speed, only use matrices with 64-bit floats; all other input formats will be transformed.
  prefs: []
  type: TYPE_NORMAL
- en: The sole regularization supported by the `"newton-cg,"` `"sag,"` and `"lbfgs"`
    solvers is the L2 regularization with the primal formulation. The `"liblinear"`
    solver supports both the L1 and L2 regularizations, however, only the L2 penalty
    has a dual formulation. The only solver that supports the elastic net regularization
    is the `"``saga"` solver.
  prefs: []
  type: TYPE_NORMAL
- en: 'When fitting the model, the underlying C program chooses features using a random
    number generator. Thus, slightly varied outputs for the same input data are common.
    Try using a smaller `tol` parameter if it occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.28: Logistic regression results ](img/B19026_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.28: Logistic regression results'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also score the model to assess the precision of our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.29: User data ](img/B19026_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.29: User data'
  prefs: []
  type: TYPE_NORMAL
- en: In order to validate the model, we can use cross-validation, which allows us
    to evaluate the estimator’s performance. This is a methodological error in learning
    the parameters of a prediction function and evaluating it on the same set of data.
    A model that simply repeats the labels of the samples it has just seen would score
    well but be unable to make any predictions about data that has not yet been seen.
    Overfitting is the term for this circumstance. It is customary to reserve a portion
    of the available data as a test set (x test, y test) when conducting a (supervised)
    machine learning experiment in order to avoid this problem.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the term “experiment” does not just refer to academic
    purposes because machine learning experiments sometimes begin in commercial contexts
    as well. Grid search methods can be used to find the optimal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `scikit-learn`, a random split into training and test sets can be quickly
    computed with the `train_test_split` helper function. Let’s load the `iris` dataset
    to fit a linear support vector machine on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.30: Data shape ](img/B19026_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.30: Data shape'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now quickly sample a training set while holding out 40% of the data
    for testing (evaluating) our classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can validate the shape of the generated train dataset by looking at the
    `numpy` array shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.31: Train data shape ](img/B19026_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.31: Train data shape'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can repeat the same with the `test` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.32: Test data shape ](img/B19026_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.32: Test data shape'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can train our machine learning model on the training data and score
    it using the `test` dataset, which holds data points not seen by the model during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.33: Logistic regression scores ](img/B19026_02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.33: Logistic regression scores'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is still a chance of overfitting on the test set when comparing various
    settings of hyperparameters for estimators, such as the C setting that must be
    manually selected for an SVM. This is because the parameters can be adjusted until
    the estimator performs at its best. In this method, the model may “leak” information
    about the test set, and evaluation measures may no longer reflect generalization
    performance. This issue can be resolved by holding out a further portion of the
    dataset as a “validation set”: training is conducted on the training set, followed
    by evaluation on the validation set, and when it appears that the experiment has
    succeeded, a final evaluation can be conducted on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: However, by dividing the available data into three sets, we dramatically cut
    down on the number of samples that can be used to train the model, and the outcomes
    can vary depending on the randomization of the pair of (train and validation)
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-validation is an approach that can be used to address this issue (CV
    for short). When doing CV, the validation set is no longer required, but a test
    set should still be kept aside for final assessment. The fundamental strategy,
    known as a k-fold CV, divides the training set into k smaller sets (other approaches
    are described below, but generally follow the same principles). Every single one
    of the k “folds” is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The folds are used as training data for a model, and the resulting model is
    validated using the remaining portion of the data (as in, it is used as a test
    set to compute a performance measure such as accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: The average of the numbers calculated in the loop is then the performance indicator
    supplied by k-fold cross-validation. Although this method can be computationally
    expensive, it does not waste a lot of data (unlike fixing an arbitrary validation
    set), which is a significant benefit in applications such as inverse inference
    where there are few samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute the cross-validated metrics by calling the `cross_val` score
    helper function on the estimator and the dataset is the simplest approach to apply
    cross-validation. The example that follows shows how to split the data, develop
    a model, and calculate the score five times in a row (using various splits each
    time) to measure the accuracy of a linear kernel support vector machine on the
    `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean score and the standard deviation are hence given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The estimator’s scoring technique is by default used to calculate the score
    at each CV iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.34: CV mean scores ](img/B19026_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.34: CV mean scores'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be altered by applying the `scoring` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the samples in the iris dataset are distributed evenly throughout the
    target classes, the accuracy and F1 score are nearly equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.35: CV scores ](img/B19026_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.35: CV scores'
  prefs: []
  type: TYPE_NORMAL
- en: The CV score defaults to using the `KFold` or `StratifiedKFold` strategies when
    the `cv` parameter is an integer, with the latter being utilized if the estimator
    comes from `ClassifierMixin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use other CV strategies by passing a CV iterator using
    the `ShuffleSplit` Sklearn class instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will show us the CV scores on multiple folds of test samples,
    which can be used to prevent the overfitting problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.36: Results using shuffle split ](img/B19026_02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.36: Results using shuffle split'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding results show us the results of the CV score.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how descriptive statistics and machine learning
    models can be used to quantify the difference between populations, which can later
    be used to validate business hypotheses as well as to assess the lift of certain
    marketing activities. We have also learned how to study the relationship of variables
    with the use of correlation and causation analysis, and how to model these relationships
    with linear models. Finally, we have built machine learning models to predict
    and classify variables.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to use results from web searches and
    how to apply this in the context of market research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Market and Customer Insights'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn how to obtain and analyze market data by leveraging
    certain tools. This part will teach you how to obtain search trends, enrich trends
    with similar queries, use scraping tools to obtain data, and structure the results
    to drive better decisions using effective visualizations to monitor key business
    performance KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part covers the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B19026_03.xhtml#_idTextAnchor291), *Finding Business Opportunities
    with Market Insights*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19026_04.xhtml#_idTextAnchor442), *Understanding Customer Preferences
    with Conjoint Analysis*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19026_05.xhtml#_idTextAnchor574), *Selecting the Optimal Price
    with Price Demand Elasticity*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19026_06.xhtml#_idTextAnchor703), *Product Recommendation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
