<html><head></head><body>
<div id="_idContainer110">
<h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor442"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor443"/><span class="koboSpan" id="kobo.2.1">Understanding Customer Preferences with Conjoint Analysis</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Conjoint analysis is a well-known approach to product and pricing research that identifies customer preferences and makes use of that knowledge to choose product features, evaluate price sensitivity, anticipate market shares, and foretell consumer acceptance of new goods </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">or services.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Conjoint analysis is often utilized for all sorts of items, including consumer goods, electrical goods, life insurance policies, retirement communities, luxury goods, and air travel, across several sectors. </span><span class="koboSpan" id="kobo.5.2">It may be used in a variety of situations that revolve around learning what kind of product customers are most likely to purchase and which features consumers value the most (and least) in a product. </span><span class="koboSpan" id="kobo.5.3">As a result, it is widely used in product management, marketing, </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">and advertising.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">Conjoint analysis is beneficial for businesses of all sizes, even small local eateries and </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">grocery stores.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">In this chapter, you will learn how to understand how conjoint analysis is used in market research and how experiments are performed. </span><span class="koboSpan" id="kobo.9.2">You will then perform a conjoint analysis using </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">ordinary least squares</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">OLS</span></strong><span class="koboSpan" id="kobo.13.1">) models and predict the performance of new product features using different </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">machine learning</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.16.1">ML</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">) models.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">conjoint analysis</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Designing a </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">conjoint experiment</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Determining a product’s </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">relevant attributes</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">OLS with Python </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">and Statsmodels</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Working with more </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">product features</span></span></li>
<li><span class="koboSpan" id="kobo.30.1">Predicting new </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">feature combinations</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">Let’s jump into the analysis using some simulation data for consumer </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">retail products.</span></span></p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor444"/><span class="koboSpan" id="kobo.34.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.35.1">In order to be able to follow the steps in this chapter, you will need to meet the </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">next requirements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.37.1">A Jupyter notebook instance running Python 3.7 and above. </span><span class="koboSpan" id="kobo.37.2">You can use a Google Colab notebook to run the steps as well if you have a Google </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">Drive account.</span></span></li>
<li><span class="koboSpan" id="kobo.39.1">An understanding of basic math and </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">statistical concepts.</span></span></li>
</ul>
<h1 id="_idParaDest-46"><a id="_idTextAnchor445"/><span class="koboSpan" id="kobo.41.1">Understanding conjoint analysis</span></h1>
<p><span class="koboSpan" id="kobo.42.1">Conjoint analysis is a research-based statistical method used in market research to determine how people evaluate the different attributes (characteristics, functions, and benefits) that make up a single product </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">or service.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">Conjoint analysis has its roots in </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.45.1">mathematical psychology, the goal of which is to determine which combination of a limited number of attributes has the greatest impact on respondents’ choices and decisions. </span><span class="koboSpan" id="kobo.45.2">Respondents are presented with a controlled set of potential products or services, and by analyzing how to choose from those products, an implicit assessment of each component of the product or service is made. </span><span class="koboSpan" id="kobo.45.3">You can decide. </span><span class="koboSpan" id="kobo.45.4">You can use these implicit ratings (utilities or fractions) to create market models that estimate market share, sales, and even the profitability of </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">new designs.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">There are different types </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.48.1">of conjoint studies that may </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">be designed:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.50.1">Ranking-based conjoint</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.51.1">Rating-based conjoint</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.52.1">Choice-based conjoint</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.53.1">Conjoint analysis is also utilized in a </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.54.1">variety of social sciences and practical sciences, including operations research, product management, and marketing. </span><span class="koboSpan" id="kobo.54.2">It is commonly employed in service design, advertising appeal analysis, and consumer acceptability testing of new product designs. </span><span class="koboSpan" id="kobo.54.3">Although it has been applied to product placement, some object to this use of </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">conjoint analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">Conjoint analysis approaches, which are a subset of a larger group of trade-off analysis tools used for systematic decision analysis, are also known as multi-attribute compositional modeling, discrete choice modeling, or expressed preference research. </span><span class="koboSpan" id="kobo.56.2">Conjoint analysis </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.57.1">breaks down a product or service into its called attributes and tests different combinations of those components to determine </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">consumer preferences.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">In the next diagram, we see how products are a combination of different levels of features—for example, a bag of chips can be represented with levels such as brand, flavor, size, </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">and price:</span></span><a id="_idTextAnchor446"/></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.61.1"><img alt="Figure 4.1: Different products as unique combinations of features " src="image/B19026_04_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.62.1">Figure 4.1: Different products as unique combinations of features</span></p>
<p><span class="koboSpan" id="kobo.63.1">Here’s how the combination of these attributes and levels may appear as options to a respondent in a conjoint </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">choice task:</span></span></p>
<p class="IMG---Figure"><a id="_idTextAnchor447"/></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.65.1"><img alt="Figure 4.2: Conjoint choice task " src="image/B19026_04_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.66.1">Figure 4.2: Conjoint choice task</span></p>
<p><span class="koboSpan" id="kobo.67.1">The conjoint analysis takes</span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.68.1"> a more realistic approach, rather than just asking what you like about the product or which features are </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">most important.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">During a survey, each person is asked to choose between products with the same level of attributes, but different combinations of them. </span><span class="koboSpan" id="kobo.70.2">This model is called choice-based conjoint, which </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.71.1">in most cases is composed of 8 to 12 “battles” of products. </span><span class="koboSpan" id="kobo.71.2">This process looks to simulate the buying behavior, and the closer it gets to real-life conditions, the better. </span><span class="koboSpan" id="kobo.71.3">The selection of attributes and the battle setup are designed as an experiment that requires domain knowledge. </span><span class="koboSpan" id="kobo.71.4">The information is later used to weigh the influence of each one of the attributes on the buying patterns of </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">the users.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">Now that we know what conjoint analysis is and how it can be used to measure how customers ponder certain product characteristics, we will look into how each of these experiments can </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">be designe</span><a id="_idTextAnchor448"/><span class="koboSpan" id="kobo.75.1">d.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor449"/><span class="koboSpan" id="kobo.76.1">Designing a conjoint experiment</span></h1>
<p><span class="koboSpan" id="kobo.77.1">A product or service</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.78.1"> area is described as a set of attributes. </span><span class="koboSpan" id="kobo.78.2">For example, a laptop may have characteristics such as screen size, screen size, brand, and price. </span><span class="koboSpan" id="kobo.78.3">Therefore, each attribute can be divided into several levels—for example, the screen size can be 13, 14, or 15 inches. </span><span class="koboSpan" id="kobo.78.4">Respondents are presented with a set of products, prototypes, mockups, or images created from a combination of all or some layers of configuration attributes to select, rank, or display products for evaluation. </span><span class="koboSpan" id="kobo.78.5">You will be asked to rank. </span><span class="koboSpan" id="kobo.78.6">Each example is similar enough for consumers to consider it a good alternative but different enough for respondents to clearly identify their preferences. </span><span class="koboSpan" id="kobo.78.7">Each example consists of a unique combination of product features. </span><span class="koboSpan" id="kobo.78.8">The data can consist of individual ratings, rankings, or a selection from </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">alternative combinations.</span></span></p>
<p><span class="koboSpan" id="kobo.80.1">Conjoint design involves four </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">different steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.82.1">Identifying the sort </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">of research.</span></span></li>
<li><span class="koboSpan" id="kobo.84.1">Determining the </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">pertinent characteristics:</span></span><ul><li><span class="koboSpan" id="kobo.86.1">Be pertinent to </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">managerial choices</span></span></li><li><span class="koboSpan" id="kobo.88.1">Have different levels </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">in reality</span></span></li><li><span class="koboSpan" id="kobo.90.1">Be anticipated to </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">sway preferences</span></span></li><li><span class="koboSpan" id="kobo.92.1">Be comprehensible and </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">clearly defined</span></span></li><li><span class="koboSpan" id="kobo.94.1">Show no excessive correlations (price and brand are </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">an exception)</span></span></li><li><span class="koboSpan" id="kobo.96.1">At least two tiers should </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">be present</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.98.1">Specifying the </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">attributes’ levels:</span></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.100.1">Unambiguous</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.101.1">Mutually exclusive</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.102.1">Realistic</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.103.1">Design questionnaire</span></strong><span class="koboSpan" id="kobo.104.1">: The number of alternatives grows exponentially with the number of attribute and level combinations. </span><span class="koboSpan" id="kobo.104.2">One of the ways in which we deal with the exponential increase is by taking a fractional factorial design approach, which is frequently used to decrease the number of profiles to be examined while making sure that there is enough data available for statistical analysis. </span><span class="koboSpan" id="kobo.104.3">This in turn produces a well-controlled collection of “profiles” for the respondent </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">to consider.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.106.1">Taking these points into </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.107.1">account when designing a conjoint experiment will allow us to accurately model and replicate the consumer pattern we want to understand. </span><span class="koboSpan" id="kobo.107.2">We must always have in mind that the purpose of this analysis is to undercover hidden patterns by simulating the buying action as close to reality </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">as poss</span><a id="_idTextAnchor450"/><span class="koboSpan" id="kobo.109.1">ible.</span></span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor451"/><span class="koboSpan" id="kobo.110.1">Determining a product’s relevant attributes</span></h1>
<p><span class="koboSpan" id="kobo.111.1">As mentioned before, we will </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.112.1">perform a conjoint analysis to weigh the importance that a group of users gives to a given characteristic of a product or service. </span><span class="koboSpan" id="kobo.112.2">To achieve this, we will perform a multivariate analysis to determine the optimal product concept. </span><span class="koboSpan" id="kobo.112.3">By evaluating the entire product (overall utility value), it is possible to calculate the degree of influence on the purchase of individual elements (partial utility value). </span><span class="koboSpan" id="kobo.112.4">For example, when a user purchases a PC, it is possible to determine which factors affect this and how much (important). </span><span class="koboSpan" id="kobo.112.5">The same method can be scaled to include many </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">more features.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">The data to be used is in the form of different combinations of notebook features in terms of RAM, storage, and price. </span><span class="koboSpan" id="kobo.114.2">Different users ranked </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">these combinations.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">We will use the following Python modules in the </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">next example:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">Pandas</span></strong><span class="koboSpan" id="kobo.119.1">: Python package for data</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.120.1"> analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">data manipulation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">NumPy</span></strong><span class="koboSpan" id="kobo.123.1">:  Python package that </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.124.1">allows the use of matrices and arrays, as well as the use of mathematical and statistical functions to operate in </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">those matrices.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.126.1">Statsmodels</span></strong><span class="koboSpan" id="kobo.127.1">: Python package that provides a </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.128.1">complement to SciPy for statistical computations, including descriptive statistics and estimation and inference for statistical models. </span><span class="koboSpan" id="kobo.128.2">It provides classes and functions for the estimation of many different </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">statistical models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.130.1">Seaborn and Matplotlib</span></strong><span class="koboSpan" id="kobo.131.1">: Python packages for effective </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">data visualization.</span></span></li>
</ul>
<ol>
<li value="1"><span class="koboSpan" id="kobo.133.1">The next block of code will import the necessary packages and functions, as well as create a sample </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.134.1">DataFrame with </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">simul</span><a id="_idTextAnchor452"/><span class="koboSpan" id="kobo.136.1">ated data:</span></span><pre class="console"><span class="koboSpan" id="kobo.137.1">
import n</span><a id="_idTextAnchor453"/><span class="koboSpan" id="kobo.138.1">umpy as np</span></pre><pre class="console"><span class="koboSpan" id="kobo.139.1">
import pa</span><a id="_idTextAnchor454"/><span class="koboSpan" id="kobo.140.1">ndas as pd</span></pre><pre class="console"><span class="koboSpan" id="kobo.141.1">
import seab</span><a id="_idTextAnchor455"/><span class="koboSpan" id="kobo.142.1">orn as sns</span></pre><pre class="console"><span class="koboSpan" id="kobo.143.1">
import matplotlib.pyp</span><a id="_idTextAnchor456"/><span class="koboSpan" id="kobo.144.1">lot as plt</span></pre><pre class="console"><span class="koboSpan" id="kobo.145.1">
import statsmodels</span><a id="_idTextAnchor457"/><span class="koboSpan" id="kobo.146.1">.</span><a id="_idTextAnchor458"/><span class="koboSpan" id="kobo.147.1">api as sm</span></pre><pre class="console"><span class="koboSpan" id="kobo.148.1">
# data: Laptop</span><a id="_idTextAnchor459"/><span class="koboSpan" id="kobo.149.1"> spec data</span></pre><pre class="console"><span class="koboSpan" id="kobo.150.1">
data = pd.DataFrame([[6000, '4GB', '1</span><a id="_idTextAnchor460"/><span class="koboSpan" id="kobo.151.1">28GB', 3],</span></pre><pre class="console"><span class="koboSpan" id="kobo.152.1">
                     [6000, '8GB', '5</span><a id="_idTextAnchor461"/><span class="koboSpan" id="kobo.153.1">12GB', 9],</span></pre><pre class="console"><span class="koboSpan" id="kobo.154.1">
                     [8000, '4GB', '5</span><a id="_idTextAnchor462"/><span class="koboSpan" id="kobo.155.1">12GB', 5],</span></pre><pre class="console"><span class="koboSpan" id="kobo.156.1">
                     [8000, '8GB', '1</span><a id="_idTextAnchor463"/><span class="koboSpan" id="kobo.157.1">28GB', 7],</span></pre><pre class="console"><span class="koboSpan" id="kobo.158.1">
                     [6000, '4GB', '12</span><a id="_idTextAnchor464"/><span class="koboSpan" id="kobo.159.1">8GB', 4]],</span></pre><pre class="console"><span class="koboSpan" id="kobo.160.1">
                    columns=['price', 'memory', </span></pre><pre class="console"><span class="koboSpan" id="kobo.161.1">
                            'storage',</span><a id="_idTextAnchor465"/><span class="koboSpan" id="kobo.162.1"> 'score'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.163.1">
data.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.164.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">following </span><a id="_idTextAnchor466"/><span class="koboSpan" id="kobo.166.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.167.1"><img alt="Figure 4.3: Product features along with the score " src="image/B19026_04_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.168.1">Figure 4.3: Product features along with the score</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.169.1">The next code will </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.170.1">separate our data into predictor and </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">target </span><a id="_idTextAnchor467"/><span class="koboSpan" id="kobo.172.1">variables:</span></span><pre class="console"><span class="koboSpan" id="kobo.173.1">
X = data[[col for col in data.columns if col !=</span><a id="_idTextAnchor468"/><span class="koboSpan" id="kobo.174.1"> 'score']]</span></pre><pre class="console"><span class="koboSpan" id="kobo.175.1">
y = dat</span><a id="_idTextAnchor469"/><span class="koboSpan" id="kobo.176.1">a['score']</span></pre><pre class="console"><span class="koboSpan" id="kobo.177.1">
X.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.178.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">following </span><a id="_idTextAnchor470"/><span class="koboSpan" id="kobo.180.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.181.1"><img alt="Figure 4.4: Product features " src="image/B19026_04_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.182.1">Figure 4.4: Product</span><a id="_idTextAnchor471"/><span class="koboSpan" id="kobo.183.1"> features</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.184.1">These next lines of code will create dummy variables using the encoded </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">categorical variables:</span></span><pre class="console"><span class="koboSpan" id="kobo.186.1">
X_dum = pd.get_dummies(X, columns=X</span><a id="_idTextAnchor472"/><span class="koboSpan" id="kobo.187.1">.columns)</span></pre><pre class="console"><span class="koboSpan" id="kobo.188.1">
X_dum.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.189.1">This results in the</span><a id="_idIndexMarker170"/> <span class="No-Break"><span class="koboSpan" id="kobo.190.1">following </span><a id="_idTextAnchor473"/><span class="koboSpan" id="kobo.191.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.192.1"><img alt="Figure 4.5: Products and features in a one-hot representation " src="image/B19026_04_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.193.1">Figure 4.5: Products and features in a one-hot representation</span></p>
<p><span class="koboSpan" id="kobo.194.1">Now that the information has been properly encoded, we can use different predictor models to try to predict, based on the product characteristics, what would be the scoring of each product. </span><span class="koboSpan" id="kobo.194.2">In the next section, we can use an OLS regression model to determine the variable importance and infer which are the characteristics that users ponder most when</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.195.1"> selecting a produ</span><a id="_idTextAnchor474"/><span class="koboSpan" id="kobo.196.1">ct </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">to buy.</span></span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor475"/><span class="koboSpan" id="kobo.198.1">OLS with Python and Statsmodels</span></h1>
<p><span class="koboSpan" id="kobo.199.1">OLS, a kind of linear least </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.200.1">squares approach, is used in statistics to estimate unidentified parameters in a linear regression model. </span><span class="koboSpan" id="kobo.200.2">By minimizing the sum of squares of the differences between the observed values</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.201.1"> of the dependent variable and the values predicted by the linear function of the independent variable, OLS derives the parameters of a linear function from a set </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.202.1">of explanatory variables in accordance </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.203.1">with the least </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">squares principle.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">As a reminder, a linear regression model establishes the relationship between a dependent variable (</span><em class="italic"><span class="koboSpan" id="kobo.206.1">y</span></em><span class="koboSpan" id="kobo.207.1">) and at least one independent variable (</span><em class="italic"><span class="koboSpan" id="kobo.208.1">x</span></em><span class="koboSpan" id="kobo.209.1">), </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.211.1"><img alt="" src="image/Formula_04_001.jpg"/></span>
</div>
</div>
<p><span class="koboSpan" id="kobo.212.1">In the OLS method, we have to choose the values of </span><span class="koboSpan" id="kobo.213.1"><img alt="" src="image/Formula_04_002.png"/></span><span class="koboSpan" id="kobo.214.1"> and </span><span class="koboSpan" id="kobo.215.1"><img alt="" src="image/Formula_04_003.png"/></span><span class="koboSpan" id="kobo.216.1">, such that the total sum of squares of the difference between the calculated and observed values of </span><span class="koboSpan" id="kobo.217.1"><img alt="" src="image/Formula_04_004.png"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.218.1">is minimized.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">OLS can be described in geometrical terms as the sum of all the squared distances between each point to the regression surface. </span><span class="koboSpan" id="kobo.219.2">This distance is measured parallel to the axis of the adjusted surface, and the lower the distances, the better the surface will adjust to the data. </span><span class="koboSpan" id="kobo.219.3">OLS is a method especially useful when the errors are homoscedastic and uncorrelated between themselves, yielding great results when the variables used in regression </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">are exogenous.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">When the errors have finite variances, the OLS method provides a minimum-variance mean-unbiased estimate. </span><span class="koboSpan" id="kobo.221.2">OLS is the maximum likelihood estimator under the additional presumption that the errors are </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">normally distributed.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">We will use Python’s statsmodels module to implement the OLS method of </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">linear regr</span><a id="_idTextAnchor476"/><span class="koboSpan" id="kobo.225.1">ession:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.226.1">
model = sm.OLS(y, sm.add_constant(</span><a id="_idTextAnchor477"/><span class="koboSpan" id="kobo.227.1">X_dum))
result = mode</span><a id="_idTextAnchor478"/><span class="koboSpan" id="kobo.228.1">l.fit()
result.summary()</span></pre>
<p><span class="koboSpan" id="kobo.229.1">An </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.230.1">OLS regression</span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.231.1"> process is used to</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.232.1"> estimate the linear </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.233.1">relationship on the multivariate data, achieving an R-squared value </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">o</span><a id="_idTextAnchor479"/><span class="koboSpan" id="kobo.235.1">f </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.236.1">0.978</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.238.1"><img alt="Figure 4.6: OLS model summary " src="image/B19026_04_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.239.1">Figure 4.6: OLS model summary</span></p>
<p><span class="koboSpan" id="kobo.240.1">The resulting summary</span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.241.1"> shows us some basic information, as well as relevant metrics. </span><span class="koboSpan" id="kobo.241.2">Some of these are the R-squared values, the </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.242.1">number of observations used to train the model, the degrees of freedom in modality, the covariance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">other information.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">Some of the most important aspects to interpret in this summary are the </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">next values:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.246.1">R-squared</span></strong><span class="koboSpan" id="kobo.247.1">: R-squared, which calculates how much of the independent variable is explained by changes in </span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.248.1">our dependent variables, is perhaps the most significant statistic this summary produces. </span><span class="koboSpan" id="kobo.248.2">0.685, expressed as a percentage, indicates that our model accounts for 68.5% of the variation in our </span><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">'score'</span></strong><span class="koboSpan" id="kobo.250.1"> variable. </span><span class="koboSpan" id="kobo.250.2">With the property that the R-squared value of your model will never decrease with additional variables, your model may appear more accurate with multiple variables, even if they only contribute a small amount. </span><span class="koboSpan" id="kobo.250.3">This property is significant for analyzing the effectiveness of multiple dependent variables on the model. </span><span class="koboSpan" id="kobo.250.4">A lower adjusted score can suggest that some variables are not adequately contributing to the model since adjusted R-squared penalizes the R-squared formula based on the number </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">of variables.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">F-statistic</span></strong><span class="koboSpan" id="kobo.253.1">: The F-statistic evaluates </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.254.1">whether a set of variables is statistically significant by contrasting a linear model built for a variable with a model that eliminates the variable’s impact on 0. </span><span class="koboSpan" id="kobo.254.2">To correctly interpret this number, you must make use of the F table and the chosen alpha value. </span><span class="koboSpan" id="kobo.254.3">This value is used by probability (F statistics) to assess the validity of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">null</span></strong><span class="koboSpan" id="kobo.256.1"> hypothesis, or whether it is accurate to state that the effect of the variable is zero. </span><span class="koboSpan" id="kobo.256.2">You can see that this situation has a probability </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">of 15.3%.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Intercept</span></strong><span class="koboSpan" id="kobo.259.1">: If all of the variables in</span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.260.1"> our model were set to 0, the intercept would be the outcome. </span><span class="koboSpan" id="kobo.260.2">This is our b, a constant that is added to declare a starting value for our row in the classic linear equation “y = mx+b”. </span><span class="koboSpan" id="kobo.260.3">These are the variables below the intersection. </span><span class="koboSpan" id="kobo.260.4">The coefficient is the first useful column in our table. </span><span class="koboSpan" id="kobo.260.5">It is the section’s value for our section. </span><span class="koboSpan" id="kobo.260.6">It is a measurement of the impact of changing each variable on the independent variable. </span><span class="koboSpan" id="kobo.260.7">The “m” in “y = mx + b” is the culprit, with “m” being the rate of change value of the variable’s coefficient in the independent variable, or the outcome of a unit change in the dependent variable. </span><span class="koboSpan" id="kobo.260.8">They have an inverse relationship if the coefficient is negative, meaning that if one rises, then the </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">other declines.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.262.1">std error</span></strong><span class="koboSpan" id="kobo.263.1">: The coefficient’s standard deviation, or how much the coefficient varies among the data </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.264.1">points, is estimated by the std error (or standard error) variable, which is a measurement of the accuracy with which the coefficient was measured and is connected. </span><span class="koboSpan" id="kobo.264.2">In cases where we have a high t statistic, which denotes a high significance for your coefficient, this is produced by a low standard error in comparison to a </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">high coefficient.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.266.1">P&gt;|t|</span></strong><span class="koboSpan" id="kobo.267.1">: One of the most significant statistics in the summary is the p-value. </span><span class="koboSpan" id="kobo.267.2">The t statistic is used to generate the p-value, which expresses how likely it is that your coefficient was determined by chance in our model. </span><span class="koboSpan" id="kobo.267.3">A low p-value, such as 0.278, indicates that there is a 27.8% chance that the provided variable has no effect on the dependent variable and that our results are the result of chance. </span><span class="koboSpan" id="kobo.267.4">The p-value will be compared to an alpha value that has been predetermined, or a threshold, by which we can attach significance to our coefficient, in proper </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">model analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">[0.025 and 0.975]</span></strong><span class="koboSpan" id="kobo.270.1">: Is the range of measurements of values of our coefficients within 95% of our data or within two standard deviations? </span><span class="koboSpan" id="kobo.270.2">Outside of these, values can generally be considered outliers. </span><span class="koboSpan" id="kobo.270.3">Is the data contained between two standard deviations, where data outside of this range can be regarded </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">as outliers?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.272.1">Omnibus</span></strong><span class="koboSpan" id="kobo.273.1">: Using skew and kurtosis as metrics, Omnibus describes the normality of the distribution</span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.274.1"> of our residuals. </span><span class="koboSpan" id="kobo.274.2">0 would represent complete </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.275.1">normalcy. </span><span class="koboSpan" id="kobo.275.2">A statistical test called Prob (Omnibus) determines the likelihood that the residuals are normally distributed. </span><span class="koboSpan" id="kobo.275.3">1 would represent a distribution that is exactly normal. </span><span class="koboSpan" id="kobo.275.4">Skew, which ranges from 0 to perfect symmetry, measures the degree of symmetry in our data. </span><span class="koboSpan" id="kobo.275.5">Kurtosis gauges how peaky our data is or how concentrated it is at 0 on a normal curve. </span><span class="koboSpan" id="kobo.275.6">Fewer outliers are implied by </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">higher kurtosis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.277.1">Durbin-Watson</span></strong><span class="koboSpan" id="kobo.278.1">: The homoscedasticity, or </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.279.1">uniform distribution of mistakes in our data, is measured by the Durbin-Watson statistic. </span><span class="koboSpan" id="kobo.279.2">Heteroscedasticity would indicate an unequal distribution, such as when the relative error grows as the number of data points grows. </span><span class="koboSpan" id="kobo.279.3">Homoscedasticity should be between 1 and 2. </span><span class="koboSpan" id="kobo.279.4">Alternative ways to measure the same value of Omnibus and Prob (Omnibus) using asymmetry and kurtosis are Jarque-Bera (JB) and Prob. </span><span class="koboSpan" id="kobo.279.5">These ideals help us validate one another. </span><span class="koboSpan" id="kobo.279.6">A measure of how sensitive our model is to changes in the data it is processing is the condition number. </span><span class="koboSpan" id="kobo.279.7">Many different conditions strongly imply multicollinearity, which is</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.280.1"> a term to describe two or more independent variables that are strongly related to each other and are falsely affecting our predicted variable </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">by redundancy.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.282.1">In our case, we will use the</span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.283.1"> results of the data and store</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.284.1"> the variable names, their weights, and p-values in a DataFrame that later on we will</span><a id="_idTextAnchor480"/><span class="koboSpan" id="kobo.285.1"> use to plot </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.287.1">
data_res = pd.DataFrame({'name':</span><a id="_idTextAnchor481"/><span class="koboSpan" id="kobo.288.1"> result.params.keys(),
                         'weight':</span><a id="_idTextAnchor482"/><span class="koboSpan" id="kobo.289.1"> result.params.values,
                         'p_</span><a id="_idTextAnchor483"/><span class="koboSpan" id="kobo.290.1">val': result.pvalues})
d</span><a id="_idTextAnchor484"/><span class="koboSpan" id="kobo.291.1">ata_res = data_res[1:]
data_res</span></pre>
<p><span class="koboSpan" id="kobo.292.1">When looking at the p-value, if the significance level is below 5%, it can be an indication that the variable is not </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">stat</span><a id="_idTextAnchor485"/><span class="koboSpan" id="kobo.294.1">istically significant:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.295.1">
data_res = data_res.so</span><a id="_idTextAnchor486"/><span class="koboSpan" id="kobo.296.1">rt_values(by='weight')
data_res</span></pre>
<p><span class="koboSpan" id="kobo.297.1">This results in th</span><a id="_idTextAnchor487"/><span class="koboSpan" id="kobo.298.1">e </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<span class="koboSpan" id="kobo.300.1"><img alt="Figure 4.7: Product features’ weights and p-values " src="image/B19026_04_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.301.1">Figure 4.7: Product features’ weights and p-values</span></p>
<p><span class="koboSpan" id="kobo.302.1">Although it’s important to consider the p-value to establish a level of statistical certainty, the model </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.303.1">should be used with all variables. </span><span class="koboSpan" id="kobo.303.2">Here, we can use the Prob (F-statistic), which in our case is higher than 0.05, thus we </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.304.1">cannot reject the null hypothesis. </span><span class="koboSpan" id="kobo.304.2">By looking also at a very high R-squared value, we can see that the model is overfitting. </span><span class="koboSpan" id="kobo.304.3">We would need to have much more data in order to create a significant model, but I’ll leave that challenge </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">to you.</span></span></p>
<p><span class="koboSpan" id="kobo.306.1">The next screenshot shows the characteristics of the product ordered by relative weight. </span><span class="koboSpan" id="kobo.306.2">In this case, users positively weigh the 8 GB memory, followed by the 128 </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">GB storage:</span></span></p>
<pre class="source-code">
<a id="_idTextAnchor488"/><span class="koboSpan" id="kobo.308.1">sns.set()
</span><a id="_idTextAnchor489"/><span class="koboSpan" id="kobo.309.1">xbar = np.arange(len(data_res['weight']))
</span><a id="_idTextAnchor490"/><span class="koboSpan" id="kobo.310.1">plt.barh(xbar, data_res['weight'])
</span><a id="_idTextAnchor491"/><span class="koboSpan" id="kobo.311.1">plt.yticks(xbar, labels=data_res['name'])
</span><a id="_idTextAnchor492"/><span class="koboSpan" id="kobo.312.1">plt.xlabel('weight')
</span><a id="_idTextAnchor493"/><span class="koboSpan" id="kobo.313.1">plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.314.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<span class="koboSpan" id="kobo.316.1"><img alt="Figure 4.8: Product features’ weights sorted " src="image/B19026_04_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.317.1">F</span><a id="_idTextAnchor494"/><span class="koboSpan" id="kobo.318.1">igure 4.8: Product features’ weights sorted</span></p>
<p><span class="koboSpan" id="kobo.319.1">It can be seen that</span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.320.1"> memory has the highest contribution to evaluation, followed </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">by storage.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">We have seen how </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.323.1">OLS can be used as a mean to estimate the importance of each product feature. </span><span class="koboSpan" id="kobo.323.2">In this case, it has been modeled as a regression over a single variable, but we could include information about the type of respondent in order to discover how different customer segments react to </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">product features.</span></span></p>
<p><span class="koboSpan" id="kobo.325.1">In the next section, we evaluate a case of a consumer goods product with even </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">more features.</span></span><a id="_idTextAnchor495"/></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor496"/><span class="koboSpan" id="kobo.327.1">Working with more product features</span></h1>
<p><span class="koboSpan" id="kobo.328.1">In the example, we will use a</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.329.1"> dataset that contains many more features than the previous example. </span><span class="koboSpan" id="kobo.329.2">In this case, we will simulate data obtained from a crisp retail vendor that has asked some of its customers to rank its products according to their level </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">of preference:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.331.1">The following block of code will read the dataset, which is a CSV file, and will prompt us with </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">the result</span><a id="_idTextAnchor497"/><span class="koboSpan" id="kobo.333.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.334.1">
# Load dat</span><a id="_idTextAnchor498"/><span class="koboSpan" id="kobo.335.1">a</span></pre><pre class="console"><span class="koboSpan" id="kobo.336.1">
conjoint_dat = pd.read_csv('/content/conjoint_data.csv'</span><a id="_idTextAnchor499"/><span class="koboSpan" id="kobo.337.1">)</span></pre><pre class="console"><span class="koboSpan" id="kobo.338.1">
conjoint_dat</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.339.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<span class="koboSpan" id="kobo.341.1"><img alt="Figure 4.9: Crisps data " src="image/B19026_04_9.jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><a id="_idTextAnchor500"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.342.1">Figure 4.9: Crisps data</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.343.1">We can see that </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.344.1">the data contains only categorical values, so it will be necessary to transform this categorical data into a one-hot vector representation using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">get_dummies</span></strong><span class="koboSpan" id="kobo.346.1"> pandas function, which is what we do in the next block </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">of code</span><a id="_idTextAnchor501"/><span class="koboSpan" id="kobo.348.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.349.1">
conjoint_dat_dum = pd.get_dummies(conjoint_dat.iloc[:,:-1], columns = conjoint_dat.iloc[:,:-1].columns</span><a id="_idTextAnchor502"/><span class="koboSpan" id="kobo.350.1">)</span></pre><pre class="console"><span class="koboSpan" id="kobo.351.1">
conjoint_dat_dum</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.352.1">We can see that now we have created a set of columns that describe the product features using 1s </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">and 0s:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.354.1"><img alt="Figure 4.10: Crisps data in a one-hot representation " src="image/B19026_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><a id="_idTextAnchor503"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.355.1">Figure 4.10: Crisps data in a one-hot representation</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.356.1">We can now construct an OLS</span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.357.1"> model using the one-hot vector representation of the product features and passing the ranking as the </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">target variable</span><a id="_idTextAnchor504"/><span class="koboSpan" id="kobo.359.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.360.1">
main_effects_model_fit = sm.OLS(conjoint_dat['ranking'].astype(int), sm.add_constant(conjoint_dat_dum)</span><a id="_idTextAnchor505"/><span class="koboSpan" id="kobo.361.1">)</span></pre><pre class="console"><span class="koboSpan" id="kobo.362.1">
result = main_effects_model_fit.fit(</span><a id="_idTextAnchor506"/><span class="koboSpan" id="kobo.363.1">)</span></pre><pre class="console"><span class="koboSpan" id="kobo.364.1">
result.summary()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.365.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.367.1"><img alt="Figure 4.11: OLS regression results " src="image/B19026_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor507"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.368.1">Figure 4.11: OLS regression results</span></p>
<p><span class="koboSpan" id="kobo.369.1">The terms </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">AIC</span></strong><span class="koboSpan" id="kobo.371.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.372.1">BIC</span></strong><span class="koboSpan" id="kobo.373.1"> in the model summary, which stand for </span><strong class="bold"><span class="koboSpan" id="kobo.374.1">Akaike’s Information Criteria</span></strong><span class="koboSpan" id="kobo.375.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.376.1">Bayesian Information Criteria</span></strong><span class="koboSpan" id="kobo.377.1">, respectively, are frequently used in model </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.378.1">selection criteria; however, they are</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.379.1"> not interchangeable. </span><span class="koboSpan" id="kobo.379.2">BIC is a type of model selection among a class of parametric models with variable numbers of parameters, whereas AIC may be thought of as a measure of the</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.380.1"> goodness of fit of an estimated statistical model. </span><span class="koboSpan" id="kobo.380.2">The penalty for additional parameters is greater in BIC than in AIC. </span><span class="koboSpan" id="kobo.380.3">BIC penalizes free parameters more severely </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">than AIC.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">AIC typically looks for undiscovered models with high-dimensional reality. </span><span class="koboSpan" id="kobo.382.2">This indicates that AIC models are not accurate models. </span><span class="koboSpan" id="kobo.382.3">BIC, on the other hand, only encounters </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">True</span></strong><span class="koboSpan" id="kobo.384.1"> models. </span><span class="koboSpan" id="kobo.384.2">Additionally, BIC is consistent, although AIC is not, it might be said. </span><span class="koboSpan" id="kobo.384.3">BIC will indicate the risk that it would underfit, while AIC is more suited to examine whether the model has anger that it would outfit. </span><span class="koboSpan" id="kobo.384.4">Although BIC is more forgiving than AIC, it becomes less forgiving as the number increases. </span><span class="koboSpan" id="kobo.384.5">Cross-validation can be made asymptotically equal with the help of AIC. </span><span class="koboSpan" id="kobo.384.6">BIC, on the other hand, is useful for </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">accurate estimation.</span></span></p>
<p><span class="koboSpan" id="kobo.386.1">The penalty for additional parameters is higher in BIC than in AIC when comparing the two. </span><span class="koboSpan" id="kobo.386.2">AIC often looks for an unidentified model with a high-dimensional reality. </span><span class="koboSpan" id="kobo.386.3">BIC, on the other hand, exclusively finds </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">True</span></strong><span class="koboSpan" id="kobo.388.1"> models. </span><span class="koboSpan" id="kobo.388.2">AIC is not consistent, whereas BIC is. </span><span class="koboSpan" id="kobo.388.3">Although BIC is more forgiving than AIC, it becomes less forgiving as the number increases. </span><span class="koboSpan" id="kobo.388.4">BIC penalizes free parameters more severely </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">than AIC.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.390.1">The next code will </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.391.1">create a DataFrame where we can store the most important values from the analysis, which in this case are the weights </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">and p-val</span><a id="_idTextAnchor508"/><span class="koboSpan" id="kobo.393.1">ues:</span></span><pre class="console"><span class="koboSpan" id="kobo.394.1">
data_res = pd.DataFrame({'name': result.params.key</span><a id="_idTextAnchor509"/><span class="koboSpan" id="kobo.395.1">s(),</span></pre><pre class="console"><span class="koboSpan" id="kobo.396.1">
                         'weight': result.params.val</span><a id="_idTextAnchor510"/><span class="koboSpan" id="kobo.397.1">ues,</span></pre><pre class="console"><span class="koboSpan" id="kobo.398.1">
                         'p_val': result.pvalu</span><a id="_idTextAnchor511"/><span class="koboSpan" id="kobo.399.1">es})</span></pre><pre class="console"><span class="koboSpan" id="kobo.400.1">
data_res = data_res</span><a id="_idTextAnchor512"/><span class="koboSpan" id="kobo.401.1">[1:]</span></pre><pre class="console"><span class="koboSpan" id="kobo.402.1">
data_res</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.403.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">following output</span><a id="_idTextAnchor513"/><span class="koboSpan" id="kobo.405.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<span class="koboSpan" id="kobo.406.1"><img alt="Figure 4.12: OLS variable weights and p-values " src="image/B19026_04_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.407.1">Figure 4.12: OLS variable weights and p-val</span><a id="_idTextAnchor514"/><span class="koboSpan" id="kobo.408.1">ues</span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.409.1">Now that we have the </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.410.1">values arranged, we can check the significance of each one of the relationships to check the validity of the assumption and visualize </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">the weights:</span></span><pre class="console"><span class="koboSpan" id="kobo.412.1">
xbar = np.arange(len(data_res['weight</span><a id="_idTextAnchor515"/><span class="koboSpan" id="kobo.413.1">']))</span></pre><pre class="console"><span class="koboSpan" id="kobo.414.1">
plt.barh(xbar, data_res['weigh</span><a id="_idTextAnchor516"/><span class="koboSpan" id="kobo.415.1">t'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.416.1">
plt.yticks(xbar, labels=data_res['nam</span><a id="_idTextAnchor517"/><span class="koboSpan" id="kobo.417.1">e'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.418.1">
plt.xlabel('weig</span><a id="_idTextAnchor518"/><span class="koboSpan" id="kobo.419.1">ht')</span></pre><pre class="console"><span class="koboSpan" id="kobo.420.1">
plt.show()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.421.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">following outpu</span><a id="_idTextAnchor519"/><span class="koboSpan" id="kobo.423.1">t</span></span></p>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.424.1">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.425.1"><img alt="Figure 4.13: Variable weights sorted by importance " src="image/B19026_04_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.426.1">Figure 4.13: Variable weights sorted by importance</span></p>
<p><span class="koboSpan" id="kobo.427.1">In this case, it can be seen that there are factors that are positively related, such as the weight of 100 grams, and the option for the product to be fat-free, while there are other factors that are</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.428.1"> negatively related, such as the weight of 400 grams, followed by that the product is </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">not fat-free.</span></span></p>
<p><span class="koboSpan" id="kobo.430.1">One of the questions that might arise is this: </span><em class="italic"><span class="koboSpan" id="kobo.431.1">If we have a regressor model to estimate the product score, can we use the same model to predict how customers will react to new product combinations?</span></em><span class="koboSpan" id="kobo.432.1"> In theory, we could, but in that case, we would have to use a scoring system instead of a ranking and have much, much more data. </span><span class="koboSpan" id="kobo.432.2">For demonstration purposes, we will continue regardless as this data is difficult to get and even more difficult </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">to disclo</span><a id="_idTextAnchor520"/><span class="koboSpan" id="kobo.434.1">se.</span></span></p>
<h1 id="_idParaDest-51"><a id="_idTextAnchor521"/><span class="koboSpan" id="kobo.435.1">Predicting new feature combinations</span></h1>
<p><span class="koboSpan" id="kobo.436.1">Now that we have properly trained our predictor, we can use it besides capturing information about the </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.437.1">product features’ importance to also provide us with information about how new product features </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">will perform:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.439.1">After going through the EDA, we will develop some predictive models and compare them. </span><span class="koboSpan" id="kobo.439.2">We will use the DataFrame where we had created dummy variables, scaling all the variables to a range of 0 </span><a id="_idTextAnchor522"/><span class="No-Break"><span class="koboSpan" id="kobo.440.1">to 1:</span></span><pre class="console"><span class="koboSpan" id="kobo.441.1">
from sklearn.preprocessing import StandardS</span><a id="_idTextAnchor523"/><span class="koboSpan" id="kobo.442.1">caler</span></pre><pre class="console"><span class="koboSpan" id="kobo.443.1">
X = conjoint_da</span><a id="_idTextAnchor524"/><span class="koboSpan" id="kobo.444.1">t_dum</span></pre><pre class="console"><span class="koboSpan" id="kobo.445.1">
y = conjoint_dat['ranking'].astype</span><a id="_idTextAnchor525"/><span class="koboSpan" id="kobo.446.1">(int)</span></pre><pre class="console"><span class="koboSpan" id="kobo.447.1">
# The target variable will be normalized from a ranking to a 1 to 10 </span><a id="_idTextAnchor526"/><span class="koboSpan" id="kobo.448.1">score</span></pre><pre class="console"><span class="koboSpan" id="kobo.449.1">
y = y.apply(lambda x: int(x/len(y)</span><a id="_idTextAnchor527"/><span class="koboSpan" id="kobo.450.1">*</span><a id="_idTextAnchor528"/><span class="koboSpan" id="kobo.451.1">10))</span></pre><pre class="console"><span class="koboSpan" id="kobo.452.1">
features = X.columns.v</span><a id="_idTextAnchor529"/><span class="koboSpan" id="kobo.453.1">alues</span></pre><pre class="console"><span class="koboSpan" id="kobo.454.1">
scaler = StandardSca</span><a id="_idTextAnchor530"/><span class="koboSpan" id="kobo.455.1">ler()</span></pre><pre class="console"><span class="koboSpan" id="kobo.456.1">
scaler.f</span><a id="_idTextAnchor531"/><span class="koboSpan" id="kobo.457.1">it(X)</span></pre><pre class="console"><span class="koboSpan" id="kobo.458.1">
X = pd.DataFrame(scaler.transfor</span><a id="_idTextAnchor532"/><span class="koboSpan" id="kobo.459.1">m(X))</span></pre><pre class="console"><span class="koboSpan" id="kobo.460.1">
X.columns = features</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.461.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">following outpu</span><a id="_idTextAnchor533"/><span class="koboSpan" id="kobo.463.1">t:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<span class="koboSpan" id="kobo.464.1"><img alt="Figure 4.14: Scaled variables " src="image/B19026_04_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.465.1">Figure 4.14: Scaled variables</span></p>
<p><span class="koboSpan" id="kobo.466.1">One of the most popular ML models is Logistic Regression, which is an algorithm that uses independent variables to make predictions. </span><span class="koboSpan" id="kobo.466.2">This algorithm can be used in the context of classification and regression tasks. </span><span class="koboSpan" id="kobo.466.3">It’s a supervised algorithm that requirs the data to be labeled. </span><span class="koboSpan" id="kobo.466.4">This algorithm uses example answers to fit the model to the target variable, which in our case is the product ranking. </span><span class="koboSpan" id="kobo.466.5">In mathematical terms, the model seeks to predict Y given a set of independent X variables. </span><span class="koboSpan" id="kobo.466.6">Logistic Regression can be defined between</span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.467.1"> binary and multinomial logistic regression. </span><span class="koboSpan" id="kobo.467.2">Namely, the characteristic of these two can be described </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.469.1">Binary</span></strong><span class="koboSpan" id="kobo.470.1">: The most </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.471.1">used of all the types of logistic regression, the algorithm seeks to differentiate between 0s and 1s, a task that is regarded </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">as classification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.473.1">Multinomial</span></strong><span class="koboSpan" id="kobo.474.1">: When the target or independent variable has three or more potential values, multinomial logistic regression is used. </span><span class="koboSpan" id="kobo.474.2">For instance, using features</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.475.1"> of chest X-rays can indicate one of three probable outcomes (absence of disease, pneumonia, or fibrosis). </span><span class="koboSpan" id="kobo.475.2">The example, in this case, is ranked according to features into one of three possible outcomes using multinomial logistic regression. </span><span class="koboSpan" id="kobo.475.3">Of course, the target variable can have more than three </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">possible values.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.477.1">Ordinal logistic regression</span></strong><span class="koboSpan" id="kobo.478.1">: If the target variable is ordinal in nature, ordinal logistic regression is utilized. </span><span class="koboSpan" id="kobo.478.2">Each category has a quantitative meaning and is</span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.479.1"> considerably ordered in this type. </span><span class="koboSpan" id="kobo.479.2">Additionally, the target variable contains more categories than just two. </span><span class="koboSpan" id="kobo.479.3">Exam results, for instance, are categorized and sorted according to quantitative criteria. </span><span class="koboSpan" id="kobo.479.4">Grades can be A, B, or C, to put </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">it simply.</span></span></li>
</ul>
<ol>
<li value="2"><span class="koboSpan" id="kobo.481.1">The primary distinction between linear and logistic regression is that whereas logistic regression is used to solve classification problems, linear regression is used to address regression difficulties. </span><span class="koboSpan" id="kobo.481.2">Target variables in regression issues might have continuous values, such as a product’s price or a participant’s age, while classification problems are concerned with predicting target variables that can only have discrete values, such as determining a person’s gender or whether a tumor is malignant </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">or</span><a id="_idTextAnchor534"/><span class="koboSpan" id="kobo.483.1"> benign:</span></span><pre class="console"><span class="koboSpan" id="kobo.484.1">
# Logistic Re</span><a id="_idTextAnchor535"/><span class="koboSpan" id="kobo.485.1">gression</span></pre><pre class="console"><span class="koboSpan" id="kobo.486.1">
from sklearn.model_selection import train_te</span><a id="_idTextAnchor536"/><span class="koboSpan" id="kobo.487.1">st_split</span></pre><pre class="console"><span class="koboSpan" id="kobo.488.1">
from sklearn.linear_model import LogisticRe</span><a id="_idTextAnchor537"/><span class="koboSpan" id="kobo.489.1">gression</span></pre><pre class="console"><span class="koboSpan" id="kobo.490.1">
from sklearn import</span><a id="_idTextAnchor538"/> <a id="_idTextAnchor539"/><span class="koboSpan" id="kobo.491.1">metrics</span></pre><pre class="console"><span class="koboSpan" id="kobo.492.1">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_st</span><a id="_idTextAnchor540"/><span class="koboSpan" id="kobo.493.1">a</span><a id="_idTextAnchor541"/><span class="koboSpan" id="kobo.494.1">te=101)</span></pre><pre class="console"><span class="koboSpan" id="kobo.495.1">
# Running logistic regressi</span><a id="_idTextAnchor542"/><span class="koboSpan" id="kobo.496.1">on model</span></pre><pre class="console"><span class="koboSpan" id="kobo.497.1">
model = LogisticRegr</span><a id="_idTextAnchor543"/><span class="koboSpan" id="kobo.498.1">ession()</span></pre><pre class="console"><span class="koboSpan" id="kobo.499.1">
result = model.fit(X_train, y_train)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.500.1">By feeding the train set features and their matching target class values to the model, it can be trained. </span><span class="koboSpan" id="kobo.500.2">This will help the model learn how to categorize new cases. </span><span class="koboSpan" id="kobo.500.3">It is crucial to assess the model’s performance on instances that have not yet been encountered because it will only be helpful if it can accurately classify </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.501.1">examples that are not part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">training set.</span></span></p>
<p><span class="koboSpan" id="kobo.503.1">The average of the error squares is measured by the </span><strong class="bold"><span class="koboSpan" id="kobo.504.1">mean squared error</span></strong><span class="koboSpan" id="kobo.505.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.506.1">MSE</span></strong><span class="koboSpan" id="kobo.507.1">), which is the average of the sums of the squares of each discrepancy between the estimated value and the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">true</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.509.1"> value.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.510.1">The MSE is always positive, although it can be 0 if the predictions are completely accurate. </span><span class="koboSpan" id="kobo.510.2">It includes the variance of the estimator (how widespread the estimates are) and the bias (how different the estimated values are from their </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">actual v</span><a id="_idTextAnchor544"/><span class="koboSpan" id="kobo.512.1">alues):</span></span><pre class="console"><span class="koboSpan" id="kobo.513.1">
prediction_test = model.predict(</span><a id="_idTextAnchor545"/><span class="koboSpan" id="kobo.514.1">X_test)</span></pre><pre class="console"><span class="koboSpan" id="kobo.515.1">
# Print the prediction a</span><a id="_idTextAnchor546"/><span class="koboSpan" id="kobo.516.1">ccuracy</span></pre><pre class="console"><span class="koboSpan" id="kobo.517.1">
print(metrics.mean_squared_error(y_test, prediction_test))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.518.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">following out</span><a id="_idTextAnchor547"/><span class="koboSpan" id="kobo.520.1">put:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<span class="koboSpan" id="kobo.521.1"><img alt="Figure 4.15: MSE of the simple regression " src="image/B19026_04_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.522.1">Figure 4.15: MSE of the simple regression</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.523.1">The MSE is always 0 or positive. </span><span class="koboSpan" id="kobo.523.2">If the MSE is large, this indicates that the linear regression model does not accurately predict the model. </span><span class="koboSpan" id="kobo.523.3">The important point is that MSE is sensitive to outliers. </span><span class="koboSpan" id="kobo.523.4">This is because the error at each data point is averaged. </span><span class="koboSpan" id="kobo.523.5">Therefore, if the outlier error is large, the MSE will be amplified. </span><span class="koboSpan" id="kobo.523.6">There is no “target” value for MSE. </span><span class="koboSpan" id="kobo.523.7">However, MSE is a good indicator of how</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.524.1"> well your model fits your data. </span><span class="koboSpan" id="kobo.524.2">You can also indicate whether you prefer one model </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">to a</span><a id="_idTextAnchor548"/><span class="koboSpan" id="kobo.526.1">nother:</span></span><pre class="console"><span class="koboSpan" id="kobo.527.1">
# To get the weights of all the va</span><a id="_idTextAnchor549"/><span class="koboSpan" id="kobo.528.1">riables</span></pre><pre class="console"><span class="koboSpan" id="kobo.529.1">
weights = pd.Series(model.co</span><a id="_idTextAnchor550"/><span class="koboSpan" id="kobo.530.1">ef_[0],</span></pre><pre class="console"><span class="koboSpan" id="kobo.531.1">
                 index=X.columns.</span><a id="_idTextAnchor551"/><span class="koboSpan" id="kobo.532.1">values)</span></pre><pre class="console"><span class="koboSpan" id="kobo.533.1">
print(weights.sort_values(ascending = False)[:10].plot(kind='bar'))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.534.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">following out</span><a id="_idTextAnchor552"/><span class="koboSpan" id="kobo.536.1">put:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<span class="koboSpan" id="kobo.537.1"><img alt="Figure 4.16: Linear regression top variable contribution " src="image/B19026_04_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.538.1">Figure 4.16: Linear regression top variable contribution</span></p>
<p><span class="koboSpan" id="kobo.539.1">Here, we can see the first 10 variables that the model has identified as positively related to</span><a id="_idIndexMarker212"/> <span class="No-Break"><span class="koboSpan" id="kobo.540.1">the score.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.541.1">The next code will show us the ones that are more </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">negatively r</span><a id="_idTextAnchor553"/><span class="koboSpan" id="kobo.543.1">elated:</span></span><pre class="console"><span class="koboSpan" id="kobo.544.1">
print(weights.sort_values(ascending = False)[-10:].plot(kind='bar'))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.545.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">following out</span><a id="_idTextAnchor554"/><span class="koboSpan" id="kobo.547.1">put:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<span class="koboSpan" id="kobo.548.1"><img alt="Figure 4.17: Linear regression negative variable contribution " src="image/B19026_04_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.549.1">Figure 4.17: Linear regression negative variable contribution</span></p>
<p><span class="koboSpan" id="kobo.550.1">Random Forest is a </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">supervised learning</span></strong><span class="koboSpan" id="kobo.552.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.553.1">SL</span></strong><span class="koboSpan" id="kobo.554.1">) algorithm. </span><span class="koboSpan" id="kobo.554.2">It can be used both for classification and regression. </span><span class="koboSpan" id="kobo.554.3">Additionally, it is the </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.555.1">most adaptable </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.556.1">and user-friendly algorithm. </span><span class="koboSpan" id="kobo.556.2">There are trees in a forest. </span><span class="koboSpan" id="kobo.556.3">A forest is supposed to be stronger the more trees it has. </span><span class="koboSpan" id="kobo.556.4">On randomly chosen data samples, random forests generate decision trees, obtain a prediction</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.557.1"> from each tree, and then vote for the best option. </span><span class="koboSpan" id="kobo.557.2">They also offer a fairly accurate indication of the significance of </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">the feature.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Applications for random forests include feature selection, picture classification, and recommendation engines. </span><span class="koboSpan" id="kobo.559.2">They can be used to categorize dependable loan candidates, spot fraud, and </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">forecast sickness.</span></span></p>
<p><span class="koboSpan" id="kobo.561.1">A random forest is a meta-estimator that employs the mean to increase predicted accuracy and reduce overfitting. </span><span class="koboSpan" id="kobo.561.2">It fits a number of decision-tree classifications over various dataset subsamples. </span><span class="koboSpan" id="kobo.561.3">If </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">bootstrap = True</span></strong><span class="koboSpan" id="kobo.563.1"> (the default), the size of the subsample is specified by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">max_samples</span></strong><span class="koboSpan" id="kobo.565.1"> argument; otherwise, each tree is constructed using the complete dataset. </span><span class="koboSpan" id="kobo.565.2">In random forests, each tree in the ensemble is constructed using a sample taken from the training set using a substitution (that is, a bootstrap sample). </span><span class="koboSpan" id="kobo.565.3">Additionally, the optimal split of all input features or any subset of </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">max_features</span></strong><span class="koboSpan" id="kobo.567.1"> size is discovered when splitting each node during </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">tree construction.</span></span></p>
<p><span class="koboSpan" id="kobo.569.1">These two random sources are used to lower the forest estimator’s variance. </span><span class="koboSpan" id="kobo.569.2">In actuality, individual decision trees frequently overfit and have considerable variance. </span><span class="koboSpan" id="kobo.569.3">Decision trees with partially dissociated prediction errors are produced when randomness is added into forests. </span><span class="koboSpan" id="kobo.569.4">The average of these projections can help certain inaccuracies disappear. </span><span class="koboSpan" id="kobo.569.5">By merging various trees, random forests reduce variation, sometimes at the expense of a modest increase in distortion. </span><span class="koboSpan" id="kobo.569.6">The variance reduction is frequently large in practice, which leads to a stronger </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">overall model.</span></span></p>
<p><span class="koboSpan" id="kobo.571.1">Instead of having each classifier select a single class, the scikit-learn implementation combines classifiers by </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.572.1">averaging their </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">probabilistic pr</span><a id="_idTextAnchor555"/><span class="koboSpan" id="kobo.574.1">edictions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.575.1">
from sklearn.ensemble import RandomForest</span><a id="_idTextAnchor556"/><span class="koboSpan" id="kobo.576.1">Classifier
model_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,random_state =50, max_features = "auto",max_leaf_n</span><a id="_idTextAnchor557"/><span class="koboSpan" id="kobo.577.1">odes = 30)
model_rf.fit(X_train</span><a id="_idTextAnchor558"/><span class="koboSpan" id="kobo.578.1">, y_train)
# Make p</span><a id="_idTextAnchor559"/><span class="koboSpan" id="kobo.579.1">redictions
prediction_test = model_rf.predi</span><a id="_idTextAnchor560"/><span class="koboSpan" id="kobo.580.1">ct(X_test)
print(metrics.mean_squared_error(y_test, prediction_test))</span></pre>
<p><span class="koboSpan" id="kobo.581.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">following </span><a id="_idTextAnchor561"/><span class="koboSpan" id="kobo.583.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<span class="koboSpan" id="kobo.584.1"><img alt="Figure 4.18: Random Forest MSE " src="image/B19026_04_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.585.1">Figure 4.18: Random Forest MSE</span></p>
<p><span class="koboSpan" id="kobo.586.1">One of the available options in terms of ML models is the Random Forest algorithm, which is implemented in the scikit-learn package and includes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">RandomForestRegressor</span></strong><span class="koboSpan" id="kobo.588.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">RandomForestClassifier</span></strong><span class="koboSpan" id="kobo.590.1"> classes. </span><span class="koboSpan" id="kobo.590.2">These classes can be fitted to data, yielding a model that can then be used to create new predictions or to obtain information about feature importance, a property that is at the core of the</span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.591.1"> importance of conjoint analysis, as in the </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">following</span><a id="_idTextAnchor562"/><span class="koboSpan" id="kobo.593.1"> example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.594.1">
importances = model_rf.feature_im</span><a id="_idTextAnchor563"/><span class="koboSpan" id="kobo.595.1">portances_
weights = pd.Series(importances, index=X.colum</span><a id="_idTextAnchor564"/><span class="koboSpan" id="kobo.596.1">ns.values)
weights.sort_values()[-10:].plot(kind = 'barh')</span></pre>
<p><span class="koboSpan" id="kobo.597.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">following </span><a id="_idTextAnchor565"/><span class="koboSpan" id="kobo.599.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.600.1"><img alt="Figure 4.19: Random Forest variable importance " src="image/B19026_04_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.601.1">Figure 4.19: Random Forest variable importance</span></p>
<p><span class="koboSpan" id="kobo.602.1">The XGBoost library is a highly effective, adaptable, and portable library for distributed gradient augmentation. </span><span class="koboSpan" id="kobo.602.2">It applies ML techniques within the Gradient Boosting framework and offers parallel tree amplification (also known as GBDT and GBM), which gives quick and accurate answers to a variety of data science issues. </span><span class="koboSpan" id="kobo.602.3">The same code can handle problems that involve more than a trillion examples and runs in massively </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">distributed contexts.</span></span></p>
<p><span class="koboSpan" id="kobo.604.1">It has been the primary impetus behind algorithms that have recently won significant ML competitions. </span><span class="koboSpan" id="kobo.604.2">It routinely surpasses all other algorithms for SL tasks due to its unrivaled speed </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">and performance.</span></span></p>
<p><span class="koboSpan" id="kobo.606.1">The primary algorithm can operate on clusters of GPUs or even on a network of PCs because the library is parallelizable. </span><span class="koboSpan" id="kobo.606.2">This makes it possible to train on hundreds of millions of training</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.607.1"> instances and solve ML tasks with </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">great performance.</span></span></p>
<p><span class="koboSpan" id="kobo.609.1">After winning a significant physics competition, it was rapidly embraced by the ML community despite being originally bui</span><a id="_idTextAnchor566"/><span class="koboSpan" id="kobo.610.1">lt </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">in C++:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.612.1">
from xgboost import XGB</span><a id="_idTextAnchor567"/><span class="koboSpan" id="kobo.613.1">Classifier
model = XGBCl</span><a id="_idTextAnchor568"/><span class="koboSpan" id="kobo.614.1">assifier()
model.fit(X_train</span><a id="_idTextAnchor569"/><span class="koboSpan" id="kobo.615.1">, y_train)
preds = model.predi</span><a id="_idTextAnchor570"/><span class="koboSpan" id="kobo.616.1">ct(X_test)
metrics.mean_squared_error(y_test, preds)</span></pre>
<p><span class="koboSpan" id="kobo.617.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">following </span><a id="_idTextAnchor571"/><span class="koboSpan" id="kobo.619.1">output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<span class="koboSpan" id="kobo.620.1"><img alt="Figure 4.20: Random Forest MSE " src="image/B19026_04_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.621.1">Figure 4.20: Random Forest MSE</span></p>
<p><span class="koboSpan" id="kobo.622.1">We can see that the MSE for the random forest is even greater than the linear regression. </span><span class="koboSpan" id="kobo.622.2">This is possible because of the lack of enough data to be able to achieve a better score, so in the next steps, we would have to consider increasing the number of data points for </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">the </span><a id="_idTextAnchor572"/><span class="koboSpan" id="kobo.624.1">analysis.</span></span></p>
<h1 id="_idParaDest-52"><a id="_idTextAnchor573"/><span class="koboSpan" id="kobo.625.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.626.1">In this chapter, we have learned how to perform conjoint analysis, which is a statistical tool that allows us to undercover consumer preferences that otherwise would be difficult to determine. </span><span class="koboSpan" id="kobo.626.2">The way in which we performed the analysis was by using OLS to estimate the performance of different combinations of features and try to isolate the impact of each one of the possible configurations in the overall perception of the client to undercover where the consumer perceives </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">the value.</span></span></p>
<p><span class="koboSpan" id="kobo.628.1">This has allowed us to create an overview of the factors that drive users to buy a product, and even be able to predict how a new combination of features will perform by using </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">ML algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.630.1">In the next chapter, we will learn how to adjust the price of items by studying the relationship between price variation and the number of quantities sold using </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">price elasticity.</span></span></p>
</div>
</body></html>