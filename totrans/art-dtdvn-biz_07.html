<html><head></head><body>
<div id="_idContainer187">
<h1 class="chapter-number" id="_idParaDest-72"><a id="_idTextAnchor881"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-73"><a id="_idTextAnchor882"/><span class="koboSpan" id="kobo.2.1">Predicting Customer Churn</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The churn rate is a metric used to determine how many clients or staff leave a business in a certain time frame. </span><span class="koboSpan" id="kobo.3.2">It might also refer to the sum of money that was lost because of the departures. </span><span class="koboSpan" id="kobo.3.3">Changes in a company’s churn rate might offer insightful information about the firm. </span><span class="koboSpan" id="kobo.3.4">Understanding the amount or proportion of consumers who don’t buy more goods or services is possible through customer </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">churn analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will understand the concept of churn and why it is important in the context of business. </span><span class="koboSpan" id="kobo.5.2">We will then prepare the data for further analysis and create an analysis to determine the most important factors to take into account to understand the churn patterns. </span><span class="koboSpan" id="kobo.5.3">Finally, we will learn how to create machine learning models to predict customers that </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">will churn.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">customer churn</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">customer data</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">variable relationships</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Predicting users who </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">will churn</span></span></li>
</ul>
<h1 id="_idParaDest-74"><a id="_idTextAnchor883"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">In order to be able to follow the steps in this chapter, you will need to meet the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">next requirements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">A Jupyter Notebook instance running Python 3.7 and above. </span><span class="koboSpan" id="kobo.20.2">You can also use the Google Colab notebook to run the steps if you have a Google </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">Drive account.</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">An understanding of basic math and </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">statistical concepts.</span></span></li>
</ul>
<h1 id="_idParaDest-75"><a id="_idTextAnchor884"/><span class="koboSpan" id="kobo.24.1">Understanding customer churn</span></h1>
<p><span class="koboSpan" id="kobo.25.1">In business, the </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.26.1">number of paying customers that fail to become repeat customers for a given product </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.27.1">or service is known as customer churn, also known </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.28.1">as customer attrition. </span><span class="koboSpan" id="kobo.28.2">Churn in this sense refers to a measurable rate of change that happens over a predetermined period </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">of time.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">Analyzing the causes of churn, engaging with customers, educating them, knowing who is at risk, identifying your most valuable customers, offering incentives, selecting the correct audience to target, and providing better service are a few strategies to reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">customer turnover.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">It’s crucial to lower churn because it increases </span><strong class="bold"><span class="koboSpan" id="kobo.33.1">Customer Acquisition Cost</span></strong><span class="koboSpan" id="kobo.34.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.35.1">CAC</span></strong><span class="koboSpan" id="kobo.36.1">) and lowers revenue. </span><span class="koboSpan" id="kobo.36.2">In actuality, maintaining and improving current client relationships is much less expensive </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.37.1">than gaining new consumers. </span><span class="koboSpan" id="kobo.37.2">The more clients you lose, the more money you’ll need to spend on acquiring new ones in order to make up for the lost revenue. </span><span class="koboSpan" id="kobo.37.3">You can use the following formula to determine CAC: CAC is calculated by dividing the cost of sales and marketing by the number of new </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.38.1">customers attracted. </span><span class="koboSpan" id="kobo.38.2">The proportion of consumers who come back to your firm is known as the </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">retention rate.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<span class="koboSpan" id="kobo.40.1"><img alt="" src="image/Formula_07_001.jpg"/></span>
</div>
</div>
<p><span class="koboSpan" id="kobo.41.1">This is different from the churn rate, which </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.42.1">measures how many clients you’ve lost over time. </span><span class="koboSpan" id="kobo.42.2">By default, a business with a high churn rate will have a lower </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">retention rate.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">Now that we have an idea of the business value that we get by identifying the patterns that make our clients churn, in the next section, we will start to explore the data and </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">its variab</span><a id="_idTextAnchor885"/><span class="koboSpan" id="kobo.46.1">les.</span></span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor886"/><span class="koboSpan" id="kobo.47.1">Exploring customer data</span></h1>
<p><span class="koboSpan" id="kobo.48.1">Our goal is to create </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.49.1">a model to estimate the likelihood of abandonment using data pertaining to Telecom customers. </span><span class="koboSpan" id="kobo.49.2">This is to answer the question of how likely it is that a consumer will discontinue utilizing </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">the service.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Initially, the data is subjected to exploratory analysis. </span><span class="koboSpan" id="kobo.51.2">Knowing the data types of each column is the first step in the process, after which any necessary adjustments to the variables </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">are made.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">To explore the data, we will plot the relationships between the churn variable and the other important factors that make up the dataset. </span><span class="koboSpan" id="kobo.53.2">Prior to suggesting a model, this work is carried out to get a preliminary understanding of the underlying relationships between </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">the variables.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">A thorough approach is taken </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.56.1">while performing descriptive statistics, which focus on client differences based on one or more attributes. </span><span class="koboSpan" id="kobo.56.2">The primary variable of interest, churn, is now the focus, and a new set of interesting graphs is produced for </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">this reason.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">To examine the variables, we have to handle unstructured data and adjust data types; the first step is to explore the data. </span><span class="koboSpan" id="kobo.58.2">In essence, we will be learning about data distribution and arranging the data for the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">clustering analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">For the analysis we will use in the next example, the following Python modules </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">were used:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Pandas</span></strong><span class="koboSpan" id="kobo.63.1">: Python package </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.64.1">for data analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">data </span></span><span class="No-Break"><a id="_idIndexMarker321"/></span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">manipulation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">NumPy</span></strong><span class="koboSpan" id="kobo.68.1">: This is a library </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.69.1">that adds support for large, multi-dimensional </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.70.1">arrays and matrices, along with an extensive collection of high-level mathematical functions to operate on </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">these arrays.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">statsmodels</span></strong><span class="koboSpan" id="kobo.73.1">: A Python </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.74.1">package that provides a complement to </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.75.1">SciPy for statistical computations, including descriptive statistics and estimation and inference for statistical models. </span><span class="koboSpan" id="kobo.75.2">It provides classes and functions for the estimation of many different </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">statistical models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.77.1">Seaborn, mpl_toolkits, and Matplotlib</span></strong><span class="koboSpan" id="kobo.78.1">: Python </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.79.1">packages for </span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.80.1">effective </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">data </span></span><span class="No-Break"><a id="_idIndexMarker328"/></span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">visualization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.83.1">We’ll now get started with the analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.85.1">The first step is in the following block of code, we will load all the required packages just mentioned, including the functions that we will be using, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.86.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.87.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">StandardScale</span><a id="_idTextAnchor887"/><span class="koboSpan" id="kobo.89.1">r</span></strong><span class="koboSpan" id="kobo.90.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.92.1">KMeans</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.94.1">
impo</span><a id="_idTextAnchor888"/><span class="koboSpan" id="kobo.95.1">rt numpy as np</span></pre><pre class="console"><span class="koboSpan" id="kobo.96.1">
impor</span><a id="_idTextAnchor889"/><span class="koboSpan" id="kobo.97.1">t pandas as pd</span></pre><pre class="console"><span class="koboSpan" id="kobo.98.1">
import</span><a id="_idTextAnchor890"/><span class="koboSpan" id="kobo.99.1"> seaborn as sns</span></pre><pre class="console"><span class="koboSpan" id="kobo.100.1">
import matplotli</span><a id="_idTextAnchor891"/><span class="koboSpan" id="kobo.101.1">b.pyplot as plt</span></pre><pre class="console"><span class="koboSpan" id="kobo.102.1">
import os</span></pre></li>
<li><span class="koboSpan" id="kobo.103.1">For readability </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.104.1">purposes, we will limit the maximum rows to </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">20</span></strong><span class="koboSpan" id="kobo.106.1">, set the maximum columns to </span><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">50</span></strong><span class="koboSpan" id="kobo.108.1">, and show the floats with </span><strong class="source-inline"><span class="koboSpan" id="kobo.109.1">2</span></strong><span class="koboSpan" id="kobo.110.1"> digi</span><a id="_idTextAnchor892"/><span class="koboSpan" id="kobo.111.1">ts </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">of precision:</span></span><pre class="console"><span class="koboSpan" id="kobo.113.1">
pd.options.displ</span><a id="_idTextAnchor893"/><span class="koboSpan" id="kobo.114.1">ay.max_rows = 20</span></pre><pre class="console"><span class="koboSpan" id="kobo.115.1">
pd.options.display.</span><a id="_idTextAnchor894"/><span class="koboSpan" id="kobo.116.1">max_columns = 50</span></pre><pre class="console"><span class="koboSpan" id="kobo.117.1">
pd.options.displ</span><a id="_idTextAnchor895"/><span class="koboSpan" id="kobo.118.1">ay.precision = 2</span></pre><pre class="console"><span class="koboSpan" id="kobo.119.1">
path = 'cus</span><a id="_idTextAnchor896"/><span class="koboSpan" id="kobo.120.1">tomer_churn.csv'</span></pre><pre class="console"><span class="koboSpan" id="kobo.121.1">
data = p</span><a id="_idTextAnchor897"/><span class="koboSpan" id="kobo.122.1">d.read_csv(path)</span></pre><pre class="console"><span class="koboSpan" id="kobo.123.1">
data.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.124.1">This block of code will load the data and show the firs</span><a id="_idTextAnchor898"/><span class="koboSpan" id="kobo.125.1">t rows </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">of it:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<span class="koboSpan" id="kobo.127.1"><img alt="Figure 7.1: Customer data " src="image/B19026_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.128.1">Figure 7.1: Customer data</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.129.1">Now, we can now look at the columns that the</span><a id="_idTextAnchor899"/> <span class="No-Break"><span class="koboSpan" id="kobo.130.1">DataFrame has:</span></span><pre class="console"><span class="koboSpan" id="kobo.131.1">
data.columns</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.132.1">In order to obtain </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.133.1">information about the type of each column and the number of missing values, we can use </span><a id="_idTextAnchor900"/><span class="koboSpan" id="kobo.134.1">the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">info</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.136.1"> method:</span></span><a id="_idTextAnchor901"/></p>
<pre class="console"><span class="koboSpan" id="kobo.137.1">
data.info()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer165">
<span class="koboSpan" id="kobo.138.1"><img alt="Figure 7.2: Pandas column data types " src="image/B19026_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.139.1">Figure 7.2: Pandas column data types</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.140.1">We can see that </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.141.1">although we don’t have </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">null</span></strong><span class="koboSpan" id="kobo.143.1"> values to impute, most of the variables are categorical – meaning that we need to cast them into </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">boolean</span></strong><span class="koboSpan" id="kobo.145.1"> numerical columns before using machine learning models or clustering methods. </span><span class="koboSpan" id="kobo.145.2">The first step is converting </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">TotalCharges</span></strong><span class="koboSpan" id="kobo.147.1"> into a nume</span><a id="_idTextAnchor902"/><span class="koboSpan" id="kobo.148.1">rical </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">data type:</span></span><pre class="console"><span class="koboSpan" id="kobo.150.1">
data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.151.1">The preceding code </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.152.1">casts the variable to a numeric variable, coercing any errors instead </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">of failing.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">We can see the results of the transformation </span><a id="_idTextAnchor903"/><span class="koboSpan" id="kobo.155.1">using </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">info</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.157.1"> again</span><a id="_idTextAnchor904"/><span class="koboSpan" id="kobo.158.1">:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.159.1">
data.info()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer166">
<span class="koboSpan" id="kobo.160.1"><img alt="Figure 7.3: Corrected data types " src="image/B19026_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.161.1">Figure 7.3: Corrected data types</span></p>
<p><span class="koboSpan" id="kobo.162.1">The resulting transformation has been successful but has generated 10 </span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">null</span></strong><span class="koboSpan" id="kobo.164.1"> values that we can </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">later drop.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.166.1">Now, we will determine the total list of categorical columns that we need to cast to dummies for </span><a id="_idTextAnchor905"/><span class="No-Break"><span class="koboSpan" id="kobo.167.1">better analysis:</span></span><pre class="console"><span class="koboSpan" id="kobo.168.1">
object_cols = [c for c in data.drop(['customerID'],axis=1).columns if dat</span><a id="_idTextAnchor906"/><span class="koboSpan" id="kobo.169.1">a[c].dtype=='O']</span></pre><pre class="console"><span class="koboSpan" id="kobo.170.1">
object_cols</span></pre></li>
<li><span class="koboSpan" id="kobo.171.1">There are several columns </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.172.1">that could be easily represented by ones and zeros given that there are </span><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">Yes</span></strong><span class="koboSpan" id="kobo.174.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">No</span></strong><span class="koboSpan" id="kobo.176.1"> values; we will determine which of these variables has only these two options and then map these values to their </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">numeri</span><a id="_idTextAnchor907"/><span class="koboSpan" id="kobo.178.1">cal counterparts:</span></span><pre class="console"><span class="koboSpan" id="kobo.179.1">
yn_cols = []</span></pre><pre class="console"><span class="koboSpan" id="kobo.180.1">
# Iterate over</span><a id="_idTextAnchor908"/><span class="koboSpan" id="kobo.181.1"> the column names</span></pre><pre class="console"><span class="koboSpan" id="kobo.182.1">
For c in object_cols:</span></pre><pre class="console"><span class="koboSpan" id="kobo.183.1">
  # count the unique values by acc</span><a id="_idTextAnchor909"/><span class="koboSpan" id="kobo.184.1">essing the column</span></pre><pre class="console"><span class="koboSpan" id="kobo.185.1">
  val_counts = data[c].value_counts()</span></pre><pre class="console"><span class="koboSpan" id="kobo.186.1">
  # If the count of unique values is equal to two, we assume that it'</span><a id="_idTextAnchor910"/><span class="koboSpan" id="kobo.187.1">s a Yes/No column</span></pre><pre class="console"><span class="koboSpan" id="kobo.188.1">
  if len(val_counts.index)==2 and all(val_counts.index.isin</span><a id="_idTextAnchor911"/><span class="koboSpan" id="kobo.189.1">(['No', 'Yes'</span><a id="_idTextAnchor912"/><span class="koboSpan" id="kobo.190.1">])):</span></pre><pre class="console"><span class="koboSpan" id="kobo.191.1">
    print(c)</span></pre><pre class="console"><span class="koboSpan" id="kobo.192.1">
    print(data[c].value_coun</span><a id="_idTextAnchor913"/><span class="koboSpan" id="kobo.193.1">ts().to_string())</span></pre><pre class="console"><span class="koboSpan" id="kobo.194.1">
    yn_cols.append(c)</span></pre></li>
<li><span class="koboSpan" id="kobo.195.1">The preceding code will generate a list of categorical </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">Yes</span></strong><span class="koboSpan" id="kobo.197.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">No</span></strong><span class="koboSpan" id="kobo.199.1"> columns to which we can map the data int</span><a id="_idTextAnchor914"/><span class="koboSpan" id="kobo.200.1">o ones </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">and zeros:</span></span><pre class="console"><span class="koboSpan" id="kobo.202.1">
# Iterate over the yes/no column names</span></pre><pre class="console"><span class="koboSpan" id="kobo.203.1">
for c in yn_cols:</span></pre><pre class="console"><span class="koboSpan" id="kobo.204.1">
  # Normalize the column values by lowering them and mapping th</span><a id="_idTextAnchor915"/><span class="koboSpan" id="kobo.205.1">em to new values.</span></pre><pre class="console"><span class="koboSpan" id="kobo.206.1">
  data[c] = data[c].str.lower().map({'yes': 1, 'no': 0})</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.207.1">We can now re-evaluate the results of </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">th</span><a id="_idTextAnchor916"/><span class="koboSpan" id="kobo.209.1">ese replacements:</span></span><a id="_idTextAnchor917"/></p>
<pre class="console"><span class="koboSpan" id="kobo.210.1">
data.head()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer167">
<span class="koboSpan" id="kobo.211.1"><img alt="Figure 7.4: Column transformed into a normalized Boolean column " src="image/B19026_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.212.1">Figure 7.4: Column transformed into a normalized Boolean column</span></p>
<p><span class="koboSpan" id="kobo.213.1">We can now look at </span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.214.1">the numerical variable distribution to get a better understanding of the data using the</span><a id="_idTextAnchor918"/> <span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">describe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.216.1"> method:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.217.1">
dat</span><a id="_idTextAnchor919"/><span class="koboSpan" id="kobo.218.1">a.describe()</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer168">
<span class="koboSpan" id="kobo.219.1"><img alt="Figure 7.5: Statistical description of the data " src="image/B19026_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.220.1">Figure 7.5: Statistical description of the data</span></p>
<p><span class="koboSpan" id="kobo.221.1">It is interesting to see that here, 27% of clients churn, which is a very large proportion. </span><span class="koboSpan" id="kobo.221.2">In other contexts, these values tend to be much lower, making the dataset highly imbalanced and imposing the need to adapt the analysis to handle these imbalances. </span><span class="koboSpan" id="kobo.221.3">Thankfully, this is not the case, as the number of occurrences of churn is representative enough. </span><span class="koboSpan" id="kobo.221.4">Nevertheless, an imbalanced dataset requires us to </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.222.1">take into consideration that we need to inspect the metrics used to evaluate the model in more depth. </span><span class="koboSpan" id="kobo.222.2">If we would just look into the precision, in our case, a model that just outputs the most common variable (the customer doesn’t churn) will have an accuracy of 73%. </span><span class="koboSpan" id="kobo.222.3">That is why we need to add more performance metrics, precision, recall, and a combination of both, such as the F1 score, and especially look at the confusion matrix to find the proportion of correctly predicted cases for each type </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">of class.</span></span></p>
<ol>
<li value="8"><span class="koboSpan" id="kobo.224.1">We can now visualize the distribution of some of these categorical variables accounting for the cases in which the users have churn. </span><span class="koboSpan" id="kobo.224.2">We can do this using </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">S</span><a id="_idTextAnchor920"/><span class="koboSpan" id="kobo.226.1">eaborn's </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">countplot</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.229.1">
impo</span><a id="_idTextAnchor921"/><span class="koboSpan" id="kobo.230.1">rt seaborn as sns</span></pre><pre class="console"><span class="koboSpan" id="kobo.231.1">
import matplot</span><a id="_idTextAnchor922"/><span class="koboSpan" id="kobo.232.1">lib.pyplot as plt</span></pre><pre class="console"><span class="koboSpan" id="kobo.233.1">
f, ax = plt.subplots</span><a id="_idTextAnchor923"/><span class="koboSpan" id="kobo.234.1">(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.235.1">
pl = sns.countplot(x=data["InternetService"],h</span><a id="_idTextAnchor924"/><span class="koboSpan" id="kobo.236.1">ue=data["Churn"])</span></pre><pre class="console"><span class="koboSpan" id="kobo.237.1">
pl.set_title("InternetS</span><a id="_idTextAnchor925"/><span class="koboSpan" id="kobo.238.1">ervice vs Churn")</span></pre><pre class="console"><span class="koboSpan" id="kobo.239.1">
pl.set_xlabel("</span><a id="_idTextAnchor926"/><span class="koboSpan" id="kobo.240.1">InternetService")</span></pre><pre class="console"><span class="koboSpan" id="kobo.241.1">
pl.set_yl</span><a id="_idTextAnchor927"/><span class="koboSpan" id="kobo.242.1">abel("Count")</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer169">
<span class="koboSpan" id="kobo.243.1"><img alt="Figure 7.6: Customer internet contract versus churn " src="image/B19026_07_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.244.1">Figure 7.6: Customer internet contract versus churn</span></p>
<p><span class="koboSpan" id="kobo.245.1">We can see that there is </span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.246.1">a big difference between the relative percentage of churn for the users that opted for the fiber optic service, compared to the ones who don’t have internet service or </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">have DSL.</span></span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.248.1">This information can be useful for diving into the reasons or developing new promotions to </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.249.1">addre</span><a id="_idTextAnchor928"/><span class="koboSpan" id="kobo.250.1">ss </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">this situation:</span></span><pre class="console"><span class="koboSpan" id="kobo.252.1">
f, ax = plt.subplot</span><a id="_idTextAnchor929"/><span class="koboSpan" id="kobo.253.1">s(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.254.1">
pl = sns.countplot(x=data["MultipleLines"],</span><a id="_idTextAnchor930"/><span class="koboSpan" id="kobo.255.1">hue=data["Churn"])</span></pre><pre class="console"><span class="koboSpan" id="kobo.256.1">
pl.set_title("Multip</span><a id="_idTextAnchor931"/><span class="koboSpan" id="kobo.257.1">leLines vs Churn")</span></pre><pre class="console"><span class="koboSpan" id="kobo.258.1">
pl.set_xlabe</span><a id="_idTextAnchor932"/><span class="koboSpan" id="kobo.259.1">l("MultipleLines")</span></pre><pre class="console"><span class="koboSpan" id="kobo.260.1">
pl.set_yl</span><a id="_idTextAnchor933"/><span class="koboSpan" id="kobo.261.1">abel("Count")</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer170">
<span class="koboSpan" id="kobo.262.1"><img alt="Figure 7.7: Customer phone contracts versus churn " src="image/B19026_07_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.263.1">Figure 7.7: Customer phone contracts versus churn</span></p>
<p><span class="koboSpan" id="kobo.264.1">We can see some relative differences, again, for clients that have multiple lines, whereas the ones that don’t have multiple lines seem to churn more. </span><span class="koboSpan" id="kobo.264.2">These differences need to be validated with a t-test or other hypothesis testing methods to </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.265.1">determine actual differences between the means of </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">the groups.</span></span></p>
<ol>
<li value="10"><span class="koboSpan" id="kobo.267.1">The next step is to visualize the relationship between the c</span><a id="_idTextAnchor934"/><span class="koboSpan" id="kobo.268.1">ontract </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">and churn:</span></span><pre class="console"><span class="koboSpan" id="kobo.270.1">
f, ax = plt.subplot</span><a id="_idTextAnchor935"/><span class="koboSpan" id="kobo.271.1">s(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.272.1">
pl = sns.countplot(x=data["Contract"],</span><a id="_idTextAnchor936"/><span class="koboSpan" id="kobo.273.1">hue=data["Churn"])</span></pre><pre class="console"><span class="koboSpan" id="kobo.274.1">
pl.set_title("C</span><a id="_idTextAnchor937"/><span class="koboSpan" id="kobo.275.1">ontract vs Churn")</span></pre><pre class="console"><span class="koboSpan" id="kobo.276.1">
pl.set_</span><a id="_idTextAnchor938"/><span class="koboSpan" id="kobo.277.1">xlabel("Contract")</span></pre><pre class="console"><span class="koboSpan" id="kobo.278.1">
pl.set_ylabel("Count")</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.279.1">The code here will show us a bar plot of contract type method</span><a id="_idTextAnchor939"/> <span class="No-Break"><span class="koboSpan" id="kobo.280.1">versus churn:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<span class="koboSpan" id="kobo.281.1"><img alt="Figure 7.8: Customer contract type versus churn " src="image/B19026_07_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.282.1">Figure 7.8: Customer contract type versus churn</span></p>
<p><span class="koboSpan" id="kobo.283.1">Here, the client churn rate is extremely high for customers with month-to-month contracts relative to the ones that have 1- or 2-year contracts. </span><span class="koboSpan" id="kobo.283.2">This information can be </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.284.1">used to create marketing strategies that seek to convert the month-to-month contracts into 1- or </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">2-year contracts.</span></span></p>
<ol>
<li value="11"><span class="koboSpan" id="kobo.286.1">Finally, our last variable exploration focuses on the type of payment method. </span><span class="koboSpan" id="kobo.286.2">The next plot will show us the relationship of churn to the typ</span><a id="_idTextAnchor940"/><span class="koboSpan" id="kobo.287.1">e of </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">payment used:</span></span><pre class="console"><span class="koboSpan" id="kobo.289.1">
f, ax = plt.subplot</span><a id="_idTextAnchor941"/><span class="koboSpan" id="kobo.290.1">s(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.291.1">
pl = sns.countplot(x=data["PaymentMethod"],</span><a id="_idTextAnchor942"/><span class="koboSpan" id="kobo.292.1">hue=data["Churn"])</span></pre><pre class="console"><span class="koboSpan" id="kobo.293.1">
pl.set_title("Paymen</span><a id="_idTextAnchor943"/><span class="koboSpan" id="kobo.294.1">tMethod vs Churn")</span></pre><pre class="console"><span class="koboSpan" id="kobo.295.1">
pl.set_xlabe</span><a id="_idTextAnchor944"/><span class="koboSpan" id="kobo.296.1">l("PaymentMethod")</span></pre><pre class="console"><span class="koboSpan" id="kobo.297.1">
pl.set_ylabel("Count")</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.298.1">The code here will show us a bar plot of payment method</span><a id="_idTextAnchor945"/> <span class="No-Break"><span class="koboSpan" id="kobo.299.1">versus churn:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<span class="koboSpan" id="kobo.300.1"><img alt="Figure 7.9: Customer payment method versus churn " src="image/B19026_07_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.301.1">Figure 7.9: Customer payment method versus churn</span></p>
<p><span class="koboSpan" id="kobo.302.1">Here, the difference is </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.303.1">clear between the ones using electronic checks rather than other kinds of </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">payment methods.</span></span></p>
<p><span class="koboSpan" id="kobo.305.1">Up next, we will explore the variable relationships between the different variables using Seaborn's </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">pairplot</span></strong><span class="koboSpan" id="kobo.307.1"> and with the use of </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">cor</span><a id="_idTextAnchor946"/><span class="koboSpan" id="kobo.309.1">relation analysis.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor947"/><span class="koboSpan" id="kobo.310.1">Exploring variable relationships</span></h1>
<p><span class="koboSpan" id="kobo.311.1">Exploring the </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.312.1">way in which variables move together can help us to determine the hidden patterns that govern the behaviors of </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">our clients:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.314.1">Our first step here will be using the Seaborn method to plot some of the relationships, mostly between numerical continuous variables such as tenure, monthly charges, and total charges, using the churn as</span><a id="_idTextAnchor948"/><span class="koboSpan" id="kobo.315.1"> the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.316.1">hue</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.317.1"> parameter:</span></span><pre class="console"><span class="koboSpan" id="kobo.318.1">
g = sns.pairplot(data[['tenure','MonthlyCharges', 'TotalCharges','Churn']], hue= "Churn",palette= (["red","bl</span><a id="_idTextAnchor949"/><span class="koboSpan" id="kobo.319.1">ue"]),height=6)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer173">
<span class="koboSpan" id="kobo.320.1"><img alt="Figure 7.10: Continuous variable relationships " src="image/B19026_07_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.321.1">Figure 7.10: Continuous variable relationships</span></p>
<p><span class="koboSpan" id="kobo.322.1">We can observe </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.323.1">from the distributions that the customers who churn tend to have a low tenure number, generally having low amounts of monthly charges, as well as having much lower total charges </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">on average.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.325.1">Now, we can finally transform and determine the object columns that we will co</span><a id="_idTextAnchor950"/><span class="koboSpan" id="kobo.326.1">nvert </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">into dummies:</span></span><pre class="console"><span class="koboSpan" id="kobo.328.1">
object_cols = [c for c in data.drop(['customerID'],axis=1).columns if </span><a id="_idTextAnchor951"/><span class="koboSpan" id="kobo.329.1">data[c].dtype=='O']</span></pre><pre class="console"><span class="koboSpan" id="kobo.330.1">
object_cols</span></pre></li>
<li><span class="koboSpan" id="kobo.331.1">Once these </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.332.1">columns are determined, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">get_dummies</span></strong><span class="koboSpan" id="kobo.334.1"> function and create a new DataFrame only wit</span><a id="_idTextAnchor952"/><span class="koboSpan" id="kobo.335.1">h </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">numeric variables:</span></span><pre class="console"><span class="koboSpan" id="kobo.337.1">
df_dummies = pd.get_dummie</span><a id="_idTextAnchor953"/><span class="koboSpan" id="kobo.338.1">s(data[object_cols])</span></pre><pre class="console"><span class="koboSpan" id="kobo.339.1">
data = pd.concat([data.drop(object_cols+['Churn'],axis=1),df_dummies,data</span><a id="_idTextAnchor954"/><span class="koboSpan" id="kobo.340.1">[['Churn']]],axis=1)</span></pre><pre class="console"><span class="koboSpan" id="kobo.341.1">
data.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.342.1">The preceding code will show us the </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">d</span><a id="_idTextAnchor955"/><span class="koboSpan" id="kobo.344.1">ata restructured:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<span class="koboSpan" id="kobo.345.1"><img alt="Figure 7.11: Restructured data " src="image/B19026_07_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.346.1">Figure 7.11: Restructured data</span></p>
<p><span class="koboSpan" id="kobo.347.1">Here, the data can effectively describe each one of the customers’ descriptive levels so that the information is represented numerically rather than categorically. </span><span class="koboSpan" id="kobo.347.2">These factors consist of tenure, subscription type, cost, call history, and demographics, among other things. </span><span class="koboSpan" id="kobo.347.3">The fact that the dimensions need to be represented numerically is because most machine learning algorithms require the data to be </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">this way.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.349.1">As the next step, we </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.350.1">will be studying the relationship of variables to determine the most important correl</span><a id="_idTextAnchor956"/><span class="koboSpan" id="kobo.351.1">ations </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">between them</span><a id="_idTextAnchor957"/><span class="koboSpan" id="kobo.353.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.354.1">
import numpy as np</span></pre><pre class="console"><span class="koboSpan" id="kobo.355.1">
from matp</span><a id="_idTextAnchor958"/><span class="koboSpan" id="kobo.356.1">lotlib import colors</span></pre><pre class="console"><span class="koboSpan" id="kobo.357.1">
d</span><a id="_idTextAnchor959"/><span class="koboSpan" id="kobo.358.1">f_corr = data.corr()</span></pre><pre class="console"><span class="koboSpan" id="kobo.359.1">
mask = np.triu(np.ones_like(d</span><a id="_idTextAnchor960"/><span class="koboSpan" id="kobo.360.1">f_corr, dtype=bool))</span></pre><pre class="console"><span class="koboSpan" id="kobo.361.1">
df_corr = df_corr</span><a id="_idTextAnchor961"/><span class="koboSpan" id="kobo.362.1">.mask(mask).round(2)</span></pre><pre class="console"><span class="koboSpan" id="kobo.363.1">
fig, ax = plt.subpl</span><a id="_idTextAnchor962"/><span class="koboSpan" id="kobo.364.1">ots(figsize=(25,25))</span></pre><pre class="console"><span class="koboSpan" id="kobo.365.1">
sns.heatmap(df_corr, annot=True,ax=ax)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.366.1">The code determines these correlations and constructs a triangular dataset that we can plot as a heat map to clearly visualize the relationship betwe</span><a id="_idTextAnchor963"/><span class="koboSpan" id="kobo.367.1">en </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">the variables.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<span class="koboSpan" id="kobo.369.1"><img alt="Figure 7.12: Variable correlations " src="image/B19026_07_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.370.1">Figure 7.12: Variable correlations</span></p>
<p><span class="koboSpan" id="kobo.371.1">Here, we can visualize the entire set of variable correlations, but we may just look at the ones that are related to our target variable. </span><span class="koboSpan" id="kobo.371.2">We can see that some variables that </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.372.1">depend on others will have a correlation of 1 to this variable – for example, in the case of </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">internet contract = no</span></strong><span class="koboSpan" id="kobo.374.1">, which has a correlation equal to 1 with </span><strong class="source-inline"><span class="koboSpan" id="kobo.375.1">streaming service = no</span></strong><span class="koboSpan" id="kobo.376.1">. </span><span class="koboSpan" id="kobo.376.2">This is because if there is no internet contract, it is obvious that you won’t be able to access a streaming service that requires an </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">internet contract.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.378.1">We can do this just by looking at the correlations relat</span><a id="_idTextAnchor964"/><span class="koboSpan" id="kobo.379.1">ed to </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">this variable:</span></span><pre class="console"><span class="koboSpan" id="kobo.381.1">
churn_corr = data.corr()['Churn'].sort_value</span><a id="_idTextAnchor965"/><span class="koboSpan" id="kobo.382.1">s(ascending = False)</span></pre><pre class="console"><span class="koboSpan" id="kobo.383.1">
churn_corr.plot(kind='bar'</span><a id="_idTextAnchor966"/><a id="_idTextAnchor967"/><span class="koboSpan" id="kobo.384.1">,figsize=(20,8))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer176">
<span class="koboSpan" id="kobo.385.1"><img alt="Figure 7.13: Most important correlations to churn " src="image/B19026_07_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.386.1">Figure 7.13: Most important correlations to churn</span></p>
<p><span class="koboSpan" id="kobo.387.1">This information is really useful, as it not only determines the variables associated with a higher degree of </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.388.1">churn but also the ones that lead to a lower churn rate, such as the tenure time and having a 2-year contract. </span><span class="koboSpan" id="kobo.388.2">It would be a good practice to remove the churn variable here as the correlation of the variable with itself is 1 and distorts </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">the graphic.</span></span></p>
<p><span class="koboSpan" id="kobo.390.1">After going through this EDA, we will develop some predictive models and </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">compare them.</span></span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor968"/><span class="koboSpan" id="kobo.392.1">Predicting users who will churn</span></h1>
<p><span class="koboSpan" id="kobo.393.1">In this example, we will train </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.394.1">logistic regression, random forest, and SVM machine learning models to predict the users that will churn based on the observed variables. </span><span class="koboSpan" id="kobo.394.2">We will need to scale the variables first and we will use the sklearn </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">MinMaxScaler</span></strong><span class="koboSpan" id="kobo.396.1"> functionality to </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">do so:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.398.1">We will start with logistic regression and scale all the variables </span><a id="_idTextAnchor969"/><span class="koboSpan" id="kobo.399.1">to a range of 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">to 1:</span></span><pre class="console"><span class="koboSpan" id="kobo.401.1">
from sklearn.preprocessin</span><a id="_idTextAnchor970"/><span class="koboSpan" id="kobo.402.1">g import MinMaxScaler</span></pre><pre class="console"><span class="koboSpan" id="kobo.403.1">
y =</span><a id="_idTextAnchor971"/><span class="koboSpan" id="kobo.404.1"> data['Churn'].values</span></pre><pre class="console"><span class="koboSpan" id="kobo.405.1">
x = data.drop(columns = ['customerID</span><a id="_idTextAnchor972"/><span class="koboSpan" id="kobo.406.1">','Churn']).fillna(0)</span></pre><pre class="console"><span class="koboSpan" id="kobo.407.1">
scaler = MinMaxScaler(f</span><a id="_idTextAnchor973"/><span class="koboSpan" id="kobo.408.1">eature_range = (0,1))</span></pre><pre class="console"><span class="koboSpan" id="kobo.409.1">
x_scaled = sc</span><a id="_idTextAnchor974"/><span class="koboSpan" id="kobo.410.1">aler.fit_transform(x)</span></pre><pre class="console"><span class="koboSpan" id="kobo.411.1">
x_scaled = pd.DataFrame(x_scal</span><a id="_idTextAnchor975"/><span class="koboSpan" id="kobo.412.1">ed,columns=x.columns)</span></pre><pre class="console"><span class="koboSpan" id="kobo.413.1">
x_scaled.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.414.1">The preceding code will create the </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">x</span></strong><span class="koboSpan" id="kobo.416.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">y</span></strong><span class="koboSpan" id="kobo.418.1"> variables, out of which we onl</span><a id="_idTextAnchor976"/><span class="koboSpan" id="kobo.419.1">y need to </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">scale </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">x</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<span class="koboSpan" id="kobo.423.1"><img alt="Figure 7.14: Model input features " src="image/B19026_07_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.424.1">Figure 7.14: Model input features</span></p>
<p><span class="koboSpan" id="kobo.425.1">It is important to scale the variables in logistic regression so that all of them are within a range of 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">to 1.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.427.1">Next, we can train the logistic regression model by splitting the data to get a v</span><a id="_idTextAnchor977"/><span class="koboSpan" id="kobo.428.1">alidation </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">set first:</span></span><pre class="console"><span class="koboSpan" id="kobo.430.1">
from sklearn.model_selection imp</span><a id="_idTextAnchor978"/><span class="koboSpan" id="kobo.431.1">ort train_test_split</span></pre><pre class="console"><span class="koboSpan" id="kobo.432.1">
from sklearn.linear_model impor</span><a id="_idTextAnchor979"/><span class="koboSpan" id="kobo.433.1">t LogisticRegression</span></pre><pre class="console"><span class="koboSpan" id="kobo.434.1">
from sk</span><a id="_idTextAnchor980"/><span class="koboSpan" id="kobo.435.1">learn import metrics</span></pre><pre class="console"><span class="koboSpan" id="kobo.436.1">
x_train, x_test, y_train, y_test = train_test_split(</span></pre><pre class="console"><span class="koboSpan" id="kobo.437.1">
      x_scaled, y, test_size=0.</span><a id="_idTextAnchor981"/><span class="koboSpan" id="kobo.438.1">3, random_state=101)</span></pre><pre class="console"><span class="koboSpan" id="kobo.439.1">
model = </span><a id="_idTextAnchor982"/><span class="koboSpan" id="kobo.440.1">LogisticRegression()</span></pre><pre class="console"><span class="koboSpan" id="kobo.441.1">
result = model.f</span><a id="_idTextAnchor983"/><span class="koboSpan" id="kobo.442.1">it(x_train, y_train)</span></pre><pre class="console"><span class="koboSpan" id="kobo.443.1">
preds_lr = model.predict(x_test)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.444.1">Finally, we can </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.445.1">print the</span><a id="_idTextAnchor984"/> <span class="No-Break"><span class="koboSpan" id="kobo.446.1">prediction accuracy:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.447.1">
print(metrics.accuracy_score(</span><a id="_idTextAnchor985"/><span class="koboSpan" id="kobo.448.1">y_test, preds_lr))</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer178">
<span class="koboSpan" id="kobo.449.1"><img alt="Figure7.15: Logistic regression model accuracy " src="image/B19026_07_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.450.1">Figure7.15: Logistic regression model accuracy</span></p>
<p><span class="koboSpan" id="kobo.451.1">We have obtained good accuracy in </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">the model.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.453.1">We can also get the weights of all the variables to weigh their importance in t</span><a id="_idTextAnchor986"/><span class="koboSpan" id="kobo.454.1">he </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">predictive model:</span></span><pre class="console"><span class="koboSpan" id="kobo.456.1">
weights = pd.Series(model.coef_[0],ind</span><a id="_idTextAnchor987"/><span class="koboSpan" id="kobo.457.1">ex=x_scaled.columns)</span></pre><pre class="console"><span class="koboSpan" id="kobo.458.1">
pd.concat([weights.head(10),weights.tail(10)]).sort_values(ascending = False).plot(kind='bar',figsize=(16,6))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.459.1">The preceding code </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.460.1">will create a data frame of the weights of the 10 most positive and 10 most negative </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">w</span><a id="_idTextAnchor988"/><span class="koboSpan" id="kobo.462.1">eighted variables.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<span class="koboSpan" id="kobo.463.1"><img alt="Figure 7.16: Model feature importance " src="image/B19026_07_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.464.1">Figure 7.16: Model feature importance</span></p>
<p><span class="koboSpan" id="kobo.465.1">It is interesting to see how the total charges and the tenure, especially the latter, have great importance in the regression. </span><span class="koboSpan" id="kobo.465.2">The importance of these variables is also validated by looking at the variable correlation. </span><span class="koboSpan" id="kobo.465.3">An important next step would be to do a deeper analysis of the relationship of this variable to the churn variable to understand the mechanism behind </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">this relationship.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.467.1">We can create the </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.468.1">confusion matrix to visualize the performance of p</span><a id="_idTextAnchor989"/><span class="koboSpan" id="kobo.469.1">redicting </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">each class:</span></span><pre class="console"><span class="koboSpan" id="kobo.471.1">
from sklearn.metrics import classification_repor</span><a id="_idTextAnchor990"/><span class="koboSpan" id="kobo.472.1">t, confusion_matrix</span></pre><pre class="console"><span class="koboSpan" id="kobo.473.1">
print(confusion_matrix(y_</span><a id="_idTextAnchor991"/><span class="koboSpan" id="kobo.474.1">test,preds_lr))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer180">
<span class="koboSpan" id="kobo.475.1"><img alt="Figure 7.17: Model confusion matrix " src="image/B19026_07_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.476.1">Figure 7.17: Model confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.477.1">In the confusion matrix, the information shown in the columns are the distinct classes (no churn and churn) and in the rows are the predicted outcomes in the same order (no churn, churn). </span><span class="koboSpan" id="kobo.477.2">The values in the diagonal represent the true positives, predicted to be a class that it in fact was. </span><span class="koboSpan" id="kobo.477.3">The values that are outside the diagonal are the values predicted wrong. </span><span class="koboSpan" id="kobo.477.4">In our case, we correctly classified </span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">1401</span></strong><span class="koboSpan" id="kobo.479.1"> cases of no churn, mislabeled </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">265</span></strong><span class="koboSpan" id="kobo.481.1"> no churn as churn, predicted </span><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">145</span></strong><span class="koboSpan" id="kobo.483.1"> cases to churn when in fact they didn’t, and correctly classified </span><strong class="source-inline"><span class="koboSpan" id="kobo.484.1">302</span></strong><span class="koboSpan" id="kobo.485.1"> cases of churn. </span><span class="koboSpan" id="kobo.485.2">We can see from the confusion matrix that the model is good at predicting the most common cases, which is that there is no churn, but it got almost a third of the classes wrong, which is important for us </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">to predict.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.487.1">Our next step is to create a classification system made up of several decision trees called a random forest. </span><span class="koboSpan" id="kobo.487.2">It attempts to produce an uncorrelated forest of decision trees, which is more accurate than a single individual tree, using bagging and feature randomness when generating each </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">individual tree.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.489.1">In the next code, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.490.1">RandomForestClassifier</span></strong><span class="koboSpan" id="kobo.491.1"> class from sklearn and train it on </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">the data:</span></span></p>
<pre class="console">
<a id="_idTextAnchor992"/><span class="koboSpan" id="kobo.493.1">from sklearn.ensemble import RandomForestClassifier
</span><a id="_idTextAnchor993"/><span class="koboSpan" id="kobo.494.1">model_rf = RandomForestClassifier(n_estimators=750 , oob_score = True, random_state =50, max_features = "auto",max_leaf_nodes = 15)
</span><a id="_idTextAnchor994"/><span class="koboSpan" id="kobo.495.1">model_rf.fit(x_train, y_train)</span></pre>
<ol>
<li value="6"><span class="koboSpan" id="kobo.496.1">Finally, we have </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.497.1">trained the model and we can </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">make predictions:</span></span><a id="_idTextAnchor995"/><pre class="console"><span class="koboSpan" id="kobo.499.1">
preds_rfc = model_rf.predict(x_test)</span><a id="_idTextAnchor996"/></pre><pre class="console"><span class="koboSpan" id="kobo.500.1">
print(metrics.accuracy_score(y_test, preds_rfc))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer181">
<span class="koboSpan" id="kobo.501.1"><img alt="Figure 7.18: Random forest model accuracy " src="image/B19026_07_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor997"/><span class="koboSpan" id="kobo.502.1">Figure 7.18: Random forest model accuracy</span></p>
<p><span class="koboSpan" id="kobo.503.1">We have obtained an accuracy very similar to the </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">regression model.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.505.1">Let’s see the variable importance of the model as the </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">next step:</span></span><pre class="console">
<a id="_idTextAnchor998"/><span class="koboSpan" id="kobo.507.1">importances = model_rf.feature_importances_</span></pre><pre class="console">
<a id="_idTextAnchor999"/><span class="koboSpan" id="kobo.508.1">weights_rf = pd.Series(importances,index=x_scaled.columns)</span></pre><pre class="console">
<a id="_idTextAnchor1000"/><span class="koboSpan" id="kobo.509.1">pd.concat([weights_rf.head(10),weights.tail(10)]).sort_values(ascending = False).plot(kind='bar',figsize=(16,6))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.510.1">The preceding code will show us the 10 most positive and 10 most negative weights of </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<span class="koboSpan" id="kobo.512.1"><img alt="Figure 7.19: Model feature importance " src="image/B19026_07_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.513.1">Figure 7.19: Model feature importance</span></p>
<p><span class="koboSpan" id="kobo.514.1">This model has a </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.515.1">different ranking, having, in the extremes, the month-to-month contract and the </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">2-year contract:</span></span></p>
<pre class="console">
<a id="_idTextAnchor1001"/><span class="koboSpan" id="kobo.517.1">print(confusion_matrix(y_test,preds_rfc))</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer183">
<span class="koboSpan" id="kobo.518.1"><img alt="Figure 7.20: Model confusion matrix " src="image/B19026_07_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.519.1">Figure 7.20: Model confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.520.1">This model predicts our target variable better, which makes it suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">our needs.</span></span></p>
<ol>
<li value="8"><span class="koboSpan" id="kobo.522.1">Next, we will train a supervised machine learning technique known as a </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">Support Vector Classifier</span></strong><span class="koboSpan" id="kobo.524.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.525.1">SVC</span></strong><span class="koboSpan" id="kobo.526.1">), which is frequently used for classification problems. </span><span class="koboSpan" id="kobo.526.2">An SVC separates the </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.527.1">data into two classes by mapping the data points to a high-dimensional space and then locating the </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">best hyperplane:</span></span><pre class="console">
<a id="_idTextAnchor1002"/><span class="koboSpan" id="kobo.529.1">from sklearn.svm import SVC</span></pre><pre class="console">
<a id="_idTextAnchor1003"/><span class="koboSpan" id="kobo.530.1">model_svm = SVC(kernel='linear')</span></pre><pre class="console">
<a id="_idTextAnchor1004"/><span class="koboSpan" id="kobo.531.1">model_svm.fit(x_train,y_train)</span></pre><pre class="console">
<a id="_idTextAnchor1005"/><span class="koboSpan" id="kobo.532.1">preds_svm = model_svm.predict(x_test)</span></pre><pre class="console">
<a id="_idTextAnchor1006"/><span class="koboSpan" id="kobo.533.1">metrics.accuracy_score(y_test, preds_svm)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.534.1">The code here will fit the model to the data and print the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">accuracy score.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<span class="koboSpan" id="kobo.536.1"><img alt="Figure 7.21: Model accuracy score " src="image/B19026_07_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.537.1">Figure 7.21: Model accuracy score</span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.538.1">The accuracy </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.539.1">score is still in the same range as the other models, so let’s look at the absolute weight importance of </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">the model:</span></span><pre class="console">
<a id="_idTextAnchor1007"/><span class="koboSpan" id="kobo.541.1">pd.Series(abs(model_svm.coef_[0]), index=x_scaled.columns).nlargest(10).plot(kind='barh',figsize=(10,8))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.542.1">The code here will prompt the 10 most important variables for </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<span class="koboSpan" id="kobo.544.1"><img alt="Figure 7.22: Model feature importance " src="image/B19026_07_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.545.1">Figure 7.22: Model feature importance</span></p>
<p><span class="koboSpan" id="kobo.546.1">We can see that tenure and total charges are variables of importance for the model, which is </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.547.1">something that we saw in the other models as well. </span><span class="koboSpan" id="kobo.547.2">The drawback of this kind of visualization is that we cannot see the orientation of </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">this importance.</span></span></p>
<ol>
<li value="10"><span class="koboSpan" id="kobo.549.1">Let’s look at the performance of predicting our </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">target variable:</span></span><pre class="console">
<a id="_idTextAnchor1008"/><span class="koboSpan" id="kobo.551.1">print(confusion_matrix(y_test,preds_svm))</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.552.1">The next step is the confusion matrix, which will allow us to determine with more certainty which labels we are </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">correctly predicting.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<span class="koboSpan" id="kobo.554.1"><img alt="Figure 7.23: Model confusion matrix " src="image/B19026_07_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.555.1">Figure 7.23: Model confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.556.1">The model is </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.557.1">less accurate than the random forest for predicting whether the clients will churn </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">or not.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">It is always good to combine the various perspectives of the confusion matrix, as the accuracy will not work as a performance metric alone if the set </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">is imbalanced.</span></span></p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor1009"/><span class="koboSpan" id="kobo.561.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.562.1">In this chapter, we analyzed a very common business case: customer churn. </span><span class="koboSpan" id="kobo.562.2">Understanding the causes of this, as well as being able to take preventive actions to avoid this, can create a lot of revenue for </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">a company.</span></span></p>
<p><span class="koboSpan" id="kobo.564.1">In the example that we have analyzed, we have seen how to clean the variables in a dataset to properly represent them and prepare them for machine learning. </span><span class="koboSpan" id="kobo.564.2">Visualizing the variables in a relationship against the target variable that we are analyzing allows us to understand the problem better. </span><span class="koboSpan" id="kobo.564.3">Finally, we trained several machine learning models that we later analyzed to understand how their performances were when predicting the </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">target variable.</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">In the next chapter, we will focus more of our attention on understanding how variables affect segments and group users with homogenous characteristics to understand </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">them better.</span></span></p>
</div>
</body></html>