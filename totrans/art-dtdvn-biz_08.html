<html><head></head><body>
<div id="_idContainer212">
<h1 class="chapter-number" id="_idParaDest-80"><a id="_idTextAnchor1010"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-81"><a id="_idTextAnchor1011"/><span class="koboSpan" id="kobo.2.1">Grouping Users with Customer Segmentation</span></h1>
<p><span class="koboSpan" id="kobo.3.1">To better understand consumer needs, we need to understand that our customers have distinct consumer patterns. </span><span class="koboSpan" id="kobo.3.2">Each mass of consumers of a given product or service can be divided into segments, described in terms of age, marital status, purchasing power, and so on. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will be performing an exploratory analysis of consumer data from a grocery store and then applying clustering techniques to separate them into segments with homogenous consumer patterns. </span><span class="koboSpan" id="kobo.3.4">This knowledge will enable us to better understand their needs, create unique offers, and target them more effectively. </span><span class="koboSpan" id="kobo.3.5">In this chapter, we will learn about the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">customer segmentation</span></span></li>
<li><span class="koboSpan" id="kobo.7.1">Exploring data about a </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">customer’s database</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Applying feature engineering to </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">standardize variables</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Creating users’ segments with </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">K-means clustering</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Describing the common characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">these clusters</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">Let us see the requirements to understand the steps and follow </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">the chapter.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor1012"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">To be able to follow the steps in this chapter, you will need to meet the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">following requirements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">A Jupyter notebook instance running Python 3.7 and above. </span><span class="koboSpan" id="kobo.20.2">You can also use the Google Colab notebook to run the steps if you have a Google </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">Drive account.</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">An understanding of basic math and </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">statistical concepts.</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">A Kaggle account—you must agree to the terms and conditions of the competition from where we will get the data, which you can find </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">here: </span></span><a href="https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis"><span class="No-Break"><span class="koboSpan" id="kobo.26.1">https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.27.1">.</span></span></li>
</ul>
<h1 id="_idParaDest-83"><a id="_idTextAnchor1013"/><span class="koboSpan" id="kobo.28.1">Understanding customer segmentation</span></h1>
<p><span class="koboSpan" id="kobo.29.1">Customer segmentation is the </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.30.1">practice of classifying customers into groups based on shared traits so that businesses may effectively and appropriately market to each group. </span><span class="koboSpan" id="kobo.30.2">In </span><strong class="bold"><span class="koboSpan" id="kobo.31.1">business-to-business</span></strong><span class="koboSpan" id="kobo.32.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.33.1">B2B</span></strong><span class="koboSpan" id="kobo.34.1">) marketing, a firm may divide its </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.35.1">clientele into several groups based on a variety of criteria, such as location, industry, the number of employees, and previous purchases of the </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">company’s goods.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">Businesses frequently divide their clientele into segments based on demographics such as age, gender, marital status, location (urban, suburban, or rural), and life stage (single, married, divorced, empty nester, retired). </span><span class="koboSpan" id="kobo.37.2">Customer segmentation calls for a business to collect data about its customers, evaluate it, and look for trends that may be utilized to </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">establish segments.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Job title, location, and products purchased—for example—are some of the details that can be learned from purchasing data to help businesses to learn about their customers. </span><span class="koboSpan" id="kobo.39.2">Some of this information might be discovered by looking at the customer’s system entry. </span><span class="koboSpan" id="kobo.39.3">An online marketer using an opt-in email list may divide marketing communications into various categories based on the opt-in offer that drew the client, for instance. </span><span class="koboSpan" id="kobo.39.4">However, other data—for example, consumer demographics such as age and marital status—will have to be gathered through </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">different methods.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Other typical information-gathering methods in consumer </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">goods include:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.43.1">Face-to-face interviews </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">with customers</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.45.1">Online surveys</span></span></li>
<li><span class="koboSpan" id="kobo.46.1">Online marketing and web </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">traffic information</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.48.1">Focus groups</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.49.1">All organizations, regardless of size, industry, and whether they sell online or in person, can use customer segmentation. </span><span class="koboSpan" id="kobo.49.2">It starts with obtaining and evaluating data and concludes with taking suitable and efficient action on the </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">information acquired.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">We will execute an </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.52.1">unsupervised clustering of data on the customer records from a grocery store’s database in this chapter. </span><span class="koboSpan" id="kobo.52.2">To maximize the value of each customer to the firm, we will segment our customer base to alter products in response to specific needs and consumer behavior. </span><span class="koboSpan" id="kobo.52.3">The ability to address the needs of various clientele also benefits </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">the fir</span><a id="_idTextAnchor1014"/><span class="koboSpan" id="kobo.54.1">m.</span></span></p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor1015"/><span class="koboSpan" id="kobo.55.1">Exploring the data</span></h2>
<p><span class="koboSpan" id="kobo.56.1">The first stage to </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.57.1">understanding customer segments is to </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.58.1">understand the data that we will be using. </span><span class="koboSpan" id="kobo.58.2">The first stage is, then, an exploration of the data to check the variables we must work with, handle non-structured data, and adjust data types. </span><span class="koboSpan" id="kobo.58.3">We will be structuring the data for the clustering analysis and gaining knowledge about the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">data distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">For the analysis we will </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.61.1">use in the next example, the following Python modules </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">are used:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.63.1">Pandas</span></strong><span class="koboSpan" id="kobo.64.1">: Python package </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.65.1">for data analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">data manipulation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">NumPy</span></strong><span class="koboSpan" id="kobo.68.1">: This is a library that </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.69.1">adds support for large, multi-dimensional arrays and matrices, along with an ample collection of high-level mathematical functions to operate on </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">these arrays.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Statsmodels</span></strong><span class="koboSpan" id="kobo.72.1">: Python package </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.73.1">that provides a complement to </span><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">scipy</span></strong><span class="koboSpan" id="kobo.75.1"> for statistical computations, including descriptive statistics and estimation and inference for statistical models. </span><span class="koboSpan" id="kobo.75.2">It provides classes and functions for the estimation of many different </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">statistical models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.77.1">Yellowbrick</span></strong><span class="koboSpan" id="kobo.78.1">: A Python package </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.79.1">of visual analysis and diagnostic tools </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.80.1">designed to facilitate </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">machine learning</span></strong><span class="koboSpan" id="kobo.82.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.83.1">ML</span></strong><span class="koboSpan" id="kobo.84.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">with scikit-learn.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Seaborn, mpl_toolkits, and Matplotlib</span></strong><span class="koboSpan" id="kobo.87.1">: Python </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.88.1">packages for </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.89.1">effective </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">data </span></span><span class="No-Break"><a id="_idIndexMarker370"/></span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">visualization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.92.1">We’ll now get started with the analysis, using the </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.94.1">The following </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.95.1">block of code will load all </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.96.1">the required packages mentioned earlier, including the functions that we will be using, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.98.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">StandardScal</span><a id="_idTextAnchor1016"/><span class="koboSpan" id="kobo.100.1">er</span></strong><span class="koboSpan" id="kobo.101.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">Kmeans</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.105.1">
imp</span><a id="_idTextAnchor1017"/><span class="koboSpan" id="kobo.106.1">ort numpy as np</span></pre><pre class="console"><span class="koboSpan" id="kobo.107.1">
impo</span><a id="_idTextAnchor1018"/><span class="koboSpan" id="kobo.108.1">rt pandas as pd</span></pre><pre class="console">
<a id="_idTextAnchor1019"/><span class="koboSpan" id="kobo.109.1">import datetime</span></pre><pre class="console"><span class="koboSpan" id="kobo.110.1">
import</span><a id="_idTextAnchor1020"/><span class="koboSpan" id="kobo.111.1"> seaborn as sns</span></pre><pre class="console"><span class="koboSpan" id="kobo.112.1">
import matplotli</span><a id="_idTextAnchor1021"/><span class="koboSpan" id="kobo.113.1">b.pyplot as plt</span></pre><pre class="console"><span class="koboSpan" id="kobo.114.1">
from matplotli</span><a id="_idTextAnchor1022"/><span class="koboSpan" id="kobo.115.1">b import colors</span></pre><pre class="console"><span class="koboSpan" id="kobo.116.1">
from matplotlib.colors import</span><a id="_idTextAnchor1023"/><span class="koboSpan" id="kobo.117.1"> ListedColormap</span></pre><pre class="console"><span class="koboSpan" id="kobo.118.1">
from sklearn.preprocessing impo</span><a id="_idTextAnchor1024"/><span class="koboSpan" id="kobo.119.1">rt LabelEncoder</span></pre><pre class="console"><span class="koboSpan" id="kobo.120.1">
from sklearn.preprocessing import</span><a id="_idTextAnchor1025"/><span class="koboSpan" id="kobo.121.1"> StandardScaler</span></pre><pre class="console"><span class="koboSpan" id="kobo.122.1">
from sklearn.decomposi</span><a id="_idTextAnchor1026"/><span class="koboSpan" id="kobo.123.1">tion import PCA</span></pre><pre class="console"><span class="koboSpan" id="kobo.124.1">
from yellowbrick.cluster import K</span><a id="_idTextAnchor1027"/><span class="koboSpan" id="kobo.125.1">ElbowVisualizer</span></pre><pre class="console"><span class="koboSpan" id="kobo.126.1">
from sklearn.cluste</span><a id="_idTextAnchor1028"/><span class="koboSpan" id="kobo.127.1">r import KMeans</span></pre><pre class="console"><span class="koboSpan" id="kobo.128.1">
from mpl_toolkits.mplot3</span><a id="_idTextAnchor1029"/><span class="koboSpan" id="kobo.129.1">d import Axes3D</span></pre><pre class="console"><span class="koboSpan" id="kobo.130.1">
from sklearn.cluster import AgglomerativeClustering</span></pre></li>
<li><span class="koboSpan" id="kobo.131.1">For readability purposes, we will limit the maximum rows to be shown to 20, set the limit of maximum columns to 50, and show the floats with 2 digit</span><a id="_idTextAnchor1030"/><span class="koboSpan" id="kobo.132.1">s </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">of precision:</span></span><pre class="console"><span class="koboSpan" id="kobo.134.1">
pd.options.displa</span><a id="_idTextAnchor1031"/><span class="koboSpan" id="kobo.135.1">y.max_rows = 20</span></pre><pre class="console"><span class="koboSpan" id="kobo.136.1">
pd.options.display.m</span><a id="_idTextAnchor1032"/><span class="koboSpan" id="kobo.137.1">ax_columns = 50</span></pre><pre class="console"><span class="koboSpan" id="kobo.138.1">
pd.options.display.precision = 2</span></pre></li>
<li><span class="koboSpan" id="kobo.139.1">Next, we will load the data, which is stored in the local data folder. </span><span class="koboSpan" id="kobo.139.2">The file is in CSV format with a tab delimiter. </span><span class="koboSpan" id="kobo.139.3">We will read the data into a Pandas DataFrame and print the data shape as well as show t</span><a id="_idTextAnchor1033"/><span class="koboSpan" id="kobo.140.1">he </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">first rows:</span></span><pre class="console"><span class="koboSpan" id="kobo.142.1">
path = "data/marketin</span><a id="_idTextAnchor1034"/><span class="koboSpan" id="kobo.143.1">g_campaign.csv"</span></pre><pre class="console"><span class="koboSpan" id="kobo.144.1">
data = pd.read_csv(</span><a id="_idTextAnchor1035"/><span class="koboSpan" id="kobo.145.1">path, sep="\t")</span></pre><pre class="console"><span class="koboSpan" id="kobo.146.1">
print("Data Shap</span><a id="_idTextAnchor1036"/><span class="koboSpan" id="kobo.147.1">e", data.shape)</span></pre><pre class="console"><span class="koboSpan" id="kobo.148.1">
data.head()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.149.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">follo</span><a id="_idTextAnchor1037"/><span class="koboSpan" id="kobo.151.1">wing output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<span class="koboSpan" id="kobo.152.1"><img alt="Figure 8.1: User data " src="image/B19026_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.153.1">Figure 8.1: User data</span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.154.1">In order to </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.155.1">get a full picture of the steps that </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.156.1">we will be taking to clean the dataset, let us have a look at the statistical summary of the data with the </span><a id="_idTextAnchor1038"/><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.157.1">describe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.158.1"> method:</span></span><pre class="console"><span class="koboSpan" id="kobo.159.1">
data.describe()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.160.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">foll</span><a id="_idTextAnchor1039"/><span class="koboSpan" id="kobo.162.1">owing output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<span class="koboSpan" id="kobo.163.1"><img alt="Figure 8.2: Descriptive statistical summary " src="image/B19026_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.164.1">Figure 8.2: Descriptive statistical summary</span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.165.1">To get more </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.166.1">information on features, we </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.167.1">can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">info</span></strong><span class="koboSpan" id="kobo.169.1"> method to display the number of </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">null</span></strong><span class="koboSpan" id="kobo.171.1"> value</span><a id="_idTextAnchor1040"/><span class="koboSpan" id="kobo.172.1">s and </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">data types:</span></span><pre class="console"><span class="koboSpan" id="kobo.174.1">
data.info()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.175.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">fol</span><a id="_idTextAnchor1041"/><span class="koboSpan" id="kobo.177.1">lowing output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<span class="koboSpan" id="kobo.178.1"><img alt="Figure 8.3: Column data types and null values " src="image/B19026_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.179.1">Figure 8.3: Column data types and null values</span></p>
<p><span class="koboSpan" id="kobo.180.1">From the </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.181.1">preceding output shown with </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.182.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">describe</span></strong><span class="koboSpan" id="kobo.184.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">info</span></strong><span class="koboSpan" id="kobo.186.1"> methods of Pandas DataFrames, we can see </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.188.1">There are 26 missing values in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">Income</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.190.1"> column</span></span></li>
<li><span class="koboSpan" id="kobo.191.1">The date variable named </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">Dt_Customer</span></strong><span class="koboSpan" id="kobo.193.1">, indicating the date a customer joined the database, is not parsed </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">DateTime</span></strong></span></li>
<li><span class="koboSpan" id="kobo.196.1">There are categorical features in our DataFrame of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">dtype</span></strong><span class="koboSpan" id="kobo.198.1"> object that we will need to encode into numerical features later to be able to apply the </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">clustering method</span></span></li>
</ul>
<ol>
<li value="6"><span class="koboSpan" id="kobo.200.1">To address the missing values, we will drop the rows that have missing income values, as it is an important variable to desc</span><a id="_idTextAnchor1042"/><span class="koboSpan" id="kobo.201.1">ribe </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">to customers:</span></span><pre class="console"><span class="koboSpan" id="kobo.203.1">
da</span><a id="_idTextAnchor1043"/><span class="koboSpan" id="kobo.204.1">ta = data.dropna()</span></pre><pre class="console"><span class="koboSpan" id="kobo.205.1">
print("Data Shape", data.shape)</span></pre></li>
<li><span class="koboSpan" id="kobo.206.1">We will parse the date column using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">pd.to_datetime</span></strong><span class="koboSpan" id="kobo.208.1"> Pandas method. </span><span class="koboSpan" id="kobo.208.2">Take into </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.209.1">account that the method will </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.210.1">infer the format of the date, but we can otherwise specify it</span><a id="_idTextAnchor1044"/><span class="koboSpan" id="kobo.211.1"> if it </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">is necessary:</span></span><pre class="console"><span class="koboSpan" id="kobo.213.1">
data["Dt_Customer"] = pd.to_datetime(data["Dt_Customer"])</span></pre></li>
<li><span class="koboSpan" id="kobo.214.1">After parsing the dates, we can look at the values of the newest and oldes</span><a id="_idTextAnchor1045"/><span class="koboSpan" id="kobo.215.1">t </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">recorded customer:</span></span><pre class="console"><span class="koboSpan" id="kobo.217.1">
str(data["Dt_Customer"].min()),str(data["</span><a id="_idTextAnchor1046"/><span class="koboSpan" id="kobo.218.1">Dt_Customer"].max())</span></pre><pre class="console"><span class="koboSpan" id="kobo.219.1">
&gt;&gt;&gt;&gt; ('2012-01-08 00:00:00', '2014-12-06 00:00:00')</span></pre></li>
<li><span class="koboSpan" id="kobo.220.1">In the next step, we will create a feature out of </span><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">Dt_Customer</span></strong><span class="koboSpan" id="kobo.222.1"> that indicates the number of days a customer is registered in the firm’s database, relative to the first user that was registered in the database, although we could use today’s date. </span><span class="koboSpan" id="kobo.222.2">We do this because we are analyzing historical records and not up-to-date data. </span><span class="koboSpan" id="kobo.222.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">Customer_For</span></strong><span class="koboSpan" id="kobo.224.1"> feature is, then, the date of when the customer was registered minus the minimum value in the date column and can be interpreted as the number of days since customers started to shop in the store relative to the</span><a id="_idTextAnchor1047"/><span class="koboSpan" id="kobo.225.1"> last </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">recorded date:</span></span><pre class="console"><span class="koboSpan" id="kobo.227.1">
data["Customer_For"] = data["Dt_Customer"]-data[</span><a id="_idTextAnchor1048"/><span class="koboSpan" id="kobo.228.1">"Dt_Customer"].min()</span></pre><pre class="console"><span class="koboSpan" id="kobo.229.1">
data["Customer_For"] = data["Customer_For"].dt.days</span></pre></li>
<li><span class="koboSpan" id="kobo.230.1">Now, we will explore the unique values in the categorical features to get a cle</span><a id="_idTextAnchor1049"/><span class="koboSpan" id="kobo.231.1">ar idea of </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">the data:</span></span><pre class="console"><span class="koboSpan" id="kobo.233.1">
data["Marital_Status"].value_counts().plot.bar(figsize=(12,6),title = 'Categories in the feature </span><a id="_idTextAnchor1050"/><span class="koboSpan" id="kobo.234.1">Marital_Status:')</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer191">
<span class="koboSpan" id="kobo.235.1"><img alt="Figure 8.4: Marital status " src="image/B19026_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.236.1">Figure 8.4: Marital status</span></p>
<p><span class="koboSpan" id="kobo.237.1">Here, we can </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.238.1">see that there are several types </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.239.1">of marital status, which may have been caused by free text entry during the data capturing. </span><span class="koboSpan" id="kobo.239.2">We will have to standardize </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">these values.</span></span></p>
<ol>
<li value="11"><span class="koboSpan" id="kobo.241.1">Next, we will look at the values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">Education</span></strong><span class="koboSpan" id="kobo.243.1"> feature using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">value_counts</span></strong><span class="koboSpan" id="kobo.245.1"> method to create a bar chart using th</span><a id="_idTextAnchor1051"/><span class="koboSpan" id="kobo.246.1">e Pandas </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">plot</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.248.1"> method:</span></span><pre class="console"><span class="koboSpan" id="kobo.249.1">
data["Education"].value_counts().plot.bar(figsize=(12,6),title = 'Categories in the fe</span><a id="_idTextAnchor1052"/><span class="koboSpan" id="kobo.250.1">ature Education:')</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer192">
<span class="koboSpan" id="kobo.251.1"><img alt="Figure 8.5: Education values " src="image/B19026_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.252.1">Figure 8.5: Education values</span></p>
<p><span class="koboSpan" id="kobo.253.1">Again, we can see </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.254.1">the effects of free text entry as there are </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.255.1">several values that have the same underlying meaning; thus, we will need to standardize them </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.257.1">In the next section, we will apply feature engineering to structure the data for better understanding and </span><a id="_idTextAnchor1053"/><span class="koboSpan" id="kobo.258.1">treatment of </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">the data.</span></span></p>
<h1 id="_idParaDest-85"><a id="_idTextAnchor1054"/><span class="koboSpan" id="kobo.260.1">Feature engineering</span></h1>
<p><span class="koboSpan" id="kobo.261.1">To be able to properly analyze the data as well as to model the clusters, we will need to clean and structure </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.262.1">the data—a step that is commonly referred to as feature engineering—as we need to restructure some of the variables according to our plan </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">of analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">In this section, we will be performing the next steps to clean and structure some of the dataset features, with the goal of simplifying the existing variables and creating features that are easier to understand and describe the </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">data properly:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.266.1">Create an </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">Age</span></strong><span class="koboSpan" id="kobo.268.1"> variable for a customer by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">Year_Birth</span></strong><span class="koboSpan" id="kobo.270.1"> feature, indicating the birth year of the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">respective person.</span></span></li>
<li><span class="koboSpan" id="kobo.272.1">Create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">Living_With</span></strong><span class="koboSpan" id="kobo.274.1"> feature to simplify the marital status, to describe the living situation </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">of couples.</span></span></li>
<li><span class="koboSpan" id="kobo.276.1">Create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">Children</span></strong><span class="koboSpan" id="kobo.278.1"> feature to indicate the total number of children in a household—that is, kids </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">and teenagers.</span></span></li>
<li><span class="koboSpan" id="kobo.280.1">Aggregate spending by product type to better capture </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">consumer behaviors.</span></span></li>
<li><span class="koboSpan" id="kobo.282.1">Indicate parenthood status with a feature </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">Is_Parent</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.286.1">So, let’s apply the steps mentioned here to structure </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">the data:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.288.1">First, let us start with the age of the customer as of today, using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">pd.to_datetime</span></strong><span class="koboSpan" id="kobo.290.1"> method to get the current year and the year of </span><a id="_idTextAnchor1055"/><span class="koboSpan" id="kobo.291.1">birth of </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">the customers:</span></span><pre class="console"><span class="koboSpan" id="kobo.293.1">
data["Age"] = pd.to_datetime('today').year - </span></pre><pre class="console"><span class="koboSpan" id="kobo.294.1">
      data["Year_Birth"]</span></pre></li>
<li><span class="koboSpan" id="kobo.295.1">Now, we will model </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.296.1">the spending on distinct items by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">sum</span></strong><span class="koboSpan" id="kobo.298.1"> method on selected columns and summin</span><a id="_idTextAnchor1056"/><span class="koboSpan" id="kobo.299.1">g along the </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">column axis:</span></span><pre class="console"><span class="koboSpan" id="kobo.301.1">
prod_cols = ["MntWines","MntFruits","MntMeatProducts",</span></pre><pre class="console"><span class="koboSpan" id="kobo.302.1">
"MntFishProducts","MntSweetP</span><a id="_idTextAnchor1057"/><span class="koboSpan" id="kobo.303.1">roducts","MntGoldProds"]</span></pre><pre class="console"><span class="koboSpan" id="kobo.304.1">
data["Spent"] = data[prod_cols].sum(axis=1)</span></pre></li>
<li><span class="koboSpan" id="kobo.305.1">As the next step, we will map the marital status values into a different encoding to simplify terms with close meaning. </span><span class="koboSpan" id="kobo.305.2">For this, we define a mapping dictionary and use it to replace the values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">marital_status</span></strong><span class="koboSpan" id="kobo.307.1"> column </span><a id="_idTextAnchor1058"/><span class="koboSpan" id="kobo.308.1">to create a </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">new feature:</span></span><pre class="console"><span class="koboSpan" id="kobo.310.1">
marital_status_dict</span><a id="_idTextAnchor1059"/><span class="koboSpan" id="kobo.311.1">= {"Married":"Partner",</span></pre><pre class="console"><span class="koboSpan" id="kobo.312.1">
                    </span><a id="_idTextAnchor1060"/><span class="koboSpan" id="kobo.313.1">  "Together":"Partner",</span></pre><pre class="console"><span class="koboSpan" id="kobo.314.1">
                </span><a id="_idTextAnchor1061"/><span class="koboSpan" id="kobo.315.1">      "Absurd":"Alone",</span></pre><pre class="console"><span class="koboSpan" id="kobo.316.1">
                      "Widow":"Alone",</span></pre><pre class="console"><span class="koboSpan" id="kobo.317.1">
              </span><a id="_idTextAnchor1062"/><span class="koboSpan" id="kobo.318.1">        "YOLO":"Alone",</span></pre><pre class="console"><span class="koboSpan" id="kobo.319.1">
                  </span><a id="_idTextAnchor1063"/><span class="koboSpan" id="kobo.320.1">    "Divorced":"Alone",</span></pre><pre class="console"><span class="koboSpan" id="kobo.321.1">
                </span><a id="_idTextAnchor1064"/><span class="koboSpan" id="kobo.322.1">      "Single":"Alone",}</span></pre><pre class="console"><span class="koboSpan" id="kobo.323.1">
data["Living_With"] = data["Marital_Status"].replace(marital_status_dict)</span></pre></li>
<li><span class="koboSpan" id="kobo.324.1">Next, we create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">Children</span></strong><span class="koboSpan" id="kobo.326.1"> feature by summing up the total number of children living in the household pl</span><a id="_idTextAnchor1065"/><span class="koboSpan" id="kobo.327.1">us teens living </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">at home:</span></span><pre class="console"><span class="koboSpan" id="kobo.329.1">
data["Children"]=data["Kidhome"]+data["Teenhome"]</span></pre></li>
<li><span class="koboSpan" id="kobo.330.1">Now, we model the total members in the household using the relatio</span><a id="_idTextAnchor1066"/><span class="koboSpan" id="kobo.331.1">nship and </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">children data:</span></span><pre class="console"><span class="koboSpan" id="kobo.333.1">
data["Family_Size"] = data["Living_With"].replace({"Alone": 1, "Partner":2})+ data["Children"]</span></pre></li>
<li><span class="koboSpan" id="kobo.334.1">Finally, we </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.335.1">capture the parenthood </span><a id="_idTextAnchor1067"/><span class="koboSpan" id="kobo.336.1">status in a </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">new variable:</span></span><pre class="console"><span class="koboSpan" id="kobo.338.1">
data["Is_Parent"] = (data.Children&gt; 0).astype(int)</span></pre></li>
<li><span class="koboSpan" id="kobo.339.1">Now, we will segment education levels into three g</span><a id="_idTextAnchor1068"/><span class="koboSpan" id="kobo.340.1">roups </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">for simplification:</span></span><pre class="console"><span class="koboSpan" id="kobo.342.1">
edu_dict = {"Basic":"Undergraduate","2n Cycle":"Undergraduate", "Graduation":"Graduate", "Master":"Postgraduat</span><a id="_idTextAnchor1069"/><span class="koboSpan" id="kobo.343.1">e", "PhD":"Postgraduate"}</span></pre><pre class="console"><span class="koboSpan" id="kobo.344.1">
data["Ed_level"]=data["Education"].replace(edu_dict)</span></pre></li>
<li><span class="koboSpan" id="kobo.345.1">Now, to simplify, we rename columns into more understandable terms us</span><a id="_idTextAnchor1070"/><span class="koboSpan" id="kobo.346.1">ing a </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">mapping dictionary:</span></span><pre class="console"><span class="koboSpan" id="kobo.348.1">
col_rename_dic</span><a id="_idTextAnchor1071"/><span class="koboSpan" id="kobo.349.1">t = {"MntWines": "Wines",</span></pre><pre class="console"><span class="koboSpan" id="kobo.350.1">
                   "MntFruits":"Fruits",</span></pre><pre class="console"><span class="koboSpan" id="kobo.351.1">
                   </span><a id="_idTextAnchor1072"/><span class="koboSpan" id="kobo.352.1">"MntMeatProducts":"Meat",</span></pre><pre class="console"><span class="koboSpan" id="kobo.353.1">
                   </span><a id="_idTextAnchor1073"/><span class="koboSpan" id="kobo.354.1">"MntFishProducts":"Fish",</span></pre><pre class="console"><span class="koboSpan" id="kobo.355.1">
                   "Mn</span><a id="_idTextAnchor1074"/><span class="koboSpan" id="kobo.356.1">tSweetProducts":"Sweets",</span></pre><pre class="console"><span class="koboSpan" id="kobo.357.1">
                   "MntGoldPr</span><a id="_idTextAnchor1075"/><span class="koboSpan" id="kobo.358.1">ods":"Gold"}</span></pre><pre class="console"><span class="koboSpan" id="kobo.359.1">
data = data.rename(columns=col_rename_dict)</span></pre></li>
<li><span class="koboSpan" id="kobo.360.1">Now, we will drop some of the redundant features to focus on the clearest features, including the ones we just created. </span><span class="koboSpan" id="kobo.360.2">Finally, we will look at the statistical descriptive analysis u</span><a id="_idTextAnchor1076"/><span class="koboSpan" id="kobo.361.1">sing the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">describe</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.363.1"> method:</span></span><pre class="console"><span class="koboSpan" id="kobo.364.1">
to_drop = ["Marital_Status", "Dt_Customer", </span></pre><pre class="console"><span class="koboSpan" id="kobo.365.1">
      "Z_CostContact", "Z_Reve</span><a id="_idTextAnchor1077"/><span class="koboSpan" id="kobo.366.1">nue", "Year_Birth", "ID"]</span></pre><pre class="console"><span class="koboSpan" id="kobo.367.1">
data = d</span><a id="_idTextAnchor1078"/><span class="koboSpan" id="kobo.368.1">ata.drop(to_drop, axis=1)</span></pre><pre class="console"><span class="koboSpan" id="kobo.369.1">
data.describe()</span></pre></li>
<li><span class="koboSpan" id="kobo.370.1">The stats show </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.371.1">us that there are some discrepancies in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">Income</span></strong><span class="koboSpan" id="kobo.373.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">Age</span></strong><span class="koboSpan" id="kobo.375.1"> features, which we will visualize to better understand these inconsistencies. </span><span class="koboSpan" id="kobo.375.2">We will star</span><a id="_idTextAnchor1079"/><span class="koboSpan" id="kobo.376.1">t with a histogram </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">Age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.380.1">
data["Age"].pl</span><a id="_idTextAnchor1080"/><span class="koboSpan" id="kobo.381.1">ot.hist(figsize=(12,6))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer193">
<span class="koboSpan" id="kobo.382.1"><img alt="Figure 8.6: Age data " src="image/B19026_08_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.383.1">Figure 8.6: Age data</span></p>
<p><span class="koboSpan" id="kobo.384.1">We can see that there are some outliers, more than 120 years old, so we will be </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">removing those.</span></span></p>
<ol>
<li value="11"><span class="koboSpan" id="kobo.386.1">Next, we look at</span><a id="_idTextAnchor1081"/><span class="koboSpan" id="kobo.387.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">income distribution:</span></span><pre class="console"><span class="koboSpan" id="kobo.389.1">
data["Income"].plo</span><a id="_idTextAnchor1082"/><span class="koboSpan" id="kobo.390.1">t.hist(figsize=(12,6))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer194">
<span class="koboSpan" id="kobo.391.1"><img alt="Figure 8.7: Income data " src="image/B19026_08_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.392.1">Figure 8.7: Income data</span></p>
<p><span class="koboSpan" id="kobo.393.1">Again, we can </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.394.1">see that most incomes are below 20,000, so we will be limiting the </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">spending level.</span></span></p>
<ol>
<li value="12"><span class="koboSpan" id="kobo.396.1">Next, we drop the outliers by setting a cap on </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">Age</span></strong><span class="koboSpan" id="kobo.398.1"> to avoid data that doesn’t reflect reality, and the income to </span><a id="_idTextAnchor1083"/><span class="koboSpan" id="kobo.399.1">include 99% of </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">the ca</span><a id="_idTextAnchor1084"/><span class="koboSpan" id="kobo.401.1">ses:</span></span><pre class="console"><span class="koboSpan" id="kobo.402.1">
prev_len = len(data)</span></pre><pre class="console"><span class="koboSpan" id="kobo.403.1">
data</span><a id="_idTextAnchor1085"/><span class="koboSpan" id="kobo.404.1"> = data[(data["Age"]&lt;99)]</span></pre><pre class="console"><span class="koboSpan" id="kobo.405.1">
data = data</span><a id="_idTextAnchor1086"/><span class="koboSpan" id="kobo.406.1">[(data["Income"]&lt;150000)]</span></pre><pre class="console"><span class="koboSpan" id="kobo.407.1">
new_l</span><a id="_idTextAnchor1087"/><span class="koboSpan" id="kobo.408.1">en = prev_len - len(data)</span></pre><pre class="console"><span class="koboSpan" id="kobo.409.1">
print('Removed outliers:',new_len)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.410.1">The preceding cod</span><a id="_idTextAnchor1088"/><span class="koboSpan" id="kobo.411.1">e prints the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">next output:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.413.1">
&gt;&gt;&gt; Removed outliers: 11</span></pre>
<ol>
<li value="13"><span class="koboSpan" id="kobo.414.1">Now, we can </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.415.1">look back at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">Age</span></strong><span class="koboSpan" id="kobo.417.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.418.1">Spend</span></strong><span class="koboSpan" id="kobo.419.1"> data distribution to better understand our customers. </span><span class="koboSpan" id="kobo.419.2">We start by creating a histogra</span><a id="_idTextAnchor1089"/><span class="koboSpan" id="kobo.420.1">m plot of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">Age</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.422.1"> feature:</span></span><pre class="console"><span class="koboSpan" id="kobo.423.1">
data["Age"].plo</span><a id="_idTextAnchor1090"/><span class="koboSpan" id="kobo.424.1">t.hist(figsize=(12,6))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer195">
<span class="koboSpan" id="kobo.425.1"><img alt="Figure 8.8: Age with no outliers " src="image/B19026_08_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.426.1">Figure 8.8: Age with no outliers</span></p>
<p><span class="koboSpan" id="kobo.427.1">The age is centered on the 50s, with a skew to the right, meaning that the average age of our cus</span><a id="_idTextAnchor1091"/><span class="koboSpan" id="kobo.428.1">tomers is above </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">45 years.</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.430.1">
data["Income"].plo</span><a id="_idTextAnchor1092"/><span class="koboSpan" id="kobo.431.1">t.hist(figsize=(12,6))</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer196">
<span class="koboSpan" id="kobo.432.1"><img alt="Figure 8.9: Income with no outliers " src="image/B19026_08_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.433.1">Figure 8.9: Income with no outliers</span></p>
<p><span class="koboSpan" id="kobo.434.1">Looking at the spend distribution, it has a normal distribution, centered on 4,000 and slightly skewed to </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">the left.</span></span></p>
<ol>
<li value="14"><span class="koboSpan" id="kobo.436.1">Up next, we </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.437.1">will create a Seaborn pair plot to show the relationships between the different variables, with color labeling accordin</span><a id="_idTextAnchor1093"/><span class="koboSpan" id="kobo.438.1">g to the </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">parental status:</span></span><pre class="console"><span class="koboSpan" id="kobo.440.1">
sns.pairplot(data[["Income", "Recency", "Customer_For", "Age", "Spent", "Is_Parent"]], hue= "Is_Parent",palet</span><a id="_idTextAnchor1094"/><span class="koboSpan" id="kobo.441.1">te= (["red","blue"]))</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer197">
<span class="koboSpan" id="kobo.442.1"><img alt="Figure 8.10: Relationship plot " src="image/B19026_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.443.1">Figure 8.10: Relationship plot</span></p>
<p><span class="koboSpan" id="kobo.444.1">These graphics allow us to quickly observe relationships between the different variables, as </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.445.1">well as their distribution. </span><span class="koboSpan" id="kobo.445.2">One of the clearest is the relationship between spend and income, in which we can see that the higher the income, the higher the expenditure, as well as observing that single parents spend more than people who are not. </span><span class="koboSpan" id="kobo.445.3">We can also see that the consumers with higher recency are parents, while single consumers have lower recency values. </span><span class="koboSpan" id="kobo.445.4">Next, let us look at the correlation among the features (excluding the categorical attributes at </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">this point).</span></span></p>
<ol>
<li value="15"><span class="koboSpan" id="kobo.447.1">We will create a correlation matrix using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.448.1">corr</span></strong><span class="koboSpan" id="kobo.449.1"> method, and show only the lower </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.450.1">triangle of data using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">numpy</span></strong><span class="koboSpan" id="kobo.452.1"> mask. </span><span class="koboSpan" id="kobo.452.2">Finally, we will use a Seaborn met</span><a id="_idTextAnchor1095"/><span class="koboSpan" id="kobo.453.1">hod to display </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">the val</span><a id="_idTextAnchor1096"/><span class="koboSpan" id="kobo.455.1">ues:</span></span><pre class="console"><span class="koboSpan" id="kobo.456.1">
df_corr = data.corr()</span></pre><pre class="console"><span class="koboSpan" id="kobo.457.1">
mask = np.triu(np.ones_</span><a id="_idTextAnchor1097"/><span class="koboSpan" id="kobo.458.1">like(df_corr, dtype=bool))</span></pre><pre class="console"><span class="koboSpan" id="kobo.459.1">
df_corr = d</span><a id="_idTextAnchor1098"/><span class="koboSpan" id="kobo.460.1">f_corr.mask(mask).round(3)</span></pre><pre class="console"><span class="koboSpan" id="kobo.461.1">
fig, ax = plt</span><a id="_idTextAnchor1099"/><span class="koboSpan" id="kobo.462.1">.subplots(figsize=(16,16))</span></pre><pre class="console"><span class="koboSpan" id="kobo.463.1">
cmap = colors.ListedColormap(["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C</span><a id="_idTextAnchor1100"/><span class="koboSpan" id="kobo.464.1">9", "#9F8A78", "#F3AB60"])</span></pre><pre class="console"><span class="koboSpan" id="kobo.465.1">
sns.heatmap(df_corr, cmap=cm</span><a id="_idTextAnchor1101"/><span class="koboSpan" id="kobo.466.1">ap,annot=True,ax=ax)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer198">
<span class="koboSpan" id="kobo.467.1"><img alt="Figure 8.11: Variable correlation " src="image/B19026_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.468.1">Figure 8.11: Variable correlation</span></p>
<p><span class="koboSpan" id="kobo.469.1">The correlations allow us to explore the variable relationships in more detail. </span><span class="koboSpan" id="kobo.469.2">We </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.470.1">can see negative correlations between children and expenditure in the mean, while there are positive relationships between children and recency. </span><span class="koboSpan" id="kobo.470.2">These correlations allow us to better understand </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">consumption patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.472.1">In the next section, we will use the concept of clustering to segment the clients into groups that sha</span><a id="_idTextAnchor1102"/><span class="koboSpan" id="kobo.473.1">re </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">common characteristics.</span></span></p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor1103"/><span class="koboSpan" id="kobo.475.1">Creating client segments</span></h1>
<p><span class="koboSpan" id="kobo.476.1">Marketers can better </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.477.1">target different audience subgroups with their marketing efforts by segmenting their audiences. </span><span class="koboSpan" id="kobo.477.2">Both product development and communications </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.478.1">might be a part of those efforts. </span><span class="koboSpan" id="kobo.478.2">Segmentation benefits a business by allowing </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.480.1">Creating targeted marketing communication on the right communication channel for each client or </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">user segment</span></span></li>
<li><span class="koboSpan" id="kobo.482.1">Applying the right pricing options to the </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">right clients</span></span></li>
<li><span class="koboSpan" id="kobo.484.1">Concentrating on the most </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">lucrative clients</span></span></li>
<li><span class="koboSpan" id="kobo.486.1">Providing better </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">client service</span></span></li>
<li><span class="koboSpan" id="kobo.488.1">Promoting and cross-promoting other goods </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">and services</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.490.1">In this section, we will be preprocessing the data to be able to apply clustering methods for customer segmentation. </span><span class="koboSpan" id="kobo.490.2">The steps that we will apply to preprocess the data are set </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">out here:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.492.1">Encoding the categorical variables using a label encoder, which will transform them into </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">numerical columns</span></span></li>
<li><span class="koboSpan" id="kobo.494.1">Scaling features </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.495.1">using the standard scaler to normalize </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">the values</span></span></li>
<li><span class="koboSpan" id="kobo.497.1">Applying </span><strong class="bold"><span class="koboSpan" id="kobo.498.1">principal component analysis</span></strong><span class="koboSpan" id="kobo.499.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.500.1">PCA</span></strong><span class="koboSpan" id="kobo.501.1">) for </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">dimensionality reduction</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.503.1">So, let’s follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">steps here:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.505.1">First, we need to list the categorical variables. </span><span class="koboSpan" id="kobo.505.2">Here, we will use the column names and check the column </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">dtype</span></strong><span class="koboSpan" id="kobo.507.1"> to</span><a id="_idTextAnchor1104"/><span class="koboSpan" id="kobo.508.1"> get only the </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">object columns:</span></span><pre class="console"><span class="koboSpan" id="kobo.510.1">
object_cols = [c for c in data.columns i</span><a id="_idTextAnchor1105"/><span class="koboSpan" id="kobo.511.1">f data[c].dtypes == 'object']</span></pre><pre class="console"><span class="koboSpan" id="kobo.512.1">
print("Categorical variables in the dataset:", object_cols)</span></pre></li>
<li><span class="koboSpan" id="kobo.513.1">Next, we will encode the </span><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">dtypes</span></strong><span class="koboSpan" id="kobo.515.1"> object using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">s</span><a id="_idTextAnchor1106"/><span class="koboSpan" id="kobo.517.1">klearn </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">LabelEncoder</span></strong></span><span class="No-Break"> <a id="_idTextAnchor1107"/><span class="koboSpan" id="kobo.519.1">function:</span></span><pre class="console"><span class="koboSpan" id="kobo.520.1">
LE = LabelEn</span><a id="_idTextAnchor1108"/><span class="koboSpan" id="kobo.521.1">coder()</span></pre><pre class="console"><span class="koboSpan" id="kobo.522.1">
for i in object_cols:</span></pre><pre class="console"><span class="koboSpan" id="kobo.523.1">
    data[i]=data[[i]].apply(LE.fit_transform)</span></pre></li>
<li><span class="koboSpan" id="kobo.524.1">We subset the data and apply scaling to the numerical variables by dropping the features on d</span><a id="_idTextAnchor1109"/><span class="koboSpan" id="kobo.525.1">eals accepted </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">and promot</span><a id="_idTextAnchor1110"/><span class="koboSpan" id="kobo.527.1">ions:</span></span><pre class="console"><span class="koboSpan" id="kobo.528.1">
scaled_ds = data.copy()</span></pre><pre class="console"><span class="koboSpan" id="kobo.529.1">
cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedC</span><a id="_idTextAnchor1111"/><span class="koboSpan" id="kobo.530.1">mp2', 'Complain', 'Response']</span></pre><pre class="console"><span class="koboSpan" id="kobo.531.1">
scaled_ds = scaled_ds.drop(cols_del, axis=1)</span></pre></li>
<li><span class="koboSpan" id="kobo.532.1">Final</span><a id="_idTextAnchor1112"/><span class="koboSpan" id="kobo.533.1">ly, we can apply </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">the scali</span><a id="_idTextAnchor1113"/><span class="koboSpan" id="kobo.535.1">ng:</span></span><pre class="console"><span class="koboSpan" id="kobo.536.1">
scaler = StandardS</span><a id="_idTextAnchor1114"/><span class="koboSpan" id="kobo.537.1">caler()</span></pre><pre class="console"><span class="koboSpan" id="kobo.538.1">
scaler.fit(scaled_ds)</span></pre><pre class="console"><span class="koboSpan" id="kobo.539.1">
scaled_ds = pd.DataFrame(scaler.transform(</span></pre><pre class="console"><span class="koboSpan" id="kobo.540.1">
      scaled_ds),columns= scaled_ds.columns )</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.541.1">There are numerous attributes in this dataset that describe the data. </span><span class="koboSpan" id="kobo.541.2">The more features there are, the </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.542.1">more difficult it is to correctly analyze them in a business environment. </span><span class="koboSpan" id="kobo.542.2">Many of these characteristics are redundant since they are connected. </span><span class="koboSpan" id="kobo.542.3">Therefore, before running the features through a classifier, we will conduct dimensionality reduction on the </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">chosen features.</span></span></p>
<p><span class="koboSpan" id="kobo.544.1">Dimensionality reduction is the process of reducing the number of random variables considered. </span><span class="koboSpan" id="kobo.544.2">To reduce </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.545.1">the dimensionality of huge datasets, a technique known as PCA is frequently utilized. </span><span class="koboSpan" id="kobo.545.2">PCA works by condensing an ample collection of variables into a smaller set that still retains much of the data in the </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">larger set.</span></span></p>
<p><span class="koboSpan" id="kobo.547.1">Accuracy naturally suffers as a dataset’s variables are reduced, but the answer to dimensionality reduction is to trade a little accuracy for simplicity since ML algorithms can analyze data much more quickly and easily with smaller datasets because there are fewer unnecessary factors to process. </span><span class="koboSpan" id="kobo.547.2">In conclusion, the basic principle of PCA is to keep as much information as possible while reducing the number of variables in the </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">data collected.</span></span></p>
<p><span class="koboSpan" id="kobo.549.1">The steps that we will be applying in this section are </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.551.1">Dimensionality reduction </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">with PCA</span></span></li>
<li><span class="koboSpan" id="kobo.553.1">Plotting the reduced DataFrame in a </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">3D plot</span></span></li>
<li><span class="koboSpan" id="kobo.555.1">Dimensionality reduction with </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">PCA, again</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.557.1">This will allow us to have a way to visualize the segments projected into three dimensions. </span><span class="koboSpan" id="kobo.557.2">In an ideal setup, we will use the weights of each component to understand what each </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.558.1">component represents and make sense of the information we are visualizing in a better way. </span><span class="koboSpan" id="kobo.558.2">For reasons of simplicity, we will focus on the visualization of the components. </span><span class="koboSpan" id="kobo.558.3">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">the steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.560.1">First, we will initiate PCA to reduce dimensions or features to thre</span><a id="_idTextAnchor1115"/><span class="koboSpan" id="kobo.561.1">e in order to </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">reduce compl</span><a id="_idTextAnchor1116"/><span class="koboSpan" id="kobo.563.1">exity:</span></span><pre class="console"><span class="koboSpan" id="kobo.564.1">
pca = PCA(n_components=3)</span></pre><pre class="console"><span class="koboSpan" id="kobo.565.1">
PCA_d</span><a id="_idTextAnchor1117"/><span class="koboSpan" id="kobo.566.1">s = pca.fit_transform(scaled_ds)</span></pre><pre class="console"><span class="koboSpan" id="kobo.567.1">
PCA_ds = pd.DataFrame(PCA_ds, columns=([</span></pre><pre class="console"><span class="koboSpan" id="kobo.568.1">
      "component_one","component_two", "component_three"]))</span></pre></li>
<li><span class="koboSpan" id="kobo.569.1">The amount of variation in a dataset that can be attributed to each of the main components (eigenvectors) produced by a PCA is measured statistically as “explained variance”. </span><span class="koboSpan" id="kobo.569.2">This simply refers to how much of a dataset’s variability may be attributed t</span><a id="_idTextAnchor1118"/><span class="koboSpan" id="kobo.570.1">o each unique </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">primary component.</span></span><pre class="console"><span class="koboSpan" id="kobo.572.1">
prin</span><a id="_idTextAnchor1119"/><span class="koboSpan" id="kobo.573.1">t(pca.explained_variance_ratio_)</span></pre><pre class="console"><span class="koboSpan" id="kobo.574.1">
&gt;&gt;&gt;&gt;[0.35092717 0.12336458 0.06470715]</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.575.1">For this project, we will reduce the dimensions to three, which manages to explain the 54% total var</span><a id="_idTextAnchor1120"/><span class="koboSpan" id="kobo.576.1">iance in the </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">observed variables:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.578.1">
print('Total explained variance',sum</span><a id="_idTextAnchor1121"/><span class="koboSpan" id="kobo.579.1">(pca.explained_variance_ratio_))
&gt;&gt;&gt;&gt; Total explained variance 0.5389989029179605</span></pre>
<ol>
<li value="3"><span class="koboSpan" id="kobo.580.1">Now we can project the data into a 3D plot </span><a id="_idTextAnchor1122"/><span class="koboSpan" id="kobo.581.1">to see the </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">points’ distribution:</span></span><pre class="console"><span class="koboSpan" id="kobo.583.1">
x,y,z=PCA_ds["component_one"],PCA_ds[</span></pre><pre class="console"><span class="koboSpan" id="kobo.584.1">
      "component</span><a id="_idTextAnchor1123"/><span class="koboSpan" id="kobo.585.1">_two"],PCA_ds["component_three"]</span></pre><pre class="console">
<a id="_idTextAnchor1124"/><span class="koboSpan" id="kobo.586.1">fig = plt.figure(figsize=(10,8))</span></pre><pre class="console"><span class="koboSpan" id="kobo.587.1">
ax = fig.a</span><a id="_idTextAnchor1125"/><span class="koboSpan" id="kobo.588.1">dd_subplot(111, projection="3d")</span></pre><pre class="console"><span class="koboSpan" id="kobo.589.1">
ax.scatter</span><a id="_idTextAnchor1126"/><span class="koboSpan" id="kobo.590.1">(x,y,z, c="maroon", marker="o" )</span></pre><pre class="console"><span class="koboSpan" id="kobo.591.1">
ax.set_title("A 3D Projection Of</span><a id="_idTextAnchor1127"/><span class="koboSpan" id="kobo.592.1"> Data In The Reduced Dimension")</span></pre><pre class="console"><span class="koboSpan" id="kobo.593.1">
plt.show()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.594.1">The preceding </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.595.1">code will show us the dimensions </span><a id="_idTextAnchor1128"/><span class="koboSpan" id="kobo.596.1">projected in </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">three dimensions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<span class="koboSpan" id="kobo.598.1"><img alt="Figure 8.12: PCA variables in 3D " src="image/B19026_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.599.1">Figure 8.12: PCA variables in 3D</span></p>
<p><span class="koboSpan" id="kobo.600.1">Since the attributes are now only three dimensions, agglomerative clustering will be used to perform the clustering. </span><span class="koboSpan" id="kobo.600.2">A hierarchical clustering technique is agglomerative clustering. </span><span class="koboSpan" id="kobo.600.3">Up until the appropriate number of clusters is reached, examples </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">are merged.</span></span></p>
<p><span class="koboSpan" id="kobo.602.1">The process of clustering involves grouping the population or data points into a number of groups so that </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.603.1">the data points within each group are more like one another than the data points within other groups. </span><span class="koboSpan" id="kobo.603.2">Simply put, the goal is to sort into clusters any groups of people who share similar characteristics. </span><span class="koboSpan" id="kobo.603.3">Finding unique groups, or “clusters”, within a data collection is the aim of clustering. </span><span class="koboSpan" id="kobo.603.4">The tool uses an ML algorithm to construct groups, where members of a group would typically share </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">similar traits.</span></span></p>
<p><span class="koboSpan" id="kobo.605.1">Two methods of pattern recognition used in ML are classification and clustering. </span><span class="koboSpan" id="kobo.605.2">Although there are some </span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.606.1">parallels between the two processes, clustering discovers similarities between things and groups them according to those features that set them apart from other groups of objects, whereas classification employs predetermined classes to which objects are assigned. </span><span class="koboSpan" id="kobo.606.2">“Clusters” is the name for </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">these collections.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">The steps involved in clustering are set </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">out here:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.610.1">Elbow method to determine the number of clusters to </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">be formed</span></span></li>
<li><span class="koboSpan" id="kobo.612.1">Clustering via </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">agglomerative clustering</span></span></li>
<li><span class="koboSpan" id="kobo.614.1">Examining the clusters formed via a </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">scatter plot</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.616.1">In K-means clustering, the ideal number of clusters is established using the elbow approach. </span><span class="koboSpan" id="kobo.616.2">The number of </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.617.1">clusters, or K, formed by various values of the cost function are plotted using the </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">elbow approac:.</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.619.1">The elbow approach is a heuristic used in cluster analysis to estimate the number of clusters present in a dataset. </span><span class="koboSpan" id="kobo.619.2">Plotting the explained variation as a function of the number of clusters, the procedure entails choosing the elbow of the curve as the appropriate number of clusters, as illustr</span><a id="_idTextAnchor1129"/><span class="koboSpan" id="kobo.620.1">ated in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">code snippe</span><a id="_idTextAnchor1130"/><span class="koboSpan" id="kobo.622.1">t:</span></span><pre class="console"><span class="koboSpan" id="kobo.623.1">
fig = plt.figure(figsize=(12,8))</span></pre><pre class="console"><span class="koboSpan" id="kobo.624.1">
elbow = KElbowVisualizer(KMeans(), k=(2,12), metric='distortion') # distortion: mean </span><a id="_idTextAnchor1131"/><span class="koboSpan" id="kobo.625.1">sum of squared dis</span><a id="_idTextAnchor1132"/><span class="koboSpan" id="kobo.626.1">tances to centers</span></pre><pre class="console"><span class="koboSpan" id="kobo.627.1">
elbow.fit(PCA_ds)</span></pre><pre class="console"><span class="koboSpan" id="kobo.628.1">
elbow.show()</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.629.1">This code </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.630.1">will plot an elbow plot, which will be a good estimation of </span><a id="_idTextAnchor1133"/><span class="koboSpan" id="kobo.631.1">the required number </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">of clusters:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<span class="koboSpan" id="kobo.633.1"><img alt="Figure 8.13: Elbow method " src="image/B19026_08_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.634.1">Figure 8.13: Elbow method</span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.635.1">According to the preceding cell, four clusters will be the best choice for this set of data. </span><span class="koboSpan" id="kobo.635.2">To obtain the final clusters, we will then fit the agglo</span><a id="_idTextAnchor1134"/><span class="koboSpan" id="kobo.636.1">merative clustering model, </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">like so:</span></span><pre class="console"><span class="koboSpan" id="kobo.638.1">
AC = Ag</span><a id="_idTextAnchor1135"/><span class="koboSpan" id="kobo.639.1">glomerativeClustering(n_clusters=</span><a id="_idTextAnchor1136"/><span class="koboSpan" id="kobo.640.1">4)</span></pre><pre class="console"><span class="koboSpan" id="kobo.641.1">
# fit model and predict cluste</span><a id="_idTextAnchor1137"/><span class="koboSpan" id="kobo.642.1">rs</span></pre><pre class="console"><span class="koboSpan" id="kobo.643.1">
yhat_AC = AC.fit_predict(PCA_ds)</span></pre><pre class="console"><span class="koboSpan" id="kobo.644.1">
PCA_ds["Clusters"] = yhat_AC</span></pre></li>
<li><span class="koboSpan" id="kobo.645.1">Finally, we will add a </span><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">Clusters</span></strong><span class="koboSpan" id="kobo.647.1"> feature to the or</span><a id="_idTextAnchor1138"/><span class="koboSpan" id="kobo.648.1">iginal DataFrame </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">for visualization:</span></span><pre class="console"><span class="koboSpan" id="kobo.650.1">
data["Clusters"]= yhat_AC</span></pre></li>
<li><span class="koboSpan" id="kobo.651.1">Now, we can visualize </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.652.1">the clusters in three dimensions us</span><a id="_idTextAnchor1139"/><span class="koboSpan" id="kobo.653.1">ing the color codes </span><a id="_idTextAnchor1140"/><span class="koboSpan" id="kobo.654.1">of </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">each cluster:</span></span><pre class="console"><span class="koboSpan" id="kobo.656.1">
classes = [</span><a id="_idTextAnchor1141"/><span class="koboSpan" id="kobo.657.1">0,1,2,3]</span></pre><pre class="console"><span class="koboSpan" id="kobo.658.1">
values = PCA_ds["Clusters"]</span></pre><pre class="console"><span class="koboSpan" id="kobo.659.1">
colors = ListedColor</span><a id="_idTextAnchor1142"/><span class="koboSpan" id="kobo.660.1">map(['red','blue','green','orange</span><a id="_idTextAnchor1143"/><span class="koboSpan" id="kobo.661.1">'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.662.1">
fig = plt.figure(figsize=(10,8</span><a id="_idTextAnchor1144"/><span class="koboSpan" id="kobo.663.1">))</span></pre><pre class="console"><span class="koboSpan" id="kobo.664.1">
ax = plt.subplot(projection='3d')</span></pre><pre class="console"><span class="koboSpan" id="kobo.665.1">
scatter = ax.sc</span><a id="_idTextAnchor1145"/><span class="koboSpan" id="kobo.666.1">atter(x, y,z, c=values, cmap=colors)</span></pre><pre class="console"><span class="koboSpan" id="kobo.667.1">
plt.legend(handles=scatter.legend_elements()[0], labels=classes)</span></pre></li>
</ol>
<p><span class="koboSpan" id="kobo.668.1">The preceding code will show a three-dimensional visualization of the PCA components co</span><a id="_idTextAnchor1146"/><span class="koboSpan" id="kobo.669.1">lored according to </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">the clusters:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<span class="koboSpan" id="kobo.671.1"><img alt="Figure 8.14: PCA variables with cluster labeling " src="image/B19026_08_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.672.1">Figure 8.14: PCA variables with cluster labeling</span></p>
<p><span class="koboSpan" id="kobo.673.1">From this, we can see that </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.674.1">each cluster occupies a specific space in the visualization. </span><span class="koboSpan" id="kobo.674.2">We will now dive into a description of each cluster </span><a id="_idTextAnchor1147"/><span class="koboSpan" id="kobo.675.1">to better understand </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">these segments.</span></span></p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor1148"/><span class="koboSpan" id="kobo.677.1">Understanding clusters as customer segments</span></h1>
<p><span class="koboSpan" id="kobo.678.1">To rigorously </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.679.1">evaluate the output obtained, we need to evaluate the depicted clusters. </span><span class="koboSpan" id="kobo.679.2">This is because clustering is an unsupervised method and the patterns extracted should always reflect reality, otherwise; we might just as well be </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">analyzing noise.</span></span></p>
<p><span class="koboSpan" id="kobo.681.1">Common traits among consumer groups can help a business choose which items or services to advertise to which segments and how to market to </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">each one.</span></span></p>
<p><span class="koboSpan" id="kobo.683.1">To do that, we will use </span><strong class="bold"><span class="koboSpan" id="kobo.684.1">exploratory data analysis</span></strong><span class="koboSpan" id="kobo.685.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.686.1">EDA</span></strong><span class="koboSpan" id="kobo.687.1">) to look at the data in the context of </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.688.1">clusters and make judgments. </span><span class="koboSpan" id="kobo.688.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">the steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.690.1">Let us first exam</span><a id="_idTextAnchor1149"/><span class="koboSpan" id="kobo.691.1">ine the clustering </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">group distribution:</span></span><pre class="console"><span class="koboSpan" id="kobo.693.1">
cluster_count = PCA_ds["C</span><a id="_idTextAnchor1150"/><span class="koboSpan" id="kobo.694.1">lusters"].value_counts().reset_index()</span></pre><pre class="console"><span class="koboSpan" id="kobo.695.1">
cluste</span><a id="_idTextAnchor1151"/><span class="koboSpan" id="kobo.696.1">r_count.columns  = ['cluster','count']</span><a id="_idTextAnchor1152"/></pre><pre class="console"><span class="koboSpan" id="kobo.697.1">
f, ax = plt.subplots(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.698.1">
fig = sns.barplot(x="cluster", y="count", palette=['red','blue','g</span><a id="_idTextAnchor1153"/><span class="koboSpan" id="kobo.699.1">reen','orange'],data=cluster_count)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer202">
<span class="koboSpan" id="kobo.700.1"><img alt="Figure 8.15: Cluster count " src="image/B19026_08_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.701.1">Figure 8.15: Cluster count</span></p>
<p><span class="koboSpan" id="kobo.702.1">The clusters </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.703.1">are fairly distributed with a predominance of cluster 0. </span><span class="koboSpan" id="kobo.703.2">It can be clearly seen that cluster 1 is our biggest set of customers, closely followed by </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">cluster 0.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.705.1">We can explore what each cluster is spending on for the targeted marketin</span><a id="_idTextAnchor1154"/><span class="koboSpan" id="kobo.706.1">g strategies </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.707.1">using the </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">following code</span><a id="_idTextAnchor1155"/><span class="koboSpan" id="kobo.709.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.710.1">
f, ax = plt.subplots(figsize=(12, 8))</span></pre><pre class="console"><span class="koboSpan" id="kobo.711.1">
pl = sns.scatterplot(data = data,x=data["Spent"], y=data["Income"]</span><a id="_idTextAnchor1156"/><span class="koboSpan" id="kobo.712.1">,hue=data["Clusters"], palette= colors)</span></pre><pre class="console"><span class="koboSpan" id="kobo.713.1">
pl.set_</span><a id="_idTextAnchor1157"/><span class="koboSpan" id="kobo.714.1">title("Cluste</span><a id="_idTextAnchor1158"/><span class="koboSpan" id="kobo.715.1">r vs Income And</span><a id="_idTextAnchor1159"/><span class="koboSpan" id="kobo.716.1"> Spending")</span></pre><pre class="console"><span class="koboSpan" id="kobo.717.1">
plt.legend()</span></pre><pre class="console"><span class="koboSpan" id="kobo.718.1">
plt.show()</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer203">
<span class="koboSpan" id="kobo.719.1"><img alt="Figure 8.16: Income versus spending " src="image/B19026_08_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.720.1">Figure 8.16: Income versus spending</span></p>
<p><span class="koboSpan" id="kobo.721.1">In the income versus spending plot, we can see the next </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">cluster patterns:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.723.1">Cluster 0 is of high spending and </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">average income</span></span></li>
<li><span class="koboSpan" id="kobo.725.1">Cluster 1 is of high spending and </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">high income</span></span></li>
<li><span class="koboSpan" id="kobo.727.1">Cluster 2 is of low spending and </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">low income</span></span></li>
<li><span class="koboSpan" id="kobo.729.1">Cluster 3 is of high spending and </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">low income</span></span></li>
</ul>
<ol>
<li value="3"><span class="koboSpan" id="kobo.731.1">Next, we will see the detailed distribution of clusters of the expenditure per product in the </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.732.1">data. </span><span class="koboSpan" id="kobo.732.2">Namely, we will explore expenditure </span><a id="_idTextAnchor1160"/><span class="koboSpan" id="kobo.733.1">patterns. </span><span class="koboSpan" id="kobo.733.2">The code is </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">illustrated her</span><a id="_idTextAnchor1161"/><span class="koboSpan" id="kobo.735.1">e:</span></span><pre class="console"><span class="koboSpan" id="kobo.736.1">
f, ax = plt.subplots(fi</span><a id="_idTextAnchor1162"/><span class="koboSpan" id="kobo.737.1">gsize=(12,6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.738.1">
sample = data.sample(750)</span></pre><pre class="console"><span class="koboSpan" id="kobo.739.1">
pl = sns.swarmplot(x=sample["Clusters"], y=sample["Spent"</span><a id="_idTextAnchor1163"/><span class="koboSpan" id="kobo.740.1">], color= "red", alpha=0.8 ,size=3)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer204">
<span class="koboSpan" id="kobo.741.1"><img alt="Figure 8.17: Spend distribution per cluster " src="image/B19026_08_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.742.1">Figure 8.17: Spend distribution per cluster</span></p>
<p><span class="koboSpan" id="kobo.743.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.744.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.745.1">.17</span></em><span class="koboSpan" id="kobo.746.1">, it can be seen how the spend is evenly distributed in cluster 0, cluster 1 is centered on high expenditure, and clusters 2 and 3 center on </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">low expenditure.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.748.1">Next, we will </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.749.1">use Seaborn to create Boxen plots of the clusters to f</span><a id="_idTextAnchor1164"/><span class="koboSpan" id="kobo.750.1">ind the spend distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">per cluster</span><a id="_idTextAnchor1165"/><span class="koboSpan" id="kobo.752.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.753.1">
f, ax = plt.subplots(figsize=(12, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.754.1">
pl = sns.boxenplot(x=data["Clusters"], y=data["Spent"], palet</span><a id="_idTextAnchor1166"/><span class="koboSpan" id="kobo.755.1">te=['red','blue','green','orange'])</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer205">
<span class="koboSpan" id="kobo.756.1"><img alt="Figure 8.18:  Spend distribution per cluster (Boxen plot) " src="image/B19026_08_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.757.1">Figure 8.18:  Spend distribution per cluster (Boxen plot)</span></p>
<p><span class="koboSpan" id="kobo.758.1">We can visualize the patterns in a different way using a </span><span class="No-Break"><span class="koboSpan" id="kobo.759.1">Boxen plot.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.760.1">Next, we will create a feature to get a sum of accepted promotions so that we can model their rela</span><a id="_idTextAnchor1167"/><span class="koboSpan" id="kobo.761.1">tionships with the </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">different clusters:</span></span><pre class="console"><span class="koboSpan" id="kobo.763.1">
data["TotalProm"] = data["AcceptedCmp1"]+ data["AcceptedCmp2"]+ data["AcceptedCmp3"]+ data["AcceptedCmp4"]+ data["AcceptedCmp5"]</span></pre></li>
<li><span class="koboSpan" id="kobo.764.1">Now, we will </span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.765.1">plot the count of total campaign</span><a id="_idTextAnchor1168"/><span class="koboSpan" id="kobo.766.1">s accepted in relation to </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">the clusters</span><a id="_idTextAnchor1169"/><span class="koboSpan" id="kobo.768.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.769.1">
f, ax = plt.subplots(figsize=(10, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.770.1">
pl = sns.countplot(x=data["TotalProm "],hue=data["Clusters"], pa</span><a id="_idTextAnchor1170"/><span class="koboSpan" id="kobo.771.1">lette= ['red','blue','green','orange'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.772.1">
pl.s</span><a id="_idTextAnchor1171"/><span class="koboSpan" id="kobo.773.1">et_title("Total Promotion</span><a id="_idTextAnchor1172"/><span class="koboSpan" id="kobo.774.1">s vs Cluster")</span></pre><pre class="console"><span class="koboSpan" id="kobo.775.1">
pl.set_xlabe</span><a id="_idTextAnchor1173"/><span class="koboSpan" id="kobo.776.1">l("Cluster")</span></pre><pre class="console"><span class="koboSpan" id="kobo.777.1">
pl.set_ylabel("Count")</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer206">
<span class="koboSpan" id="kobo.778.1"><img alt="Figure 8.19: Promotions applied per cluster " src="image/B19026_08_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.779.1">Figure 8.19: Promotions applied per cluster</span></p>
<p><span class="koboSpan" id="kobo.780.1">We can see that although there is no characteristic pattern in the promotions per cluster, we can see that cluster 0 and cluster 2 are the ones with the highest number of </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">applied promotions.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.782.1">Now, we can </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.783.1">visualize the number </span><a id="_idTextAnchor1174"/><span class="koboSpan" id="kobo.784.1">of deals purchased per type </span><span class="No-Break"><span class="koboSpan" id="kobo.785.1">of cluster</span><a id="_idTextAnchor1175"/><span class="koboSpan" id="kobo.786.1">:</span></span><pre class="console"><span class="koboSpan" id="kobo.787.1">
f, ax = plt.subplots(figsize=(12, 6))</span></pre><pre class="console"><span class="koboSpan" id="kobo.788.1">
pl = sns.boxenplot(y=data["NumDealsPurchases"],x=data["Clusters"], pa</span><a id="_idTextAnchor1176"/><span class="koboSpan" id="kobo.789.1">lette= ['red','blue','green','orange</span><a id="_idTextAnchor1177"/><span class="koboSpan" id="kobo.790.1">'])</span></pre><pre class="console"><span class="koboSpan" id="kobo.791.1">
pl.set_title("Purchased Deals")</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer207">
<span class="koboSpan" id="kobo.792.1"><img alt="Figure 8.20: Purchased deals per cluster " src="image/B19026_08_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.793.1">Figure 8.20: Purchased deals per cluster</span></p>
<p><span class="koboSpan" id="kobo.794.1">Promotional campaigns failed to be widespread, but the transactions were successful. </span><span class="koboSpan" id="kobo.794.2">The results from clusters 0 and 2 are the best. </span><span class="koboSpan" id="kobo.794.3">Cluster 1, one of our top clients, is not interested in the promotions, though. </span><span class="koboSpan" id="kobo.794.4">Nothing draws cluster 1 in a </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">strong way.</span></span></p>
<p><span class="koboSpan" id="kobo.796.1">Now that the clusters have been created and their purchasing patterns have been examined, let us look at everyone in these clusters. </span><span class="koboSpan" id="kobo.796.2">To determine who is our star customer and who requires further attention from the retail store’s marketing team, we will profile the clusters that have </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">been developed.</span></span></p>
<p><span class="koboSpan" id="kobo.798.1">Considering the cluster characterization, we will graph some of the elements that are </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.799.1">indicative of the customer’s personal traits. </span><span class="koboSpan" id="kobo.799.2">We will draw conclusions based on </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">the results.</span></span></p>
<ol>
<li value="8"><span class="koboSpan" id="kobo.801.1">We will use a Seaborn joint plot to visualize both the relationships an</span><a id="_idTextAnchor1178"/><span class="koboSpan" id="kobo.802.1">d distributions of </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">different variables:</span></span><pre class="console"><span class="koboSpan" id="kobo.804.1">
sns.jointplot(x=data['Education'], y=data["Spent"], hue =data["Clusters"], kind="kde", palette=['red',</span><a id="_idTextAnchor1179"/><span class="koboSpan" id="kobo.805.1">'blue','green','orange'],height=10)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer208">
<span class="koboSpan" id="kobo.806.1"><img alt="Figure 8.21: Spend versus education distribution per cluster " src="image/B19026_08_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.807.1">Figure 8.21: Spend versus education distribution per cluster</span></p>
<p><span class="koboSpan" id="kobo.808.1">Cluster 0 is centered </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.809.1">on medium education but with a peak in high education. </span><span class="koboSpan" id="kobo.809.2">Cluster 2 is the lowest in terms </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">of educat</span><a id="_idTextAnchor1180"/><span class="koboSpan" id="kobo.811.1">ion.</span></span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.812.1">Next, we will look at </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">family size:</span></span><pre class="console"><span class="koboSpan" id="kobo.814.1">
sns.jointplot(x=data['Family_Size'], y=data["Spent"], hue =data["Clusters"], kind="kde", palette=['red',</span><a id="_idTextAnchor1181"/><span class="koboSpan" id="kobo.815.1">'blue','green','orange'],height=10)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer209">
<span class="koboSpan" id="kobo.816.1"><img alt="Figure 8.22: Spend versus family size distribution per cluster " src="image/B19026_08_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.817.1">Figure 8.22: Spend versus family size distribution per cluster</span></p>
<p><span class="koboSpan" id="kobo.818.1">Cluster 1 represents </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.819.1">small family sizes, and cluster 0 represents couples and families. </span><span class="koboSpan" id="kobo.819.2">Clusters 2 and 3 are </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">evenly distributed.</span></span></p>
<ol>
<li value="10"><span class="koboSpan" id="kobo.821.1">We’ll now loo</span><a id="_idTextAnchor1182"/><span class="koboSpan" id="kobo.822.1">k at the spend versus </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">customer cluster:</span></span><pre class="console"><span class="koboSpan" id="kobo.824.1">
sns.jointplot(x=data['Customer_For'], y=data["Spent"], hue =data["Clusters"], kind="kde", palette=['red',</span><a id="_idTextAnchor1183"/><span class="koboSpan" id="kobo.825.1">'blue','green','orange'],height=10)</span></pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer210">
<span class="koboSpan" id="kobo.826.1"><img alt="Figure 8.23: Spend versus customer distribution per cluster " src="image/B19026_08_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.827.1">Figure 8.23: Spend versus customer distribution per cluster</span></p>
<p><span class="koboSpan" id="kobo.828.1">Cluster 3 is the group with older clients. </span><span class="koboSpan" id="kobo.828.2">While it is interesting to see that although </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.829.1">cluster 0 is the one with the highest spending, it is skewed to the left in terms of d</span><a id="_idTextAnchor1184"/><span class="koboSpan" id="kobo.830.1">ays since the user has been </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">a customer.</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.832.1">
sns.jointplot(x=data['Age'], y=data["Spent"], hue =data["Clusters"], kind="kde", palette=['red',</span><a id="_idTextAnchor1185"/><span class="koboSpan" id="kobo.833.1">'blue','green','orange'],height=10)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer211">
<span class="koboSpan" id="kobo.834.1"><img alt="Figure 8.24: Spend versus age distribution per cluster " src="image/B19026_08_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.835.1">Figure 8.24: Spend versus age distribution per cluster</span></p>
<p><span class="koboSpan" id="kobo.836.1">Cluster 0 is the one </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.837.1">with older customers, and the one </span><a id="_idTextAnchor1186"/><span class="koboSpan" id="kobo.838.1">with the youngest clients is </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">cluster 2.</span></span></p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor1187"/><span class="koboSpan" id="kobo.840.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.841.1">In this chapter, we have performed unsupervised clustering. </span><span class="koboSpan" id="kobo.841.2">After dimensionality reduction, agglomerative clustering was used. </span><span class="koboSpan" id="kobo.841.3">To better profile customers in clusters based on their family structures, income, and spending habits, we divided users into four clusters. </span><span class="koboSpan" id="kobo.841.4">This can be applied while creating more effective </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">marketing plans.</span></span></p>
<p><span class="koboSpan" id="kobo.843.1">In the next chapter, we will dive into the prediction of sales using time-series data to be able to determine revenue expectations given a set of historical sales, as well as understand their relationship with </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">other variables.</span></span></p>
</div>
</body></html>