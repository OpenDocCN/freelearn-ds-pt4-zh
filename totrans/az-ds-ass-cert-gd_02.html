<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer015">
			<h1 id="_idParaDest-15"><em class="italic"><a id="_idTextAnchor014"/>Chapter 1</em>: An Overview of Modern Data Science</h1>
			<p>Data science has its roots in the early eighteenth century and has gained tremendous popularity during the last couple of decades. </p>
			<p>In this book, you will learn how to run a data science project within Azure, the Microsoft public cloud infrastructure. You will gain all skills needed to become a certified Azure Data Scientist Associate. You will start with this chapter, which gives some foundational terminology used throughout the book. Then, you will deep dive into <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">AzureML</strong>) services. You will start by provisioning a workspace. You will then work on the no-code, low-code experiences build in the AzureML Studio web interface. Then, you will deep dive into the code-first data science experimentation, working with the AzureML <strong class="bold">Software Development Kit</strong> (<strong class="bold">SDK</strong>). </p>
			<p>In this chapter, you will learn some fundamental data science-related terms needed for the DP 100 exam. You will start by understanding the typical life cycle of a data science project. You will then read about big data and how Apache Spark technology enables you to train machine learning models against them. Then, you will explore what the <strong class="bold">DevOps</strong> mindset is and how it can help you become a member of a highly efficient, multi-disciplinary, agile team that builds machine learning-enhanced products.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>The evolution of data science</li>
				<li>Working on a data science project</li>
				<li>Using Spark in data science</li>
				<li>Adopting the DevOps mindset</li>
			</ul>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>The evolution of data science</h1>
			<p>If you <a id="_idIndexMarker000"/>try to find the roots of the data science practices, you will probably end up discovering evidence at the beginning of civilization. In the eighteenth century, governments were gathering demographic and financial data for taxation purposes, a practice<a id="_idIndexMarker001"/> called <strong class="bold">statistics</strong>. As years progressed, the use of this term was expanded to include the summarization and analysis of the data collected. In 1805, Adrien-Marie Legendre, a French mathematician, published a paper describing <a id="_idIndexMarker002"/>the <strong class="bold">least squares</strong> to fit linear equations, although most people credit Carl Friedrich Gauss for the complete description he published a couple of years later. In 1900, Karl Pearson published in the <em class="italic">Philosophical Magazine</em> his observations on<a id="_idIndexMarker003"/> the <strong class="bold">chi-square</strong> statistic, a cornerstone in data science for hypothesis testing. In 1962, John Tukey, the scientist famous <a id="_idIndexMarker004"/>for the <strong class="bold">fast Fourier transformation</strong> and the <strong class="bold">box plot</strong>, published<a id="_idIndexMarker005"/> a paper expressing his passion for data analysis and how statistics needed to evolve into a new science. </p>
			<p>On the other hand, with the rise of informatics in the middle of the twentieth century, the field of <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) was<a id="_idIndexMarker006"/> introduced in 1955 by John McCarthy as the official term for thinking machines. AI is a field of computer science that develops systems that can imitate intelligent human behavior. Using programming languages such <a id="_idIndexMarker007"/>as <strong class="bold">Information Processing Language</strong> (<strong class="bold">IPL</strong>) and <strong class="bold">LISt Processor</strong> (<strong class="bold">LISP</strong>), developers were writing programs that could manipulate lists <a id="_idIndexMarker008"/>and various other data structures to solve complex problems. In 1955, Arthur Samuel's checkers player was the first piece of software that would <em class="italic">learn</em> from the games it has already played by storing board states and the chance of winning if ending up in that state in a cache. This checkers program may have been the first example of <strong class="bold">machine learning</strong>, a subfield<a id="_idIndexMarker009"/> of AI that utilizes historical data and the patterns encoded in the data to train models and enable systems to mimic human tasks without explicitly coding the entire logic. In fact, you can think of machine learning models as software code that is generated by training an algorithm against a dataset to recognize certain types of patterns.</p>
			<p>In 2001, William S. Cleveland published the first article in which the term <strong class="bold">data science</strong> was used in the way we refer to it today, a science at the intersection of statistics, data analysis, and informatics that tries to explain phenomena based on data.</p>
			<p>Although most people correlate data science with machine learning, data science has a much broader scope, which includes the analysis and preparation of data before the actual machine learning model training process, as you will see in the next section.</p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Working on a data science project</h1>
			<p>A data science project<a id="_idIndexMarker010"/> aims to infuse an application with intelligence extracted from data. In this section, you will discover the common tasks and key considerations needed within such a project. There are quite a few well-established life cycle processes, such<a id="_idIndexMarker011"/> as <strong class="bold">Team Data Science Process</strong> (<strong class="bold">TDSP</strong>) and <strong class="bold">Cross-Industry Standard Process for Data Mining</strong> (<strong class="bold">CRISP-DM</strong>), that <a id="_idIndexMarker012"/>describe the iterative stages executed in a typical project. The most common stages are shown in <em class="italic">Figure 1.1</em>:</p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="Images/B16777_01_001.jpg" alt="Figure 1.1 – The iterative stages of a data science project&#13;&#10;" width="1037" height="731"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – The iterative stages of a data science project</p>
			<p>Although the diagram shows some indicative flows between the phases, you are free to jump from one phase to any other if needed. Moreover, this approach is iterative, and the data science team should go through multiple iterations, improving its business understanding and the resulting model until the success criteria are met. You will read more about the benefits of an iterative process in this chapter's <em class="italic">Adopting the DevOps mindset</em> section. The data science process starts from the business understanding phase, something you will read more about in the next section.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>Understanding of the business problem</h2>
			<p>The first stage in a data science project<a id="_idIndexMarker013"/> is that of business understanding. In this stage, the data science team collaborates with the business stakeholders to define a short, straightforward question that machine learning will try to answer. </p>
			<p><em class="italic">Figure 1.2</em> shows the five most frequent questions that machine learning can answer:</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="Images/B16777_01_002.jpg" alt="Figure 1.2 – Five questions machine learning can answer&#13;&#10;" width="1650" height="655"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Five questions machine learning can answer</p>
			<p>Behind each of those questions, there is a group of modeling techniques you will use:</p>
			<ul>
				<li><strong class="bold">Regression</strong> models <a id="_idIndexMarker014"/>allow you to predict a numeric value based on one or more features. For example, in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, you will be trying to predict a numeric value based on 10 measurements that were taken one year before the value you are trying to predict. Training a regression model is a <strong class="bold">supervised</strong> machine learning task, meaning that you <a id="_idIndexMarker015"/>need to provide enough sample data to train the model to predict the desired numeric value.</li>
				<li><strong class="bold">Classification</strong> models<a id="_idIndexMarker016"/> allow you to predict a class label for a given set of inputs. This label can be as simple as a yes/no label or a blue, green, or red color. For example, in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>, you will be training a classification model to detect whether a customer is going to cancel their phone subscription or not. Predicting whether a person is going to stop doing something is referred to as <strong class="bold">churn</strong> or <a id="_idIndexMarker017"/>attrition detection. Training a classification model is a supervised machine learning task and requires a labeled dataset to train the model. A labeled dataset contains both the inputs and the label that you want the model to predict.</li>
				<li><strong class="bold">Clustering</strong> is an <strong class="bold">unsupervised</strong> machine learning task<a id="_idIndexMarker018"/> that groups data. In contrast to the previous two model types, clustering<a id="_idIndexMarker019"/> doesn't require any training data. It operates on the given dataset and creates the desired number of clusters, assigning each data point to the collection it belongs. A common use case of clustering models is when you try to identify distinct consumer groups in your customer base that you will be targeting with specific marketing campaigns.</li>
				<li><strong class="bold">Recommender</strong> systems are<a id="_idIndexMarker020"/> designed to<a id="_idIndexMarker021"/> recommend the best options based on user profiles. Search engines, e-shops, and popular video streaming platforms utilize this type of model to produce personalized recommendations on what to do next.</li>
				<li><strong class="bold">Anomaly detection</strong> models <a id="_idIndexMarker022"/>can detect outliers from a dataset or within a data stream. Outliers are items that don't belong with the rest of the elements, indicating anomalies. For example, if a vibration sensor of a machine starts sending abnormal measurements, it may be a good indication that the device is about to fail.</li>
			</ul>
			<p>During the business understanding phase, you will try to understand the problem statement and define the success criteria. Setting up proper expectations of what machine learning can and cannot do is key to ensure alignment between teams. </p>
			<p>Throughout a data<a id="_idIndexMarker023"/> science project, it is common to have multiple rounds of business understandings. The data science team acquires a lot of insights after exploring datasets or training a model. It is helpful to bring those gathered insights to the business stakeholders and either verify your team's hypothesis or gain even more insights into the problem you are tackling. For example, business stakeholders may explain a pattern that you may detect in the data but cannot explain its rationale.</p>
			<p>Once you get a good grasp of what you are trying to solve, you need to get some data, explore it, and even label it, something you will read about in the next section.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Acquiring and exploring the data</h2>
			<p>After understanding the <a id="_idIndexMarker024"/>problem you are trying to solve, it's time to <a id="_idIndexMarker025"/>gather the data to support the machine learning process. Data can have many forms and formats. It can be either well-structured tabular data stored in database systems or even files, such as images, stored in file shares. Initially, you will not know which data to collect, but you must start from somewhere. A typical anecdote while looking for data is the belief that there is always an Excel file that will contain critical information, and you must keep asking for it until you find it.</p>
			<p>Once you have located the data, you will have to analyze it to understand whether the dataset is complete or not. Data is often stored within on-premises systems or <strong class="bold">Online Transactional Processing</strong> (<strong class="bold">OLTP</strong>) databases that <a id="_idIndexMarker026"/>you cannot easily access. Even if data is accessible, it is not advised to explore it directly within the source system, as you may accidentally impact the performance of the underlying engine that hosts the data. For example, a complex query on top of a sales table may affect the performance of the e-shop solution. In these cases, it is common to export the required datasets in a file format, such as the most<a id="_idIndexMarker027"/> interoperable <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>) format or the much more optimized for analytical processing <strong class="bold">Parquet</strong> format. These <a id="_idIndexMarker028"/>files are then uploaded to cheap cloud storage and become available for further analysis.</p>
			<p>Within Microsoft Azure, the most common target is either a Blob container within a storage account or a folder in the filesystem <a id="_idIndexMarker029"/>of <strong class="bold">Azure Data Lake Gen 2</strong>, which offers a far more granular access control mechanism. Copying the data can be done in a one-off manner by using tools <a id="_idIndexMarker030"/>such as <strong class="bold">AzCopy</strong> or <strong class="bold">Storage Explorer</strong>. If you <a id="_idIndexMarker031"/>would like to configure a repeatable process that could pull incrementally new data on a schedule, you can use more advanced tools such as the<a id="_idIndexMarker032"/> pipelines of <strong class="bold">Azure Data Factory</strong> or <strong class="bold">Azure Synapse Analytics</strong>. In <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>, you will review the components <a id="_idIndexMarker033"/>needed to pull data from on-premises and the available datastores to which you can connect from within the AzureML workspace to access the various datasets. In the <em class="italic">Working with datasets</em> section of <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>, you will read about the dataset types supported by AzureML and how you can explore them to gain insights into the info stored within them.</p>
			<p>A common task when <a id="_idIndexMarker034"/>gathering data is the data cleansing step. You<a id="_idIndexMarker035"/> remove duplicate records, impute missing values, or fix common data entry issues during this step. For example, you could harmonize a country text field by replacing <em class="italic">UK</em> records with <em class="italic">United Kingdom</em>. Within AzureML, you can perform such cleansing operations either in the designer that you will see in <a href="B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 6</em></a>, <em class="italic">Visual Model Training and Publishing</em>, or through the notebooks experience you will be working with from <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>, onward. Although you may start doing those cleansing operations with AzureML, as the project matures, these cleansing activities tend to move within the pipelines <a id="_idIndexMarker036"/>of <strong class="bold">Azure Data Factory</strong> or <strong class="bold">Azure Synapse Analytics</strong>, which<a id="_idIndexMarker037"/> pulls the data out of the source systems.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While doing data cleansing, be aware of <strong class="bold">yak shaving</strong>. The term <em class="italic">yak shaving</em> was coined in the 90s to describe the <a id="_idIndexMarker038"/>situation where, while working on a task, you realize that you must do another task, which leads to another one, and so on. This chain of tasks may take you away from your original goal. For example, you may realize that some records have invalid encoding on the country text field example, but you can understand the referenced country. You decide to change the export encoding of the CSV file, and you realize that the export tool you were using is old and doesn't support UTF-8. That leads you to a quest to find a system administrator to get your software updated. Instead of going down that route, make a note of what needs to be done and add it to your backlog. You can fix this issue in the next iteration when you will have a better understanding of whether you actually need this field or not.</p>
			<p>Another common<a id="_idIndexMarker039"/> task is labeling the dataset, especially if <a id="_idIndexMarker040"/>you will be dealing with supervised machine learning models. For example, if you are curating a dataset to predict whether a customer will churn or not, you will have to flag the records of the customers that canceled their subscriptions. A more complex labeling case is when you create a sentiment analysis model for social media messages. In that case, you will need to get a feed of messages, go through them, and assign a label on whether it is a positive or negative sentiment.</p>
			<p>Within AzureML Studio, you can create labeling projects that allow you to scale the labeling efforts of datasets. AzureML allows you to define either a text labeling or an image labeling task. You then bring in team members to label the data based on the given instructions. Once the team has started labeling the data, AzureML automatically trains a model relative to your defined task. When the model is good enough, it starts providing suggestions to the labelers to improve their productivity. <em class="italic">Figure 1.3</em> shows the labeling project creation wizard and the various options available currently in the image labeling task: </p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="Images/B16777_01_003.jpg" alt="Figure 1.3 – Creating an AzureML labeling project &#13;&#10;" width="1312" height="1197"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – Creating an AzureML labeling project </p>
			<p>Through this <a id="_idIndexMarker041"/>project phase, you should have discovered the related<a id="_idIndexMarker042"/> source systems and produced a cleansed dataset ready for the machine learning training. In the next section, you will learn how to create additional data features that will assist the model training process, a process known <a id="_idIndexMarker043"/>as <strong class="bold">feature engineering</strong>.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Feature engineering</h2>
			<p>During the<a id="_idIndexMarker044"/> feature engineering<a id="_idIndexMarker045"/> phase, you will be generating new data features that will better represent the problem you are trying to solve and help machines learn from the dataset. For example, the following code block creates a new feature named <strong class="source-inline">product_id</strong> by transforming the <strong class="source-inline">product</strong> column of the sales dataset:</p>
			<p class="source-code">product_map = { "orange juice": 1, "lemonade juice": 2 }</p>
			<p class="source-code">dataset["product_id"] = dataset["product"].map(product_map)</p>
			<p>This code block uses the <strong class="bold">pandas</strong> <strong class="source-inline">map</strong> method to convert text into numerical values. The <strong class="source-inline">product</strong> column is referred to as <a id="_idIndexMarker046"/>being a <strong class="bold">categorical</strong> variable, as all records are within a finite number of categories, in this case, <strong class="source-inline">orange juice</strong> or <strong class="source-inline">lemonade juice</strong>. If you had a 1-to-5 rating feature in the same dataset, that would have been a discrete numeric variable with a finite number of values that it can take, in this case, only <em class="italic">1</em>, <em class="italic">2</em>, <em class="italic">3</em>, <em class="italic">4</em>, or <em class="italic">5</em>. If you had a column that kept how many liters or gallons the customer bought, that <a id="_idIndexMarker047"/>would have been a <strong class="bold">continuous</strong> numeric variable<a id="_idIndexMarker048"/> that could take any numeric value greater than or equal to zero, such as half a liter. Besides numeric values, dates fields are also considered as continuous variables.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although the <strong class="source-inline">product_id</strong> feature is a <strong class="bold">discrete</strong> numeric variable in the preceding example, features such as that are commonly treated as a categorical variable, as you will see in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>.</p>
			<p>There are many featurization techniques<a id="_idIndexMarker049"/> available. An indicative list is as follows:</p>
			<ul>
				<li><strong class="bold">Scaling of numeric features</strong>: This<a id="_idIndexMarker050"/> technique converts all numeric features into ranges that can be easily compared. For example, in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, you will be training a machine learning model on top of medical measurements. Blood glucose measurements range from 80 to 200 mg/dL, while blood pressure measurements range from 60 to 128 mm Hg. These numeric values are scaled down using their mean value, a transformation referred to as standardization <a id="_idIndexMarker051"/>or <strong class="bold">Z-score</strong> normalization. Their values end up within the -1 to 1 range, which allows machines to extract better insights.</li>
				<li><strong class="bold">Split</strong>: Splitting a column<a id="_idIndexMarker052"/> into two new features is something very common. For example, the full name will be split into name and surname for further analysis.</li>
				<li><strong class="bold">Binning</strong>: This <a id="_idIndexMarker053"/>technique groups continuous features into distinct<a id="_idIndexMarker054"/> groups or bins that may expose important information regarding the problem you are trying to solve. For example, if you have the year of birth, you can create bins to group the different generations. In this case, folks with a year of birth between 1965 and 1980 would have been the <em class="italic">generation X</em> group, and people in the 1981 to 1996 range would have formed the <em class="italic">millennial</em> bin. It is common to use the clustering models that you saw in the <em class="italic">Understanding of the business problem</em> section to produce cohorts and define those bins.</li>
				<li><strong class="bold">One-hot encoding of categorical features</strong>: Some machine learning algorithms cannot handle<a id="_idIndexMarker055"/> categorical data and require all inputs to be numeric. In the example with <strong class="source-inline">product</strong>, you performed a label encoding. You converted the categorical variable into a numeric one. A typical example for label encoding is t-shirt sizes where you convert small to <em class="italic">1</em>, medium to <em class="italic">2</em>, and large to <em class="italic">3</em>. In the <strong class="source-inline">product</strong> example<a id="_idIndexMarker056"/> though, you accidentally defined the order between <strong class="source-inline">orange juice</strong> (<strong class="source-inline">1</strong>) and <strong class="source-inline">lemonade juice</strong> (<strong class="source-inline">2</strong>), which may confuse a machine learning algorithm. In this case, instead of the ordinal encoding used in the example that produced the <strong class="source-inline">product_id</strong> feature, you could have utilized one-hot encoding. In this case, you would introduce two binary features called <em class="italic">orange_juice</em> and <em class="italic">lemonade_juice</em>. These features would accept either <em class="italic">0</em> or <em class="italic">1</em> values, depending on which juice the customer bought.</li>
				<li><strong class="bold">Generate lag features</strong>: If you<a id="_idIndexMarker057"/> deal with time-series data, you may need to produce features from values from preceding time. For example, if you are trying to forecast the temperature 10 minutes from now, you may need to have the current temperature and the temperature 30 minutes ago and 1 hour ago. These two additional past temperatures are lag features that you will have to engineer.<p class="callout-heading">Important note</p><p class="callout">Making all those transformations in big datasets may require a tremendous amount of memory and processing time. This is where technologies such as Spark come into play to parallelize the process. You will learn more about Spark in the <em class="italic">Using Spark in data science</em> section of this chapter.</p></li>
			</ul>
			<p>In <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>, you will use the <strong class="source-inline">MinMaxScaler</strong> method from the <strong class="source-inline">sklearn</strong> library to scale numeric features. </p>
			<p>As a last step in the feature engineering stage, you normally remove unnecessary or highly correlated <a id="_idIndexMarker058"/>features, a process called <strong class="bold">feature selection</strong>. You will be<a id="_idIndexMarker059"/> dropping columns that will not be used to train the machine learning model. By dropping those columns, you reduce the memory requirements of the machines that will be doing the training, you reduce the computation time needed to train the model, and the resulting model will be much smaller in size.</p>
			<p>While creating those features, it is logical that you may need to go back to the <em class="italic">Acquiring and exploring the data</em> phase or even to the <em class="italic">Understanding of the business problem</em> stage to get more data and insights. At some point, though, your training dataset will be ready to train the model, something you will read about in the next section.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Training the model</h2>
			<p>As soon as you have<a id="_idIndexMarker060"/> prepared the dataset, the machine learning training process can begin. If the model requires supervised learning and you have enough data, you split them into a training dataset and validation dataset in a 70% to 30% or 80% to 20% ratio. You select the model type you want to train, specify the model's training<a id="_idIndexMarker061"/> parameters (called <strong class="bold">hyperparameters</strong>), and train the model. With the remaining validation dataset, you evaluate the trained model's performance according to a <strong class="bold">metric</strong> and you decide whether the model is good enough to move to the next stage, or perhaps return to the <em class="italic">Understanding of the business problem</em> stage. The training process of a supervised model is depicted in <em class="italic">Figure 1.4</em>:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="Images/B16777_01_004.jpg" alt="Figure 1.4 – Training a supervised machine learning model&#13;&#10;" width="1042" height="434"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Training a supervised machine learning model</p>
			<p>There are a couple of variations to the preceding statement: </p>
			<ul>
				<li>If the model is in the unsupervised learning category, such as the clustering algorithms, you just pass all the data to train the model. You then evaluate whether the detected clusters address the business need or not, modify the hyperparameters, and try again.</li>
				<li>If you have a model<a id="_idIndexMarker062"/> that requires supervised learning but don't have enough data, the <strong class="bold">k-fold cross validation</strong> technique<a id="_idIndexMarker063"/> is commonly used. With k-fold, you specify the number of folds you want to split the dataset. AzureML uses <strong class="bold">AutoML</strong> and <a id="_idIndexMarker064"/>performs either 10 folds if the data is less than 1,000 rows or 3 folds if the dataset is between 1,000 and 20,000 rows. Once you have those folds, you start an iterative process where you do the following:<ol><li>Keep a fold away for validation and train with the rest of the folds a new model.</li><li>Evaluate the produced model against the fold that you kept out.</li><li>Record the model score and discard the model.</li><li>Repeat <em class="italic">step I</em> by keeping another fold away for validation until all folds have been used for validation.</li><li>Produce the aggregated model's performance.<p class="callout-heading">Important note</p><p class="callout">In the machine learning research literature, there is an approach called <strong class="bold">semi-supervised</strong> learning. In that <a id="_idIndexMarker065"/>approach, a small amount of labeled data is combined with a large amount of unlabeled data to train the model.</p></li></ol></li>
			</ul>
			<p>Instead of training a single<a id="_idIndexMarker066"/> model, evaluating the results, and trying again with a different set of hyperparameters, you can automate the process and evaluate multiple models in parallel. This process is called hyperparameter tuning, something you will dive deep into in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>. In the same chapter, you will learn how you can even automate the model selection, an AzureML capability referred to as AutoML.</p>
			<p><strong class="bold">Metrics</strong> help <a id="_idIndexMarker067"/>you select the model that minimizes the difference between the<a id="_idIndexMarker068"/> predicted value and the actual one. They differ <a id="_idIndexMarker069"/>depending on the model type you are training. In<a id="_idIndexMarker070"/> regression <a id="_idIndexMarker071"/>models, metrics try to<a id="_idIndexMarker072"/> minimize the error between the predicted <a id="_idIndexMarker073"/>value and the actual one. The most common ones are <strong class="bold">Mean Absolute Error</strong> (<strong class="bold">MAE</strong>), <strong class="bold">Root Mean</strong><strong class="bold"> Squared Error</strong> (<strong class="bold">RMSE</strong>), <strong class="bold">Relative Squared Error</strong> (<strong class="bold">RSE</strong>), <strong class="bold">Relative Absolute</strong><strong class="bold"> Error</strong> (<strong class="bold">RAE</strong>), the <strong class="bold">coefficient of determination</strong> (<strong class="bold">R²</strong>), and <strong class="bold">Normalized Root Mean Squared Error</strong> (<strong class="bold">NRMSE</strong>), which you are going to see in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>.</p>
			<p>In a classification model, metrics are slightly different, as they have to evaluate both how many results it got right and how many it misclassified. For example, in the churn binary classification problem, there are four possible results:</p>
			<ul>
				<li>The model predicted that the customer would churn, and the customer churned. This is considered <a id="_idIndexMarker074"/>a <strong class="bold">True Positive</strong> (<strong class="bold">TP</strong>).</li>
				<li>The model predicted that the customer would churn, but the customer remained loyal. This<a id="_idIndexMarker075"/> is considered a <strong class="bold">False Positive</strong> (<strong class="bold">FP</strong>), since the model was wrong about the customer leaving.</li>
				<li>The model predicted that the customer would not churn, and the customer churned. This is <a id="_idIndexMarker076"/>considered a <strong class="bold">False Negative</strong> (<strong class="bold">FN</strong>), since the model was wrong about the customer being loyal.</li>
				<li>The model predicted that the customer would not churn, and the customer remained loyal. This is <a id="_idIndexMarker077"/>considered a <strong class="bold">True Negative</strong> (<strong class="bold">TN</strong>).</li>
			</ul>
			<p>These four states <a id="_idIndexMarker078"/>make up the <strong class="bold">confusion matrix</strong> that is shown in <em class="italic">Figure 1.5</em>:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="Images/B16777_01_005.jpg" alt="Figure 1.5 – The classification model's evaluation&#13;&#10;" width="1641" height="1035"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – The classification model's evaluation</p>
			<p>Through that confusion matrix, you can calculate other<a id="_idIndexMarker079"/> metrics, such as <strong class="bold">accuracy</strong>, which calculates the total number of correct results in the evaluation test (in this case, <strong class="bold">1132</strong> TP + <strong class="bold">2708</strong> TN = 3840 records versus <strong class="bold">2708</strong> + <strong class="bold">651</strong> + <strong class="bold">2229</strong> + <strong class="bold">1132</strong> = 6720 total records). On the <a id="_idIndexMarker080"/>other hand, <strong class="bold">precision</strong> or <strong class="bold">Positive Predictive Value</strong> (<strong class="bold">PPV</strong>) evaluates <a id="_idIndexMarker081"/>how many true predictions are actually true (in this case, <strong class="bold">1132</strong> TP versus <strong class="bold">1132</strong> + <strong class="bold">2229</strong> total true predictions). <strong class="bold">Recall</strong>, also <a id="_idIndexMarker082"/>known as <strong class="bold">sensitivity</strong>, measures how many actual true values were correctly<a id="_idIndexMarker083"/> classified (in this case, <strong class="bold">1132</strong> TP versus <strong class="bold">1132</strong> + <strong class="bold">651</strong> total true actuals). Depending on the business problem you are trying to solve, you will have to find the balance between the various metrics, as one metric may be more helpful than others. For example, during the COVID-19 pandemic, a model that determines whether someone is infected with recall equal to one would identify all infected patients. However, it may have accidentally misclassified some of the not-infected ones, which other metrics, such as precision, would have caught.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Be aware when your model fits your data too well. This is something that we refer to as <strong class="bold">overfitting</strong>, and it may<a id="_idIndexMarker084"/> indicate that the model has identified a certain pattern within your training dataset that may not exist in real life. Such models tend to perform poorly when put into production and make inferences on top of unknown data. A common reason for overfitting is a biased training dataset that exposes only a subset of real-world examples. Another reason is target leakage, which means that somehow the value you are trying to predict is passed as an input to the model, perhaps through a feature engineered using the target column. See the <em class="italic">Further reading</em> section for guidance on how to handle overfitting and imbalanced data.</p>
			<p>As you have seen so far, there are<a id="_idIndexMarker085"/> many things to consider while training a machine learning model, and throughout this book, you will get some hands-on experience in training models. In most cases, the first thing you will have to select is the type of <a id="_idIndexMarker086"/>computer that is going to run the training process. Currently, you have two options, <strong class="bold">Central Processing Unit</strong> (<strong class="bold">CPU</strong>) or <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>) compute targets. Both targets have at least a CPU in <a id="_idIndexMarker087"/>them, as this is the core element of any modern computer. The difference is that the GPU compute targets also offer some very powerful graphic cards that can perform massive parallel data processing, making training much faster. To take advantage of the GPU, the model you are training needs to support GPU-based <a id="_idIndexMarker088"/>training. GPU is usually used in neural network training with<a id="_idIndexMarker089"/> frameworks such <a id="_idIndexMarker090"/>as <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, and <strong class="bold">Keras</strong>.</p>
			<p>Once you have trained a machine learning model that satisfies the success criteria defined during the <em class="italic">Understanding of the business problem</em> stage of the data science project, it is time to operationalize it and start making inferences with it. That's what you will read about in the next section.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Deploying the model</h2>
			<p>When it comes to <a id="_idIndexMarker091"/>model operationalization, you have two main approaches:</p>
			<ul>
				<li><strong class="bold">Real-time inferences</strong>: The<a id="_idIndexMarker092"/> model is always loaded, waiting to make inferences on top of incoming data. Typical use cases are web and mobile applications that invoke a model to predict based on user input.</li>
				<li><strong class="bold">Batch inferences</strong>: The model is loaded every time the batch process is invoked, and it<a id="_idIndexMarker093"/> generates predictions on top of the incoming batch of records. For example, imagine that you have trained a model to identify your face in pictures and you want to label all the images you have on your hard drive. You will configure a process to use the model against each image, storing the results in a text or CSV file.</li>
			</ul>
			<p>The main difference between these two is whether you already have the data to perform the predictions or not. If you already have the data and they do not change, you can make inferences in batch mode. For example, if you are trying to predict the football scores for next week's matches, you can run a batch inference and store the results in a database. When customers ask for specific predictions, you can retrieve the value from the database. During the football match, though, the model predicting the end score needs features such as the current number of players and how many injuries there are, information that will become available in real time. In those situations, you might want to deploy a web service that exposes a REST API, where you send in the required information and the model is making the real-time inference. You will dive deep into both real-time and batch approaches in <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>.</p>
			<p>In this section, you <a id="_idIndexMarker094"/>reviewed the project life cycle of a data science project and went through all the stages, from understanding what needs to be done all the way to operationalizing a model by deploying a batch or real-time service. Especially for real-time streaming, you may have heard the <a id="_idIndexMarker095"/>term <strong class="bold">structured streaming</strong>, a scalable processing engine built on Spark to allow developers to perform real-time inferences the same way they would perform batch inference on top of static data. You will learn more about Spark in the next section.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Using Spark in data science</h1>
			<p>At the beginning of the<a id="_idIndexMarker096"/> twenty-first century, the big data problem became a <a id="_idIndexMarker097"/>reality. Data stored in data centers was growing in volumes and velocity. In 2021, we refer to datasets as big data when they reach at least a couple of terabytes in size, while it is not uncommon to see even petabytes of data in large organizations. These datasets increase at a rapid rate, which can be from a couple of gigabytes per day to even per minute, for example, when you are storing user interactions with a website in an online store to perform clickstream analysis.</p>
			<p>In 2009, a research project<a id="_idIndexMarker098"/> started at the University of California, Berkeley, trying to provide the parallel computing tools needed to handle big data. In 2014, the first version of Apache Spark was released from this research project. Members from that research team founded <a id="_idIndexMarker099"/>the <strong class="bold">Databricks</strong> company, one of the most significant contributors to the open source Apache Spark project.</p>
			<p>Apache Spark provides an <a id="_idIndexMarker100"/>easy-to-use scalable solution that allows people to perform parallel processing on top of data in a distributed manner. The main idea behind the Spark architecture is that a driver node is responsible for executing your code. Your code is split into smaller parallel actions that can be performed against smaller portions of data. These smaller jobs are scheduled to be executed by the worker nodes, as seen in <em class="italic">Figure 1.6</em>:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="Images/B16777_01_006.jpg" alt="Figure 1.6 – Parallel processing of big data in a Spark cluster&#13;&#10;" width="1232" height="732"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Parallel processing of big data in a Spark cluster</p>
			<p>For example, suppose <a id="_idIndexMarker101"/>you wanted to calculate how many products your company <a id="_idIndexMarker102"/>sold during the last year. In that case, Spark could spin up 12 jobs that would produce the monthly aggregates, and then the results would be processed by another job that would sum up the totals for all months. If you were tempted to load the entire dataset into memory and perform those aggregates directly from there, let's examine how much memory you would need within that computer. Let's assume that the sales data for a single month is stored in a CSV file that is 1 GB. This file will require approximately 10 GB of memory to load. The compressed <strong class="bold">Parquet</strong> files<a id="_idIndexMarker103"/> will require even more memory. For example, a similar 1 GB parquet file may end up needing 40 GB of memory to load as a <strong class="source-inline">pandas.</strong><strong class="source-inline">DataFrame</strong> object. As you can understand, loading all 12 files in memory simultaneously is an impossible task. You need to parallelize the processing, something Spark can do for you automatically. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Parquet files are stored in a columnar format, which allows you to load partially any number of columns you need. In the 1 GB Parquet example, if you load only half the columns from the dataset, you will probably need only 20 GB of memory. This is one of the reasons why the Parquet format is widely used in analytical loads.</p>
			<p>Spark is written in the <a id="_idIndexMarker104"/>Scala programming language. It offers APIs for Scala, Python, Java, R, and even C#. Still, the data science community is either working on Scala to<a id="_idIndexMarker105"/> achieve maximum computational performance and utilizing the Java library ecosystem or Python, which is widely adopted by the modern data science community. When you are writing Python code to utilize the <a id="_idIndexMarker106"/>Spark engine, you are using the PySpark tool to perform operations on top of <strong class="bold">Resilient Distributed Datasets</strong> (<strong class="bold">RDDs</strong>) or <strong class="source-inline">Spark.DataFrame</strong> objects introduced later in Spark Framework. To benefit from the distributed nature of Spark, you need to be handling big datasets. This means that Spark may be overkill if you deal with only hundreds of thousands of records or even a couple of millions of records.</p>
			<p>Spark offers two machine learning libraries, the <a id="_idIndexMarker107"/>old <strong class="bold">MLlib</strong> and the new version called <strong class="bold">Spark ML</strong>. Spark ML <a id="_idIndexMarker108"/>uses the <strong class="source-inline">Spark.DataFrame</strong> structure, a distributed collection of data, and offers similar functionality to the <strong class="source-inline">DataFrame</strong> objects used in Python pandas or R. Moreover, the <strong class="bold">Koalas</strong> project provides an implementation that <a id="_idIndexMarker109"/>allows data scientists with existing knowledge of <strong class="source-inline">pandas.DataFrame</strong> manipulations to use their existing coding skills on top of Spark.</p>
			<p>AzureML allows you to execute Spark jobs on top of PySpark, either using its native compute clusters or by attaching<a id="_idIndexMarker110"/> to <strong class="bold">Azure Databricks</strong> or <strong class="bold">Synapse Spark pools</strong>. Although <a id="_idIndexMarker111"/>you will not write any PySpark code in this book, in <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>, you will learn how to achieve similar parallelization benefits without the need for Spark or a driver node.</p>
			<p>No matter whether you are coding in regular Python, PySpark, R, or Scala, you are producing some code artifacts that are probably part of a larger system. In the next section, you will explore the DevOps mindset, which emphasizes the communication and collaboration of software engineers, data scientists, and system administrators to achieve faster release of valuable product features.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Adopting the DevOps mindset</h1>
			<p>DevOps is a team <a id="_idIndexMarker112"/>mindset that tries to minimize the silos between developers and system operators to shorten the development life cycle of a product. Developers are constantly changing a product to introduce new features and modify existing behaviors. On the other side, system operators need to keep the production systems stable and up and running. In the past, these two groups of people were isolated, and developers were <em class="italic">throwing</em> the new piece of software over to the operations team who would try to deploy it in production. As you can imagine, things didn't work that well all the time, causing frictions between those two groups. When it comes to DevOps, one fundamental practice is that a team needs to be autonomous and should contain all required disciplines, both <em class="italic">developers</em> and <em class="italic">operators</em>.</p>
			<p>When it comes to data science, some people refer to the<a id="_idIndexMarker113"/> practice as <strong class="bold">MLOps,</strong> but the fundamental ideas remain the same. A team should be self-sufficient, capable of developing all required components for the overall solution, from the data engineering parts that bring in data and the training of the models all the way to operationalizing the model in production. These teams usually work in an <strong class="bold">agile</strong> manner, which embraces an iterative approach, seeking constant improvement based on feedback, as seen in <em class="italic">Figure 1.7</em>:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="Images/B16777_01_007.jpg" alt="Figure 1.7 – The feedback flow in an agile MLOps team&#13;&#10;" width="822" height="487"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – The feedback flow in an agile MLOps team</p>
			<p>The MLOps team operates on its backlog and performs the iterative steps you saw in the <em class="italic">Working on a data science project</em> section. Once the model is ready, the system administrators, who are part of the team, are aware of what needs to be done to take the model into <a id="_idIndexMarker114"/>production. The model is monitored closely, and if a defect or performance degradation is observed, a backlog item is created for the MLOps team to address in their next sprint.</p>
			<p>In order to minimize the development and deployment life cycle of new features in production, automation needs to be embraced. The goal of a DevOps team is to minimize the number of human interventions in the deployment process and automate as many repeatable tasks as possible. </p>
			<p><em class="italic">Figure 1.8</em> shows the most frequently used components while developing real-time models using the MLOps mindset:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="Images/B16777_01_008.jpg" alt="Figure 1.8 – Components usually seen in MLOps-driven data science projects&#13;&#10;" width="1321" height="793"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – Components usually seen in MLOps-driven data science projects</p>
			<p>Let's analyze those <a id="_idIndexMarker115"/>components:</p>
			<ul>
				<li><strong class="bold">ARM templates</strong> allow you<a id="_idIndexMarker116"/> to automate the deployment of Azure resources. This enables the team to spin up and down development, testing, or even production environments in no time. These artifacts are stored within Azure DevOps in a Git version-control repository. The deployment of multiple environments is automated using Azure DevOps pipelines. You are going to read about ARM templates in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>. </li>
				<li>Using <strong class="bold">Azure Data Factory</strong>, the data<a id="_idIndexMarker117"/> science team orchestrates the pulling and cleansing of the data from the source systems. The data is copied within a data lake, which is accessible from the AzureML workspace. Azure Data Factory uses ARM templates to define its orchestration pipelines, templates that are stored within the Git repository to track changes and be able to deploy in multiple environments.</li>
				<li>Within the AzureML workspace, data scientists are working on their code. Initially, they start working on Jupyter notebooks. Notebooks are a great way to prototype some ideas, as you will see in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. As the project progresses, the scripts are exported from the notebooks and are organized into coding scripts. All those code artifacts are version-controlled into Git, using the <a id="_idIndexMarker118"/>terminal and commands such as the ones seen in <em class="italic">Figure 1.9</em>:</li>
			</ul>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="Images/B16777_01_009.jpg" alt="Figure 1.9 – Versioning a notebook and a script file using Git within AzureML&#13;&#10;" width="1650" height="716"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – Versioning a notebook and a script file using Git within AzureML</p>
			<ul>
				<li>When a model is trained, if it is performing better than the model that is currently in production, it is registered within AzureML, and an event is emitted. This event is captured by the AzureML DevOps plugin, which triggers the automatic deployment of the model in the test environment. The model is tested within that environment, and if all tests pass and no errors have been logged in <strong class="bold">Application Insights</strong>, which is<a id="_idIndexMarker119"/> monitoring the deployment, the artifacts can be <a id="_idIndexMarker120"/>automatically deployed to the next environment, all the way to production.</li>
			</ul>
			<p>The ability to ensure both <a id="_idIndexMarker121"/>code and model quality plays a crucial role in this automation process. In Python, you can use various tools, such as Flake8, Bandit, and Black, to ensure code quality, check for common security issues, and consistently format your code base. You can also use the <strong class="source-inline">pytest</strong> framework to write your functional testing, where you will be testing the model results against a golden dataset. With <strong class="source-inline">pytest</strong>, you can even perform integration testing to verify that the end-to-end system is working as expected.</p>
			<p>Adopting DevOps is a never-ending journey. The team will become better every time you repeat the process. The trick is to build trust in the end-to-end development and deployment process so that everyone is confident to make changes and deploy them in production. When the process fails, understand why it failed and learn from your mistakes. Create the mechanisms that will prevent future failures and move on.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Summary</h1>
			<p>In this chapter, you learned about the origins of data science and how it relates to machine learning. You then learned about the iterative nature of a data science project and discovered the various phases you will be working on. Starting from the problem understanding phase, you will then acquire and explore data, create new features, train a model, and then deploy to verify your hypothesis. Then, you saw how you can scale out the processing of big data files using the Spark ecosystem. In the last section, you discovered the DevOps mindset that helps agile teams be more efficient, meaning that they develop and deploy new product features in short periods of time. You saw the components that are commonly used within an MLOps-driven team, and you saw that in the epicenter of that diagram, you find AzureML.</p>
			<p>In the next chapter, you will learn how to deploy an AzureML workspace and understand the Azure resources that you will be using in your data science journey throughout this book.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Further reading</h1>
			<p>This section offers a list of helpful web resources that will help you augment your knowledge of the topics addressed in this chapter:</p>
			<ul>
				<li>AzCopy command-line tool to copy blobs and files to a storage account: <a href="http://aka.ms/azcopy">http://aka.ms/azcopy</a>.</li>
				<li>Azure Storage Explorer is a free tool to manage all your Azure cloud storage resources: <a href="https://azure.microsoft.com/features/storage-explorer/">https://azure.microsoft.com/features/storage-explorer/</a>.</li>
				<li><em class="italic">The Hitchhiker's Guide to the Data Lake</em> is an extensive guide on key considerations and best practices while building a data lake: <a href="https://aka.ms/adls/hitchhikersguide">https://aka.ms/adls/hitchhikersguide</a>.</li>
				<li>Optimizing data processing with AzureML: <a href="https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing">https://docs.microsoft.com/azure/machine-learning/concept-optimize-data-processing</a>.</li>
				<li>The Koalas project: <a href="https://koalas.readthedocs.io">https://koalas.readthedocs.io</a>.</li>
				<li>Guidance to prevent model overfitting and to handle unbalanced data: <a href="https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls">https://docs.microsoft.com/azure/machine-learning/concept-manage-ml-pitfalls</a>.</li>
				<li>MLOps guidance for data scientists and app developers working in AzureML and Azure DevOps: <a href="https://aka.ms/MLOps">https://aka.ms/MLOps</a>.</li>
			</ul>
		</div>
	</div></body></html>