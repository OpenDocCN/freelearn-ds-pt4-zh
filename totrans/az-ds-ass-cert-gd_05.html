<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer116">
			<h1 id="_idParaDest-54"><em class="italic"><a id="_idTextAnchor053"/>Chapter 4</em>: Configuring the Workspace</h1>
			<p>In this chapter, you will work inside the <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">ML</strong>) Studio web interface and learn how to configure the infrastructure needed to run an experiment inside its workspace. Then, you will learn how to provision or attach to existing compute resources and establish the connection between the Azure ML workspace and the various datastores that host your data. With these resources configured, you will be able to register a dataset and explore the capabilities offered in Azure ML to monitor those datasets. </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Provisioning compute resources</li>
				<li>Connecting to datastores</li>
				<li>Working with datasets</li>
			</ul>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control</strong> (<strong class="bold">IAM</strong>) role at the resource group level. Within that resource group, you should deploy an ML resource named <strong class="source-inline">packt-learning-mlw</strong>, as described in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Provisioning compute resources</h1>
			<p>Compute resources <a id="_idIndexMarker224"/>allow you to execute code scripts during your data exploratory analysis, the training phase, and when operationalizing ML models. The <strong class="bold">Azure ML</strong> workspace offers the<a id="_idIndexMarker225"/> following types of compute resources:</p>
			<ul>
				<li><strong class="bold">Compute instances</strong>: These are virtual machines dedicated to each data scientist that is working in the <strong class="bold">Azure ML workspace</strong>. </li>
				<li><strong class="bold">Compute clusters</strong>: These are scalable computer clusters that can run multiple training or inference steps in parallel.</li>
				<li><strong class="bold">Inference clusters</strong>: These<a id="_idIndexMarker226"/> are <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) clusters that can operationalize Docker images, which expose your models through a REST API.</li>
				<li><strong class="bold">Attached compute</strong>: These are existing <a id="_idIndexMarker227"/>compute resources, such as Ubuntu <strong class="bold">Virtual Machines</strong> (<strong class="bold">VMs</strong>) or <strong class="bold">Synapse Spark pools</strong>, that can be attached to the <a id="_idIndexMarker228"/>workspace to execute some of the steps of your training or inference pipelines.</li>
			</ul>
			<p>When you visit the <strong class="bold">Manage</strong> | <strong class="bold">Compute</strong> section of Azure ML Studio, you will see and be able to manage each of these types by selecting the corresponding tab, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="Images/B16777_04_001.jpg" alt="Figure 4.1 – Compute types in Azure ML Studio&#13;&#10;" width="1633" height="1440"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Compute types in Azure ML Studio</p>
			<p>In the following sections, you will<a id="_idIndexMarker229"/> discover each of these compute types and understand the important configuration parameters that you must be aware of.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Provisioning and attaching compute resources can also be done through the Azure ML CLI and the Azure ML Python SDK. You will see examples of provisioning the same resources via the Python SDK in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The Azure ML Python SDK</em>.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Compute instances</h2>
			<p>A compute instance<a id="_idIndexMarker230"/> is a VM that will facilitate your daily work as a data scientist. This is a managed, Ubuntu-based workstation that comes preconfigured <a id="_idIndexMarker231"/>with data science tools such as Jupyter Labs, RStudio, and various deep learning frameworks such as <strong class="bold">PyTorch</strong> and <strong class="bold">TensorFlow</strong>. <em class="italic">Managed</em> means that <a id="_idIndexMarker232"/>you won't have to manually update the operating system or ensure that it is patched against the latest security vulnerabilities. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Compute instances are ideal for corporate users who may not be able to install Python on their corporate computers. Compute instances only require you to have a modern web browser and internet access. Once you are connected to a compute instance, you have access to all the software packages you will need to work with your Azure ML workspace.</p>
			<p>All your files and preferences are securely stored within the <strong class="source-inline">/home/&lt;username&gt;/cloudfiles/code/</strong> folder of the VM. This folder is not part of the VM's disk, but it is mounted from a remote file share located in your Azure ML storage account, as shown in the following diagram. This file share allows you to share code files and notebooks across multiple compute instances, and you can even mount that folder locally on your own computer:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="Images/B16777_04_002.jpg" alt="Figure 4.2 – Remote file share mounted on multiple compute instances&#13;&#10;" width="822" height="232"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Remote file share mounted on multiple compute instances</p>
			<p>Compute instances primarily enable the <strong class="bold">Notebooks</strong> experience of the studio web interface, but they can also be used for training and inferencing at a small scale. In fact, compute instances <a id="_idIndexMarker233"/>provide job queuing capabilities and allow you to run up to two jobs per core, something that's very useful for testing and debugging scenarios. You will use your compute instance to perform data drift analysis in the <em class="italic">Data drift detection</em> section, later in this chapter. In the next section, you will learn how to provision your first compute instance.</p>
			<h3>Provisioning a compute instance</h3>
			<p>Let's learn how to <a id="_idIndexMarker234"/>provision an instance:</p>
			<ol>
				<li>In the studio web interface, navigate to the <strong class="bold">Manage</strong> | <strong class="bold">Compute</strong> section and select the <strong class="bold">Compute instances</strong> tab. If no compute instances have been provisioned, you will see a short introduction to compute instances: you can click the <strong class="bold">New</strong> button to start the compute provisioning wizard, as shown on the left-hand side of <em class="italic">Figure 4.3</em>. If other compute instances have already been provisioned in the workspace, you can start the same wizard by clicking on the <strong class="bold">New</strong> button from the top menu, as shown on the right-hand side of the following screenshot:<div id="_idContainer057" class="IMG---Figure"><img src="Images/B16777_04_003.jpg" alt="Figure 4.3 – Starting the compute instance provisioning wizard&#13;&#10;" width="1650" height="801"/></div><p class="figure-caption">Figure 4.3 – Starting the compute instance provisioning wizard</p></li>
				<li>The first thing you will <a id="_idIndexMarker235"/>need to select is the virtual machine's size. You can specify whether you need GPU-enabled machines or normal CPU machines. If you plan to run computer vision experiments or deep neural network training, a GPU machine can accelerate the training and inference process if the framework supports GPUs. Moreover, you can add filters to limit the list based on the minimum requirements you have for your workspace. In our case, we will select a CPU-only compute instance that has at least 14 GB of RAM and at least 4 cores, as shown in the following screenshot:<div id="_idContainer058" class="IMG---Figure"><img src="Images/B16777_04_004.jpg" alt="Figure 4.4 – The first page of the compute instance provisioning wizard&#13;&#10;" width="1542" height="1219"/></div><p class="figure-caption">Figure 4.4 – The first page of the compute instance provisioning wizard</p><p>In the results table, you can review the characteristics of each VM and get an estimation of how much it will cost per hour.</p><p class="callout-heading">Important note </p><p class="callout">Virtual machines' costs depend on their size, but also on the region where they are provisioned. For example, while authoring this book, East US 2 had the lowest average price in USD per hour, while West Europe was among the most expensive regions.</p><p>The following <a id="_idIndexMarker236"/>table contains a bit more information about the first three virtual machine sizes that appear in the result list. The main difference between the <strong class="bold">Standard_D3_v2</strong> and <strong class="bold">Standard_DS3_v2</strong> virtual machines is the premium storage disk. This provides disk caching capabilities, something that allows the VM to achieve performance levels that exceed the underlying disk performance. Therefore, by default, the wizard suggests that you select the <strong class="bold">Standard_DS3_v2</strong> virtual machine size:</p><div id="_idContainer059" class="IMG---Figure"><img src="Images/B16777_04_005.jpg" alt="Figure 4.5 – Comparison of compute sizes based on the docs.microsoft.com site&#13;&#10;" width="1113" height="225"/></div><p class="figure-caption">Figure 4.5 – Comparison of compute sizes based on the docs.microsoft.com site</p></li>
				<li>Leave the <strong class="bold">Standard_DS3_v2</strong> size selected and click <strong class="bold">Next</strong> to configure the advanced settings for the compute instance:<div id="_idContainer060" class="IMG---Figure"><img src="Images/B16777_04_006.jpg" alt="Figure 4.6 – The second page of the compute instance provisioning wizard&#13;&#10;" width="1638" height="873"/></div><p class="figure-caption">Figure 4.6 – The second page of the compute instance provisioning wizard</p><p class="callout-heading">Important note</p><p class="callout">If you are on a free trial, then you have a fixed core quota, which you cannot change unless you switch to a pay-as-you-go subscription. You may need to select <strong class="bold">Standard_DS2_v2</strong> to reduce the number of cores your compute instance will be using. You will need at least two more cores for the computer cluster you will be provisioning in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The Azure ML Python SDK</em>. </p></li>
				<li>Now, you need to <a id="_idIndexMarker237"/>provide a computer name. This is the name you will be using to reference the specific computer. The compute name should be unique within the Azure region. This means that you may need to change the name to something unique, potentially by adding some numbers in the name; for example, <strong class="source-inline">ds-021-workstation</strong>.</li>
				<li>Optionally, enable<a id="_idIndexMarker238"/> the SSH access flag. This option allows you to specify the public portion of the SSH key, which will give you remote access to the compute instance. The wizard allows you to generate that key directly within the wizard. Alternatively, you can generate one by following the instructions provided in the <em class="italic">Generating an SSH key pair</em> section. This option is not needed if you only plan to use the studio experience to conduct your data science experiments:<p class="figure-caption"> </p><div id="_idContainer061" class="IMG---Figure"><img src="Images/B16777_04_007.jpg" alt="Figure 4.7 – Enabling SSH access to the compute instance&#13;&#10;" width="1118" height="441"/></div><p class="figure-caption">Figure 4.7 – Enabling SSH access to the compute instance</p></li>
				<li>Click on the <strong class="bold">Create</strong> button to provision the compute instance. This will complete the wizard. At this point, the compute instance will be created and then start:<div id="_idContainer062" class="IMG---Figure"><img src="Images/B16777_04_008.jpg" alt="Figure 4.8 – Waiting for the compute instance to be created and transition to the Running state&#13;&#10;" width="814" height="191"/></div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 4.8 – Waiting for the compute instance to be created and transition to the Running state</p>
			<p>In the following<a id="_idIndexMarker239"/> sections, you will be given a brief introduction to SSH key-based authentication and how to generate an SSH key if you are not familiar with the process. Moreover, you will explore the advanced options of the wizard, options we will not need for the purposes of this book.</p>
			<h3>Generating an SSH key pair</h3>
			<p>An SSH key<a id="_idIndexMarker240"/> pair consists of two files – a private key and a public key. This key pair allows end users to encrypt text using the public portion of the key. The encrypted text can only be decrypted by the private portion of the SSH key, as shown in the following diagram. The private portion of the SSH key needs to be stored in a secure place, while the public portion of the key can be freely distributed to anyone:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="Images/B16777_04_009.jpg" alt="Figure 4.9 – A private key can decrypt information that's been encrypted with a public key&#13;&#10;" width="796" height="598"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – A private key can decrypt information that's been encrypted with a public key</p>
			<p>Using this property<a id="_idIndexMarker241"/> of the SSH key pair, you can configure your public key with a server so that it can use it for authentication. In a nutshell, when you try to connect to the server, the server will create a random challenge and encrypt it using the public portion of your key – the one you configured while provisioning the compute instance. You will have to decrypt that challenge using the private portion of the key, and then respond with an answer that will validate that you managed to decrypt the server's message. This flow will grant you access to the remote server over SSH.</p>
			<p>There are multiple open source tools that can help you generate an SSH key pair on your local machine. Azure offers a very easy way to generate an SSH key pair on your browser, and then store the public portion of the key as a resource in the Azure portal. Let's take a look: </p>
			<ol>
				<li value="1">Navigate to <a href="https://portal.azure.com">https://portal.azure.com</a> and click on the <strong class="bold">Create a resource</strong> button to add a <a id="_idIndexMarker242"/>new SSH key, similar to what you did in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>, while provisioning the Azure ML workspace. Search for the <strong class="source-inline">SSH Key</strong> resource and click on <strong class="bold">Create</strong>:<div id="_idContainer064" class="IMG---Figure"><img src="Images/B16777_04_010.jpg" alt="Figure 4.10 – SSH key resource in the marketplace&#13;&#10;" width="212" height="101"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 4.10 – SSH key resource in the marketplace</p></li>
				<li>Select the <strong class="source-inline">packt-azureml-rg</strong> resource group and provide a key-pair name, such as <strong class="source-inline">azureml-compute</strong>. Click on <strong class="bold">Review + create</strong> to navigate to the last step of the wizard:<div id="_idContainer065" class="IMG---Figure"><img src="Images/B16777_04_011.jpg" alt="Figure 4.11 – Generating an SSH key pair &#13;&#10;" width="753" height="493"/></div><p class="figure-caption">Figure 4.11 – Generating an SSH key pair </p></li>
				<li>Select <strong class="bold">Create</strong> to start the <a id="_idIndexMarker243"/>key generation process. The public and private portion of the key are generated in memory of your browser. A pop-up will prompt you to download the private key portion of the SSH key pair. Click on <strong class="bold">Download private key and create resource</strong> button. This will make your browser download a file named <strong class="source-inline">azureml-compute.pem</strong>. Make sure you store the file in a secure location:</li>
			</ol>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="Images/B16777_04_012.jpg" alt="Figure 4.12 – Storing the private portion of the SSH key&#13;&#10;" width="1095" height="826"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – Storing the private portion of the SSH key</p>
			<p>Once this process is<a id="_idIndexMarker244"/> done, an SSH key resource will appear in the resource group you selected on the wizard:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="Images/B16777_04_013.jpg" alt="Figure 4.13 – The SSH key resource you deployed&#13;&#10;" width="385" height="70"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – The SSH key resource you deployed</p>
			<p>In that resource, you can find the public portion of the SSH key, which you can copy and then paste into the compute <a id="_idIndexMarker245"/>instance provision wizard step you saw in the <em class="italic">Provisioning a compute instance</em> section:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="Images/B16777_04_014.jpg" alt="Figure 4.14 – The public portion of the generated key pair. At the top, &#13;&#10;you can see the downloaded private portion&#13;&#10;" width="1384" height="1321"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – The public portion of the generated key pair. At the top, you can see the downloaded private portion</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The SSH key resource requires the <strong class="source-inline">Microsoft.Compute</strong> provider to be registered in the Azure subscription that you are planning to use. If you are the owner of the subscription, Azure will automatically register the providers for you when you deploy the resources; otherwise, you will need to request the subscription owner to register this provider for you while following the instructions provided in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>So far, you have learned how to provision a compute instance and configure an SSH key, which will allow you to remote connect to that compute. You can also use this SSH key to connect to remote clusters, which you will provision in the next section, <em class="italic">Compute clusters</em>. In the following subsection, you will learn about the advanced configuration options of the compute instance provisioning <a id="_idTextAnchor057"/>wizard.</p>
			<h3>Advanced compute instance settings</h3>
			<p>In the compute provisioning wizard, you can optionally configure some advanced settings. One of them, is the <strong class="bold">Enable virtual network</strong> option, which allows you to attach the provisioned <a id="_idIndexMarker246"/>compute within a virtual network and to a specific subnet of that network, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B16777_04_015.jpg" alt="Figure 4.15 – Attaching the compute instance to a specific subnet&#13;&#10;" width="292" height="205"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Attaching the compute instance to a specific subnet</p>
			<p>This feature unlocks multiple advanced networking topologies. The most common one is when you are planning to access data sources that are not accessible over the internet. For example, if you have a storage account that you have firewall-protected to deny access over the internet, you <a id="_idIndexMarker247"/>normally deploy a <strong class="bold">private endpoint</strong> in a specific subnet to allow access to that specific<a id="_idIndexMarker248"/> storage account. When you provision your compute instance and configure it to be on the same subnet using the preceding option, the compute instance will be able to access the protected storage account, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="Images/B16777_04_016.jpg" alt="Figure 4.16 – Accessing a storage account that is only accessible through a private endpoint&#13;&#10;" width="969" height="332"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – Accessing a storage account that is only accessible through a private endpoint</p>
			<p>Another advanced option shown in the wizard is <strong class="bold">Assign to another user</strong>. This option ties back to the <em class="italic">Creating custom roles</em> section of <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>, where you learned how to create custom roles for your Azure ML workspace. In enterprise environments, it is common to not allow end users to deploy whatever compute instance they want. This is done by creating a custom role and allowing only the following operations for the virtual machines:</p>
			<ul>
				<li><strong class="bold">Microsoft.Compute/virtualMachines/start/action</strong></li>
				<li><strong class="bold">Microsoft.Compute/virtualMachines/restart/action</strong></li>
				<li><strong class="bold">Microsoft.Compute/virtualMachines/deallocate/action</strong></li>
			</ul>
			<p>In those environments, an administrator (or someone who has the <strong class="bold">Microsoft.Compute/virtualMachines/write</strong> permission) can provision compute instances and assign them to a specific person who may not be able to provision the compute instance on their own, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="Images/B16777_04_017.jpg" alt="Figure 4.17 – Assigning the provisioned compute instance to a fellow data scientist&#13;&#10;" width="245" height="94"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17 – Assigning the provisioned compute instance to a fellow data scientist</p>
			<p>Although this is a<a id="_idIndexMarker249"/> nice feature that the web interface wizard provides, it doesn't scale well when you need to provision multiple compute instances for multiple data scientists. Therefore, most of the time, administrators prefer to deploy compute instances through <strong class="bold">ARM template</strong> deployment. They can generate and download the template through this wizard and then deploy it for multiple users using the <strong class="bold">Azure CLI</strong> and pass the user ID as a parameter, as you saw in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>So far, you have seen how to provision a compute instance. In the next section, you will learn how to manage compute instances.</p>
			<h3>Managing your compute instances</h3>
			<p>Once you have provisioned at least a single compute instance, the <strong class="bold">Manage </strong>| <strong class="bold">Compute </strong>| <strong class="bold">Compute instances</strong> interface changes to a list that shows the available instances in the workspace. By default, the list is filtered to show only the instances that you can use, meaning those that you provisioned on your own or someone else provisioned on your behalf:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="Images/B16777_04_018.jpg" alt="Figure 4.18 – Compute instances list&#13;&#10;" width="1404" height="338"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.18 – Compute instances list</p>
			<p>From here, you can start, stop, restart, and delete the compute instances. When you start a compute instance, the resource's status changes to <strong class="bold">Running</strong> and the <strong class="bold">Applications</strong> column <a id="_idIndexMarker250"/>offers links to open a Terminal on the compute instance or open the Jupyter, JupyterLab, RStudio, and VS Code third-party authoring experiences.</p>
			<p>Before you open any of those three editing experiences, you will have to accept an important notice regarding the code you can execute in those environments, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="Images/B16777_04_019.jpg" alt="Figure 4.19 – Warning message about the code you execute within Azure ML Studio&#13;&#10;" width="461" height="200"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.19 – Warning message about the code you execute within Azure ML Studio</p>
			<p>It is important for you to understand that if you download a random script from the internet, it may contain malicious code, which may enable others to steal data or even access tokens from your account, something that may enable them to access Azure resources on your behalf.</p>
			<p>JupyterLab and Jupyter are very popular authoring experiences for Jupyter notebooks, Python script editing, and accessing the terminal to execute various commands, as shown in the following screenshot. When you click to open these editing experiences, a new browser tab will open. If you take a look at the URL on the new browser tab, you will notice that it consists of the compute instance's name, the region where this compute instance is located, and the suffix <strong class="bold">instances.azureml.ms</strong>. This is the reason why, in the previous section, <em class="italic">Provisioning a compute instance</em>, when you were provisioning a compute instance, you had to select a name that had to be unique within the Azure region where you are deploying the specific compute instance.</p>
			<p>All these third-party authoring experiences have a strong community around them, and you can use them if you are already familiar with them. However, note that Azure ML offers the <strong class="bold">Author</strong> | <strong class="bold">Notebooks</strong> experience, an augmented editing experience on top of JupyterLab that adds capabilities <a id="_idIndexMarker251"/>such as IntelliSense, something you will be using from <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The Azure ML Python SDK</em>, onward:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="Images/B16777_04_020.jpg" alt="Figure 4.20 – The JupyterLab editing experience&#13;&#10;" width="938" height="666"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.20 – The JupyterLab editing experience</p>
			<p>Clicking on the <strong class="bold">Terminal</strong> link in the <strong class="bold">Applications</strong> columns will open a new browser tab. You will be transferred to the <strong class="bold">Author</strong> | <strong class="bold">Notebooks</strong> section. Here, a web-based terminal will open, allowing you to issue commands to the compute instance:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="Images/B16777_04_021.jpg" alt="Figure 4.21 – Getting access to a terminal through the browser&#13;&#10;" width="1644" height="784"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.21 – Getting access to a terminal through the browser</p>
			<p>When you don't need the compute instance, such as during the weekend, you can stop it to avoid incurring costs. The compute instance will transition to the <strong class="bold">Stopped</strong> status and the <strong class="bold">Applications</strong> links will be disabled. Starting a stopped compute instance takes some time.</p>
			<p>If you have finished <a id="_idIndexMarker252"/>working with a compute instance, such as when the research phase of the project has been completed, you can <strong class="bold">Delete</strong> it to deallocate the reserved CPU cores that count against your subscription's quota. You can view the current quota by clicking on the corresponding <strong class="bold">View quota</strong> option from the menu shown in <em class="italic">Figure 4.18</em>.</p>
			<p>For now, you can stop your compute instance. You will start it again in the <em class="italic">Data drift detection</em> section:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="Images/B16777_04_022.jpg" alt="Figure 4.22 – Stopped compute instance&#13;&#10;" width="867" height="131"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.22 – Stopped compute instance</p>
			<p>In this section, you learned how to provision and manage a compute instance that will provide you with the necessary computational power to author notebooks and scripts, as well as potentially execute small-scale training and inference pipelines. In the next section, you will learn how to provision a compute cluster, a compute resource that will be able to scale up and<a id="_idIndexMarker253"/> down to accommodate multiple training and inference pipelines in parallel.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Compute clusters</h2>
			<p>A compute<a id="_idIndexMarker254"/> cluster is a group of interconnected virtual machines that scale out and in to accommodate a queue of tasks. This means that the cluster can have only <a id="_idIndexMarker255"/>a few or even zero nodes in it to avoid incurring costs when it's not needed, and it can also scale out to multiple nodes when you want to run a lot of tasks in parallel or perform a distributed ML training process.</p>
			<p>The creation process<a id="_idIndexMarker256"/> is very similar to provisioning a compute instance. Let's take a look:</p>
			<ol>
				<li value="1">Start by clicking the <strong class="bold">New</strong> button in the corresponding <strong class="bold">Compute clusters</strong> tab, as shown in <em class="italic">Figure 4.23</em>.<div id="_idContainer077" class="IMG---Figure"><img src="Images/B16777_04_023.jpg" alt="Figure 4.23 – Creating a new compute cluster&#13;&#10;" width="1650" height="1326"/></div><p class="figure-caption">Figure 4.23 – Creating a new compute cluster</p></li>
				<li>You will notice that the compute cluster provisioning wizard offers one additional option in comparison to the compute instances called <strong class="bold">Virtual machine priority</strong>, as shown in the following screenshot. Low-priority virtual machines take advantage<a id="_idIndexMarker257"/> of the surplus capacity in the Azure region where you want to provision a compute cluster. These virtual machines offer a significantly reduced price compared to dedicated VMs, but the compute nodes are not guaranteed to be available when you need them, or even if they will remain in your possession until the scheduled job is completed. This means that you may need to wait a long time until you can allocate such a VM, and a step in your training process may stop in the middle of its execution. Given these characteristics of low-priority VMs, you normally use this type of cluster when you have jobs that are not time-sensitive and consist of small running steps, or steps that automatically persist their state and can resume execution if they are evicted. For the purposes of this book, you can select the <strong class="bold">Dedicated</strong> option to <a id="_idIndexMarker258"/>avoid unexpected long waiting times when allocating compute nodes.</li>
				<li>In the <strong class="bold">Virtual machine type</strong> option, select <strong class="bold">GPU</strong> and select the cheapest VM size available from the <strong class="bold">Select from recommended options</strong> list seen in <em class="italic">Figure 4.24</em>.<p class="callout-heading">Important note</p><p class="callout">By default, free trial subscriptions do not allow you to provision GPU computes. Even if you change to a pay-as-you-go subscription, you will need to make a request through the Azure portal to increase your quota. If you run into a lack of quota issue, you can select CPU-based computes instead of GPU-based ones. For the purposes of this book, you do not need GPU-based clusters.</p><div id="_idContainer078" class="IMG---Figure"><img src="Images/B16777_04_024.jpg" alt="Figure 4.24 – The first page of the compute cluster provisioning wizard&#13;&#10;" width="1650" height="1212"/></div><p class="figure-caption">Figure 4.24 – The first page of the compute cluster provisioning wizard</p></li>
				<li>Click <strong class="bold">Next</strong> to continue to the second page of the wizard:</li>
				<li>On the second page<a id="_idIndexMarker259"/> of the wizard, you will need to specify a cluster name. This name is going to be how you reference this cluster in the web experience and through code, so make sure it's something that represents what this cluster is meant for, such as <strong class="source-inline">gpu-cluster</strong>:<div id="_idContainer079" class="IMG---Figure"><img src="Images/B16777_04_025.jpg" alt="Figure 4.25 – The second page of the compute cluster provisioning wizard&#13;&#10;" width="1650" height="1190"/></div><p class="figure-caption">Figure 4.25 – The second page of the compute cluster provisioning wizard</p><p>You can also tweak the<a id="_idIndexMarker260"/> minimum and maximum number of nodes and the idle seconds before the cluster scales down. Every time you request the cluster to perform a job, the tasks of the job are added to the cluster's scheduler. If the cluster doesn't have enough nodes to execute the scheduled tasks, it will scale out by adding a compute node to the cluster. Adding a node to the cluster takes some time, as you need to allocate the VM. Therefore, instead of deallocating the VM immediately once the scheduled tasks have completed, the cluster can wait for the defined idle period, just in case a new task gets scheduled.</p><p>Similar to the compute instances, you can <strong class="bold">Enable SSH access</strong> if you want to remotely connect to the compute cluster nodes to troubleshoot job executions. Due to the<a id="_idIndexMarker261"/> ephemeral nature of the cluster nodes, the wizard allows you to specify an <strong class="bold">Admin password</strong> if you want, instead of a <strong class="bold">SSH public key</strong>, as shown in the following screenshot:</p><p class="figure-caption"> </p><div id="_idContainer080" class="IMG---Figure"><img src="Images/B16777_04_026.jpg" alt="Figure 4.26 – Compute clusters allow you to use an Admin password instead of an SSH public key&#13;&#10;" width="712" height="520"/></div><p class="figure-caption">Figure 4.26 – Compute clusters allow you to use an Admin password instead of an SSH public key</p><p>Under <strong class="bold">Advanced settings</strong>, you can find the <strong class="bold">Enable virtual network</strong> option, which you saw when we looked at compute instances in the previous section. In addition to that option, you have the option to <strong class="bold">Assign a managed identity</strong> to the compute cluster:</p><div id="_idContainer081" class="IMG---Figure"><img src="Images/B16777_04_027.jpg" alt="Figure 4.27 – Assigning a managed identity to the compute cluster&#13;&#10;" width="285" height="126"/></div><p class="figure-caption">Figure 4.27 – Assigning a managed identity to the compute cluster</p><p>Azure allows<a id="_idIndexMarker262"/> you to attach an <strong class="bold">Azure Active Directory</strong> (<strong class="bold">AAD</strong>) identity to the compute cluster<a id="_idIndexMarker263"/> nodes, allowing the code that executes in those VMs to access Azure resources using that identity. <strong class="bold">Managed identities</strong> eliminate the need to have credentials stored within your scripts. The identity is attached to the specific VM, and <a id="_idIndexMarker264"/>your code can request AAD access tokens through the Azure Instance Metadata Service or through the Python SDK without a password, as long as the code is executed within that specific VM.</p></li>
				<li>For the purposes of this book, you will not modify any option here. Name the cluster <strong class="source-inline">gpu-cluster</strong> and click on <strong class="bold">Create</strong> to create your first zero node, GPU-based compute cluster:</li>
			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="Images/B16777_04_028.jpg" alt="Figure 4.28 – Your first GPU-based compute cluster is ready to use&#13;&#10;" width="870" height="223"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.28 – Your first GPU-based compute cluster is ready to use</p>
			<p>Notice that in the preceding screenshot, the compute cluster has been provisioned successfully but that there are 0 nodes in it, which means that it doesn't incur any cost. You can also see the following metrics in this list:</p>
			<ul>
				<li><strong class="bold">Idle nodes</strong>: These are the nodes waiting for a task to be scheduled or to be de-allocated once the idle time has passed.</li>
				<li><strong class="bold">Busy nodes</strong>: These are the nodes that are currently executing a task.</li>
				<li><strong class="bold">Unprovisioned nodes</strong>: These are the nodes that haven't been allocated yet but can potentially be allocated if the number of scheduled tasks increases.<p>From this list, you can delete the cluster if you don't want it anymore.</p><p>If you click on the <a id="_idIndexMarker265"/>compute cluster's name, you will be able to see the cluster's details, as shown in the following screenshot. From this view, you can edit the minimum and maximum number of nodes, the idle seconds before the cluster scales down, and change how the managed identity that you configured previously is assigned. In fact, it is common for data science teams to modify their predefined compute clusters in the morning so that they have at least one node in them. It helps them avoid waiting for the first node to be allocated. When the day is over, they change the setting down to zero to save on costs:</p></li>
			</ul>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="Images/B16777_04_029.jpg" alt="Figure 4.29 – Compute cluster's details about where you can edit its configuration&#13;&#10;" width="1650" height="1358"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.29 – Compute cluster's details about where you can edit its configuration</p>
			<p>In this section, you<a id="_idIndexMarker266"/> learned how to provision a compute cluster. These clusters are used to perform training jobs and batch inferences. In the next section, you will learn how to provision an <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), which allows you to perform real-time inferences at a large scale.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Inference clusters</h2>
			<p>Kubernetes is a portable, extensible, open<a id="_idIndexMarker267"/> source platform for managing<a id="_idIndexMarker268"/> containerized workloads and services. It has been widely used to operationalize various forms of applications, from web applications to model inference REST APIs, due to its ability to auto scale and auto recover from failures. <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) is the managed version of the Kubernetes cluster in Azure, a<a id="_idIndexMarker269"/> service that lets you focus on your workload and let Azure manage the operating bits of the cluster, such as its master nodes.</p>
			<p>If you are not familiar with AKS, then don't worry – the following diagram provides a high-level overview of the components involved. In a nutshell, you can configure <strong class="bold">Node pools</strong>, a group of virtual machines that have the same configuration; for example, virtual machines with GPU cards on them. These<a id="_idIndexMarker270"/> pools can have one <strong class="bold">node</strong> (or more), which is a virtual machine. Within each node, you can host one or more <strong class="bold">pods</strong>. Each pod consists <a id="_idIndexMarker271"/>of a couple of <strong class="bold">Docker images</strong>, which form an application unit, one of which may be the model you want to operationalize. Each pod can be replicated into multiple nodes, either to accommodate increased load or for resiliency reasons in the case a node goes down:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="Images/B16777_04_030.jpg" alt="Figure 4.30 – High-level overview of AKS concepts showing Pod X being replicated in two nodes&#13;&#10;" width="1650" height="727"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.30 – High-level overview of AKS concepts showing Pod X being replicated in two nodes</p>
			<p>From within Azure ML Studio, you can create or attach an existing AKS cluster to your workspace. You do <em class="italic">not</em> need to create an AKS cluster for the purposes of this book. Let's get started:</p>
			<ol>
				<li value="1">The creation wizard can be invoked by clicking on the <strong class="bold">New</strong> button in the <strong class="bold">Inference clusters</strong> tab, seen in <em class="italic">Figure 4.31</em>:<div id="_idContainer085" class="IMG---Figure"><img src="Images/B16777_04_031.jpg" alt="Figure 4.31 – Create or attach an AKS cluster to the Azure ML workspace&#13;&#10;" width="1657" height="1374"/></div><p class="figure-caption">Figure 4.31 – Create or attach an AKS cluster to the Azure ML workspace</p><p class="callout-heading">Important note</p><p class="callout">When you provision an AKS cluster, a new resource group is created within your Azure subscription that hosts all the components needed for AKS to work. This requires additional permissions at the subscription level. If you can't create resource groups, AKS cluster provisioning will fail.</p></li>
				<li>In the first step of the wizard, you can either attach an existing AKS cluster or create a new one. If you choose to create one, you will have to specify the Azure region where you want the AKS cluster to be deployed. You will also need to specify the node pool's VM size, similar to what you did when you deployed a compute instance:<div id="_idContainer086" class="IMG---Figure"><img src="Images/B16777_04_032.jpg" alt="Figure 4.32 – Step 1 of provisioning an inference AKS cluster&#13;&#10;" width="939" height="607"/></div><p class="figure-caption">Figure 4.32 – Step 1 of provisioning an inference AKS cluster</p></li>
				<li>Clicking <strong class="bold">Next</strong> will bring you to the <strong class="bold">Settings</strong> page, where you need to specify the name of the AKS cluster. You also need to specify the purpose of the cluster. If this is a production cluster, the number of virtual CPUs in the cluster must be more than 12; this means that if you selected a 4 core VM size, you will need at least three nodes to be able to provision a production-ready AKS cluster. If this cluster is for development and testing, you can provision just one node. </li>
				<li>Besides the name and the number of nodes in the node pool, you can configure the networking options of the cluster and the SSL certificate that will be used to secure the connection to the applications, if you want to expose them through an HTTPS endpoint. For the purposes of this book, you do not need to modify any of those options:<div id="_idContainer087" class="IMG---Figure"><img src="Images/B16777_04_033.jpg" alt="Figure 4.33 – Step 2 of provisioning an inference AKS cluster&#13;&#10;" width="848" height="604"/></div><p class="figure-caption">Figure 4.33 – Step 2 of provisioning an inference AKS cluster</p></li>
				<li>Once your cluster has been created, you will be able to delete it or detach it from the workspace through the list shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="Images/B16777_04_034.jpg" alt="Figure 4.34 – List of AKS inference clusters&#13;&#10;" width="704" height="168"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.34 – List of AKS inference clusters</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">AKS is the production-ready way of deploying real-time endpoints. In the exam, when you are asked where you would deploy a production load, AKS should be the right answer. Nonetheless, because an AKS cluster is an expensive resource, this book will not use it in its examples. If you are using a free subscription, you will probably not have enough cores quota to be able to provision one. If you did provision one, make sure you keep an eye on the cost to avoid running out of credit.</p>
			<p>In this section, you learned about how Azure ML can help you attach to or provision an AKS cluster so that you can host your production real-time inference endpoints. In the next section, you will learn how to attach existing compute resources to your workspace.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Attached compute</h2>
			<p>If you already <a id="_idIndexMarker272"/>have compute resources provisioned, not necessarily in the subscription you have deployed your Azure ML workspace, you can attach them to your workspace. Attaching those resources allows you to reuse them, especially in<a id="_idIndexMarker273"/> cases where they are underutilized. A common scenario is for a department to have an Ubuntu-based <strong class="bold">Data Science Virtual Machine</strong> (<strong class="bold">DSVM</strong>), which may be <a id="_idIndexMarker274"/>running 24 hours, 7 days of the week, to serve a legacy web application. You can reuse this resource in your experiments by attaching it to your workspace and then referencing it to execute various tasks, the same way you would reference a compute cluster to perform a task.</p>
			<p>The studio experience allows you to attach multiple types of computes, including the following popular targets:</p>
			<ul>
				<li><strong class="bold">Virtual machines</strong>: You can attach existing Ubuntu-based virtual machines that are publicly accessible over the internet. This option includes potential DSVMs you may already have.</li>
				<li><strong class="bold">Azure Databricks</strong> and <strong class="bold">HDInsights</strong>: These options allow you to attach existing <strong class="bold">Apache Spark</strong>-based computes to your workspace.</li>
				<li><strong class="bold">Azure Data Factory</strong>: The Azure Data Factory resource allows you to perform copy activities from one data source to another. For example, you can copy from a storage account to a SQL database using that resource. Azure Data Factory is currently only supported through the Azure ML SDK and not from the studio experience. </li>
			</ul>
			<p>For the purposes of the DP100 exam, you will not need to attach any resources. The following screenshot shows how you can initiate the attach wizard from within the studio experience:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B16777_04_035.jpg" alt="Figure 4.35 – Attaching existing compute resources to your workspace&#13;&#10;" width="1396" height="1135"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.35 – Attaching existing compute resources to your workspace</p>
			<p>In this section, you learned how to provision and attach compute resources to your Azure ML workspace. This allows you to execute code during the data exploration, model training, and model inference phases of your data science projects. In the next section, you will learn <a id="_idIndexMarker275"/>how to configure connectivity to various data sources, something that will enable you to access data.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor061"/>Connecting to datastores</h1>
			<p>Datastores are the engines <a id="_idIndexMarker276"/>where your data resides and provide access to anyone authorized to do so. In most Python examples you see on the internet, there is a connection string that contains the credentials to connect to a database or a blob store. There are a couple of drawbacks associated with this technique:</p>
			<ul>
				<li>The credentials stored within these scripts are considered a security violation, and you can accidentally expose your protected datasets by publishing a script in a public repository such as GitHub.</li>
				<li>You need to manually update all the scripts when the credentials change.</li>
			</ul>
			<p>Azure ML allows you to have a single centralized location where you define the connection properties to various stores. Your credentials are securely stored as <strong class="bold">secrets</strong> within the workspace's associated <strong class="bold">key vault</strong>. In your scripts, you reference the datastore using its name and you can access its data without having to specify the credentials. If, at some point in time, the credentials of a datastore change, you can centrally update them, and all your scripts and pipelines will continue to work.</p>
			<p>You can view all the registered datastores by navigating to the <strong class="bold">Manage </strong>| <strong class="bold">Datastores</strong> section of the studio:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B16777_04_036.jpg" alt="Figure 4.36 – List of registered datastores in the workspace&#13;&#10;" width="1211" height="431"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.36 – List of registered datastores in the workspace</p>
			<p>Note that, by default, you already have two datastores registered. The default one, named <strong class="source-inline">workspaceblobstore</strong>, is the default blob storage where all the pipeline metrics and artifacts are stored. Your workspace needs to have a default datastore. As you will see in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The Azure ML Python SDK</em>, you can even reference that store very easily through the Python SDK. The other store, named <strong class="source-inline">workspacefilestore</strong>, is a file<a id="_idIndexMarker277"/> share datastore that you can mount on your local machine and upload files to.</p>
			<p>From this list, you can do the following:</p>
			<ul>
				<li>Update the credentials of a datastore: You need to click on the name of the datastore, which will get you its registration details. From there, you can click on <strong class="bold">Update credentials</strong> to specify the updated value or change the type of authentication, something you will see in the next section.</li>
				<li><strong class="bold">Unregister</strong> a datastore: You can unregister any datastore that is not marked as the default datastore.</li>
				<li><strong class="bold">Set as default datastore</strong>: Change the default datastore to the one you selected from the list.</li>
			</ul>
			<p>Finally, from this list, you can create a <strong class="bold">New datastore</strong> registration, an action that activates the new datastore wizard shown in the following screenshot:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="Images/B16777_04_037.jpg" alt="Figure 4.37 – New datastore wizard&#13;&#10;" width="463" height="461"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.37 – New datastore wizard</p>
			<p>Here, you need to <a id="_idIndexMarker278"/>specify a unique datastore name within the Azure ML workspace. You must do this to reference this store in your scripts and the various components of the studio experience. The next thing you need to select is the datastore type. There are a couple of Azure-native datastores that are supported by the Azure ML workspace, something you will explore in the next section.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>Types of datastores</h2>
			<p>Azure ML supports two <a id="_idIndexMarker279"/>categories of datastores: the ones based on files, such as blob storage, file shares, and data lake stores, and relational databases, such as Azure SQL and Azure PostgreSQL.</p>
			<p>The currently supported datastores are shown in the following diagram:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="Images/B16777_04_038.jpg" alt="Figure 4.38 – Azure ML supported datastores&#13;&#10;" width="1290" height="730"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.38 – Azure ML supported datastores</p>
			<p>The <a id="_idIndexMarker280"/>recommendation is to use <strong class="bold">Azure Blob Storage</strong>-based datastores. These stores are the most cost-effective ones. They provide multiple tiers, such as the more expensive premium one, which provides you with increased throughput speeds, something that can reduce your training times if you are processing large volumes of data.</p>
			<p>On the other hand, <strong class="bold">Azure Data Lake Storage Gen 2</strong> builds on top of <strong class="bold">Azure Blob Storage</strong> by adding hierarchical<a id="_idIndexMarker281"/> namespaces. This feature allows data lakes to assign access permissions at a folder level. Large enterprises usually structure their data lakes with various zones where they store their data. Each zone has its own <strong class="bold">Access Control List</strong> (<strong class="bold">ACL</strong>), which gives permissions to specific groups of people. This means that you may be able to see the contents of one folder and not the contents of<a id="_idIndexMarker282"/> another, while in <strong class="bold">Azure Blob Storage</strong>, once you get access to a container, you can see all the data within it.</p>
			<p>If your data resides in a datastore that is not supported out of the box by Azure ML, you can copy the data over to an <strong class="bold">Azure Blob Storage</strong> or <strong class="bold">Azure Data Lake Storage Gen 2</strong> easily using the copy tool from <strong class="bold">Azure Data Factory</strong>. <strong class="bold">Azure Data Factory</strong> allows you to copy data from almost anywhere, even if it resides within on-premises databases, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="Images/B16777_04_039.jpg" alt="Figure 4.39 – Copying on-premises data to an Azure ML supported datastore using Azure Data Factory and the Self-Hosted Integration Runtime&#13;&#10;" width="1650" height="406"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.39 – Copying on-premises data to an Azure ML supported datastore using Azure Data Factory and the Self-Hosted Integration Runtime</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the <em class="italic">Attached compute</em> section, you saw that you can attach an <strong class="bold">Azure Data Factory</strong> (<strong class="bold">ADF</strong>) through the Azure ML SDK. Attaching ADF allows you to copy data between Azure ML supported datastores through code using <strong class="source-inline">DataTransferStep</strong>. Copying data from the on-premises network can be done in the same ADF, but you will have to author, execute, and monitor the data pulling pipeline from within ADF.</p>
			<p>In this section, you look at the <a id="_idIndexMarker283"/>types of datastores supported by Azure ML. In the next section, you will learn about the various authentication methods supported by those datastores.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>Datastore security considerations</h2>
			<p>Depending on the<a id="_idIndexMarker284"/> datastore, you will have to specify different type of credentials to register it in the Azure ML workspace. For the Azure Blob and Azure File Share datastores, you can use the following credentials:</p>
			<ul>
				<li><strong class="bold">Account key</strong>: This gives access to the entire Azure Storage Account.</li>
				<li><strong class="bold">Shared Access Signature</strong> (<strong class="bold">SAS</strong>) <strong class="bold">token</strong>: This is a more granular way to assign permissions to the various services of the storage account. Using the <strong class="bold">Account key</strong>, you can generate an SAS token that allows access to only a specific blob container and only for a limited amount of time.</li>
			</ul>
			<p>For Azure Data Lake Storage datastores, due to their advanced security features, you will need to provide an <strong class="bold">Azure Active Directory</strong> identity, which will be accessing the data. This is a <strong class="bold">service principal</strong> that is uniquely identified by the <strong class="bold">Azure Active Directory</strong> Tenant ID (referred to as <strong class="source-inline">tenant_id</strong>) where this entity is registered and has a unique ID (referred to as <strong class="source-inline">client_id</strong>). This identity has a password (referred to as <strong class="source-inline">client_secret</strong>) that enables your code to access the datastores impersonating that identity.</p>
			<p>For the relational database datastores, you will need to specify the database's name, the server's name, and the server port to connect to. For credentials, you can either provide a <strong class="bold">service principal</strong>, if the<a id="_idIndexMarker285"/> datastore supports it, or provide the necessary <strong class="bold">SQL authentication</strong> credentials, which consist of a database user ID and a password.</p>
			<p>Some of the datastores allow you to use the workspace's managed identity for data preview and profiling. This option adds the system assigned managed identity that has been assigned to the workspace as a Reader to the specific resource, allowing the workspace to load a preview of the data within the studio experience. This option is available on the datastore registration page, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="Images/B16777_04_040.jpg" alt="Figure 4.40 – Granting access to the workspace's managed identity&#13;&#10;" width="561" height="80"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.40 – Granting access to the workspace's managed identity</p>
			<p>So far, you have learned how to<a id="_idIndexMarker286"/> register various datastores in an Azure ML workspace. In the next section, you will learn how to use these registrations to define datasets.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>Working with datasets</h1>
			<p>In the previous sections, you <a id="_idIndexMarker287"/>were configuring compute and datastore resources under the <strong class="bold">Manage</strong> section of the studio. With this infrastructure configured, you can start pulling data into your registered datastores and register datasets in the <strong class="bold">Assets</strong> section of the studio:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="Images/B16777_04_041.jpg" alt="Figure 4.41 – Datasets in the Assets section of the Azure ML Studio experience&#13;&#10;" width="1650" height="1468"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.41 – Datasets in the Assets section of the Azure ML Studio experience</p>
			<p><strong class="bold">Datasets</strong> is an abstraction layer on top of the data that you are using for training and inference. It contains a reference to the physical data's location and provides a series of metadata that can help you understand their shape and statistical properties. When you want to access the dataset, you can reference it via its name, and you don't have to worry about credentials or exact file paths. Moreover, all the data scientists working on the same workspace can access the same datasets, allowing them to experiment on the same data in parallel.</p>
			<p>There are two types of datasets – file-based ones and tabular ones. File datasets reference a list of files in a datastore. For example, if you are building a computer vision model, you will need images that can be downloaded or mounted to your compute as a <strong class="source-inline">FileDataset</strong>. The Tabular dataset represents tabular data residing in either file-based datastores or relational database datastores. For example, you can reference a couple of folders containing <strong class="bold">Comma-Separated Value</strong> (<strong class="bold">CSV</strong>) files and you can read all the records as a pandas <strong class="bold">DataFrame</strong> through the <strong class="source-inline">TabularDataset</strong> construct, without having to parse the physical files.</p>
			<p>Another feature of datasets is that you can snapshot their properties and metadata using versions. Imagine that you have a folder structure that follows the <strong class="source-inline">weather/&lt;year&gt;/&lt;month&gt;/</strong> pattern. For example, you would find the weather measurements for January 2021 stored under <strong class="source-inline">weather/2021/01/measurements.parquet</strong>. As time flies, you will be getting more and more folders, each containing a single file under them. To reproduce your training results, you<a id="_idIndexMarker288"/> will want to reference the dataset that only contains files up to January 2021. This is exactly where dataset versioning comes in handy. While training a model, you register a version of the dataset that contains all the files you used for training. Later, you can refer to the dataset and request a specific version of it, which will give you a reference to all the files that used to be available back then.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Dataset versions do <em class="italic">not</em> copy the underlying data. They only store a reference to the actual files and the dataset metadata you will read about in the upcoming sections. This means that if you change the contents of a file instead of adding a new file, the dataset version will not load the same data.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Registering datasets</h2>
			<p>You can register datasets <a id="_idIndexMarker289"/>from various sources, as shown in the following screenshot, including from the datastore you learned how to register in the <em class="italic">Connecting to datastores</em> section: </p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="Images/B16777_04_042.jpg" alt="Figure 4.42 – Possible options for registering datasets&#13;&#10;" width="182" height="175"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.42 – Possible options for registering datasets</p>
			<p>To get a better understanding of how the dataset registration process works, we are going to register two tabular<a id="_idIndexMarker290"/> datasets that are hosted on the web. These datasets consist of a single <strong class="bold">parquet</strong> file each. We will use these two datasets later in this chapter to understand the data drift detection feature. Let's get started:</p>
			<ol>
				<li value="1">Select <strong class="bold">From web files</strong> from the menu shown in the preceding screenshot to start the <strong class="bold">Create dataset from web files</strong> wizard.</li>
				<li>On the first page of the wizard, provide the following information:<ul><li><strong class="bold">Web URL</strong>: <a href="https://bit.ly/survey-drift-base">https://bit.ly/survey-drift-base</a></li><li><strong class="bold">Name</strong>: <strong class="source-inline">survey-drift-base</strong></li><li><strong class="bold">Dataset type</strong>: <strong class="source-inline">Tabular</strong></li></ul></li>
				<li>Click <strong class="bold">Next</strong>:<div id="_idContainer097" class="IMG---Figure"><img src="Images/B16777_04_043.jpg" alt="Figure 4.43 – The first step of the dataset registration wizard&#13;&#10;" width="648" height="500"/></div><p class="figure-caption">Figure 4.43 – The first step of the dataset registration wizard</p></li>
				<li>The wizard will <a id="_idIndexMarker291"/>parse the file and figure out the file type and the schema of your dataset. You will need to validate the selection by clicking <strong class="bold">Next</strong>. Note that the wizard supports multiple file formats, as shown in the following screenshot:<div id="_idContainer098" class="IMG---Figure"><img src="Images/B16777_04_044.jpg" alt="Figure 4.44 – The second step of the dataset registration wizard&#13;&#10;" width="829" height="465"/></div><p class="figure-caption">Figure 4.44 – The second step of the dataset registration wizard</p></li>
				<li>In the next step, you <a id="_idIndexMarker292"/>can define advanced options regarding the schema. For the baseline dataset, leave the default options as-is. Click <strong class="bold">Next</strong>, which will lead you to the confirmation step. </li>
				<li>In this step, you can review your selections in the previous steps, and you can also schedule your first data science analysis task – profiling the dataset. This process generates the profile that you will explore in the next section. Enable the option and select <strong class="source-inline">gpu-cluster</strong>, which you provisioned in the previous section, as shown in the following screenshot: <p class="callout-heading">Important note</p><p class="callout">Within the <strong class="bold">Select compute for profiling</strong> option, you can select from both the compute instances and the compute clusters you provisioned in the <em class="italic">Compute instances</em> and <em class="italic">Compute clusters</em> sections. Selecting the compute cluster will force the cluster to scale from zero nodes to one node, analyze the dataset, and then scale down to zero nodes again. If you want, you can navigate to the <strong class="bold">Manage </strong>| <strong class="bold">Compute</strong> section and observe this scale out by clicking on the compute cluster's name. If you select the compute instance instead of the compute cluster, the job will be scheduled, and it will be executed when the compute instance starts.</p></li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="Images/B16777_04_045.jpg" alt="Figure 4.45 – Last step of the dataset registration process&#13;&#10;" width="1650" height="1216"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.45 – Last step of the dataset registration process</p>
			<p>You will need to register one<a id="_idIndexMarker293"/> more dataset. The process here is almost identical, only this time, you will mark the dataset as a time series one:</p>
			<ol>
				<li value="1">Click on <strong class="bold">Create dataset</strong> and select <strong class="bold">From web files</strong>, as shown in the following screenshot:<div id="_idContainer100" class="IMG---Figure"><img src="Images/B16777_04_046.jpg" alt="Figure 4.46 – Create dataset menu in the dataset list&#13;&#10;" width="536" height="401"/></div><p class="figure-caption">Figure 4.46 – Create dataset menu in the dataset list</p></li>
				<li>Follow the same <a id="_idIndexMarker294"/>steps as you did previously and input the following information:<ul><li><strong class="bold">Web URL</strong>: <a href="https://bit.ly/survey-drift-target">https://bit.ly/survey-drift-target</a></li><li><strong class="bold">Name</strong>: <strong class="source-inline">survey-drift-target</strong> </li></ul></li>
				<li>During the schema step, make sure that you select <strong class="bold">Timestamp</strong> from the <strong class="bold">Properties</strong> section of the <strong class="bold">inference_date</strong> column, as shown in the following screenshot. This option flags this tabular dataset as a <strong class="bold">time series</strong> dataset, something that allows you to perform additional analysis, as you will see in the <em class="italic">Data drift detection</em> section:<div id="_idContainer101" class="IMG---Figure"><img src="Images/B16777_04_047.jpg" alt="Figure 4.47 – Configuring a tabular dataset so that it becomes a time series dataset&#13;&#10;" width="1209" height="417"/></div><p class="figure-caption">Figure 4.47 – Configuring a tabular dataset so that it becomes a time series dataset</p></li>
				<li>Schedule data profile analysis and complete the dataset registration process. <p class="callout-heading">Important note</p><p class="callout">If you are following along, you may notice that for the <strong class="bold">inference_date</strong> column, you can specify either <strong class="bold">Timestamp</strong> of <strong class="bold">Partition timestamp</strong>. To mark your dataset as a <strong class="bold">Time series</strong> one, you will need to specify the <strong class="bold">Timestamp</strong> property in at least one <strong class="bold">Date</strong> column. In addition to that, if your data has been partitioned into a folder structure with time information, such as <strong class="source-inline">year=2021/month=05/day=01/data.parquet</strong>, you can create a virtual column through that path<a id="_idIndexMarker295"/> pattern and define that as your <strong class="bold">Partition timestamp</strong>. This improves the importance of time series functionality and allows you to load specific dates by selectively reading the required files only.</p></li>
			</ol>
			<p>You should be able to see two registered datasets, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="Images/B16777_04_048.jpg" alt="Figure 4.48 – List of registered datasets in the Azure ML workspace&#13;&#10;" width="754" height="270"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.48 – List of registered datasets in the Azure ML workspace</p>
			<p>From this view, you can select a<a id="_idIndexMarker296"/> dataset and then click on the <strong class="bold">Unregister</strong> button to remove the registration. Upon clicking on a dataset, you can view more details about it, including the profile analysis you performed on top of the datasets, something you will see in the next section.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Exploring the dataset</h2>
			<p>In the dataset list, click <a id="_idIndexMarker297"/>on the <strong class="bold">survey-drift-target</strong> dataset to open its details. In the first tab, <strong class="bold">Details</strong>, you can modify the description of the dataset and specify tags that are associated with the dataset. Tags are name-value pairs. In the following screenshot, you can see that we specified <strong class="bold">survey</strong> as the value of the <strong class="bold">experiment</strong> tag:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="Images/B16777_04_049.jpg" alt="Figure 4.49 – Dataset details showing all the metadata associated with the specific dataset&#13;&#10;" width="1601" height="1144"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.49 – Dataset details showing all the metadata associated with the specific dataset</p>
			<p>In the <strong class="bold">Consume</strong> tab, you<a id="_idIndexMarker298"/> can copy the Python SDK code that you are going to use in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The Azure ML Python SDK</em>, to get access to the dataset:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="Images/B16777_04_050.jpg" alt="Figure 4.50 – Consuming a snippet that gives access to the dataset&#13;&#10;" width="1144" height="757"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.50 – Consuming a snippet that gives access to the dataset</p>
			<p>In the <strong class="bold">Explore</strong> tab, you will be<a id="_idIndexMarker299"/> able to preview a sample of the data that is included in the dataset, exactly as you saw during the registration process:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="Images/B16777_04_051.jpg" alt="Figure 4.51 – Previewing a sample of the dataset&#13;&#10;" width="559" height="277"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.51 – Previewing a sample of the dataset</p>
			<p>If you click on the <strong class="bold">Profile</strong> tab, you <a id="_idIndexMarker300"/>will be able to see the statistical analysis of the dataset, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="Images/B16777_04_052.jpg" alt="Figure 4.52 – Statistical analysis of the dataset&#13;&#10;" width="1650" height="520"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.52 – Statistical analysis of the dataset</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If your dataset contains fewer than 10,000 rows, profiling is done automatically for you, without you having to schedule the processing aspect for the dataset. If the dataset contains more than 10,000 rows, then Azure ML performs an analysis on the first 10,000 rows and shows a warning message that prompts you to schedule a complete profiling analysis, something you can do by clicking on the <strong class="bold">Generate profile</strong> button from the menu.</p>
			<p>Finally, on the <strong class="bold">Models</strong> tab, you<a id="_idIndexMarker301"/> can see the models that relate to this dataset, something that you will do in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting Machines do the Model Training</em>, when you will be registering the best model that you will be deploying as a web service.</p>
			<p>Having registered a dataset, you can configure periodic monitoring for the dataset for data drifting, something you will learn about in the next section.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Data drift detection</h2>
			<p>Data drift detection is a<a id="_idIndexMarker302"/> technique that allows you to compare a time series dataset with a reference dataset, and then check whether the statistical properties of the<a id="_idIndexMarker303"/> features you are comparing have changed significantly. For example, let's assume that you trained an ML model that predicts if someone is going to participate in a survey based on their age. You used the <strong class="source-inline">survey-drift-base</strong> dataset to train that model. The following graph shows a density curve, which shows the distribution of age in the training dataset:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="Images/B16777_04_053.jpg" alt="Figure 4.53 – Negative skewed unimodal distribution of the age feature in the training dataset&#13;&#10;" width="1324" height="978"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.53 – Negative skewed unimodal distribution of the age feature in the training dataset</p>
			<p>When you <a id="_idIndexMarker304"/>operationalized the model, you kept track of the inferences that it<a id="_idIndexMarker305"/> made on a weekly basis, and you logged this information in the <strong class="source-inline">survey-drift-target</strong> dataset, which you registered previously. This dataset contains the inferences that you did during the first 2 weeks of 2020. Data drift detection enables you to detect if the distribution of the input features changed over time. Let's take a look:</p>
			<ol>
				<li value="1">Navigate to <strong class="bold">Assets </strong>| <strong class="bold">Datasets</strong> | <strong class="bold">Dataset monitors</strong> and click on the <strong class="bold">Create</strong> button to start the dataset monitor wizard:<div id="_idContainer108" class="IMG---Figure"><img src="Images/B16777_04_054.jpg" alt="Figure 4.54 – Creating a new dataset monitor&#13;&#10;" width="793" height="753"/></div><p class="figure-caption">Figure 4.54 – Creating a new dataset monitor</p></li>
				<li>On the target <a id="_idIndexMarker306"/>dataset, you will see all the registered time series <a id="_idIndexMarker307"/>datasets you want to monitor for data drift. This is the inference that your model has been doing in production. Select <strong class="source-inline">survey-drift-target (Version:1)</strong> and click <strong class="bold">Next</strong>:<div id="_idContainer109" class="IMG---Figure"><img src="Images/B16777_04_055.jpg" alt="Figure 4.55 – The first step in data drift monitor configuration&#13;&#10;" width="668" height="297"/></div><p class="figure-caption">Figure 4.55 – The first step in data drift monitor configuration</p></li>
				<li>On the next <a id="_idIndexMarker308"/>page, you need to select your reference point. This<a id="_idIndexMarker309"/> can either be a specific point in time from within the time series tabular dataset or a specific dataset. In your case, select the <strong class="source-inline">survey-drift-base (Version:1)</strong> dataset, which is the dataset that was used to train the ML model:<div id="_idContainer110" class="IMG---Figure"><img src="Images/B16777_04_056.jpg" alt="Figure 4.56 – Selecting the baseline dataset during the data drift monitor configuration&#13;&#10;" width="668" height="326"/></div><p class="figure-caption">Figure 4.56 – Selecting the baseline dataset during the data drift monitor configuration</p></li>
				<li>In the last step of the wizard, you need to define the following information:<ul><li><strong class="bold">Name</strong>: The name of the monitoring process you are about to configure. Name the process <strong class="source-inline">survey-drift-monitor</strong>.</li><li><strong class="bold">Features</strong>: Select one <a id="_idIndexMarker310"/>or more common features between the two datasets to monitor their distributions and whether there is data drift. In this case, the only common feature between the two datasets is the age feature.</li><li><strong class="bold">Compute target</strong>: The cluster that will be spinning up and down to perform the analysis.</li><li><strong class="bold">Frequency</strong>: The frequency<a id="_idIndexMarker311"/> specifies the time interval for the target data to be examined for drift. This property cannot be changed once the monitor has been created. You can choose between day, week, or month. Keep in mind that you need at fewer 50 samples per time interval to perform data drift analysis. This means that if you have less than 50 rows per day, you cannot use that as your frequency and you should opt for week, or even month, instead.</li><li><strong class="bold">Latency</strong>: It is common to have a delay between the actual scoring of a row and refreshing the target dataset. In this field, you specify how long to wait before assuming that the target dataset got the latest records; then, the monitor can perform data drift analysis. </li><li><strong class="bold">Email address</strong>: This is where to send an email if the dataset has drifted more than what's been specified for the <strong class="bold">Threshold</strong> parameter.</li></ul></li>
				<li>For the purposes of this book, you can disable the schedule, as shown in the following screenshot. You will manually run the data drift analysis.</li>
				<li>Click on the <strong class="bold">Create</strong> button to create the monitor: <div id="_idContainer111" class="IMG---Figure"><img src="Images/B16777_04_057.jpg" alt="Figure 4.57 – Data drift monitor settings&#13;&#10;" width="865" height="772"/></div><p class="figure-caption">Figure 4.57 – Data drift monitor settings</p></li>
				<li>Click on the name <a id="_idIndexMarker312"/>of the new monitor you created from<a id="_idIndexMarker313"/> the monitor list:</li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="Images/B16777_04_058.jpg" alt="Figure 4.58 – Data drift monitors list&#13;&#10;" width="659" height="193"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.58 – Data drift monitors list</p>
			<p>The data drift monitor is meant to run on a schedule for new data. In your case, you want to analyze the existing data in the target dataset. Let's take a look:</p>
			<ol>
				<li value="1">Click on the <strong class="bold">Analyze existing data</strong> button, which will bring up the backfill wizard shown in the following screenshot: <div id="_idContainer113" class="IMG---Figure"><img src="Images/B16777_04_059.jpg" alt="Figure 4.59 – Manually starting an analysis of past dates&#13;&#10;" width="907" height="562"/></div><p class="figure-caption">Figure 4.59 – Manually starting an analysis of past dates</p></li>
				<li>Select from December 31, 2019 to January 15, 2020. This is the time range that contains all the records from the target dataset. </li>
				<li>Select the compute cluster that will do the analysis.</li>
				<li>Click <strong class="bold">Submit</strong>. </li>
			</ol>
			<p>Once the analysis is complete, a<a id="_idIndexMarker314"/> process that will take some time, you<a id="_idIndexMarker315"/> will be able to see the data drift results, which indicate that a big data drift has been observed in our dataset. Note that the summary is referring to the latest inferences, which were done on January 5, 2020. You can manually select previous periods by clicking on the graphs for the corresponding dates:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="Images/B16777_04_060.jpg" alt="Figure 4.60 – Data drift detected between the base dataset and the target one&#13;&#10;" width="1074" height="350"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.60 – Data drift detected between the base dataset and the target one</p>
			<p>If you scroll down to the feature distribution, you will be able to clearly see the distribution drift on the age feature. This indicates that the model is making inferences on a population that has different characteristics from the one it was trained on. This is a good indication that you may need to retrain the model, to bring it up to date with the new feature distribution:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="Images/B16777_04_061.jpg" alt="Figure 4.61 – The baseline is a negative skewed distribution, while the latest inferences follow a positive skewed distribution&#13;&#10;" width="1127" height="502"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.61 – The baseline is a negative skewed distribution, while the latest inferences follow a positive skewed distribution</p>
			<p>In this section, you<a id="_idIndexMarker316"/> learned how to configure data drift detection, which <a id="_idIndexMarker317"/>you did by comparing the data that your model was observing in production against the dataset that was used to train the model. This is a powerful feature that allows you to determine whether you need to retrain the model with newer data, especially if the feature distribution has changed/drifted over time.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Summary</h1>
			<p>In this chapter, you learned how to provision and attach compute resources to your Azure ML workspace. You also learned how you can register various datastores so that you can access data in a secure manner. Finally, you explored the dataset registration capabilities of Azure ML Studio, something that allows you to easily access the data for your experiments. Having registered the datasets, you can configure data drift monitors, which warn you if the features' distribution changes over time, something that could indicate that the ML model that was trained on that dataset needs to be retrained. You should now feel comfortable configuring your Azure ML workspace, one of the key skills that's measured in the DP-100 certification.</p>
			<p>In the next chapter, you will learn how to leverage the datasets that you registered in the workspace to perform <strong class="bold">Auto ML</strong> analysis, a process that will run multiple ML experiments on top of the compute clusters you provisioned to detect the best algorithm for your dataset.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Questions</h1>
			<p>In each chapter, you will find a couple of questions so that you can test your knowledge regarding what was covered in this chapter:</p>
			<ol>
				<li value="1">How many data scientists can work on a single compute instance that has 8 cores and 56 GB of RAM?<p>a. Only one.</p><p>b. Up to two.</p><p>c. Up to five.</p><p>d. As many as they want, as long as they don't deplete the compute resources.</p></li>
				<li>What type of credentials do you need to provide to access a data lake store that's either Gen 1 or Gen 2?<p>a. A <strong class="bold">Personal Access Token</strong> (<strong class="bold">PAT</strong>)</p><p>b. A service principal's client ID and secret</p><p>c. Your own AAD user credentials</p><p>d. No credentials are needed</p></li>
				<li>Which of the following Azure tools can help you orchestrate data moving from an on-premises environment?<p>a. Blob storage</p><p>b. Azure Active Directory</p><p>c. Azure Data Factory</p><p>d. Azure ML workspace</p></li>
			</ol>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/>Further reading</h1>
			<p>This section offers a list of useful web resources that will help you augment your knowledge and understanding of the topics discussed in this chapter:</p>
			<ul>
				<li>You can learn more about how to use managed identity from within a compute cluster at the following link: <a href="https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python#managed-identity-usage">https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python#managed-identity-usage</a>.</li>
				<li>The instance metadata service allows you to request tokens for Azure resources using the attached managed identity. You can learn more about this at<a href="https://docs.microsoft.com/azure/virtual-machines/linux/instance-metadata-service"> https://docs.microsoft.com/azure/virtual-machines/linux/instance-metadata-service</a>.</li>
				<li>You can learn more about the access control model of Azure Data Lake Storage Gen2 at <a href="https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-access-control-model">https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-access-control-model</a>.</li>
				<li>You can learn how to easily copy data and configure regular data ingestions using Azure Data Factory's copy data tool at <a href="https://docs.microsoft.com/azure/data-factory/quickstart-create-data-factory-copy-data-tool">https://docs.microsoft.com/azure/data-factory/quickstart-create-data-factory-copy-data-tool</a>.</li>
				<li>You can learn how to grant limited access to Azure Storage Accounts using SAS tokens at <a href="https://docs.microsoft.com/azure/storage/common/storage-sas-overview">https://docs.microsoft.com/azure/storage/common/storage-sas-overview</a>.</li>
				<li>You can learn more about service principals, which can be used to access Azure Data Lake datastores, at <a href="https://docs.microsoft.com/azure/active-directory/develop/app-objects-and-service-principals">https://docs.microsoft.com/azure/active-directory/develop/app-objects-and-service-principals</a>.</li>
			</ul>
		</div>
	</div></body></html>