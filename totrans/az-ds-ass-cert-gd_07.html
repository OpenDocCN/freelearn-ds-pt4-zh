<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer146">
			<h1 id="_idParaDest-72"><em class="italic"><a id="_idTextAnchor072"/>Chapter 5</em>: Letting the Machines Do the Model Training</h1>
			<p>In this chapter, you will create your first <strong class="bold">Automated Machine Learning</strong> (<strong class="bold">Automated ML</strong> or <strong class="bold">AutoML</strong>) experiment. AutoML refers to the process of trying multiple modeling <a id="_idIndexMarker318"/>techniques and selecting the model that produces the best predictions against the training dataset you specify. First, you will navigate through the AutoML wizard that is part of the Azure Machine Learning Studio web experience and understand the different options that need to be configured. You will then learn how to monitor the progress of an AutoML experiment and how to deploy the best-produced model as a web service hosted in an <strong class="bold">Azure Container Instance</strong> (<strong class="bold">ACI</strong>) to be able to make real-time inferences.</p>
			<p>The best way to go through this chapter is by sitting in front of a computer with this book by you. By using your Azure subscription and this book together, you can start your journey through AutoML.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Configuring an AutoML experiment</li>
				<li>Monitoring execution of an experiment</li>
				<li>Deploying best model as a web service</li>
			</ul>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control</strong> (<strong class="bold">IAM</strong>) role at the resource group level. Within that resource group, you should then deploy a <strong class="bold">machine learning</strong> resource named <strong class="source-inline">packt-learning-mlw</strong>, as described in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Configuring an AutoML experiment</h1>
			<p>If you were asked to <a id="_idIndexMarker319"/>train a model to make predictions against a dataset, you would need to do a couple of things, including normalizing the dataset, splitting it into train and validation data, running multiple experiments to understand which algorithm is performing best against the dataset, and then finetuning the best model. Automated machine learning shortens this process by fully automating the time-consuming, iterative tasks. It allows all users, from normal PC users to experienced data scientists, to build multiple machine learning models against a target dataset and select the model that performs the best, based on a metric you select. </p>
			<p>This process consists of the following steps:</p>
			<ol>
				<li><strong class="bold">Preparing the experiment</strong>: Select the dataset you are going to use for training, select the <a id="_idIndexMarker320"/>column that you are trying to predict, and configure the experiment's parameters. This is the configuration phase you will read about in this section.</li>
				<li><strong class="bold">Data guardrails</strong>: This is <a id="_idIndexMarker321"/>the first step of executing the experiment. It performs basic data guardrails on top of the provided training dataset. AutoML tries to identify potential issues with your data; for example, all the training data must have the same values in the column you are trying to predict.</li>
				<li><strong class="bold">Training multiple models</strong>: Train multiple combinations of data normalization and <a id="_idIndexMarker322"/>algorithms to find the best model that optimizes (maximizes or minimizes) the desired metric. This process continues until one of the exit criteria is met, either a time constraint or a specified model performance target.</li>
				<li><strong class="bold">Creating an ensemble model</strong>: Here, you <a id="_idIndexMarker323"/>train a model that combines the results of the best models trained so far and produces a potentially improved inference.</li>
				<li><strong class="bold">Selecting the best model</strong>: The<a id="_idIndexMarker324"/> best model is selected based on the metric you specified.</li>
			</ol>
			<p>Azure Machine Learning provides a web-based wizard that allows you to configure such an experiment. In <a href="B16777_03_Final_VK_ePub.xhtml#_idTextAnchor045"><em class="italic">Chapter 3</em></a>, <em class="italic">Azure Machine Learning Studio Components</em>, you explored<a id="_idIndexMarker325"/> the <strong class="bold">Azure Machine Learning Studio</strong> \ web experience. </p>
			<p>In this chapter, you will create an AutoML classification model that will predict whether a customer will churn or not. This model will be able to predict whether a customer will continue being a loyal customer or whether they will terminate their active mobile phone contract. You will use a fabricated dataset from a fictional telecom company. The dataset shows, for each customer, information about how long they have been with the company and how much they are using their active subscription. Let's get started:</p>
			<ol>
				<li value="1">To <a id="_idIndexMarker326"/>start the AutoML experiment, you will need to open a browser and navigate to the Azure Machine Learning Studio. You will land on the home page, as shown in the following screenshot:<div id="_idContainer118" class="IMG---Figure"><img src="Images/B16777_05_001.jpg" alt="Figure 5.1 – Azure Machine Learning Studio home screen&#13;&#10;" width="1589" height="1013"/></div><p class="figure-caption">Figure 5.1 – Azure Machine Learning Studio home screen</p></li>
				<li>On the Azure Machine Learning Studio home screen, navigate to the <strong class="bold">Author</strong> | <strong class="bold">Automated ML</strong> section by clicking the <strong class="bold">Start now</strong> button under <strong class="bold">Automated ML</strong>, as highlighted in the preceding screenshot. This will open the <strong class="bold">Automated ML</strong> home screen, as shown here:<div id="_idContainer119" class="IMG---Figure"><img src="Images/B16777_05_002.jpg" alt="Figure 5.2 – The Automated ML home screen&#13;&#10;" width="1576" height="1013"/></div><p class="figure-caption">Figure 5.2 – The Automated ML home screen</p><p>On this home <a id="_idIndexMarker327"/>screen, you can find the recently executed Automated ML experiments. Since this is the first time you are using this workspace, you shouldn't find any runs listed here.</p></li>
				<li>By pressing the <strong class="bold">New Automated ML Run</strong> button, you will start the <strong class="bold">Create a New Automated ML</strong> wizard, as shown here:</li>
			</ol>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="Images/B16777_05_003.jpg" alt="Figure 5.3 – Starting the Automated ML wizard&#13;&#10;" width="1288" height="617"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Starting the Automated ML wizard</p>
			<p>In this first <a id="_idIndexMarker328"/>step of the wizard, named <strong class="bold">Select dataset</strong>, you can either select an existing dataset or create a new one. From this list, you will be able to see the two datasets that you registered in <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>. You will not need those datasets. In the next section, you will learn how to create a new dataset to use for the Automated ML experiment you are about to perform.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Registering the dataset</h2>
			<p>In <a id="_idIndexMarker329"/>the <strong class="bold">Select dataset</strong> step of the <strong class="bold">Automated ML</strong> wizard, you can register a new dataset to be used for the AutoML experimentation. Follow these steps to register the fabricated churn dataset:</p>
			<ol>
				<li value="1">Click on <strong class="bold">Create dataset</strong> on the top of the screen.</li>
				<li>Select <strong class="bold">From web files</strong> from the drop-down menu that appears. This will start the <strong class="bold">Create dataset from web files</strong> wizard shown in the following screenshot.</li>
				<li>On the first page of the wizard (<strong class="bold">Basic info</strong>), provide the following information:<p>a) <strong class="bold">Web URL</strong>: <a href="https://bit.ly/churn-dataset">https://bit.ly/churn-dataset</a>.</p><p>b) <strong class="bold">Name</strong>: <strong class="source-inline">churn-dataset</strong>.</p><p>c) <strong class="bold">Dataset type</strong>: <strong class="source-inline">Tabular</strong>. This is <a id="_idIndexMarker330"/>an option that you cannot change since AutoML currently only supports tabular datasets.</p><p>d) <strong class="bold">Description</strong>: <strong class="source-inline">Dataset to train a model that predicts customer churn</strong>:</p><div id="_idContainer121" class="IMG---Figure"><img src="Images/B16777_05_004.jpg" alt="Figure 5.4 – Basic info when creating a dataset from web files&#13;&#10;" width="814" height="502"/></div><p class="figure-caption">Figure 5.4 – Basic info when creating a dataset from web files</p></li>
				<li>Once you've filled everything in, press <strong class="bold">Next</strong>. It will take a while to download and parse the file. The wizard will move to the <strong class="bold">Settings and preview</strong> screen:<div id="_idContainer122" class="IMG---Figure"><img src="Images/B16777_05_005.jpg" alt="Figure 5.5 – The Settings and preview screen of the Create dataset from web files wizard&#13;&#10;" width="1242" height="745"/></div><p class="figure-caption">Figure 5.5 – The Settings and preview screen of the Create dataset from web files wizard</p></li>
				<li>The <a id="_idIndexMarker331"/>steps shown in the preceding screenshot provide important information for the demo dataset. The file format of the sample file is automatically detected to be the <strong class="bold">Parquet</strong> file format. You<a id="_idIndexMarker332"/> can modify the selection if needed. <p>The demo dataset consists of seven columns:</p><ul><li><strong class="bold">ld</strong> is a<a id="_idIndexMarker333"/> sequential record number that is not part of the original Parquet file. Auto ML generates this sequence number to let you validate the data you can see in the preview window. This column will not be part of the registered dataset.</li><li><strong class="bold">id</strong> is a <a id="_idIndexMarker334"/>string that uniquely identifies each customer in the dataset.</li><li><strong class="bold">customer_tenure</strong> is <a id="_idIndexMarker335"/>an integer value telling us how long each customer has been with the fictional telecom company. The value represents months.</li><li><strong class="bold">product_tenure</strong> is <a id="_idIndexMarker336"/>an integer value that tells us how long a customer has owned the currently active subscription. It is measured in months.</li><li><strong class="bold">activity_last_6_month</strong> tells <a id="_idIndexMarker337"/>us how many hours the customer has talked on the phone over the last 6 months.</li><li><strong class="bold">activity_last_12_month</strong> tells us<a id="_idIndexMarker338"/> how many hours the customer has talked on the phone over the last 12 months.</li><li><strong class="bold">churned</strong> is a flag<a id="_idIndexMarker339"/> that informs us whether the customer renewed the subscription or terminated the active contract. This is the column you will be trying to predict.<p>From this fabricated dataset, the most relevant features for the classification model you are trying to build are <strong class="bold">customer_tenure</strong>, <strong class="bold">product_tenure</strong>, <strong class="bold">activity_last_6_month</strong>, and <strong class="bold">activity_last_12_month</strong>. The <strong class="bold">churned</strong> column is the <strong class="bold">target</strong> for the model you are trying to build. The <strong class="bold">id</strong> column allows you to link a model's prediction back to the actual customer who may churn.</p></li></ul></li>
				<li>Pressing <strong class="bold">Next</strong> will open the wizard's <strong class="bold">Schema</strong> screen, as shown here: <div id="_idContainer123" class="IMG---Figure"><img src="Images/B16777_05_006.jpg" alt="Figure 5.6 – Schema step of the Create dataset from web files wizard&#13;&#10;" width="1650" height="737"/></div><p class="figure-caption">Figure 5.6 – Schema step of the Create dataset from web files wizard</p><p>This screen <a id="_idIndexMarker340"/>provides a detailed view of the schema of the dataset. On this wizard screen, you can exclude columns by switching the <strong class="bold">Include</strong> toggle for the specific column you would like to exclude.</p><p>The <strong class="bold">Type</strong> column allows you to specify the data types of the columns of the dataset. The supported types <a id="_idIndexMarker341"/>that are available are <strong class="bold">String</strong>, <strong class="bold">Boolean</strong>, <strong class="bold">Integer</strong>, <strong class="bold">Decimal</strong>, and <strong class="bold">Date</strong>. The dataset you are registering is <a id="_idIndexMarker342"/>stored in <strong class="bold">Parquet</strong> file format, as you saw back in <em class="italic">Step 4</em>. This<a id="_idIndexMarker343"/> file <a id="_idIndexMarker344"/>format stores additional <a id="_idIndexMarker345"/>metadata information regarding the type of each column. This information is read by the wizard and the right data types are selected for you. If you were using a file format that doesn't include the <a id="_idIndexMarker346"/>data types, such as <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>) files, the wizard would try to guess the types, but you would have to validate the selections.</p><p>The <strong class="bold">Properties</strong> column is enabled when you can specify additional details for the <strong class="bold">Type</strong> of data that a column stores. For example, if you select <strong class="source-inline">Date</strong> as the type of a column, you will be able to select whether this column is a timestamp, something that <a id="_idIndexMarker347"/>will mark the dataset as a time series one. <strong class="bold">Date format</strong> is also enabled when you select <strong class="source-inline">Date</strong> as the column's type. This allows you to define the date pattern that should be used to parse the specific column. For example, you can use <strong class="source-inline">%Y-%m-%d</strong> to specify that the date is stored in a year-month-day format. </p></li>
				<li>Once you've finished exploring the wizard's <strong class="bold">Schema</strong> screen, press <strong class="bold">Next</strong>. The wizard's <strong class="bold">Confirm details</strong> screen provides you with an overview of the new dataset you want to register in your workspace, as shown here: <div id="_idContainer124" class="IMG---Figure"><img src="Images/B16777_05_007.jpg" alt="Figure 5.7 – The Confirm details page &#13;&#10;" width="1430" height="864"/></div><p class="figure-caption">Figure 5.7 – The Confirm details page </p><p>This screen summarizes the dataset information you read about in the <em class="italic">Working with datasets</em> section of <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>. If you select the <strong class="bold">Profile this dataset after creation</strong> checkbox, you can select a compute target to generate the profile of the <a id="_idIndexMarker348"/>newly created dataset. You do not need to generate the profile for this dataset since it only has 6,720 rows and Azure ML will automatically provide a full profile for you.</p></li>
				<li>Click the <strong class="bold">Create</strong> button to complete the wizard.</li>
			</ol>
			<p>Now that you have registered <strong class="source-inline">churn-dataset</strong>, you can continue with the AutoML wizard, where you will select the newly registered dataset, configure the experiment parameters, and kick off the AutoML process, something you will do in the next section.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Returning to the AutoML wizard</h2>
			<p>Now that you have <a id="_idIndexMarker349"/>created <strong class="source-inline">churn-dataset</strong>, you can continue with the AutoML wizard. The wizard contains three steps, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="Images/B16777_05_008.jpg" alt="Figure 5.8 – AutoML wizard steps &#13;&#10;" width="1074" height="310"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – AutoML wizard steps </p>
			<p>Let's begin with the steps!</p>
			<ol>
				<li value="1">As the first step, select <strong class="source-inline">churn-dataset</strong>, which you created in the previous section, <em class="italic">Registering the dataset</em>, and click <strong class="bold">Next</strong>, as shown here:<div id="_idContainer126" class="IMG---Figure"><img src="Images/B16777_05_009.jpg" alt="Figure 5.9 – The Select dataset step of the Create a new Automated ML run wizard&#13;&#10;" width="1452" height="747"/></div><p class="figure-caption">Figure 5.9 – The Select dataset step of the Create a new Automated ML run wizard</p></li>
				<li>In the <strong class="bold">Configure run</strong> step, you can visualize the selected dataset and configure the basic <a id="_idIndexMarker350"/>experiment parameters:<div id="_idContainer127" class="IMG---Figure"><img src="Images/B16777_05_010.jpg" alt="Figure 5.10 – The Configure run step of the Create a new Automated ML run wizard&#13;&#10;" width="1323" height="924"/></div><p class="figure-caption">Figure 5.10 – The Configure run step of the Create a new Automated ML run wizard</p><p>The <a id="_idIndexMarker351"/>experiment options you need to configure <a id="_idIndexMarker352"/>are as follows:</p><ul><li><strong class="bold">Experiment name</strong>: Every <a id="_idIndexMarker353"/>time you perform a data science experiment, you need to keep track of the various runs you have done to be able to compare the results. For example, you may end up running multiple AutoML attempts while trying out different wizard options. In this case, you would like to keep all the attempts under the same experiment to keep them grouped. From this wizard, you can <strong class="bold">Select existing</strong> experiment, or you can <strong class="bold">Create new</strong>. Create a new experiment and specify <strong class="source-inline">churn-automl-experiment</strong> for <strong class="bold">New experiment name</strong>, as shown in the preceding screenshot.</li><li><strong class="bold">Target column</strong>: This is the <a id="_idIndexMarker354"/>dataset column<a id="_idIndexMarker355"/> that the model will be trained to predict. Select the <strong class="source-inline">churned</strong> column. Based on the type of the target column, the wizard will automatically select the best task for this column, as you will see in the next wizard step.</li><li><strong class="bold">Select compute cluster</strong>: You will need to select the compute cluster that will be used to <a id="_idIndexMarker356"/>perform the AutoML runs. You can select <strong class="source-inline">gpu-cluster</strong> here, which you created in the <em class="italic">Compute clusters</em> section of <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>. If you don't have a cluster registered in your workspace, you can use a compute instance or you can start the dedicated wizard by clicking on the <strong class="bold">New</strong> button shown in the preceding screenshot.<p class="callout-heading">Important note</p><p class="callout">In <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>, you were asked to provision a GPU-based cluster named <strong class="source-inline">gpu-cluster</strong>. By default, free trial subscriptions do not allow you to provision GPU computes. For this experiment, you can select CPU-based clusters instead of GPU-based ones.</p></li></ul></li>
				<li>Once you have configured the run details, click on the <strong class="bold">Next</strong> button.</li>
				<li>The next page of the wizard is the <strong class="bold">Select task and settings</strong> page, as shown here:<div id="_idContainer128" class="IMG---Figure"><img src="Images/B16777_05_011.jpg" alt="Figure 5.11 – The Create a new Automated ML run wizard – Select task type&#13;&#10;" width="1547" height="941"/></div><p class="figure-caption">Figure 5.11 – The Create a new Automated ML run wizard – Select task type</p><p>In this step, you can configure the following:</p><p>a) <strong class="bold">Select task type</strong>: AutoML <a id="_idIndexMarker357"/>currently supports three types of tasks:</p><ul><li><strong class="bold">Classification</strong>: The <a id="_idIndexMarker358"/>produced model can predict the category a record belongs to. A category can be anything, such as a yes or<a id="_idIndexMarker359"/> no <strong class="bold">Boolean</strong> value or a blue, green, or yellow color value. In our case, you are trying to predict if the customer is going to churn, which can be answered with a yes or a no.</li><li><strong class="bold">Regression</strong>: Use <a id="_idIndexMarker360"/>regression models when you want to predict a numeric value, such as the price of an apartment or the diabetes disease level. We will look at this in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>.</li><li><strong class="bold">Time series forecasting</strong>: You <a id="_idIndexMarker361"/>usually use this type of model to predict time series values such as a stock's price while considering the progress of the value over time. This type of model is a specialization on top of the regression models. This means that all regression models can be used, but there are also a <a id="_idIndexMarker362"/>couple of more specialized algorithms <a id="_idIndexMarker363"/>such as Facebook's <strong class="bold">Prophet</strong> algorithm<a id="_idIndexMarker364"/> or the <strong class="bold">Auto-Regressive Integrated Moving Average</strong> (<strong class="bold">ARIMA</strong>) technique.</li></ul><p>Depending on the target column you selected, the wizard will automatically guess the task you are <a id="_idIndexMarker365"/>trying to perform. In our case, it selected <strong class="bold">Classification</strong>.</p><p>b) <strong class="bold">View additional configuration settings</strong>: Depending on the task type you selected, you <a id="_idIndexMarker366"/>can configure various settings, including the target metric to be used to evaluate the models and the exit criteria that will terminate the search for the best model. Depending on the task's type, the options on that wizard page change. You can see some of these options in the following diagram. You will visit this part of the wizard in <em class="italic">Step 5</em>:</p><div id="_idContainer129" class="IMG---Figure"><img src="Images/B16777_05_012.jpg" alt="Figure 5.12 – Configuration settings depending on the selected task type.&#13;&#10;" width="1650" height="652"/></div><p class="figure-caption">Figure 5.12 – Configuration settings depending on the selected task type.</p><p>c) <strong class="bold">View featurization settings</strong>: This allows you to configure various operations regarding<a id="_idIndexMarker367"/> the features that the model will use to make its prediction. In this section, you can exclude features such as row unique ID or irrelevant information to speed up the training process and reduce the size of the final model. You can also specify the type of each feature and the imputation function, which takes care of the missing values in the dataset. You will visit this section of the wizard in <em class="italic">Step 6</em>.</p></li>
				<li>By clicking on <strong class="bold">View additional configuration settings</strong>, additional configurations will appear for the classification task that you selected on the main wizard page: <div id="_idContainer130" class="IMG---Figure"><img src="Images/B16777_05_013.jpg" alt="Figure 5.13 – Additional configurations for the classification task&#13;&#10;" width="1222" height="1197"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 5.13 – Additional configurations for the classification task</p><p>In<a id="_idIndexMarker368"/> the <strong class="bold">Additional configurations</strong> section, you can set up the following parameters, which are common for all types of tasks:</p><ul><li><strong class="bold">Primary metric</strong>: This<a id="_idIndexMarker369"/> is the metric that's used to monitor the training progress of the model and the metric that's used to compare the produced models so that you can select the best one. <strong class="bold">Classification</strong> tasks have different metrics from <strong class="bold">Regression</strong> and <strong class="bold">Time series forecasting</strong> ones, as shown in <em class="italic">Figure 5.12</em>.</li><li><strong class="bold">Explain best model</strong>: If <a id="_idIndexMarker370"/>checked, the best model gets explained and the results are stored within the run. You will learn more about model interpretability in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. </li><li><strong class="bold">Blocked algorithms</strong>: This<a id="_idIndexMarker371"/> is a list of algorithms that AutoML will not use during training. You can use this list to see the supported algorithms per task. You can exclude specific algorithms to avoid using one that is known to overfit for your scenario or focus the AutoML process to explore a small subset of the algorithms deeper.</li><li><strong class="bold">Exit criterion</strong>: This option<a id="_idIndexMarker372"/> allows you to specify the termination criteria of the best model search. You can specify the following exit criteria:<ul><li><strong class="bold">Training job time (hours)</strong>: The maximum amount of time, in hours, for an experiment to search for the best model.</li><li><strong class="bold">Metric score threshold</strong>: When AutoML trains a model with a primary metric that's better than this threshold value, it will cancel all remaining jobs and exit.</li></ul></li><li><strong class="bold">Validation type</strong>: You<a id="_idIndexMarker373"/> can modify the way you are validating the model's results by specifying a specific one, such as the famous <strong class="bold">k-fold cross validation</strong> one. For k-fold, the idea is that you split the dataset into k sets and then train <a id="_idIndexMarker374"/>with the k-1 sets and validate with the last one. Then, you rotate the sets and keep a new one for validation. You can use <strong class="source-inline">auto</strong> in most cases unless you have a reason to modify it. By default, 10-folds cross-validation is used if you have fewer than 1,000 rows, 3-folds is used if you have more than 1,000 and fewer than 20,000 rows, and if you have more than 20,000 rows, the dataset is split into 90% training data and 10% validation data.</li><li><strong class="bold">Max concurrent iterations</strong>: This represents the maximum number of iterations<a id="_idIndexMarker375"/> that would be executed in parallel. This option allows you to spin up multiple nodes in the cluster to search in parallel various models. Therefore, this option needs to be less than the max nodes your cluster can scale to.<p>To save on cost and time, set <strong class="bold">Exit criterion</strong> | <strong class="bold">Training job time (hours)</strong> to <strong class="source-inline">0.5</strong>, which is half an hour or 30 minutes. You can leave the rest of the options as is, as shown in the preceding screenshot. Click <strong class="bold">Save</strong> and return to the <strong class="bold">Select task type</strong> page of the wizard you saw in <em class="italic">Step 4</em>.</p></li></ul></li>
				<li>By clicking<a id="_idIndexMarker376"/> on <strong class="bold">View featurization settings</strong>, the <strong class="bold">Featurization</strong> screen <a id="_idIndexMarker377"/>will open, as shown here:<div id="_idContainer131" class="IMG---Figure"><img src="Images/B16777_05_014.jpg" alt="Figure 5.14 – The Featurization view on the create AutoML wizard&#13;&#10;" width="1055" height="495"/></div><p class="figure-caption">Figure 5.14 – The Featurization view on the create AutoML wizard</p><p>On this screen, you can define actions that influence the data preparation phase for the training process. The following options can be configured:</p><ul><li><strong class="bold">Included</strong>: You <a id="_idIndexMarker378"/>can exclude columns that should not be considered by the algorithm. The target column cannot be excluded.</li><li><strong class="bold">Feature type</strong>: By <a id="_idIndexMarker379"/>default, the <strong class="bold">Auto</strong> value is selected, which will automatically detect the type of each column you have in the dataset. It can be one of the following types: <strong class="bold">Numeric</strong>, <strong class="bold">DateTime</strong>, <strong class="bold">Categorical</strong>, <strong class="bold">Categorical hash</strong>, or <strong class="bold">Text</strong>. If you want, you can manually configure a column to be one of the available types.</li><li><strong class="bold">Impute with</strong>: If you <a id="_idIndexMarker380"/>have missing values in your dataset, this option will do in-place imputation based on the selected methodology. The options you can choose from are <strong class="bold">Auto</strong>, which is the default one, <strong class="bold">Most frequent</strong>, and <strong class="bold">Fill with constant</strong>. Especially for <strong class="bold">Numeric</strong> features, you can also use the <strong class="bold">Mean</strong> or the <strong class="bold">Median</strong> imputation strategy.<p class="callout-heading">Important note</p><p class="callout">Imagine that you had a product ID feature that explicitly mentioned the subscription product each customer is registered for. This feature would have numeric values, such as 1,055 and 1,060. This feature could accidentally be marked as a numeric feature, even though it is a categorical one. If you were missing values in the training dataset, the automatic approach may have imputed missing values with the average product ID, which doesn't make any sense. AutoML is smart enough to understand that if a numeric feature only has a few unique values repeating, this feature may be a categorical one, but you can explicitly assist the machine learning models by marking them as <strong class="bold">Categorical</strong> regarding <strong class="bold">Feature type</strong>. </p></li></ul></li>
				<li>On this wizard page, you should exclude the <strong class="bold">id</strong> feature. The <strong class="bold">id</strong> feature provides information on who the actual customer is. It doesn't provide any relevant information to the classification problem, so it should be excluded to save on computational resources. Click <strong class="bold">Save</strong> to return to the <strong class="bold">Select task type</strong> page of the wizard you saw in <em class="italic">Step 4</em>.</li>
			</ol>
			<p>By clicking <strong class="bold">Finish</strong>, as shown in <em class="italic">Figure 5.11</em>, you can finalize the configuration of your AutoML experiment, and the run will automatically start. The browser will redirect you to the AutoML run execution page, which allows you to monitor the AutoML training process and review the training results. You are going to explore that page in the next section.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor077"/>Monitoring the execution of the experiment</h1>
			<p>In the previous section, <em class="italic">Configuring an Automated ML experiment</em>, you submitted an AutoML experiment <a id="_idIndexMarker381"/>to execute on a remote compute cluster. Once you have submitted the job, your browser should redirect you to a page similar to the following:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/B16777_05_015.jpg" alt="Figure 5.15 – Running a new Automated ML run for the first time since the run finished&#13;&#10;" width="1258" height="1098"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – Running a new Automated ML run for the first time since the run finished</p>
			<p>At the top of the page, the name of the run of the experiment is autogenerated. In the preceding screenshot, it is called <strong class="source-inline">AutoML_05558d1d-c8ab-48a5-b652-4d47dc102d29</strong>. By clicking the pencil icon, you can edit this name and make it something more memorable. Change the name to <strong class="source-inline">my-first-experiment-run</strong>. Right below<a id="_idIndexMarker382"/> the run's name, you can click on one of the following commands:</p>
			<ul>
				<li><strong class="bold">Refresh</strong>: This will refresh the information provided on the page. While running an experiment, you can get the latest and greatest information.</li>
				<li><strong class="bold">Generate notebook</strong>: This will create a notebook with all the Python code that is needed to run the same experiment using the Azure ML SDK. You will learn more about the code needed to run an AutoML experiment through code in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>.</li>
				<li><strong class="bold">Cancel</strong>: This will cancel the current run. This option is only available if the experiment is still running.<p class="callout-heading">Important note</p><p class="callout">When you cancel a run, it may take a while until the run is canceled. By clicking on the <strong class="bold">Refresh</strong> button, you can check the progress of the cancelation process.</p></li>
				<li><strong class="bold">Delete</strong>: This deletes the selected run. It is only enabled if the run has been canceled or has finished executing.</li>
			</ul>
			<p>The run experiment page provides a lot of information regarding the entire process and is structured using tabs. By default, you start from the tab called <strong class="bold">Details</strong>. In this tab, you will find the following important information:</p>
			<ul>
				<li>On the <strong class="bold">Properties</strong> box, which is located on the left of the preceding screenshot, the most important information is located in the following fields: <p>a) <strong class="bold">Status</strong>, which describes the state of the current run. This can be running, canceled, errored, or completed.</p><p>b) <strong class="bold">Compute target</strong> is where the run is executed.</p><p>c) <strong class="bold">Raw JSON</strong> is a link that allows you to review all the configuration information of the current run in a machine-readable format.</p></li>
				<li>In the <strong class="bold">Run summary</strong> box on the right, you will find <strong class="bold">Task type</strong>, which is the type of model you are training. By clicking on <strong class="bold">View configuration settings</strong>, you can get a summary of the configuration parameters for the current experiment.</li>
				<li>The <strong class="bold">Description</strong> box allows you to document the hypothesis you are evaluating in the current experiment. By clicking the pencil icon, you can add a description of your experiment run, information that will allow you to recollect what you were looking for with this experiment and what the outcome was.</li>
			</ul>
			<p>The <a id="_idIndexMarker383"/>second tab of the run page is named <strong class="bold">Data guardrails</strong>. This tab provides you with detailed qualitative and quantitative information on the dataset you are using in your experiment. Depending on the task type, you will have different types of validation being performed. The status of each validation is <strong class="bold">Passed</strong> if everything was OK, <strong class="bold">Failed</strong> if your dataset has an issue that needs to be resolved, or <strong class="bold">Done</strong> if AutoML found an issue with your dataset and fixed it for you. In the <strong class="bold">Done</strong> cases, you will be able to see additional information on what was fixed in your dataset by clicking on the <strong class="bold">+View additional details</strong> button.</p>
			<p>The third tab is called <strong class="bold">Model</strong> and contains a list of all the models AutoML has trained so far, as seen in <em class="italic">Figure 5.16</em>. The model with the best metric score will be listed at the top. The table shows <strong class="bold">Algorithm name</strong>, the specific model's explanation results (if they are available) (<strong class="bold">Explained</strong>), the model's score (<strong class="bold">Accuracy</strong>, in this case), the percentage of the data used to train the model (<strong class="bold">Sampling</strong>), and information regarding when the model was trained and how long it took (the <strong class="bold">Created</strong> and <strong class="bold">Duration</strong> columns). If you select a model from the list, the following three commands will be enabled:</p>
			<ul>
				<li><strong class="bold">Deploy</strong> initiates the deployment of the selected model, which you will learn about in the next section.</li>
				<li><strong class="bold">Download</strong> allows you to download the selected model to your local disk. It is a ZIP file that <a id="_idIndexMarker384"/>contains <strong class="source-inline">model.pkl</strong>, which includes the actual trained model and all supporting files needed to perform inference.</li>
				<li><strong class="bold">Explain model</strong> kicks off the <strong class="bold">Explanations</strong> wizard, where you need to select a <strong class="bold">compute cluster</strong> to <a id="_idIndexMarker385"/>calculate the model explanations for the specific model. By default, AutoML will explain the best model, but you may want to explain additional models to compare them. Once the explanations have been calculated, you can view them by clicking on <strong class="bold">View explanation</strong> in the <strong class="bold">Explained</strong> column. This will open the <strong class="bold">Explanations</strong> tab of the selected model, which has populated the report. You will learn more about model interpretability in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>.</li>
			</ul>
			<p>The<a id="_idIndexMarker386"/> fourth tab on the main page, called <strong class="bold">Outputs + logs</strong>, displays the outputs and log of the specific run in a simple files explorer. These logs are the ones of the overall AutoML process. If you want to view the logs of a specific model training process, you will need to select the model from the <strong class="bold">Models</strong> tab and then visit the <strong class="bold">Outputs + logs</strong> section of that child run. The files explorer that's available in this tab allows you to navigate through the folder structure on the left-hand side. If you select a file, its contents will be displayed on the right-hand side.</p>
			<p>So far, you have managed to train a couple of models and see the best model to predict whether a customer will churn or not. In the next section, you will learn how to operationalize this model with only a couple of clicks.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor078"/>Deploying the best model as a web service</h1>
			<p>In the <a id="_idIndexMarker387"/>previous section, you navigated around the run experiment page while reviewing the information related to the run execution and the results of the exploration, which are the trained models. In this section, we will revisit the <strong class="bold">Models</strong> tabs and start deploying the best model as a web service to be able to make real-time inferences. Navigate to the run's details page, as shown in <em class="italic">Figure 5.15</em>. Let's get started:</p>
			<ol>
				<li value="1">Click on the <strong class="bold">Models</strong> tab. You should see a page similar to the one shown here:<div id="_idContainer133" class="IMG---Figure"><img src="Images/B16777_05_016.jpg" alt="Figure 5.16 – The Models tab as a starting point for deploying a model&#13;&#10;" width="1650" height="1243"/></div><p class="figure-caption">Figure 5.16 – The Models tab as a starting point for deploying a model</p></li>
				<li>In this list, you <a id="_idIndexMarker388"/>can select any model you want to deploy. Select the row with the best model, as shown in the preceding screenshot. Click the <strong class="bold">Deploy</strong> command at the top of the list. The <strong class="bold">Deploy a model</strong> dialog will appear, as shown here:<div id="_idContainer134" class="IMG---Figure"><img src="Images/B16777_05_017.jpg" alt="Figure 5.17 – The Deploy a model dialogue&#13;&#10;" width="935" height="1220"/></div><p class="figure-caption">Figure 5.17 – The Deploy a model dialogue</p></li>
				<li>In the <strong class="bold">Deploy a model</strong> dialog, you <a id="_idIndexMarker389"/>will be able to define a deployment name and the compute type to be able to proceed: <ul><li><strong class="bold">Name</strong>: The name of the service that will expose the model. Type in <strong class="source-inline">myfirstmlwebservice</strong>.</li><li><strong class="bold">Compute type</strong>: There are two types you can choose from:<ul><li><strong class="bold">Azure Kubernetes Cluster</strong> (<strong class="bold">AKS</strong>): This<a id="_idIndexMarker390"/> option should be selected when you want to deploy your model for a production workload and are handling multiple requests in parallel. This option allows for both key-based and token based authentications if you want to protect the endpoint. It also supports using <strong class="bold">Field Programmable Gate Arrays</strong> (<strong class="bold">FPGAs</strong>) for even faster model inferences. In this case you would also need to specify the <strong class="bold">Compute name</strong> property of the AKS cluster where you want to deploy the model. The list should contain all inference clusters you may have registered in <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>.</li><li><strong class="bold">Azure Container Instance</strong>: If you<a id="_idIndexMarker391"/> plan to do functional<a id="_idIndexMarker392"/> testing or you want to deploy a model for the development environments, you can deploy the web service as a single Azure Container Instance. This compute type is cheaper, but it doesn't scale and it only supports key-based authentication. For this book, you can select this type to deploy the model.</li></ul></li></ul></li>
				<li>By pressing <strong class="bold">Deploy</strong>, you can start deploying the model. <p>In the top-right corner of your browser, a popup window will appear, as shown in the following screenshot, letting you know that your model has started being deployed:</p><div id="_idContainer135" class="IMG---Figure"><img src="Images/B16777_05_018.jpg" alt="Figure 5.18 – Endpoint deployment window – deployment InProgress&#13;&#10;" width="618" height="158"/></div><p class="figure-caption">Figure 5.18 – Endpoint deployment window – deployment InProgress</p></li>
				<li>The pop-up window will quickly disappear, but you can revisit it by clicking on the notification <strong class="bold">bell</strong> icon in the top-right corner, as shown here:<div id="_idContainer136" class="IMG---Figure"><img src="Images/B16777_05_019.jpg" alt="Figure 5.19 – Azure Machine Learning Studio taskbar in the top-right corner&#13;&#10;" width="174" height="57"/></div><p class="figure-caption">Figure 5.19 – Azure Machine Learning Studio taskbar in the top-right corner</p></li>
				<li>The model will take a couple of minutes to deploy. By looking at your notifications, you<a id="_idIndexMarker393"/> can check the progress of your deployment. If the notification turns green, as shown in the following screenshot, then the deployment is completed:<div id="_idContainer137" class="IMG---Figure"><img src="Images/B16777_05_020.jpg" alt="Figure 5.20 – Endpoint deployment window – deployment Completed&#13;&#10;" width="626" height="147"/></div><p class="figure-caption">Figure 5.20 – Endpoint deployment window – deployment Completed</p></li>
				<li>By clicking on the <strong class="bold">Deploy details</strong> link in the notification, your browser will redirect you to the endpoint page of the deployed model, as shown here:<div id="_idContainer138" class="IMG---Figure"><img src="Images/B16777_05_021.jpg" alt="Figure 5.21 – Endpoint page of the deployed model&#13;&#10;" width="1621" height="1169"/></div><p class="figure-caption">Figure 5.21 – Endpoint page of the deployed model</p><p class="callout-heading">Important note</p><p class="callout">There are multiple ways you can reach the endpoint page of your model. For example, you can see a list of all the published endpoints by going to the <strong class="bold">Assets</strong> | <strong class="bold">Endpoints</strong> menu in Azure ML Studio. To reach the same page, select the endpoint you want to inspect.</p><p>The endpoint <a id="_idIndexMarker394"/>page of the deployed model has a similar tab-based structure to the run experiment page. The first tab is called <strong class="bold">Details</strong> and provides information regarding the deployment of the model. Out of them, the most important ones are located in the <strong class="bold">Attributes</strong> box shown in the <a id="_idIndexMarker395"/>preceding screenshot. They are as follows:</p><ul><li><strong class="bold">Service ID</strong> is the name of the service you specified in <em class="italic">Step 3</em> of the deployment wizard.</li><li><strong class="bold">Deployment state</strong> provides the state of the deployed or to-be-deployed model. There are five states the endpoint can be in. You can find more information regarding states and potential issues in the <em class="italic">Further reading</em> section of this chapter.</li><li><strong class="bold">Rest endpoint</strong> is the link you can copy into an application's code to call the deployed model via a REST API.</li></ul></li>
				<li>The second tab is called <strong class="bold">Test</strong> and allows you to test the deployed model. By default, you get the <strong class="bold">Fields</strong> mode, which is a form containing all the inputs your model is expecting, as shown here:<p class="figure-caption"> </p><div id="_idContainer139" class="IMG---Figure"><img src="Images/B16777_05_022.jpg" alt="Figure 5.22 – The endpoint page of the deployed model&#13;&#10;" width="660" height="872"/></div><p class="figure-caption">Figure 5.22 – The endpoint page of the deployed model</p></li>
				<li>If you want to perform a mini-batch inference, meaning that you want to send multiple records at once, you can switch to <strong class="bold">CSV</strong> from the top-right corner. In that mode, you can copy the contents of a CSV file into the text box that will appear and hit the <strong class="bold">Test</strong> button.<p>In either mode, once you hit the <strong class="bold">Test</strong> button, the input data will be submitted to the REST API and results will be provided.</p></li>
				<li>The<a id="_idIndexMarker396"/> third tab of the endpoint page is called <strong class="bold">Consume</strong>. On that page, you can find basic code for the consumption of the REST API in <strong class="bold">C#</strong>, <strong class="bold">Python</strong>, and <strong class="bold">R</strong>. This <a id="_idIndexMarker397"/>code is automatically generated using a file <a id="_idIndexMarker398"/>called <strong class="source-inline">swagger.json</strong>, which is <a id="_idIndexMarker399"/>automatically generated in the endpoint. This <a id="_idIndexMarker400"/>file describes the expected inputs and outputs of the REST APIs and is commonly used by web developers to provide documentation on the REST endpoints they produce. </li>
				<li>Finally, the <strong class="bold">Deployment logs</strong> tab shows a detailed log of the deployment of the model. You can troubleshoot potential deployment issues using this tab.</li>
			</ol>
			<p>Congratulations – you have successfully deployed your very own real-time endpoint that makes inferences on whether a customer will churn or not! In the next section, you will learn about the artifacts that are created when your model is deployed.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Understanding the deployment of the model</h2>
			<p>In the <a id="_idIndexMarker401"/>previous section, you deployed a model as a REST endpoint. In this section, you will understand what happened behind the scenes and discover the artifacts that were generated in the process.</p>
			<p>You started from the list of models that the AutoML process has trained. In that list, you selected a model and clicked on the <strong class="bold">Deploy</strong> command. By doing that, you registered the selected model in the Azure ML workspace and once that was done, the process continued with deploying the endpoint, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="Images/B16777_05_023.jpg" alt="Figure 5.23 – From AutoML model list to endpoint deployment&#13;&#10;" width="603" height="203"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23 – From AutoML model list to endpoint deployment</p>
			<p>Model registration <a id="_idIndexMarker402"/>creates a versioned record within the Azure ML workspace that allows you to keep track of what datasets were used to train the specific model and where the specific model was deployed. You will learn more about model registration in <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>. Let's take a look:</p>
			<ol>
				<li value="1">To see this model registration, in Azure ML Studio, navigate to <strong class="bold">Assets</strong> | <strong class="bold">Models</strong>. You will end up on the <strong class="bold">Model List</strong> page, as shown here:<div id="_idContainer141" class="IMG---Figure"><img src="Images/B16777_05_024.jpg" alt="Figure 5.24 – The Model List page&#13;&#10;" width="1573" height="782"/></div><p class="figure-caption">Figure 5.24 – The Model List page</p><p>The <strong class="bold">Model List</strong> page lists all registered models within the Azure ML workspace. These are the models that are being tracked by the workspace. Each model has a unique name. You <a id="_idIndexMarker403"/>can have multiple versions of the same model, something that automatically happens when you try to register a model with the same name. The model list allows you to select a model and perform various actions on it, such as deleting and deploying it.</p><p class="callout-heading">Important note</p><p class="callout">You can only delete a registered model if it is not being used by any endpoint.</p></li>
				<li>By clicking on the <strong class="bold">Name</strong> property of a model, you will end up on the details page of the selected registered model, as shown here:<div id="_idContainer142" class="IMG---Figure"><img src="Images/B16777_05_025.jpg" alt="Figure 5.25 – Registered model details page&#13;&#10;" width="1432" height="879"/></div><p class="figure-caption">Figure 5.25 – Registered model details page</p><p>Here, you can get the general details regarding the model, such as which run trained the specific model, what framework was used, and the experiment name where this run is registered. Note that since this model was trained using AutoML, the framework is the generic AutoML one. In <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>, you will be able to register your own models and specify the framework you used to train the model. </p><p>In<a id="_idIndexMarker404"/> the <strong class="bold">Artifacts</strong> tab, you will find the <strong class="bold">model.pkl</strong> file, which contains the trained model. In the <strong class="bold">Explanations</strong> and <strong class="bold">Fairness</strong> tabs, you can view the interpretability results, if they have been generated for the specific model. You will learn more about model interpretability in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. In the <strong class="bold">Datasets</strong> tab, you can see a reference to the specific version of the dataset that you used while configuring the AutoML experiment. This allows you to have lineage between the dataset that was used for training and the models you have deployed. </p></li>
				<li>Once the model has been registered, the deployment wizard creates an endpoint. Back in Azure ML Studio, click on the <strong class="bold">Assets</strong> | <strong class="bold">Endpoint</strong> menu item. This will bring you to the <strong class="bold">Endpoints</strong> page shown here:<div id="_idContainer143" class="IMG---Figure"><img src="Images/B16777_05_026.jpg" alt="Figure 5.26 – The Endpoints page&#13;&#10;" width="1332" height="685"/></div><p class="figure-caption">Figure 5.26 – The Endpoints page</p><p>This list shows all real-time endpoints you have deployed from this Azure ML workspace. You will notice the <strong class="bold">myfirstmlwebservice</strong> endpoint, which you deployed in<a id="_idIndexMarker405"/> the previous section. By clicking on its name, you will end up on the endpoint's page, which you saw in <em class="italic">Figure 5.21</em>.</p></li>
				<li>Behind the scenes, this endpoint is a container instance that was deployed in the <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>, right next to your Azure ML workspace resource. Navigate to the Azure portal and open the <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You should have resources similar to the ones shown here:<div id="_idContainer144" class="IMG---Figure"><img src="Images/B16777_05_027.jpg" alt="Figure 5.27 – The resources in the packt-azureml-rg resource group of the Azure portal&#13;&#10;" width="1327" height="862"/></div><p class="figure-caption">Figure 5.27 – The resources in the packt-azureml-rg resource group of the Azure portal</p></li>
				<li>Here, you will see<a id="_idIndexMarker406"/> that a <strong class="bold">Container instance</strong> has been deployed named <strong class="bold">myfirstmllwebservice-&lt;random id&gt;</strong>, which is the name of the endpoint you saw in <em class="italic">Figure 5.26</em>. This is the engine that is hosting the REST API you deployed in the previous section.<p class="callout-heading">Important note</p><p class="callout">Never delete Azure ML workspace artifacts directly from the resource group. This will leave orphan registrations in your workspace, such as an endpoint that points to a deleted container instance.</p></li>
			</ol>
			<p>In this section, you <a id="_idIndexMarker407"/>saw what happens behind the scenes when you deploy a model from AutoML. In the next section, you are going to delete the endpoint you deployed to avoid spending money on a real-time endpoint you don't need.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>Cleaning up the model deployment</h2>
			<p>In this section, you <a id="_idIndexMarker408"/>will clean up the deployed model. You should remove the deployments of the models that you are not planning to use. Otherwise, you will be paying for unused provisioned resources.</p>
			<p>Navigate to Azure ML Studio. Click on the <strong class="bold">Assets</strong> | <strong class="bold">Endpoint</strong> menu item. This will bring you to the endpoints list page shown in <em class="italic">Figure 5.26</em>. Select the <strong class="source-inline">myfirstmlwebservice</strong> endpoint and click on the <strong class="bold">Delete</strong> command. Confirm your desire to delete the endpoint by clicking <strong class="bold">Delete</strong> in the pop-up window shown here:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="Images/B16777_05_028.jpg" alt="Figure 5.28 – Delete real-time endpoint pop-up window&#13;&#10;" width="810" height="360"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.28 – Delete real-time endpoint pop-up window</p>
			<p>After the confirmation, the endpoint and the container instance you saw back in the Azure portal will get deleted. If you like, you<a id="_idIndexMarker409"/> can verify this by visiting the <strong class="source-inline">packt-azureml-rg</strong> <strong class="bold">resource group</strong>. </p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>Summary</h1>
			<p>In this chapter, you learned how to configure an AutoML process to discover the best model that can predict whether a customer will churn or not. First, you used the AutoML wizard of the Azure Machine Learning Studio web experience to configure the experiment. Then, you monitored the execution of the run in the <strong class="bold">Experiments</strong> section of the studio interface. Once the training was completed, you reviewed the trained models and saw the information that had been stored regarding the best model. Then, you deployed that machine learning model in an Azure Container Instance and tested that the real-time endpoint performs the requested inferences. In the end, you deleted the deployment to avoid incurring costs in your Azure subscription.</p>
			<p>In the next chapter, you will continue exploring the no-code/low code aspects of the Azure Machine Learning Studio experience by looking at the designer, which allows you to graphically design a training pipeline and operationalize the produced model.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>Question</h1>
			<p>You need to train a classification model but only consider linear models during the AutoML process. Which of the following allows you to do that in the Azure Machine Learning Studio experience?</p>
			<p>a) Add all algorithms other than linear ones to the blocked algorithms list.</p>
			<p>b) Set the Exit criterion option to a metric score threshold.</p>
			<p>c) Disable the automatic featurization option.</p>
			<p>d) Disable the deep learning option on the classification task.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor083"/>Further reading</h1>
			<p>This section offers additional content as useful web resources:</p>
			<ul>
				<li>Basic concepts regarding AutoML:</li>
				<li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml">https://docs.microsoft.com/azure/machine-learning/concept-automated-ml</a>.</li>
				<li>Deep dive into how to use AutoML: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models">https://docs.microsoft.com/azure/machine-learning/how-to-use-automated-ml-for-ml-models</a>.</li>
				<li>Cross-validation in sklearn: <a href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a>.</li>
				<li>Understanding the service state of an endpoint: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=azcli#understanding-service-state">https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli#understanding-service-state</a>.</li>
				<li>Generating client SDKs from <strong class="source-inline">swagger.json</strong> files: <a href="https://swagger.io/tools/swagger-codegen/">https://swagger.io/tools/swagger-codegen/</a>.</li>
			</ul>
		</div>
	</div></body></html>