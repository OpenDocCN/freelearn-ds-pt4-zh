<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer226">
			<h1 id="_idParaDest-115"><em class="italic"><a id="_idTextAnchor117"/>Chapter 8</em>: Experimenting with Python Code</h1>
			<p>In this chapter, you will understand how to train <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) models with code. You will start with a simple ML model using the <strong class="bold">Python</strong> <strong class="source-inline">scikit-learn</strong> library, which is commonly referred to as <strong class="source-inline">sklearn</strong>. You will understand how you can keep track of the training metrics using the <strong class="bold">Azure</strong> <strong class="bold">Machine Learning</strong> (<strong class="bold">AzureML</strong>) <strong class="bold">SDK</strong> and <strong class="bold">MLflow</strong>. Then, you will see how you can scale out the training process in compute clusters.</p>
			<p>In this chapter, we are going to cover the following topics:</p>
			<ul>
				<li>Training a simple <strong class="source-inline">sklearn</strong> model within notebooks</li>
				<li>Tracking metrics in Experiments</li>
				<li>Scaling the training process with compute clusters</li>
			</ul>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control (IAM)</strong> role at the resource group level. Within that resource group, you should have already deployed an Azure ML resource named <strong class="source-inline">packt-learning-mlw</strong>. These resources should be already available to you if you followed the instructions in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>You will also need to have a basic understanding of the Python language. The code snippets target Python 3.6 or newer versions. You should also be familiar with working in the notebook experience within AzureML Studio, which was covered in the previous chapter.</p>
			<p>This chapter assumes you have registered the <strong class="source-inline">scikit-learn</strong> <strong class="source-inline">diabetes</strong> dataset in your AzureML workspace and you have created a compute cluster named <strong class="source-inline">cpu-sm-cluster</strong>, as described in the <em class="italic">Defining datastores</em>, <em class="italic">Working with datasets</em>, and <em class="italic">Working with compute targets</em> sections in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. </p>
			<p>You can find all notebooks and code snippets for this chapter in GitHub at <a href="http://bit.ly/dp100-ch08">http://bit.ly/dp100-ch08</a>.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor119"/>Training a simple sklearn model within notebooks</h1>
			<p>The<a id="_idIndexMarker533"/> goal of this section is to create a <a id="_idIndexMarker534"/>Python script that will produce a simple model on top of the <strong class="source-inline">diabetes</strong> dataset that you registered in <em class="italic">Working with datasets</em> in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. The model will be getting numeric inputs and will be predicting a numeric output. To create this model, you will need to prepare the data, train the model, evaluate how the trained model performs, and then store it so that you will be able to reuse it in the future, as seen in <em class="italic">Figure 8.1</em>:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="Images/B16777_08_001.jpg" alt="Figure 8.1 – Process to produce the diabetes-predicting model&#13;&#10;" width="1650" height="284"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Process to produce the diabetes-predicting model</p>
			<p>Let's start by understanding the dataset you will be working with. The <strong class="source-inline">diabetes</strong> dataset consists of data from 442 <strong class="source-inline">diabetes</strong> patients. Each row represents one patient. Each row consists of 10 features (<strong class="bold">0</strong> to <strong class="bold">9</strong> in <em class="italic">Figure 8.2</em>) such as age, blood pressure, and blood sugar level. These features have been transformed (mean-centered and scaled), a process similar to the data featurization you saw in automated ML in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting The Machines Do the Model Training</em>. The eleventh column, named <strong class="source-inline">target</strong>, is the quantitative measure of the <strong class="source-inline">diabetes</strong> disease progression 1 year after the features were recorded. </p>
			<p>You can explore the dataset further within the AzureML Studio interface as seen in <em class="italic">Figure 8.2</em>: </p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="Images/B16777_08_002.jpg" alt="Figure 8.2 – The registered diabetes dataset&#13;&#10;" width="1650" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – The registered diabetes dataset</p>
			<p>Normally in the <a id="_idIndexMarker535"/>preparation phase, you load the<a id="_idIndexMarker536"/> raw data, curate rows that have missing values, normalize feature values, and then split the dataset into train and validation data. Since the data is already preprocessed, you will just need to load the data and split it into two:</p>
			<ol>
				<li>Navigate to the <strong class="bold">Notebooks</strong> section of your AzureML Studio web interface. Create a folder named <strong class="source-inline">chapter08</strong> and then create a notebook named <strong class="source-inline">chapter08.ipynb</strong>:<div id="_idContainer202" class="IMG---Figure"><img src="Images/B16777_08_003.jpg" alt="Figure 8.3 – Creating the chapter08 notebook you will be working on&#13;&#10;" width="890" height="777"/></div><p class="figure-caption">Figure 8.3 – Creating the chapter08 notebook you will be working on</p></li>
				<li>In the first<a id="_idIndexMarker537"/> cell of the notebook, add<a id="_idIndexMarker538"/> the following code:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">diabetes_ds = ws.datasets['diabetes']</p><p class="source-code">training_data, validation_data =\</p><p class="source-code">diabetes_ds.random_split(percentage = 0.8)</p><p class="source-code">X_train =\</p><p class="source-code">training_data.drop_columns('target').to_pandas_dataframe()</p><p class="source-code">y_train =\</p><p class="source-code">training_data.keep_columns('target').to_pandas_dataframe()</p><p class="source-code">X_validate =\</p><p class="source-code">validation_data.drop_columns('target').to_pandas_dataframe()</p><p class="source-code">y_validate =\</p><p class="source-code">validation_data.keep_columns('target').to_pandas_dataframe()</p><p>In this code snippet, you get a reference to your workspace and retrieve the dataset named <strong class="source-inline">diabetes</strong>. Then you split it into two <strong class="source-inline">TabularDataset</strong> using the <strong class="source-inline">random_split()</strong> method. The first dataset is <strong class="source-inline">training_data</strong>, which contains 80% of the data, while the <strong class="source-inline">validation_data</strong> dataset references the other 20% of the data. These datasets contain both the features and the label you want to predict. Using the <strong class="source-inline">drop_columns()</strong> and <strong class="source-inline">keep_columns()</strong> methods of <strong class="source-inline">TabularDataset</strong>, you can separate the features from the <strong class="source-inline">label</strong> columns. You then load the data in memory in a <strong class="bold">pandas</strong> DataFrame using the <strong class="source-inline">to_pandas_dataframe()</strong> method of <strong class="source-inline">TabularDataset</strong>. You end up with four pandas DataFrames:</p><ul><li><strong class="source-inline">X_train</strong>: Contains 80% of the rows. Each row has 10 columns (<strong class="source-inline">0</strong> to <strong class="source-inline">9</strong>).</li><li><strong class="source-inline">y_train</strong>: Contains 80% of the rows. Each row has 1 column (<strong class="source-inline">target</strong>).</li><li><strong class="source-inline">X_validate</strong>: Contains 20% of the rows. Each row has 10 columns (<strong class="source-inline">0</strong> to<strong class="source-inline"> 9</strong>).</li><li><strong class="source-inline">y_validate</strong>: Contains 20% of the rows. Each row has 1 column (<strong class="source-inline">target</strong>).</li></ul><p>The <strong class="source-inline">diabetes</strong> dataset is very popular in scientific literature. It is used as an example to<a id="_idIndexMarker539"/> train <em class="italic">regression</em> models. The <strong class="source-inline">scikit-learn</strong> library offers a dedicated<a id="_idIndexMarker540"/> module named <strong class="source-inline">sklearn.linear_model</strong> containing a lot of linear regression models we can use. Now that you have prepared the data, your next task is to train the model.</p></li>
				<li>In this step, you are going to train a <strong class="source-inline">LassoLars</strong> model, which is an abbreviation <a id="_idIndexMarker541"/>for <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong> (<strong class="bold">LASSO</strong>) model, fit with <a id="_idIndexMarker542"/>the <strong class="bold">Least-Angle Regression</strong> (<strong class="bold">LARS</strong>) selection algorithm, used in LASSO mode. In a new notebook cell, add the following code:<p class="source-code">from sklearn.linear_model import LassoLars</p><p class="source-code">alpha = 0.1</p><p class="source-code">model = LassoLars(alpha=alpha)</p><p class="source-code">model.fit(X_train, y_train)</p><p>In <em class="italic">line 3</em> of this<a id="_idIndexMarker543"/> code block, the constructor of the <strong class="source-inline">LassoLars</strong> class accepts a float parameter named <strong class="source-inline">alpha</strong>, which is known as a <em class="italic">regularization parameter</em> or <em class="italic">penalty term</em>. Its primary purpose is to protect the model from overfitting to the training dataset. Since this parameter controls the training process, it is referred to as being a <em class="italic">hyperparameter</em>. This parameter cannot be changed once the model has been trained. In this code block, you are instantiating an untrained model, setting <strong class="source-inline">0.1</strong> for the <strong class="source-inline">alpha</strong> parameter. In the next chapter, <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>, you will tune this parameter and try to locate the best value for your dataset.</p><p>Then, you are <a id="_idIndexMarker544"/>using the <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> DataFrames to fit() the model, which means you are training the model against<a id="_idIndexMarker545"/> the training dataset. After this process, the <strong class="source-inline">model</strong> variable references a trained model that you can use to make predictions.</p></li>
				<li>The next task is to evaluate the model you produced based on a metric. The most common metrics to evaluate a regression model are as follows: <ul><li>Mean or median absolute error.</li><li>Mean squared error or log error. Another common variation of this metric is the <strong class="bold">Root Mean Squared Error </strong>(<strong class="bold">RMSE</strong>).</li><li>R2, which is known as the coefficient of determination.</li><li>Explained variance.</li><li>Spearman correlation.</li></ul><p>You will use <a id="_idIndexMarker546"/>the <strong class="bold">RMSE</strong> to measure the performance of your model utilizing the <strong class="source-inline">mean_squared_error</strong> method of the <strong class="source-inline">sklearn.metrics</strong> package. A common issue with this metric is that a model trained on data with a larger range of values has a higher rate of error than the same model trained on data with a smaller range. You are going to use a technique called <em class="italic">metric normalization</em> that basically divides the metric by the range of the data. The resulting metric is known <a id="_idIndexMarker547"/>as the <strong class="bold">Normalized Root Mean Squared Error</strong> (<strong class="bold">NRMSE</strong>).</p><p>In a new notebook cell, write the following code:</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">predictions = model.predict(X_validate)</p><p class="source-code">rmse = mean_squared_error(predictions, y_validate, squared = False)</p><p class="source-code"># Range of data using the peak to peak numpy function</p><p class="source-code">range_y_validate = y_validate.to_numpy().ptp()</p><p class="source-code"># Normalize dividing by the range of the data</p><p class="source-code">nrmse = rmse/range_y_validate</p><p class="source-code">print(f"Normalized Root Mean Squared Error: {nrmse}")</p><p>You start by<a id="_idIndexMarker548"/> predicting the values<a id="_idIndexMarker549"/> using the <strong class="source-inline">X_validate</strong> DataFrame. You calculate the RMSE, comparing the predictions with the ground truth stored in the <strong class="source-inline">y_validate</strong> DataFrame. Then, you calculate the range of values (maximum minus minimum) using the <strong class="source-inline">ptp()</strong> method of <strong class="bold">NumPy</strong>. As<a id="_idIndexMarker550"/> the last step, you calculate the NRMSE by dividing the RMSE metric by the range you calculated.</p><p>Since you are splitting the data randomly, and the training dataset is not always the same, the calculated NRMSE will differ from one run to another. Its value will be approximately <strong class="source-inline">0.2</strong>.</p><p>The last step is to store the trained model to be able to reuse it in the future. You are going to create a folder named <strong class="source-inline">outputs</strong>, and you are going to persist the model to a file. The persistence of a Python object to a file is done using the <strong class="source-inline">dump()</strong> method of the <strong class="source-inline">joblib</strong> library.</p><p>In a new notebook cell, input the following source code:</p><p class="source-code">import os</p><p class="source-code">import joblib</p><p class="source-code">os.makedirs('./outputs', exist_ok=True)</p><p class="source-code">model_file_n<a id="_idTextAnchor120"/>ame = f'model_{nrmse:.4f}_{alpha:.4f}.pkl'</p><p class="source-code">joblib.dump(value=model,</p><p class="source-code">        filename=os.path.join('./outputs/',model_file_name))</p><p>You create the <strong class="source-inline">outputs</strong> folder if it does not exist. Then, you store the model in a filename<a id="_idIndexMarker551"/> containing the <strong class="source-inline">model_</strong> prefix, followed<a id="_idIndexMarker552"/> by the NRMSE metric calculated in <em class="italic">Step 4</em>, followed by an <strong class="source-inline">_</strong>, and then the <strong class="source-inline">alpha</strong> parameter used to instantiate the model. You should be able to see the serialized model in the file explorer, as seen in <em class="italic">Figure 8.4</em>: </p></li>
			</ol>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="Images/B16777_08_004.jpg" alt="Figure 8.4 – Serialized model stored in the outputs folder&#13;&#10;" width="742" height="593"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Serialized model stored in the outputs folder</p>
			<p>The <a id="_idIndexMarker553"/>naming convention you used in <em class="italic">Step 5</em> helps you <a id="_idIndexMarker554"/>keep track of how well the model performs and tracks the parameter you used in this run. The AzureML SDK offers various methods to monitor, organize, and manage your training runs, something you will explore in the next section.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor121"/>Tracking metrics in Experiments</h1>
			<p>When you<a id="_idIndexMarker555"/> are training a model, you are performing a trial <a id="_idIndexMarker556"/>and you are logging various aspects of that process, including metrics such as the NRMSE that you need to compare model performance. The AzureML workspace offers the concept of <strong class="bold">Experiments</strong> – that is, a container to group such trials/runs together.</p>
			<p>To create a new Experiment, you just need to specify the workspace you will use and provide a name that contains up to 36 letters, numbers, underscores, and dashes. If the Experiment already exists, you will get a reference to it. Add a cell in your <strong class="source-inline">chapter08.ipynb</strong> notebook and add the following code:</p>
			<p class="source-code">from azureml.core import Workspace, Experiment</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">exp = Experiment(workspace=ws, name="chapter08")</p>
			<p>You start by getting a reference to the existing AzureML workspace and then create the <strong class="source-inline">chapter08</strong> Experiment if it doesn't already exist. If you navigate to the <strong class="bold">Assets</strong> | <strong class="bold">Experiments</strong> section of the Studio interface you will notice an empty Experiment appears in the list, as seen in <em class="italic">Figure 8.5</em>:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="Images/B16777_08_005.jpg" alt="Figure 8.5 – Empty Experiment created with the SDK&#13;&#10;" width="1650" height="653"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Empty Experiment created with the SDK</p>
			<p>To create a <a id="_idIndexMarker557"/>run under the <strong class="source-inline">chapter08</strong> Experiment, you<a id="_idIndexMarker558"/> can add the following code in a new cell:</p>
			<p class="source-code">run = exp.start_logging()</p>
			<p class="source-code">print(run.get_details())</p>
			<p>The <strong class="source-inline">run</strong> variable gives you access to an instance of the <strong class="source-inline">Run</strong> class of the AzureML SDK, which represents a single trial of an Experiment. Each <strong class="source-inline">run</strong> instance has a unique ID that identifies the specific run in the workspace. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the <em class="italic">Scaling the training process with compute clusters</em> section, you will use the <strong class="source-inline">get_context</strong> method of the <strong class="source-inline">Run</strong> class to get a reference to the <strong class="source-inline">run</strong> instance where the Python script is being executed. The <strong class="source-inline">run</strong> is normally automatically created when you submit a script to execute under an Experiment. The <strong class="source-inline">start_logging</strong> method is used rarely and only when you want to manually create a <strong class="source-inline">run</strong> and log metrics. The most common cases are when you are using notebook cells to train a model or when you are training a model on a remote compute such as your local computer or a <strong class="bold">Databricks</strong> workspace.</p>
			<p>The <strong class="source-inline">run</strong> class offers a rich logging API. The most frequent method used is the generic <strong class="source-inline">log()</strong> one, which allows you to log metrics with the following code:</p>
			<p class="source-code">run.log("nrmse", 0.01)</p>
			<p class="source-code">run.log(name="nrmse", value=0.015, description="2nd measure")</p>
			<p>In this code, you <a id="_idIndexMarker559"/>log the value <strong class="source-inline">0.01</strong> for the <strong class="source-inline">nrmse</strong> metric, and<a id="_idIndexMarker560"/> then you log the value <strong class="source-inline">0.015</strong> for the same metric, passing the optional <strong class="source-inline">description</strong> parameter.</p>
			<p>If you navigate to the <strong class="bold">Experiments</strong> section of the Studio interface and select the <strong class="source-inline">chapter08</strong> Experiment, you will notice there is a single <strong class="source-inline">run</strong> that is currently <strong class="bold">Running</strong>. If you open that <strong class="source-inline">run</strong> and navigate to the <strong class="bold">Metrics</strong> tab, you will be able to notice the two measurements of the <strong class="bold">nrmse</strong> metric, depicted either as a chart or a table, as seen in <em class="italic">Figure 8.6</em>:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="Images/B16777_08_006.jpg" alt="Figure 8.6 – The two measurements of nrmse as seen in the Studio experience&#13;&#10;" width="1650" height="1032"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – The two measurements of nrmse as seen in the Studio experience</p>
			<p>The <strong class="source-inline">Run</strong> class offers <a id="_idIndexMarker561"/>a rich list of logging methods, including the <a id="_idIndexMarker562"/>following ones:</p>
			<ul>
				<li>The <strong class="source-inline">log_list</strong> method allows you to log a list of values for the specific metric. An example of this method is the following code:<p class="source-code">run.log_list("accuracies", [0.5, 0.57, 0.62])</p><p>This code will produce <em class="italic">Figure 8.7</em> in the <em class="italic">Metrics</em> section of the run:</p></li>
			</ul>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="Images/B16777_08_007.jpg" alt="Figure 8.7 – Graph representing three values logged with the log_list method&#13;&#10;" width="912" height="697"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Graph representing three values logged with the log_list method</p>
			<ul>
				<li>The <strong class="source-inline">log_table</strong> and <strong class="source-inline">log_row</strong> methods allow you to log tabular data. Note that, with this <a id="_idIndexMarker563"/>method, you can specify the labels in<a id="_idIndexMarker564"/> the <em class="italic">x</em> axis in contrast to the <strong class="source-inline">log_list</strong> method:<p class="source-code">run.log_table("table", {"x":[1, 2], "y":[0.1, 0.2]})</p><p class="source-code">run.log_row("table", x=3, y=0.3)</p><p>This code snippet will produce <em class="italic">Figure 8.8</em> in the <em class="italic">Metrics</em> section of the run:</p></li>
			</ul>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="Images/B16777_08_008.jpg" alt="Figure 8.8 – Tabular metric logged using the log_table and log_row methods&#13;&#10;" width="909" height="683"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Tabular metric logged using the log_table and log_row methods</p>
			<ul>
				<li>Specialized<a id="_idIndexMarker565"/> methods such as <strong class="source-inline">log_accuracy_table</strong>, <strong class="source-inline">log_confusion_matrix</strong>, <strong class="source-inline">log_predictions</strong>, and <strong class="source-inline">log_residuals</strong> provide a custom rendering of the logged data.</li>
				<li>The <strong class="source-inline">log_image</strong> method <a id="_idIndexMarker566"/>allows you to log graphs or images from the well-known <strong class="source-inline">matplotlib</strong> Python library or other plotting libraries.</li>
				<li>The <strong class="source-inline">upload_file</strong>, <strong class="source-inline">upload_files</strong>, and <strong class="source-inline">upload_folder</strong> methods allow you to upload Experiment residuals and associate them with the current run. These methods are commonly used to upload various binary artifacts that can be produced during the <strong class="source-inline">run</strong> execution, such as interactive HTML graphs created by open source libraries such as <strong class="source-inline">plotly</strong>.</li>
			</ul>
			<p>You can optionally create child runs to isolate a subsection of the trial. Child runs log their own metrics, and you can optionally log in to the parent run as well. For example, the following code snippet creates a child run, logs a metric named <strong class="source-inline">child_metric</strong> (which is only visible within that run), and then logs in the parent's metrics <strong class="source-inline">metric_from_child</strong>:</p>
			<p class="source-code">child_run = run.child_run()</p>
			<p class="source-code">child_run.log("child_metric", 0.01)</p>
			<p class="source-code">child_run.parent.log("metric_from_child", 0.02)</p>
			<p>Once you have completed the run, you need to change its <strong class="bold">Running</strong> status. You can use one of the following methods:</p>
			<ul>
				<li>The <strong class="source-inline">complete</strong> method indicates that the run was completed successfully. This method also uploads <a id="_idIndexMarker567"/>the <strong class="source-inline">outputs</strong> folder (if it exists) to the <strong class="source-inline">runs</strong> artifacts without <a id="_idIndexMarker568"/>needing to explicitly call the <strong class="source-inline">upload_folder</strong> method of the <strong class="source-inline">Run</strong> class.</li>
				<li>The <strong class="source-inline">cancel</strong> method indicates that the job was canceled. You will notice runs being canceled in AutoML Experiments because the timeout period was reached.</li>
				<li>The deprecated <strong class="source-inline">fail</strong> method indicates an error occurred.</li>
			</ul>
			<p>The following code snippet cancels the child run and completes the root run, printing the status, which should read <strong class="bold">Completed</strong>:</p>
			<p class="source-code">child_run.cancel()</p>
			<p class="source-code">run.complete()</p>
			<p class="source-code">print(run.get_status())</p>
			<p>In this section, you got an overview of the logging capabilities of AzureML. In the next section, you will refactor the code you created in the <em class="italic">Training a simple sklearn model within notebooks</em> section and add logging capabilities.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor122"/>Tracking model evolution</h2>
			<p>In the previous <a id="_idIndexMarker569"/>section, you may have noticed that the <strong class="source-inline">outputs</strong> folder that you created in the <em class="italic">Training a simple sklearn model within notebooks</em> section of this chapter was automatically uploaded to the run when you executed the <strong class="source-inline">complete</strong> method. To avoid uploading those stale artifacts, you will need to delete the <strong class="source-inline">outputs</strong> folder:</p>
			<ol>
				<li value="1">Add a cell in your <strong class="source-inline">chapter08.ipynb</strong> notebook and delete the <strong class="source-inline">outputs</strong> folder using <a id="_idIndexMarker570"/>the following code snippet:<p class="source-code">import shutil</p><p class="source-code">try:</p><p class="source-code">  shutil.rmtree("./outputs")</p><p class="source-code">except FileNotFoundError: </p><p class="source-code">  pass</p></li>
				<li>As a next step, you will refactor the training and evaluation code to a single method, passing in the <strong class="source-inline">alpha</strong> parameter and the <strong class="source-inline">training</strong> and <strong class="source-inline">validation</strong> datasets:<p class="source-code">from sklearn.linear_model import LassoLars</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">def train_and_evaluate(alpha, X_t, y_t, X_v, y_v):</p><p class="source-code">  model = LassoLars(alpha=alpha)</p><p class="source-code">  model.fit(X_t, y_t)</p><p class="source-code">  predictions = model.predict(X_v)</p><p class="source-code">  rmse = mean_squared_error(predictions, y_v, squared = False)</p><p class="source-code">  range_y_validate = y_v.to_numpy().ptp()</p><p class="source-code">  nrmse = rmse/range_y_validate</p><p class="source-code">  print(f"NRMSE: {nrmse}")</p><p class="source-code">  return model, nrmse</p><p class="source-code">trained_model, model_nrmse = train_and_evaluate(0.1, </p><p class="source-code">                        X_train, y_train,</p><p class="source-code">                        X_validate, y_validate) </p><p>This code is the exact equivalent of the code you wrote in the <em class="italic">Training a simple sklearn model within notebooks</em> section. You can now train multiple models using <strong class="source-inline">train_and_evaluate</strong> and passing different values for the <strong class="source-inline">alpha</strong> parameter, a process referred to as <em class="italic">hyperparameter tuning</em>. In the last line of this code snippet, you get a reference to the resulting trained model and its NRMSE metric.</p><p class="callout-heading">Important note</p><p class="callout">If you get an error as follows: <strong class="source-inline">NameError: name 'X_train' is not defined</strong>, you will need to rerun the cell of your notebook where you defined the <strong class="source-inline">X_train</strong>, <strong class="source-inline">y_train</strong>, <strong class="source-inline">X_validate</strong>, and <strong class="source-inline">y_validate</strong> variables. This is an indication that the Python kernel has restarted, and all the variables have been lost from memory.</p><p>So far, you have refactored the existing code and kept the same functionality. To enable logging through the <strong class="source-inline">Run</strong> class you explored in the previous section, you will need to pass the reference to the current run instance to the <strong class="source-inline">train_and_evaluate</strong> method. </p></li>
				<li>In a new cell, add<a id="_idIndexMarker571"/> the following snippet, which will override the existing declaration of the <strong class="source-inline">train_and_evaluate</strong> method:<p class="source-code">def train_and_evaluate(<strong class="bold">run</strong>, alpha, X_t, y_t, X_v, y_v):</p><p class="source-code">  model = LassoLars(alpha=alpha)</p><p class="source-code">  model.fit(X_t, y_t)</p><p class="source-code">  predictions = model.predict(X_v)</p><p class="source-code">  rmse = mean_squared_error(predictions, y_v, squared = False)</p><p class="source-code">  range_y_validate = y_v.to_numpy().ptp()</p><p class="source-code">  nrmse = rmse/range_y_validate</p><p class="source-code">  <strong class="bold">run.log("nrmse", nrmse)</strong></p><p class="source-code">  <strong class="bold">run.log_row("nrmse over α", α=alpha, nrmse=nrmse)</strong></p><p class="source-code">  return model, nrmse</p><p>Notice that you are using the <strong class="source-inline">log</strong> and <strong class="source-inline">log_row</strong> methods to log the NRMSE metric of the trained model.</p><p class="callout-heading">Important note</p><p class="callout">If you cannot type the <em class="italic">α</em> letter shown in the preceding example, you can use the <em class="italic">a</em> character instead.</p></li>
				<li>Having this <strong class="source-inline">train_and_evaluate</strong> method, you can do a hyperparameter tuning and train<a id="_idIndexMarker572"/> multiple models for multiple values of the <strong class="source-inline">α</strong> (<strong class="source-inline">alpha</strong>) parameter, using the following code:<p class="source-code">from azureml.core import Workspace, Experiment</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">exp = Experiment(workspace=ws, name="chapter08")</p><p class="source-code">with exp.start_logging() as run:</p><p class="source-code">    print(run.get_portal_url())</p><p class="source-code">    for a in [0.001, 0.01, 0.1, 0.25, 0.5]:</p><p class="source-code">        train_and_evaluate(run, a, </p><p class="source-code">                            X_train, y_train,</p><p class="source-code">                            X_validate, y_validate)</p><p>Note that instead of calling the <strong class="source-inline">complete</strong> method, we use the <strong class="source-inline">with .. as</strong> Python design pattern. As the <strong class="source-inline">run</strong> variables move out of scope, it is automatically marked as completed. </p></li>
				<li>Using the <strong class="source-inline">get_portal_url</strong> in <em class="italic">Step 4</em>, you printed the link to the studio's <strong class="bold">Experiment</strong> section of the run you just executed. Click on the link and open the run details and then open the <strong class="bold">Metrics</strong> section. You should see the metrics you just logged, <strong class="bold">nrmse</strong> showing the error increase during the 5 (x axis shows <strong class="bold">0</strong> to <strong class="bold">4</strong>) <strong class="source-inline">log</strong> method calls, while the <strong class="bold">nrmse over α</strong> shows the error increase as we increase the value of the <strong class="source-inline">α</strong> (<strong class="source-inline">alpha</strong>) parameter, something you logged using the <strong class="source-inline">log_row</strong> method. You should see graphs similar to the ones shown in <em class="italic">Figure 8.9</em>:</li>
			</ol>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="Images/B16777_08_009.jpg" alt="Figure 8.9 – Evolution of the nrmse metric for the diabetes model&#13;&#10;" width="1650" height="635"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Evolution of the nrmse metric for the diabetes model</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In this section, you are just storing the metrics on the <strong class="source-inline">Run</strong> instance and not the actual trained models. You could have stored the generated models by generating the <strong class="source-inline">.pkl</strong> file and then using the <strong class="source-inline">upload_file</strong> method to upload it in the run's artifacts. In <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>, you are going to learn about the model registry capabilities of the AzureML SDK, which provides a superior experience to keep track of the actual models.</p>
			<p>In this section, you <a id="_idIndexMarker573"/>saw how you can enable metric logging using the AzureML SDK. When it comes to tracking Experiment metrics, the data science community is using a popular open source framework called MLflow. In the next section, you will learn how to use that library to track metrics in the AzureML workspace.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor123"/>Using MLflow to track Experiments</h2>
			<p>The MLflow library is a<a id="_idIndexMarker574"/> popular open source library for managing the life <a id="_idIndexMarker575"/>cycle of your data science Experiments. This library allows you to store artifacts and metrics locally or on a server. The AzureML workspace provides an MLflow server that you can use to do the following:</p>
			<ul>
				<li>Track and log Experiment metrics through the <strong class="bold">MLflow</strong> <strong class="bold">Tracking</strong> component.</li>
				<li>Orchestrate code execution on AzureML compute clusters through the <strong class="bold">MLflow</strong> <strong class="bold">Projects</strong> component (similar to the pipelines you will see in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>).</li>
				<li>Manage models in the AzureML model registry, which you will see in <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>.</li>
			</ul>
			<p>In this section, you will focus on the MLflow Tracking component to track metrics. The following snippet uses the <strong class="source-inline">MLflow</strong> library to track the parameters and the metrics of the <strong class="source-inline">diabetes</strong> model you have created in the previous section under an Experiment named <strong class="source-inline">chapter08-mlflow</strong>:</p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">def train_and_evaluate(alpha, X_t, y_t, X_v, y_v):</p>
			<p class="source-code">  model = LassoLars(alpha=alpha)</p>
			<p class="source-code">  model.fit(X_t, y_t)</p>
			<p class="source-code">  predictions = model.predict(X_v)</p>
			<p class="source-code">  rmse = mean_squared_error(predictions, y_v, squared = False)</p>
			<p class="source-code">  range_y_validate = y_v.to_numpy().ptp()</p>
			<p class="source-code">  nrmse = rmse/range_y_validate</p>
			<p class="source-code">  mlflow.log_metric("nrmse", nrmse)</p>
			<p class="source-code">  return model, nrmse</p>
			<p class="source-code">mlflow.set_experiment("chapter08-mlflow")</p>
			<p class="source-code">with mlflow.start_run():</p>
			<p class="source-code">    mlflow.sklearn.autolog()</p>
			<p class="source-code">    trained_model, model_nrmse = train_and_evaluate(0.1, </p>
			<p class="source-code">                                    X_train, y_train,</p>
			<p class="source-code">                                    X_validate, y_validate)</p>
			<p>One of the<a id="_idIndexMarker576"/> most well-known features of the MLflow Tracking<a id="_idIndexMarker577"/> component is the automatic logging capabilities it provides. Calling the <strong class="source-inline">mlflow.sklearn.autolog()</strong> method before your training code enables automatic logging of <strong class="source-inline">sklearn</strong> metrics, params, and produced models. Similar to the <strong class="source-inline">autolog</strong> method specific to <strong class="source-inline">sklearn</strong>, there are packages for most of the common training frameworks, such as PyTorch, fast.ai, Spark, and others.</p>
			<p>Using the <strong class="source-inline">log_metric</strong> method, you explicitly ask the MLflow library to log a metric. In this case, you log the NRMSE metric, which is not captured automatically by the automatic logging capability.</p>
			<p>As you can see in <em class="italic">Figure 8.10</em> the MLflow Tracking component logs all artifacts and the trained model in a folder structure under the <strong class="source-inline">mlruns</strong> folder next to the notebook:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="Images/B16777_08_010.jpg" alt="Figure 8.10 – Tracking metrics using the local FileStore mode of the MLflow Tracking component&#13;&#10;" width="1506" height="1125"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Tracking metrics using the local FileStore mode of the MLflow Tracking component</p>
			<p>This is the <a id="_idIndexMarker578"/>default setting, referred to as <strong class="source-inline">local FileStore</strong>. You can use the AzureML workspace as a <em class="italic">remote tracking server</em>. To do <a id="_idIndexMarker579"/>so, you need to use the <strong class="source-inline">mlflow.set_tracking_uri()</strong> method to connect to a tracking URI.</p>
			<p>To enable the MLflow to AzureML integration, you need to ensure that your environment has the <strong class="source-inline">azureml-mlflow</strong> Python library. This package is already present in the AzureML compute instances. If you were working on a Databricks workspace, you would need to install it manually using the <strong class="source-inline">pip install azureml-mlflow</strong> command.</p>
			<p>To get the tracking <strong class="bold">URI</strong> and run the same Experiment using AzureML as the remote tracking server, use the following code snippet:</p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">from azureml.core import Workspace</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code"><strong class="bold">mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())</strong></p>
			<p class="source-code">mlflow.set_experiment("chapter08-mlflow")</p>
			<p class="source-code">with mlflow.start_run():</p>
			<p class="source-code">    mlflow.sklearn.autolog()</p>
			<p class="source-code">    trained_model, model_nrmse = train_and_evaluate(0.1, </p>
			<p class="source-code">                                    X_train, y_train,</p>
			<p class="source-code">                                    X_validate, y_validate)</p>
			<p>The <strong class="source-inline">get_mlflow_tracking_uri</strong> method of the <strong class="source-inline">Workspace</strong> class returns a URL that is <a id="_idIndexMarker580"/>valid for 1 hour. If your Experiment takes <a id="_idIndexMarker581"/>more than an hour to complete, you will need to generate a new URI and assign it using the <strong class="source-inline">set_tracking_uri</strong> method, as seen in the preceding snippet.</p>
			<p>You should be able to see the run and the tracked metrics in the Studio experience, as seen in <em class="italic">Figure 8.11</em>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="Images/B16777_08_011.jpg" alt="Figure 8.11 – Metrics logged using the MLflow library with AzureML as the remote tracking server&#13;&#10;" width="1650" height="785"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Metrics logged using the MLflow library with AzureML as the remote tracking server</p>
			<p>So far, you have<a id="_idIndexMarker582"/> been using the compute instance in the <a id="_idIndexMarker583"/>AzureML workspace, and you were training ML models in the <strong class="bold">Notebook</strong> kernel. This approach works well for small models or rapid prototypes over<a id="_idIndexMarker584"/> sample data. At some point, you will need to handle more demanding workloads, either with bigger memory requirements or even distributed training capabilities in multiple computer nodes. This can be achieved by delegating the training process to the compute clusters you created in <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>. In the next section, you will learn how to execute Python s<a id="_idTextAnchor124"/>cripts in your AzureML compute clusters.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor125"/>Scaling the training process with compute clusters</h1>
			<p>In <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>, you created a compute cluster named <strong class="source-inline">cpu-sm-cluster</strong>. In this <a id="_idIndexMarker585"/>section, you are going to<a id="_idIndexMarker586"/> submit a training job to be executed on that cluster. To do that, you will need to create a Python script that will be executed on the remote compute target.</p>
			<p>Navigate to the <strong class="bold">Notebooks</strong> section of your AzureML workspace and in the <strong class="bold">Files</strong> tree view, create a folder named <strong class="source-inline">greeter-job</strong> under the <strong class="source-inline">chapter08</strong> folder you have been working with so far. Add a Python file named <strong class="source-inline">greeter.py</strong>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="Images/B16777_08_012.jpg" alt="Figure 8.12 – Adding a simple Python script to execute on a remote compute cluster&#13;&#10;" width="659" height="563"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Adding a simple Python script to execute on a remote compute cluster</p>
			<p>Open<a id="_idIndexMarker587"/> that file <a id="_idIndexMarker588"/>and add the following code in it:</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument('--greet-name', type=str, </p>
			<p class="source-code">                    dest='name', help='The name to greet')</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p class="source-code">name = args.name</p>
			<p class="source-code">print(f"Hello {name}!")</p>
			<p>This script uses the <strong class="source-inline">ArgumentParser</strong> class from the <strong class="source-inline">argparse</strong> module to parse the parameters passed to the script. It is trying to locate a <strong class="source-inline">--greet-name</strong> parameter and assign the discovered value to the <strong class="source-inline">name</strong> attribute of the object it returns (<strong class="source-inline">args.name</strong>). Then, it prints a greeting message for the given name. To try the script, open a terminal and type the following:</p>
			<p class="source-code">python greeter.py --greet-name packt</p>
			<p>This command will produce the output seen in <em class="italic">Figure 8.13</em>:</p>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="Images/B16777_08_013.jpg" alt="Figure 8.13 – Testing the simple script you will execute on a remote compute&#13;&#10;" width="1286" height="456"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Testing the simple script you will execute on a remote compute</p>
			<p>To <a id="_idIndexMarker589"/>execute this simple Python script on a remote<a id="_idIndexMarker590"/> compute cluster, go back to the <strong class="source-inline">chapter08.ipynb</strong> notebook, add a new cell, and type the following code:</p>
			<p class="source-code">from azureml.core import Workspace, Experiment</p>
			<p class="source-code">from azureml.core import ScriptRunConfig</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">target = ws.compute_targets['cpu-sm-cluster']</p>
			<p class="source-code">script = ScriptRunConfig(</p>
			<p class="source-code">    source_directory='greeter-job',</p>
			<p class="source-code">    script='greeter.py',</p>
			<p class="source-code">    compute_target=target,</p>
			<p class="source-code">    arguments=['--greet-name', 'packt']</p>
			<p class="source-code">)</p>
			<p class="source-code">exp = Experiment(ws, 'greet-packt')</p>
			<p class="source-code">run = exp.submit(script)</p>
			<p class="source-code">print(run.get_portal_url())</p>
			<p class="source-code">run.wait_for_completion(show_output=True)</p>
			<p>In this code, you are<a id="_idIndexMarker591"/> doing the following:</p>
			<ol>
				<li value="1">Get a reference<a id="_idIndexMarker592"/> to the workspace, and then you assign to the <strong class="source-inline">target</strong> variable a reference to the <strong class="source-inline">cpu-sm-cluster</strong> cluster.</li>
				<li>Create a <strong class="source-inline">ScriptRunConfig</strong> to execute the <strong class="source-inline">greeter.py</strong> script that is located in the <strong class="source-inline">greeter-job</strong> folder. This script will execute in the <strong class="source-inline">target</strong> compute passing the <strong class="source-inline">--greet-name</strong> and <strong class="source-inline">packt</strong> arguments, which are going to be concatenated with a space between them.</li>
				<li>Create an Experiment called <strong class="source-inline">greet-packt</strong>, and you submit the script configuration to execute under this Experiment. The <strong class="source-inline">submit</strong> method creates a new <strong class="source-inline">Run</strong> instance. </li>
				<li>You use the <strong class="source-inline">get_portal_url</strong> method to get the portal URL for the specific <strong class="source-inline">Run</strong> instance. You then call the <strong class="source-inline">wait_for_completion</strong> method, setting the <strong class="source-inline">show_output</strong> parameter to <strong class="source-inline">True</strong>. To wait for the run to complete, turn on verbose logging and print the logs in the output of the cell.<p class="callout-heading">Important note</p><p class="callout">In the first version of the AzureML SDK, instead of <strong class="source-inline">ScriptRunConfig</strong>, you would have used the <strong class="source-inline">Estimator</strong> class, which is deprecated. Moreover, there are deprecated specialized <strong class="source-inline">Estimator</strong> classes for specific frameworks such as the <strong class="source-inline">TensorFlow</strong> class that provided a way to run TensorFlow-specific code. This approach has been deprecated in favor of the environments you will read about in the <em class="italic">Understanding execution environments</em> section that follows. Nonetheless, the syntax and the parameters of those deprecated classes are very similar to <strong class="source-inline">ScriptRunConfig</strong>. You should be able to read deprecated code without any issue. Keep that in mind if you see an old question in the certification exam referencing these deprecated classes.</p></li>
			</ol>
			<p>You have successfully<a id="_idIndexMarker593"/> completed a remote <a id="_idIndexMarker594"/>execution of a run. In the next section, you will explore the logs of the run you just completed and understand better the mechanics of AzureML.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor126"/>Exploring the outputs and logs of a run</h2>
			<p>In<a id="_idIndexMarker595"/> this section, you are going to explore the outputs of the<a id="_idIndexMarker596"/> remote execution you performed in the <em class="italic">Scaling the training process with compute clusters</em> section. This will give you insights into how the AzureML platform works and help you troubleshoot potential errors you will be facing while developing your training scripts.</p>
			<p>Open the link you printed in the previous section using the <strong class="source-inline">get_portal_url</strong> method or navigate to the <strong class="bold">Experiments</strong> section of the Studio interface, select the <strong class="source-inline">greet-packt</strong> Experiment, and open <strong class="bold">Run 1</strong>. Navigate to the <strong class="bold">Outputs + logs</strong> tab of the run:</p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="Images/B16777_08_014.jpg" alt="Figure 8.14 – Outputs + logs tab of an Experiment's run&#13;&#10;" width="1619" height="924"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Outputs + logs tab of an Experiment's run</p>
			<p>These <a id="_idIndexMarker597"/>outputs are very helpful in troubleshooting potential <a id="_idIndexMarker598"/>script errors. The <strong class="source-inline">azureml-logs</strong> folder contains the platform logs. Most of those files are logs from the underlying engine. The log that contains the standard output from your script is <strong class="source-inline">70_driver_log.txt</strong>. This is the log file you will need to look at first to troubleshoot a potential script execution failure. If you have multiple processes, you will see multiple files with a numeric suffix such as <strong class="source-inline">70_driver_log_x.txt</strong>.</p>
			<p>The <strong class="source-inline">logs</strong> folder is a special folder you can use in your scripts to output logs. Everything that the script writes in that folder will automatically be uploaded to the run's <strong class="bold">logs</strong>, similar to the <strong class="source-inline">outputs</strong> folder you saw in the <em class="italic">Tracking metrics in Experiments</em> section. AzureML also outputs system logs in that folder under the <strong class="source-inline">azureml</strong> folder you see in <em class="italic">Figure 8.14</em>.</p>
			<p>Navigate to the <strong class="bold">Snapshot</strong> tab. AzureML automatically makes a snapshot of the directory you specified in <strong class="source-inline">ScriptRunConfig</strong>. This directory can contain up to 300 MB and up to 2,000 files. If you need more script files, you can use a datastore. If you edited the script file in the <strong class="bold">Notebooks</strong> section, you would notice two files in there – the <strong class="source-inline">.py</strong> script and a <strong class="source-inline">.amltmp</strong> file, which is a temporary file used by the notebook editor:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="Images/B16777_08_015.jpg" alt="Figure 8.15 – Temporary file uploaded in the snapshot&#13;&#10;" width="925" height="452"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Temporary file uploaded in the snapshot</p>
			<p>To avoid <a id="_idIndexMarker599"/>creating snapshots of unwanted files, you can add a <strong class="source-inline">.gitignore</strong> or <strong class="source-inline">.amlignore</strong> file in the folder next to the script and exclude files that <a id="_idIndexMarker600"/>follow a specific pattern. Navigate to the <strong class="bold">Notebooks</strong> section and add a <strong class="source-inline">.amlignore</strong> file in the <strong class="source-inline">greeter-job</strong> folder, if the file is not already added when you created the folder, as seen in <em class="italic">Figure 8.16</em>: </p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="Images/B16777_08_016.jpg" alt="Figure 8.16 – Adding the .amlignore file to exclude temp files from being added to the snapshot&#13;&#10;" width="585" height="512"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Adding the .amlignore file to exclude temp files from being added to the snapshot</p>
			<p>Open the <strong class="source-inline">.amlignore</strong> file and add the following lines in it to exclude all files with a .a <strong class="source-inline">mltmp</strong> file <a id="_idIndexMarker601"/>extension and the <strong class="source-inline">.amlignore</strong> file that you<a id="_idIndexMarker602"/> are editing:</p>
			<p class="source-code">*.amltmp</p>
			<p class="source-code">.amlignore</p>
			<p>Open the <strong class="source-inline">chapter08.ipynb</strong> notebook, add a cell, and add the following code to resubmit the script:</p>
			<p class="source-code">from azureml.widgets import RunDetails</p>
			<p class="source-code">run = exp.submit(script)</p>
			<p class="source-code">RunDetails(run).show()</p>
			<p>You are resubmitting the existing instance of the <strong class="source-inline">ScriptRunConfig</strong> you created in the previous step. If you restarted<a id="_idIndexMarker603"/> the <strong class="bold">Jupyter</strong> kernel, run the previous cell to assign the <strong class="source-inline">exp</strong> and <strong class="source-inline">script</strong> variables once more. </p>
			<p>This time, you are using the <strong class="source-inline">RunDetails</strong> widget provided by the AzureML SDK. This is a <strong class="bold">Jupyter</strong> <strong class="bold">Notebook</strong> widget<a id="_idIndexMarker604"/> used to view the progress of a script execution. This widget is asynchronous and provides updates until the run finishes.</p>
			<p>If you want to print the run status, including the contents of the log files, you can use the following code snippet:</p>
			<p class="source-code">run.get_details_with_logs()</p>
			<p>Once the run completes, navigate to the <strong class="bold">Snapshot</strong> tab of that run. You will notice that the temp files are gone.</p>
			<p>Notice that the <a id="_idIndexMarker605"/>execution of this run took significantly less time to complete. Navigate <a id="_idIndexMarker606"/>to the run's log. Notice that the <strong class="source-inline">20_image_build_log.txt</strong> file did not appear in the logs this time, as seen in <em class="italic">Figure 8.17</em>: </p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="Images/B16777_08_017.jpg" alt="Figure 8.17 – Faster run execution and missing the 20_image_build_log.txt file&#13;&#10;" width="1650" height="766"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 8.17 – Faster run execution and missing the 20_image_build_log.txt file</p>
			<p>This is the <strong class="bold">Docker</strong> image-building log for the environment used to execute the scripts. This is a very time-consuming process. These images are built and stored in the container <a id="_idIndexMarker607"/>registry that got deployed with your AzureML workspace. Since <a id="_idIndexMarker608"/>you didn't modify the execution environment, AzureML reused the previously created image in the follow-up run. In the next section, you will understand better what an environment is and how you can modify it.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor127"/>Understanding execution environments</h2>
			<p>In the AzureML<a id="_idIndexMarker609"/> workspace terminology, an <strong class="bold">Environment</strong> means a<a id="_idIndexMarker610"/> list of software requirements needed for your scripts to execute. These software requirements include the following:</p>
			<ul>
				<li>The Python packages that your code requires to be installed</li>
				<li>The environment variables that may be needed from your code</li>
				<li>Various pieces of auxiliary software, such as GPU drivers or the <strong class="bold">Spark</strong> engine, that may be required for your code to operate properly</li>
			</ul>
			<p>Environments are <em class="italic">managed</em> and <em class="italic">versioned</em> entities that enable reproducible, auditable, and portable ML workflows across different compute targets.</p>
			<p>AzureML provides a list of <strong class="bold">curated environments</strong> for you to use for either training or inferencing. For <a id="_idIndexMarker611"/>example, the <strong class="source-inline">AzureML-Minimal</strong> curated environment contains just the minimal Python package requirements to enable run tracking you saw in the <em class="italic">Tracking model evolution</em> section. The <strong class="source-inline">AzureML-AutoML</strong> environment, on the other hand, is a much bigger curated environment and provides the required Python packages for your scripts to be able to run an AutoML Experiment. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">AzureML services are constantly being updated, and old environments are deprecated in favor of newer ones. Even if the <strong class="source-inline">AzureML-Minimal</strong> and <strong class="source-inline">AzureML-AutoML</strong> environments are not visible in the web interface of AzureML Studio, they should be available for you to use. If you encounter any errors, please download the latest code from the GitHub repository of this chapter.</p>
			<p>In <em class="italic">Figure 8.18</em>, you can see how many additional packages are available with the <strong class="source-inline">AzureML-AutoML</strong> environment compared to the minimalistic <strong class="source-inline">AzureML-Minimal</strong> one:</p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="Images/B16777_08_018.jpg" alt="Figure 8.18 – Python package difference between the AzureML-Minimal and &#13;&#10;AzureML-AutoML environments&#13;&#10;" width="1641" height="1004"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – Python package difference between the AzureML-Minimal and AzureML-AutoML environments</p>
			<p><em class="italic">Figure 8.18</em> shows the <strong class="source-inline">Conda</strong><a id="_idIndexMarker612"/> environment definition for the <strong class="source-inline">AzureML-Minimal</strong> environment <em class="italic">version 46</em> versus the <strong class="source-inline">AzureML-AutoML</strong> environment <em class="italic">version 61</em>. <strong class="bold">Conda</strong> is <a id="_idIndexMarker613"/>an open source package management system that allows you to define execution environments, mostly used for Python environments, using a <strong class="bold">Yet Another Markup Language</strong> (<strong class="bold">YAML</strong>) format like the one you see in <em class="italic">Figure 8.18</em>.  <strong class="source-inline">Conda</strong> takes this YAML <a id="_idIndexMarker614"/>file and installs Python <em class="italic">version 3.6.2</em> and the <strong class="source-inline">pip</strong> requirements listed beneath the <strong class="source-inline">- pip:</strong> notation. As you can notice, all <strong class="source-inline">pip</strong> packages have specific versions defined using the <strong class="source-inline">==x.x.x</strong> notation. This means that the same Python packages will be installed every time you use this YAML file, something that helps maintain a stable environment for the repeatability of your Experiments.</p>
			<p>Installing the packages when you create an environment is a time-consuming process. This is where the Docker technology you saw in the previous section comes in handy. Docker is an open source project for automating the deployment of applications as portable, self-sufficient containers. This means that instead of creating a new environment every time you want to run a script, you can create a Docker container image, also referred to as a Docker image, where all Python dependencies are <em class="italic">baked in</em> the image once. You can reuse the image from that point on to start a container and execute your scripts. In fact, all the AzureML-curated environments are available as Docker images in the <strong class="source-inline">viennaglobal.azurecr.io</strong> container registry.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although it is common to create Docker images for your environments, it is not always required. If you are running the Experiments on your local computer or locally on the AzureML compute instance, you can use an existing <strong class="source-inline">Conda</strong> environment and avoid using a Docker image. If you are planning to use a remote compute, for example, an AzureML compute cluster, a Docker image is required because otherwise, you cannot ensure that the provisioned machine will have all the software components needed by your code to execute.</p>
			<p>To better understand what you<a id="_idIndexMarker615"/> have read so far, you will rerun the previous <strong class="source-inline">greeter.py</strong> script using the <strong class="source-inline">AzureML-Minimal</strong> environmen:. </p>
			<ol>
				<li value="1">In your notebook, add a new cell and add the following code:<p class="source-code">from azureml.core import Environment</p><p class="source-code">minimal_env =\</p><p class="source-code">Environment.get(ws, name="AzureML-Minimal")</p><p class="source-code">print(minimal_env.name, minimal_env.version)</p><p class="source-code">print(minimal_env.Python.conda_dependencies.serialize_to_string())</p><p>This code retrieves the <strong class="source-inline">AzureML-Minimal</strong> environment, defined in the AzureML workspace referenced by the <strong class="source-inline">ws</strong> variable, which was initialized earlier in the notebook. Then, it prints the name and the version of the environment and the <strong class="source-inline">Conda</strong> environment YAML definition you saw in <em class="italic">Figure 8.18</em>.</p></li>
				<li>Add a new <a id="_idIndexMarker616"/>cell and type the following:<p class="source-code">from azureml.core import Experiment, ScriptRunConfig</p><p class="source-code">target = ws.compute_targets['cpu-sm-cluster']</p><p class="source-code">script = ScriptRunConfig(</p><p class="source-code">    source_directory='greeter-job',</p><p class="source-code">    script='greeter.py',</p><p class="source-code">    <strong class="bold">environment=minimal_env</strong>,</p><p class="source-code">    compute_target=target,</p><p class="source-code">    arguments=['--greet-name', 'packt']</p><p class="source-code">)</p><p class="source-code">exp = Experiment(ws, 'greet-packt')</p><p class="source-code">run = exp.submit(script)</p><p class="source-code">print(run.get_portal_url())</p><p class="source-code">run.wait_for_completion(show_output=True)</p><p>The only difference from the code you typed in the previous section is the definition of the <strong class="source-inline">environment</strong> argument in the <strong class="source-inline">ScriptRunConfig</strong> constructor.</p></li>
			</ol>
			<p>Observe the output of the run's execution. If you look closer, you will see the following line:</p>
			<p class="source-code">Status: Downloaded newer image for viennaglobal.azurecr.io/azureml/azureml_&lt;something&gt;:latest</p>
			<p>This line is part of the <strong class="source-inline">55_azureml-execution-something.txt</strong> file in <strong class="source-inline">azureml-logs</strong>. The line informs you that it is pulling a Docker image from the <strong class="source-inline">viennaglobal</strong> container registry, which <strong class="bold">Microsoft</strong> owns. In contrast to that, in the previous section, in the run where you didn't specify a curated environment, the image was pulled from your own container registry – the one provisioned with your AzureML workspace, as seen in <em class="italic">Figure 8.19</em>:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="Images/B16777_08_019.jpg" alt="Figure 8.19 – Image pulled from your own container registry &#13;&#10;in the execution without using a curated environment&#13;&#10;" width="1650" height="218"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – Image pulled from your own container registry in the execution without using a curated environment</p>
			<p>This observation brings us to the next type of AzureML-supported environment, the system-managed one – something you will explore in the next section. </p>
			<h3>Defining a system-managed environment</h3>
			<p>S<strong class="bold">ystem-managed</strong> environments<a id="_idIndexMarker617"/> allow you to define your code's dependencies by using either a <strong class="source-inline">Conda</strong> environment definition or a simple <strong class="source-inline">pip</strong> <strong class="source-inline">requirements.txt</strong> file. In the previous section, where you didn't define the <strong class="source-inline">environment</strong> argument in the <strong class="source-inline">ScriptRunConfig</strong> constructor, a default <strong class="source-inline">Conda</strong> environment definition file was used to create the system-managed environment that was stored in your <strong class="bold">Azure</strong> <strong class="bold">Container Registry</strong> associated with your AzureML workspace. Let's explicitly create a system-managed environment to use with your code:</p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">Notebooks</strong> section of your AzureML workspace and the <strong class="bold">Files</strong> tree view. </li>
				<li>Click on the three dots of the <strong class="source-inline">greeter-job</strong> folder to open the context menu (or just right-click on the name) and select the <strong class="bold">Duplicate</strong> action. Name the new folder <strong class="source-inline">greeter-banner-job</strong>, as seen in the following screenshot: <div id="_idContainer219" class="IMG---Figure"><img src="Images/B16777_08_020.jpg" alt="" width="1246" height="558"/></div><p class="figure-caption">Figure 8.20 – Duplicating the greeter-job folder as a new one named greeter-banner-job</p></li>
				<li>Open<a id="_idIndexMarker618"/> the <strong class="source-inline">greeter.py</strong> file in the new folder and change the code to the following:<p class="source-code">import argparse</p><p class="source-code"><strong class="bold">from asciistuff import Banner </strong></p><p class="source-code"><strong class="bold">import os</strong></p><p class="source-code">parser = argparse.ArgumentParser()</p><p class="source-code">parser.add_argument('--greet-name', type=str, </p><p class="source-code">                    dest='name', help='The name to greet')</p><p class="source-code">args = parser.parse_args()</p><p class="source-code">name = args.name</p><p class="source-code"><strong class="bold">greet_header = os.environ.get('GREET_HEADER','Message:')</strong></p><p class="source-code"><strong class="bold">print(greet_header)</strong></p><p class="source-code">print(<strong class="bold">Banner(</strong>f"Hello {name}!"<strong class="bold">)</strong>)</p><p>The modified code imports the <strong class="source-inline">Banner</strong> method from the <strong class="source-inline">asciistuff</strong> open source Python package. This method is used in the last print. This will output a fancy <strong class="bold">ASCII</strong> banner you will see in <em class="italic">Figure 8.21</em>. The code also imports the <strong class="source-inline">os</strong> module, which allows you to read the environment variables using the <strong class="source-inline">os.environ.get()</strong> method. The <a id="_idIndexMarker619"/>code tries to read the environment variable named <strong class="source-inline">GREET_HEADER</strong>, and if it is not defined, the default value, <strong class="source-inline">Message:</strong>, is assigned to the <strong class="source-inline">greet_header</strong> variable, which is printed before the banner message.</p><p class="callout-heading">Important note</p><p class="callout">If you try to execute the modified <strong class="source-inline">greeter.py</strong> in a terminal within your AzureML <em class="italic">compute instance</em>, it will fail because you don't have the <strong class="source-inline">asciistuff</strong> package installed. To install it in your compute instance, you can use the <strong class="source-inline">pip install asciistuff</strong> command.</p></li>
				<li>The <strong class="source-inline">asciistuff</strong> package is a pip package that you will need to install in your executing environment for your code to work. To define that code dependency, you are going to create a <strong class="source-inline">Conda</strong> environment definition file. In the <strong class="source-inline">chapter08</strong> folder, add a new file named <strong class="source-inline">greeter-banner-job.yml</strong>. Add the following content to it:<p class="source-code">name: banner-env</p><p class="source-code">dependencies:</p><p class="source-code">- python=3.6.2</p><p class="source-code">- pip:</p><p class="source-code">  - asciistuff==1.2.1 </p><p>This YAML file defines a new <strong class="source-inline">Conda</strong> environment named <strong class="source-inline">banner-env</strong>, which is based on Python <em class="italic">version 3.6.2</em> and installs the <em class="italic">1.2.1</em> version of the <strong class="source-inline">pip</strong> package, <strong class="source-inline">asciistuff</strong>.</p></li>
				<li>To create an AzureML environment based on the <strong class="source-inline">Conda</strong> environment you just defined, you need to go to the <strong class="source-inline">chapter08.ipynb</strong> notebook, add a cell, and type the following code:<p class="source-code">from azureml.core import Environment</p><p class="source-code">banner_env = Environment.from_conda_specification(</p><p class="source-code">                 name = "banner-env",</p><p class="source-code">                 file_path = "greeter-banner-job.yml")</p><p class="source-code">banner_env.environme<a id="_idTextAnchor128"/>nt_variables["GREET_HEADER"] = \</p><p class="source-code">                                 "Env. var. header:"</p><p>This code snippet creates an AzureML environment named <strong class="source-inline">banner-env</strong> using the <strong class="source-inline">from_conda_specification()</strong> method of the <strong class="source-inline">Environment</strong> class. The <strong class="source-inline">banner_env</strong> variable contains the newly defined environment. In the follow-up line, you define the <strong class="source-inline">GREET_HEADER</strong> environment variable, and you assign the <strong class="source-inline">Env. var. header:</strong> value. This environment is not registered in the workspace, and it <a id="_idIndexMarker620"/>doesn't need to be registered in order to use it. If you do want to save it in the workspace to be able to reference it in the same way you reference the curated environments and you want to keep versions of it, you can use the <strong class="source-inline">register()</strong> method, using the <strong class="source-inline">banner_env.register(ws)</strong> code where you pass as an argument a variable that points to the workspace where the Environment will be registered.</p><p class="callout-heading">Important note</p><p class="callout">If you plan to start working on your local computer and then scale out on more powerful compute clusters, you should consider creating and registering a system-managed environment that includes all your required Python packages. This will allow you to reuse it in both local and remote executions.</p></li>
				<li>To use this newly defined environment, add a new cell in the notebook and type the following code: <p class="source-code">script = ScriptRunConfig(</p><p class="source-code">    source_directory='<strong class="bold">greeter-banner-job</strong>',</p><p class="source-code">    script='greeter.py',</p><p class="source-code">    environment=<strong class="bold">banner_env</strong>,</p><p class="source-code">    compute_target=target,</p><p class="source-code">    arguments=['--greet-name', 'packt']</p><p class="source-code">)</p><p class="source-code">exp = Experiment(ws, 'greet-packt')</p><p class="source-code">run = exp.submit(script)</p><p class="source-code">print(run.get_portal_url())</p><p class="source-code">run.wait_for_completion(show_output=True)</p><p>Note that there are a couple of minor differences compared to the last <strong class="source-inline">ScriptRunConfig:</strong>:</p><ul><li>The source<a id="_idIndexMarker621"/> directory has changed to point to the <strong class="source-inline">greeter-banner-job</strong> folder, which contains the updated script.</li><li>The environment argument is specified, passing your very own defined <strong class="source-inline">banner_env</strong> environment. </li></ul></li>
			</ol>
			<p>The output of this Experiment should look like the one depicted in <em class="italic">Figure 8.21</em>:</p>
			<div>
				<div id="_idContainer220" class="IMG---Figure">
					<img src="Images/B16777_08_021.jpg" alt="Figure 8.21 – Header text read from an environment variable and banner-based hello greeting&#13;&#10;" width="1206" height="688"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.21 – Header text read from an environment variable and banner-based hello greeting</p>
			<p>As you noticed, in the <a id="_idIndexMarker622"/>system-managed environment you just created, you didn't specify anything about the base operating system (for example, whetherit's <strong class="bold">Ubuntu</strong> <em class="italic">16.04</em> or Ubuntu <em class="italic">20.04</em>). You assumed that <strong class="source-inline">Conda</strong> is already installed in the base system. You just specified the <strong class="source-inline">Conda</strong> dependencies that got installed. If you want even bigger flexibility, you can explicitly configure the environment and install all your software requirements manually. These environments are referred to<a id="_idIndexMarker623"/> as <strong class="bold">user-managed</strong> environments. Most often, these user-managed environments are custom-made Docker images that encapsulate all the required dependencies. For example, you may need a custom build of the PyTorch framework or even a custom build version of Python. In these cases, you are responsible for installing the Python packages and configuring the entire environment. For the purposes of this book, you will be working with either curated or system-managed environments.</p>
			<p>So far, you have explored how to execute a simple greeter Python application on a remote compute. In the next section, you will resume your <strong class="source-inline">diabetes</strong> model train<a id="_idTextAnchor129"/>ing and see how you can train that model on a remote compute cluster.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor130"/>Training the diabetes model on a compute cluster</h2>
			<p>In the<a id="_idIndexMarker624"/> previous section, you learned how you can run a script on a remote compute cluster by calling the <strong class="source-inline">exp.submit(script)</strong> method from within a notebook, as seen in <em class="italic">Figure 8.22</em>:</p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="Images/B16777_08_022.jpg" alt="Figure 8.22 – Executing a script on a compute cluster&#13;&#10;" width="1650" height="1228"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.22 – Executing a script on a compute cluster</p>
			<p>When you called the <strong class="source-inline">submit</strong> method, the following actions happened behind the scenes:</p>
			<ol>
				<li value="1">The AzureML SDK made a <strong class="bold">REST</strong> API call to the AzureML workspace to trigger the <strong class="source-inline">ScriptRunConfig</strong> execution.</li>
				<li>The AzureML workspace checked whether a Docker image of the <strong class="source-inline">Environment</strong> already exists. If it didn't exist, it was created within Azure Container Registry.</li>
				<li>The job is submitted to the compute cluster, which scales up to allocate a compute node. The following operations are performed within the newly allocated compute node:</li>
			</ol>
			<ol>
				<li value="1">The Docker image with the Environment is pulled to the compute node.</li>
				<li>The script referenced by <strong class="source-inline">ScriptRunConfig</strong> is loaded in the running Docker instance.</li>
				<li>Metrics and metadata are stored in the AzureML workspace.</li>
				<li>Outputs are stored back in the storage account.</li>
			</ol>
			<p>In the <em class="italic">Training a simple sklearn model with notebooks</em> section, you created a training <a id="_idIndexMarker625"/>script within the <strong class="source-inline">chapter08.ipynb</strong> notebook. The training was happening within the Jupyter server's process, inside your compute instance. To run the same training in a compute cluster, you will need to do the following:</p>
			<ol>
				<li value="1">Move the code to a Python script file.</li>
				<li>Create an AzureML environment to run the training.</li>
				<li>Submit <strong class="source-inline">ScriptRunConfig</strong> in an Experiment.</li>
			</ol>
			<p>In the next sections, you will see how to transform the script you used in the <em class="italic">Tracking model evolution</em> section to be able to execute it on a remote compute cluster.</p>
			<h3>Moving the code to a Python script file</h3>
			<p>If you look at the<a id="_idIndexMarker626"/> script you created in the <em class="italic">Tracking model evolution</em> section, in the code that was doing the training, you used the <strong class="source-inline">run</strong> variable to log metrics. This variable was referencing the <strong class="source-inline">Run</strong> object you got when you called <strong class="source-inline">exp.start_logging()</strong>. In the previous section, you learned about <strong class="source-inline">ScriptRunConfig</strong>, which you submitted in an Experiment and returned an instance of the <strong class="source-inline">Run</strong> class. This instance is created within the notebook of the compute instance. How will the script file that is executing on a remote cluster get access to the same <strong class="source-inline">Run</strong> object?</p>
			<p>AzureML's <strong class="source-inline">Run</strong> class provides a method called <strong class="source-inline">get_context()</strong>, which returns the current service execution context. In the case of <strong class="source-inline">ScriptRunConfig</strong>, this execution context is the same <strong class="source-inline">Run</strong> that was created when you called <strong class="source-inline">exp.submit(script)</strong>: </p>
			<p class="source-code">from azureml.core.run import Run</p>
			<p class="source-code">run = Run.get_context()</p>
			<p>Further to the <strong class="source-inline">run</strong> variable, in the training script, you had the <strong class="source-inline">ws</strong> variable, which was a reference to the AzureML workspace. You used that variable to get access to the <strong class="source-inline">diabetes</strong> dataset. You got a reference to the workspace by calling the <strong class="source-inline">from_config</strong> method. The issue with this approach is that the first time you called that method, you needed to manually authenticate and authorize the compute to access the workspace on your behalf. This will not be feasible to do on the remote compute.</p>
			<p>The <strong class="source-inline">run</strong> variable gives you access to the corresponding workspace by navigating in the Experiment attribute and then to the workspace attribute of that Experiment:</p>
			<p class="source-code">ws = run.experiment.workspace</p>
			<p>There is one <a id="_idIndexMarker627"/>caveat for these lines of code, though. Your code assumes that the Python script was submitted through <strong class="source-inline">ScriptRunConfig</strong>. If you run the Python script locally in a terminal, using the following command line, you will get an error:</p>
			<p class="source-code">python training.py --alpha 0.1</p>
			<p>The <strong class="source-inline">get_context()</strong> method will return an object of the <strong class="source-inline">_OfflineRun</strong> class, which inherits from the <strong class="source-inline">Run</strong> class. This class provides all logging capabilities you saw in the <em class="italic">Tracking metrics in Experiments</em> section, but instead of uploading the metrics or the artifacts to the workspace, it just prints out the attempt in the terminal. Obviously, there is no Experiment associated with that run and this is going to cause the script to throw an error. Thus, you need to retrieve the workspace reference using the <strong class="source-inline">from_config()</strong> method you have been using so far. Since the terminal is part of the compute instance, the script will execute passing your credentials and will not prompt you to authenticate, as you will see later in this section. If you run this code on your local computer, you will need to authenticate your device, as you saw in the <em class="italic">Authenticating from your device</em> section of <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p>
			<p>The complete code that allows you to run both offline in a terminal and submitted in a compute cluster is the following:</p>
			<p class="source-code">from azureml.core import Workspace</p>
			<p class="source-code">from azureml.core.run import Run, _OfflineRun</p>
			<p class="source-code">run = Run.get_context()</p>
			<p class="source-code">ws = None</p>
			<p class="source-code">if type(run) == _OfflineRun:</p>
			<p class="source-code">    ws = Workspace.from_config()</p>
			<p class="source-code">else:</p>
			<p class="source-code">    ws = run.experiment.workspace</p>
			<p>These are the<a id="_idIndexMarker628"/> only changes you will need to make to your script to submit it for remote execution and take advantage of the AzureML SDK capabilities.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Python developers commonly use an <strong class="source-inline">_</strong> as a prefix for classes, attributes, or methods that they want to mark as internal. This means that the marked code is for consumption by classes within the <strong class="source-inline">SDK</strong> library and shouldn't be used by external developers. The marked code may change in the future without any warning. It is considered a bad practice to use classes that start with the <strong class="source-inline">_</strong> prefix. Nonetheless, the <strong class="source-inline">_OfflineRun</strong> class is extensively used in the public samples of the AzureML SDK and is safe to use.</p>
			<p>Let's make those changes in your workspace. In the file tree, create a folder under <strong class="source-inline">chapter08</strong> named <strong class="source-inline">diabetes-training</strong> and add a <strong class="source-inline">training.py</strong> file in there, as seen in <em class="italic">Figure 8.23</em>:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="Images/B16777_08_023.jpg" alt="Figure 8.23 – Creating the training script for the remote diabetes model training&#13;&#10;" width="561" height="494"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.23 – Creating the training script for the remote diabetes model training</p>
			<p>Add the following <a id="_idIndexMarker629"/>code blocks in the <strong class="source-inline">training.py</strong> script. Instead of typing all this code, you can download it directly from the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section of this chapter:</p>
			<p class="source-code">from sklearn.linear_model import LassoLars</p>
			<p class="source-code">from sklearn.metrics import mean_squared_error</p>
			<p class="source-code">from azureml.core import Workspace</p>
			<p class="source-code">from azureml.core.run import Run, _OfflineRun</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">import joblib</p>
			<p>These are all the imports you will need within the script file. It is a good practice to have all your <strong class="source-inline">import</strong> statements on the top of your script files to easily discover the required modules needed for your code to execute properly. If you use <strong class="source-inline">flake8</strong> to lint your code base, it will complain if you don't follow this best practice:</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument('--alpha', type=float, </p>
			<p class="source-code">                  dest='<strong class="bold">alpha</strong>', help='The alpha parameter')</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p>This script file expects an <strong class="source-inline">--alpha</strong> parameter to be passed to it. In this code block, this parameter is parsed using the <strong class="source-inline">argparse</strong> module you saw in the <em class="italic">Scaling the training process with compute clusters</em> section, and the <strong class="source-inline">float</strong> value is assigned to the <strong class="source-inline">args.alpha</strong> variable, as it is specified in the <strong class="source-inline">dest</strong> argument. The <strong class="source-inline">parse_args</strong> method will throw<a id="_idIndexMarker630"/> an error if you pass non-defined arguments to the script. Some people prefer using <strong class="source-inline">args, unknown_args = parser.parse_known_args()</strong> instead of the fourth line of this code block, which allows the script to execute even if it receives more than the expected arguments, assigning the unknown ones in the <strong class="source-inline">unknown_args</strong> variable:</p>
			<p class="source-code">run = Run.get_context()</p>
			<p class="source-code">ws = None</p>
			<p class="source-code">if type(run) == _OfflineRun:</p>
			<p class="source-code">    ws = Workspace.from_config()</p>
			<p class="source-code">else:</p>
			<p class="source-code">    ws = run.experiment.workspace</p>
			<p>In this code block, you get a reference to the <strong class="source-inline">Run</strong> object and the <strong class="source-inline">Workspace</strong> using the snippet you saw at the beginning of this section. Once you get the reference to the <strong class="source-inline">Workspace</strong>, you can load the <strong class="source-inline">diabetes</strong> dataset, as seen in the next script block:</p>
			<p class="source-code">diabetes_ds = ws.datasets['diabetes']</p>
			<p class="source-code">training_data, validation_data = \</p>
			<p class="source-code">               diabetes_ds.random_split(</p>
			<p class="source-code">                            percentage = 0.8, <strong class="bold">seed</strong>=1337)</p>
			<p class="source-code">X_train = training_data.drop_columns('target') \</p>
			<p class="source-code">                       .to_pandas_dataframe()</p>
			<p class="source-code">y_train = training_data.keep_columns('target') \</p>
			<p class="source-code">                       .to_pandas_dataframe()</p>
			<p class="source-code">X_validate = validation_data.drop_columns('target') \</p>
			<p class="source-code">                            .to_pandas_dataframe()</p>
			<p class="source-code">y_validate = validation_data.keep_columns('target') \</p>
			<p class="source-code">                            .to_pandas_dataframe()</p>
			<p>In this block, you get a reference to the <strong class="source-inline">diabetes</strong> dataset and split it to the required <strong class="source-inline">X_train</strong>, <strong class="source-inline">y_train</strong>, <strong class="source-inline">X_validate</strong>, and <strong class="source-inline">y_validate</strong> pandas DataFrames you saw in the <em class="italic">Training a simple sklearn model within notebooks</em> section of this chapter. Note that you specify the <strong class="source-inline">seed</strong> parameter in the <strong class="source-inline">random_split</strong> method. This <strong class="source-inline">seed</strong> parameter is used to initialize the state of the underlying random function used by the <strong class="source-inline">split</strong> method to<a id="_idIndexMarker631"/> randomly select the rows from the dataset. By doing that, the random function will generate the same random numbers every time it is invoked. This means that <strong class="source-inline">training_data</strong> and <strong class="source-inline">validation_data</strong> will be the same every time you run the script. Having the same training and validation dataset will assist in properly comparing multiple executions of the same script with different <strong class="source-inline">alpha</strong> parameters:</p>
			<p class="source-code">def train_and_evaluate(run, alpha, X_t, y_t, X_v, y_v):</p>
			<p class="source-code">  model = LassoLars(alpha=alpha)</p>
			<p class="source-code">  model.fit(X_t, y_t)</p>
			<p class="source-code">  predictions = model.predict(X_v)</p>
			<p class="source-code">  rmse = mean_squared_error(predictions,y_v,squared=False)</p>
			<p class="source-code">  range_y_validate = y_v.to_numpy().ptp()</p>
			<p class="source-code">  nrmse = rmse/range_y_validate</p>
			<p class="source-code">  run.log("nrmse", nrmse)</p>
			<p class="source-code">  run.log_row("nrmse over α", α=alpha, nrmse=nrmse)</p>
			<p class="source-code">  return model, nrmse</p>
			<p>In this code block, you define the <strong class="source-inline">train_and_evaluate</strong> method, which is the same one used in the <em class="italic">Tracking model evolution</em> section of this chapter:</p>
			<p class="source-code">model, nrmse = train_and_evaluate(run, args.alpha,</p>
			<p class="source-code">                  X_train, y_train, X_validate, y_validate)</p>
			<p>After the method<a id="_idIndexMarker632"/> definition, you invoke the training process passing all the required arguments:</p>
			<p class="source-code">os.makedirs('./outputs', exist_ok=True)</p>
			<p class="source-code">model_file_name = 'model.pkl'</p>
			<p class="source-code">joblib.dump(value=model, filename=</p>
			<p class="source-code">           os.path.join('./outputs/',model_file_name))</p>
			<p>The last code block stores the model in the <strong class="source-inline">outputs</strong> folder next to the script's location.</p>
			<p>You can run the script on your local compute instance, and you will notice that the model trains as expected and the metrics are logged in the terminal, as seen in <em class="italic">Figure 8.24</em>. This is the expected behavior of the <strong class="source-inline">_OfflineRun</strong> class you read about before: </p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="Images/B16777_08_024.jpg" alt="Figure 8.24 – Running the training script locally&#13;&#10;" width="1650" height="120"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.24 – Running the training script locally</p>
			<p>So far, you have created the training script. In the next section, you will create the AzureML environment that will contain all the required dependencies to execute that script on a remote compute.</p>
			<h3>Creating the AzureML environment to run the training script</h3>
			<p>The training script <a id="_idIndexMarker633"/>you created in the <em class="italic">Tracking model evolution</em> section uses the <strong class="source-inline">scikit-learn</strong> library, also known as <strong class="source-inline">sklearn</strong>. The Jupyter kernel that you are using in the notebook experience already has the <strong class="source-inline">sklearn</strong> library installed. To see the version that is currently installed in your kernel, go to the <strong class="source-inline">chapter08.ipynb</strong> notebook and add the following code snippet in a new cell:</p>
			<p class="source-code">!pip show scikit-learn</p>
			<p>This command will use Python's <strong class="source-inline">pip</strong> package manager to show the details of the currently installed <strong class="source-inline">scikit-learn</strong> package, as seen in <em class="italic">Figure 8.25</em>:</p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="Images/B16777_08_025.jpg" alt="Figure 8.25 – Package information for the installed scikit-learn library&#13;&#10;" width="1089" height="464"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25 – Package information for the installed scikit-learn library</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you are unsure of the library name, you can use the <strong class="source-inline">pip freeze</strong> command to get a full list of installed packages in the current Python environment.</p>
			<p>You can also find the <a id="_idIndexMarker634"/>version of the installed library within a Python script using the <strong class="source-inline">sklearn.__version__</strong> attribute (note the two underscores). In a new notebook cell, add the following lines of Python code:</p>
			<p class="source-code">import sklearn</p>
			<p class="source-code">print(sklearn.__version__)</p>
			<p>You should be able to see exactly the same version printed in the output. Most of the Python SDKs and libraries have this <strong class="source-inline">__version__</strong> attribute, such as the PyTorch and the TensorFlow frameworks.</p>
			<p>There are two ways to install the <strong class="source-inline">scikit-learn</strong> package; as a <strong class="source-inline">Conda</strong> package or as a <strong class="source-inline">pip</strong> package. <strong class="source-inline">Conda</strong> offers a curated list of Python packages, and it is the recommended approach. In the <em class="italic">Understanding execution environments</em> section, you saw how to create an environment using a <strong class="source-inline">Conda</strong> specification file. In this section, you will learn a different<a id="_idIndexMarker635"/> approach where you create the environment within the Python code. Add a new cell in the <strong class="source-inline">chapter08.ipynb</strong> notebook and type the following:</p>
			<p class="source-code">from azureml.core import Environment</p>
			<p class="source-code">from azureml.core.conda_dependencies import CondaDependencies </p>
			<p class="source-code">import sklearn</p>
			<p class="source-code">diabetes_env = Environment(name="diabetes-training-env")</p>
			<p class="source-code">diabetes_env.Python.conda_dependencies = CondaDependencies()</p>
			<p class="source-code">diabetes_env.Python.conda_dependencies.add_conda_package(</p>
			<p class="source-code">                   f"scikit-learn=={sklearn.__version__}")</p>
			<p class="source-code">diabetes_env.python.conda_dependencies.add_pip_package("azureml-dataprep[pandas]")</p>
			<p>In the preceding code snippet, you create a new system-managed environment and then use <strong class="source-inline">add_conda_package</strong> to add the specific version of <strong class="source-inline">scikit-learn</strong>. You also use <strong class="source-inline">add_pip_package</strong> to add the <strong class="source-inline">azureml-dataprep[pandas]</strong> package, which is required in order to use the <strong class="source-inline">to_pandas_dataframe</strong> method within the <strong class="source-inline">training.py</strong> script. You could have added additional pip packages such as the <strong class="source-inline">asciistuff</strong> package you installed before. Instead of adding one package at a time using the <strong class="source-inline">add_pip_package</strong> method, you can use the <strong class="source-inline">create</strong> method of the <strong class="source-inline">CondaDependencies</strong> class, as seen in the following snippet:</p>
			<p class="source-code">diabetes_env.Python.conda_dependencies = \</p>
			<p class="source-code">CondaDependencies.create(</p>
			<p class="source-code">      conda_packages=[</p>
			<p class="source-code">                   f"scikit-learn=={sklearn.__version__}"],</p>
			<p class="source-code">      pip_packages=["azureml-defaults", "azureml-dataprep[pandas]"])</p>
			<p>You can request for multiple packages to be present in the environment by adding them in the <strong class="source-inline">conda_packages</strong> and <strong class="source-inline">pip_packages</strong> arrays. Note that since you do not append packages to the default <strong class="source-inline">CondaDependencies</strong>, you need to manually include the <strong class="source-inline">azureml-defaults</strong> package needed for the <strong class="source-inline">training.py</strong> script to access the <strong class="source-inline">azureml.core</strong> module.</p>
			<p>You may be wondering<a id="_idIndexMarker636"/> why we haven't defined <strong class="source-inline">joblib</strong> in the Python dependencies. The <strong class="source-inline">scikit-learn</strong> package depends on the <strong class="source-inline">joblib</strong> package, and it will automatically be installed in the environment. If you want, you can explicitly specify it in the list of dependencies with the following code:</p>
			<p class="source-code">import joblib</p>
			<p class="source-code">diabetes_env.Python.conda_dependencies.add_pip_package(f"joblib=={joblib.__version__}")</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Although it is not mandatory to specify the version of the packages you want to add to the environment, it is a good practice. If you wrote <strong class="source-inline">add_conda_package("scikit-learn")</strong>, skipping to specify the version of the package, AzureML would assume you are referring to the latest version. The first time you would have used the environment in AzureML, the Docker image would have been created, installing whatever was the newest version of the <strong class="source-inline">scikit-learn</strong> package at the time of the Docker image creation. That version may have been more recent than the one you used to create your script, and it may be incompatible with the code you wrote. Although minor version differences may not affect your code, major versions may introduce breaking changes, as was done when TensorFlow moved from <em class="italic">version 1</em> to <em class="italic">2</em>. </p>
			<p>If you don't want to create a new environment with your code dependencies, you can use one of the AzureML-curated environments. You can select either the highly specialized GPU-based <strong class="source-inline">AzureML-Scikit-learn0.24-Cuda11-OpenMpi4.1.0-py36</strong> environment or you can use the more generic <strong class="source-inline">AzureML-Tutorial</strong> curated <a id="_idIndexMarker637"/>environment, which contains the most used data science libraries such as <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">MLflow</strong>, and <strong class="source-inline">matplotlib</strong>.</p>
			<p>So far, you have written the training script and you defined the AzureML environment with the required <strong class="source-inline">sklearn</strong> library. In the next section, you are going to kick off the training on a compute cluster.</p>
			<h3>Submitting ScriptRunConfig in an Experiment</h3>
			<p>Once you have<a id="_idIndexMarker638"/> the script and the AzureML environment definition, you can submit <strong class="source-inline">ScriptRunConfig</strong> to execute on the remote compute cluster. In a new cell in the <strong class="source-inline">chapter08.ipynb</strong> notebook, add the following code:</p>
			<p class="source-code">from azureml.core import Workspace, Experiment</p>
			<p class="source-code">from azureml.core import ScriptRunConfig</p>
			<p class="source-code">ws <a id="_idTextAnchor131"/>= Workspace.from_config()</p>
			<p class="source-code">target = ws.compute_targets['cpu-sm-cluster']</p>
			<p class="source-code">script = ScriptRunConfig(</p>
			<p class="source-code">    source_directory='diabetes-training',</p>
			<p class="source-code">    script='training.py',</p>
			<p class="source-code">    environment=diabetes_env,</p>
			<p class="source-code">    compute_target=target,</p>
			<p class="source-code">    arguments=['--alpha', 0.01]</p>
			<p class="source-code">)</p>
			<p class="source-code">exp = Experiment(ws, 'chapter08-diabetes')</p>
			<p class="source-code">run = exp.submit(script)</p>
			<p class="source-code">run.wait_for_completion(show_output=True)</p>
			<p>This code is the same one used to submit the <strong class="source-inline">greeter.py</strong> scripts in the previous sections. You get a reference to the AzureML workspace and the compute cluster where you will execute the job. You define a <strong class="source-inline">ScriptRunConfig</strong> object where you define the location of the script to execute, the environment you defined in the previous section, and the target compute. You also pass the <strong class="source-inline">alpha</strong> argument to the script. In the last bit of code, you create an Experiment and submit <strong class="source-inline">ScriptRunConfig</strong> to execute.</p>
			<p>With this piece of code, you triggered the flow you saw in <em class="italic">Figure 8.22</em> in the <em class="italic">Training the diabetes model on a compute cluster</em> section earlier in the chapter.</p>
			<p>Once the<a id="_idIndexMarker639"/> training is complete, you will be able to navigate to the Experiment, select the run<strong class="source-inline">,</strong> and observe the collected metrics from the training process, as seen in <em class="italic">Figure 8.26</em>:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="Images/B16777_08_026.jpg" alt="Figure 8.26 – Logged metrics from a script running on a remote compute cluster&#13;&#10;" width="1650" height="1126"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.26 – Logged metrics from a script running on a remote compute cluster</p>
			<p>So far, you have managed to execute the <strong class="source-inline">diabetes</strong> model training script in a single node on a remote compute cluster, and you have logged the metrics and the trained model in the <a id="_idIndexMarker640"/>AzureML Experiment's run. </p>
			<p>In the next section, you will discover different ways to scale out your computational efforts and take advantage of more than a single node on the compute cluster.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor132"/>Utilizing more than a single compute node during model training</h2>
			<p>As you saw in the <em class="italic">Compute clusters</em> section of <a href="B16777_04_Final_VK_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Configuring the Workspace</em>, a cluster<a id="_idIndexMarker641"/> can scale from 0 compute nodes to as many as you like. There are a couple of reasons why you would need more than a single node in a cluster during the model training phase. They are as follows:</p>
			<ul>
				<li><strong class="bold">Parallel execution of unrelated model training instances</strong>: When you are working in a team, it is common to have multiple Experiments running in parallel. Each job can run on a single node, as you did in the previous section.</li>
				<li><strong class="bold">Parallel training of a single model, also known as distributed training</strong>: This is an advanced scenario where you are using frameworks such as the <strong class="bold">Apache</strong> <strong class="bold">Horovod</strong> distributed <a id="_idIndexMarker642"/>deep learning training framework that PyTorch and TensorFlow use. There are two types of distributed training options:<ul><li><strong class="bold">Data parallelism</strong>: Where the<a id="_idIndexMarker643"/> training data is split into partitions equal to the amount of compute nodes you have. Each node performs a training batch of the model against the assigned data, and then all nodes synchronize the updated model parameters before moving to the next batch.</li><li><strong class="bold">Model parallelism</strong>: Where you<a id="_idIndexMarker644"/> are training bits of the model on different compute nodes. Each node is responsible for training only a small segment of the entire model, and the synchronization between nodes occurs every time a propagation step is needed. </li></ul></li>
				<li><strong class="bold">Parallel training of multiple instances of the same model to select the best alternative</strong>: Models may accept parameters when they are initialized, such as the <strong class="source-inline">alpha</strong> parameter of the <strong class="source-inline">LassoLars</strong> model you trained in the previous section. You may want to explore multiple values for those parameters <a id="_idIndexMarker645"/>to select the model that performs best on the training dataset. This is a process called hyperparameter tuning, and you will learn more about it in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>.</li>
				<li><strong class="bold">Parallel training of multiple models to select the best alternative</strong>: This is the AutoML process you already discovered in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. You will also see this method again in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model,</em> in the <em class="italic">Running AutoML Experiments with code</em> section.</li>
			</ul>
			<p>In this section, you learned about different approaches to utilize multiple nodes in a compute cluster. You will deep dive into the last two methods in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor133"/>Summary</h1>
			<p>In this chapter, you got an overview of the various ways you can create an ML model in the AzureML workspace. You started with a simple regression model that was trained within the Jupyter notebook's kernel process. You learned how you can keep track of the metrics from the models you train. Then, you scaled the training process into the <strong class="source-inline">cpu-sm-cluster</strong> compute cluster you created in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. While scaling out to a remote compute cluster, you learned what the AzureML environments are and how you can troubleshoot remote executions by looking at the logs.</p>
			<p>In the next chapter, you will build on this knowledge and use multiple computer nodes to perform a parallelized <em class="italic">hyperparameter tuning</em> process, which will locate the best parameters for your model. You will also learn how you can completely automate the model selection, training, and tuning using the AutoML capabilities of the AzureML SDK.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor134"/>Questions</h1>
			<p>In each chapter, you will find a couple of questions to check your understanding of the topics discussed:.</p>
			<ol>
				<li value="1">You want to log the number of validation rows you will use within a script. Which method of the <strong class="source-inline">Run</strong> class will you use?<p>a. <strong class="source-inline">log_table</strong></p><p>b. <strong class="source-inline">log_row</strong></p><p>c. <strong class="source-inline">log</strong></p></li>
				<li>You want to run a Python script that utilizes <strong class="source-inline">scikit-learn</strong>. How would you configure the AzureML environment?<p>a. Add the <strong class="source-inline">scikit-learn Conda dependency.</strong> </p><p>b. Add the <strong class="source-inline">sklearn Conda dependency.</strong></p><p>c. Use the AzureML <strong class="source-inline">Azure-Minimal</strong> environment, which already contains the needed dependencies.</p></li>
				<li>You need to use <strong class="source-inline">MLflow</strong> to track the metrics generated in an Experiment and store them in your AzureML workspace. Which two pip packages do you need to have in your Conda environment?<p>a. <strong class="source-inline">mlflow</strong></p><p>b. <strong class="source-inline">azureml-mlflow</strong></p><p>c. <strong class="source-inline">sklearn</strong></p><p>d. <strong class="source-inline">logger</strong></p></li>
				<li>You need to use <strong class="source-inline">MLflow</strong> to track the value <strong class="source-inline">0.1</strong> for the <strong class="source-inline">training_rate</strong> metric. Which of the following code achieves this requirement? Assume all classes are correctly imported at the top of the script:<p>a. <strong class="source-inline">mlflow.log_metric('training_rate', 0.1)</strong></p><p>b. <strong class="source-inline">run.log('training_rate', 0.1)</strong></p><p>c. <strong class="source-inline">logger.log('training_rate', 0.1)</strong></p></li>
			</ol>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor135"/>Further reading</h1>
			<p>This section offers a list of web resources to help you augment your knowledge of the AzureML SDK and the various code snippets used in this chapter:</p>
			<ul>
				<li>Source of the diabetes dataset: <a href="https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html">https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html</a></li>
				<li><em class="italic">LassoLars</em> model documentation on <em class="italic">scikit-learn</em> website: <a href="https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso">https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso</a></li>
				<li>The <em class="italic">plotly</em> open source graphing library: <a href="https://github.com/plotly/plotly.py">https://github.com/plotly/plotly.py</a></li>
				<li>MLflow Tracking API reference: <a href="https://mlflow.org/docs/latest/quickstart.html#using-the-tracking-api">https://mlflow.org/docs/latest/quickstart.html#using-the-tracking-api</a></li>
				<li>Syntax for the <strong class="source-inline">.amlignore</strong> and .<strong class="source-inline">gitignore</strong> files: <a href="https://git-scm.com/docs/gitignore">https://git-scm.com/docs/gitignore</a></li>
				<li><strong class="bold">Flake8</strong> for code linting: <a href="https://flake8.pycqa.org">https://flake8.pycqa.org</a></li>
			</ul>
		</div>
	</div></body></html>