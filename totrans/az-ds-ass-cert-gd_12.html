<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer239">
			<h1 id="_idParaDest-129"><em class="italic"><a id="_idTextAnchor136"/>Chapter 9</em>: Optimizing the ML Model</h1>
			<p>In this chapter, you will learn about two techniques you can use to discover the optimal model for your dataset. You will start by exploring the <strong class="bold">HyperDrive</strong> package of the AzureML SDK. This <a id="_idIndexMarker646"/>package allows you to fine-tune the model's performance by tweaking the parameters it exposes, a process also known as <strong class="bold">hyperparameter tuning</strong>. You will then explore the <strong class="bold">Automated ML</strong> (<strong class="bold">AutoML</strong>) package of the<a id="_idIndexMarker647"/> AzureML SDK, which allows you to automate the model selection, training, and optimization process through code.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Hyperparameter tuning using HyperDrive</li>
				<li>Running AutoML experiments with code</li>
			</ul>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor137"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control (IAM)</strong> role on the resource group level. Within that resource group, you should have already deployed a <strong class="bold">machine learning</strong> resource named <strong class="source-inline">packt-learning-mlw</strong>, as described in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>You will also need to have a basic understanding of the <strong class="bold">Python</strong> language. The code snippets target Python version 3.6 or newer. You should also be familiar with working in the notebook experience within AzureML Studio, something that was covered in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>.</p>
			<p>This chapter assumes you have registered the <strong class="bold">scikit-learn</strong> diabetes dataset in your AzureML workspace and that you have created a compute cluster named <strong class="bold">cpu-sm-cluster</strong>, as described in the sections <em class="italic">Defining datastores</em>, <em class="italic">Working with datasets</em>, and <em class="italic">Working with compute targets</em> in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. </p>
			<p>You can find all notebooks and code snippets of this chapter in GitHub at the URL <a href="http://bit.ly/dp100-ch09">http://bit.ly/dp100-ch09</a>.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor138"/>Hyperparameter tuning using HyperDrive</h1>
			<p>In <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, you trained a <strong class="source-inline">LassoLars</strong> model that was accepting the <strong class="source-inline">alpha</strong> parameter. In order to avoid overfitting to the training dataset, the <strong class="source-inline">LassoLars</strong> model uses a <a id="_idIndexMarker648"/>technique called <strong class="bold">regularization</strong>, which basically introduces a penalty term within the optimization formula of<a id="_idIndexMarker649"/> the model. You can think of this technique as if the linear regression that we are trying to fit consists of a normal linear function that is being fitted with the least-squares function plus this penalty term. The <strong class="source-inline">alpha</strong> parameter specifies how important this penalty term is, something that directly impacts the training outcome. Parameters that affect the training process are referred to as <a id="_idIndexMarker650"/>being <strong class="bold">hyperparameters</strong>. To understand better what a <strong class="bold">hyperparameter</strong> is, we are going to explore the hyperparameters of a decision tree. In a decision tree classifier model, like the <strong class="source-inline">DecisionTreeClassifier</strong> class located in the <strong class="source-inline">scikit-learn</strong> library, you can define the maximum depth of the tree through the <strong class="bold">hyperparameter</strong> <strong class="source-inline">max_depth</strong>, which is an integer. In the same model, you can control the maximum amount of leaf nodes by specifying a numeric value to the <strong class="source-inline">max_leaf_nodes</strong> <strong class="bold">hyperparameter</strong>. </p>
			<p>These hyperparameters control the size of the decision tree, as depicted in <em class="italic">Figure 9.1</em>:</p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="Images/B16777_09_001.jpg" alt="Figure 9.1 – Decision tree hyperparameters&#13;&#10;" width="1650" height="505"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Decision tree hyperparameters</p>
			<p><strong class="bold">Hyperparameter tuning</strong> is<a id="_idIndexMarker651"/> the process of finding the optimal values for the <strong class="bold">hyperparameters</strong> that produce the best-performing model <a id="_idIndexMarker652"/>against the data you are using for training. To be able to evaluate the performance of each <strong class="bold">hyperparameter</strong> combination, the model must be trained, and the performance metric must be evaluated. In the case of the diabetes model in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, you<a id="_idIndexMarker653"/> were evaluating the models using the <strong class="bold">Normalized Root Mean Squared Error</strong> (<strong class="bold">NRMSE</strong>) metric.</p>
			<p>The AzureML SDK offers the <strong class="source-inline">HyperDriveConfig</strong> class , which allows you to perform <strong class="bold">hyperparameter tuning</strong> for your models, parallelizing the search for the best <strong class="bold">hyperparameter</strong> combination by performing model training and evaluation at each node of the compute cluster in parallel. <strong class="source-inline">HyperDriveConfig</strong> is a wrapper to the <strong class="source-inline">ScriptRunConfig</strong> class you used in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. This means that you need to pass in the <strong class="source-inline">run_config</strong> parameter the <strong class="source-inline">ScriptRunConfig</strong> that you want to use to train your model. You also need to specify the metric that your code is logging and what your goal is for that metric. In the diabetes case, you are trying to minimize the <strong class="bold">NRMSE</strong> metric. You can then kick off a <strong class="bold">hyperparameter tuning</strong> process with the same <strong class="source-inline">submit</strong> method you saw in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. The pseudo-code that shows the end-to-end process, where the <strong class="source-inline">script</strong> variable refers to the <strong class="source-inline">ScriptRunConfig</strong> object that defines which training script you are going to use, is the following:</p>
			<p class="source-code">hd_config = HyperDriveConfig(</p>
			<p class="source-code">             run_config=script,</p>
			<p class="source-code">             primary_metric_name="nrmse",</p>
			<p class="source-code">             primary_metric_goal=PrimaryMetricGoal.MINIMIZE</p>
			<p class="source-code">             ,…)</p>
			<p class="source-code">experiment = Experiment(ws, "chapter09-hyperdrive")</p>
			<p class="source-code">hyperdrive_run = experiment.submit(hd_config)</p>
			<p>Besides <strong class="source-inline">ScriptRunConfig</strong>, you will need to pass the <strong class="bold">hyperparameter</strong> sampling configuration that <strong class="source-inline">HyperDriveConfig</strong> will use. <strong class="bold">Hyperparameters</strong> can accept either discrete or continuous values:</p>
			<ul>
				<li>A typical example of discrete values is integers or string values. For example, in the <strong class="bold">TensorFlow</strong> framework, you can select the activation function to use by passing a <a id="_idIndexMarker654"/>string value to the <strong class="source-inline">activation</strong> <strong class="bold">hyperparameter</strong>. These string values represent the built-in activation functions that the <strong class="bold">TensorFlow</strong> framework supports. You can select values like <strong class="source-inline">selu</strong> for<a id="_idIndexMarker655"/> the <strong class="bold">Scaled Exponential Linear Unit</strong> (<strong class="bold">SELU</strong>) or <strong class="source-inline">relu</strong> for<a id="_idIndexMarker656"/> the <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>).</li>
				<li>A typical example of continuous values is float values. The <strong class="source-inline">alpha</strong> parameter in the <strong class="source-inline">LassoLars</strong> model you have been training is a <strong class="bold">hyperparameter</strong> that accepts float values.</li>
			</ul>
			<p>When you <a id="_idIndexMarker657"/>are exploring the possible <strong class="bold">hyperparameter</strong> combinations, you need to <a id="_idIndexMarker658"/>specify the search space that you are going to explore. The AzureML SDK offers a couple of functions that allow you to define the search space you are about to explore. These functions are part of the <strong class="source-inline">azureml.train.hyperdrive.parameter_expressions</strong> module.</p>
			<p>In the case of discrete <strong class="bold">hyperparameters</strong>, you can use the <strong class="source-inline">choice</strong> function, which allows you to specify the list of options the <strong class="bold">hyperparameter</strong> can take. For example, you could have defined the search space for the discrete string values of the <strong class="source-inline">activation</strong> <strong class="bold">hyperparameter</strong> you saw previously with the following script:</p>
			<p class="source-code">choice('selu','relu')</p>
			<p>This script will try both the <strong class="source-inline">selu</strong> and <strong class="source-inline">relu</strong> activation functions while looking for the optimal model.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you are interested in working with neural networks, you will probably need to understand these activation functions better. There are great books that can help you get started in neural network design. For the DP-100 exam, you will not need this knowledge.</p>
			<p>Note that even in the case of the continuous <strong class="source-inline">alpha</strong> <strong class="bold">hyperparameter</strong> of the <strong class="source-inline">LassoLars</strong> model, you can still use the <strong class="source-inline">choice</strong> method to define discrete values to explore. For example, the following use of <strong class="source-inline">choice</strong> is the equivalent of what you did back in the <em class="italic">Tracking model evolution</em> section of <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>:</p>
			<p class="source-code">choice(0.001, 0.01, 0.1, 0.25, 0.5)</p>
			<p>You can also <a id="_idIndexMarker659"/>define the probability distribution for the samples that you will be getting while you are exploring the search space. For example, if you want to provide an equal chance to all values, you will use a uniform distribution. On the other hand, you can use a normal distribution to focus the search area on the center of the search space. The AzureML SDK offers a couple of methods you can use, such as <strong class="source-inline">uniform(low, high)</strong>, <strong class="source-inline">loguniform(low, high)</strong>, <strong class="source-inline">normal(μ,σ)</strong>, and <strong class="source-inline">lognormal(μ, σ)</strong>. You can use the <strong class="source-inline">q</strong> prefixed equivalents for discrete values, such as <strong class="source-inline">quniform(low, high, q)</strong>, <strong class="source-inline">qloguniform(low, high, q)</strong>, <strong class="source-inline">qnormal(μ, σ, q)</strong>, and <strong class="source-inline">qlognormal(μ, σ</strong><strong class="source-inline">, q)</strong>, where the <strong class="source-inline">q</strong> parameter is the quantization factor that converts continuous values into discrete ones. </p>
			<p>On the GitHub page of this book, you can find the code that plots 1,000 samples being generated with the distributions of these functions. The results can be seen in <em class="italic">Figure 9.2</em>:</p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="Images/B16777_09_002.jpg" alt="Figure 9.2 – Advanced discrete and continuous hyperparameter value distributions. Sample values are ordered. The x axis shows the ordered value's index number&#13;&#10;" width="1644" height="1222"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Advanced discrete and continuous hyperparameter value distributions. Sample values are ordered. The x axis shows the ordered value's index number</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In <em class="italic">Figure 9.2</em>, in the <strong class="source-inline">loguniform</strong> and <strong class="source-inline">lognormal</strong> plots, the line of the discrete function with quantization factor 1 overlaps with the one from the continuous function. Therefore, you can only see two lines.</p>
			<p>Once you have <a id="_idIndexMarker660"/>defined the search space, you need to specify the sampling strategy that you will use to select each <strong class="bold">hyperparameter</strong> combination that you are going to be testing. The AzureML SDK supports the following methods for sampling the search space defined in the <strong class="source-inline">azureml.train.hyperdrive</strong> module:</p>
			<ul>
				<li><strong class="bold">Grid sampling</strong>: This<a id="_idIndexMarker661"/> method supports <a id="_idTextAnchor139"/><em class="italic">only</em> discrete <strong class="bold">hyperparameter</strong> values that are <a id="_idIndexMarker662"/>defined using the <strong class="source-inline">choice</strong> method you saw above. The Azure ML SDK will search all possible <strong class="bold">hyperparameter</strong> combinations of those discrete values. Imagine that you wanted to explore the following four parameter combinations:<ul><li>a=0.01 and b=10</li><li>a=0.01 and b=100</li><li>a=0.5 and b=10</li><li>a=0.5 and b=100</li></ul><p>The following code snippet defines the search space for these four combinations:</p><p class="source-code">from azureml.train.hyperdrive import GridParameterSampling</p><p class="source-code">from azureml.train.hyperdrive import choice</p><p class="source-code">param_sampling = GridParameterSampling( {</p><p class="source-code">        "a": choice(0.01, 0.5),</p><p class="source-code">        "b": choice(10, 100)</p><p class="source-code">    }</p><p class="source-code">)</p></li>
				<li><strong class="bold">Random sampling</strong>: This<a id="_idIndexMarker663"/> technique is implemented in the <strong class="source-inline">RandomParameterSampling</strong> class. It allows you to randomly select <strong class="bold">hyperparameter</strong> values from the available options. It supports both discrete and continuous <strong class="bold">hyperparameters</strong>.</li>
				<li><strong class="bold">Bayesian sampling</strong>: This<a id="_idIndexMarker664"/> method picks samples based on how the previous samples performed. It requires at least 20 iterations x the number of <strong class="bold">hyperparameter</strong> parameters you are fine-tuning. This means that if you have two parameters you are fine-tuning, you will need at least 20 x 2 = 40 runs in the <strong class="source-inline">max_total_runs</strong> you will read about next. It supports both discrete and continuous <strong class="bold">hyperparameters</strong>. </li>
			</ul>
			<p>Let's put everything<a id="_idIndexMarker665"/> you have learned so far into action: </p>
			<ol>
				<li>Navigate to the <strong class="bold">Author</strong> | <strong class="bold">Notebooks</strong> section of your AzureML Studio web interface. </li>
				<li>Create a folder named <strong class="source-inline">chapter09</strong>. </li>
				<li>You will need to create a <strong class="source-inline">diabetes-training</strong> folder in the <strong class="bold">chapter09</strong> folder you just created and add a <strong class="source-inline">training.py</strong> script. The script is the same as the one used in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, in the section <em class="italic">Moving the code to a Python script file</em>. You can copy the contents from there. The final <strong class="bold">Files</strong> tree is depicted in <em class="italic">Figure 9.3</em>.</li>
				<li>Create a notebook named <strong class="bold">chapter09.ipynb</strong> within the <strong class="bold">chapter09</strong> folder. <em class="italic">Figure 9.3</em> shows what the final <strong class="bold">Files</strong> tree will look like:<div id="_idContainer229" class="IMG---Figure"><img src="Images/B16777_09_003.jpg" alt="Figure 9.3 – The Files tree structure that contains the code and the chapter09 notebook&#13;&#10;" width="1262" height="427"/></div><p class="figure-caption">Figure 9.3 – The Files tree structure that contains the code and the chapter09 notebook</p></li>
				<li>Add the following initialization code in the first cell:<p class="source-code">from azureml.core import (</p><p class="source-code">    Workspace, Environment</p><p class="source-code">)</p><p class="source-code">from azureml.core.conda_dependencies import \</p><p class="source-code">     CondaDependencies </p><p class="source-code">import sklearn</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">diabetes_env = Environment(name=»diabetes-training-env»)</p><p class="source-code">diabetes_env.python.conda_dependencies = \</p><p class="source-code">     CondaDependencies.create(</p><p class="source-code">      conda_packages=[</p><p class="source-code">          f"scikit-learn=={sklearn.__version__}"],</p><p class="source-code">      pip_packages=["azureml-defaults",</p><p class="source-code">                    "azureml-dataprep[pandas]"])</p><p class="source-code">target = ws.compute_targets['cpu-sm-cluster'] </p><p>This is a code similar to the one you used in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. The only difference is that you are using the <strong class="source-inline">create</strong> method instead of adding the packages one by one.</p></li>
				<li>In a new <a id="_idIndexMarker666"/>cell, define the <strong class="source-inline">ScriptRunConfig</strong> object that will execute the <strong class="source-inline">training.py</strong> script:<p class="source-code">from azureml.core import ScriptRunConfig</p><p class="source-code">script = ScriptRunConfig(</p><p class="source-code">    source_directory='diabetes-training',</p><p class="source-code">    script='training.py',</p><p class="source-code">    environment=diabetes_env,</p><p class="source-code">    compute_target=target</p><p class="source-code">)</p><p>This <strong class="source-inline">ScriptRunConfig</strong> object is almost identical to the one you created in the <em class="italic">Training the diabetes model on a compute cluster</em> section of <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. The only difference is that you do not pass the <strong class="source-inline">arguments</strong> parameter. In particular, you don't specify the <strong class="source-inline">--alpha</strong> argument. This argument will automatically be appended by the <strong class="source-inline">HyperDriveConfig</strong> object you will configure in the next step.</p></li>
				<li>Add and <a id="_idIndexMarker667"/>execute the following code in a new cell:<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p><p class="source-code">from azureml.train.hyperdrive import (</p><p class="source-code">   RandomParameterSampling, uniform, PrimaryMetricGoal</p><p class="source-code">)</p><p class="source-code">param_sampling = RandomParameterSampling({</p><p class="source-code">        'alpha': uniform(0.00001, 0.1),</p><p class="source-code">    }</p><p class="source-code">)</p><p class="source-code">hd_config = HyperDriveConfig(</p><p class="source-code">               run_config=script,                          </p><p class="source-code">               hyperparameter_sampling=param_sampling,</p><p class="source-code">               primary_metric_name="nrmse", </p><p class="source-code">               primary_metric_goal=                   </p><p class="source-code">                          PrimaryMetricGoal.MINIMIZE,</p><p class="source-code">               max_total_runs=20,</p><p class="source-code">               max_concurrent_runs=4)</p><p>In this code, you define a <strong class="source-inline">RandomParameterSampling</strong> approach to explore uniformly distributed values, ranging from 0.00001 to 0.1, for the <strong class="source-inline">alpha</strong> argument<a id="_idIndexMarker668"/> that will be passed to the training script you created in <em class="italic">step 3</em>. This training script accepts the <strong class="source-inline">--alpha</strong> argument, which is then passed to the <strong class="source-inline">alpha</strong> <strong class="bold">hyperparameter</strong> of the <strong class="source-inline">LassoLars</strong> model.</p><p>You assign this <strong class="source-inline">RandomParameterSampling</strong> configuration to the <strong class="source-inline">hyperparameter_sampling</strong> argument of <strong class="source-inline">HyperDriveConfig</strong>.</p><p>You have also configured the <strong class="source-inline">run_config</strong> property of <strong class="source-inline">HyperDriveConfig</strong> to use the <strong class="source-inline">ScriptRunConfig</strong> object you defined in <em class="italic">step 6</em>. Note that the <strong class="source-inline">RandomParameterSampling</strong> class will be passing the <strong class="source-inline">alpha</strong> parameter needed by the script.</p><p>You then define that the produced models will be evaluated <a id="_idIndexMarker669"/>using the <strong class="bold">NRMSE</strong> metric that the training script is logging (the <strong class="source-inline">primary_metric_name</strong> parameter). You also specify that you are trying to minimize that value (the <strong class="source-inline">primary_metric_goal</strong> parameter), since it's the error you want to minimize.</p><p>The last two parameters, <strong class="source-inline">max_total_runs</strong> and <strong class="source-inline">max_concurrent_runs</strong>, control the resources you are willing to invest in finding the best model. The <strong class="source-inline">max_total_runs</strong> parameter controls the maximum number of experiments to run. This can be between 1 and 1,000 runs. This is a required parameter. <strong class="source-inline">max_concurrent_runs</strong> is an optional parameter and controls the maximum concurrency of the conducted runs. In this case, you defined <em class="italic">4</em>, which means that only four nodes will be provisioned in the <strong class="bold">cpu-sm-cluster</strong> cluster that you <a id="_idIndexMarker670"/>are using for <strong class="source-inline">ScriptRunConfig</strong>. This means that the cluster will still have one <a id="_idIndexMarker671"/>unprovisioned node, since the maximum number of nodes it can scale up to is five, as you defined in the section <em class="italic">Working with compute targets</em> of <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. There is one more optional parameter you can use to limit the amount of time you are searching for the optimal <strong class="bold">hyperparameter</strong> combination. The <strong class="source-inline">max_duration_minutes</strong> parameter, which you did not specify in the sample above, defines the maximum duration in minutes to run the <strong class="bold">hyperparameter tuning</strong> process. After that timeout, all subsequent scheduled runs are automatically canceled.</p></li>
				<li>In a new cell, add the following code:<p class="source-code">from azureml.core import Experiment</p><p class="source-code">experiment = Experiment(ws, "chapter09-hyperdrive")</p><p class="source-code">hyperdrive_run = experiment.submit(hd_config)</p><p class="source-code">hyperdrive_run.wait_for_completion(show_output=True)</p><p>In this code, you submit <strong class="source-inline">HyperDriveConfig</strong> to execute under the <strong class="bold">chapter09-hyperdrive</strong> experiment. The <strong class="source-inline">hyperdrive_run</strong> variable is an instance of <strong class="source-inline">HyperDriveRun</strong>, which inherits from the normal <strong class="source-inline">Run</strong> class. </p></li>
				<li>You can review the results of the process in the Studio web UI. Navigate to the <strong class="bold">Experiments</strong> tab of AzureML Studio and select that <strong class="bold">chapter09-hyperdrive</strong> experiment. You will see <strong class="bold">Run 1</strong> or something similar in the list of runs. This run consists of multiple child runs; each child run is a single <strong class="bold">hyperparameter</strong> combination. In this case, since you only have a single <strong class="bold">hyperparameter</strong>, each child run has a different value for the <strong class="source-inline">alpha</strong> hyperparameter. You can visually explore the effect the various values of the <strong class="source-inline">alpha</strong> parameter have regarding the <strong class="bold">nrmse</strong> metric, as seen in <em class="italic">Figure 9.4</em>:<div id="_idContainer230" class="IMG---Figure"><img src="Images/B16777_09_004.jpg" alt="Figure 9.4 – Effect of the alpha parameter on the nrmse metric&#13;&#10;" width="1642" height="1103"/></div><p class="figure-caption">Figure 9.4 – Effect of the alpha parameter on the nrmse metric</p><p>Note that <a id="_idIndexMarker672"/>the best <strong class="bold">nrmse</strong> value will be different in your execution. In this case, it is <strong class="bold">0.18388</strong>. That value was achieved with an <strong class="source-inline">alpha</strong> value of <strong class="bold">0.09781554163695343</strong>. Don't worry if you cannot read the values from the charts or the table. You can get the exact values by selecting the best run. In this case, it's run 17, while it may be a different number in your case. Clicking on the link that reads <strong class="bold">Run 17</strong> will open the details view of the specific run, and you can review the logs and the metrics of that run. If you clicked to view the details of the child run, navigate back to <strong class="source-inline">HyperDriveRun</strong> (<strong class="bold">Run 1</strong>).</p><p class="callout-heading">Important note</p><p class="callout">Run numbers may be different in your executions. Every time you execute the cells, a new run number is created, continuing from the previous number. So, if you execute code that performs one hyperdrive run with 20 child runs, the last child run will be run 21. The next time you execute the same code, the hyperdrive run will start from run 22, and the last child will be run 42. The run numbers referred to in this section are the ones shown in the various figures, and it is normal to observe differences, especially if you had to rerun a couple of cells.</p></li>
				<li>Navigate to the <strong class="bold">Outputs + logs</strong> tab of the completed <strong class="bold">Run 1</strong> run. You will notice <a id="_idIndexMarker673"/>that there is a single file under the <strong class="bold">azureml-logs</strong> folder named <strong class="bold">hyperdrive.txt</strong>, as shown in <em class="italic">Figure 9.5</em>: <div id="_idContainer231" class="IMG---Figure"><img src="Images/B16777_09_005.jpg" alt="Figure 9.5 – Log file in HyperDriveRun, picking up the first four jobs from &#13;&#10;the hyperparameter space that will be executed in parallel&#13;&#10;" width="1650" height="328"/></div><p class="figure-caption">Figure 9.5 – Log file in HyperDriveRun, picking up the first four jobs from the hyperparameter space that will be executed in parallel</p><p>This file contains all the jobs that were scheduled to complete the hyperparameter tuning process. The actual run logs and the stored model are stored within the child runs. If you need to debug a code issue, you will have to open one of them to see the script errors.</p></li>
				<li>You can also get the best model's run and the corresponding <strong class="bold">nrmse</strong> value through the AzureML SDK. In the <strong class="bold">chapter09.ipynb</strong> notebook, add a new cell and type the following code:<p class="source-code">best_run = hyperdrive_run.get_best_run_by_primary_metric()</p><p class="source-code">best_run_metrics = best_run.get_metrics(name='nrmse')</p><p class="source-code">parameter_values = best_run.get_details()[</p><p class="source-code">                        'runDefinition']['arguments']</p><p class="source-code">print('Best Run Id: ', best_run.id)</p><p class="source-code">print('- NRMSE:', best_run_metrics['nrmse'])</p><p class="source-code">print('- alpha:', parameter_values[1])</p><p>The <strong class="source-inline">get_best_run_by_primary_metric</strong> method retrieves the best run of <strong class="source-inline">HyperDriveRun</strong> that the <strong class="source-inline">hyperdrive_run</strong> variable references. From there, you can <a id="_idIndexMarker674"/>read the <strong class="bold">NRMSE</strong> metrics <a id="_idIndexMarker675"/>using the <strong class="source-inline">get_metrics</strong> method of the <strong class="source-inline">Run</strong> object, and you can get the details of the execution using the <strong class="source-inline">get_details</strong> method. In those details, there is a <strong class="source-inline">runDefinition</strong> object that contains an <strong class="source-inline">arguments</strong> list, as shown in <em class="italic">Figure 9.6</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="Images/B16777_09_006.jpg" alt="Figure 9.6 – Demystifying the best_run.get_details()['runDefinition']['arguments'] code&#13;&#10;" width="558" height="342"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Demystifying the best_run.get_details()['runDefinition']['arguments'] code</p>
			<p>In this section, you saw how to run a <strong class="bold">hyperparameter tuning</strong> process to find the optimal value for your model's <strong class="bold">hyperparameters</strong>. In the next section, you will see how you can optimize the time you search for the best values by using an early termination policy.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor140"/>Using the early termination policy</h2>
			<p>One of the <a id="_idIndexMarker676"/>parameters of the <strong class="source-inline">HyperDriveConfig</strong> constructor is the <strong class="source-inline">policy</strong> one. This argument accepts an <strong class="source-inline">EarlyTerminationPolicy</strong> object, which defines the policy with which runs can be terminated early. By default, this parameter has a <strong class="source-inline">None</strong> value, which means that the <strong class="source-inline">NoTerminationPolicy</strong> class will be used, allowing each run to execute until completion.</p>
			<p>To be able to use an early termination policy, your script must be performing multiple iterations during each run. </p>
			<p>In the <strong class="bold">Files</strong> view, add a folder named <strong class="bold">termination-policy-training</strong> and add a <strong class="bold">training.py</strong> file to it, as shown in <em class="italic">Figure 9.7</em>:</p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="Images/B16777_09_007.jpg" alt="Figure 9.7 – Adding a training script that performs multiple epochs&#13;&#10;" width="1650" height="905"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Adding a training script that performs multiple epochs</p>
			<p>Add the following code in the training script:</p>
			<p class="source-code">from azureml.core.run import Run</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import time</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument("--a", type=int, dest="a", help="The alpha parameter")</p>
			<p class="source-code">parser.add_argument("--b", type=int, dest="b", help="The beta parameter")</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p class="source-code">if (args.a &gt; 2):</p>
			<p class="source-code">    args.a = 0</p>
			<p class="source-code">run = Run.get_context()</p>
			<p class="source-code">def fake_train(run, a, b):</p>
			<p class="source-code">    time.sleep(5)</p>
			<p class="source-code">    metric = a + b</p>
			<p class="source-code">    run.log("fake_metric", metric)</p>
			<p class="source-code">for epoch in range(20):</p>
			<p class="source-code">    fake_train(run, args.a * epoch, args.b)</p>
			<p>The <a id="_idIndexMarker677"/>script gets two parameters, <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong>, and then calls the <strong class="source-inline">fake_train</strong> method 20 times. In data science literature, people refer to those 20 times as 20 <strong class="bold">epochs</strong>, which<a id="_idIndexMarker678"/> are the training cycles over the entire training dataset. </p>
			<p>In every epoch, the <strong class="source-inline">a</strong> parameter is multiplied by the iteration number, which is an integer value from <em class="italic">0</em> all the way to <em class="italic">19</em>, and the <strong class="source-inline">fake_train</strong> method is invoked. The <strong class="source-inline">fake_train</strong> method sleeps for 5 seconds to simulate a training process and then adds the modified <strong class="source-inline">a</strong> value to the <strong class="source-inline">b</strong> parameter. The result is logged in the <strong class="source-inline">fake_metric</strong> metric.</p>
			<p>Moreover, in <em class="italic">line 8</em>, the code checks the <strong class="source-inline">a</strong> parameter passed to the script. If it is greater than <em class="italic">2</em>, it changes to value <em class="italic">0</em>. This means that the fake model you are training will be performing better as the <strong class="source-inline">a</strong> value increases to value <em class="italic">2</em>, and then its performance will drop, as shown in <em class="italic">Figure 9.8</em>.</p>
			<p>Note that you don't need to read any dataset and, thus, you do not need the reference to <strong class="source-inline">Workspace</strong>. This is why <em class="italic">line 10</em> in the code above doesn't need to check if this is an <strong class="source-inline">_OfflineRun</strong> object or not, as you did in the section <em class="italic">Moving the code t<a id="_idTextAnchor141"/>o a Python script file</em> in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. </p>
			<p>If you were to run <strong class="source-inline">HyperDriveConfig</strong> with grid search on all values between <em class="italic">1 </em>and <em class="italic">4</em> for the <strong class="bold">hyperparameters</strong>, you would get 16 runs. The output of those runs is shown in <em class="italic">Figure 9.8</em>. On the left side of the figure, the diagram shows the <strong class="source-inline">fake_metric</strong> evolution over the epochs. On<a id="_idIndexMarker679"/> the right side of the figure, you can see how the <strong class="source-inline">fake_metric</strong> is affected by the various values of the <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> <strong class="bold">hyperparameters</strong>. On the right side of <em class="italic">Figure 9.8</em>, you can see that the models trained with values <em class="italic">1</em> and <em class="italic">2</em> on the <strong class="bold">hyperparameter</strong> <strong class="source-inline">a</strong> perform better than the models trained with <strong class="source-inline">a</strong> parameter <em class="italic">3</em> and <em class="italic">4</em>, regarding the <strong class="source-inline">fake_metric</strong>:</p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="Images/B16777_09_008.jpg" alt="Figure 9.8 – Hyperparameter tuning without early termination policy&#13;&#10;" width="1627" height="503"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Hyperparameter tuning without early termination policy</p>
			<p>Ideally, you would like to reduce the amount of time waiting for all the runs to complete. <strong class="source-inline">EarlyTerminationPolicy</strong> allows you to monitor the jobs that are running, and if they are performing poorly compared to the rest of the jobs, cancel them early. The resulting output would be like the one in <em class="italic">Figure 9.9</em>, where you can see that some of the jobs were terminated before reaching the twentieth reported interval (the graph starts counting from 0), saving time and compute resources:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="Images/B16777_09_009.jpg" alt="Figure 9.9 – Hyperparameter tuning with aggressive early termination policy&#13;&#10;" width="1650" height="577"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Hyperparameter tuning with aggressive early termination policy</p>
			<p>The AzureML SDK offers<a id="_idIndexMarker680"/> a few built-in <strong class="source-inline">EarlyTerminationPolicy</strong> implementations, located in the <strong class="source-inline">azureml.train.hyperdrive</strong> module:</p>
			<ul>
				<li><strong class="source-inline">NoTerminationPolicy</strong>: This<a id="_idIndexMarker681"/> is the default stopping policy that allows all runs to complete.</li>
				<li><strong class="source-inline">MedianStoppingPolicy</strong>: The <a id="_idIndexMarker682"/>median stopping policy computes the running averages across all runs. It then cancels runs whose best performance is worse than the median of the running averages. You can think of this policy as comparing the performance of each run against the average performance of the previous runs. The nice thing about this policy is that it considers all runs that have happened so far and does not just compare the current run with the best runs so far. This feature allows the median stopping policy to avoid being trapped in local optimum values.</li>
				<li><strong class="source-inline">BanditPolicy</strong>: The <a id="_idIndexMarker683"/>bandit policy computes the distance between the current run and the best-performing one and then terminates it based on some slack criteria. You can define either the absolute distance (the <strong class="source-inline">slack_amount</strong> parameter) or the maximum allowed ratio (the <strong class="source-inline">slack_factor</strong> parameter) allowed from the best performing run.</li>
				<li><strong class="source-inline">TruncationSelectionPolicy</strong>: The<a id="_idIndexMarker684"/> truncation selection policy is the most aggressive policy, which cancels a certain percentage (the <strong class="source-inline">truncation_percentage</strong> parameter) of runs that rank the lowest for their performance on the primary metric. When ranking a relatively young run, at an early iteration, the policy compares them with the equivalent iteration performance <a id="_idIndexMarker685"/>of the older runs. Thus, this policy strives for fairness in ranking the runs by accounting for improving model performance with training time.</li>
			</ul>
			<p>All policies take two optional parameters:</p>
			<ul>
				<li><strong class="source-inline">evaluation_interva</strong>l: The<a id="_idIndexMarker686"/> frequency for applying the policy.</li>
				<li><strong class="source-inline">delay_evaluation</strong>: This <a id="_idIndexMarker687"/>delays the first policy evaluation for a specified number of intervals, giving time for young runs to reach a mature state.</li>
			</ul>
			<p>Let's do hyperparameter tuning on the script you created above using the most recommended policy, <strong class="source-inline">MedianStoppingPolicy</strong>: </p>
			<ol>
				<li value="1">Go to the <strong class="bold">chapter09.ipynb</strong> notebook and add the following code in a new cell:<p class="source-code">from azureml.core import Workspace, ScriptRunConfig</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">target = ws.compute_targets["cpu-sm-cluster"]</p><p class="source-code">script = ScriptRunConfig(</p><p class="source-code">    source_directory="termination-policy-training",</p><p class="source-code">    script=»training.py»,</p><p class="source-code">    environment=ws.environments[«AzureML-Minimal»],</p><p class="source-code">    compute_target=target,</p><p class="source-code">)</p><p>This code establishes the connection to the workspace and defines the <strong class="source-inline">ScriptRunConfig</strong> object that will be used in the hyperparameter tuning process.</p></li>
				<li>In a new cell, add <a id="_idIndexMarker688"/>the following code:<p class="source-code">from azureml.train.hyperdrive import (</p><p class="source-code">    GridParameterSampling,    </p><p class="source-code">    choice,</p><p class="source-code">    MedianStoppingPolicy,</p><p class="source-code">    HyperDriveConfig,</p><p class="source-code">    PrimaryMetricGoal</p><p class="source-code">)</p><p class="source-code">param_sampling = GridParameterSampling(</p><p class="source-code">    {</p><p class="source-code">        "a": choice(1, 2, 3, 4),</p><p class="source-code">        "b": choice(1, 2, 3, 4),</p><p class="source-code">    }</p><p class="source-code">)</p><p class="source-code">early_termination_policy = MedianStoppingPolicy(</p><p class="source-code">    evaluation_interval=1, delay_evaluation=5</p><p class="source-code">)</p><p class="source-code">hd_config = HyperDriveConfig(</p><p class="source-code">    policy=early_termination_policy,</p><p class="source-code">    run_config=script,</p><p class="source-code">    hyperparameter_sampling=param_sampling,</p><p class="source-code">    primary_metric_name="fake_metric",</p><p class="source-code">    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,</p><p class="source-code">    max_total_runs=50,</p><p class="source-code">    max_concurrent_runs=4</p><p class="source-code">)</p><p>This <strong class="source-inline">HyperDriveConfig</strong> object is using <strong class="source-inline">MedianStoppingPolicy</strong> as its policy <a id="_idIndexMarker689"/>parameter to evaluate all runs after their first <em class="italic">5</em> iterations and compares their results on every iteration with the median of the running averages. </p></li>
				<li>In a new cell, add the following code to start the execution of the <strong class="source-inline">HyperDriveConfig</strong> object you defined in <em class="italic">step 2</em>:<p class="source-code">experiment = Experiment(ws, "chapter09-hyperdrive")</p><p class="source-code">hyperdrive_run = experiment.submit(hd_config)</p><p class="source-code">hyperdrive_run.wait_for_completion(show_output=True)</p><p><em class="italic">Figure 9.10</em> shows <a id="_idIndexMarker690"/>the results of this <strong class="source-inline">HyperDriveRun</strong> run, where only 8 out of 16 jobs were terminated early:</p></li>
			</ol>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="Images/B16777_09_010.jpg" alt="Figure 9.10 – Hyperparameter tuning with median stopping early termination policy&#13;&#10;" width="1284" height="370"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Hyperparameter tuning with median stopping early termination policy</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the code above, the <strong class="source-inline">max_total_runs</strong> argument has a value of 50. This is the top limit of how many child runs can potentially occur. In this example, you only have 16 combinations. This means that the experiment will run only 16 times and then it will stop, since the whole search area has been searched. If you wanted the <strong class="source-inline">max_total_runs</strong> parameter to have an effect, you should specify a value less than 16.</p>
			<p>So far, you <a id="_idIndexMarker691"/>have seen how you can optimize a specific model against the data you have. In the next section, you will see how you can search for the best model to run an AutoML experiment through the SDK, similar to what you did in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>, through the studio user interface.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor142"/>Running AutoML experiments with code</h1>
			<p>So far, in <a id="_idIndexMarker692"/>this chapter, you were fine-tuning a <strong class="source-inline">LassoLars</strong> model, performing a hyperparameter tuning process to identify the best value for the <strong class="source-inline">alpha</strong> parameter based on the training data. In this section, you will use <strong class="bold">AutoML</strong> in the AzureML SDK to automatically select the best combination of data preprocessing, model, and hyperparameter settings for your training dataset. </p>
			<p>To configure <a id="_idIndexMarker693"/>an <strong class="bold">AutoML</strong> experiment through the Azure<a id="_idTextAnchor143"/>ML SDK, you will need to configure an <strong class="source-inline">AutoMLConfig</strong> object. You will need to define the <strong class="bold">Task type</strong>, the <strong class="bold">Metric</strong>, the <strong class="bold">Training data</strong>, and the <strong class="bold">Compute budget</strong> you want to invest. The output of this process is a list of models from which you can select the best run and the best model associated with that run, as shown in <em class="italic">Figure 9.11</em>:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="Images/B16777_09_011.jpg" alt="Figure 9.11 – AutoML process&#13;&#10;" width="1650" height="1041"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – AutoML process</p>
			<p>Depending on<a id="_idIndexMarker694"/> the type of problem you are trying to model, you must select the <strong class="source-inline">task</strong> parameter, selecting either <strong class="source-inline">classification</strong>, <strong class="source-inline">regression</strong>, or <strong class="source-inline">forecasting</strong>, as shown in <em class="italic">Figure 9.12</em>:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="Images/B16777_09_012.jpg" alt="Figure 9.12 – AutoML task types, algorithms, and supported metrics&#13;&#10;" width="938" height="834"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – AutoML task types, algorithms, and supported metrics</p>
			<p><em class="italic">Figure 9.12</em> shows only a subset of the supported algorithms that the AzureML SDK supports. The <strong class="source-inline">azureml.train.automl.constants.SupportedModels</strong> package contains the <strong class="source-inline">classification</strong>, <strong class="source-inline">regression</strong>, and <strong class="source-inline">forecasting</strong> classes that list all <a id="_idIndexMarker695"/>supported algorithms as attributes. Since forecasting is just a more specialized version of regression, all algorithms from regression can be used. AutoML supports some additional, more specialized, forecasting algorithms, such as the very<a id="_idIndexMarker696"/> popular <strong class="bold">ARIMA</strong> technique or <a id="_idIndexMarker697"/>Facebook's <strong class="bold">Prophet</strong> algorithm.</p>
			<p>The <strong class="source-inline">primary_metric</strong> parameter determines the metric to be used during model training for optimization. The metrics are the same for both regression and forecasting. Classification algorithms use different metrics, as shown in <em class="italic">Figure 9.12</em>.</p>
			<p>Training <a id="_idIndexMarker698"/>data can be provided in the <strong class="source-inline">training_data</strong> parameter, either in the format of a<a id="_idIndexMarker699"/> pandas <strong class="bold">DataFrame</strong> or through the AzureML native <strong class="source-inline">Dataset</strong> objects. The training data is in tabular format and includes the <strong class="source-inline">target</strong> column. You define the name of the column you want to predict, passing the <strong class="source-inline">label_column_name</strong> parameter. By default, AutoML will use that dataset for both the training and validation of produced models. If the dataset is more than 20,000 rows, the dataset is split, keeping 10% for validation. If the dataset is smaller than 20,000 rows, cross-validation is used. If you want to specify how many folds to create out of <strong class="source-inline">training_data</strong>, you can use the <strong class="source-inline">n_cross_validations</strong> parameter. Another approach is to provide the <strong class="source-inline">validation_size</strong> parameter, which is the percentage (values <em class="italic">0.0</em> to <em class="italic">1.0</em>) to hold out of the training data and use as validation. If you want to manually split the data into training and validation data, then you can assign your validation data to the <strong class="source-inline">validation_data</strong> parameter, as you will do later in this section.</p>
			<p><strong class="bold">Compute budget</strong> is the <a id="_idIndexMarker700"/>amount of money you are willing to spend to find the best machine learning model out of your training data. It consists of three parts:</p>
			<ul>
				<li><strong class="bold">The compute cluster's node type</strong>: The more capabilities your compute cluster's type<a id="_idIndexMarker701"/> has, the bigger the cost per second is when you run the AutoML job. This is a setting you configured when you created the compute cluster, and this cannot change at this point in time unless you create a new cluster.</li>
				<li><strong class="bold">The number of nodes to use for the AutoML job</strong>: You can define the <strong class="source-inline">max_concurrent_iterations</strong> parameter <a id="_idIndexMarker702"/>to use up to the maximum number of nodes your compute cluster has. This will allow you to run parallel iterations but increases the cost. By default, this parameter is <em class="italic">1</em> and allows only a single iteration at a time.</li>
				<li><strong class="bold">The amount of time to search for the best model</strong>: You can either define a literal <a id="_idIndexMarker703"/>number of hours to search for the best model using the <strong class="source-inline">experiment_timeout_hours</strong> parameter or you can define the <strong class="source-inline">experiment_exit_score</strong> parameter, which defines the score to achieve and then stop further exploration. Another way to limit your compute spending is to limit the number of different algorithms and parameter combinations to explore. By default, AutoML will explore 1,000 combinations, and you can restrict that by specifying the <strong class="source-inline">iterations</strong> parameter.</li>
			</ul>
			<p>Now that you <a id="_idIndexMarker704"/>have explored all the options that you need to configure in the <strong class="source-inline">AutoMLConfig</strong> object, navigate to your <strong class="source-inline">chapter09.ipynb</strong> notebook, add a new cell, and type the following code:</p>
			<p class="source-code">from azureml.core import Workspace, Dataset</p>
			<p class="source-code">from azureml.train.automl import AutoMLConfig</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">compute_target = ws.compute_targets["cpu-sm-cluster"]</p>
			<p class="source-code">diabetes_dataset = Dataset.get_by_name(workspace=ws, name='diabetes')</p>
			<p class="source-code">train_ds,validate_ds = diabetes_dataset.random_split(percentage=0.8, seed=1337)</p>
			<p class="source-code">experiment_config = AutoMLConfig(</p>
			<p class="source-code">    task = "regression",</p>
			<p class="source-code">    primary_metric = 'normalized_root_mean_squared_error',</p>
			<p class="source-code">    training_data = train_ds,</p>
			<p class="source-code">    label_column_name = "target",</p>
			<p class="source-code">    validation_data = validate_ds,</p>
			<p class="source-code">    compute_target = compute_target,</p>
			<p class="source-code">    experiment_timeout_hours = 0.25,</p>
			<p class="source-code">    iterations = 4</p>
			<p class="source-code">)</p>
			<p>In this code, you get the reference to the workspace, your compute cluster, and the <strong class="source-inline">diabetes</strong> dataset, which you are splitting into a training one and a validation one. You then create an <strong class="source-inline">AutoMLConfig</strong> object that<a id="_idIndexMarker705"/> will do <strong class="bold">regression</strong>, using the <strong class="bold">NRMSE</strong> metric <a id="_idIndexMarker706"/>that you used in this chapter in the section <em class="italic">Hyperparameter tuning using HyperDrive</em>. You<a id="_idIndexMarker707"/> specify the training data and configure that you are looking to predict the <strong class="source-inline">target</strong> column. You also specify the <strong class="source-inline">validation_data</strong> parameter.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Instead of splitting the dataset, you could have passed the entire dataset in the <strong class="source-inline">training_data</strong> parameter and skipped the <strong class="source-inline">validation_data</strong> parameter. Since the dataset consists of only 442 rows, AutoML would have split the training dataset into 10 folds, which would have been used to perform the cross-validation technique. </p>
			<p>You then define the <strong class="source-inline">compute_target</strong> experiment to use for this training and determine your computation budget by allowing the experiment to run for a quarter of an hour (the <strong class="source-inline">experiment_timeout_hours</strong> parameter), which is 15 minutes, and exploring only 4 model and parameter combinations (the <strong class="source-inline">iterations</strong> parameter). In your case, the <strong class="source-inline">iterations</strong> parameter will probably be the reason that will terminate the <strong class="bold">AutoML</strong> experiment.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">For forecasting, you would need to specify <strong class="source-inline">forecasting_parameters</strong> in addition to the regression parameters you defined previously. The <strong class="source-inline">ForecastingParameters</strong> class has the following parameters that are commonly used:</p>
			<p class="callout">1) <strong class="source-inline">time_column_name</strong>: The column that represents the time dimension of the time series.</p>
			<p class="callout">2) <strong class="source-inline">max_horizon</strong>: The desired forecast horizon in units of the time-series frequency. This is by default <em class="italic">1</em>, meaning that your model will be able to forecast a single slot in the future. The slot is the frequency your dataset uses. If your dataset has 1 row for every hour and you want to forecast for 7 days, <strong class="source-inline">max_horizon</strong> needs to be 7 days x 24 slots per day = 168. </p>
			<p>So far, you have created <strong class="source-inline">experiment_config</strong>, which contains the configuration of the <strong class="bold">AutoML</strong> experiment you are about to perform. Add a new cell and add the following code to<a id="_idIndexMarker708"/> kick off the AutoML training process:</p>
			<p class="source-code">from azureml.core.experiment import Experiment</p>
			<p class="source-code">my_experiment = Experiment(ws, 'chapter09-automl-experiment')</p>
			<p class="source-code">run = my_experiment.submit(experiment_config, </p>
			<p class="source-code">                           show_output=True)</p>
			<p>The <strong class="source-inline">run</strong> variable contains a reference to the <strong class="source-inline">AutoMLRun</strong> object that was created using the <strong class="source-inline">submit</strong> method. After a couple of minutes, the process will be complete. To get the current best run and best model, you can use the <strong class="source-inline">get_output()</strong> method, as shown in the following snippet:</p>
			<p class="source-code">best_run, best_model = run.get_output()</p>
			<p>Alternatively, you can directly access the best run and the best model using the corresponding <strong class="source-inline">Tuple</strong> index, as shown in the following snippet:</p>
			<p class="source-code">best_run = run.get_output()[0]</p>
			<p class="source-code">best_model = run.get_output()[1]</p>
			<p>In every automated machine learning experiment, your data is automatically scaled or normalized to help algorithms perform well. This data transformation is becoming part of the trained model. This means that your data is passing through a data transformer first, and then the model is being trained with new feature names that are not directly visible to you. You will see an example of <strong class="source-inline">sklearn.composeColumnTransformer</strong> in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. To review the actual steps that are embedded within the AutoML model, you can use the <strong class="source-inline">steps</strong> attribute of the produced model:</p>
			<p class="source-code">best_model.steps</p>
			<p>The first step is named <strong class="source-inline">datatransformer</strong> and contains the imputers used for our <strong class="source-inline">diabetes</strong> dataset. This step is named <strong class="source-inline">datatransformer</strong> for both regression and classification tasks. For forecasting tasks, this step is named <strong class="source-inline">timeseriestransformer</strong>, and it contains additional date-based transformations. To get a list of <a id="_idIndexMarker709"/>transformations and the names of engineered features, you can use the following code snippet:</p>
			<p class="source-code">print(best_model.named_steps['datatransformer'] \</p>
			<p class="source-code">                 .get_featurization_summary())</p>
			<p class="source-code">feature_names=best_model.named_steps['datatransformer'] \</p>
			<p class="source-code">                 .get_engineered_feature_names()</p>
			<p class="source-code">print("Engineered feature names:")</p>
			<p class="source-code">print(feature_names)</p>
			<p>In this section, you searched for the best model against the diabetes regression problem using <strong class="bold">AutoML</strong>. This concludes the most frequently used ways you can optimize a machine learning model given a specific dataset.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor144"/>Summary</h1>
			<p>In this chapter, you explored the most-used approaches in optimizing a specific model to perform well against a dataset and how you can even automate the process of model selection. You started by performing parallelized <strong class="bold">hyperparameter tuning</strong> using the <strong class="source-inline">HyperDriveConfig</strong> class to optimize the <strong class="source-inline">alpha</strong> parameter of the <strong class="source-inline">LassoLars</strong> model you have been training against the <strong class="source-inline">diabetes</strong> dataset. Then, you automated the model selection, using AutoML to detect the best combination of algorithms and parameters that predicts the <strong class="source-inline">target</strong> column of the <strong class="source-inline">diabetes</strong> dataset.</p>
			<p>In the next chapter, you will build on top of this knowledge, learning how to use the AzureML SDK to interpret the model results.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor145"/>Questions</h1>
			<ol>
				<li value="1">You want to get the best model trained by an <strong class="bold">AutoML</strong> run. Which code is correct?<p>a. <strong class="source-inline">model = run.get_output()[0]</strong></p><p>b. <strong class="source-inline">model = run.get_output()[1]</strong></p><p>c. <strong class="source-inline">model = run.get_outputs()[0]</strong></p><p>d. <strong class="source-inline">model = run.get_outputs()[1]</strong></p></li>
				<li>You want to run a forecasting <strong class="bold">AutoML</strong> experiment on top of data you receive from a sensor. You receive one record every day from the sensor. You want to be able to predict the values for <em class="italic">5</em> days. Which of the following parameters should you pass to the <strong class="source-inline">ForecastingParameters</strong> class?<p>a. <em class="italic">forecast_horizon = 5 * 1</em></p><p>b. <em class="italic">forecast_horizon = 5 * 24</em></p><p>c. <em class="italic">forecast_horizon = 5 * 12</em></p></li>
			</ol>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor146"/>Further reading</h1>
			<p>This section offers a list of helpful web resources that will help you augment your knowledge of the AzureML SDK and the various code snippets used in this chapter:</p>
			<ul>
				<li>The <strong class="source-inline">HyperDriveConfig</strong> class: <a href="https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py">https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py</a></li>
				<li>The <strong class="source-inline">AutoMLConfig</strong> class: <a href="https://docs.microsoft.com/en-us/Python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig">https://docs.microsoft.com/en-us/Python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig</a></li>
				<li>Data featurization in automated machine learning: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features</a></li>
				<li>Auto-train a forecast model: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-forecast</a></li>
				<li>Reference to the diabetes dataset that was loaded from the scikit-learn library: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html</a></li>
			</ul>
		</div>
	</div></body></html>