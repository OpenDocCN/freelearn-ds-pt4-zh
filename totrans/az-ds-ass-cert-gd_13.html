<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer261">
			<h1 id="_idParaDest-137"><em class="italic"><a id="_idTextAnchor147"/></em><a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>: Understanding Model Results</h1>
			<p>In this chapter, you will learn how to analyze the results of your machine learning models to interpret why the model made the inference it did. Understanding why the model predicted a value is the key to avoiding black box model deployments and to be able to understand the limitations your model may have. In this chapter, you will learn about the available interpretation features of Azure Machine Learning and visualize the model explanation results. You will also learn how to analyze potential model errors and detect cohorts where the model is performing poorly. Finally, you will explore tools that will help you assess your model's fairness and allow you to mitigate potential issues. </p>
			<p>In this chapter, we're going to cover the following topics:</p>
			<ul>
				<li>Creating responsible machine learning models</li>
				<li>Interpreting the predictions of the model</li>
				<li>Analyzing model errors</li>
				<li>Detecting potential model fairness issues</li>
			</ul>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor148"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access Control</strong> (<strong class="bold">IAM</strong>) role on the resource group level. Within that resource group, you should have already deployed a <strong class="bold">machine learning</strong> resource named <strong class="source-inline">packt-learning-mlw</strong>. These resources should be available to you if you followed the instructions in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>You will also need to have a basic understanding of the <strong class="bold">Python</strong> language. The code snippets in this chapter target Python version 3.6 or later. You should also be familiar with working in the notebook experience within Azure Machine Learning Studio, something that was covered in the previous chapters.</p>
			<p>This chapter assumes you have created a compute cluster named <strong class="source-inline">cpu-sm-cluster</strong>, as described in the <em class="italic">Working with compute targets</em> section of <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. </p>
			<p>You can find all the notebooks and code snippets for this chapter in this book's repository at <a href="http://bit.ly/dp100-ch10">http://bit.ly/dp100-ch10</a>.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor149"/>Creating responsible machine learning models</h1>
			<p>Machine<a id="_idIndexMarker710"/> learning allows you to create models that can influence decisions and shape the future. With great power comes great responsibility, and this is where AI governance becomes a necessity, something commonly referred to as responsible AI principles and practices. Azure Machine Learning offers tools to support the responsible creation of AI under the following three pillars:</p>
			<ul>
				<li><strong class="bold">Understand</strong>: Before publishing any machine learning model, you need to be able to interpret and explain the model's behavior. Moreover, you need to assess and mitigate potential model unfairness against specific cohorts. This chapter focuses on the tools that assist you in understanding your models.</li>
				<li><strong class="bold">Protect</strong>: Here, you put mechanisms in place to protect people and their data. When training a model, data from real people is used. For example, in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, you trained a model on top of medical data from diabetic patients. Although the specific training dataset didn't have any <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>), the original dataset contained this sensitive<a id="_idIndexMarker711"/> information. There are open source libraries such as <strong class="bold">SmartNoise </strong>that offer basic building blocks that can be used to implement data handling mechanisms using vetted and mature differential privacy research techniques. <p>For example, a querying engine built with SmartNoise could<a id="_idIndexMarker712"/> allow data scientists to perform aggregate queries on top of sensitive data and add statistical <em class="italic">noise</em> in the results to prevent accidental identification of a single row within the dataset. Other open source libraries, such<a id="_idIndexMarker713"/> as <strong class="bold">presidio</strong>, offer a different approach to data protection, allowing you to quickly identify and anonymize private information such as credit card numbers, names, locations, financial data, and more. These libraries are more focused on raw text inputs, inputs you generally use when building <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) models. They offer<a id="_idIndexMarker714"/> modules you can use to anonymize your data before using them to train a model. Another approach to protecting people and their data is to encrypt the data and perform the entire model training process using the encrypted dataset without decrypting it. This is<a id="_idIndexMarker715"/> feasible through <strong class="bold">Homomorphic Encryption</strong> (<strong class="bold">HE</strong>), which is an encryption technique that allows certain mathematical operations to be performed on top of the encrypted data without <a id="_idIndexMarker716"/>requiring access to the private (decryption) key. The results of the computations are encrypted and can only be revealed by the owner of the private key. This means that using <strong class="bold">HE</strong>, you can add two encrypted values, <strong class="bold">A</strong> and <strong class="bold">B</strong>, and get the value <strong class="bold">C</strong>, which can only be decrypted by the private key that encrypted values <strong class="bold">A</strong> and <strong class="bold">B</strong>, as shown in the following diagram:</p></li>
			</ul>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="Images/B16777_10_001.jpg" alt="Figure 10.1 – Using HE to perform operations on top of encrypted data&#13;&#10;" width="1650" height="612"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Using HE to perform operations on top of encrypted data</p>
			<ul>
				<li><strong class="bold">Control</strong>: Controlling and documenting the end-to-end process is an essential principle in all software engineering activities. DevOps practices are commonly used to ensure end-to-end process automation and governance. One of the key practices in DevOps is to document the right information in each step of the process, allowing you to make responsible decisions at each stage. An Azure Machine Learning workspace allows you to tag and add descriptions to the various artifacts you create in your end-to-end machine learning process. The following diagram <a id="_idIndexMarker717"/>shows how you can add a description to the <strong class="bold">AutoML</strong> run you performed in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>:</li>
			</ul>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="Images/B16777_10_002.jpg" alt="Figure 10.2 – Adding descriptions to runs to document them&#13;&#10;" width="1650" height="372"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Adding descriptions to runs to document them</p>
			<p>Similar to adding descriptions to runs, you can add tags to the various artifacts you produce, such as the models. Tags are key/value pairs, such as <strong class="source-inline">PyTorch</strong> being the value of the <strong class="source-inline">Framework</strong> tag key. You might want to document the following information as part of a <a id="_idIndexMarker718"/>model <strong class="bold">datasheet</strong>:</p>
			<ul>
				<li>The intended use of the model</li>
				<li>The model architecture, including the framework used</li>
				<li>Training and evaluation data used</li>
				<li>Trained model performance metrics</li>
				<li>Fairness information, which you will read about in this chapter</li>
			</ul>
			<p>This information can be part of tags, and the <strong class="bold">datasheet</strong> can be a Markdown document that's automatically generated through these tags.</p>
			<p>In this section, you got an overview of the tools and the technologies that can help you create responsible AI. All three pillars are equally important, but for the DP100 exam, you will focus on the tools in the understand category, starting with model interpretability, which you will learn more about in the next section.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor150"/>Interpreting the predictions of the model</h1>
			<p>Being able to interpret <a id="_idIndexMarker719"/>the predictions of a model helps <a id="_idIndexMarker720"/>data scientists, auditors, and business leaders understand model behavior by looking at the top important factors that drive the model's predictions. It also enables them to perform what-if analysis to validate the impact of features on predictions. The Azure Machine Learning workspace integrates with <strong class="bold">InterpretML</strong> to provide these capabilities.</p>
			<p>InterpretML is an<a id="_idIndexMarker721"/> open source community that provides tools to perform model interpretability. The community contains a couple of projects. The most famous ones are as follows:</p>
			<ul>
				<li><strong class="bold">Interpret</strong> and <strong class="bold">Interpret-Community</strong> repositories, which focus on interpreting models that<a id="_idIndexMarker722"/> use tabular <a id="_idIndexMarker723"/>data, such as the diabetes dataset you have been working on within this book. You are going to work with the interpret-community repository in this section.</li>
				<li><strong class="bold">interpret-text</strong> extends <a id="_idIndexMarker724"/>the interpretability efforts into text classification models.</li>
				<li><strong class="bold">Diverse Counterfactual Explanations</strong> (<strong class="bold">DiCE</strong>) for machine learning allows you to detect the minimum number of<a id="_idIndexMarker725"/> changes that you need to perform in a data row to change the model's output. For example, suppose you have a loan approval model that just rejected a loan application. The customer asks what can be done to get the loan approved. <strong class="bold">DiCE</strong> could provide the minimum changes to approve the loan, such as reducing the number of credit cards or increasing the annual salary by 1%.</li>
			</ul>
			<p>There are two approaches when it comes to interpreting machine learning models:</p>
			<ul>
				<li><strong class="bold">Glassbox models</strong>: These are<a id="_idIndexMarker726"/> self-explanatory models, such as<a id="_idIndexMarker727"/> decision trees. For example, the <strong class="bold">sklearn</strong> <strong class="source-inline">DecisionTreeClassifier</strong> offers the <strong class="source-inline">feature_importances_</strong> attribute, which allows you to understand how features affect the model's predictions. The <strong class="bold">InterpretML</strong> community provides a couple <a id="_idIndexMarker728"/>more advanced <strong class="bold">glassbox</strong> model implementations. These models, once trained, allow you to retrieve an explainer and review which feature is driving what result, also known as <strong class="bold">interpretability results</strong>. Explainers for these models are lossless, meaning that they explain the importance of each feature accurately.</li>
				<li><strong class="bold">Black box explanations</strong>: If the <a id="_idIndexMarker729"/>model you are training doesn't come with a native explainer, you can create a black box explainer to interpret the model's results. You must provide the trained model and a test dataset, and the explainer observes how the value permutations affect the model's predictions. For example, in the loan approval model, this may tweak the age and the income of a rejected record to observe whetherthat changes the prediction. The information that's gained by performing these experiments is used to produce interpretations of the feature's importance. This technique can be applied to any machine learning model, so it is considered model agnostic. Due to their nature, these explainers are lossy, meaning that they may not accurately represent each feature's importance. There are a couple of <a id="_idIndexMarker730"/>well-known black-box <a id="_idIndexMarker731"/>techniques<a id="_idIndexMarker732"/> in the scientific literature, such as <strong class="bold">Shapley Additive Explanations</strong> (<strong class="bold">SHAP</strong>), <strong class="bold">Local Interpretable Model-Agnostic Explanations</strong> (<strong class="bold">LIME</strong>), <strong class="bold">Partial Dependence</strong> (<strong class="bold">PD</strong>), <strong class="bold">Permutation Feature Importance</strong> (<strong class="bold">PFI</strong>), <strong class="bold">feature interaction</strong>, and <strong class="bold">Morris sensitivity analysis</strong>. A <a id="_idIndexMarker733"/>subcategory of <a id="_idIndexMarker734"/>the black <a id="_idIndexMarker735"/>box explainers is the <strong class="bold">gray box explainers</strong>, which<a id="_idIndexMarker736"/> utilize information regarding the model's structure to get better and faster explanations. For example, there <a id="_idIndexMarker737"/>are specialized explainers for tree models (<strong class="bold">tree explainer</strong>), linear models (<strong class="bold">linear explainer</strong>), and <a id="_idIndexMarker738"/>even deep<a id="_idIndexMarker739"/> neural<a id="_idIndexMarker740"/> networks (<strong class="bold">deep explainer</strong>). </li>
			</ul>
			<p>Model explainers can provide <a id="_idIndexMarker741"/>two types of explanations:</p>
			<ul>
				<li><strong class="bold">Local-</strong> or <strong class="bold">instance-level feature importance</strong> focuses <a id="_idIndexMarker742"/>on the contribution of features for a specific prediction. For example, it can assist in answering why the model denied a particular customer's loan. Not all techniques support local explanations. For instance, <strong class="bold">PFI</strong>-based ones do not support instance-level feature importance.</li>
				<li><strong class="bold">Global-</strong> or <strong class="bold">aggregate-level feature importance</strong> explains<a id="_idIndexMarker743"/> how the model performs overall, considering all predictions done by the model. For example, it can answer which feature is the most important one regarding loan approval.</li>
			</ul>
			<p>Now that you have the basic theory behind model interpretation, it is time for you to get some hands-on experience. You will start by training a simple <strong class="bold">sklearn</strong> model.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor151"/>Training a loans approval model</h2>
			<p>In this section, you will <a id="_idIndexMarker744"/>train a classification model against a loans approval dataset that you will generate. You will use this model in the upcoming sections to analyze its results. Let's get started:</p>
			<ol>
				<li>Navigate to the <strong class="bold">Notebooks</strong> section of your Azure Machine Learning Studio web interface. Create a folder called <strong class="source-inline">chapter10</strong> and then create a notebook called <strong class="source-inline">chapter10.ipynb</strong>, as shown here:<div id="_idContainer242" class="IMG---Figure"><img src="Images/B16777_10_003.jpg" alt="Figure 10.3 – Creating the chapter10 notebook in the chapter10 folder&#13;&#10;" width="822" height="754"/></div><p class="figure-caption">Figure 10.3 – Creating the chapter10 notebook in the chapter10 folder</p></li>
				<li>You will need to <a id="_idIndexMarker745"/>install the latest packages of the <strong class="source-inline">interpret-community</strong> library, Microsoft's responsible AI widgets, and <strong class="bold">Fairlearn</strong> and <a id="_idIndexMarker746"/>restart the Jupyter kernel. Add the following code in the first cell:<p class="source-code">!pip install --upgrade interpret-community</p><p class="source-code">!pip install --upgrade raiwidgets</p><p class="source-code">!pip install --upgrade fairlearn</p><p>Once installed, restart the Jupyter kernel to ensure that the new packages will be loaded. To do so, select <strong class="bold">Kernel operations</strong> | <strong class="bold">Restart kernel</strong> from the notebook menu, as shown in the following screenshot. It is also advised that you comment out the contents of this cell to avoid rerunning them every time you want to revisit this notebook. Prefix each line with a <strong class="source-inline">#</strong> or delete the cell:</p><div id="_idContainer243" class="IMG---Figure"><img src="Images/B16777_10_004.jpg" alt="Figure 10.4 – Restarting the Jupyter kernel after installing the necessary packages&#13;&#10;" width="730" height="672"/></div><p class="figure-caption">Figure 10.4 – Restarting the Jupyter kernel after installing the necessary packages</p></li>
				<li>After restarting you<a id="_idIndexMarker747"/> kernel, add a new cell in the notebook. Generate a loans dataset using the following code:<p class="source-code">from sklearn.datasets import make_classification</p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">features, target = make_classification(</p><p class="source-code">    n_samples=500, n_features=3, n_redundant=1, shift=0,</p><p class="source-code">    scale=1,weights=[0.7, 0.3], random_state=1337)</p><p class="source-code">def fix_series(series, min_val, max_val):</p><p class="source-code">    series = series - min(series)</p><p class="source-code">    series = series / max(series)</p><p class="source-code">    series = series * (max_val - min_val) + min_val</p><p class="source-code">    return series.round(0)</p><p class="source-code">features[:,0] = fix_series(features[:,0], 0, 10000)</p><p class="source-code">features[:,1] = fix_series(features[:,1], 0, 10)</p><p class="source-code">features[:,2] = fix_series(features[:,2], 18, 85)</p><p class="source-code">classsification_df = pd.DataFrame(features, dtype='int')</p><p class="source-code">classsification_df.set_axis(</p><p class="source-code">   ['income','credit_cards', 'age'],</p><p class="source-code">   axis=1, inplace=True)</p><p class="source-code">classsification_df['approved_loan'] = target</p><p class="source-code">classsification_df.head()</p><p>This code will generate a dataset with the following normally distributed features: </p><ul><li><strong class="source-inline">income</strong> with a minimum value of <strong class="source-inline">0</strong> and a maximum value of <strong class="source-inline">10000</strong>.</li><li><strong class="source-inline">credit_cards</strong> with a minimum value of <strong class="source-inline">0</strong> and a maximum value of <strong class="source-inline">10</strong>.</li><li><strong class="source-inline">age</strong> with a minimum value of <strong class="source-inline">18</strong> and a maximum value of <strong class="source-inline">85</strong>.</li></ul><p>The label you will be predicting is <strong class="source-inline">approved_loan</strong>, which is a Boolean, and only 30% (<strong class="source-inline">weights</strong>) of the 500 samples (<strong class="source-inline">n_samples</strong>) are approved loans.</p></li>
				<li>Later in this chapter, you <a id="_idIndexMarker748"/>are going to run an <strong class="bold">AutoML</strong> experiment against this dataset. Register the dataset, as you saw in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>. Add the following code in your notebook:<p class="source-code">from azureml.core import Workspace, Dataset</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">dstore = ws.get_default_datastore()</p><p class="source-code">loans_dataset = \</p><p class="source-code">Dataset.Tabular.register_pandas_dataframe(</p><p class="source-code">    dataframe=classsification_df,</p><p class="source-code">    target=(dstore,"/samples/loans"),</p><p class="source-code">    name="loans",</p><p class="source-code">    description="A genarated dataset for loans")</p><p>If you visit the<a id="_idIndexMarker749"/> registered dataset, you can view the profile of the dataset, as shown here:</p><div id="_idContainer244" class="IMG---Figure"><img src="Images/B16777_10_005.jpg" alt="Figure 10.5 – Generated dataset profile&#13;&#10;" width="1650" height="1512"/></div><p class="figure-caption">Figure 10.5 – Generated dataset profile</p></li>
				<li>To be able to train and<a id="_idIndexMarker750"/> evaluate the model, you will need to split the dataset into train and test datasets. Use the following code to do so:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X = classsification_df[['income','credit_cards', 'age']]</p><p class="source-code">y = classsification_df['approved_loan'].values</p><p class="source-code">x_train, x_test, y_train, y_test = \</p><p class="source-code">        train_test_split(X, y, </p><p class="source-code">                       test_size=0.2, random_state=42)</p><p>First, you split the dataset into two, one with the features and one with the label you are trying to predict. Then, you use the <strong class="bold">sklearn</strong> <strong class="source-inline">train_test_split</strong> method to split the 500-sample data into one that contains 500 * 0.2 = 100 test records and the train set, which contains the remaining 400 samples.</p></li>
				<li>The next step is to <a id="_idIndexMarker751"/>initialize the model and fit it against the training dataset. In <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>, you learned how Azure Machine Learning's <strong class="bold">AutoML</strong> transforms the data that is going to become part of the trained model. This is done using <strong class="bold">sklearn</strong> pipelines, where you can chain various transformations and feature engineering steps into a model. You can create a simplified version of what <strong class="bold">AutoML</strong> does using the following code:<p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.pipeline import Pipeline</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">datatransformer = ColumnTransformer(</p><p class="source-code">    transformers=[</p><p class="source-code">        ('scale', MinMaxScaler(), x_train.columns)])</p><p class="source-code">model_pipeline = Pipeline(steps=[</p><p class="source-code">                 ('datatransformer', datatransformer),</p><p class="source-code">                 ('model', RandomForestClassifier())])</p><p class="source-code">model_pipeline.fit(x_train, y_train)</p><p>This code creates a pipeline with two steps:</p><ul><li>The <strong class="source-inline">datatransformer</strong> step is a <strong class="source-inline">ColumnTransformer</strong> that applies <strong class="source-inline">MinMaxScaler</strong> to all features. This transformer scales each feature's values.</li><li>The <strong class="source-inline">model</strong> step is the <a id="_idIndexMarker752"/>actual model you are training, which is a <strong class="source-inline">RandomForestClassifier</strong>.</li></ul><p>Then, you must call the <strong class="source-inline">fit</strong> method of the instantiated pipeline to train it against the training dataset.</p><p class="callout-heading">Important note</p><p class="callout">You do not need to use <strong class="source-inline">Pipeline</strong> to benefit from the interpretability features discussed in this chapter. Instead of creating a pipeline, you could have used the model directly by assigning it to the <strong class="source-inline">model_pipeline</strong> variable; for example, <strong class="source-inline">model_pipeline=RandomForestClassifier()</strong>. The addition of the <strong class="source-inline">datatransformer</strong> step was done to help you understand how AutoML constructs its pipelines. Using <strong class="source-inline">MinMaxScaler</strong> also<a id="_idIndexMarker753"/> increases the accuracy of the resulting model. Feel free to try different scalers to observe the differences in the resulting model.</p></li>
				<li>Now that you have a trained model, you can test it. Let's test against three fictional customers:<ul><li>A 45-year-old who has two credit cards and a monthly income of <strong class="source-inline">2000</strong></li><li>A 45-year-old who has nine credit cards and a monthly income of <strong class="source-inline">2000</strong></li><li>A 45-year-old who has two credit cards and a monthly income of <strong class="source-inline">10000</strong></li></ul><p>To do that, use the following code in a new notebook cell:</p><p class="source-code">test_df = pd.DataFrame(data=[</p><p class="source-code">    [2000, 2, 45],</p><p class="source-code">    [2000, 9, 45],</p><p class="source-code">    [10000, 2, 45]</p><p class="source-code">], columns= ['income','credit_cards', 'age'])</p><p class="source-code">test_pred = model_pipeline.predict(test_df)</p><p class="source-code">print(test_pred)</p><p>The printed result is <strong class="source-inline">[0 1 1]</strong>, which means that the first customer's loan will be rejected while the second and the third ones will be approved. This indicates that the <strong class="source-inline">income</strong> and <strong class="source-inline">credit_cards</strong> features may play an important role in the prediction of the model.</p></li>
				<li>Since the <a id="_idIndexMarker754"/>trained model is a decision tree and belongs to the glassbox model category, you can get the importance of the features that were calculated during the training process. Use the following code in a new notebook cell:<p class="source-code">model_pipeline.named_steps['model'].feature_importances_</p><p>This code gets a reference to the actual <strong class="source-inline">RandomForestClassifier</strong> instance and invokes a <strong class="bold">sklearn</strong> attribute called <strong class="source-inline">feature_importances_</strong>. The output of this is something like <strong class="source-inline">array([0.66935129, 0.11090936, 0.21973935])</strong>, which shows that <strong class="source-inline">income</strong> (the first value) is the most important feature, but it seems that <strong class="source-inline">age</strong> (the third value) is more important than <strong class="source-inline">credit_cards</strong> (the second value) in contrast to the observations we made in <em class="italic">Step 7</em>.</p><p class="callout-heading">Important note</p><p class="callout">The model's training process is not deterministic, meaning that your results will be different from the results seen in this book's examples. The numbers should be similar but not the same.</p></li>
			</ol>
			<p>In this section, you trained a simple <strong class="bold">sklearn</strong> decision tree and explored the native capabilities that the <strong class="bold">glassbox</strong> models offer. Unfortunately, not all models offer this <strong class="source-inline">feature_importances_</strong> attribute. In the next section, you will use a more advanced technique that allows you to explain any model. </p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor152"/>Using the tabular explainer</h2>
			<p>So far, you have used<a id="_idIndexMarker755"/> the capabilities of the <strong class="bold">sklearn</strong> library to train and understand the results of the model. From this point on, you will use the interpret community's package to interpret your trained model more accurately. You will use <strong class="bold">SHAP</strong>, a <a id="_idIndexMarker756"/>black box technique that tells you which features play what role in moving a prediction from <strong class="bold">Rejected</strong> to <strong class="bold">Approved</strong> and vice versa. Let's get started:</p>
			<ol>
				<li value="1">In a new notebook cell, add the following code:<p class="source-code">from interpret.ext.blackbox import TabularExplainer</p><p class="source-code">explainer = TabularExplainer(</p><p class="source-code">              model_pipeline.named_steps['model'],</p><p class="source-code">              initialization_examples=x_train, </p><p class="source-code">              features= x_train.columns,</p><p class="source-code">              classes=["Reject", "Approve"],</p><p class="source-code">              transformations=</p><p class="source-code">                model_pipeline.named_steps['datatransformer']) </p><p>This code creates a <strong class="source-inline">TabularExplainer</strong>, which is a wrapper class around the SHAP interpretation techniques. This means that this object will select the best SHAP interpretation method, depending on the passed-in model. In this case, since the model is a tree-based one, it will<a id="_idIndexMarker757"/> choose the <strong class="bold">tree explainer</strong>.</p></li>
				<li>Using this explainer, you <a id="_idIndexMarker758"/>are going to get the <strong class="bold">local</strong> or <strong class="bold">instance-level feature importance</strong> to <a id="_idIndexMarker759"/>gain more insights into why the model gave the results it did in <em class="italic">Step 7</em> of the <em class="italic">Training a loans approval model</em> section. In a new notebook cell, add the following code:<p class="source-code">local_explanation = explainer.explain_local(test_df)</p><p class="source-code">sorted_local_values = \</p><p class="source-code">    local_explanation.get_ranked_local_values()</p><p class="source-code">sorted_local_names = \</p><p class="source-code">    local_explanation.get_ranked_local_names()</p><p class="source-code">for sample_index in range(0,test_df.shape[0]):</p><p class="source-code">    print(f"Test sample number {sample_index+1}")</p><p class="source-code">    print("\t", test_df.iloc[[sample_index]]</p><p class="source-code">                         .to_dict(orient='list'))</p><p class="source-code">    prediction = test_pred[sample_index]</p><p class="source-code">    print("\t", f"The prediction was {prediction}")</p><p class="source-code">    importance_values = \</p><p class="source-code">        sorted_local_values[prediction][sample_index]</p><p class="source-code">    importance_names = \</p><p class="source-code">        sorted_local_names[prediction][sample_index]</p><p class="source-code">    local_importance = dict(zip(importance_names,</p><p class="source-code">                                importance_values))</p><p class="source-code">    print("\t", "Local feature importance")</p><p class="source-code">    print("\t", local_importance)</p><p>This code produces the results shown in the following screenshot. If you focus on <strong class="bold">Test sample number 2</strong>, you will notice that it shows that the <strong class="bold">credit_cards</strong> feature was the most important reason (see the <strong class="bold">0.33</strong> value) for the specific sample to be predicted as <strong class="bold">Approved</strong> (<strong class="bold">The prediction was 1</strong>). The negative values for <strong class="bold">income</strong> (whose value is approximately <strong class="bold">-0.12</strong>) in the same sample indicate that this feature was pushing the model to <strong class="bold">reject</strong> the loan:</p><div id="_idContainer245" class="IMG---Figure"><img src="Images/B16777_10_006.jpg" alt="Figure 10.6 – Local importance features show the importance of each feature for each test sample&#13;&#10;" width="1396" height="525"/></div><p class="figure-caption">Figure 10.6 – Local importance features show the importance of each feature for each test sample</p></li>
				<li>You can also get the <strong class="bold">global</strong> or <strong class="bold">aggregate-level feature importance</strong> to explain how the model<a id="_idIndexMarker760"/> performs overall against a test dataset. Add the <a id="_idIndexMarker761"/>following code in a new cell in your notebook:<p class="source-code">global_explanation = explainer.explain_global(x_test)</p><p class="source-code">print("Feature names:", </p><p class="source-code">        global_explanation.get_ranked_global_names())</p><p class="source-code">print("Feature importances:",</p><p class="source-code">        global_explanation.get_ranked_global_values())</p><p class="source-code">print(f"Method used: {explainer._method}")</p><p>Using this code, you can retrieve the features in an order based on their importance. In this case, it is <strong class="source-inline">income</strong>, <strong class="source-inline">age</strong>, and then <strong class="source-inline">credit_cards</strong>, whose corresponding importance values are approximately <strong class="source-inline">0.28</strong>, <strong class="source-inline">0.09</strong>, and <strong class="source-inline">0.06</strong>, respectively (the actual values may differ in your execution). Note that these values are not the same as the ones you got in <em class="italic">Step 8</em> of the <em class="italic">Training a loans approval model</em> section, although the order remains the same. This is normal since <strong class="bold">SHAP</strong> is a <a id="_idIndexMarker762"/>black box approach and generates the feature's importance based on the samples provided. The last line prints <strong class="source-inline">Method used: shap.tree</strong>, which<a id="_idIndexMarker763"/> indicates that <strong class="source-inline">TabularExplainer</strong> interpreted the <a id="_idIndexMarker764"/>model using the <strong class="bold">tree explainer</strong>, as mentioned in <em class="italic">Step 1</em> of this section.</p></li>
				<li>Finally, you must render the explanation dashboard to review the <strong class="source-inline">global_explanation</strong> results you generated in <em class="italic">Step 3</em>. Add the following code in your notebook:<p class="source-code">from raiwidgets import ExplanationDashboard</p><p class="source-code">ExplanationDashboard(global_explanation, model_pipeline, dataset=x_test, true_y=y_test)</p><p>This will render an interactive widget that you can use to understand your model's predictions against the test dataset that you provided. Clicking on the <strong class="bold">Aggregate feature importance</strong> tab, you should see the same results you saw in <em class="italic">Step 3</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="Images/B16777_10_007.jpg" alt="Figure 10.7 – The explanation dashboard provided by the interpret community&#13;&#10;" width="1650" height="723"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – The explanation dashboard provided by the interpret community</p>
			<p>You will explore this dashboard in more detail in the <em class="italic">Reviewing the interpretation results</em> section.</p>
			<p>So far, you have trained a model and used the <strong class="bold">SHAP</strong> interpretation technique to explain the feature importance of your model's predictions, either at a global or local level for specific inferences. In the next section, you will learn more about the alternative interpretation techniques available in the Interpret-Community package.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor153"/>Understanding the tabular data interpretation techniques</h2>
			<p>In the previous section, you <a id="_idIndexMarker765"/>used the tabular explainer to automatically select one of the<a id="_idIndexMarker766"/> available <strong class="bold">SHAP</strong> techniques. Currently, the interpret community supports the following SHAP explainers:</p>
			<ul>
				<li><strong class="bold">Tree explainer</strong> is used <a id="_idIndexMarker767"/>to explain decision tree models.</li>
				<li><strong class="bold">Linear explainer</strong> explains <a id="_idIndexMarker768"/>linear models and can also explain inter-feature correlations.</li>
				<li><strong class="bold">Deep explainer</strong> provides<a id="_idIndexMarker769"/> approximate explanations for deep learning models.</li>
				<li><strong class="bold">Kernel explainer</strong> is the most <a id="_idIndexMarker770"/>generic and the slowest one. It can explain any function's output, making it suitable for any model.</li>
			</ul>
			<p>An alternative to the <strong class="bold">SHAP</strong> interpretation techniques is to build an easier-to-explain surrogate model, such as the <strong class="bold">glassbox</strong> models that the interpret community offers, to reproduce the output of the given black box and then explain that surrogate. This technique is used by<a id="_idIndexMarker771"/> the <strong class="bold">Mimic</strong> explainer, and you need to provide one of the following glass box models:</p>
			<ul>
				<li><strong class="bold">LGBMExplainableModel</strong>, which <a id="_idIndexMarker772"/>is a <strong class="bold">LightGBM</strong> (a fast, high-performance framework <a id="_idIndexMarker773"/>based on decision trees) explainable model</li>
				<li><strong class="bold">LinearExplainableModel</strong>, which is a<a id="_idIndexMarker774"/> linear explainable model</li>
				<li><strong class="bold">SGDExplainableModel</strong>, which<a id="_idIndexMarker775"/> is a stochastic gradient descent explainable model</li>
				<li><strong class="bold">DecisionTreeExplainableModel</strong>, which <a id="_idIndexMarker776"/>is a decision tree explainable model</li>
			</ul>
			<p>If you wanted to use Mimic explainer<a id="_idIndexMarker777"/> in <em class="italic">Step 1</em> of the previous section, the code for this would look like this:</p>
			<p class="source-code">from interpret.ext.glassbox import (</p>
			<p class="source-code">    LGBMExplainableModel,</p>
			<p class="source-code">    LinearExplainableModel,</p>
			<p class="source-code">    SGDExplainableModel,</p>
			<p class="source-code">    DecisionTreeExplainableModel</p>
			<p class="source-code">)</p>
			<p class="source-code">from interpret.ext.blackbox import MimicExplainer</p>
			<p class="source-code">mimic_explainer = MimicExplainer(</p>
			<p class="source-code">           model=model_pipeline, </p>
			<p class="source-code">           initialization_examples=x_train,</p>
			<p class="source-code">           explainable_model= DecisionTreeExplainableModel,</p>
			<p class="source-code">           augment_data=True, </p>
			<p class="source-code">           max_num_of_augmentations=10,</p>
			<p class="source-code">           features=x_train.columns,</p>
			<p class="source-code">           classes=["Reject", "Approve"], </p>
			<p class="source-code">           model_task='classification')</p>
			<p>You can select any surrogate model from the <strong class="source-inline">import</strong> statement you can see in <em class="italic">line 1</em>. In this sample, you are using the <strong class="source-inline">DecisionTreeExplainableModel</strong> one. To get the global <a id="_idIndexMarker778"/>explanations, the code is the same as the code you wrote in <em class="italic">Step 3</em> and looks like this:</p>
			<p class="source-code">mimic_global_explanation = \</p>
			<p class="source-code">       mimic_explainer.explain_global(x_test)</p>
			<p class="source-code">print("Feature names:", </p>
			<p class="source-code">       mimic_global_explanation.get_ranked_global_names())</p>
			<p class="source-code">print("Feature importances:",</p>
			<p class="source-code">       mimic_global_explanation.get_ranked_global_values())</p>
			<p class="source-code">print(f"Method used: {mimic_explainer._method}")</p>
			<p>Although the order of the feature importance is the same, the calculated feature importance values are different, as shown here:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="Images/B16777_10_008.jpg" alt="Figure 10.8 – Mimic explainer feature importance calculated using the decision tree glassbox model&#13;&#10;" width="1041" height="113"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Mimic explainer feature importance calculated using the decision tree glassbox model</p>
			<p>Similar to the <strong class="bold">global</strong> or <strong class="bold">aggregate-level feature importance</strong>, you can <a id="_idIndexMarker779"/>use the same <strong class="source-inline">mimic_explainer</strong> to calculate <strong class="bold">local</strong> or <strong class="bold">instance-level feature importance</strong> using<a id="_idIndexMarker780"/> the same code as in <em class="italic">Step 2</em> in the previous section. The explanations can be seen in the following<a id="_idIndexMarker781"/> screenshot:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="Images/B16777_10_009.jpg" alt="Figure 10.9 – Local feature importance calculated using the decision &#13;&#10;tree glassbox model of the Mimic explainer&#13;&#10;" width="1370" height="521"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Local feature importance calculated using the decision tree glassbox model of the Mimic explainer</p>
			<p>The last interpretation<a id="_idIndexMarker782"/> technique offered by the interpret community is the one based on <strong class="bold">PFI</strong>. This technique permutates the values of each feature and observes how the model's predictions change. To create a PFI explainer to interpret your model, you will need the following code:</p>
			<p class="source-code">from interpret.ext.blackbox import PFIExplainer</p>
			<p class="source-code">pfi_explainer = PFIExplainer(model_pipeline,</p>
			<p class="source-code">                             features=x_train.columns,</p>
			<p class="source-code">                             classes=["Reject", "Approve"])</p>
			<p>Getting the global explanations requires passing in the <strong class="source-inline">true_labels</strong> parameter, which is the ground truth for the dataset, which are the actual values:</p>
			<p class="source-code">pfi_global_explanation = \</p>
			<p class="source-code">        pfi_explainer.explain_global(x_test, </p>
			<p class="source-code">                                     true_labels=y_test)</p>
			<p class="source-code">print("Feature names:", </p>
			<p class="source-code">        pfi_global_explanation.get_ranked_global_names())</p>
			<p class="source-code">print("Feature importances:",</p>
			<p class="source-code">        pfi_global_explanation.get_ranked_global_values())</p>
			<p class="source-code">print(f"Method used: {pfi_explainer._method}")</p>
			<p>The result of this code can be seen here. Due to the<a id="_idIndexMarker783"/> way <strong class="bold">PFI</strong> works, the <strong class="source-inline">credit_cards</strong> and <strong class="source-inline">age</strong> features may be the other way around in your results, since they have very similar feature importance values:</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="Images/B16777_10_010.jpg" alt="Figure 10.10 – Global feature importance calculated by the PFI explainer&#13;&#10;" width="1108" height="145"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Global feature importance calculated by the PFI explainer</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Due to the nature of the <strong class="bold">PFI</strong> explainer, you <em class="italic">cannot</em> use it to create <strong class="bold">local</strong> or <strong class="bold">instance-level feature importance</strong>. Keep that<a id="_idIndexMarker784"/> in mind if, during the exam, you are asked whetherthis technique could provide local explanations.</p>
			<p>In this section, you looked at all the interpretation techniques that are supported by the Interpret-Community package. In the next section, you will explore the capabilities that the explanation dashboard offers and how this dashboard is embedded within Azure Machine Learning Studio.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor154"/>Reviewing the interpretation results</h2>
			<p>Azure Machine <a id="_idIndexMarker785"/>Learning offers rich integration with the interpret community's efforts. One of those integration points is the explanation dashboard, which is embedded in every run. You can use <strong class="source-inline">ExplanationClient</strong> from the <strong class="source-inline">azureml.interpret</strong> package to upload and download model explanations to and from your workspace. To upload the global explanations that you created using <strong class="source-inline">TabularExplainer</strong> in the <em class="italic">Using the tabular explainer</em> section, navigate to the <strong class="bold">Author</strong> | <strong class="bold">Notebooks</strong> section of your Azure Machine Learning Studio web interface, open the <strong class="source-inline">chapter10.ipynb</strong> notebook, and add a new cell at the<a id="_idIndexMarker786"/> end of the file with the following code:</p>
			<p class="source-code">from azureml.core import Workspace, Experiment</p>
			<p class="source-code">from azureml.interpret import ExplanationClient</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">exp = Experiment(workspace=ws, name="chapter10")</p>
			<p class="source-code">run = exp.start_logging()</p>
			<p class="source-code">client = ExplanationClient.from_run(run)</p>
			<p class="source-code">client.upload_model_explanation(</p>
			<p class="source-code">    global_explanation, true_ys= y_test,</p>
			<p class="source-code">    comment='global explanation: TabularExplainer')</p>
			<p class="source-code">run.complete()</p>
			<p class="source-code">print(run.get_portal_url())</p>
			<p>This code starts a new run within the <strong class="source-inline">chapter10</strong> experiment. From that run, you create an <strong class="source-inline">ExplanationClient</strong>, which you use to upload the model explanations you generated and the ground truth (<strong class="source-inline">true_ys</strong>), which helps the dashboard evaluate the model's performance.</p>
			<p>If you visit the portal link that this code prints out, you will navigate to a run, where, in the <strong class="bold">Explanations</strong> tab, you will need to select <strong class="bold">Explanation ID</strong> on the left and then review the explanation dashboard by visiting the <strong class="bold">Aggregate feature importance</strong> tab, as shown here:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="Images/B16777_10_011.jpg" alt="Figure 10.11 – Reviewing the global explanations stored within the Azure Machine Learning workspace&#13;&#10;" width="1576" height="1125"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Reviewing the global explanations stored within the Azure Machine Learning workspace</p>
			<p><strong class="source-inline">ExplanationClient</strong> is<a id="_idIndexMarker787"/> used by Azure Machine Learning's <strong class="bold">AutoML</strong> capability to explain the best model found, as you read about in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. To kick off <strong class="bold">AutoML</strong> training against the loans dataset that you registered in the previous section, go back to your <strong class="source-inline">chapter10.ipynb</strong> notebook and add the following code block in a new cell:</p>
			<p class="source-code">from azureml.core import Workspace, Dataset, Experiment</p>
			<p class="source-code">from azureml.train.automl import AutoMLConfig</p>
			<p class="source-code">ws = Workspace.from_config()</p>
			<p class="source-code">compute_target = ws.compute_targets["cpu-sm-cluster"]</p>
			<p class="source-code">loans_dataset = Dataset.get_by_name(</p>
			<p class="source-code">                            workspace=ws, name='loans')</p>
			<p class="source-code">train_ds,validate_ds = loans_dataset.random_split(</p>
			<p class="source-code">                             percentage=0.8, seed=1337)</p>
			<p>This code looks very similar to the code you used in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>, in the <em class="italic">Running AutoML experiments with code</em> section. In this code block, you are getting a reference to the Azure Machine Learning workspace, the <strong class="source-inline">loans</strong> dataset, and then you are splitting the dataset into training and validation sets. </p>
			<p>In the same or a new<a id="_idIndexMarker788"/> cell, add the following code block:</p>
			<p class="source-code">experiment_config = AutoMLConfig(</p>
			<p class="source-code">    task = "classification",</p>
			<p class="source-code">    primary_metric = 'accuracy',</p>
			<p class="source-code">    training_data = train_ds,</p>
			<p class="source-code">    label_column_name = "approved_loan",</p>
			<p class="source-code">    validation_data = validate_ds,</p>
			<p class="source-code">    compute_target = compute_target,</p>
			<p class="source-code">    experiment_timeout_hours = 0.25,</p>
			<p class="source-code">    iterations = 4,</p>
			<p class="source-code">    model_explainability = True)</p>
			<p class="source-code">automl_experiment = Experiment(ws, 'loans-automl')</p>
			<p class="source-code">automl_run = automl_experiment.submit(experiment_config)</p>
			<p class="source-code">automl_run.wait_for_completion(show_output=True)</p>
			<p>In this code block, you are kicking off <strong class="bold">AutoML</strong> training for a classification task, you are using accuracy as the primary metric, and you are explicitly setting <strong class="source-inline">model_explainability</strong> (which is <strong class="source-inline">True</strong> by default). This option schedules a model explanation of the best model once the <strong class="bold">AutoML</strong> process concludes. Once the run completes, navigate to the run's UI and open the <strong class="bold">Models</strong> tab, as shown here:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="Images/B16777_10_012.jpg" alt="Figure 10.12 – Explanations become available for the best model in the AutoML run&#13;&#10;" width="1561" height="1024"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Explanations become available for the best model in the AutoML run</p>
			<p>Click on the <strong class="bold">View explanation</strong> link of the best model to navigate to the <strong class="bold">Explanations</strong> tab of the child run that trained the specific model. Once you land in the <strong class="bold">Explanations</strong> tab, you will notice that <strong class="bold">AutoML</strong> stored two global explanations: one for the raw features and one for the engineered features. You can switch between those two explanations by selecting the appropriate ID on the left-hand side of the screen, as shown in the following <a id="_idIndexMarker789"/>screenshot. Raw features are the ones from the original dataset. Engineered features are the ones you get after preprocessing. The engineered features are the internal inputs to the model. If you select the lower <strong class="bold">explanation ID</strong> and visit the <strong class="bold">Aggregate feature importance</strong> area, you will notice that <strong class="bold">AutoML</strong> has converted the credit card number into a categorical feature. Moreover, the model's input is 12 features compared to the three features you produced in your model training. </p>
			<p>You can review those features and their corresponding feature importance here: </p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="Images/B16777_10_013.jpg" alt="Figure 10.13 – Global explanations for engineered features&#13;&#10;" width="1650" height="636"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Global explanations for engineered features</p>
			<p>Since the engineered<a id="_idIndexMarker790"/> features are more difficult to understand, go to the top <strong class="bold">explanation ID</strong>, which is where you have the three raw features you have worked with so far. Navigate to the <strong class="bold">Dataset explorer</strong> tab, as shown here:</p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="Images/B16777_10_014.jpg" alt="Figure 10.14 – Dataset explorer in the raw features explanations&#13;&#10;" width="1490" height="1145"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Dataset explorer in the raw features explanations</p>
			<p>Here, we can see the <a id="_idIndexMarker791"/>following:</p>
			<ol>
				<li value="1">The <strong class="bold">Mimic</strong> explainer<a id="_idIndexMarker792"/> was used to explain the specific model (which is an <strong class="bold">XGBoostClassifier</strong>, as seen in <em class="italic">Figure 10.12</em>). The <strong class="bold">glassbox</strong> model<a id="_idIndexMarker793"/> that was used as a surrogate model was an <strong class="bold">LGBMExplainableModel</strong>, as shown at the top left of the preceding screenshot.</li>
				<li>You can edit the cohorts or define new ones to be able to focus your analysis on specific subgroups by selecting them from the <strong class="bold">Select a dataset cohort to explore</strong> dropdown. To define a new cohort, you need to specify the dataset filtering criteria you want to apply. For example, in the preceding screenshot, we have defined a cohort named <strong class="bold">age_45</strong>, which has a single filter (age == 45). There are <strong class="bold">4 datapoints</strong> in the test dataset that are used by this explanation dashboard.</li>
				<li>You can modify the <em class="italic">x</em>-axis and <em class="italic">y</em>-axis fields by clicking on the highlighted areas marked as <strong class="bold">3</strong> in the preceding screenshot. This allows you to change the view and get insights about the correlations of the features with the predicted values or the ground truth, the correlation between features, and any other view that makes<a id="_idIndexMarker794"/> sense for your model understanding analysis.</li>
			</ol>
			<p>In the <strong class="bold">Aggregate feature importance</strong> tab, as shown here, you can view the feature importance for all the data or for the specific cohorts you have defined:</p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="Images/B16777_10_015.jpg" alt="Figure 10.15 – Aggregate feature importance for the raw features with &#13;&#10;the cohorts and dependency of the rejected loans based on income&#13;&#10;" width="1377" height="1004"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.15 – Aggregate feature importance for the raw features with the cohorts and dependency of the rejected loans based on income</p>
			<p>In this example, the <strong class="bold">income</strong> feature is more important for the <strong class="bold">age_45</strong> cohort than the general public, which is represented by <strong class="bold">All data</strong>. If you click on a feature importance bar, the graph below updates to show you how this feature is affecting the model's decision to reject a loan request (<strong class="bold">Class 0</strong>). In this example, you can see that incomes that are from 0 up to a bit more than 5,000 <em class="italic">push</em> the model to reject the loan, while incomes from 6,000 onward have a negative impact, which means that they try to <em class="italic">push</em> the model to approve the loan.</p>
			<p>There are plenty of<a id="_idIndexMarker795"/> features in the explanation dashboard, and new features appear all the time as contributions to the interpret community. In this section, you reviewed the most important features of the dashboard, which have helped you understand why the model makes the predictions it does and how to potentially debug corner cases where it performs poorly.</p>
			<p>In the next section, you will learn about error analysis, which is part of Microsoft's overall responsible AI widgets package. This tool allows you to understand the blind spots of your models, which are the cases where your model is performing poorly.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor155"/>Analyzing model errors</h1>
			<p><strong class="bold">Error analysis</strong> is a <a id="_idIndexMarker796"/>model assessment/debugging tool that enables you to gain a deeper understanding of your machine learning model errors. Error analysis helps you identify cohorts within your dataset with higher error rates than the rest of the records. You can observe the misclassified and erroneous data points more closely to investigate whether any systematic patterns can be spotted, such as whether no data is available for a specific cohort. Error <a id="_idIndexMarker797"/>analysis is also a powerful way to describe the current shortcomings of the system and communicate that to other stakeholders and auditors.</p>
			<p>The tool consists of several visualization components that can help you understand where the errors appear.</p>
			<p>Navigate to the <strong class="bold">Author</strong> | <strong class="bold">Notebooks</strong> section of your Azure Machine Learning Studio web interface and open the <strong class="source-inline">chapter10.ipynb</strong> notebook. From <strong class="bold">Menu</strong>, in the <strong class="bold">Editors</strong> sub-menu, click <strong class="bold">Edit in Jupyter</strong> to open the same notebook in Jupyter and continue editing it there, as shown here:</p>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="Images/B16777_10_016.jpg" alt="Figure 10.16 – Editing a notebook in Jupyter for better compatibility with the widget&#13;&#10;" width="1210" height="834"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – Editing a notebook in Jupyter for better compatibility with the widget</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">At the time of writing this book, the error analysis dashboard doesn't work on the Notebooks experience due to security restrictions imposed by the Notebooks experience that prevent certain features from working properly. If you try to run it within Notebooks, it doesn't produce the necessary visualizations. This is why you are going to open the notebook in Jupyter, something that may not be needed by the time you read this book.</p>
			<p>In the Jupyter<a id="_idIndexMarker798"/> environment, add a new cell at the end of the file with the following code:</p>
			<p class="source-code">from raiwidgets import ErrorAnalysisDashboard</p>
			<p class="source-code">ErrorAnalysisDashboard(global_explanation, model_pipeline, </p>
			<p class="source-code">                       dataset=x_test, true_y=y_test)</p>
			<p>Notice that this code is very similar to the code you used to trigger the explanation dashboard.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure that you close the notebook from any other editing experience you may have, such as the Notebooks experience within Azure Machine Learning Studio. If the file is modified accidentally from another editor, you may lose some of your code.</p>
			<p>The tool opens in the<a id="_idIndexMarker799"/> global view, as shown here:</p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="Images/B16777_10_017.jpg" alt="Figure 10.17 – The error analysis dashboard loaded within the Jupyter environment&#13;&#10;" width="1650" height="1226"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – The error analysis dashboard loaded within the Jupyter environment</p>
			<p>In this view, you are<a id="_idIndexMarker800"/> looking at the model's error rates on overall data. In this view, you can see a binary tree that depicts data partitions between interpretable subgroups, which have unexpectedly high or low error rates. In our example, all the errors of the model occur for incomes less than or equal to <strong class="bold">6144</strong>, which accounts for a <strong class="bold">7.25%</strong> error rate, meaning that <strong class="bold">7.25%</strong> of the loans with monthly incomes less than <strong class="bold">6144</strong> were misclassified. Error coverage is the portion of all errors that fall into the node, and in this case, all the errors are located in this node (<strong class="bold">100%</strong>). The numbers within the node show the data representation. Here, <strong class="bold">5</strong> samples were wrong out of <strong class="bold">69</strong> records that belong in that node.</p>
			<p>Once you have selected a node in <strong class="bold">Tree map</strong>, you can click on <strong class="bold">Cohort settings</strong> or <strong class="bold">Cohort info</strong> and save those records as a cohort of interest. This cohort can be used in the explanation dashboard. By clicking on the <strong class="bold">Explanation</strong> button, you will be taken to the <strong class="bold">Data explorer</strong> view, as shown here:</p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="Images/B16777_10_018.jpg" alt="Figure 10.18 – The data explorer for the specific cohort selected in the tree map&#13;&#10;" width="1650" height="1391"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.18 – The data explorer for the specific cohort selected in the tree map</p>
			<p>This view has preselected <a id="_idIndexMarker801"/>the node's cohort. It has similar functionality to the explanation dashboard, such as seeing the feature importance that impacts the overall model predictions for a selected cohort. This view also offers the <strong class="bold">Local explanation</strong> tab, which allows you to understand individual error records and even perform what-if analysis to understand when the model would classify that record correctly.</p>
			<p>By clicking on the <strong class="bold">Error explorer</strong> link at the top-left corner of the widget, you will navigate back to the <strong class="bold">Tree map</strong> view. From the <strong class="bold">Error explorer:</strong> dropdown, select <strong class="bold">Heat map</strong> instead of <strong class="bold">Tree map</strong>, which <a id="_idIndexMarker802"/>is currently selected. This will lead you to the error heat map view, as shown here:</p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="Images/B16777_10_019.jpg" alt="Figure 10.19 – Error heat map view&#13;&#10;" width="1547" height="1576"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.19 – Error heat map view</p>
			<p>This view slices the data in a one- or two-dimensional way based on the features selected on the top left-hand side. The heat map visualizes cells with higher errors with a darker red color to bring the user's attention to regions with high error discrepancies. The cells with stripes indicate that no samples were evaluated, potentially indicating hidden pockets of errors.</p>
			<p>In this section, you were <a id="_idIndexMarker803"/>provided with an overview of the capabilities of the error analysis dashboard and how it can help you understand where your model is making errors. This tool can help you identify those error pockets and mitigate them by designing new features, collecting better data, discarding some of the current training data, or performing better hyperparameter tuning.</p>
			<p>In the next section, you will learn<a id="_idIndexMarker804"/> about Fairlearn, a tool that will help you assess your model's fairness and mitigate any observed unfairness issues. </p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor156"/>Detecting potential model fairness issues</h1>
			<p>Machine learning models can<a id="_idIndexMarker805"/> behave unfairly due to multiple reasons:</p>
			<ul>
				<li>Historical bias in society may be reflected in the data that was used to train the model.</li>
				<li>The decisions made by the developers of the model may have been skewed.</li>
				<li>Lack of representative data used to train the model. For example, there may be too few data points from a specific group of people.</li>
			</ul>
			<p>Since it is hard to identify the actual reasons that cause the model to behave unfairly, the definition of a model behaving unfairly is defined by its impact on people. There are two significant types of harm that a model can cause:</p>
			<ul>
				<li><strong class="bold">Allocation harm</strong>: This <a id="_idIndexMarker806"/>happens when the model withholds opportunities, resources, or information from a group of people. For example, during the hiring process or the loan lending example we have been working on so far, you may not have the opportunity to be hired or get a loan.</li>
				<li><strong class="bold">Quality-of-service harm</strong>: This <a id="_idIndexMarker807"/>happens when the system doesn't offer everyone the same quality of service. For example, it has reduced accuracy in terms of face detection for specific groups of people.</li>
			</ul>
			<p>Based on that, it is evident that model fairness issues cannot be solved automatically because there is no <a id="_idIndexMarker808"/>mathematical formulation. <strong class="bold">Fairlearn</strong> is a <a id="_idIndexMarker809"/>toolkit that provides tools that help assess and mitigate the fairness of the predictions of classification and regression models.</p>
			<p>In our case, if we treat age groups as a sensitive feature, we can analyze the model's behavior based on its accuracy with the following code:</p>
			<p class="source-code">from fairlearn.metrics import MetricFrame</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">y_pred = model_pipeline.predict(x_test)</p>
			<p class="source-code">age = x_test['age']</p>
			<p class="source-code">model_metrics = MetricFrame(accuracy_score, y_test, </p>
			<p class="source-code">                             y_pred, sensitive_features=age)</p>
			<p class="source-code">print(model_metrics.overall)</p>
			<p class="source-code">print(model_metrics.by_group[model_metrics.by_group &lt; 1])</p>
			<p>This code gets the predictions of the model you trained in the <em class="italic">Training a loans approval model</em> section and creates the predictions for the <strong class="source-inline">x_test</strong> dataset. Then, it assigns all values from the <strong class="source-inline">x_test['age']</strong> feature to the <strong class="source-inline">age</strong> variable. Then, by using <strong class="source-inline">MetricFrame</strong>, we can calculate the <strong class="source-inline">accuracy_score</strong> metric of the model either for the entire test dataset, which is stored in the <strong class="source-inline">overall</strong> attribute, or the accuracy by group, which is stored in the <strong class="source-inline">by_group</strong> attribute. This code prints the overall accuracy score and the accuracy score for the groups with less than 1. The results are shown in the following screenshot:</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="Images/B16777_10_020.jpg" alt="Figure 10.20 – The model has a 0.96 accuracy but for 65-year-olds its accuracy is 0.5&#13;&#10;" width="460" height="232"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.20 – The model has a 0.96 accuracy but for 65-year-olds its accuracy is 0.5</p>
			<p>Although the dataset was <a id="_idIndexMarker810"/>generated, you can see that the model's accuracy for 65-year-olds is only 50%. Note that although the model was trained with ages 18 to 85, only 35 subgroups were detected in the dataset, indicating that we may not be testing it accurately.</p>
			<p>Similar to <strong class="source-inline">ExplanationDashboard</strong> and <strong class="source-inline">ErrorAnalysisDashboard</strong>, the responsible AI widgets (<strong class="source-inline">raiwidgets</strong>) package offers a <strong class="source-inline">FairnessDashboard</strong>, which can be used to analyze the fairness of the model results.</p>
			<p class="callout-heading"> Important note</p>
			<p class="callout">At the time of writing this book, <strong class="source-inline">FairnessDashboard</strong> works in Jupyter. In the Notebooks experience, there are some technical glitches. Open your notebook in Jupyter to get the best experience out of <strong class="source-inline">FairnessDashboard</strong>.</p>
			<p>In a new cell, add the following code to invoke the fairness dashboard using the age-sensitive feature you defined in the preceding code:</p>
			<p class="source-code">from raiwidgets import FairnessDashboard</p>
			<p class="source-code">FairnessDashboard(sensitive_features=age, </p>
			<p class="source-code">                  y_true=y_test, y_pred=y_pred)</p>
			<p>After the launch, the <a id="_idIndexMarker811"/>widget will guide you through the fairness assessment process, where you will need to define the following:</p>
			<ul>
				<li><strong class="bold">Sensitive features</strong>: Here, you must<a id="_idIndexMarker812"/> configure the sensitive features. Sensitive features are used to split your data into groups, as we saw previously. In this case, it will prompt you to create five bins for the age groups (18-29, 30-40, 41-52, 53-64, and 64-75), and you can modify the binning process or even request it to treat each age on its own by selecting the <strong class="bold">Treat as categorical</strong> option it provides.</li>
				<li><strong class="bold">Performance metrics</strong>: Performance metrics<a id="_idIndexMarker813"/> are used to evaluate the quality of your model overall and in each group. In this case, you can select accuracy as we did previously. You can change this even after the wizard finishes.</li>
				<li><strong class="bold">Fairness metrics</strong>: Fairness metrics<a id="_idIndexMarker814"/> represent either the difference or ratio between the extreme values of a performance metric, or simply the worst value of any group. An example of such a metric is <strong class="bold">Accuracy score ratio</strong>, which is the minimum ratio accuracy score between any two groups. You can change this even after the wizard finishes.</li>
			</ul>
			<p>The resulting dashboard allows you to drill through your model's impact on the subgroups. It consists of two areas – the summarization table and the visualization area – where you can select different graphical representations, as shown here:</p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="Images/B16777_10_021.jpg" alt="Figure 10.21 – Fairness dashboard showing the accuracy of the model in various age groups&#13;&#10;" width="1650" height="1685"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.21 – Fairness dashboard showing the accuracy of the model in various age groups</p>
			<p>Once you have<a id="_idIndexMarker815"/> identified a fairness issue with your model, you can use the <strong class="bold">Fairlearn</strong> library to mitigate them. The <strong class="bold">Fairlearn</strong> library offers two approaches:</p>
			<ul>
				<li><strong class="bold">Postprocessing</strong>: These <a id="_idIndexMarker816"/>algorithms, similar to <strong class="source-inline">ThresholdOptimizer</strong>, adjust the output of the underlying model to achieve an explicit constraint, such as the constrain of equalizing odds. Equalizing odds in our binary classification model means that true-positive and false-positive rates should match across groups.</li>
				<li><strong class="bold">Reduction</strong>: This <a id="_idIndexMarker817"/>approach reweights and relabels the input data based on a constraint and then fits the underlying model by passing the <strong class="source-inline">sample_weight</strong> parameter that the <strong class="source-inline">fit</strong> <strong class="bold">sklearn</strong> method accepts.</li>
			</ul>
			<p>Using these<a id="_idIndexMarker818"/> techniques, you can balance the fairness of your model by sacrificing some of your model's performance to meet the needs of your business.</p>
			<p>The <strong class="bold">Fairlearn</strong> package is constantly evolving and has been integrated within the Azure Machine Learning SDK and the Studio web experience, enabling data scientists to upload model fairness insights into the Azure Machine Learning run history and observe the <strong class="bold">Fairlearn</strong> dashboard within Azure Machine Learning Studio.</p>
			<p>In this section, you learned how to detect potential unfair behaviors that your model may have. You also read about the possible mitigation techniques that can be implemented within the <strong class="bold">Fairlearn</strong> package. This concludes the tools provided by the Azure Machine Learning workspace and the open source communities that allow you to understand your models and assist you in creating AI.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, you were given an overview of the various tools that can help you understand your models. You started with the Interpret-Community package, which allows you to understand why the model is making its predictions. You learned about the various interpretation techniques and explored the explanation dashboard, which provides views such as feature importance. You then saw the error analysis dashboard, which allows you to determine where the model is performing poorly. Finally, you learned about the fairness evaluation techniques, the corresponding dashboard that enables you to explore potentially unfair results, and the methods you can use to mitigate potential fairness issues.</p>
			<p>In the next chapter, you will learn about Azure Machine Learning pipelines, which allow you to orchestrate model training and model results interpretation in a repeatable manner.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor158"/>Questions</h1>
			<p>In each chapter, you will find a couple of questions to help you conduct a knowledge check regarding the topics that have been discussed in each chapter:</p>
			<ol>
				<li value="1">You are using <strong class="source-inline">TabularExplainer</strong> to interpret a <strong class="bold">sklearn</strong> <strong class="source-inline">DecisionTreeClassifier</strong>. Which underlying SHAP explainer will be used?<p>a. <strong class="source-inline">DecisionTreeExplainer</strong></p><p>b. <strong class="source-inline">TreeExplainer</strong></p><p>c. <strong class="source-inline">KernelExplainer</strong></p><p>d. <strong class="source-inline">LinearExplainer</strong></p></li>
				<li>You want to interpret a <strong class="bold">sklearn</strong> <strong class="source-inline">DecisionTreeClassifier</strong> using <strong class="source-inline">MimicExplainer</strong>. Which of the following models can you use for the <strong class="source-inline">explainable_model</strong> parameter?<p>a. <strong class="source-inline">LGBMExplainableModel</strong></p><p>b. <strong class="source-inline">LinearExplainableModel</strong></p><p>c. <strong class="source-inline">SGDExplainableModel</strong></p><p>d. <strong class="source-inline">DecisionTreeExplainableModel</strong></p><p>e. All of the above</p></li>
				<li>Can you use <strong class="source-inline">PFIExplainer</strong> to produce local feature importance values?<p>a. Yes</p><p>b. No</p></li>
			</ol>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor159"/>Further reading</h1>
			<p>This section offers a list of useful web resources that will help you augment your knowledge of the Azure Machine Learning SDK and the various code snippets that were used in this chapter:</p>
			<ul>
				<li><strong class="bold">The SmartNoise</strong> library for differential privacy: <a href="https://github.com/opendp/smartnoise-core">https://github.com/opendp/smartnoise-core</a></li>
				<li>HE resources: <a href="https://www.microsoft.com/en-us/research/project/homomorphic-encryption/">https://www.microsoft.com/en-us/research/project/homomorphic-encryption/</a></li>
				<li>Deploying an encrypted inference web service: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-homomorphic-encryption-seal">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-homomorphic-encryption-seal</a></li>
				<li><strong class="bold">Presidio</strong>, the data protection and anonymization API: <a href="https://github.com/Microsoft/presidio">https://github.com/Microsoft/presidio</a></li>
				<li>Sample repository for aDevOps process in data science projects, also known as <strong class="bold">MLOps</strong>: <a href="https://aka.ms/mlOps">https://aka.ms/mlOps</a></li>
				<li><strong class="bold">Model Cards for Model Reporting</strong>: <a href="https://arxiv.org/pdf/1810.03993.pdf">https://arxiv.org/pdf/1810.03993.pdf</a></li>
				<li>The <strong class="bold">InterpretML</strong> website, with links to the GitHub repository of the community: <a href="https://interpret.ml/">https://interpret.ml/</a></li>
				<li>The <strong class="bold">Error Analysis</strong> home page, including guides on how to use the toolkit: <a href="https://erroranalysis.ai/">https://erroranalysis.ai/</a></li>
				<li>The <strong class="bold">Fairlearn</strong> home page: <a href="https://fairlearn.org/">https://fairlearn.org/</a></li>
			</ul>
		</div>
	</div></body></html>