["```py\n    from sklearn.datasets import make_classification\n    import pandas as pd\n    import numpy as np\n    features, target = make_classification(\n        n_samples=500, n_features=3, n_redundant=1, shift=0,\n        scale=1,weights=[0.7, 0.3], random_state=1337)\n    def fix_series(series, min_val, max_val):\n        series = series - min(series)\n        series = series / max(series)\n        series = series * (max_val - min_val) + min_val\n        return series.round(0)\n    features[:,0] = fix_series(features[:,0], 0, 10000)\n    features[:,1] = fix_series(features[:,1], 0, 10)\n    features[:,2] = fix_series(features[:,2], 18, 85)\n    classsification_df = pd.DataFrame(features, dtype='int')\n    classsification_df.set_axis(\n       ['income','credit_cards', 'age'],\n       axis=1, inplace=True)\n    classsification_df['approved_loan'] = target\n    classsification_df.head()\n    ```", "```py\n    from azureml.core import Workspace, Dataset\n    ws = Workspace.from_config()\n    dstore = ws.get_default_datastore()\n    loans_dataset = \\\n    Dataset.Tabular.register_pandas_dataframe(\n        dataframe=classsification_df,\n        target=(dstore,\"/samples/loans\"),\n        name=\"loans\",\n        description=\"A genarated dataset for loans\")\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X = classsification_df[['income','credit_cards', 'age']]\n    y = classsification_df['approved_loan'].values\n    x_train, x_test, y_train, y_test = \\\n            train_test_split(X, y, \n                           test_size=0.2, random_state=42)\n    ```", "```py\n    test_df = pd.DataFrame(data=[\n        [2000, 2, 45],\n        [2000, 9, 45],\n        [10000, 2, 45]\n    ], columns= ['income','credit_cards', 'age'])\n    test_pred = model_pipeline.predict(test_df)\n    print(test_pred)\n    ```", "```py\n    model_pipeline.named_steps['model'].feature_importances_\n    ```", "```py\n    from interpret.ext.blackbox import TabularExplainer\n    explainer = TabularExplainer(\n                  model_pipeline.named_steps['model'],\n                  initialization_examples=x_train, \n                  features= x_train.columns,\n                  classes=[\"Reject\", \"Approve\"],\n                  transformations=\n                    model_pipeline.named_steps['datatransformer']) \n    ```", "```py\n    local_explanation = explainer.explain_local(test_df)\n    sorted_local_values = \\\n        local_explanation.get_ranked_local_values()\n    sorted_local_names = \\\n        local_explanation.get_ranked_local_names()\n    for sample_index in range(0,test_df.shape[0]):\n        print(f\"Test sample number {sample_index+1}\")\n        print(\"\\t\", test_df.iloc[[sample_index]]\n                             .to_dict(orient='list'))\n        prediction = test_pred[sample_index]\n        print(\"\\t\", f\"The prediction was {prediction}\")\n        importance_values = \\\n            sorted_local_values[prediction][sample_index]\n        importance_names = \\\n            sorted_local_names[prediction][sample_index]\n        local_importance = dict(zip(importance_names,\n                                    importance_values))\n        print(\"\\t\", \"Local feature importance\")\n        print(\"\\t\", local_importance)\n    ```", "```py\n    from raiwidgets import ExplanationDashboard\n    ExplanationDashboard(global_explanation, model_pipeline, dataset=x_test, true_y=y_test)\n    ```", "```py\nfrom interpret.ext.glassbox import (\n    LGBMExplainableModel,\n    LinearExplainableModel,\n    SGDExplainableModel,\n    DecisionTreeExplainableModel\n)\nfrom interpret.ext.blackbox import MimicExplainer\nmimic_explainer = MimicExplainer(\n           model=model_pipeline, \n           initialization_examples=x_train,\n           explainable_model= DecisionTreeExplainableModel,\n           augment_data=True, \n           max_num_of_augmentations=10,\n           features=x_train.columns,\n           classes=[\"Reject\", \"Approve\"], \n           model_task='classification')\n```", "```py\nmimic_global_explanation = \\\n       mimic_explainer.explain_global(x_test)\nprint(\"Feature names:\", \n       mimic_global_explanation.get_ranked_global_names())\nprint(\"Feature importances:\",\n       mimic_global_explanation.get_ranked_global_values())\nprint(f\"Method used: {mimic_explainer._method}\")\n```", "```py\nfrom interpret.ext.blackbox import PFIExplainer\npfi_explainer = PFIExplainer(model_pipeline,\n                             features=x_train.columns,\n                             classes=[\"Reject\", \"Approve\"])\n```", "```py\npfi_global_explanation = \\\n        pfi_explainer.explain_global(x_test, \n                                     true_labels=y_test)\nprint(\"Feature names:\", \n        pfi_global_explanation.get_ranked_global_names())\nprint(\"Feature importances:\",\n        pfi_global_explanation.get_ranked_global_values())\nprint(f\"Method used: {pfi_explainer._method}\")\n```", "```py\nfrom azureml.core import Workspace, Experiment\nfrom azureml.interpret import ExplanationClient\nws = Workspace.from_config()\nexp = Experiment(workspace=ws, name=\"chapter10\")\nrun = exp.start_logging()\nclient = ExplanationClient.from_run(run)\nclient.upload_model_explanation(\n    global_explanation, true_ys= y_test,\n    comment='global explanation: TabularExplainer')\nrun.complete()\nprint(run.get_portal_url())\n```", "```py\nfrom azureml.core import Workspace, Dataset, Experiment\nfrom azureml.train.automl import AutoMLConfig\nws = Workspace.from_config()\ncompute_target = ws.compute_targets[\"cpu-sm-cluster\"]\nloans_dataset = Dataset.get_by_name(\n                            workspace=ws, name='loans')\ntrain_ds,validate_ds = loans_dataset.random_split(\n                             percentage=0.8, seed=1337)\n```", "```py\nexperiment_config = AutoMLConfig(\n    task = \"classification\",\n    primary_metric = 'accuracy',\n    training_data = train_ds,\n    label_column_name = \"approved_loan\",\n    validation_data = validate_ds,\n    compute_target = compute_target,\n    experiment_timeout_hours = 0.25,\n    iterations = 4,\n    model_explainability = True)\nautoml_experiment = Experiment(ws, 'loans-automl')\nautoml_run = automl_experiment.submit(experiment_config)\nautoml_run.wait_for_completion(show_output=True)\n```", "```py\nfrom raiwidgets import ErrorAnalysisDashboard\nErrorAnalysisDashboard(global_explanation, model_pipeline, \n                       dataset=x_test, true_y=y_test)\n```", "```py\nfrom fairlearn.metrics import MetricFrame\nfrom sklearn.metrics import accuracy_score\ny_pred = model_pipeline.predict(x_test)\nage = x_test['age']\nmodel_metrics = MetricFrame(accuracy_score, y_test, \n                             y_pred, sensitive_features=age)\nprint(model_metrics.overall)\nprint(model_metrics.by_group[model_metrics.by_group < 1])\n```", "```py\nfrom raiwidgets import FairnessDashboard\nFairnessDashboard(sensitive_features=age, \n                  y_true=y_test, y_pred=y_pred)\n```"]