<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer271">
			<h1 id="_idParaDest-150"><em class="italic"><a id="_idTextAnchor160"/>Chapter 11</em>: Working with Pipelines</h1>
			<p>In this chapter, you will learn how you can author repeatable processes, defining pipelines that consist of multiple steps. You can use these pipelines to author training pipelines that transform your data and then train models, or you can use them to perform batch inferences using pre-trained models. Once you register one of those pipelines, you can invoke it using either an HTTP endpoint or through the SDK, or even configure them to execute on a schedule. With this knowledge, you will be able to implement and consume pipelines by using the <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">AzureML</strong>) SDK.</p>
			<p>In this chapter, we are going to cover the following main topics:</p>
			<ul>
				<li>Understanding AzureML pipelines</li>
				<li>Authoring a pipeline</li>
				<li>Publishing a pipeline to expose it as an endpoint</li>
				<li>Scheduling a recurring pipeline</li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor161"/>Technical requirements</h1>
			<p>You will need to have access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control (IAM)</strong> role on the resource group level. Within that resource group, you should have already deployed a <strong class="bold">machine learning</strong> resource named <strong class="source-inline">packt-learning-mlw</strong>, as described in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>You will also need to have a basic understanding of the <strong class="bold">Python</strong> language. The code snippets target Python version 3.6 or newer. You should also be familiar with working in the notebook experience within AzureML studio, something that was covered in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>.</p>
			<p>This chapter assumes you have registered the <strong class="bold">loans</strong> dataset you generated in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. It is also assumed that you have created a compute cluster named <strong class="bold">cpu-sm-cluster</strong>, as described in the <em class="italic">Working with compute targets</em> section in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p>
			<p>You can find all the notebooks and code snippets for this chapter in GitHub at the following URL: <a href="http://bit.ly/dp100-ch11">http://bit.ly/dp100-ch11</a>.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor162"/>Understanding AzureML pipelines</h1>
			<p>In <a href="B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 6</em></a>, <em class="italic">Visual Model Training and Publishing</em>, you saw how you can design a training process using <a id="_idIndexMarker819"/>building boxes. Similar to those workflows, the AzureML SDK allows you to author <strong class="source-inline">Pipelines</strong> that orchestrate multiple steps. For example, in this chapter, you will author a <strong class="source-inline">Pipeline</strong> that consists of two steps. The first step pre-processes the <strong class="bold">loans</strong> dataset that is regarded as raw training data and stores it in a temporary location. The second step then reads this data and trains a machine learning model, which will be stored in a blob store location. In this example, each step will be nothing more than a Python script file that is being executed in a specific compute target using a predefined <strong class="source-inline">Environment</strong>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Do not confuse the <strong class="bold">AzureML</strong> <strong class="source-inline">Pipelines</strong> with the <strong class="bold">sklearn</strong> <strong class="source-inline">Pipelines</strong> you read in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. The <strong class="bold">sklearn</strong> ones <a id="_idIndexMarker820"/>allow you to chain various transformations and feature engineering methods to transform the data that the model is fed. These transformations are performed every time you try to pass any data to the model, either during training or at inference time. You can think of the <strong class="bold">sklearn</strong> <strong class="source-inline">Pipelines</strong> as a wrapper around the actual model class that you want to train and use for inferences.</p>
			<p>The AzureML SDK offers quite a few building blocks that you can use to construct a <strong class="source-inline">Pipeline</strong>. <em class="italic">Figure 11.1</em> contains the most popular classes that you may encounter in the exam and real-life code:</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="Images/B16777_11_001.jpg" alt="Figure 11.1 – Classes available in the AzureML SDK to author your pipelines&#13;&#10;" width="1362" height="685"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Classes available in the AzureML SDK to author your pipelines</p>
			<p>The <strong class="bold">Pipeline</strong> is the core <a id="_idIndexMarker821"/>class that defines a workflow that stitches together multiple steps. You can pass in parameters to a pipeline by defining them using the <strong class="bold">PipelineParameter</strong> class. These parameters can be references in one or more steps within the <strong class="bold">Pipeline</strong>. Once you have finished defining a pipeline, you can publish it to register it in the AzureML workspace as a versioned object that can be referenced using the <strong class="bold">PublishedPipeline</strong> class. This published pipeline has an endpoint that you can use to trigger its execution. If you want, you can define a <strong class="bold">Schedule</strong> and have this <strong class="bold">PublishedPipeline</strong> class triggered at a specific time interval. <strong class="bold">PipelineData</strong> defines temporary storage where one step can drop some files for the next one to pick them up. The data dependency between those two steps creates an implicit execution order in the <strong class="bold">Pipeline</strong>, meaning that the dependent step will wait for the first step to complete. You will be using all these classes in this chapter.</p>
			<p>In the <strong class="bold">azureml.pipeline.steps</strong> module, you will find all the available steps you can use. The most commonly used steps are the following:</p>
			<ul>
				<li><strong class="bold">PythonScriptStep</strong>: This<a id="_idIndexMarker822"/> step allows you to execute a Python script. You will be using this step in this chapter.</li>
				<li><strong class="bold">AutoMLStep</strong>: This<a id="_idIndexMarker823"/> step encapsulates an automated ML run. It requires <a id="_idTextAnchor163"/>the <strong class="source-inline">AutoMLConfig</strong> object you saw in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>.</li>
				<li><strong class="bold">HyperDriveStep</strong>: This<a id="_idIndexMarker824"/> step allows you to run a hyperparameter tuning, passing the <strong class="source-inline">HyperDriveConfig</strong> parameter you saw in <a href="B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Optimizing the ML Model</em>.</li>
				<li><strong class="bold">DataTransferStep</strong>: A <strong class="bold">Pipeline</strong> step that<a id="_idIndexMarker825"/> allows you to transfer data between AzureML-supported storage options.</li>
				<li><strong class="bold">DatabricksStep</strong>: This <a id="_idIndexMarker826"/>allows you to execute a DataBricks notebook, Python script, or JAR file in an attached DataBricks cluster.</li>
				<li><strong class="bold">ParallelRunStep</strong>: This<a id="_idIndexMarker827"/> step allows you to process large amounts of data in parallel. You will be using this step in the next chapter, <a href="B16777_12_Final_VK_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Models with Code</em>, where you will be creating a batch inference pipeline.<p class="callout-heading">Important note</p><p class="callout">In the past, AzureML provided an <strong class="source-inline">Estimator</strong> class that represented a generic training script. Some framework-specific estimators inherited from that generic <strong class="source-inline">Estimator</strong> class, such as <strong class="source-inline">TensorFlow</strong> and <strong class="source-inline">PyTorch</strong>. To incorporate one of those estimators in your pipelines, you would have used an <strong class="source-inline">EstimatorStep</strong>. The whole <strong class="source-inline">Estimator</strong> class and its derivatives have been deprecated in favor of <strong class="source-inline">ScriptRunConfig</strong>, which you have used in the previous chapters. If, during the exam, you see a deprecated reference to an <strong class="source-inline">EstimatorStep</strong>, you can treat it as a <strong class="source-inline">PythonScriptStep</strong>.</p></li>
			</ul>
			<p>The last major piece of a <strong class="bold">Pipeline</strong> is the data that flows through it.</p>
			<ul>
				<li><strong class="bold">DataPath</strong> allows<a id="_idIndexMarker828"/> you to reference a path in the data store. For example, in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>, in the <em class="italic">Working with datasets</em> section, you used the <strong class="source-inline">(dstore,"/samples/diabetes")</strong> tuple to indicate where you wanted to store the data when you called the <strong class="source-inline">register_pandas_dataframe</strong> method of a <strong class="source-inline">TabularDataset</strong>. Instead of that tuple, you could have passed the equivalent <strong class="source-inline">DataPath(datastore=dstore, path_on_datastore="/samples/diabetes")</strong>.</li>
				<li><strong class="bold">OutputFileDatasetConfig</strong> configures <a id="_idIndexMarker829"/>the datastore location where you want to upload the contents of a local path that resides within a compute target that executes a specific test. In <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, in the <em class="italic">Tracking metrics in experiments</em> section, you saw how the <strong class="source-inline">outputs</strong> folder was automatically uploaded to a <strong class="source-inline">Run</strong> execution. Similar to that folder, you can define additional local folders that will be automatically uploaded to a target path in a target datastore. In this chapter, you will be using this class to store the produced model in a specific location within the default blob storage account.</li>
				<li><strong class="bold">DataReference</strong> represents<a id="_idIndexMarker830"/> a path in a datastore and can be used to describe how and where data should be made available in a run. It is no longer the recommended approach for data referencing in AzureML. If you encounter it in an obsolete exam question, you can treat it as a <strong class="bold">DataPath</strong> object.</li>
			</ul>
			<p>In this section, you learned about the building blocks you can use to construct an AzureML <strong class="bold">Pipeline</strong>. In the next section, you will get some hands-on experience of using those classes. </p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor164"/>Authoring a pipeline</h1>
			<p>Let's assume that you need to create<a id="_idIndexMarker831"/> a repeatable workflow that has two steps:</p>
			<ol>
				<li>It loads the data from a registered dataset and splits it into training and test datasets. These datasets are <a id="_idIndexMarker832"/>converted into a special construct needed by the <strong class="bold">LightGBM</strong> tree-based algorithm. The converted constructs are stored to be used by the next step. In our case, you will use the <strong class="bold">loans</strong> dataset that you registered in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. You will be writing the code for this step within a folder named <strong class="source-inline">step01</strong>.</li>
				<li>It loads the pre-processed data and<a id="_idIndexMarker833"/> trains a <strong class="bold">LightGBM</strong> model that is then stored in the <strong class="source-inline">/models/loans/</strong> folder of the default datastore attached to the AzureML workspace. You will be writing the code for this step within a folder named <strong class="source-inline">step02</strong>.<p>Each step will be a <a id="_idIndexMarker834"/>separate Python file, taking some arguments to specify where to read the data from and where to write the data to. These scripts will utilize the same mechanics as the scripts you authored in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. What is different in this chapter is that instead of invoking each Python script separately, you will create a <strong class="source-inline">Pipeline</strong> that will invoke those steps one after the other. In <em class="italic">Figure 11.2</em>, you can see the overall inputs and outputs each script is going to have, along with the parameters you will need to configure for each step to execute: </p></li>
			</ol>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="Images/B16777_11_002.jpg" alt="Figure 11.2 – Inputs and outputs of each pipeline step&#13;&#10;" width="1650" height="566"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Inputs and outputs of each pipeline step</p>
			<p>Based on <em class="italic">Figure 11.2</em>, for each step, you will need to define the compute target and the <strong class="source-inline">Environment</strong> that will be used to execute the specific Python script. Although each step can have a separate compute target and a separate <strong class="source-inline">Environment</strong> specified, you will be running both steps using the same <strong class="source-inline">Environment</strong>, and the same compute target to simplify the code. You will be using the out-of-the-box <strong class="bold">AzureML-Tutorial</strong> <strong class="source-inline">Environment</strong>, which contains standard data science packages, including <a id="_idIndexMarker835"/>the <strong class="bold">LightGBM</strong> library that your scripts will require. You will be executing the steps in the <strong class="bold">cpu-sm-cluster</strong> cluster you created in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p>
			<p>You will start by <a id="_idIndexMarker836"/>authoring the <strong class="source-inline">Pipeline</strong>, and then you will author the actual Python scripts required for each step. Navigate to the <strong class="bold">Notebooks</strong> section of your AzureML studio web interface. In your user folder, create a folder named <strong class="source-inline">chapter11</strong> and then create a notebook named <strong class="source-inline">chapter11.ipynb</strong>, as seen in <em class="italic">Figure 11.3</em>: </p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="Images/B16777_11_003.jpg" alt="Figure 11.3 – Adding the chapter11 notebook to your working files&#13;&#10;" width="1266" height="1005"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Adding the chapter11 notebook to your working files</p>
			<p>Open the newly created notebook and follow the steps to author an AzureML pipeline using the AzureML SDK:</p>
			<ol>
				<li value="1">You will start by <a id="_idIndexMarker837"/>getting a reference to your workspace. Then you will get references to the <strong class="source-inline">loans</strong> dataset and the <strong class="source-inline">cpu-sm-cluster</strong>. Add the following code to a cell in your notebook:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">loans_ds = ws.datasets['<strong class="bold">loans</strong>']</p><p class="source-code">compute_target = ws.compute_targets['<strong class="bold">cpu-sm-cluster</strong>']</p><p>If you are having difficulties understanding this code snippet, please review <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p></li>
				<li>You will need to create a configuration object that will dictate the use of the <strong class="bold">AzureML-Tutorial</strong> <strong class="source-inline">Environment</strong> when each step gets executed. To do that, you will need to create a <strong class="source-inline">RunConfiguration</strong> using the following code:<p class="source-code">from azureml.core import RunConfiguration</p><p class="source-code">runconfig = RunConfiguration()</p><p class="source-code">runconfig.environment = ws.environments['<strong class="bold">AzureML-Tutorial</strong>']</p><p>In this code, you create a new <strong class="source-inline">RunConfiguration</strong> object, and you assign the <a id="_idIndexMarker838"/>predefined <strong class="bold">AzureML-Tutorial</strong> <strong class="source-inline">Environment</strong> to its <strong class="source-inline">environment</strong> attribute. To help you understand how this <strong class="source-inline">RunConfiguration</strong> object relates to the work you have been doing in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, the <strong class="source-inline">ScriptRunConfig</strong> you have been using in that chapter had an optional <strong class="source-inline">run_config</strong> parameter where you could have passed this <strong class="source-inline">RunConfiguration</strong> object you defined in this cell.</p></li>
				<li>You will then need to define a temporary storage folder where the first step will drop the output files. You will use the <strong class="source-inline">PipelineData</strong> class using the following code:<p class="source-code">from azureml.pipeline.core import PipelineData</p><p class="source-code">step01_output = PipelineData(</p><p class="source-code">    "training_data",</p><p class="source-code">    datastore= ws.get_default_datastore(),</p><p class="source-code">    is_directory=True) </p><p>In this code, you are<a id="_idIndexMarker839"/> creating an intermediate data location named <strong class="source-inline">training_data</strong>, which is stored as a folder in the default datastore that is registered in your AzureML workspace. You should not care about the actual path of this temporary data, but if you are curious, the actual path of that folder in the default storage container is something like <strong class="source-inline">azureml/{step01_run_id}/training_data</strong>.</p></li>
				<li>Now that you have all the prerequisites for your pipeline's first step, it is time to define it. In a new cell, add the following code:<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p><p class="source-code">step_01 = PythonScriptStep(</p><p class="source-code">   'prepare_data.py', </p><p class="source-code">    source_directory='step01',</p><p class="source-code">    arguments = [</p><p class="source-code">        "--dataset", loans_ds.as_named_input('loans'), </p><p class="source-code">        "--output-path", step01_output],</p><p class="source-code">    name='Prepare data',</p><p class="source-code">    runconfig=runconfig,</p><p class="source-code">    compute_target=compute_target,</p><p class="source-code">    outputs=[step01_output],</p><p class="source-code">    allow_reuse=True</p><p class="source-code">)</p><p>This code defines a <strong class="source-inline">PythonScriptStep</strong> that will be using the source code in the <strong class="source-inline">step01</strong> folder. It will execute the script named <strong class="source-inline">prepare_data.py</strong>, passing the following arguments:</p><ul><li><strong class="source-inline">--dataset</strong>: This<a id="_idIndexMarker840"/> passes the <strong class="source-inline">loans_ds</strong> dataset ID to that variable. This dataset ID is a unique <strong class="bold">GUID</strong> representing the specific version of the dataset as it is registered within your AzureML workspace. The code goes one step further and makes a call to the <strong class="source-inline">as_named_input</strong> method. This method is available in both <strong class="source-inline">FileDataset</strong> and <strong class="source-inline">TabularDataset</strong> and is only applicable when a <strong class="source-inline">Run</strong> executes within the AzureML workspace. To invoke the method, you must provide a name, in this case, <strong class="source-inline">loans</strong>, that can be used within the script to retrieve the dataset. The AzureML SDK will make the <strong class="source-inline">TabularDataset</strong> object available within the <strong class="source-inline">prepare_data.py</strong> script in the <strong class="source-inline">input_datasets</strong> dictionary of the <strong class="source-inline">run</strong> object. Within the <strong class="source-inline">prepare_data.py</strong> script, you can get a reference to that dataset using the following code:<p class="source-code">run = Run.get_context()</p><p class="source-code">loans_dataset = run.input_datasets["loans"]</p></li><li><strong class="source-inline">--output-path</strong>: This passes the <strong class="source-inline">PipelineData</strong> object you created in <em class="italic">Step 3</em>. This parameter will be a string representing a path where the script can store its output files. The datastore location is mounted to the local storage of the compute node that is about to execute the specific step. This mounting path is passed to the script, allowing your script to transparently write the outputs directly to the datastore.<p>Coming back to the arguments you pass to the <strong class="source-inline">PythonScriptStep</strong> initialization, you define a name that will be visible in the visual representation of the pipeline seen in <em class="italic">Figure 11.6</em>. In the <strong class="source-inline">runconfig</strong> parameter, you pass the <strong class="source-inline">RunConfiguration</strong> object that you defined in <em class="italic">Step 2</em>. In the <strong class="source-inline">compute_target</strong> parameter, you pass the reference to the <strong class="source-inline">cpu-sm-cluster</strong> cluster that you got in <em class="italic">Step 1</em>.</p><p>In the <strong class="source-inline">outputs</strong> parameter, you pass an array of outputs to which this step will be posting data. This<a id="_idIndexMarker841"/> is a very important parameter to define the right execution order of the steps within the pipeline. Although you are passing the <strong class="source-inline">PipelineData</strong> object as an argument to the script, the AzureML SDK is not aware of whether your script will be writing or reading data from that location. By explicitly adding the <strong class="source-inline">PipelineData</strong> object to the <strong class="source-inline">outputs</strong> parameter, you mark this step as a producer of the data stored in the <strong class="source-inline">PipelineData</strong> object. Thus, anyone referencing the same object in the corresponding <strong class="source-inline">inputs</strong> parameter will need to execute after this <strong class="source-inline">PythonScriptStep</strong>.</p><p>The <strong class="source-inline">allow_reuse</strong> Boolean parameter allows you to reuse the outputs of this <strong class="source-inline">PythonScriptStep</strong> if the inputs of the script and the source code within the <strong class="source-inline">step01</strong> folder haven't changed since the last execution of the pipeline. Since the only input of this step is a specific version of the <strong class="bold">loans</strong> <strong class="source-inline">TabularDataset</strong>, it cannot change. Although you did not specify a particular version when you referenced the <strong class="source-inline">TabularDataset</strong>, the latest version was automatically selected. This version was pinned to the pipeline's definition at creation time. The pipeline will keep executing on the pinned version, even if you create a new version of the <strong class="source-inline">TabularDataset</strong>. Moreover, since the <strong class="source-inline">allow_reuse</strong> parameter is set to <strong class="source-inline">True</strong>, this step will run only once, and from there on, the results will be automatically reused. At the end of this section, you will see how this affects the pipeline execution time when you rerun the same pipeline.</p><p class="callout-heading">Important note</p><p class="callout">If you wanted to force the pipeline to read the latest version of the <strong class="bold">loans</strong> dataset, you could re-register the pipeline after creating the new version of the dataset. The authoring code would be exactly the same, only this time, the <strong class="source-inline">loans_ds</strong> variable would reference the latest version of the <strong class="source-inline">TabularDataset</strong>. At the end of this section, you will also learn how you can pass the training dataset as a <strong class="source-inline">PipelineParameter</strong>.</p></li></ul></li>
				<li>Now that you have<a id="_idIndexMarker842"/> defined the <strong class="source-inline">PythonScriptStep</strong>, it is time to add the missing Python script to your files. Next to your notebook, under the <strong class="source-inline">chapter11</strong> folder you are currently working on, add a new folder named <strong class="source-inline">step01</strong>. Within that folder, add a new Python script file named <strong class="source-inline">prepare_data.py</strong>. The final folder structure should be similar to the one shown in <em class="italic">Figure 11.4</em>:<div id="_idContainer265" class="IMG---Figure"><img src="Images/B16777_11_004.jpg" alt="Figure 11.4 – Folder structure for the prepare_data.py script that will be executed in your pipeline&#13;&#10;" width="634" height="865"/></div><p class="figure-caption">Figure 11.4 – Folder structure for the prepare_data.py script that will be executed in your pipeline</p></li>
				<li>Add the following <a id="_idIndexMarker843"/>code blocks within the <strong class="source-inline">prepare_data.py</strong> file. Instead of typing all this code, you can download it directly from the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section of this chapter:<p class="source-code">import argparse</p><p class="source-code">from azureml.core.run import Run</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">import lightgbm as lgb</p><p class="source-code">import os</p><p>These are all the<a id="_idIndexMarker844"/> imports you will need within the script file. You are going to create the training and test datasets using the <strong class="source-inline">train_test_split</strong> method for the <strong class="bold">sklearn</strong> library. Moreover, you are going to be referring to the <strong class="source-inline">lightgbm</strong> library with the <strong class="source-inline">lgb</strong> short alias:</p><p class="source-code">parser = argparse.ArgumentParser()</p><p class="source-code">parser.add_argument("--dataset", type=str, dest="dataset")</p><p class="source-code">parser.add_argument("--output-path", type=str, </p><p class="source-code">    dest="output_path",</p><p class="source-code">    help="Directory to store datasets")</p><p class="source-code">args = parser.parse_args()</p><p>This script creates an <strong class="source-inline">ArgumentParser</strong> that parses the arguments you passed when you defined the <strong class="source-inline">PythonScriptStep</strong> in <em class="italic">Step 4</em>. As a reminder, the <strong class="source-inline">--dataset</strong> parameter is going to contain the dataset ID that the script will need to process, and the <strong class="source-inline">--output-path</strong> parameter will be the local path location where the script is supposed to write the transformed datasets:</p><p class="source-code">run = Run.get_context()</p><p class="source-code">loans_dataset = run.input_datasets["loans"]</p><p>Right after parsing the arguments, you are getting a reference to the <strong class="source-inline">Run</strong> context. From there, you get a reference to the <strong class="source-inline">loans</strong> dataset, something that becomes available to you because you called the <strong class="source-inline">as_named_input</strong> method as discussed in <em class="italic">Step 4</em>. Later in this section, you will read about how you could have rewritten this code block to be able to run the same script in your local computer without a <strong class="source-inline">Run</strong> context:</p><p class="source-code">print(f"Dataset id: {loans_dataset.id}")</p><p>This code block<a id="_idIndexMarker845"/> prints the ID of the dataset that your code picked as a reference. If you print the ID passed to the <strong class="source-inline">--dataset</strong> parameter, and which is stored in the <strong class="source-inline">args.dataset</strong> variable, you will notice that these two values are identical:</p><p class="source-code">loans_df = loans_dataset.to_pandas_dataframe()</p><p class="source-code">x = loans_df[["income", "credit_cards", "age"]]</p><p class="source-code">y = loans_df["approved_loan"].values</p><p class="source-code">feature_names = x.columns.to_list()</p><p class="source-code">x_train, x_test, y_train, y_test = train_test_split(</p><p class="source-code">    x, y, test_size=0.2, random_state=42, stratify=y</p><p class="source-code">)</p><p>In this code block, you load the dataset into memory and use the <strong class="bold">sklearn</strong> <strong class="source-inline">train_test_split</strong> method to split the dataset into training and test features (<strong class="source-inline">x_train</strong> and <strong class="source-inline">x_test</strong>) and training and test labels (<strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong>):</p><p class="source-code">train_data = lgb.Dataset(x_train, label=y_train, feature_name=feature_names)</p><p class="source-code">test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)</p><p>The features and the labels are then converted into <strong class="source-inline">train_data</strong> and <strong class="source-inline">test_data</strong>, which are <strong class="bold">LightGBM</strong> <strong class="source-inline">Dataset</strong> objects. <strong class="bold">LightGBM</strong> requires a special <strong class="source-inline">Dataset</strong> format for training and validation. Note that the validation dataset stored in the <strong class="source-inline">test_data</strong> variable needs to reference the training data (<strong class="source-inline">train_data</strong>). This is a failsafe mechanism embedded by <strong class="bold">LightGBM</strong> to prevent the accidental passing of incompatible validation data. <strong class="bold">LightGBM</strong> constructs the <a id="_idIndexMarker846"/>validation dataset using the reference dataset as a template and then populates the actual validation data. This process ensures that all categorical values are available in both the training and the validation dataset structures. If you don't do this, you will end up with the cryptic <em class="italic">Cannot add validation data, since it has different bin mappers with training data</em> error message when you attempt to train the <strong class="bold">LightGBM</strong> model:</p><p class="source-code">output_path = args.output_path</p><p class="source-code">if not os.path.exists(output_path):</p><p class="source-code">    os.makedirs(output_path)</p><p class="source-code">train_data.save_binary(os.path.join(output_path, "train_dataset.bin"))</p><p class="source-code">test_data.save_binary(os.path.join(output_path, "validation_dataset.bin"))</p><p>In the last code block of the script, you create the <strong class="source-inline">output_path</strong> folder, if it doesn't already exist, and then use the native <strong class="source-inline">save_binary</strong> method of the <strong class="bold">LightGBM</strong> <strong class="source-inline">Dataset</strong> to serialize the dataset into a binary file that is optimized for storing and loading.</p><p>In contrast to the scripts you created in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, the <strong class="source-inline">prepare_data.py</strong> file cannot execute on your local computer as an <strong class="source-inline">_OfflineRun</strong>. This is because you have a dependency on the <strong class="source-inline">input_datasets</strong> dictionary that is only available if the <strong class="source-inline">Run</strong> is executing within the AzureML workspace. If <a id="_idIndexMarker847"/>you wanted to test this file locally before using it within the <strong class="source-inline">Pipeline</strong>, you could use the following code instead:</p><p class="source-code">run = Run.get_context()</p><p class="source-code">loans_dataset = None</p><p class="source-code">if type(run) == _OfflineRun:</p><p class="source-code">    from azureml.core import Workspace, Dataset</p><p class="source-code">    ws = Workspace.from_config()</p><p class="source-code">    if args.dataset in ws.datasets:</p><p class="source-code">        loans_dataset = ws.datasets[args.dataset]</p><p class="source-code">    else:</p><p class="source-code">        loans_dataset = Dataset.get_by_id(ws, args.dataset)</p><p class="source-code">else:</p><p class="source-code">    loans_dataset = run.input_datasets["loans"]</p><p>This code checks whether this is an offline run. In that case, it first gets a reference to the workspace as you saw in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>, and then it checks whether the <strong class="source-inline">--dataset</strong> parameter stored in the <strong class="source-inline">args.dataset</strong> variable is a dataset name. If it is, the latest version of the dataset is assigned to the <strong class="source-inline">loans_dataset</strong> variable. If it is not a name, the script assumes it is a GUID, which should represent the ID of a specific dataset version. In that case, the script tries the <strong class="source-inline">get_by_id</strong> method to retrieve the specific dataset or throw an error if the value passed is not a known dataset ID. If the run is online, you can still use the <strong class="source-inline">input_datasets</strong> dictionary to retrieve the dataset reference.</p></li>
				<li>Back to your notebook, you will start defining the prerequisites for the second step, the model training phase of your <strong class="source-inline">Pipeline</strong>. In <em class="italic">Figure 11.2</em>, you saw that this step requires a parameter named <strong class="source-inline">learning_rate</strong>. Instead of hardcoding the<a id="_idIndexMarker848"/> learning rate hyperparameter of the <strong class="bold">LightGBM</strong> model within your training script, you will use a <strong class="source-inline">PipelineParameter</strong> to pass in this value. This parameter will be defined at <strong class="source-inline">Pipeline</strong> level, and it will be passed to the training script as an argument, as you will see in <em class="italic">Step 9</em>. To create such a parameter, use the following code:<p class="source-code">from azureml.pipeline.core import PipelineParameter</p><p class="source-code">learning_rate_param = PipelineParameter( name="learning_rate", default_value=0.05)</p><p>This code defines a new <strong class="source-inline">PipelineParameter</strong> named <strong class="source-inline">learning_rate</strong>. The default value will be <strong class="source-inline">0.05</strong>, meaning that you can omit to pass this parameter when you execute the pipeline, and this default value will be used. You will see later in <em class="italic">Step 13</em> how you can execute the <strong class="source-inline">Pipeline</strong> and specify a value other than the default.</p></li>
				<li>You will store the trained model in the <strong class="source-inline">/models/loans/</strong> folder of the default datastore attached to the AzureML workspace. To specify the exact location where you want to store the files, you will use the <strong class="source-inline">OutputFileDatasetConfig</strong> class. In a new notebook cell, add the following code:<p class="source-code">from azureml.data import OutputFileDatasetConfig</p><p class="source-code">datastore = ws.get_default_datastore()</p><p class="source-code">step02_output = OutputFileDatasetConfig(</p><p class="source-code">    name = "model_store",</p><p class="source-code">    destination = (datastore, '/models/loans/'))</p><p>In this script, you are <a id="_idIndexMarker849"/>getting a reference to the default datastore. Then, you create an <strong class="source-inline">OutputFileDatasetConfig</strong> object, passing a tuple to the <strong class="source-inline">destination</strong> parameter. This tuple consists of the datastore you selected and the path within that datastore. You could have selected any datastore you have attached in the AzureML workspace. This <strong class="source-inline">OutputFileDatasetConfig</strong> object defines the destination to copy the outputs to. If you don't specify the <strong class="source-inline">destination</strong> argument, the default <strong class="source-inline">/dataset/{run-id}/{output-name}</strong> value is used. Note that <strong class="source-inline">destination</strong> allows you to use placeholders while defining the path. The default value uses both the <strong class="source-inline">{run-id}</strong> and <strong class="source-inline">{output-name}</strong> placeholders that are currently supported. These placeholders will be replaced with the corresponding values at the appropriate time.</p></li>
				<li>Now that you<a id="_idIndexMarker850"/> have all the prerequisites defined, you can define the second step of your <strong class="source-inline">Pipeline</strong>. In a new cell in your notebook, add the following code:<p class="source-code">step_02 = PythonScriptStep(</p><p class="source-code">   'train_model.py', </p><p class="source-code">    source_directory='step02',</p><p class="source-code">    arguments = [</p><p class="source-code">        "--learning-rate", learning_rate_param,</p><p class="source-code">        "--input-path", step01_output,</p><p class="source-code">        "--output-path", step02_output],</p><p class="source-code">    name='Train model',</p><p class="source-code">    runconfig=runconfig,</p><p class="source-code">    compute_target=compute_target,</p><p class="source-code">    inputs=[step01_output],</p><p class="source-code">    outputs=[step02_output]</p><p class="source-code">)</p><p>Similar to the <strong class="source-inline">step_01</strong> folder you created in <em class="italic">Step 4</em>; this code defines a <strong class="source-inline">PythonScriptStep</strong> that will invoke the <strong class="source-inline">train_model.py</strong> script located in the <strong class="source-inline">step02</strong> folder. It will populate the <strong class="source-inline">--learning-rate</strong> argument using the value passed to <a id="_idIndexMarker851"/>the <strong class="source-inline">PipelineParameter</strong> you defined in <em class="italic">Step 7</em>. It will also pass the output of <strong class="source-inline">step_01</strong> to the <strong class="source-inline">--input-path</strong> argument. Note that <strong class="source-inline">step01_output</strong> is also added to the list of inputs of this <strong class="source-inline">PythonScriptStep</strong>. This forces <strong class="source-inline">step_02</strong> to wait for <strong class="source-inline">step_01</strong> to complete in order to consume the data stored in <strong class="source-inline">step01_output</strong>. The last script argument is <strong class="source-inline">--output-path</strong>, where you pass the <strong class="source-inline">OutputFileDatasetConfig</strong> object you created in the previous step. This object is also added to the list of outputs of this <strong class="source-inline">PythonScriptStep</strong>.</p></li>
				<li>Let's create the<a id="_idIndexMarker852"/> Python script that will be executed by <strong class="source-inline">step_02</strong>. Next to your notebook, under the <strong class="source-inline">chapter11</strong> folder you are currently working on, add a new folder named <strong class="source-inline">step02</strong>. Within that folder, add a new Python script file named <strong class="source-inline">train_model.py</strong>. The final folder structure should be similar to the one shown in <em class="italic">Figure 11.5</em>:<div id="_idContainer266" class="IMG---Figure"><img src="Images/B16777_11_005.jpg" alt="Figure 11.5 – The training script that will be executed in the step02 folder of your pipeline&#13;&#10;" width="668" height="937"/></div><p class="figure-caption">Figure 11.5 – The training script that will be executed in the step02 folder of your pipeline</p></li>
				<li>Open the <strong class="source-inline">train_model.py</strong> file and add the following code blocks to it. Instead of<a id="_idIndexMarker853"/> typing all this code, you can download it directly from the GitHub repository mentioned in the <em class="italic">Technical requirements</em> section of this chapter:<p class="source-code">import argparse</p><p class="source-code">import os</p><p class="source-code">import lightgbm as lgb</p><p class="source-code">import joblib</p><p class="source-code">parser = argparse.ArgumentParser()</p><p>This block imports all modules you will need in the file and creates an <strong class="source-inline">ArgumentParser</strong> to read the arguments you will be passing to this script. If you so wished, you could have used another famous library for script parameter parsing called <strong class="bold">click</strong>. You can learn<a id="_idIndexMarker854"/> more about this library in the <em class="italic">Further reading</em> section:</p><p class="source-code">parser.add_argument(</p><p class="source-code">    "--learning-rate",</p><p class="source-code">    type=float,</p><p class="source-code">    dest="learning_rate",</p><p class="source-code">    help="Learning date for LightGBM",</p><p class="source-code">    default=0.01,</p><p class="source-code">)</p><p>In this block, you define the <strong class="source-inline">learning_rate</strong> argument. Note that this is a float with a default value different from the value you defined in <em class="italic">Step 7</em>, an example that shows that those two default values do not need to be the same. When executing the pipeline, <strong class="source-inline">PipelineParameter</strong> will be the one defining the actual value:</p><p class="source-code">parser.add_argument(</p><p class="source-code">    "--input-path",</p><p class="source-code">    type=str,</p><p class="source-code">    dest="input_path",</p><p class="source-code">    help="Directory containing the datasets",</p><p class="source-code">    default="../data",</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    "--output-path",</p><p class="source-code">    type=str,</p><p class="source-code">    dest="output_path",</p><p class="source-code">    help="Directory to store model",</p><p class="source-code">    default="./model",</p><p class="source-code">)</p><p class="source-code">args = parser.parse_args()</p><p>You then parse <strong class="source-inline">input_path</strong> and <strong class="source-inline">output_path</strong>, which are string values that point to local folders within the compute where this script is executing. The last line parses the<a id="_idIndexMarker855"/> incoming arguments and assigns the results to the <strong class="source-inline">args</strong> variable:</p><p class="source-code">print(f"Loading data from {args.input_path}")</p><p class="source-code">train_data = lgb.Dataset(os.path.join(args.input_path, "train_dataset.bin"))</p><p class="source-code">validation_data = lgb.Dataset(os.path.join(args.input_path, "validation_dataset.bin"))</p><p>After parsing the script arguments, the training and validation datasets are loaded:</p><p class="source-code">param = {</p><p class="source-code">    "task": "train",</p><p class="source-code">    "objective": "binary",</p><p class="source-code">    "metric": "auc",</p><p class="source-code">    "num_leaves": 5,</p><p class="source-code">    "learning_rate": args.learning_rate</p><p class="source-code">}</p><p class="source-code">model = lgb.train(</p><p class="source-code">    param,</p><p class="source-code">    train_set = train_data,</p><p class="source-code">    valid_sets = validation_data,</p><p class="source-code">    early_stopping_rounds = 5</p><p class="source-code">)</p><p>In this code block, a binary<a id="_idIndexMarker856"/> classification training process is configured that will use the <strong class="bold">Area Under Curve </strong>(<strong class="source-inline">auc</strong>) metric to evaluate the training progression. <strong class="bold">LightGBM</strong> uses an iterative training approach where the model will train until the validation score stops improving for the last <strong class="bold">5</strong> iterations, as you specified when passing in <a id="_idIndexMarker857"/>the <strong class="source-inline">early_stopping_rounds</strong> parameter:</p><p class="source-code">output_path = args.output_path</p><p class="source-code">if not os.path.exists(output_path):</p><p class="source-code">    os.makedirs(output_path)</p><p class="source-code">joblib.dump(value=model, filename=os.path.join(output_path, "model.joblib"))</p><p>Once the training is complete, the model is serialized using<a id="_idIndexMarker858"/> the <strong class="source-inline">joblib</strong> library and stored in the <strong class="source-inline">output_path</strong> folder.</p></li>
				<li>Back to the notebook, it's time you defined the actual <strong class="source-inline">Pipeline</strong> you have been building so far. In a new cell, add the following code:<p class="source-code">from azureml.pipeline.core import Pipeline</p><p class="source-code">pipeline = Pipeline(workspace=ws, steps=[step_01, step_02])</p><p>You define a new <strong class="source-inline">Pipeline</strong> object, passing in a list with all the steps you want to include. Note that the order of the steps is not important since the real execution order is defined by the <strong class="source-inline">step01_output</strong> <strong class="source-inline">PipelineData</strong> dependency you specified between those two steps.</p></li>
				<li>To execute the <a id="_idIndexMarker859"/>pipeline, you will need to submit it in an <strong class="source-inline">Experiment</strong>. In a new notebook cell, add the following code:<p class="source-code">from azureml.core import Experiment</p><p class="source-code">experiment = Experiment(ws, "chapter-11-runs")</p><p class="source-code">pipeline_run = experiment.submit(</p><p class="source-code">    pipeline,</p><p class="source-code">    pipeline_parameters= {</p><p class="source-code">        "learning_rate" : 0.5</p><p class="source-code">    }</p><p class="source-code">)</p><p class="source-code">pipeline_run.wait_for_completion()</p><p>This code defines a new <strong class="source-inline">Experiment</strong> named <strong class="source-inline">chapter-11-runs</strong> and submits the pipeline to run, passing the value of <strong class="source-inline">0.5</strong> to the <strong class="source-inline">learning_rate</strong> parameter you defined in <em class="italic">Step 7</em>.</p><p>One of the first outputs of the pipeline execution is the link to the AzureML portal. Clicking on that link will get you to the pipeline execution run, as seen in <em class="italic">Figure 11.6</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="Images/B16777_11_006.jpg" alt="Figure 11.6 – A graphical representation of the pipeline you authored in this section&#13;&#10;" width="1108" height="847"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – A graphical representation of the pipeline you authored in this section</p>
			<p>Suppose you try to <a id="_idIndexMarker860"/>rerun the pipeline by executing the code you wrote in <em class="italic">Step 13</em> for a second time. In that case, you will notice that the execution will be almost instant (just a few seconds compared to the minute-long execution you should have seen the first time). The pipeline detected that no input had changed and reused the outputs of the previously executed steps. This demonstrates what the <strong class="source-inline">allow_reuse=True</strong> in <em class="italic">Step 4</em> does, and it also proves that even though we didn't specify that parameter in <em class="italic">Step 9</em>, the default value is <strong class="source-inline">True</strong>. This means that, by default, all steps will reuse previous executions if the inputs and the code files are the same as the ones of an earlier execution. If you want to force a retrain even if the same <strong class="source-inline">learning_rate</strong> variable is passed to the pipeline, you can specify <strong class="source-inline">allow_reuse=False</strong> in <em class="italic">Step 9</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you wanted to pass the training dataset as a <strong class="source-inline">PipelineParameter</strong>, you would have to use the following code:</p>
			<p class="callout"><strong class="source-inline">from azureml.data.dataset_consumption_config import DatasetConsumptionConfig</strong></p>
			<p class="callout"><strong class="source-inline">ds_pipeline_param = PipelineParameter(name="dataset ", default_value=loans_ds)</strong></p>
			<p class="callout"><strong class="source-inline">dataset_consumption = DatasetConsumptionConfig("loans", ds_pipeline_param)</strong></p>
			<p class="callout">Using this code and passing the <strong class="source-inline">dataset_consumption</strong> object in <em class="italic">Step 4</em> instead of <strong class="source-inline">loans_ds.as_named_input('loans')</strong> would allow you to select the input dataset and its version while submitting a pipeline to execute.</p>
			<p>So far, you have <a id="_idIndexMarker861"/>defined a pipeline that executes two Python scripts. <strong class="source-inline">step_01</strong> pre-processes the training data and stores it in an intermediate data store for <strong class="source-inline">step_02</strong> to pick up. From there, the second step trains a <strong class="bold">LightGBM</strong> model and stores it in the <strong class="source-inline">/models/loans/</strong> folder of the default datastore attached to the AzureML workspace. If you have followed the steps accurately, the pipeline will have been completed successfully. In real life, though, coding issues creep in, and your pipeline may fail to complete. In the next section, you will learn how to troubleshoot potential pipeline runtime issues.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor165"/>Troubleshooting code issues</h2>
			<p>So far, your code has worked<a id="_idIndexMarker862"/> like a charm. What happens if a script has a coding issue or if a dependency is missing? In that case, your pipeline will fail. In the graphical representation you saw in <em class="italic">Figure 11.6</em>, you will be able to identify the failing step. If you want to get the details of a specific child step, you will have to first locate it using <strong class="source-inline">find_step_run</strong> of the <strong class="source-inline">pipeline_run</strong> object you got when you executed the pipeline. In a new cell within your notebook, add the following code:</p>
			<p class="source-code">train_step_run = pipeline_run.find_step_run("Train model")[0]</p>
			<p class="source-code">train_step_run.get_details_with_logs()</p>
			<p>This code finds all steps with the name <strong class="bold">Train model</strong>, and you select the first result located on the <strong class="source-inline">0</strong> index. This retrieves a <strong class="source-inline">StepRun</strong> object, which is for the <strong class="source-inline">step_02</strong> folder you defined in the previous section. <strong class="source-inline">StepRun</strong> inherits from the base <strong class="source-inline">Run</strong> class, exposing the <strong class="source-inline">get_details_with_logs</strong> method that is also available in the <strong class="source-inline">ScriptRun</strong> class you were using in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. This method is handy in troubleshooting potential issues with your dependencies or your script code. It produces a lot of helpful information regarding the execution of the script, including the log files.</p>
			<p>If you prefer the AzureML studio web experience, you can navigate to the <strong class="source-inline">Pipeline</strong> run. In the graphical representation of the pipeline, select the step you want to see the logs for. View the logs in the <strong class="bold">Outputs + logs</strong> tab, as shown in <em class="italic">Figure 11.7</em>:</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="Images/B16777_11_007.jpg" alt="Figure 11.7 – Viewing the logs of the Train model step in the web portal&#13;&#10;" width="1650" height="822"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Viewing the logs of the Train model step in the web portal</p>
			<p>So far, you have learned how to author a <strong class="source-inline">Pipeline</strong> and how to troubleshoot potential runtime errors. The <strong class="source-inline">Pipeline</strong> you created is not yet registered within your workspace, something you will do in the next section.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor166"/>Publishing a pipeline to expose it as an endpoint</h1>
			<p>So far, you have defined a<a id="_idIndexMarker863"/> pipeline using the AzureML SDK. If you had to restart the kernel of your Jupyter notebook, you would lose the reference to the pipeline you defined, and you would have to rerun all the cells to recreate the pipeline object. The AzureML SDK allows you to publish a pipeline that effectively registers it as a versioned object within the workspace. Once a pipeline is published, it can be submitted without the Python code that constructed it.</p>
			<p>In a new cell in your notebook, add the following code:</p>
			<p class="source-code">published_pipeline = pipeline.publish(</p>
			<p class="source-code">    "Loans training pipeline", </p>
			<p class="source-code">    description="A pipeline to train a LightGBM model")</p>
			<p>This code publishes the pipeline and returns a <strong class="source-inline">PublishedPipeline</strong> object, the versioned object registered within the workspace. The most interesting attribute of that object is the <strong class="source-inline">endpoint</strong>, which returns the REST endpoint URL to trigger the execution of the specific pipeline.</p>
			<p>To invoke the published pipeline, you will need an authentication header. To acquire this security header, you can use the <strong class="source-inline">InteractiveLoginAuthentication</strong> class, as seen in the following code snippet:</p>
			<p class="source-code">from azureml.core.authentication import InteractiveLoginAuthentication</p>
			<p class="source-code">auth = InteractiveLoginAuthentication()</p>
			<p class="source-code">aad_token = auth.get_authentication_header()</p>
			<p>Then you can use the <a id="_idIndexMarker864"/>Python <strong class="source-inline">requests</strong> package to make a <strong class="source-inline">POST</strong> request to the specific endpoint using the following code:</p>
			<p class="source-code">import requests</p>
			<p class="source-code">response = requests.post(published_pipeline.endpoint, </p>
			<p class="source-code">            headers=aad_token, </p>
			<p class="source-code">            json={"ExperimentName": "chapter-11-runs",</p>
			<p class="source-code">            "ParameterAssignments": {"learning_rate" : 0.02}})</p>
			<p class="source-code">print(f"Made a POST request to {published_pipeline.endpoint} and got {response.status_code}.")</p>
			<p class="source-code">print(f"The portal url for the run is {response.json()['RunUrl']}")</p>
			<p>This code only needs <a id="_idIndexMarker865"/>the URL and not the actual pipeline code. If you ever lose the endpoint URL, you can retrieve it by code through the <strong class="source-inline">list</strong> method of the <strong class="source-inline">PublishedPipeline</strong> class, which enumerates all the published pipelines registered in the workspace. The preceding script invokes the <strong class="source-inline">REST</strong> endpoint using the HTTP POST verb and passing the value <strong class="source-inline">0.02</strong> as the <strong class="source-inline">learning_rate</strong> parameter. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you are unfamiliar with the <strong class="bold">Hyper Text Transfer Protocol</strong> (<strong class="bold">HTTP</strong>) and the <strong class="source-inline">POST</strong> method, also referred to as a verb, you can learn more in the <em class="italic">Further reading</em> section.</p>
			<p>The resulting <a id="_idIndexMarker866"/>object from this HTTP request contains information about the execution of the pipeline, including <strong class="source-inline">RunUrl</strong>, which allows you to visit the AzureML studio portal to monitor the pipeline execution.</p>
			<p>When you publish the pipeline, the registered object becomes available in the AzureML studio portal. If you navigate to <strong class="bold">Endpoints</strong> | <strong class="bold">Pipeline endpoints</strong>, you will find a list of all your published pipeline endpoints, as seen in <em class="italic">Figure 11.8</em>: </p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="Images/B16777_11_008.jpg" alt="Figure 11.8 – The published pipeline endpoint&#13;&#10;" width="1361" height="886"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – The published pipeline endpoint</p>
			<p>Once you select a <a id="_idIndexMarker867"/>pipeline, you can trigger it using a graphical wizard that allows you to specify the pipeline parameters and the experiment under which the pipeline will execute.</p>
			<p>In this section, you saw how you can publish a pipeline to be able to reuse it without having the pipeline definition code. You saw how you can trigger the registered pipeline using the <strong class="source-inline">REST</strong> endpoint. In the next section, you will learn how to schedule the pipeline to schedule monthly retraining.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor167"/>Scheduling a recurring pipeline</h1>
			<p>Being able to invoke a <a id="_idIndexMarker868"/>pipeline through the published <strong class="source-inline">REST</strong> endpoint is great when you have third-party systems that need to invoke a training process after a specific event has occurred. For example, suppose you are using <strong class="bold">Azure Data Factory</strong> to<a id="_idIndexMarker869"/> copy data from your on-premises databases. You could use the <strong class="bold">Machine Learning Execute Pipeline</strong> activity and trigger a published pipeline, as shown in <em class="italic">Figure 11.9</em>:</p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="Images/B16777_11_009.jpg" alt="Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML &#13;&#10;published pipeline following a copy activity&#13;&#10;" width="866" height="226"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML published pipeline following a copy activity</p>
			<p>If you wanted to <a id="_idIndexMarker870"/>schedule the pipeline to be triggered monthly, you would need to publish the pipeline as you did in the previous section, get the published pipeline ID, create a <strong class="source-inline">ScheduleRecurrence</strong>, and then create the <strong class="source-inline">Schedule</strong>. Return to your notebook where you already have a reference to <strong class="source-inline">published_pipeline</strong>. Add a new cell with the following code:</p>
			<p class="source-code">from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">recurrence = ScheduleRecurrence(frequency="Month", </p>
			<p class="source-code">                                interval=1, </p>
			<p class="source-code">                                start_time = datetime.now())</p>
			<p class="source-code">schedule = Schedule.create(workspace=ws,</p>
			<p class="source-code">                  name="chapter-11-schedule",</p>
			<p class="source-code">                  pipeline_id=published_pipeline.id, </p>
			<p class="source-code">                  experiment_name="chapter-11-scheduled-run",</p>
			<p class="source-code">                  recurrence=recurrence,</p>
			<p class="source-code">                  wait_for_provisioning=True,</p>
			<p class="source-code">                  description="Schedule to retrain model")</p>
			<p class="source-code">print("Created schedule with id: {}".format(schedule.id))</p>
			<p>In this code, you define a <strong class="source-inline">ScheduleRecurrence</strong> with monthly frequency. By specifying the <strong class="source-inline">start_time = datetime.now()</strong>, you are preventing the immediate execution of the pipeline, which is the default behavior when creating a new <strong class="source-inline">Schedule</strong>. Once you have the recurrence you want to use, you can schedule the pipeline execution by calling the <strong class="source-inline">create</strong> method of the <strong class="source-inline">Schedule</strong> class. You are passing in the ID of the <strong class="source-inline">published_pipeline</strong> you want to trigger, and you specify the experiment <a id="_idIndexMarker871"/>name under which each execution will occur. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Scheduling the execution of the specific pipeline doesn't make any sense as no additional training will ever happen since both steps have <strong class="source-inline">allow_reuse=True</strong>. If you wanted to retrain every month, you would probably want this setting to be <strong class="source-inline">False</strong> and force the execution of both steps when the pipeline schedule was invoked. Moreover, in a scheduled pipeline, it is common that the very first step fetches new data from various sources attached to the AzureML workspace and then transforms the data and trains the model.</p>
			<p>If you want to disable a scheduled execution, you can use the <strong class="source-inline">disable</strong> method of the <strong class="source-inline">Schedule</strong> class. The following code disables all scheduled pipelines in your workspace:</p>
			<p class="source-code">from azureml.pipeline.core.schedule import Schedule</p>
			<p class="source-code">schedules = Schedule.list(ws, active_only=True) </p>
			<p class="source-code">print("Your workspace has the following schedules set up:")</p>
			<p class="source-code">for schedule in schedules:</p>
			<p class="source-code">    print(f"Disabling {schedule.id} (Published pipeline: {schedule.pipeline_id}")</p>
			<p class="source-code">    schedule.disable(wait_for_provisioning=True)</p>
			<p>This code lists all <a id="_idIndexMarker872"/>active schedules within the workspace and then disables them one by one. Make sure you don't accidentally disable a pipeline that should have been scheduled in your workspace. </p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor168"/>Summary</h1>
			<p>In this chapter, you learned how you can define AzureML pipelines using the AzureML SDK. These pipelines allow you to orchestrate various steps in a repeatable manner. You started by defining a training pipeline consisting of two steps. You then learned how to trigger the pipeline and how to troubleshoot potential code issues. Then you published the pipeline to register it within the AzureML workspace and acquire an HTTP endpoint that third-party software systems could use to trigger pipeline executions. In the last section, you learned how to schedule the recurrence of a published pipeline.</p>
			<p>In the next chapter, you will learn how to operationalize the models you have been training so far in the book. Within that context, you will use the knowledge you acquired in this chapter to author batch inference pipelines, something that you can publish and trigger with HTTP or have it scheduled, as you learned in this chapter.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor169"/>Questions</h1>
			<p>In each chapter, you will find a couple of questions to validate your understanding of the topics discussed in this chapter.</p>
			<ol>
				<li value="1">What affects the execution order of the pipeline steps?<p>a. The order in which the steps were defined when constructing the <strong class="source-inline">Pipeline</strong> object.</p><p>b. The data dependencies between the steps.</p><p>c. All steps execute in parallel, and you cannot affect the execution order.</p></li>
				<li>True or false: All steps within a pipeline need to execute within the same compute target and <strong class="source-inline">Environment</strong>.</li>
				<li>True or false: <strong class="source-inline">PythonScriptStep</strong>, by default, reuses the previous execution results if nothing has changed in the parameters or the code files.</li>
				<li>You are trying to debug a child run execution issue. Which of the following methods should you call in the <strong class="source-inline">StepRun</strong> object?<p>a. <strong class="source-inline">get_file_names</strong> </p><p>b. <strong class="source-inline">get_details_with_logs</strong></p><p>c. <strong class="source-inline">get_metrics</strong></p><p>d. <strong class="source-inline">get_details</strong></p></li>
				<li>You have just defined a pipeline in Python code. What steps do you need to make to schedule a daily execution of that pipeline?</li>
			</ol>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor170"/>Further reading</h1>
			<p>This section offers a list of helpful web resources to help you augment your knowledge of the AzureML SDK and the various code snippets used in this chapter.</p>
			<ul>
				<li>Documentation regarding the <strong class="bold">LightGBM</strong> framework used in this chapter: <a href="https://lightgbm.readthedocs.io">https://lightgbm.readthedocs.io</a> </li>
				<li>HTTP request methods: <a href="https://www.w3schools.com/tags/ref_httpmethods.asp%0D">https://www.w3schools.com/tags/ref_httpmethods.asp</a></li>
				<li>Requests Python library for making HTTP requests: <a href="https://docs.Python-requests.org">https://docs.Python-requests.org</a></li>
				<li>Executing an AzureML pipeline through <strong class="bold">Azure Data Factory</strong>: <a href="https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service">https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service</a></li>
				<li>The <strong class="bold">click</strong> Python library for script parameter parsing and the creation of <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) applications: <a href="https://click.palletsprojects.com/">https://click.palletsprojects.com/</a></li>
			</ul>
		</div>
	</div></body></html>