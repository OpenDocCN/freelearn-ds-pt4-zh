["```py\n    from azureml.core import Workspace\n    ws = Workspace.from_config()\n    loans_ds = ws.datasets['loans']\n    compute_target = ws.compute_targets['cpu-sm-cluster']\n    ```", "```py\n    from azureml.core import RunConfiguration\n    runconfig = RunConfiguration()\n    runconfig.environment = ws.environments['RunConfiguration object, and you assign the predefined Environment to its environment attribute. To help you understand how this RunConfiguration object relates to the work you have been doing in *Chapter 8*, *Experimenting with Python Code*, the ScriptRunConfig you have been using in that chapter had an optional run_config parameter where you could have passed this RunConfiguration object you defined in this cell.\n    ```", "```py\n    from azureml.pipeline.core import PipelineData\n    step01_output = PipelineData(\n        \"training_data\",\n        datastore= ws.get_default_datastore(),\n        is_directory=True) \n    ```", "```py\n    from azureml.pipeline.steps import PythonScriptStep\n    step_01 = PythonScriptStep(\n       'prepare_data.py', \n        source_directory='step01',\n        arguments = [\n            \"--dataset\", loans_ds.as_named_input('loans'), \n            \"--output-path\", step01_output],\n        name='Prepare data',\n        runconfig=runconfig,\n        compute_target=compute_target,\n        outputs=[step01_output],\n        allow_reuse=True\n    )\n    ```", "```py\n        run = Run.get_context()\n        loans_dataset = run.input_datasets[\"loans\"]\n        ```", "```py\n    import argparse\n    from azureml.core.run import Run\n    from sklearn.model_selection import train_test_split\n    import lightgbm as lgb\n    import os\n    ```", "```py\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", type=str, dest=\"dataset\")\n    parser.add_argument(\"--output-path\", type=str, \n        dest=\"output_path\",\n        help=\"Directory to store datasets\")\n    args = parser.parse_args()\n    ```", "```py\n    run = Run.get_context()\n    loans_dataset = run.input_datasets[\"loans\"]\n    ```", "```py\n    print(f\"Dataset id: {loans_dataset.id}\")\n    ```", "```py\n    loans_df = loans_dataset.to_pandas_dataframe()\n    x = loans_df[[\"income\", \"credit_cards\", \"age\"]]\n    y = loans_df[\"approved_loan\"].values\n    feature_names = x.columns.to_list()\n    x_train, x_test, y_train, y_test = train_test_split(\n        x, y, test_size=0.2, random_state=42, stratify=y\n    )\n    ```", "```py\n    train_data = lgb.Dataset(x_train, label=y_train, feature_name=feature_names)\n    test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n    ```", "```py\n    run = Run.get_context()\n    loans_dataset = None\n    if type(run) == _OfflineRun:\n        from azureml.core import Workspace, Dataset\n        ws = Workspace.from_config()\n        if args.dataset in ws.datasets:\n            loans_dataset = ws.datasets[args.dataset]\n        else:\n            loans_dataset = Dataset.get_by_id(ws, args.dataset)\n    else:\n        loans_dataset = run.input_datasets[\"loans\"]\n    ```", "```py\n    from azureml.pipeline.core import PipelineParameter\n    learning_rate_param = PipelineParameter( name=\"learning_rate\", default_value=0.05)\n    ```", "```py\n    from azureml.data import OutputFileDatasetConfig\n    datastore = ws.get_default_datastore()\n    step02_output = OutputFileDatasetConfig(\n        name = \"model_store\",\n        destination = (datastore, '/models/loans/'))\n    ```", "```py\n    step_02 = PythonScriptStep(\n       'train_model.py', \n        source_directory='step02',\n        arguments = [\n            \"--learning-rate\", learning_rate_param,\n            \"--input-path\", step01_output,\n            \"--output-path\", step02_output],\n        name='Train model',\n        runconfig=runconfig,\n        compute_target=compute_target,\n        inputs=[step01_output],\n        outputs=[step02_output]\n    )\n    ```", "```py\n    import argparse\n    import os\n    import lightgbm as lgb\n    import joblib\n    parser = argparse.ArgumentParser()\n    ```", "```py\n    parser.add_argument(\n        \"--input-path\",\n        type=str,\n        dest=\"input_path\",\n        help=\"Directory containing the datasets\",\n        default=\"../data\",\n    )\n    parser.add_argument(\n        \"--output-path\",\n        type=str,\n        dest=\"output_path\",\n        help=\"Directory to store model\",\n        default=\"./model\",\n    )\n    args = parser.parse_args()\n    ```", "```py\n    print(f\"Loading data from {args.input_path}\")\n    train_data = lgb.Dataset(os.path.join(args.input_path, \"train_dataset.bin\"))\n    validation_data = lgb.Dataset(os.path.join(args.input_path, \"validation_dataset.bin\"))\n    ```", "```py\n    param = {\n        \"task\": \"train\",\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"num_leaves\": 5,\n        \"learning_rate\": args.learning_rate\n    }\n    model = lgb.train(\n        param,\n        train_set = train_data,\n        valid_sets = validation_data,\n        early_stopping_rounds = 5\n    )\n    ```", "```py\n    output_path = args.output_path\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    joblib.dump(value=model, filename=os.path.join(output_path, \"model.joblib\"))\n    ```", "```py\n    from azureml.pipeline.core import Pipeline\n    pipeline = Pipeline(workspace=ws, steps=[step_01, step_02])\n    ```", "```py\n    from azureml.core import Experiment\n    experiment = Experiment(ws, \"chapter-11-runs\")\n    pipeline_run = experiment.submit(\n        pipeline,\n        pipeline_parameters= {\n            \"learning_rate\" : 0.5\n        }\n    )\n    pipeline_run.wait_for_completion()\n    ```", "```py\ntrain_step_run = pipeline_run.find_step_run(\"Train model\")[0]\ntrain_step_run.get_details_with_logs()\n```", "```py\npublished_pipeline = pipeline.publish(\n    \"Loans training pipeline\", \n    description=\"A pipeline to train a LightGBM model\")\n```", "```py\nfrom azureml.core.authentication import InteractiveLoginAuthentication\nauth = InteractiveLoginAuthentication()\naad_token = auth.get_authentication_header()\n```", "```py\nimport requests\nresponse = requests.post(published_pipeline.endpoint, \n            headers=aad_token, \n            json={\"ExperimentName\": \"chapter-11-runs\",\n            \"ParameterAssignments\": {\"learning_rate\" : 0.02}})\nprint(f\"Made a POST request to {published_pipeline.endpoint} and got {response.status_code}.\")\nprint(f\"The portal url for the run is {response.json()['RunUrl']}\")\n```", "```py\nfrom azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\nfrom datetime import datetime\nrecurrence = ScheduleRecurrence(frequency=\"Month\", \n                                interval=1, \n                                start_time = datetime.now())\nschedule = Schedule.create(workspace=ws,\n                  name=\"chapter-11-schedule\",\n                  pipeline_id=published_pipeline.id, \n                  experiment_name=\"chapter-11-scheduled-run\",\n                  recurrence=recurrence,\n                  wait_for_provisioning=True,\n                  description=\"Schedule to retrain model\")\nprint(\"Created schedule with id: {}\".format(schedule.id))\n```", "```py\nfrom azureml.pipeline.core.schedule import Schedule\nschedules = Schedule.list(ws, active_only=True) \nprint(\"Your workspace has the following schedules set up:\")\nfor schedule in schedules:\n    print(f\"Disabling {schedule.id} (Published pipeline: {schedule.pipeline_id}\")\n    schedule.disable(wait_for_provisioning=True)\n```"]