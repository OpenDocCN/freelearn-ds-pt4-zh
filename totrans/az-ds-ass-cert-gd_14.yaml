- en: '*Chapter 11*: Working with Pipelines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how you can author repeatable processes, defining
    pipelines that consist of multiple steps. You can use these pipelines to author
    training pipelines that transform your data and then train models, or you can
    use them to perform batch inferences using pre-trained models. Once you register
    one of those pipelines, you can invoke it using either an HTTP endpoint or through
    the SDK, or even configure them to execute on a schedule. With this knowledge,
    you will be able to implement and consume pipelines by using the **Azure Machine
    Learning** (**AzureML**) SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AzureML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authoring a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing a pipeline to expose it as an endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling a recurring pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need to have access to an Azure subscription. Within that subscription,
    you will need a `packt-azureml-rg`. You will need to have either a `Contributor`
    or `Owner` `packt-learning-mlw`, as described in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need to have a basic understanding of the **Python** language.
    The code snippets target Python version 3.6 or newer. You should also be familiar
    with working in the notebook experience within AzureML studio, something that
    was covered in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes you have registered the **loans** dataset you generated
    in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147), *Understanding
    Model Results*. It is also assumed that you have created a compute cluster named
    **cpu-sm-cluster**, as described in the *Working with compute targets* section
    in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML
    Python SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the notebooks and code snippets for this chapter in GitHub
    at the following URL: [http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AzureML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B16777_06_Final_VK_ePub.xhtml#_idTextAnchor084), *Visual Model
    Training and Publishing*, you saw how you can design a training process using
    building boxes. Similar to those workflows, the AzureML SDK allows you to author
    `Pipelines` that orchestrate multiple steps. For example, in this chapter, you
    will author a `Pipeline` that consists of two steps. The first step pre-processes
    the `Environment`.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse the `Pipelines` with the `Pipelines` you read in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147),
    *Understanding Model Results*. The `Pipelines` as a wrapper around the actual
    model class that you want to train and use for inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AzureML SDK offers quite a few building blocks that you can use to construct
    a `Pipeline`. *Figure 11.1* contains the most popular classes that you may encounter
    in the exam and real-life code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Classes available in the AzureML SDK to author your pipelines'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Classes available in the AzureML SDK to author your pipelines
  prefs: []
  type: TYPE_NORMAL
- en: The **Pipeline** is the core class that defines a workflow that stitches together
    multiple steps. You can pass in parameters to a pipeline by defining them using
    the **PipelineParameter** class. These parameters can be references in one or
    more steps within the **Pipeline**. Once you have finished defining a pipeline,
    you can publish it to register it in the AzureML workspace as a versioned object
    that can be referenced using the **PublishedPipeline** class. This published pipeline
    has an endpoint that you can use to trigger its execution. If you want, you can
    define a **Schedule** and have this **PublishedPipeline** class triggered at a
    specific time interval. **PipelineData** defines temporary storage where one step
    can drop some files for the next one to pick them up. The data dependency between
    those two steps creates an implicit execution order in the **Pipeline**, meaning
    that the dependent step will wait for the first step to complete. You will be
    using all these classes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **azureml.pipeline.steps** module, you will find all the available steps
    you can use. The most commonly used steps are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PythonScriptStep**: This step allows you to execute a Python script. You
    will be using this step in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AutoMLConfig` object you saw in [*Chapter 9*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136),
    *Optimizing the ML Model*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HyperDriveConfig` parameter you saw in [*Chapter 9*](B16777_09_Final_VK_ePub.xhtml#_idTextAnchor136),
    *Optimizing the ML Model*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataTransferStep**: A **Pipeline** step that allows you to transfer data
    between AzureML-supported storage options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DatabricksStep**: This allows you to execute a DataBricks notebook, Python
    script, or JAR file in an attached DataBricks cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Estimator` class that represented a generic training script. Some framework-specific
    estimators inherited from that generic `Estimator` class, such as `TensorFlow`
    and `PyTorch`. To incorporate one of those estimators in your pipelines, you would
    have used an `EstimatorStep`. The whole `Estimator` class and its derivatives
    have been deprecated in favor of `ScriptRunConfig`, which you have used in the
    previous chapters. If, during the exam, you see a deprecated reference to an `EstimatorStep`,
    you can treat it as a `PythonScriptStep`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last major piece of a **Pipeline** is the data that flows through it.
  prefs: []
  type: TYPE_NORMAL
- en: '`(dstore,"/samples/diabetes")` tuple to indicate where you wanted to store
    the data when you called the `register_pandas_dataframe` method of a `TabularDataset`.
    Instead of that tuple, you could have passed the equivalent `DataPath(datastore=dstore,
    path_on_datastore="/samples/diabetes")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outputs` folder was automatically uploaded to a `Run` execution. Similar to
    that folder, you can define additional local folders that will be automatically
    uploaded to a target path in a target datastore. In this chapter, you will be
    using this class to store the produced model in a specific location within the
    default blob storage account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataReference** represents a path in a datastore and can be used to describe
    how and where data should be made available in a run. It is no longer the recommended
    approach for data referencing in AzureML. If you encounter it in an obsolete exam
    question, you can treat it as a **DataPath** object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you learned about the building blocks you can use to construct
    an AzureML **Pipeline**. In the next section, you will get some hands-on experience
    of using those classes.
  prefs: []
  type: TYPE_NORMAL
- en: Authoring a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that you need to create a repeatable workflow that has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It loads the data from a registered dataset and splits it into training and
    test datasets. These datasets are converted into a special construct needed by
    the `step01`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It loads the pre-processed data and trains a `/models/loans/` folder of the
    default datastore attached to the AzureML workspace. You will be writing the code
    for this step within a folder named `step02`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each step will be a separate Python file, taking some arguments to specify
    where to read the data from and where to write the data to. These scripts will
    utilize the same mechanics as the scripts you authored in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*. What is different in this chapter is that instead
    of invoking each Python script separately, you will create a `Pipeline` that will
    invoke those steps one after the other. In *Figure 11.2*, you can see the overall
    inputs and outputs each script is going to have, along with the parameters you
    will need to configure for each step to execute:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Inputs and outputs of each pipeline step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – Inputs and outputs of each pipeline step
  prefs: []
  type: TYPE_NORMAL
- en: Based on *Figure 11.2*, for each step, you will need to define the compute target
    and the `Environment` that will be used to execute the specific Python script.
    Although each step can have a separate compute target and a separate `Environment`
    specified, you will be running both steps using the same `Environment`, and the
    same compute target to simplify the code. You will be using the out-of-the-box
    `Environment`, which contains standard data science packages, including the **LightGBM**
    library that your scripts will require. You will be executing the steps in the
    **cpu-sm-cluster** cluster you created in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will start by authoring the `Pipeline`, and then you will author the actual
    Python scripts required for each step. Navigate to the `chapter11` and then create
    a notebook named `chapter11.ipynb`, as seen in *Figure 11.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Adding the chapter11 notebook to your working files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – Adding the chapter11 notebook to your working files
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the newly created notebook and follow the steps to author an AzureML pipeline
    using the AzureML SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will start by getting a reference to your workspace. Then you will get
    references to the `loans` dataset and the `cpu-sm-cluster`. Add the following
    code to a cell in your notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you are having difficulties understanding this code snippet, please review
    [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML Python
    SDK*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will need to create a configuration object that will dictate the use of
    the `Environment` when each step gets executed. To do that, you will need to create
    a `RunConfiguration` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will then need to define a temporary storage folder where the first step
    will drop the output files. You will use the `PipelineData` class using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, you are creating an intermediate data location named `training_data`,
    which is stored as a folder in the default datastore that is registered in your
    AzureML workspace. You should not care about the actual path of this temporary
    data, but if you are curious, the actual path of that folder in the default storage
    container is something like `azureml/{step01_run_id}/training_data`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have all the prerequisites for your pipeline''s first step, it
    is time to define it. In a new cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code defines a `PythonScriptStep` that will be using the source code in
    the `step01` folder. It will execute the script named `prepare_data.py`, passing
    the following arguments:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`--dataset`: This passes the `loans_ds` dataset ID to that variable. This dataset
    ID is a unique `as_named_input` method. This method is available in both `FileDataset`
    and `TabularDataset` and is only applicable when a `Run` executes within the AzureML
    workspace. To invoke the method, you must provide a name, in this case, `loans`,
    that can be used within the script to retrieve the dataset. The AzureML SDK will
    make the `TabularDataset` object available within the `prepare_data.py` script
    in the `input_datasets` dictionary of the `run` object. Within the `prepare_data.py`
    script, you can get a reference to that dataset using the following code:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`--output-path`: This passes the `PipelineData` object you created in *Step
    3*. This parameter will be a string representing a path where the script can store
    its output files. The datastore location is mounted to the local storage of the
    compute node that is about to execute the specific step. This mounting path is
    passed to the script, allowing your script to transparently write the outputs
    directly to the datastore.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Coming back to the arguments you pass to the `PythonScriptStep` initialization,
    you define a name that will be visible in the visual representation of the pipeline
    seen in *Figure 11.6*. In the `runconfig` parameter, you pass the `RunConfiguration`
    object that you defined in *Step 2*. In the `compute_target` parameter, you pass
    the reference to the `cpu-sm-cluster` cluster that you got in *Step 1*.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `outputs` parameter, you pass an array of outputs to which this step
    will be posting data. This is a very important parameter to define the right execution
    order of the steps within the pipeline. Although you are passing the `PipelineData`
    object as an argument to the script, the AzureML SDK is not aware of whether your
    script will be writing or reading data from that location. By explicitly adding
    the `PipelineData` object to the `outputs` parameter, you mark this step as a
    producer of the data stored in the `PipelineData` object. Thus, anyone referencing
    the same object in the corresponding `inputs` parameter will need to execute after
    this `PythonScriptStep`.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: The `allow_reuse` Boolean parameter allows you to reuse the outputs of this
    `PythonScriptStep` if the inputs of the script and the source code within the
    `step01` folder haven't changed since the last execution of the pipeline. Since
    the only input of this step is a specific version of the `TabularDataset`, it
    cannot change. Although you did not specify a particular version when you referenced
    the `TabularDataset`, the latest version was automatically selected. This version
    was pinned to the pipeline's definition at creation time. The pipeline will keep
    executing on the pinned version, even if you create a new version of the `TabularDataset`.
    Moreover, since the `allow_reuse` parameter is set to `True`, this step will run
    only once, and from there on, the results will be automatically reused. At the
    end of this section, you will see how this affects the pipeline execution time
    when you rerun the same pipeline.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wanted to force the pipeline to read the latest version of the `loans_ds`
    variable would reference the latest version of the `TabularDataset`. At the end
    of this section, you will also learn how you can pass the training dataset as
    a `PipelineParameter`.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have defined the `PythonScriptStep`, it is time to add the missing
    Python script to your files. Next to your notebook, under the `chapter11` folder
    you are currently working on, add a new folder named `step01`. Within that folder,
    add a new Python script file named `prepare_data.py`. The final folder structure
    should be similar to the one shown in *Figure 11.4*:![Figure 11.4 – Folder structure
    for the prepare_data.py script that will be executed in your pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16777_11_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.4 – Folder structure for the prepare_data.py script that will be executed
    in your pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the following code blocks within the `prepare_data.py` file. Instead of
    typing all this code, you can download it directly from the GitHub repository
    mentioned in the *Technical requirements* section of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These are all the imports you will need within the script file. You are going
    to create the training and test datasets using the `train_test_split` method for
    the `lightgbm` library with the `lgb` short alias:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This script creates an `ArgumentParser` that parses the arguments you passed
    when you defined the `PythonScriptStep` in *Step 4*. As a reminder, the `--dataset`
    parameter is going to contain the dataset ID that the script will need to process,
    and the `--output-path` parameter will be the local path location where the script
    is supposed to write the transformed datasets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Right after parsing the arguments, you are getting a reference to the `Run`
    context. From there, you get a reference to the `loans` dataset, something that
    becomes available to you because you called the `as_named_input` method as discussed
    in *Step 4*. Later in this section, you will read about how you could have rewritten
    this code block to be able to run the same script in your local computer without
    a `Run` context:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code block prints the ID of the dataset that your code picked as a reference.
    If you print the ID passed to the `--dataset` parameter, and which is stored in
    the `args.dataset` variable, you will notice that these two values are identical:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this code block, you load the dataset into memory and use the `train_test_split`
    method to split the dataset into training and test features (`x_train` and `x_test`)
    and training and test labels (`y_train` and `y_test`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The features and the labels are then converted into `train_data` and `test_data`,
    which are `Dataset` objects. `Dataset` format for training and validation. Note
    that the validation dataset stored in the `test_data` variable needs to reference
    the training data (`train_data`). This is a failsafe mechanism embedded by `output_path`
    folder, if it doesn't already exist, and then use the native `save_binary` method
    of the `Dataset` to serialize the dataset into a binary file that is optimized
    for storing and loading.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In contrast to the scripts you created in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, the `prepare_data.py` file cannot execute on
    your local computer as an `_OfflineRun`. This is because you have a dependency
    on the `input_datasets` dictionary that is only available if the `Run` is executing
    within the AzureML workspace. If you wanted to test this file locally before using
    it within the `Pipeline`, you could use the following code instead:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code checks whether this is an offline run. In that case, it first gets
    a reference to the workspace as you saw in [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*, and then it checks whether the `--dataset` parameter
    stored in the `args.dataset` variable is a dataset name. If it is, the latest
    version of the dataset is assigned to the `loans_dataset` variable. If it is not
    a name, the script assumes it is a GUID, which should represent the ID of a specific
    dataset version. In that case, the script tries the `get_by_id` method to retrieve
    the specific dataset or throw an error if the value passed is not a known dataset
    ID. If the run is online, you can still use the `input_datasets` dictionary to
    retrieve the dataset reference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Back to your notebook, you will start defining the prerequisites for the second
    step, the model training phase of your `Pipeline`. In *Figure 11.2*, you saw that
    this step requires a parameter named `learning_rate`. Instead of hardcoding the
    learning rate hyperparameter of the `PipelineParameter` to pass in this value.
    This parameter will be defined at `Pipeline` level, and it will be passed to the
    training script as an argument, as you will see in *Step 9*. To create such a
    parameter, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code defines a new `PipelineParameter` named `learning_rate`. The default
    value will be `0.05`, meaning that you can omit to pass this parameter when you
    execute the pipeline, and this default value will be used. You will see later
    in *Step 13* how you can execute the `Pipeline` and specify a value other than
    the default.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will store the trained model in the `/models/loans/` folder of the default
    datastore attached to the AzureML workspace. To specify the exact location where
    you want to store the files, you will use the `OutputFileDatasetConfig` class.
    In a new notebook cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this script, you are getting a reference to the default datastore. Then,
    you create an `OutputFileDatasetConfig` object, passing a tuple to the `destination`
    parameter. This tuple consists of the datastore you selected and the path within
    that datastore. You could have selected any datastore you have attached in the
    AzureML workspace. This `OutputFileDatasetConfig` object defines the destination
    to copy the outputs to. If you don't specify the `destination` argument, the default
    `/dataset/{run-id}/{output-name}` value is used. Note that `destination` allows
    you to use placeholders while defining the path. The default value uses both the
    `{run-id}` and `{output-name}` placeholders that are currently supported. These
    placeholders will be replaced with the corresponding values at the appropriate
    time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have all the prerequisites defined, you can define the second
    step of your `Pipeline`. In a new cell in your notebook, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Similar to the `step_01` folder you created in *Step 4*; this code defines a
    `PythonScriptStep` that will invoke the `train_model.py` script located in the
    `step02` folder. It will populate the `--learning-rate` argument using the value
    passed to the `PipelineParameter` you defined in *Step 7*. It will also pass the
    output of `step_01` to the `--input-path` argument. Note that `step01_output`
    is also added to the list of inputs of this `PythonScriptStep`. This forces `step_02`
    to wait for `step_01` to complete in order to consume the data stored in `step01_output`.
    The last script argument is `--output-path`, where you pass the `OutputFileDatasetConfig`
    object you created in the previous step. This object is also added to the list
    of outputs of this `PythonScriptStep`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's create the Python script that will be executed by `step_02`. Next to your
    notebook, under the `chapter11` folder you are currently working on, add a new
    folder named `step02`. Within that folder, add a new Python script file named
    `train_model.py`. The final folder structure should be similar to the one shown
    in *Figure 11.5*:![Figure 11.5 – The training script that will be executed in
    the step02 folder of your pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16777_11_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.5 – The training script that will be executed in the step02 folder
    of your pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the `train_model.py` file and add the following code blocks to it. Instead
    of typing all this code, you can download it directly from the GitHub repository
    mentioned in the *Technical requirements* section of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This block imports all modules you will need in the file and creates an `ArgumentParser`
    to read the arguments you will be passing to this script. If you so wished, you
    could have used another famous library for script parameter parsing called `learning_rate`
    argument. Note that this is a float with a default value different from the value
    you defined in *Step 7*, an example that shows that those two default values do
    not need to be the same. When executing the pipeline, `PipelineParameter` will
    be the one defining the actual value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You then parse `input_path` and `output_path`, which are string values that
    point to local folders within the compute where this script is executing. The
    last line parses the incoming arguments and assigns the results to the `args`
    variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After parsing the script arguments, the training and validation datasets are
    loaded:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this code block, a binary classification training process is configured
    that will use the `auc`) metric to evaluate the training progression. `early_stopping_rounds`
    parameter:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the training is complete, the model is serialized using the `joblib` library
    and stored in the `output_path` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Back to the notebook, it''s time you defined the actual `Pipeline` you have
    been building so far. In a new cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You define a new `Pipeline` object, passing in a list with all the steps you
    want to include. Note that the order of the steps is not important since the real
    execution order is defined by the `step01_output` `PipelineData` dependency you
    specified between those two steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To execute the pipeline, you will need to submit it in an `Experiment`. In
    a new notebook cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code defines a new `Experiment` named `chapter-11-runs` and submits the
    pipeline to run, passing the value of `0.5` to the `learning_rate` parameter you
    defined in *Step 7*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'One of the first outputs of the pipeline execution is the link to the AzureML
    portal. Clicking on that link will get you to the pipeline execution run, as seen
    in *Figure 11.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – A graphical representation of the pipeline you authored in
    this section'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – A graphical representation of the pipeline you authored in this
    section
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you try to rerun the pipeline by executing the code you wrote in *Step
    13* for a second time. In that case, you will notice that the execution will be
    almost instant (just a few seconds compared to the minute-long execution you should
    have seen the first time). The pipeline detected that no input had changed and
    reused the outputs of the previously executed steps. This demonstrates what the
    `allow_reuse=True` in *Step 4* does, and it also proves that even though we didn't
    specify that parameter in *Step 9*, the default value is `True`. This means that,
    by default, all steps will reuse previous executions if the inputs and the code
    files are the same as the ones of an earlier execution. If you want to force a
    retrain even if the same `learning_rate` variable is passed to the pipeline, you
    can specify `allow_reuse=False` in *Step 9*.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to pass the training dataset as a `PipelineParameter`, you would
    have to use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from azureml.data.dataset_consumption_config import DatasetConsumptionConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ds_pipeline_param = PipelineParameter(name="dataset ", default_value=loans_ds)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset_consumption = DatasetConsumptionConfig("loans", ds_pipeline_param)`'
  prefs: []
  type: TYPE_NORMAL
- en: Using this code and passing the `dataset_consumption` object in *Step 4* instead
    of `loans_ds.as_named_input('loans')` would allow you to select the input dataset
    and its version while submitting a pipeline to execute.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have defined a pipeline that executes two Python scripts. `step_01`
    pre-processes the training data and stores it in an intermediate data store for
    `step_02` to pick up. From there, the second step trains a `/models/loans/` folder
    of the default datastore attached to the AzureML workspace. If you have followed
    the steps accurately, the pipeline will have been completed successfully. In real
    life, though, coding issues creep in, and your pipeline may fail to complete.
    In the next section, you will learn how to troubleshoot potential pipeline runtime
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting code issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, your code has worked like a charm. What happens if a script has a coding
    issue or if a dependency is missing? In that case, your pipeline will fail. In
    the graphical representation you saw in *Figure 11.6*, you will be able to identify
    the failing step. If you want to get the details of a specific child step, you
    will have to first locate it using `find_step_run` of the `pipeline_run` object
    you got when you executed the pipeline. In a new cell within your notebook, add
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This code finds all steps with the name `0` index. This retrieves a `StepRun`
    object, which is for the `step_02` folder you defined in the previous section.
    `StepRun` inherits from the base `Run` class, exposing the `get_details_with_logs`
    method that is also available in the `ScriptRun` class you were using in [*Chapter
    8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117), *Experimenting with Python
    Code*. This method is handy in troubleshooting potential issues with your dependencies
    or your script code. It produces a lot of helpful information regarding the execution
    of the script, including the log files.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you prefer the AzureML studio web experience, you can navigate to the `Pipeline`
    run. In the graphical representation of the pipeline, select the step you want
    to see the logs for. View the logs in the **Outputs + logs** tab, as shown in
    *Figure 11.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Viewing the logs of the Train model step in the web portal'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.7 – Viewing the logs of the Train model step in the web portal
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to author a `Pipeline` and how to troubleshoot
    potential runtime errors. The `Pipeline` you created is not yet registered within
    your workspace, something you will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing a pipeline to expose it as an endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you have defined a pipeline using the AzureML SDK. If you had to restart
    the kernel of your Jupyter notebook, you would lose the reference to the pipeline
    you defined, and you would have to rerun all the cells to recreate the pipeline
    object. The AzureML SDK allows you to publish a pipeline that effectively registers
    it as a versioned object within the workspace. Once a pipeline is published, it
    can be submitted without the Python code that constructed it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell in your notebook, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This code publishes the pipeline and returns a `PublishedPipeline` object, the
    versioned object registered within the workspace. The most interesting attribute
    of that object is the `endpoint`, which returns the REST endpoint URL to trigger
    the execution of the specific pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To invoke the published pipeline, you will need an authentication header. To
    acquire this security header, you can use the `InteractiveLoginAuthentication`
    class, as seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can use the Python `requests` package to make a `POST` request to
    the specific endpoint using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This code only needs the URL and not the actual pipeline code. If you ever lose
    the endpoint URL, you can retrieve it by code through the `list` method of the
    `PublishedPipeline` class, which enumerates all the published pipelines registered
    in the workspace. The preceding script invokes the `REST` endpoint using the HTTP
    POST verb and passing the value `0.02` as the `learning_rate` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you are unfamiliar with the `POST` method, also referred to as a verb, you
    can learn more in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting object from this HTTP request contains information about the execution
    of the pipeline, including `RunUrl`, which allows you to visit the AzureML studio
    portal to monitor the pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you publish the pipeline, the registered object becomes available in the
    AzureML studio portal. If you navigate to **Endpoints** | **Pipeline endpoints**,
    you will find a list of all your published pipeline endpoints, as seen in *Figure
    11.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – The published pipeline endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – The published pipeline endpoint
  prefs: []
  type: TYPE_NORMAL
- en: Once you select a pipeline, you can trigger it using a graphical wizard that
    allows you to specify the pipeline parameters and the experiment under which the
    pipeline will execute.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you saw how you can publish a pipeline to be able to reuse
    it without having the pipeline definition code. You saw how you can trigger the
    registered pipeline using the `REST` endpoint. In the next section, you will learn
    how to schedule the pipeline to schedule monthly retraining.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling a recurring pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Being able to invoke a pipeline through the published `REST` endpoint is great
    when you have third-party systems that need to invoke a training process after
    a specific event has occurred. For example, suppose you are using **Azure Data
    Factory** to copy data from your on-premises databases. You could use the **Machine
    Learning Execute Pipeline** activity and trigger a published pipeline, as shown
    in *Figure 11.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML'
  prefs: []
  type: TYPE_NORMAL
- en: published pipeline following a copy activity
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_11_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 – Sample Azure Data Factory pipeline triggering an AzureML published
    pipeline following a copy activity
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to schedule the pipeline to be triggered monthly, you would need
    to publish the pipeline as you did in the previous section, get the published
    pipeline ID, create a `ScheduleRecurrence`, and then create the `Schedule`. Return
    to your notebook where you already have a reference to `published_pipeline`. Add
    a new cell with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this code, you define a `ScheduleRecurrence` with monthly frequency. By specifying
    the `start_time = datetime.now()`, you are preventing the immediate execution
    of the pipeline, which is the default behavior when creating a new `Schedule`.
    Once you have the recurrence you want to use, you can schedule the pipeline execution
    by calling the `create` method of the `Schedule` class. You are passing in the
    ID of the `published_pipeline` you want to trigger, and you specify the experiment
    name under which each execution will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling the execution of the specific pipeline doesn't make any sense as
    no additional training will ever happen since both steps have `allow_reuse=True`.
    If you wanted to retrain every month, you would probably want this setting to
    be `False` and force the execution of both steps when the pipeline schedule was
    invoked. Moreover, in a scheduled pipeline, it is common that the very first step
    fetches new data from various sources attached to the AzureML workspace and then
    transforms the data and trains the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to disable a scheduled execution, you can use the `disable` method
    of the `Schedule` class. The following code disables all scheduled pipelines in
    your workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This code lists all active schedules within the workspace and then disables
    them one by one. Make sure you don't accidentally disable a pipeline that should
    have been scheduled in your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how you can define AzureML pipelines using the
    AzureML SDK. These pipelines allow you to orchestrate various steps in a repeatable
    manner. You started by defining a training pipeline consisting of two steps. You
    then learned how to trigger the pipeline and how to troubleshoot potential code
    issues. Then you published the pipeline to register it within the AzureML workspace
    and acquire an HTTP endpoint that third-party software systems could use to trigger
    pipeline executions. In the last section, you learned how to schedule the recurrence
    of a published pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to operationalize the models you have
    been training so far in the book. Within that context, you will use the knowledge
    you acquired in this chapter to author batch inference pipelines, something that
    you can publish and trigger with HTTP or have it scheduled, as you learned in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In each chapter, you will find a couple of questions to validate your understanding
    of the topics discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What affects the execution order of the pipeline steps?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. The order in which the steps were defined when constructing the `Pipeline`
    object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. The data dependencies between the steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. All steps execute in parallel, and you cannot affect the execution order.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'True or false: All steps within a pipeline need to execute within the same
    compute target and `Environment`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: `PythonScriptStep`, by default, reuses the previous execution
    results if nothing has changed in the parameters or the code files.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are trying to debug a child run execution issue. Which of the following
    methods should you call in the `StepRun` object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `get_file_names`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. `get_details_with_logs`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. `get_metrics`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. `get_details`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have just defined a pipeline in Python code. What steps do you need to make
    to schedule a daily execution of that pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section offers a list of helpful web resources to help you augment your
    knowledge of the AzureML SDK and the various code snippets used in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation regarding the **LightGBM** framework used in this chapter: [https://lightgbm.readthedocs.io](https://lightgbm.readthedocs.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HTTP request methods: [https://www.w3schools.com/tags/ref_httpmethods.asp](https://www.w3schools.com/tags/ref_httpmethods.asp%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Requests Python library for making HTTP requests: [https://docs.Python-requests.org](https://docs.Python-requests.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Executing an AzureML pipeline through **Azure Data Factory**: [https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **click** Python library for script parameter parsing and the creation
    of **Command-Line Interface** (**CLI**) applications: [https://click.palletsprojects.com/](https://click.palletsprojects.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
