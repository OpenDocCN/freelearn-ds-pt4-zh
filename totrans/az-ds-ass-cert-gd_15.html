<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer288">
			<h1 id="_idParaDest-160"><em class="italic"><a id="_idTextAnchor171"/>Chapter 12</em>: Operationalizing Models with Code</h1>
			<p>In this chapter, you are going to learn how to operationalize the machine learning models you have been training, in this book, so far. You will explore two approaches: exposing a real-time endpoint by hosting a REST API that you can use to make inferences and expanding your pipeline authoring knowledge to make inferences on top of big data, in parallel, efficiently. You will begin by registering a model in the workspace to keep track of the artifact. Then, you will publish a REST API; this is something that will allow your model to integrate with third-party applications such as <strong class="bold">Power BI</strong>. Following this, you will author a pipeline to process half a million records within a couple of minutes in a very cost-effective manner.</p>
			<p>In this chapter, we are going to cover the following topics: </p>
			<ul>
				<li>Understanding the various deployment options</li>
				<li>Registering models in the workspace</li>
				<li>Deploying real-time endpoints</li>
				<li>Creating a batch inference pipeline</li>
			</ul>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor172"/>Technical requirements</h1>
			<p>You will require access to an Azure subscription. Within that subscription, you will need a <strong class="bold">resource group</strong> named <strong class="source-inline">packt-azureml-rg</strong>. You will need to have either a <strong class="source-inline">Contributor</strong> or <strong class="source-inline">Owner</strong> <strong class="bold">Access control</strong> (<strong class="bold">IAM</strong>) role on the resource group level. Within that resource group, you should have already deployed a machine learning resource, named <strong class="source-inline">packt-learning-mlw</strong>. These resources should be already available to you if you followed the instructions in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>.</p>
			<p>Additionally, you will require a basic understanding of the <strong class="bold">Python</strong> language. The code snippets in this chapter target Python version 3.6 or later. You should also be familiar with working with notebooks within AzureML studio; this is something that was covered in <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p>
			<p>This chapter assumes you have registered the <strong class="bold">loans</strong> dataset that you generated in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. It also assumes that you have created a compute cluster, named <strong class="bold">cpu-sm-cluster</strong>, as described in the <em class="italic">Working with compute targets</em> section of <a href="B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102"><em class="italic">Chapter 7</em></a>, <em class="italic">The AzureML Python SDK</em>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">AzureML is constantly being updated. If you face any issues with the code samples in this book, try upgrading your AzureML SDK by adding the following code into a new notebook cell:</p>
			<p class="callout"><strong class="source-inline">!pip install --upgrade azureml-core azureml-sdk[notebooks]</strong></p>
			<p class="callout">Then, restart the Jupyter kernel, as you learned in the <em class="italic">Training a loans approval model</em> section of <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>. Additionally, try downloading the latest version of the notebooks from the GitHub page of this book. If the problem persists, feel free to open an issue on this book's GitHub page.</p>
			<p>You can find all of the notebooks and code snippets for this chapter on GitHub at <a href="http://bit.ly/dp100-ch12">http://bit.ly/dp100-ch12</a>.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor173"/>Understanding the various deployment options</h1>
			<p>We have been working with Python code since <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. So far, you have trained<a id="_idIndexMarker873"/> various models, evaluated<a id="_idIndexMarker874"/> them based on metrics, and saved the trained model using the <strong class="source-inline">dump</strong> method of the <strong class="bold">joblib</strong> library. The AzureML workspace allows you to store and version those artifacts by registering them in the model registry that we discussed in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. Registering the model allows you to version both the saved model and the metadata regarding the specific model, such as its performance according to various metrics. You will learn how to register models from the SDK in the <em class="italic">Registering models in the workspace</em> section.</p>
			<p>Once the model has been registered, you have to decide how you want to operationalize the model, either by deploying a real-time endpoint or by creating a batch process, as displayed in <em class="italic">Figure 12.1</em>:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="Images/B16777_12_001.jpg" alt="Figure 12.1 – A path from training to operationalization&#13;&#10;" width="1225" height="589"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – A path from training to operationalization</p>
			<p>There are two main categories in terms of how a model processes incoming data:</p>
			<ul>
				<li><strong class="bold">Real-time inferences</strong>: Let's suppose that you trained an anti-fraud model that a web banking<a id="_idIndexMarker875"/> application will use. This model<a id="_idIndexMarker876"/> will score each transaction and block those that are at high risk. In this scenario, the model needs to be up and running all of the time. Usually, the web banking application will make an HTTP request to your model's endpoint and send the transaction data for the model to evaluate. Then, the model will respond with the probability of that record being fraudulent, perhaps by invoking the <strong class="source-inline">predict_proba</strong> method of the classifier you trained. You will read more about this scenario in the <em class="italic">Deploying real-time endpoints</em> section of this chapter.</li>
				<li><strong class="bold">Batch inferences</strong>: In <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>, you trained an AutoML model<a id="_idIndexMarker877"/> to churn customer predictions. The model was trained using features such as the<a id="_idIndexMarker878"/> consumer's activity over the last 6 to 12 months. Let's suppose that you wanted to evaluate all of your customers and drive a marketing campaign for those who are likely to churn. You would have to run a once-off process that reads all of your customer information, calculates the required features, and then invokes the model for each of them to produce a prediction. The result<a id="_idIndexMarker879"/> can be stored in a CSV file to be consumed by the marketing department. In this approach, you only need the model for a short period of time, that is, only while making the predictions. You do not require a real-time endpoint, such as the one you deployed in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>, as you do not need the model to make ad hoc inferences. You can read more about this scenario in the <em class="italic">Creating a batch inference pipeline</em> section of this chapter.</li>
			</ul>
			<p>All models can be used in either real-time or batch inferences. It is up to you to decide whether you require ad hoc model inferences or a scheduled process that produces and stores the inference results. Operationalizing models in batch mode tends to be more cost-effective, as you can utilize low-priority compute clusters to perform inferences. In that scenario, you do not need to pay to have a real-time endpoint infrastructure waiting to make live inferences.</p>
			<p>In the next section, you will start the path to operationalization by training and registering the model that you will be using throughout the rest of the chapter.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor174"/>Registering models in the workspace</h1>
			<p>Registering a model<a id="_idIndexMarker880"/> allows you to keep different versions of the trained<a id="_idIndexMarker881"/> models. Each model version has artifacts and metadata. Among the metadata, you can keep references to experiment with runs and datasets. This allows you to track the lineage between the data used to train a model, the run ID that trained the model, and the actual model artifacts themselves, as displayed in <em class="italic">Figure 12.2</em>: </p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="Images/B16777_12_002.jpg" alt="Figure 12.2 – Building the lineage from the training dataset all the way to the registered model&#13;&#10;" width="963" height="269"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Building the lineage from the training dataset all the way to the registered model</p>
			<p>In this section, you<a id="_idIndexMarker882"/> will train a model and<a id="_idIndexMarker883"/> register it in your<a id="_idIndexMarker884"/> AzureML workspace. Perform<a id="_idIndexMarker885"/> the following steps:</p>
			<ol>
				<li>Navigate to the <strong class="bold">Notebooks</strong> section of your AzureML studio web interface. </li>
				<li>Create a folder, named <strong class="source-inline">chapter12</strong>, and then create a notebook named <strong class="source-inline">chapter12.ipynb</strong>, as shown in <em class="italic">Figure 12.3</em>: <div id="_idContainer274" class="IMG---Figure"><img src="Images/B16777_12_003.jpg" alt="Figure 12.3 – Adding the chapter12 notebook to your working files&#13;&#10;" width="1022" height="849"/></div><p class="figure-caption">Figure 12.3 – Adding the chapter12 notebook to your working files</p></li>
				<li>Add and execute<a id="_idIndexMarker886"/> the following code<a id="_idIndexMarker887"/> snippets in separate notebook<a id="_idIndexMarker888"/> cells. You will start by getting a reference<a id="_idIndexMarker889"/> to the workspace resources:<p class="source-code">from azureml.core import Workspace, Experiment</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">loans_ds = ws.datasets['loans']</p><p class="source-code">experiment = Experiment(ws, "chapter-12-train")</p><p>In the preceding code, you<a id="_idIndexMarker890"/> get a reference to a workspace, that is, the <strong class="bold">loans</strong> dataset that<a id="_idIndexMarker891"/> you generated in <a href="B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147"><em class="italic">Chapter 10</em></a>, <em class="italic">Understanding Model Results</em>, along<a id="_idIndexMarker892"/> with a reference<a id="_idIndexMarker893"/> to an experiment named <strong class="source-inline">chapter-12-train</strong>.</p></li>
				<li>Split the dataset into training and validation using the following code:<p class="source-code">training_data, validation_data = loans_ds.random_split(</p><p class="source-code">                       percentage = 0.8, seed=42)</p><p class="source-code">X_train = training_data.drop_columns('approved_loan') \</p><p class="source-code">            .to_pandas_dataframe()</p><p class="source-code">y_train = training_data.keep_columns('approved_loan') \</p><p class="source-code">            .to_pandas_dataframe().values.ravel()</p><p class="source-code">X_validate = validation_data.drop_columns('approved_loan') \</p><p class="source-code">                .to_pandas_dataframe()</p><p class="source-code">y_validate = validation_data.keep_columns('approved_loan') \</p><p class="source-code">                .to_pandas_dataframe().values.ravel()</p><p>The code splits the dataset into 80% training data and 20% validation data. The <strong class="source-inline">seed</strong> argument initializes the internal random state of the <strong class="source-inline">random_split</strong> method, allowing you to hardcode the data split and generate the same <strong class="source-inline">training_data</strong> and <strong class="source-inline">validation_data</strong> every time you invoke this code. </p><p>Here, <strong class="source-inline">X_train</strong> is a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> that contains the <strong class="source-inline">income</strong>, <strong class="source-inline">credit_cards</strong>, and <strong class="source-inline">age</strong> features (that is, all of the columns besides <strong class="source-inline">approved_loan</strong>).</p><p>In comparison, <strong class="source-inline">y_train</strong> contains the values you want to predict. First, you load a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> that only contains the <strong class="source-inline">approved_loan</strong> column. Then, you convert that DataFrame into a <strong class="bold">NumPy</strong> array using the <strong class="source-inline">values</strong> attribute. This array has a single element<a id="_idIndexMarker894"/> array for each row. For example, <em class="italic">[[0],[1]]</em> represents two records: a not-approved loan with a value of <em class="italic">0</em> and an approved one with a value of <em class="italic">1</em>. Following this, you call the <strong class="source-inline">ravel</strong> method to flatten the array, which converts the given example into <em class="italic">[0, 1]</em>. Although you could have<a id="_idIndexMarker895"/> used the <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> directly to train<a id="_idIndexMarker896"/> the model, a warning message<a id="_idIndexMarker897"/> will inform you that an automatic<a id="_idIndexMarker898"/> convention has occurred, prompting you to use the <strong class="source-inline">ravel</strong> method that you observed in this cell.</p><p>The same process repeats for the <strong class="source-inline">X_validate</strong> DataFrame and the <strong class="source-inline">y_validate</strong> array that will be used to evaluate the model's performance.</p></li>
				<li>Train a model and log the achieved accuracy using the following code:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">run = experiment.start_logging()</p><p class="source-code">sk_model = LogisticRegression()</p><p class="source-code">sk_model.fit(X_train, y_train)</p><p class="source-code">y_predicted = sk_model.predict(X_validate)</p><p class="source-code">accuracy = accuracy_score(y_validate, y_predicted)</p><p class="source-code">print(accuracy)</p><p class="source-code">run.log("accuracy", accuracy)</p><p class="source-code">run.complete()</p><p>Here, you start with a run in the experiment, as defined in <em class="italic">Step 3</em>. You will use this run to register the metrics, logs, and artifacts of the model training process. Then, you train a <strong class="source-inline">LogisticRegression</strong> model, and you use the <strong class="source-inline">accuracy_score</strong> function to calculate the accuracy of the trained model. Following this, you print the calculated accuracy and log it as a metric in the run. In the end, you <strong class="source-inline">complete</strong> the run to finalize its execution.</p></li>
				<li>Now that<a id="_idIndexMarker899"/> you have a trained model referenced<a id="_idIndexMarker900"/> by the <strong class="source-inline">sk_model</strong> variable, you<a id="_idIndexMarker901"/> are going to save it using<a id="_idIndexMarker902"/> the following code:<p class="source-code">import os</p><p class="source-code">import joblib</p><p class="source-code">os.makedirs('./model', exist_ok=True)</p><p class="source-code">joblib.dump(value=sk_model,</p><p class="source-code">            filename=</p><p class="source-code">              os.path.join('./model/','model.joblib'))</p><p>First, you create a folder named <strong class="source-inline">model</strong>. The name of that folder is not important. In that folder, you <strong class="source-inline">dump</strong> the trained model using the <strong class="source-inline">joblib</strong> library. The model is stored in a file named <strong class="source-inline">model.joblib</strong>.</p><p class="callout-heading">Important note</p><p class="callout">The<strong class="source-inline">.joblib</strong> filename extension is not standard, and you can use whatever you like as long as you are consistent. Some people use the <strong class="source-inline">.pkl</strong> filename extension instead, which was used in the past because we were serializing Python object structures using Python's built-in <strong class="source-inline">pickle</strong> module. Nowadays, the <strong class="source-inline">joblib</strong> library is the recommended way that is proposed by <strong class="bold">scikit-learn</strong> because it is more efficient in serializing large NumPy arrays, which is very common<a id="_idIndexMarker903"/> with the trained models. </p></li>
				<li>Now that you<a id="_idIndexMarker904"/> have the artifacts ready, you can<a id="_idIndexMarker905"/> register the model<a id="_idIndexMarker906"/> using the following<a id="_idIndexMarker907"/> code:<p class="source-code">from sklearn import __version__ as sk_version</p><p class="source-code">from azureml.core import Model</p><p class="source-code">run.upload_folder("model", "./model")</p><p class="source-code">model = run.register_model(</p><p class="source-code">        model_name="chapter12-loans",</p><p class="source-code">        model_path="./model/",</p><p class="source-code">        tags={ "accuracy": accuracy},</p><p class="source-code">        properties={ "accuracy": accuracy},</p><p class="source-code">        model_framework= Model.Framework.SCIKITLEARN,</p><p class="source-code">        model_framework_version= sk_version,</p><p class="source-code">        datasets=[("training", loans_ds)]</p><p class="source-code">)</p><p>In the first line, you import the <strong class="source-inline">__version__</strong> variable of the <strong class="source-inline">sklearn</strong> package, which is a string showing the version currently loaded in your environment. Then, you create an alias for that variable (using the <strong class="source-inline">as</strong> statement), and you reference it inside your code as <strong class="source-inline">sk_version</strong>. This is the version of the <strong class="source-inline">sklearn</strong> library that you used to train the model. Additionally, you import the <strong class="source-inline">Model</strong> class from the AzureML SDK to use it as a reference in the following lines.</p><p>After importing your references, you upload the contents of the local <strong class="source-inline">./model</strong> folder, which you created in <em class="italic">Step 6</em>, to the run's outputs, underneath a folder named <strong class="source-inline">model</strong>. This allows AzureML to have access to the artifacts that you are about to register; otherwise, you will receive an <strong class="source-inline">ModelPathNotFoundException</strong> error.</p><p>Having all of the prerequisites ready, you can register the model. The model will be named <strong class="source-inline">chapter12-loans</strong> (the <strong class="source-inline">model_name</strong> argument) using the artifacts that just got uploaded in the <strong class="source-inline">model</strong> folder (the <strong class="source-inline">model_path</strong> argument) of the run's outputs. You<a id="_idIndexMarker908"/> specify the accuracy as both a tag (the <strong class="source-inline">tags</strong> argument) and<a id="_idIndexMarker909"/> a property (the <strong class="source-inline">properties</strong> argument) of that model. You indicate<a id="_idIndexMarker910"/> that you used the <strong class="source-inline">SCIKITLEARN</strong> framework (the <strong class="source-inline">model_framework</strong> argument) to train<a id="_idIndexMarker911"/> the model, and you specify which version of the framework you used (the <strong class="source-inline">model_framework_version</strong> argument). In the last line, you specify that you used the <strong class="source-inline">loans_ds</strong> dataset as a <strong class="source-inline">training</strong> dataset (the <strong class="source-inline">datasets</strong> argument).</p><p class="callout-heading">Important note</p><p class="callout">If you try to rerun the same cell, a <em class="italic">Resource Conflict</em> error will occur because you cannot override files that already exist in the run's outputs folder. If you comment out the <strong class="source-inline">upload_folder</strong> line by using <strong class="source-inline">#</strong> as a line prefix and rerun the cell, you will register a new version of the same model, using the artifacts that already exist in the specific run.</p></li>
				<li>Navigate to <strong class="bold">Assets</strong> | <strong class="bold">Models</strong> from<a id="_idIndexMarker912"/> AzureML studio's menu on the left-hand side. You will reach the model list<a id="_idIndexMarker913"/> you saw in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. Select the <strong class="bold">chapter12-loans</strong> model that you just registered. You should<a id="_idIndexMarker914"/> see something<a id="_idIndexMarker915"/> similar to <em class="italic">Figure 12.4</em>:<div id="_idContainer275" class="IMG---Figure"><img src="Images/B16777_12_004.jpg" alt="Figure 12.4 – The registered model in AzureML studio&#13;&#10;" width="1123" height="980"/></div><p class="figure-caption">Figure 12.4 – The registered model in AzureML studio</p><p>Notice that all of the information you logged is available on this page, including links to the run that trained the model and the dataset you used for training (underneath the <strong class="bold">Datasets</strong> tab). Here, <strong class="bold">accuracy</strong> is logged both as a tag and a property of the model. The<a id="_idIndexMarker916"/> difference between the two<a id="_idIndexMarker917"/> is that you can modify<a id="_idIndexMarker918"/> the tags (by clicking on the pencil icon in the UI or from the code), while the properties cannot<a id="_idIndexMarker919"/> change. If you click on <strong class="bold">Deploy</strong> from this screen, you will get the deployment wizard that you saw in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. The wizard will automatically deploy a real-time endpoint for this model because you used a supported framework (<strong class="source-inline">Model.Framework.SCIKITLEARN</strong>). This type of deployment is considered a no-code deployment, which is a capability that AzureML offers for supported frameworks. Otherwise, you need to specify a scoring file; this is something that we will cover in the <em class="italic">Deploying real-time endpoints</em> section.</p></li>
				<li>If you want to register a pretrained model that you downloaded from the internet, you will<a id="_idIndexMarker920"/> not have a <strong class="source-inline">Run</strong> object to call the <strong class="source-inline">register_model</strong> method. You can use the <strong class="source-inline">register</strong> method of the <strong class="source-inline">Model</strong> class, as demonstrated in the following code snippet:<p class="source-code">from azureml.core import Model</p><p class="source-code">offline_model = Model.register(</p><p class="source-code">        ws,</p><p class="source-code">        model_name="chapter12-pre-trained-loans",</p><p class="source-code">        model_path="./model/",</p><p class="source-code">        properties={"accuracy": 0.828},</p><p class="source-code">        model_framework= "ScikitLearn",</p><p class="source-code">        model_framework_version= "0.22.2.post1"</p><p class="source-code">) </p><p>In the preceding code, you register, in your AzureML workspace (the <strong class="source-inline">ws</strong> variable), the artifacts that are located inside the <em class="italic">local</em> <strong class="source-inline">model</strong> folder (the <strong class="source-inline">model_path</strong> argument) as a model named <strong class="source-inline">chapter12-pre-trained-loans</strong> (the <strong class="source-inline">model_name</strong> argument). This is a model trained using version <strong class="source-inline">0.22.2.post1</strong> (the <strong class="source-inline">model_framework_version</strong> argument) of the <strong class="source-inline">sklearn</strong> library (the <strong class="source-inline">model_framework_version</strong> argument). Additionally, its accuracy, which is <strong class="source-inline">0.828</strong>, is stored as a model property.</p></li>
				<li>If you had a process<a id="_idIndexMarker921"/> to train new models, such as the scheduled<a id="_idIndexMarker922"/> pipeline that you created in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>, you will have to verify whether the newly<a id="_idIndexMarker923"/> trained model has better metrics than the one already registered. Then, if it is better, proceed with the registration of the model. To do that, you can use code<a id="_idIndexMarker924"/> similar to the following:<p class="source-code">from sklearn.linear_model import RidgeClassifier</p><p class="source-code">new_model = RidgeClassifier(solver='svd')</p><p class="source-code">new_model.fit(X_train, y_train)</p><p class="source-code">y_predicted = new_model.predict(X_validate)</p><p class="source-code">accuracy = accuracy_score(y_validate, y_predicted)</p><p class="source-code">registered_model = Model(ws, name="chapter12-loans")</p><p class="source-code">r_version = registered_model.version</p><p class="source-code">r_acc = float(registered_model.properties['accuracy'])</p><p class="source-code">if accuracy &gt; r_acc:</p><p class="source-code">    print(f"New model has better accuracy {accuracy}")</p><p class="source-code">else:</p><p class="source-code">    print(f"Registered model with version {r_version}" \</p><p class="source-code">           " has better accuracy {r_acc}")</p><p>In the preceding code, you train a <strong class="source-inline">RidgeClassifier</strong>-based model that uses <strong class="bold">Singular Value Decomposition</strong> (<strong class="bold">SVD</strong>) to compute the ridge coefficients of the model. The code<a id="_idIndexMarker925"/> is similar to the one that you used in <em class="italic">Step 5</em>. Then, you get a reference to the latest version of the workspace's registered model, named <strong class="source-inline">chapter12-loans</strong>, which you registered in <em class="italic">Step 7</em>. The <strong class="source-inline">registered_model</strong> variable<a id="_idIndexMarker926"/> has the same reference as the <strong class="source-inline">model</strong> variable you got in <em class="italic">Step 7</em>; only, this time, you create that<a id="_idIndexMarker927"/> reference using<a id="_idIndexMarker928"/> the <strong class="source-inline">Model</strong> class and not by registering<a id="_idIndexMarker929"/> a model. From that model, you read the <strong class="source-inline">version</strong> attribute and the <strong class="source-inline">accuracy</strong> property. You could retrieve the accuracy from the <strong class="source-inline">tags</strong> dictionary instead of the <strong class="source-inline">properties</strong> dictionary of the model. You convert the accuracy value into a float because tags and properties store their values as strings. Following this, you then compare the new model's accuracy to the one that has already been registered (which is stored in the <strong class="source-inline">r_acc</strong> variable). If the new model is better than the registered one, you print a message. In this case, you repeat <em class="italic">Step 6</em> and <em class="italic">Step 7</em> to store the model and then register the new, improved version of the model.</p><p class="callout-heading">Important note</p><p class="callout">To register a new model version, you just need to register the new model with the same name. By registering a new model with the same name, AzureML will automatically create a new version for you.</p></li>
				<li>Optionally, as<a id="_idIndexMarker930"/> a last step, delete the locally stored<a id="_idIndexMarker931"/> model using the following code:<p class="source-code">import shutil</p><p class="source-code">shutil.rmtree('./model',ignore_errors=True)</p><p>This code deletes the <strong class="source-inline">model</strong> folder<a id="_idIndexMarker932"/> that you created in <em class="italic">Step 6</em>, including<a id="_idIndexMarker933"/> the serialized model you no longer need. The <strong class="source-inline">ignore_errors</strong> parameter allows you to run this cell even if the folder doesn't exist without raising any errors.</p></li>
			</ol>
			<p>In this section, you trained a model in your notebook within the Jupyter kernel. Then, you registered the model inside your workspace. You could have used the same registration code in the <strong class="source-inline">train_model.py</strong> script, which you authored in <em class="italic">Step 11</em> of the <em class="italic">Authoring a pipeline</em> section of <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>, to register the <strong class="bold">LightGBM</strong> model that you were training within the pipeline. You would need to get a reference<a id="_idIndexMarker934"/> to the execution context using the <strong class="source-inline">run=Run.get_context()</strong> method, and then you would need to upload the serialized model and register the model, as you did in <em class="italic">Step 7</em>. As an additional activity, try to modify the <strong class="source-inline">train_model.py</strong> script and <strong class="source-inline">chapter11.ipynb</strong> to create a pipeline that registers the model that is being trained within the pipeline. A potential solution to this activity is available in the <strong class="source-inline">train_model_and_register.py</strong> script. This can be found in the <strong class="source-inline">step02</strong> folder of the GitHub repository at <a href="http://bit.ly/dp100-ch11">http://bit.ly/dp100-ch11</a>.</p>
			<p>In the next section, you will start operationalizing the model that you registered in this section by deploying it as a web service that will serve real-time inferences. </p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor175"/>Deploying real-time endpoints</h1>
			<p>Let's imagine that you<a id="_idIndexMarker935"/> have an e-banking solution that has a process for customers to request loans. You want to properly set the expectations of the customer and prepare them for potential rejection. When the customer submits their loan application form, you want to invoke the model you registered in the <em class="italic">Registering models in the workspace</em> section, that is, the model named <strong class="bold">chapter12-loans</strong>, and pass in the information that the customer filled out on the application form. If the model predicts that the loan will not be approved, a message will appear on the confirmation<a id="_idIndexMarker936"/> page of the loan request, preparing the customer for the potential rejection of the loan request. </p>
			<p><em class="italic">Figure 12.5</em> shows an oversimplified architecture to depict the flow of requests that start from the customer to the real-time endpoint of the model:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="Images/B16777_12_005.jpg" alt="Figure 12.5 – An oversimplified e-banking architecture showing the flow &#13;&#10;of requests from the customer to the model&#13;&#10;" width="829" height="496"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – An oversimplified e-banking architecture showing the flow of requests from the customer to the model</p>
			<p>The easiest way to deploy a model is via the no-code deployment approach that AzureML offers for specific<a id="_idIndexMarker937"/> machine learning frameworks, including <strong class="bold">TensorFlow</strong>, <strong class="bold">ONNX</strong>, and the <strong class="source-inline">sklearn</strong> library that you used<a id="_idIndexMarker938"/> in the previous section. Perform the<a id="_idIndexMarker939"/> following steps:</p>
			<ol>
				<li value="1">Go to the <strong class="source-inline">chapter12.ipynb</strong> notebook, and add the following code to get a reference to the last version of the <strong class="bold">chapter12-loans</strong> model that you created in the previous section:<p class="source-code">from azureml.core import Workspace, Model</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">model = Model(ws, name="chapter12-loans")</p></li>
				<li>To deploy a real-time endpoint, use the following code:<p class="source-code">no_code_service = Model.deploy(ws, "no-code-loans",</p><p class="source-code">                               [model])</p><p class="source-code">no_code_service.wait_for_deployment(show_output=True)</p><p>This code deploys a new real-time endpoint service, named <strong class="source-inline">no-code-loans</strong>, and then waits for the deployment to complete.</p></li>
				<li>To get the scoring<a id="_idIndexMarker940"/> URI for the newly deployed endpoint, use the following code:<p class="source-code">print(no_code_service.scoring_uri)</p><p>This is a URL in the format of <a href="http://guid.region.azurecontainer.io/score">http://guid.region.azurecontainer.io/score</a>, which<a id="_idIndexMarker941"/> accepts <strong class="bold">POST</strong> requests with a <strong class="bold">JSON</strong> payload, as follows:</p><p class="source-code">{"data": [[2000,2,45]]}</p><p>This payload<a id="_idIndexMarker942"/> will trigger an inference request for a customer with a monthly income of 2,000, who has 2 credit<a id="_idIndexMarker943"/> cards and is 45 years old. You can use tools such as <strong class="bold">Postman</strong> or <strong class="bold">curl</strong> to craft such an HTTP request<a id="_idIndexMarker944"/> and invoke the endpoint.</p></li>
				<li>Instead of making an HTTP request using a tool such as <strong class="source-inline">curl</strong>, you can use the <strong class="source-inline">no_code_service</strong> reference and invoke the <strong class="source-inline">run</strong> method by passing in the JSON payload that you would normally send to the service:<p class="source-code">import json</p><p class="source-code">input_payload = json.dumps({</p><p class="source-code">    'data': [[2000, 2, 45], [2000, 9, 45]],</p><p class="source-code">    'method': 'predict'</p><p class="source-code">})</p><p class="source-code">output = no_code_service.run(input_payload)</p><p class="source-code">print(output)</p><p>The preceding code imports the <strong class="source-inline">json</strong> library, which helps you to serialize objects into JSON strings. You create the payload using the <strong class="source-inline">dumps</strong> method. Note that the payload is slightly different from the simple version you saw in <em class="italic">Step 3</em>. Instead of passing a single customer's information, in this example, you pass the information of two customers: the one you passed before and another one who has <em class="italic">9</em> credit cards instead of <em class="italic">2</em>. Moreover, you are specifying which method to invoke. By default, the method name of the model is <strong class="source-inline">predict</strong>, which is the one you have been using in the previous chapters to make inferences. Finally, you print the output, which should appear similar to the following:</p><p class="source-code">{'predict': [0, 1]}</p><p>The preceding result shows that the first loan will be rejected, while the second one will be approved.</p><p>Most of the classification<a id="_idIndexMarker945"/> models offer another method called <strong class="source-inline">predict_proba</strong>, which returns an array<a id="_idIndexMarker946"/> with the probabilities of each label. In the <strong class="source-inline">loans</strong> approval case, this array will only contain 2 probabilities that sum to 1, that is, the probability of the loan getting approved and the probability of it getting rejected. If you change the method name from <strong class="source-inline">predict</strong> to <strong class="source-inline">predict_proba</strong> and re-execute the cell, you will get the following result:</p><p class="source-code">{'predict_proba': [[0.998, 0.002], [0.173, 0.827]]}</p><p>The preceding result shows that the model is 99.8% confident that the first loan will be rejected and 82.7% confident that the second loan will be approved.</p></li>
				<li>Optionally, navigate to <strong class="bold">Assets</strong> | <strong class="bold">Endpoints</strong> to view the <strong class="bold">no-code-loans</strong> endpoint that you just deployed. Note that the compute type is a container instance, as displayed in <em class="italic">Figure 12.6</em>: <div id="_idContainer277" class="IMG---Figure"><img src="Images/B16777_12_006.jpg" alt="Figure 12.6 – Real-time endpoint information for the endpoint you just deployed&#13;&#10;" width="1485" height="1370"/></div><p class="figure-caption">Figure 12.6 – Real-time endpoint information for the endpoint you just deployed</p><p>AzureML hosts the web<a id="_idIndexMarker947"/> application that is serving the model in <strong class="bold">Azure Container Instance</strong> (<strong class="bold">ACI</strong>), which is similar to the one you deployed in <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>. Note that the container instance that was deployed is configured to use <strong class="bold">0.1</strong> CPU cores and <strong class="bold">0.5 GB</strong> memory. If your model<a id="_idIndexMarker948"/> requires more resources, you could specify the size of the ACI service that should be deployed for the specific model. You can do this by passing a <strong class="source-inline">ResourceConfiguration</strong> object to the <strong class="source-inline">resource_configuration</strong> parameter while registering the model, as shown in the following code snippet:</p><p class="source-code">from azureml.core import Model</p><p class="source-code">from azureml.core.resource_configuration import ResourceConfiguration</p><p class="source-code">model = Model.register(workspace=ws,</p><p class="source-code">            model_name="chapter12-demanding-loans",</p><p class="source-code">            model_path="./model/",</p><p class="source-code">            model_framework=Model.Framework.SCIKITLEARN, </p><p class="source-code">            model_framework_version="0.22.2.post1",</p><p class="source-code">            resource_configuration=ResourceConfiguration( </p><p class="source-code">                             cpu=1, memory_in_gb=1.5))</p><p>In the preceding code, you register a model named <strong class="source-inline">chapter12-demanding-loans</strong>. You specify that it needs <strong class="source-inline">1</strong> CPU and <strong class="source-inline">1.5</strong> GB of RAM. Note that if you deleted the <strong class="source-inline">model</strong> folder in <em class="italic">Step 11</em> of the <em class="italic">Registering models in the workspace</em> section, this code would fail to register the new model, as it will not be able to find the model artifact.</p></li>
				<li>To save on costs, you should delete the service using the following code:<p class="source-code">no_code_service.delete()</p></li>
			</ol>
			<p>So far, you have<a id="_idIndexMarker949"/> deployed a real-time endpoint using the no-code approach, which deploys the model as a container instance. This is only feasible if the model is trained using specific supported models. In the next section, you will learn how to deploy models using more advanced options.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor176"/>Understanding the model deployment options</h2>
			<p>In the previous section, you deployed<a id="_idIndexMarker950"/> a model using the no-code approach. Behind the scenes, AzureML used an <strong class="source-inline">environment</strong> with all of the required model dependencies, which, in our case, was <strong class="source-inline">sklearn</strong>, generated a Python script to load the model and make inferences when data arrived at the endpoint, and published an ACI service using an <strong class="source-inline">AciServiceDeploymentConfiguration</strong> class.</p>
			<p>If you had a model that was trained<a id="_idIndexMarker951"/> with a non-supported framework or if you wanted to get better control of the deployment model, you could deploy the model using the AzureML SDK classes, as depicted in <em class="italic">Figure 12.7</em>:</p>
			<div>
				<div id="_idContainer278" class="IMG---Figure">
					<img src="Images/B16777_12_007.jpg" alt="Figure 12.7 – The components required in a real-time endpoint deployment&#13;&#10;" width="1650" height="344"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – The components required in a real-time endpoint deployment</p>
			<p>Here, the <strong class="bold">InferenceConfig</strong> class specifies the model dependencies. It requires an entry script that will load the model and process the incoming requests along with an environment in which the entry script will execute. This environment contains all of the dependencies required by the model to load and make inferences.</p>
			<p>The entry script should have the following two methods:</p>
			<ul>
				<li><strong class="source-inline">Init</strong>: During this step, the script loads the trained model into memory. Depending on how you stored<a id="_idIndexMarker952"/> the state of the model to the disk, you<a id="_idIndexMarker953"/> can use the corresponding method to deserialize the model. For example, if you used the <strong class="source-inline">joblib</strong> library to serialize your model, you can use the <strong class="source-inline">load</strong> method of the same library to load it into memory. Some models provide their own serialization and deserialization methods, but the process remains the same; the state of the trained model is persisted in one file or multiple files, which you can later use to load the trained model into memory. Depending on how big your model is, the initialization phase might require a significant amount of time. Smaller <strong class="source-inline">sklearn</strong> models should load into memory in only a few milliseconds, while larger neural networks might require a couple of seconds to load.</li>
				<li><strong class="source-inline">run</strong>: This is the method invoked when a dataset is received by the real-time endpoint for inference. In this method, you must use the model loaded in the <strong class="source-inline">init</strong> code<a id="_idIndexMarker954"/> to invoke the prediction method it offers to make inferences<a id="_idIndexMarker955"/> on the incoming data. As mentioned earlier, most<a id="_idIndexMarker956"/> of the models offer the <strong class="source-inline">predict</strong> method, which you can invoke and pass into the data that you want to make<a id="_idIndexMarker957"/> an inference. Most of the classification models offer an additional method, called <strong class="source-inline">predict_proba</strong>, which returns the probabilities of each class. The AutoML forecasting models offer the <strong class="source-inline">forecast</strong> method instead of the <strong class="source-inline">predict</strong> method. Neural networks have a different approach when it comes to making predictions. For example, in the first version of TensorFlow, you would have to invoke a prediction method through a <strong class="source-inline">session.run()</strong> method call.</li>
			</ul>
			<p>Once you have configured the model dependencies, you need to decide where you want to deploy the model. The AzureML SDK<a id="_idIndexMarker958"/> offers three classes: <strong class="source-inline">LocalWebserviceDeploymentConfiguration</strong>, <strong class="source-inline">AciServiceDeploymentConfiguration</strong>, and <strong class="source-inline">AksServiceDeploymentConfiguration</strong>. These allow<a id="_idIndexMarker959"/> you to deploy on your local machine into ACI or <strong class="bold">Azure Kubernetes Services</strong> (<strong class="bold">AKS</strong>), as displayed in <em class="italic">Figure 12.8</em>:</p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="Images/B16777_12_008.jpg" alt="Figure 12.8 – Picking the right compute target for your model&#13;&#10;" width="1378" height="827"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Picking the right compute target for your model</p>
			<p>As you might have gathered, in <em class="italic">Figure 12.8</em>, you can deploy to your local computer by specifying the port you want the service to listen to. This is a nice approach in which to debug any potential loading issues of your model or verify the integration with the remaining systems<a id="_idIndexMarker960"/> on your local computer. The next option is to use ACI, which is meant for test environments or small-scale production environments. You can only use CPUs and not GPUs in the <strong class="source-inline">AciServiceDeploymentConfiguration</strong> class. You can protect the endpoint using a key-based authentication by setting the <strong class="source-inline">auth_enabled</strong> parameter to <strong class="source-inline">True</strong>. This authentication method<a id="_idIndexMarker961"/> requires you to pass a static key as an <strong class="bold">Authorization</strong> header into your HTTP requests. </p>
			<p>On the other side, <strong class="source-inline">AksServiceDeploymentConfiguration</strong> deploys the service inside an AKS cluster. This allows you to use GPUs if your model can make use of them and if the cluster you are deploying to has GPU-capable nodes. This deployment configuration allows you to choose between key-based authentication or a token-based one. Token-based authentication<a id="_idIndexMarker962"/> requires the end user to acquire an access token from the <strong class="bold">Azure Active Directory</strong> that protects the AzureML workspace, which will allow you to access the endpoint deployed within it. This token is short-lived and conceals the caller's identity in contrast to key-based authentication, which is the only available option in ACI. Another production-ready feature of the AKS deployment is the ability to dynamically scale up and down to handle the fluctuation in the number of incoming requests. In the e-banking scenario at hand, customers tend to visit the e-banking solution during working hours, and the system is pretty much idle at night. Moreover, at the end of the month, the incoming traffic peaks. In such a workload, you want to be able to scale your endpoint to accommodate for the increase in traffic when needed. AKS can automatically spin up multiple containers of your model and load balance the traffic among them when the incoming traffic increases significantly. When the traffic returns to normal, it can only keep a single container as a hot standby for potential incoming traffic.</p>
			<p>Now that you have a better<a id="_idIndexMarker963"/> understanding of the deployment options, you will deploy the same model in ACI<a id="_idIndexMarker964"/> using the classes that you saw in <em class="italic">Figure 12.7</em>: </p>
			<ol>
				<li value="1">The first thing you will need to create is the entry script. Underneath the <strong class="bold">chapter12</strong> folder, create a new folder named <strong class="bold">script</strong> and place a <strong class="bold">score.py</strong> file inside it, as shown in <em class="italic">Figure 12.9</em>: <div id="_idContainer280" class="IMG---Figure"><img src="Images/B16777_12_009.jpg" alt="Figure 12.9 – Adding the score.py file for the real-time endpoint&#13;&#10;" width="766" height="909"/></div><p class="figure-caption">Figure 12.9 – Adding the score.py file for the real-time endpoint</p></li>
				<li>In the <strong class="bold">score.py</strong> file, add<a id="_idIndexMarker965"/> the following code:<p class="source-code">import os</p><p class="source-code">import joblib</p><p class="source-code">import json</p><p class="source-code">def init():</p><p class="source-code">    global model</p><p class="source-code">    model_path = \</p><p class="source-code">        os.path.join(os.getenv("AZUREML_MODEL_DIR"), </p><p class="source-code">                     "model/model.joblib")</p><p class="source-code">    print(f"Loading model from {model_path}")</p><p class="source-code">    model = joblib.load(model_path)</p><p class="source-code">def run(raw_data):</p><p class="source-code">    try:</p><p class="source-code">        print(raw_data)</p><p class="source-code">        data = json.loads(raw_data)["data"]</p><p class="source-code">        result = model.predict(data)</p><p class="source-code">        return result.tolist()</p><p class="source-code">    except Exception as e:</p><p class="source-code">        error = str(e)</p><p class="source-code">        return error</p><p>This code implements<a id="_idIndexMarker966"/> the two methods that you read about earlier in this section. In the <strong class="source-inline">init</strong> method, you are getting the path of the serialized model using the <strong class="source-inline">AZUREML_MODEL_DIR</strong> environment variable. When AzureML spins up the Docker image that will be serving the model, this variable points to the folder where the model is located; for example, <strong class="source-inline">/tmp/azureml_umso8bpm/chapter12-loans/1</strong> could be the location where you find the first version of the <strong class="source-inline">chapter12-loans</strong> model. In that folder, the actual artifact, named <strong class="source-inline">model.joblib</strong>, is located in the <strong class="source-inline">model</strong> folder, which you uploaded in <em class="italic">Step 5</em> of the <em class="italic">Deploying real-time endpoints</em> section. You use <strong class="source-inline">os.path.join</strong> to get the final path of the model, and then you load the model in a <strong class="source-inline">global</strong> variable named <strong class="source-inline">model</strong>. If you want to use the AzureML SDK to get the location of the model, you could use <strong class="source-inline">model_path = Model.get_model_path(model_name, version=version)</strong>, which uses the same environment variable under the hood. However, note that you would need to install the AzureML SDK in your environment to be able to import the <strong class="source-inline">Model</strong> class from it; this is something that is not necessary with the preceding code.</p><p class="callout-heading">Important note</p><p class="callout">Note that you are using <strong class="source-inline">print</strong> to write the model path and incoming <strong class="source-inline">raw_data</strong> into the console. You will learn how to view those messages in the <em class="italic">Monitoring with Application Insights</em> section.</p><p>In the <strong class="source-inline">run</strong> method, you are using the <strong class="source-inline">try</strong> <strong class="source-inline">except</strong> block to catch potential errors while<a id="_idIndexMarker967"/> trying to read the request's input. If such an error occurs, the exception is serialized into a string (using the <strong class="source-inline">str()</strong> method), which is returned to the end user. Note that returning the exceptions to the caller is a security anti-pattern, as you might accidentally expose valuable information to a potential attacker, but it is helpful while debugging. Instead of returning the error message, you could<a id="_idIndexMarker968"/> use a <strong class="source-inline">print</strong> statement or a more advanced library such as <strong class="bold">OpenCensus</strong>, and then review the error within Application Insights, as we will cover in the <em class="italic">Monitoring with Application Insights</em> section. You can learn more about OpenCensus by following the links in the <em class="italic">Further reading</em> section of this chapter. Within the <strong class="source-inline">try</strong> block, you deserialize the incoming JSON payload, as demonstrated in <em class="italic">Step 3</em> of the <em class="italic">Deploying real-time endpoints</em> section. Then, you call the <strong class="source-inline">predict</strong> method of the <strong class="source-inline">model</strong> object you have loaded in memory through the <strong class="source-inline">init</strong> method. Following this, you return the model results as a list that will be serialized into an array.</p><p class="callout-heading">Important note</p><p class="callout">You will never directly invoke either the <strong class="source-inline">init</strong> or the <strong class="source-inline">run</strong> method. There is another piece of code that AzureML will be putting inside the final Docker image, which is the HTTP inference server. This server will be responsible for calling your <strong class="source-inline">init</strong> method when the server boots up and will pass the incoming HTTP data into the <strong class="source-inline">run</strong> method. Moreover, the result that you return in the <strong class="source-inline">run</strong> method will be serialized into a <strong class="bold">JSON</strong> and will be returned to the caller.</p></li>
				<li>The next thing you need is an <strong class="source-inline">Environment</strong> that has all of the necessary dependencies to run the <strong class="source-inline">score.py</strong> script that you created. Open your <strong class="source-inline">chapter12.ipynb</strong> notebook<a id="_idIndexMarker969"/> and add the following code inside a new cell:<p class="source-code">from azureml.core import Environment</p><p class="source-code">from azureml.core.conda_dependencies import CondaDependencies </p><p class="source-code">import sklearn</p><p class="source-code">myEnv= Environment(name="sklearn-inference")</p><p class="source-code">myEnv.Python.conda_dependencies = CondaDependencies()</p><p class="source-code">myEnv.Python.conda_dependencies.add_conda_package(</p><p class="source-code">               f"scikit-learn=={sklearn.__version__}")</p><p class="source-code">myEnv.Python.conda_dependencies.add_pip_package(</p><p class="source-code">               "azureml-defaults&gt;=1.0.45")</p><p>In the preceding code, you created an <strong class="source-inline">Environment</strong> as demonstrated in <a href="B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 8</em></a>, <em class="italic">Experimenting with Python Code</em>. You add the <strong class="source-inline">scikit-learn</strong> <strong class="bold">conda</strong> package to the version that is available in the compute where you trained<a id="_idIndexMarker970"/> the model. Moreover, you add the <strong class="source-inline">azureml-defaults</strong> <strong class="source-inline">pip</strong> package, which contains the necessary functionality to host the model as a web service. Because you are building your own <strong class="source-inline">Environment</strong>, you need to add this package and use, at the very least, version 1.0.45. This is the bare minimum environment that you can use to run your scoring script. Additionally, AzureML provides a curated environment that you can use, such as <strong class="source-inline">AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference</strong>, which contains everything you need to make an inference request using a model trained in <strong class="source-inline">sklearn</strong> version 0.24.1.</p></li>
				<li>You have defined everything that is needed by the <strong class="source-inline">InferenceConfig</strong> class. Add a new cell and type<a id="_idIndexMarker971"/> in the following code to put everything together:<p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">inference_config = InferenceConfig(</p><p class="source-code">                      source_directory= "./script",</p><p class="source-code">                      entry_script='score.py', </p><p class="source-code">                      environment=myEnv)</p><p>This code creates the configuration you will need to make inferences with your model. It uses the <strong class="source-inline">score.py</strong> file that is located inside the <strong class="source-inline">script</strong> folder and executes that file in the <strong class="source-inline">myEnv</strong> environment, which you defined in <em class="italic">Step 3</em>.</p></li>
				<li>Now you have two out of the three components depicted in <em class="italic">Figure 12.7</em>. In this step, you will create an <strong class="source-inline">AciServiceDeploymentConfiguration</strong> class, and you will deploy the model in ACI. In a new cell, add the following code:<p class="source-code">from azureml.core.webservice import AciWebservice</p><p class="source-code">deployment_config = AciWebservice.deploy_configuration(</p><p class="source-code">                    cpu_cores=1, memory_gb=1)</p><p class="source-code">service = Model.deploy(ws, "aci-loans", [model], </p><p class="source-code">                inference_config, deployment_config)</p><p class="source-code">service.wait_for_deployment(show_output=True)</p><p>Here, we use the <strong class="source-inline">AciWebservice</strong> class to get a deployment configuration for the container instance you want to deploy. In the preceding code, you specify that you require 1 CPU core and 1 GB of RAM. Then, you deploy the model into a new service, named <strong class="source-inline">aci-loans</strong>, and you wait for the deployment to complete.</p><p class="callout-heading">Important note</p><p class="callout">If you are running into issues while attempting to deploy the container, you can view the error messages in the printed outputs or use the <strong class="source-inline">service.get_logs()</strong> method. Most likely, it is an issue with your code base within the <strong class="source-inline">score.py</strong> script. You can locally test the code by installing the <strong class="source-inline">azureml-inference-server-http</strong> pip package and running the following command:</p><p class="callout"><strong class="bold"> azmlinfsrv --entry_script score.py</strong></p><p class="callout">This will show you any potential errors when loading your script. Once you have fixed the script, the preceding command should open the web server that is listening to port <strong class="source-inline">5001</strong>. Another approach to debugging such situations is to use <strong class="source-inline">LocalWebservice</strong>, as we will discuss later. If your code is fine, then you might be running into memory issues. This should be visible in the service logs. In that case, refer to the next section to learn how you can profile your model to determine its resource requirements.</p></li>
				<li>To test the deployed<a id="_idIndexMarker972"/> service, you can use the following code, which is similar to the one that you used in the previous section:<p class="source-code">import json</p><p class="source-code">input_payload = json.dumps({</p><p class="source-code">    'data': [[2000, 2, 45]]</p><p class="source-code">})</p><p class="source-code">output = service.run(input_payload)</p><p class="source-code">print(output)</p><p>Note that the <strong class="source-inline">method</strong> property of the payload, which you used in <em class="italic">Step 4</em> of the <em class="italic">Deploying real-time endpoints</em> section, will not have any effect on this deployment and is omitted from the payload. If you wanted to support this property, you would have to write the code within the <strong class="source-inline">run</strong> method of the <strong class="source-inline">score.py</strong> file to read that property and call the corresponding method of the model.</p></li>
				<li>To save on costs, delete the service when you are done testing it using the following code:<p class="source-code">service.delete()</p></li>
				<li>If you want to deploy<a id="_idIndexMarker973"/> the same service in your local computer, you can use the following code:<p class="source-code">from azureml.core.webservice import LocalWebservice</p><p class="source-code">deployment_config = LocalWebservice.deploy_configuration(port=1337)</p><p class="source-code">service = Model.deploy(ws, "local-loans", [model], inference_config, deployment_config)</p><p class="source-code">service.wait_for_deployment()</p><p>Instead of the <strong class="source-inline">AciWebservice</strong> class, you use the <strong class="source-inline">LocalWebservice</strong> to create a local service that listens to port <strong class="source-inline">1337</strong>. If you are running the notebooks on your local computer, you need to visit <strong class="source-inline">http://localhost:1337</strong> and view the service endpoint's health status. Now that you have run this code within the AzureML notebooks, the local computer is the compute instance you are working on. To view port <strong class="source-inline">1337</strong> of the compute instance named <strong class="bold">compute-instance-name</strong>, which is deployed in <strong class="bold">region</strong>, please visit <a href="https://compute-instance-name-1337.region.instances.azureml.ms">https://compute-instance-name-1337.region.instances.azureml.ms</a>. This endpoint is only accessible by you since you are assigned to the specific compute instance. Once you are done playing with the locally deployed endpoint, you can delete it by using the <strong class="source-inline">service.delete()</strong> code, as demonstrated in <em class="italic">Step 7</em>.</p></li>
			</ol>
			<p>Similar to the <strong class="source-inline">AciWebservice</strong> and the <strong class="source-inline">LocalWebservice</strong>, you can use <strong class="source-inline">AksWebservice</strong> to create an <strong class="source-inline">AksServiceDeploymentConfiguration</strong>. While deploying it, you would need to specify an additional parameter in the <strong class="source-inline">Model.deploy</strong> method, that is, the <strong class="source-inline">deployment_target</strong> parameter. This parameter allows you to specify the <strong class="source-inline">AksCompute</strong> inference cluster that you want to deploy the model to.</p>
			<p>Aside from the local computer, ACI, and AKS deployment options that you saw earlier, AzureML offers<a id="_idIndexMarker974"/> multiple other deployment options. For example, <strong class="bold">Azure Functions</strong> allows you to run your models inside a serverless infrastructure, and <strong class="bold">Azure App Services</strong> hosts the model as a traditional web application that is always<a id="_idIndexMarker975"/> ready to serve incoming requests. On the other hand, you can use <strong class="bold">IoT Edge</strong>, which allows you to deploy the service on an Edge device such as a Raspberry<a id="_idIndexMarker976"/> Pi or a GPU-based Jetson Nano. Finally, you can even package the model inside a Docker container image, which can be operationalized inside an isolated air gap data center.</p>
			<p>In this section, you deployed<a id="_idIndexMarker977"/> an ACI real-time inference endpoint requesting 1 CPU core and 1 GB of RAM. In the next section, you will explore how you can optimize your resource requirements by profiling the model's performance.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor177"/>Profiling the model's resource requirements</h2>
			<p>Before bringing something into production, it is very common to perform a stress test. Essentially, this test bombards<a id="_idIndexMarker978"/> the real-time endpoint with requests<a id="_idIndexMarker979"/> and measures the responsiveness and performance of the endpoint. You can do something similar with your models to understand what type of resources you will need in order for them to perform as expected. For example, you might need to ensure that all inferences are performed within 200 ms.</p>
			<p>In this section, you are going to create a test dataset that will be used to stress-test the real-time endpoint<a id="_idIndexMarker980"/> and observe its performance. Each row in the dataset will contain a single inference request.</p>
			<p>Navigate to your <strong class="source-inline">chapter12.ipynb</strong> notebook and perform the following steps:</p>
			<ol>
				<li value="1">In a new cell, add the following code:<p class="source-code">loans_ds = ws.datasets['loans']</p><p class="source-code">prof_df = loans_ds.drop_columns('approved_loan') \</p><p class="source-code">                        .to_pandas_dataframe()</p><p class="source-code">prof_df['sample_request'] = \</p><p class="source-code">    "{'data':[[" + prof_df['income'].map(str) \</p><p class="source-code">  + ","+ prof_df['credit_cards'].map(str) \</p><p class="source-code">  + "," + prof_df['age'].map(str) + "]]}"</p><p class="source-code">prof_df = prof_df[['sample_request']]</p><p class="source-code">prof_df.head()</p><p>This code loads the <strong class="source-inline">loans</strong> dataset, drops the <strong class="source-inline">approved_loan</strong> column, which we don't need, and loads it inside a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong>. Following this, you create a new column named <strong class="source-inline">sample_request</strong> that concatenates the columns to produce a string such as the following:</p><p class="source-code">{"data": [[2000,2,45]]}</p><p>Then, you keep<a id="_idIndexMarker981"/> only that column and print the top 5 rows to verify that the requests look as expected. Note that it does not matter whether the data is the one we used to train the model. It could even be random records. We only care about the number of requests we will be making and not what the inference result is going to look like.</p></li>
				<li>Store the newly created dataset inside the workspace using the following code:<p class="source-code">from azureml.core import Dataset</p><p class="source-code">dstore = ws.get_default_datastore()</p><p class="source-code">loan_req_ds = Dataset.Tabular.register_pandas_dataframe(</p><p class="source-code">    dataframe=prof_df,</p><p class="source-code">    target=(dstore,"/samples/loans-requests"),</p><p class="source-code">    name="loans-requests",</p><p class="source-code">    description="Sample requests for the loans model")</p><p>The preceding code registers the DataFrame as the <strong class="source-inline">loans-requests</strong> dataset. The data is stored inside <strong class="source-inline">/samples/loans-requests</strong> of the default datastore. The <strong class="source-inline">loans_req_ds</strong> variable has a reference to the newly registered <strong class="source-inline">tabular</strong> <strong class="source-inline">Dataset</strong>.</p></li>
				<li>Now that you have the necessary data, you can start the model profiling process using the following code:<p class="source-code">profile = Model.profile(ws,</p><p class="source-code">              'chapter12-loan',</p><p class="source-code">              [model], inference_config,</p><p class="source-code">              input_dataset=loan_req_ds,</p><p class="source-code">              cpu=2, memory_in_gb=1)</p><p class="source-code">profile.wait_for_completion(True)</p><p class="source-code">print(profile.get_details())</p><p>Note that the profile method requires the <strong class="source-inline">model</strong> and the <strong class="source-inline">inference_config</strong> that you used during model deployment in the previous section. Additionally, you<a id="_idIndexMarker982"/> need to specify your ACI size to use to perform the analysis. In the preceding code, you request 2 CPUs and 1 GB of RAM. The analysis could take a long time, sometimes, more than 20 minutes. After the analysis completes, you will view the results, including the 1 CPU as the <strong class="source-inline">recommendedCpu</strong> and 0.5 GB of RAM as the <strong class="source-inline">recommendedMemoryInGB</strong> value.</p><p class="callout-heading">Important note</p><p class="callout">The name of the model profile should be unique within the workspace. An error will occur if you try to rerun the code of <em class="italic">Step 3</em> without changing the name.</p></li>
			</ol>
			<p>Behind the scenes, an experiment named <strong class="bold">chapter12-loan-ModelProfile</strong> is created. In that experiment, a <strong class="source-inline">ModelProfile</strong> run is executed, which deploys an ACI service with the model. Once the service is up and running, the process sends the 500 requests that you specified in the <strong class="bold">loan_req_ds</strong> dataset and records the response time of the model while monitoring the CPU and memory utilization of the deployed container instance. AzureML<a id="_idIndexMarker983"/> can suggest the recommended CPU and memory that you should configure for your real-time endpoint based on those statistics. </p>
			<p>In the next section, you will use those values to deploy an ACI service. Following this, you will explore how to monitor its performance once deployed in production and log the incoming data using <strong class="bold">Application Insights</strong>.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor178"/>Monitoring with Application Insights</h2>
			<p>As you learned in <a href="B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026"><em class="italic">Chapter 2</em></a>, <em class="italic">Deploying Azure Machine Learning Workspace Resources</em>, when you deploy the AzureML workspace, an Application Insights account named <strong class="source-inline">packtlearningm&lt;random_number&gt;</strong> is deployed in the same resource group. This Azure resource<a id="_idIndexMarker984"/> allows you to monitor <a id="_idIndexMarker985"/>the performance of your applications. Especially for web applications, such as the real-time endpoint you are<a id="_idIndexMarker986"/> deploying, Application Insights allows you to monitor the request and response times, the failure rate of the endpoint, any potential exceptions raised in your code, and even log traces that you want to emit from your code base.</p>
			<p>In the <em class="italic">Understanding the model deployment options</em> section earlier, you created a <strong class="source-inline">score.py</strong> file that contained a couple of <strong class="source-inline">print</strong> statements. These messages were written inside the console of the endpoint and could be found either by calling the <strong class="source-inline">service.get_logs()</strong> method or navigating to the <strong class="bold">Deployment logs</strong> tab of the deployment, as shown in <em class="italic">Figure 12.10</em>:</p>
			<div>
				<div id="_idContainer281" class="IMG---Figure">
					<img src="Images/B16777_12_010.jpg" alt="Figure 12.10 – The model path and incoming raw_data logged in the console of the container instance&#13;&#10;" width="1152" height="1125"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 – The model path and incoming raw_data logged in the console of the container instance</p>
			<p>The problem<a id="_idIndexMarker987"/> with this approach is that the logs do not persist. If you redeploy the<a id="_idIndexMarker988"/> container instance, you will lose the logs. Moreover, if you have multiple models deployed, you will need a centralized place to be able to monitor all of them together. These are two of the many benefits that Application Insights brings to your solution.</p>
			<p>Go back to your <strong class="source-inline">chapter12.ipynb</strong> notebook to redeploy the ACI container and enable Application Insights for it. Inside a new cell, add the following code:</p>
			<p class="source-code">from azureml.core.webservice import AciWebservice</p>
			<p class="source-code">deployment_config = AciWebservice.deploy_configuration(</p>
			<p class="source-code">   cpu_cores=1, memory_gb=0.5, enable_app_insights= True)</p>
			<p class="source-code">service = Model.deploy(ws, "aci-loans", [model], inference_config, deployment_config)</p>
			<p class="source-code">service.wait_for_deployment(show_output=True) </p>
			<p>Notice that you are using the <strong class="source-inline">1</strong> CPU core and <strong class="source-inline">0.5</strong> GB RAM that was recommended in the <em class="italic">Profiling the model resource requirements</em> section. Additionally, note that you are enabling Application Insights in the deployment configuration by passing the <strong class="source-inline">enable_app_insights= True</strong> argument. If you had already deployed the service and you wanted to enable Application Insights for it, you could use the following code to update its configuration:</p>
			<p class="source-code">service.update(enable_app_insights=True)</p>
			<p>Let's send a couple<a id="_idIndexMarker989"/> of requests to the service to be able to better understand what <a id="_idIndexMarker990"/>Application Insights can do for you. Inside a new cell, add the following code:</p>
			<p class="source-code">import json</p>
			<p class="source-code">input_payload = json.dumps({'data': [[2000, 2, 45], [2000, 9, 45]]})</p>
			<p class="source-code">for x in range(10):</p>
			<p class="source-code">   print(service.run(input_payload))</p>
			<p>This code sends <em class="italic">10</em> identical requests to the service, one after the other, generating some artificial traffic that should be logged in Application Insights. The easiest way to find the URL that is pointing to the Azure portal and directly inside the Application Insights resource is to visit the endpoint's information page, as displayed in <em class="italic">Figure 12.11</em>:</p>
			<div>
				<div id="_idContainer282" class="IMG---Figure">
					<img src="Images/B16777_12_011.jpg" alt="Figure 12.11 – The Application Insights URL that is associated with your AzureML workspace&#13;&#10;" width="1247" height="1053"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 – The Application Insights URL that is associated with your AzureML workspace</p>
			<p>Note that this <strong class="bold">Application Insights url</strong> link is not specific to the <strong class="bold">aci-loans</strong> deployment. This link<a id="_idIndexMarker991"/> will be the same for all of your real-time endpoints, allowing you to <a id="_idIndexMarker992"/>centrally monitor all of your real-time endpoints. Clicking on that link will take you inside Application Insights, as shown in <em class="italic">Figure 12.12</em>:</p>
			<div>
				<div id="_idContainer283" class="IMG---Figure">
					<img src="Images/B16777_12_012.jpg" alt="Figure 12.12 – Application Insights showing the 10 requests that you sent with the last code&#13;&#10;" width="1650" height="816"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12 – Application Insights showing the 10 requests that you sent with the last code</p>
			<p>From this dashboard, you can click on the graphs and drill down to the signal details; or you can view all the traces<a id="_idIndexMarker993"/> that your application is writing inside the console. To view <a id="_idIndexMarker994"/>them, navigate to <strong class="bold">Monitoring</strong> | <strong class="bold">Logs</strong>, click on the <strong class="bold">traces</strong>, select a time range that you want to investigate, and click on the <strong class="bold">Run</strong> button. You should see all of the <strong class="bold">STDOUT</strong> messages appear in the results, and you can drill down into the details, as displayed in <em class="italic">Figure 12.13</em>:</p>
			<div>
				<div id="_idContainer284" class="IMG---Figure">
					<img src="Images/B16777_12_013.jpg" alt="Figure 12.13 – Reading all of the traces emitted by your model's real-time &#13;&#10;endpoint in Application Insights&#13;&#10;" width="1650" height="1504"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.13 – Reading all of the traces emitted by your model's real-time endpoint in Application Insights</p>
			<p>You can create<a id="_idIndexMarker995"/> complex <a id="_idIndexMarker996"/>queries in this <strong class="bold">Logs</strong> section using a powerful SQL-like language known as <strong class="bold">Kusto</strong>. You can even create<a id="_idIndexMarker997"/> automated<a id="_idIndexMarker998"/> alerts based on those queries, notifying you, for example, whenever you have had more than 100 loans rejected in the last 30 minutes.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Application Insights supports the logging of small payloads of up to 64 KB at a time. If you plan to log more than that, for example, a mini-batch input with more than 64 KB of data, you should consider working with the <strong class="source-inline">DataCollector</strong> class of the AzureML SDK. This class allows you to log data directly into a storage account; however, it is only available if you deploy in AKS.</p>
			<p>Before moving on to the next section, do not forget to delete the deployed service to prevent any accidental cost charges for the ACI service. You can delete the service from the <strong class="bold">Assets</strong> | <strong class="bold">Endpoints</strong> list in the studio experience or via the code using the following line:</p>
			<p class="source-code">service.delete()</p>
			<p>In this section, you<a id="_idIndexMarker999"/> learned how to monitor your real-time endpoint once you have it deployed in production. In <em class="italic">Figure 12.12</em>, you might have noticed<a id="_idIndexMarker1000"/> that there were a couple of <strong class="bold">Failed requests</strong> logged. You can drill down into those errors in Application Insights, or you can look at <em class="italic">Figure 12.10</em>, where you will see the logs complaining about a missing <strong class="source-inline">swagger</strong> file. In the next section, you will learn how to fix those failed requests and enable rich integration with third-party applications that want to consume the results of your model.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor179"/>Integrating with third-party applications</h2>
			<p>So far, you have deployed<a id="_idIndexMarker1001"/> a web service that accepts an array of arrays as input. This is a cryptic input that you need to explain to whoever wants to consume your real-time endpoint. In <a href="B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072"><em class="italic">Chapter 5</em></a>, <em class="italic">Letting the Machines Do the Model Training</em>, you read about the <strong class="source-inline">swagger</strong> file that could be used to generate code to automatically consume your endpoints. To produce<a id="_idIndexMarker1002"/> such a file, you can use the open source <strong class="source-inline">inference-schema</strong> package and decorate your code with metadata that will drive the generation of the <strong class="source-inline">swagger.json</strong> file.</p>
			<p>In order to make your model slightly easier to consume by third-party applications, you should accept the following payload:</p>
			<p class="source-code">{"data":[{"income": 2000, "credit_cards": 2, "age": 45}]}</p>
			<p>Here, you will <a id="_idIndexMarker1003"/>need to create a new version of the<a id="_idIndexMarker1004"/> scoring file. Instead of cloning and editing the existing scoring file, you can download the modified <strong class="source-inline">score_v2.py</strong> version directly from the GitHub page, as mentioned in the <em class="italic">Technical requirements</em> section. In the <strong class="bold">Notebooks</strong> section, duplicate the <strong class="bold">score.py</strong> file located in the <strong class="bold">script</strong> folder by right-clicking on it and selecting the <strong class="bold">Duplicate</strong> command, as shown in <em class="italic">Figure 12.14</em>: </p>
			<div>
				<div id="_idContainer285" class="IMG---Figure">
					<img src="Images/B16777_12_014.jpg" alt="Figure 12.14 – Creating the v2 file of the entry script&#13;&#10;" width="1023" height="517"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.14 – Creating the v2 file of the entry script</p>
			<p>Name the clone <strong class="source-inline">score_v2.py</strong>, and modify the code to look like the following code block:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import joblib</p>
			<p class="source-code">from inference_schema.schema_decorators import input_schema, output_schema</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType</p>
			<p>At the beginning of the script<a id="_idIndexMarker1005"/> file, you are importing additional helper classes, which will be used later in the code. Notice <a id="_idIndexMarker1006"/>that you will no longer need the <strong class="source-inline">json</strong> module:</p>
			<p class="source-code">def init():</p>
			<p class="source-code">    global model</p>
			<p class="source-code">    model_path = os.path.join(os.getenv(\</p>
			<p class="source-code">"AZUREML_MODEL_DIR"), "model/model.joblib")</p>
			<p class="source-code">    model = joblib.load(model_path)</p>
			<p>You won't modify the <strong class="source-inline">init</strong> method: </p>
			<p class="source-code">data_sample = pd.DataFrame(</p>
			<p class="source-code">    {</p>
			<p class="source-code">        "income": pd.Series([2000.0], dtype="float64"),</p>
			<p class="source-code">        "credit_cards": pd.Series([1], dtype="int"),</p>
			<p class="source-code">        "age": pd.Series([25], dtype="int")</p>
			<p class="source-code">    }</p>
			<p class="source-code">)</p>
			<p class="source-code">output_sample = np.array([0])</p>
			<p>In the preceding code block, you create a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> that will act as the sample for the objects contained in the incoming request's <strong class="source-inline">data</strong> attribute. This <strong class="source-inline">data_sample</strong> object has an <strong class="source-inline">income</strong> feature, which is <strong class="source-inline">float64</strong>, and the <strong class="source-inline">credit_cards</strong> and <strong class="source-inline">age</strong> features, which are integers. Similarly, for the output, you define <strong class="source-inline">output_sample</strong> as a NumPy array or numeric value. You can use the <strong class="source-inline">data_sample</strong> and <strong class="source-inline">output_sample</strong> objects inside the decorators of the following code block:</p>
			<p class="source-code">@input_schema("data", PandasParameterType(data_sample))</p>
			<p class="source-code">@output_schema(NumpyParameterType(output_sample))</p>
			<p class="source-code">def run(data):</p>
			<p class="source-code">    try:</p>
			<p class="source-code">        result = model.predict(data)</p>
			<p class="source-code">        return result.tolist()</p>
			<p class="source-code">    except Exception as e:</p>
			<p class="source-code">        error = str(e)</p>
			<p class="source-code">        return error</p>
			<p>Here, you use the <strong class="source-inline">data_sample</strong> object with the <strong class="source-inline">@input_schema</strong> decorator. Additionally, you use <strong class="source-inline">PandasParameterType</strong>, which indicates that the parameter named <strong class="bold">data</strong> will be a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> that follows the schema defined by the <strong class="source-inline">data_sample</strong> example. You use the <strong class="source-inline">@output_schema</strong> decorator to specify that your service returns <a id="_idIndexMarker1007"/>a NumPy array as an output, similar to <strong class="source-inline">output_sample</strong>. Once you have configured these schemas, you will notice that you do<a id="_idIndexMarker1008"/> not have to deserialize the incoming payload within the <strong class="source-inline">run</strong> method. The <strong class="source-inline">data</strong> object is an already deserialized <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong>.</p>
			<p>If you want to process binary files instead of tabular data, for instance, processing an image, you can use the <strong class="source-inline">@rawhttp</strong> directive, which will pass the raw HTTP request to your <strong class="source-inline">run</strong> method. Working with plain HTTP requests gives you greater flexibility, including setting the response headers; this is something required<a id="_idIndexMarker1009"/> when you configure security features such as <strong class="bold">Cross-Origin Resource Sharing</strong> (<strong class="bold">CORS</strong>). You can find resources to learn more about those advanced scenarios in the <em class="italic">Further reading</em> section of this chapter.</p>
			<p>Now that you have the code of the <strong class="source-inline">score_v2.py</strong> script file ready, you need to publish the real-time endpoint. To create a real-time endpoint for the new scoring function, add the following code inside a cell within your notebook:</p>
			<p class="source-code">from azureml.core.model import InferenceConfig</p>
			<p class="source-code">from azureml.core.webservice import AciWebservice</p>
			<p class="source-code">myEnv.Python.conda_dependencies.add_pip_package("inference_schema[pandas-support]&gt;=1.1.0")</p>
			<p class="source-code">inference_config = InferenceConfig(source_directory= "./script", entry_script='score_v2.py', environment=myEnv)</p>
			<p class="source-code">deployment_config = AciWebservice.deploy_configuration( cpu_cores=1, memory_gb=0.5)</p>
			<p class="source-code">service = Model.deploy(ws, "aci-loans", [model], inference_config, deployment_config)</p>
			<p class="source-code">service.wait_for_deployment(show_output=True)</p>
			<p>In the preceding code, you<a id="_idIndexMarker1010"/> append the <strong class="source-inline">inference_schema</strong> pip packages in the <strong class="source-inline">myEnv</strong> dependencies, which you defined in the <em class="italic">Understanding the model deployment options</em> section earlier. Note that you are installing that package with the <strong class="source-inline">pandas-support</strong> extra, which will include the <strong class="source-inline">pandas</strong> package. The <strong class="source-inline">numpy</strong> dependency that your <strong class="source-inline">score_v2.py</strong> file depends upon will automatically be installed by pip since it is a dependency of the <strong class="source-inline">pandas</strong> package.</p>
			<p>Following this, you specify<a id="_idIndexMarker1011"/> that you are using the <strong class="source-inline">score_v2.py</strong> entry script and deploy the new service. The new service will have a <strong class="source-inline">swagger.json</strong> file available for third-party applications such as Power BI to read and automatically understand how to invoke your model. You can get the Swagger URI to point to that file on the endpoint's page, as shown in <em class="italic">Figure 12.11</em>. On the endpoint's page, you should notice that the <strong class="bold">Test</strong> tab has been enhanced to guide you on what fields you need to provide to invoke the model. On the code side, you can invoke the model with the following payloads:</p>
			<p class="source-code">import json</p>
			<p class="source-code">service = ws.webservices['aci-loans']</p>
			<p class="source-code">input_payload = json.dumps({"data":[</p>
			<p class="source-code">    {"income": 2000,"credit_cards": 2,"age": 45},</p>
			<p class="source-code">    {"income": 2000, "credit_cards": 9,"age": 45}</p>
			<p class="source-code">]})</p>
			<p class="source-code">print(service.run(input_payload))</p>
			<p class="source-code">input_payload = json.dumps({'data': [</p>
			<p class="source-code">    [2000, 2, 45], [2000, 9, 45]</p>
			<p class="source-code">]})</p>
			<p class="source-code">print(service.run(input_payload))</p>
			<p>Before moving on to the next section, make sure you delete the ACI service you just deployed by using the following code:</p>
			<p class="source-code">service.delete()</p>
			<p>So far, you have been <a id="_idIndexMarker1012"/>deploying real-time inference endpoints that could fulfill ad hoc inference requests through a REST API. In the next section, you will learn how to deploy a batch inference pipeline that can process big data in parallel using <strong class="source-inline">ParallelRunStep</strong>.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor180"/>Creating a batch inference pipeline</h1>
			<p>In <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>, you learned how to create pipelines that orchestrate multiple steps. These pipelines can be invoked using a REST API, similar to the real-time endpoint<a id="_idIndexMarker1013"/> that you created in the previous section. One key difference is that in the real-time endpoint, the infrastructure is constantly on, waiting for a request to arrive, while in the published pipelines, the cluster will spin up only after the pipeline has been triggered.</p>
			<p>You could use these pipelines to orchestrate batch inference on top of data residing in a dataset. For example, let's imagine that you just trained the <strong class="source-inline">loans</strong> model you have been using in this chapter. You want to run the model against all of the pending loan requests and store the results; this is so that you can implement an email campaign targeting the customers that might get their loan rejected. The easiest approach is to create a single <strong class="source-inline">PythonScriptStep</strong> that will process each record sequentially and store the results in the output folder, as you learned in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>. Instead of doing that, you could break the dataset into multiple batches and then have them processed, in parallel, in multiple processes running inside each node of your cluster, as displayed in <em class="italic">Figure 12.15</em>:</p>
			<div>
				<div id="_idContainer286" class="IMG---Figure">
					<img src="Images/B16777_12_015.jpg" alt="Figure 12.15 – Parallel processing big datasets by splitting them into smaller &#13;&#10;batches and processing them in parallel&#13;&#10;" width="1650" height="759"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.15 – Parallel processing big datasets by splitting them into smaller batches and processing them in parallel</p>
			<p>In this section, you will create a batch processing pipeline that will be making inferences using the <strong class="source-inline">chapter12-loans</strong> model you trained in this chapter. You already have a dataset named <strong class="source-inline">loans</strong>, but it is too small to show how <strong class="source-inline">ParallelRunStep</strong> can help you speed up by parallelizing<a id="_idIndexMarker1014"/> the inferences. You will generate a new dataset that will be 1,024 times bigger by copying the same DataFrame repeatedly. Then, you will create a pipeline similar to the one that you created in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>. This time, you will use the <strong class="source-inline">ParallelRunConfig</strong> and the <strong class="source-inline">ParallelRunStep</strong> classes to parallelize the processing of the dataset. The configuration class requires an entry script, similar to the entry script that you saw in the previous section. Additionally, you need to define the following two methods:</p>
			<ul>
				<li><strong class="source-inline">init()</strong>: This method loads the model<a id="_idIndexMarker1015"/> and prepares the process for the incoming batches. No output is expected from this method.</li>
				<li><strong class="source-inline">run(mini_batch)</strong>: This method does the actual data processing. This method will be invoked<a id="_idIndexMarker1016"/> multiple times, passing a different <strong class="source-inline">mini_batch</strong> parameter every time. You have to return an array containing one row for each item you managed to process within this function as an output. For example, if the <strong class="source-inline">mini_batch</strong> parameter had 100 rows and you return an array of 98 items, you will indicate that you failed to process 2 of those records. The <strong class="source-inline">mini_batch</strong> parameter could either be a <strong class="source-inline">pandas</strong> <strong class="source-inline">DataFrame</strong> if you<a id="_idIndexMarker1017"/> are processing <strong class="source-inline">TabularDataset</strong> or an array that contains the file paths you need to process if you are processing a <strong class="source-inline">FileDataset</strong>.</li>
			</ul>
			<p>Navigate to your <strong class="source-inline">chapter12.ipynb</strong> notebook and perform the following steps:</p>
			<ol>
				<li value="1">Start by getting<a id="_idIndexMarker1018"/> a reference to the workspace, the dataset, and the compute cluster you are going to use for your pipeline:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">loans_ds = ws.datasets['loans']</p><p class="source-code">compute_target = ws.compute_targets['cpu-sm-cluster']</p><p>The code should be self-explanatory, as you used it in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>.</p></li>
				<li>Create a new, bigger dataset based on the <strong class="source-inline">loans</strong> dataset:<p class="source-code">from azureml.core import Dataset</p><p class="source-code">loans_df = loans_ds.drop_columns('approved_loan') \  </p><p class="source-code">                   .to_pandas_dataframe()</p><p class="source-code">for x in range(10):</p><p class="source-code">    loans_df = loans_df.append(loans_df)</p><p class="source-code">dstore = ws.get_default_datastore()</p><p class="source-code">pending_loans_ds =\</p><p class="source-code">Dataset.Tabular.register_pandas_dataframe(</p><p class="source-code">    dataframe=loans_df,</p><p class="source-code">    target=(dstore,"/samples/pending-loans"),</p><p class="source-code">    name="pending-loans",</p><p class="source-code">    description="Pending loans to be processed")</p><p>In the preceding code, you are loading the <strong class="source-inline">loans</strong> <strong class="source-inline">DataFrame</strong> into memory without the <strong class="source-inline">approved_loan</strong> column. This dataset only contains 500 rows. Then, you append the dataset to itself 10 times. This will create a much bigger dataset containing 512,000 rows, which you register as <strong class="source-inline">pending-loans</strong>.</p></li>
				<li>Now, it's time to create<a id="_idIndexMarker1019"/> the script that will be processing this dataset. In the <strong class="source-inline">chapter12</strong> folder, add a <strong class="source-inline">pipeline_step</strong> folder and then add a <strong class="source-inline">tabular_batch.py</strong> file with the following contents:<p class="source-code">from azureml.core import Model</p><p class="source-code">import joblib</p><p class="source-code">def init():</p><p class="source-code">    global model</p><p class="source-code">    model_path = Model.get_model_path("chapter12-loans")</p><p class="source-code">    model = joblib.load(model_path)</p><p class="source-code">def run(mini_batch):</p><p class="source-code">    print(mini_batch.info())</p><p class="source-code">    mini_batch["approved"] = model.predict(mini_batch)</p><p class="source-code">    return mini_batch.values.tolist()</p><p>This script has two methods, as metioned earlier. In the <strong class="source-inline">init</strong> method, you use the <strong class="source-inline">get_model_path</strong> method of the <strong class="source-inline">Model</strong> class to get the location of the model that you have been using so far. From the script's perspective, the model will reside in a folder on the same computer where the script is running. Then, you use <strong class="source-inline">joblib</strong> to load the model inside a <strong class="source-inline">global</strong> variable named <strong class="source-inline">model</strong>. In the <strong class="source-inline">run</strong> method, you print the size of the incoming DataFrame, and then you create a new column, named <em class="italic">approved</em>, where you store all of the model inferences. You return a list containing a four-element array for each row you processed, similar to the following records:</p><p class="source-code">[7298, 2, 35, 1]</p><p class="source-code">[4698, 7, 70, 0]</p><p>If you were to process <strong class="source-inline">FileDataset</strong> instead of the <strong class="source-inline">TabularDataset</strong> that you are processing<a id="_idIndexMarker1020"/> in this section, the corresponding <strong class="source-inline">file_batch.py</strong> file would look like the following:</p><p class="source-code">def init():</p><p class="source-code">    print('Load model here')</p><p class="source-code">def run(mini_batch):</p><p class="source-code">    output = []</p><p class="source-code">    for file_path in mini_batch:</p><p class="source-code">        output.append([file_path, 0])</p><p class="source-code">    return output</p><p>You load your model, as usual, inside the <strong class="source-inline">init</strong> method, for example, a neural network that will implement image classification. In the <strong class="source-inline">run</strong> method, the <strong class="source-inline">mini_batch</strong> parameter is an array containing the file paths of the files you need to process. You can loop through those files and make the inferences using your model. As an output, you return the filename and result of the model, as demonstrated in the following example:</p><p class="source-code">['/path/sample_cat.jpg', 0]</p><p class="source-code">['/path/sample_dog.jpg', 1]</p><p>You will observe, in <em class="italic">Step 5</em>, that those results will be aggregated in a single file defined in <strong class="source-inline">ParallelRunConfig</strong>. </p></li>
				<li>You will need to create an environment<a id="_idIndexMarker1021"/> to execute your pipeline step. Add the following code inside a cell:<p class="source-code">from azureml.core import Environment</p><p class="source-code">from azureml.core.conda_dependencies import CondaDependencies </p><p class="source-code">import sklearn</p><p class="source-code">pEnv= Environment(name="sklearn-parallel")</p><p class="source-code">pEnv.Python.conda_dependencies = CondaDependencies()</p><p class="source-code">pEnv.Python.conda_dependencies.add_conda_package(f"scikit-learn=={<a id="_idTextAnchor181"/>sklearn.__version__}")</p><p class="source-code">pEnv.Python.conda_dependencies.add_pip_package("azureml-core")</p><p class="source-code">pEnv.Python.conda_dependencies.add_pip_package("azureml-dataset-runtime[pandas,fuse]")</p><p>You need to install the <strong class="source-inline">scikit-learn</strong> conda package, just as you have been doing so far. For <strong class="source-inline">ParallelRunConfig</strong> to work, you will need to include the <strong class="source-inline">azureml-core</strong> and <strong class="source-inline">azureml-dataset-runtime[pandas,fuse]</strong> <strong class="source-inline">pip</strong> packages.</p></li>
				<li>Next, create the <strong class="source-inline">ParallelRunConfig</strong> class that configures how the run will split the workload and what script to use for data processing. Add the following code inside a new notebook cell:<p class="source-code">from azureml.pipeline.steps import ParallelRunConfig</p><p class="source-code">parallel_ru<a id="_idTextAnchor182"/>n_config = ParallelRunConfig(</p><p class="source-code">    source_directory='pipeline_step',</p><p class="source-code">    entry_script='tabular_batch.py',</p><p class="source-code">    mini_batch_size='100Kb',</p><p class="source-code">    error_threshold=-1,</p><p class="source-code">    output_action='append_row',</p><p class="source-code">    append_row_file_name="loans_outputs.txt",</p><p class="source-code">    environment=pEnv,</p><p class="source-code">    compute_target=compute_target, </p><p class="source-code">    node_count=1,</p><p class="source-code">    process_count_per_node=2</p><p class="source-code">)</p><p>Here, you will run the <strong class="source-inline">tabular_batch.py</strong> script located inside the <strong class="source-inline">pipeline_step</strong> folder. You are going to split the dataset into smaller batches of, approximately, 100 KB. If you were processing a <strong class="source-inline">FileDataset</strong>, you would need to specify the number of files to put in each batch. Here, <strong class="source-inline">error_threshold</strong> specifies the number of record or file failures that should be ignored while processing the data. <strong class="source-inline">-1</strong> means you are okay with any number of processing errors. The <strong class="source-inline">output_action</strong> parameter accepts either <strong class="source-inline">append_row</strong> values or <strong class="source-inline">summary_only</strong> values. Using the <strong class="source-inline">append_row</strong> value, you can request all outputs from the <strong class="source-inline">run</strong> method invocations to be appended inside a single output file, the name of which is <strong class="source-inline">parallel_run_step.txt</strong>, unless you override it via the <strong class="source-inline">append_row_file_name</strong> parameter, as demonstrated in the preceding example. The order<a id="_idIndexMarker1022"/> of the records in that file is not guaranteed since the records are processed in parallel. Usually, you would return the customer ID, or the loan application ID, and the model's inference. Using that ID, you could link back the original record with the model's prediction. In the current example, we don't have any ID; therefore, we return the entire row, just as we did in the <strong class="source-inline">tabular_batch.py</strong> script in <em class="italic">Step 3</em>.</p><p>Following this, you specify the environment and the cluster where this pipeline step will be executed. In the end, you specify that this pipeline step will run in a single node, and it will spin up <em class="italic">two</em> processes per participating node. If you used two nodes, you would have four processes running in parallel. In the current example, two parallel processes are enough to handle the processing in only a couple of minutes.</p><p>If you have a heavy processing script that requires more than 60 seconds to process the <strong class="source-inline">mini_batch_size</strong> parameter you specified, you can increase the timeout value by setting the <strong class="source-inline">run_invocation_timeout</strong> parameter.</p></li>
				<li>As a next step, you will define<a id="_idIndexMarker1023"/> the output location of <strong class="source-inline">append_row_file_name</strong> that you specified earlier:<p class="source-code">from azureml.data import OutputFileDatasetConfig</p><p class="source-code">datastore = ws.get_default_datastore()</p><p class="source-code">step_output = OutputFileDatasetConfig(</p><p class="source-code">    name= "results_store",</p><p class="source-code">    destination=(datastore, '/inferences/loans/'))</p><p>You will store that aggregation file in the default datastore, underneath the <strong class="source-inline">/inferences/loans/</strong> folder.</p></li>
				<li>Now it's time to create the first and only step of the pipeline, that is, <strong class="source-inline">ParallelRunStep</strong>:<p class="source-code">from azureml.pipeline.steps import ParallelRunStep</p><p class="source-code">parallel_step = ParallelRunStep(</p><p class="source-code">    name='chapter12-parallel-loans',</p><p class="source-code">    inputs=[pending_loans_ds.as_named_input('loans')],</p><p class="source-code">    output=step_output,</p><p class="source-code">    parallel_run_config=parallel_run_config,</p><p class="source-code">    allow_reuse=False</p><p class="source-code">)</p><p>Name that step <strong class="source-inline">chapter12-parallel-loans</strong> and pass the <strong class="source-inline">pending_loans_ds</strong> dataset that you registered in <em class="italic">Step 2</em> earlier. The output is stored in <strong class="source-inline">OutputFileDatasetConfig</strong>, which you created in <em class="italic">Step 6</em>. Specify that this step should not be reused (<strong class="source-inline">allow_reuse</strong>); this allows you to trigger the pipeline multiple times to always get the latest data in the dataset along with the latest registered model.</p></li>
				<li>Create and execute<a id="_idIndexMarker1024"/> a pipeline using the following code:<p class="source-code">from azureml.core import Experiment</p><p class="source-code">from azureml.pipeline.core import Pipeline</p><p class="source-code">pipeline = Pipeline(workspace=ws, steps=[parallel_step])</p><p class="source-code">pipeline_run = Experiment(ws, 'chapter12-parallel-run').submit(pipeline)</p></li>
				<li>You can watch the execution logs by using the <strong class="source-inline">RunDetails</strong> widget with the following code:<p class="source-code">from azureml.widgets import RunDetails</p><p class="source-code">RunDetails(pipeline_run).show()</p><p>Alternatively, you can wait for the execution to complete with the following code:</p><p class="source-code">pipeline_run.wait_for_completion(show_output=True)</p></li>
				<li>From that point on, you can publish and even schedule the pipeline, as discussed in <a href="B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Pipelines</em>.</li>
			</ol>
			<p>You can visit the pipeline in AzureML studio and observe the outputs and the logs it produced, as shown in <em class="italic">Figure 12.16</em>. Notice that you will find a single node and two processes. Each process has multiple <strong class="source-inline">run</strong> method<a id="_idIndexMarker1025"/> invocations. Each time the <strong class="source-inline">run</strong> method was invoked, a DataFrame that required 117.3 KB in memory was passed in, which is close to the 100 KB that you requested in <em class="italic">Step 5</em> earlier:</p>
			<div>
				<div id="_idContainer287" class="IMG---Figure">
					<img src="Images/B16777_12_016.jpg" alt="Figure 12.16 – The logs from the parallel execution showing the information of the mini_batch DataFrame&#13;&#10;" width="1638" height="1354"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.16 – The logs from the parallel execution showing the information of the mini_batch DataFrame</p>
			<p>In this section, you learned how to create<a id="_idIndexMarker1026"/> a batch processing pipeline that can process a significant amount of data in parallel. This concludes the operationalization options that you need to be aware of for the exam, covering both the real-time and batch modes. </p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor183"/>Summary</h1>
			<p>In this chapter, you explored various ways in which to use the machine learning models that you have been training in this book. You can make either real-time inferences or batch process a large number of records in a cost-effective manner. You started by registering the model you would use for inferences. From there, you can either deploy a real-time endpoint in ACI for testing or in AKS for production workloads that require high availability and automatic scaling. You explored how to profile your model to determine the recommended container size to host the real-time endpoint. Following this, you discovered Application Insights, which allows you to monitor production endpoints and identify potential production issues. Through Application Insights, you noticed that the real-time endpoint you produced wasn't exposing a <strong class="source-inline">swagger.json</strong> file that was needed by third-party applications, such as Power BI, to automatically consume your endpoint. You modified the scoring function to include metadata regarding your model's inputs and outputs, thus completing the real-time inference section of this chapter. </p>
			<p>Then, you moved on to the batch inferencing side, where you authored a pipeline that can process half a million records, in parallel, in only a few minutes. Combining this parallelization with low-priority computes, you can achieve great cost savings when inferencing larger data volumes.</p>
			<p>Congratulations! You have completed your journey of discovering the basic capabilities of the AzureML workspace. Now you can now conduct machine learning experiments in the workspace, and you can operationalize the resulting models using the option that suits the business problem that you are trying to solve. With this knowledge, you should be able to pass the <em class="italic">DP-100</em> exam, <em class="italic">Designing and Implementing a Data Science Solution on Azure</em>, with flying colors.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor184"/>Questions</h1>
			<p>In each chapter, you will find a number of questions to validate your understanding of the topics that have been discussed:</p>
			<ol>
				<li value="1">You want to deploy a real-time endpoint that will handle transactions from a live betting website. The traffic from this website will have spikes during games and will be very low during the night. Which of the following compute targets should you use?<p>a. ACI</p><p>b. A compute instance</p><p>c. A compute cluster</p><p>d. AKS</p></li>
				<li>You want to monitor a real-time endpoint deployed in AKS and determine the average response time of the service. Which monitoring solution should you use?<p>a. ACI</p><p>b. Azure Container Registry</p><p>c. Application Insights</p></li>
				<li>You have a computer vision model, and you want to process 100 images in parallel. You author a pipeline with a parallel step. You want to process 10 images at a time. Which of the following <strong class="source-inline">ParallelRunConfig</strong> parameters should you set?<p>a. <strong class="source-inline">mini_batch_size=10</strong></p><p>b. <strong class="source-inline">error_threshold=10</strong></p><p>c. <strong class="source-inline">node_count=10</strong></p><p>d. <strong class="source-inline">process_count_per_node=10</strong></p></li>
			</ol>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor185"/>Further reading</h1>
			<p>This section offers a list of helpful web resources to help you augment your knowledge of the AzureML SDK and the various code snippets used in this chapter:</p>
			<ul>
				<li>Model persistence guidance from scikit-learn: <a href="https://scikit-learn.org/stable/modules/model_persistence.html">https://scikit-learn.org/stable/modules/model_persistence.html</a></li>
				<li>Testing a REST API using Postman: <a href="https://www.postman.com/product/api-client/">https://www.postman.com/product/api-client/</a></li>
				<li>The <strong class="bold">curl</strong> command-line tool to make web requests: <a href="https://curl.se/%0D">https://curl.se/</a></li>
				<li>Monitor Python applications using OpenCensus: <a href="https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python">https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python</a></li>
				<li>How to use the inference server to test your entry scripts locally: <a href="https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http">https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http</a></li>
				<li>Packaging the model inside an autonomous Docker container: <a href="https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models">https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models</a></li>
				<li>The ONNX machine learning format used to store models that can be loaded in multiple platforms: <a href="https://docs.microsoft.com/azure/machine-learning/concept-onnx">https://docs.microsoft.com/azure/machine-learning/concept-onnx</a> </li>
				<li>An introduction to Application Insights: <a href="https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview">https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview</a></li>
				<li>An introduction to the Kusto Query Language: <a href="https://docs.microsoft.com/azure/data-explorer/kusto/concepts/">https://docs.microsoft.com/azure/data-explorer/kusto/concepts/</a></li>
				<li>The advanced real-time endpoint entry script authoring guide: <a href="https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script">https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script</a></li>
				<li>Integrating AzureML models in Power BI: <a href="https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi">https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi</a> </li>
				<li>Using the <strong class="source-inline">ParallelRunStep</strong> class to train hundreds of models: <a href="https://github.com/microsoft/solution-accelerator-many-models">https://github.com/microsoft/solution-accelerator-many-models</a></li>
			</ul>
		</div>
	</div></body></html>