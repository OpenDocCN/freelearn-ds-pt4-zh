- en: '*Chapter 12*: Operationalizing Models with Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you are going to learn how to operationalize the machine learning
    models you have been training, in this book, so far. You will explore two approaches:
    exposing a real-time endpoint by hosting a REST API that you can use to make inferences
    and expanding your pipeline authoring knowledge to make inferences on top of big
    data, in parallel, efficiently. You will begin by registering a model in the workspace
    to keep track of the artifact. Then, you will publish a REST API; this is something
    that will allow your model to integrate with third-party applications such as
    **Power BI**. Following this, you will author a pipeline to process half a million
    records within a couple of minutes in a very cost-effective manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the various deployment options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering models in the workspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying real-time endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a batch inference pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require access to an Azure subscription. Within that subscription,
    you will need a `packt-azureml-rg`. You will need to have either a `Contributor`
    or `Owner` `packt-learning-mlw`. These resources should be already available to
    you if you followed the instructions in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you will require a basic understanding of the **Python** language.
    The code snippets in this chapter target Python version 3.6 or later. You should
    also be familiar with working with notebooks within AzureML studio; this is something
    that was covered in [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102),
    *The AzureML Python SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes you have registered the **loans** dataset that you generated
    in [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147), *Understanding
    Model Results*. It also assumes that you have created a compute cluster, named
    **cpu-sm-cluster**, as described in the *Working with compute targets* section
    of [*Chapter 7*](B16777_07_Final_VK_ePub.xhtml#_idTextAnchor102), *The AzureML
    Python SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'AzureML is constantly being updated. If you face any issues with the code samples
    in this book, try upgrading your AzureML SDK by adding the following code into
    a new notebook cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '`!pip install --upgrade azureml-core azureml-sdk[notebooks]`'
  prefs: []
  type: TYPE_NORMAL
- en: Then, restart the Jupyter kernel, as you learned in the *Training a loans approval
    model* section of [*Chapter 10*](B16777_10_Final_VK_ePub.xhtml#_idTextAnchor147),
    *Understanding Model Results*. Additionally, try downloading the latest version
    of the notebooks from the GitHub page of this book. If the problem persists, feel
    free to open an issue on this book's GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all of the notebooks and code snippets for this chapter on GitHub
    at [http://bit.ly/dp100-ch12](http://bit.ly/dp100-ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the various deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have been working with Python code since [*Chapter 8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117),
    *Experimenting with Python Code*. So far, you have trained various models, evaluated
    them based on metrics, and saved the trained model using the `dump` method of
    the **joblib** library. The AzureML workspace allows you to store and version
    those artifacts by registering them in the model registry that we discussed in
    [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072), *Letting the Machines
    Do the Model Training*. Registering the model allows you to version both the saved
    model and the metadata regarding the specific model, such as its performance according
    to various metrics. You will learn how to register models from the SDK in the
    *Registering models in the workspace* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been registered, you have to decide how you want to operationalize
    the model, either by deploying a real-time endpoint or by creating a batch process,
    as displayed in *Figure 12.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – A path from training to operationalization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – A path from training to operationalization
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main categories in terms of how a model processes incoming data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`predict_proba` method of the classifier you trained. You will read more about
    this scenario in the *Deploying real-time endpoints* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch inferences**: In [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, you trained an AutoML model to churn
    customer predictions. The model was trained using features such as the consumer''s
    activity over the last 6 to 12 months. Let''s suppose that you wanted to evaluate
    all of your customers and drive a marketing campaign for those who are likely
    to churn. You would have to run a once-off process that reads all of your customer
    information, calculates the required features, and then invokes the model for
    each of them to produce a prediction. The result can be stored in a CSV file to
    be consumed by the marketing department. In this approach, you only need the model
    for a short period of time, that is, only while making the predictions. You do
    not require a real-time endpoint, such as the one you deployed in [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, as you do not need the model to
    make ad hoc inferences. You can read more about this scenario in the *Creating
    a batch inference pipeline* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All models can be used in either real-time or batch inferences. It is up to
    you to decide whether you require ad hoc model inferences or a scheduled process
    that produces and stores the inference results. Operationalizing models in batch
    mode tends to be more cost-effective, as you can utilize low-priority compute
    clusters to perform inferences. In that scenario, you do not need to pay to have
    a real-time endpoint infrastructure waiting to make live inferences.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will start the path to operationalization by training
    and registering the model that you will be using throughout the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Registering models in the workspace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Registering a model allows you to keep different versions of the trained models.
    Each model version has artifacts and metadata. Among the metadata, you can keep
    references to experiment with runs and datasets. This allows you to track the
    lineage between the data used to train a model, the run ID that trained the model,
    and the actual model artifacts themselves, as displayed in *Figure 12.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Building the lineage from the training dataset all the way
    to the registered model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Building the lineage from the training dataset all the way to
    the registered model
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will train a model and register it in your AzureML workspace.
    Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Notebooks** section of your AzureML studio web interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder, named `chapter12`, and then create a notebook named `chapter12.ipynb`,
    as shown in *Figure 12.3*:![Figure 12.3 – Adding the chapter12 notebook to your
    working files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16777_12_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.3 – Adding the chapter12 notebook to your working files
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add and execute the following code snippets in separate notebook cells. You
    will start by getting a reference to the workspace resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you get a reference to a workspace, that is, the `chapter-12-train`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the dataset into training and validation using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code splits the dataset into 80% training data and 20% validation data.
    The `seed` argument initializes the internal random state of the `random_split`
    method, allowing you to hardcode the data split and generate the same `training_data`
    and `validation_data` every time you invoke this code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, `X_train` is a `pandas` `DataFrame` that contains the `income`, `credit_cards`,
    and `age` features (that is, all of the columns besides `approved_loan`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In comparison, `y_train` contains the values you want to predict. First, you
    load a `pandas` `DataFrame` that only contains the `approved_loan` column. Then,
    you convert that DataFrame into a `values` attribute. This array has a single
    element array for each row. For example, *[[0],[1]]* represents two records: a
    not-approved loan with a value of *0* and an approved one with a value of *1*.
    Following this, you call the `ravel` method to flatten the array, which converts
    the given example into *[0, 1]*. Although you could have used the `pandas` `DataFrame`
    directly to train the model, a warning message will inform you that an automatic
    convention has occurred, prompting you to use the `ravel` method that you observed
    in this cell.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The same process repeats for the `X_validate` DataFrame and the `y_validate`
    array that will be used to evaluate the model's performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train a model and log the achieved accuracy using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, you start with a run in the experiment, as defined in *Step 3*. You will
    use this run to register the metrics, logs, and artifacts of the model training
    process. Then, you train a `LogisticRegression` model, and you use the `accuracy_score`
    function to calculate the accuracy of the trained model. Following this, you print
    the calculated accuracy and log it as a metric in the run. In the end, you `complete`
    the run to finalize its execution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have a trained model referenced by the `sk_model` variable, you
    are going to save it using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, you create a folder named `model`. The name of that folder is not important.
    In that folder, you `dump` the trained model using the `joblib` library. The model
    is stored in a file named `model.joblib`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The`.joblib` filename extension is not standard, and you can use whatever you
    like as long as you are consistent. Some people use the `.pkl` filename extension
    instead, which was used in the past because we were serializing Python object
    structures using Python's built-in `pickle` module. Nowadays, the `joblib` library
    is the recommended way that is proposed by **scikit-learn** because it is more
    efficient in serializing large NumPy arrays, which is very common with the trained
    models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have the artifacts ready, you can register the model using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the first line, you import the `__version__` variable of the `sklearn` package,
    which is a string showing the version currently loaded in your environment. Then,
    you create an alias for that variable (using the `as` statement), and you reference
    it inside your code as `sk_version`. This is the version of the `sklearn` library
    that you used to train the model. Additionally, you import the `Model` class from
    the AzureML SDK to use it as a reference in the following lines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After importing your references, you upload the contents of the local `./model`
    folder, which you created in *Step 6*, to the run's outputs, underneath a folder
    named `model`. This allows AzureML to have access to the artifacts that you are
    about to register; otherwise, you will receive an `ModelPathNotFoundException`
    error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Having all of the prerequisites ready, you can register the model. The model
    will be named `chapter12-loans` (the `model_name` argument) using the artifacts
    that just got uploaded in the `model` folder (the `model_path` argument) of the
    run's outputs. You specify the accuracy as both a tag (the `tags` argument) and
    a property (the `properties` argument) of that model. You indicate that you used
    the `SCIKITLEARN` framework (the `model_framework` argument) to train the model,
    and you specify which version of the framework you used (the `model_framework_version`
    argument). In the last line, you specify that you used the `loans_ds` dataset
    as a `training` dataset (the `datasets` argument).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you try to rerun the same cell, a *Resource Conflict* error will occur because
    you cannot override files that already exist in the run's outputs folder. If you
    comment out the `upload_folder` line by using `#` as a line prefix and rerun the
    cell, you will register a new version of the same model, using the artifacts that
    already exist in the specific run.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to `Model.Framework.SCIKITLEARN`). This type of deployment is considered
    a no-code deployment, which is a capability that AzureML offers for supported
    frameworks. Otherwise, you need to specify a scoring file; this is something that
    we will cover in the *Deploying real-time endpoints* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to register a pretrained model that you downloaded from the internet,
    you will not have a `Run` object to call the `register_model` method. You can
    use the `register` method of the `Model` class, as demonstrated in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you register, in your AzureML workspace (the `ws` variable),
    the artifacts that are located inside the *local* `model` folder (the `model_path`
    argument) as a model named `chapter12-pre-trained-loans` (the `model_name` argument).
    This is a model trained using version `0.22.2.post1` (the `model_framework_version`
    argument) of the `sklearn` library (the `model_framework_version` argument). Additionally,
    its accuracy, which is `0.828`, is stored as a model property.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you had a process to train new models, such as the scheduled pipeline that
    you created in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*, you will have to verify whether the newly trained model
    has better metrics than the one already registered. Then, if it is better, proceed
    with the registration of the model. To do that, you can use code similar to the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you train a `RidgeClassifier`-based model that uses `chapter12-loans`,
    which you registered in *Step 7*. The `registered_model` variable has the same
    reference as the `model` variable you got in *Step 7*; only, this time, you create
    that reference using the `Model` class and not by registering a model. From that
    model, you read the `version` attribute and the `accuracy` property. You could
    retrieve the accuracy from the `tags` dictionary instead of the `properties` dictionary
    of the model. You convert the accuracy value into a float because tags and properties
    store their values as strings. Following this, you then compare the new model's
    accuracy to the one that has already been registered (which is stored in the `r_acc`
    variable). If the new model is better than the registered one, you print a message.
    In this case, you repeat *Step 6* and *Step 7* to store the model and then register
    the new, improved version of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To register a new model version, you just need to register the new model with
    the same name. By registering a new model with the same name, AzureML will automatically
    create a new version for you.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Optionally, as a last step, delete the locally stored model using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code deletes the `model` folder that you created in *Step 6*, including
    the serialized model you no longer need. The `ignore_errors` parameter allows
    you to run this cell even if the folder doesn't exist without raising any errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, you trained a model in your notebook within the Jupyter kernel.
    Then, you registered the model inside your workspace. You could have used the
    same registration code in the `train_model.py` script, which you authored in *Step
    11* of the *Authoring a pipeline* section of [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*, to register the `run=Run.get_context()` method, and
    then you would need to upload the serialized model and register the model, as
    you did in *Step 7*. As an additional activity, try to modify the `train_model.py`
    script and `chapter11.ipynb` to create a pipeline that registers the model that
    is being trained within the pipeline. A potential solution to this activity is
    available in the `train_model_and_register.py` script. This can be found in the
    `step02` folder of the GitHub repository at [http://bit.ly/dp100-ch11](http://bit.ly/dp100-ch11).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will start operationalizing the model that you registered
    in this section by deploying it as a web service that will serve real-time inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying real-time endpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's imagine that you have an e-banking solution that has a process for customers
    to request loans. You want to properly set the expectations of the customer and
    prepare them for potential rejection. When the customer submits their loan application
    form, you want to invoke the model you registered in the *Registering models in
    the workspace* section, that is, the model named **chapter12-loans**, and pass
    in the information that the customer filled out on the application form. If the
    model predicts that the loan will not be approved, a message will appear on the
    confirmation page of the loan request, preparing the customer for the potential
    rejection of the loan request.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.5* shows an oversimplified architecture to depict the flow of requests
    that start from the customer to the real-time endpoint of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – An oversimplified e-banking architecture showing the flow'
  prefs: []
  type: TYPE_NORMAL
- en: of requests from the customer to the model
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – An oversimplified e-banking architecture showing the flow of requests
    from the customer to the model
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to deploy a model is via the no-code deployment approach that
    AzureML offers for specific machine learning frameworks, including `sklearn` library
    that you used in the previous section. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the `chapter12.ipynb` notebook, and add the following code to get a reference
    to the last version of the **chapter12-loans** model that you created in the previous
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To deploy a real-time endpoint, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code deploys a new real-time endpoint service, named `no-code-loans`, and
    then waits for the deployment to complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get the scoring URI for the newly deployed endpoint, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is a URL in the format of [http://guid.region.azurecontainer.io/score](http://guid.region.azurecontainer.io/score),
    which accepts **POST** requests with a **JSON** payload, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This payload will trigger an inference request for a customer with a monthly
    income of 2,000, who has 2 credit cards and is 45 years old. You can use tools
    such as **Postman** or **curl** to craft such an HTTP request and invoke the endpoint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instead of making an HTTP request using a tool such as `curl`, you can use
    the `no_code_service` reference and invoke the `run` method by passing in the
    JSON payload that you would normally send to the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code imports the `json` library, which helps you to serialize
    objects into JSON strings. You create the payload using the `dumps` method. Note
    that the payload is slightly different from the simple version you saw in *Step
    3*. Instead of passing a single customer''s information, in this example, you
    pass the information of two customers: the one you passed before and another one
    who has *9* credit cards instead of *2*. Moreover, you are specifying which method
    to invoke. By default, the method name of the model is `predict`, which is the
    one you have been using in the previous chapters to make inferences. Finally,
    you print the output, which should appear similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding result shows that the first loan will be rejected, while the second
    one will be approved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Most of the classification models offer another method called `predict_proba`,
    which returns an array with the probabilities of each label. In the `loans` approval
    case, this array will only contain 2 probabilities that sum to 1, that is, the
    probability of the loan getting approved and the probability of it getting rejected.
    If you change the method name from `predict` to `predict_proba` and re-execute
    the cell, you will get the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding result shows that the model is 99.8% confident that the first
    loan will be rejected and 82.7% confident that the second loan will be approved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Optionally, navigate to `chapter12-demanding-loans`. You specify that it needs
    `1` CPU and `1.5` GB of RAM. Note that if you deleted the `model` folder in *Step
    11* of the *Registering models in the workspace* section, this code would fail
    to register the new model, as it will not be able to find the model artifact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To save on costs, you should delete the service using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So far, you have deployed a real-time endpoint using the no-code approach, which
    deploys the model as a container instance. This is only feasible if the model
    is trained using specific supported models. In the next section, you will learn
    how to deploy models using more advanced options.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the model deployment options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you deployed a model using the no-code approach. Behind
    the scenes, AzureML used an `environment` with all of the required model dependencies,
    which, in our case, was `sklearn`, generated a Python script to load the model
    and make inferences when data arrived at the endpoint, and published an ACI service
    using an `AciServiceDeploymentConfiguration` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you had a model that was trained with a non-supported framework or if you
    wanted to get better control of the deployment model, you could deploy the model
    using the AzureML SDK classes, as depicted in *Figure 12.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – The components required in a real-time endpoint deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – The components required in a real-time endpoint deployment
  prefs: []
  type: TYPE_NORMAL
- en: Here, the **InferenceConfig** class specifies the model dependencies. It requires
    an entry script that will load the model and process the incoming requests along
    with an environment in which the entry script will execute. This environment contains
    all of the dependencies required by the model to load and make inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entry script should have the following two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Init`: During this step, the script loads the trained model into memory. Depending
    on how you stored the state of the model to the disk, you can use the corresponding
    method to deserialize the model. For example, if you used the `joblib` library
    to serialize your model, you can use the `load` method of the same library to
    load it into memory. Some models provide their own serialization and deserialization
    methods, but the process remains the same; the state of the trained model is persisted
    in one file or multiple files, which you can later use to load the trained model
    into memory. Depending on how big your model is, the initialization phase might
    require a significant amount of time. Smaller `sklearn` models should load into
    memory in only a few milliseconds, while larger neural networks might require
    a couple of seconds to load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run`: This is the method invoked when a dataset is received by the real-time
    endpoint for inference. In this method, you must use the model loaded in the `init`
    code to invoke the prediction method it offers to make inferences on the incoming
    data. As mentioned earlier, most of the models offer the `predict` method, which
    you can invoke and pass into the data that you want to make an inference. Most
    of the classification models offer an additional method, called `predict_proba`,
    which returns the probabilities of each class. The AutoML forecasting models offer
    the `forecast` method instead of the `predict` method. Neural networks have a
    different approach when it comes to making predictions. For example, in the first
    version of TensorFlow, you would have to invoke a prediction method through a
    `session.run()` method call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have configured the model dependencies, you need to decide where you
    want to deploy the model. The AzureML SDK offers three classes: `LocalWebserviceDeploymentConfiguration`,
    `AciServiceDeploymentConfiguration`, and `AksServiceDeploymentConfiguration`.
    These allow you to deploy on your local machine into ACI or **Azure Kubernetes
    Services** (**AKS**), as displayed in *Figure 12.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Picking the right compute target for your model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Picking the right compute target for your model
  prefs: []
  type: TYPE_NORMAL
- en: As you might have gathered, in *Figure 12.8*, you can deploy to your local computer
    by specifying the port you want the service to listen to. This is a nice approach
    in which to debug any potential loading issues of your model or verify the integration
    with the remaining systems on your local computer. The next option is to use ACI,
    which is meant for test environments or small-scale production environments. You
    can only use CPUs and not GPUs in the `AciServiceDeploymentConfiguration` class.
    You can protect the endpoint using a key-based authentication by setting the `auth_enabled`
    parameter to `True`. This authentication method requires you to pass a static
    key as an **Authorization** header into your HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, `AksServiceDeploymentConfiguration` deploys the service inside
    an AKS cluster. This allows you to use GPUs if your model can make use of them
    and if the cluster you are deploying to has GPU-capable nodes. This deployment
    configuration allows you to choose between key-based authentication or a token-based
    one. Token-based authentication requires the end user to acquire an access token
    from the **Azure Active Directory** that protects the AzureML workspace, which
    will allow you to access the endpoint deployed within it. This token is short-lived
    and conceals the caller's identity in contrast to key-based authentication, which
    is the only available option in ACI. Another production-ready feature of the AKS
    deployment is the ability to dynamically scale up and down to handle the fluctuation
    in the number of incoming requests. In the e-banking scenario at hand, customers
    tend to visit the e-banking solution during working hours, and the system is pretty
    much idle at night. Moreover, at the end of the month, the incoming traffic peaks.
    In such a workload, you want to be able to scale your endpoint to accommodate
    for the increase in traffic when needed. AKS can automatically spin up multiple
    containers of your model and load balance the traffic among them when the incoming
    traffic increases significantly. When the traffic returns to normal, it can only
    keep a single container as a hot standby for potential incoming traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have a better understanding of the deployment options, you will
    deploy the same model in ACI using the classes that you saw in *Figure 12.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you will need to create is the entry script. Underneath the
    **chapter12** folder, create a new folder named **script** and place a **score.py**
    file inside it, as shown in *Figure 12.9*:![Figure 12.9 – Adding the score.py
    file for the real-time endpoint
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16777_12_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.9 – Adding the score.py file for the real-time endpoint
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `init` method, you are getting the path of the serialized model using
    the `AZUREML_MODEL_DIR` environment variable. When AzureML spins up the Docker
    image that will be serving the model, this variable points to the folder where
    the model is located; for example, `/tmp/azureml_umso8bpm/chapter12-loans/1` could
    be the location where you find the first version of the `chapter12-loans` model.
    In that folder, the actual artifact, named `model.joblib`, is located in the `model`
    folder, which you uploaded in *Step 5* of the *Deploying real-time endpoints*
    section. You use `os.path.join` to get the final path of the model, and then you
    load the model in a `global` variable named `model`. If you want to use the AzureML
    SDK to get the location of the model, you could use `model_path = Model.get_model_path(model_name,
    version=version)`, which uses the same environment variable under the hood. However,
    note that you would need to install the AzureML SDK in your environment to be
    able to import the `Model` class from it; this is something that is not necessary
    with the preceding code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that you are using `print` to write the model path and incoming `raw_data`
    into the console. You will learn how to view those messages in the *Monitoring
    with Application Insights* section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `run` method, you are using the `try` `except` block to catch potential
    errors while trying to read the request's input. If such an error occurs, the
    exception is serialized into a string (using the `str()` method), which is returned
    to the end user. Note that returning the exceptions to the caller is a security
    anti-pattern, as you might accidentally expose valuable information to a potential
    attacker, but it is helpful while debugging. Instead of returning the error message,
    you could use a `print` statement or a more advanced library such as `try` block,
    you deserialize the incoming JSON payload, as demonstrated in *Step 3* of the
    *Deploying real-time endpoints* section. Then, you call the `predict` method of
    the `model` object you have loaded in memory through the `init` method. Following
    this, you return the model results as a list that will be serialized into an array.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will never directly invoke either the `init` or the `run` method. There
    is another piece of code that AzureML will be putting inside the final Docker
    image, which is the HTTP inference server. This server will be responsible for
    calling your `init` method when the server boots up and will pass the incoming
    HTTP data into the `run` method. Moreover, the result that you return in the `run`
    method will be serialized into a **JSON** and will be returned to the caller.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next thing you need is an `Environment` that has all of the necessary dependencies
    to run the `score.py` script that you created. Open your `chapter12.ipynb` notebook
    and add the following code inside a new cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you created an `Environment` as demonstrated in [*Chapter
    8*](B16777_08_Final_VK_ePub.xhtml#_idTextAnchor117), *Experimenting with Python
    Code*. You add the `scikit-learn` `azureml-defaults` `pip` package, which contains
    the necessary functionality to host the model as a web service. Because you are
    building your own `Environment`, you need to add this package and use, at the
    very least, version 1.0.45\. This is the bare minimum environment that you can
    use to run your scoring script. Additionally, AzureML provides a curated environment
    that you can use, such as `AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference`,
    which contains everything you need to make an inference request using a model
    trained in `sklearn` version 0.24.1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You have defined everything that is needed by the `InferenceConfig` class.
    Add a new cell and type in the following code to put everything together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code creates the configuration you will need to make inferences with your
    model. It uses the `score.py` file that is located inside the `script` folder
    and executes that file in the `myEnv` environment, which you defined in *Step
    3*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now you have two out of the three components depicted in *Figure 12.7*. In
    this step, you will create an `AciServiceDeploymentConfiguration` class, and you
    will deploy the model in ACI. In a new cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `AciWebservice` class to get a deployment configuration for
    the container instance you want to deploy. In the preceding code, you specify
    that you require 1 CPU core and 1 GB of RAM. Then, you deploy the model into a
    new service, named `aci-loans`, and you wait for the deployment to complete.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you are running into issues while attempting to deploy the container, you
    can view the error messages in the printed outputs or use the `service.get_logs()`
    method. Most likely, it is an issue with your code base within the `score.py`
    script. You can locally test the code by installing the `azureml-inference-server-http`
    pip package and running the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`5001`. Another approach to debugging such situations is to use `LocalWebservice`,
    as we will discuss later. If your code is fine, then you might be running into
    memory issues. This should be visible in the service logs. In that case, refer
    to the next section to learn how you can profile your model to determine its resource
    requirements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To test the deployed service, you can use the following code, which is similar
    to the one that you used in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the `method` property of the payload, which you used in *Step 4* of
    the *Deploying real-time endpoints* section, will not have any effect on this
    deployment and is omitted from the payload. If you wanted to support this property,
    you would have to write the code within the `run` method of the `score.py` file
    to read that property and call the corresponding method of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To save on costs, delete the service when you are done testing it using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you want to deploy the same service in your local computer, you can use
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Instead of the `AciWebservice` class, you use the `LocalWebservice` to create
    a local service that listens to port `1337`. If you are running the notebooks
    on your local computer, you need to visit `http://localhost:1337` and view the
    service endpoint's health status. Now that you have run this code within the AzureML
    notebooks, the local computer is the compute instance you are working on. To view
    port `1337` of the compute instance named `service.delete()` code, as demonstrated
    in *Step 7*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to the `AciWebservice` and the `LocalWebservice`, you can use `AksWebservice`
    to create an `AksServiceDeploymentConfiguration`. While deploying it, you would
    need to specify an additional parameter in the `Model.deploy` method, that is,
    the `deployment_target` parameter. This parameter allows you to specify the `AksCompute`
    inference cluster that you want to deploy the model to.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the local computer, ACI, and AKS deployment options that you saw
    earlier, AzureML offers multiple other deployment options. For example, **Azure
    Functions** allows you to run your models inside a serverless infrastructure,
    and **Azure App Services** hosts the model as a traditional web application that
    is always ready to serve incoming requests. On the other hand, you can use **IoT
    Edge**, which allows you to deploy the service on an Edge device such as a Raspberry
    Pi or a GPU-based Jetson Nano. Finally, you can even package the model inside
    a Docker container image, which can be operationalized inside an isolated air
    gap data center.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you deployed an ACI real-time inference endpoint requesting
    1 CPU core and 1 GB of RAM. In the next section, you will explore how you can
    optimize your resource requirements by profiling the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the model's resource requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before bringing something into production, it is very common to perform a stress
    test. Essentially, this test bombards the real-time endpoint with requests and
    measures the responsiveness and performance of the endpoint. You can do something
    similar with your models to understand what type of resources you will need in
    order for them to perform as expected. For example, you might need to ensure that
    all inferences are performed within 200 ms.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you are going to create a test dataset that will be used to
    stress-test the real-time endpoint and observe its performance. Each row in the
    dataset will contain a single inference request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to your `chapter12.ipynb` notebook and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code loads the `loans` dataset, drops the `approved_loan` column, which
    we don''t need, and loads it inside a `pandas` `DataFrame`. Following this, you
    create a new column named `sample_request` that concatenates the columns to produce
    a string such as the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, you keep only that column and print the top 5 rows to verify that the
    requests look as expected. Note that it does not matter whether the data is the
    one we used to train the model. It could even be random records. We only care
    about the number of requests we will be making and not what the inference result
    is going to look like.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Store the newly created dataset inside the workspace using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code registers the DataFrame as the `loans-requests` dataset.
    The data is stored inside `/samples/loans-requests` of the default datastore.
    The `loans_req_ds` variable has a reference to the newly registered `tabular`
    `Dataset`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that you have the necessary data, you can start the model profiling process
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the profile method requires the `model` and the `inference_config`
    that you used during model deployment in the previous section. Additionally, you
    need to specify your ACI size to use to perform the analysis. In the preceding
    code, you request 2 CPUs and 1 GB of RAM. The analysis could take a long time,
    sometimes, more than 20 minutes. After the analysis completes, you will view the
    results, including the 1 CPU as the `recommendedCpu` and 0.5 GB of RAM as the
    `recommendedMemoryInGB` value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The name of the model profile should be unique within the workspace. An error
    will occur if you try to rerun the code of *Step 3* without changing the name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Behind the scenes, an experiment named `ModelProfile` run is executed, which
    deploys an ACI service with the model. Once the service is up and running, the
    process sends the 500 requests that you specified in the **loan_req_ds** dataset
    and records the response time of the model while monitoring the CPU and memory
    utilization of the deployed container instance. AzureML can suggest the recommended
    CPU and memory that you should configure for your real-time endpoint based on
    those statistics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will use those values to deploy an ACI service. Following
    this, you will explore how to monitor its performance once deployed in production
    and log the incoming data using **Application Insights**.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with Application Insights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you learned in [*Chapter 2*](B16777_02_Final_VK_ePub.xhtml#_idTextAnchor026),
    *Deploying Azure Machine Learning Workspace Resources*, when you deploy the AzureML
    workspace, an Application Insights account named `packtlearningm<random_number>`
    is deployed in the same resource group. This Azure resource allows you to monitor
    the performance of your applications. Especially for web applications, such as
    the real-time endpoint you are deploying, Application Insights allows you to monitor
    the request and response times, the failure rate of the endpoint, any potential
    exceptions raised in your code, and even log traces that you want to emit from
    your code base.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Understanding the model deployment options* section earlier, you created
    a `score.py` file that contained a couple of `print` statements. These messages
    were written inside the console of the endpoint and could be found either by calling
    the `service.get_logs()` method or navigating to the **Deployment logs** tab of
    the deployment, as shown in *Figure 12.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – The model path and incoming raw_data logged in the console
    of the container instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – The model path and incoming raw_data logged in the console of
    the container instance
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is that the logs do not persist. If you redeploy
    the container instance, you will lose the logs. Moreover, if you have multiple
    models deployed, you will need a centralized place to be able to monitor all of
    them together. These are two of the many benefits that Application Insights brings
    to your solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to your `chapter12.ipynb` notebook to redeploy the ACI container and
    enable Application Insights for it. Inside a new cell, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that you are using the `1` CPU core and `0.5` GB RAM that was recommended
    in the *Profiling the model resource requirements* section. Additionally, note
    that you are enabling Application Insights in the deployment configuration by
    passing the `enable_app_insights= True` argument. If you had already deployed
    the service and you wanted to enable Application Insights for it, you could use
    the following code to update its configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s send a couple of requests to the service to be able to better understand
    what Application Insights can do for you. Inside a new cell, add the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This code sends *10* identical requests to the service, one after the other,
    generating some artificial traffic that should be logged in Application Insights.
    The easiest way to find the URL that is pointing to the Azure portal and directly
    inside the Application Insights resource is to visit the endpoint''s information
    page, as displayed in *Figure 12.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – The Application Insights URL that is associated with your
    AzureML workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – The Application Insights URL that is associated with your AzureML
    workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this **Application Insights url** link is not specific to the **aci-loans**
    deployment. This link will be the same for all of your real-time endpoints, allowing
    you to centrally monitor all of your real-time endpoints. Clicking on that link
    will take you inside Application Insights, as shown in *Figure 12.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Application Insights showing the 10 requests that you sent
    with the last code'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Application Insights showing the 10 requests that you sent with
    the last code
  prefs: []
  type: TYPE_NORMAL
- en: 'From this dashboard, you can click on the graphs and drill down to the signal
    details; or you can view all the traces that your application is writing inside
    the console. To view them, navigate to **Monitoring** | **Logs**, click on the
    **traces**, select a time range that you want to investigate, and click on the
    **Run** button. You should see all of the **STDOUT** messages appear in the results,
    and you can drill down into the details, as displayed in *Figure 12.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – Reading all of the traces emitted by your model''s real-time'
  prefs: []
  type: TYPE_NORMAL
- en: endpoint in Application Insights
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.13 – Reading all of the traces emitted by your model's real-time endpoint
    in Application Insights
  prefs: []
  type: TYPE_NORMAL
- en: You can create complex queries in this **Logs** section using a powerful SQL-like
    language known as **Kusto**. You can even create automated alerts based on those
    queries, notifying you, for example, whenever you have had more than 100 loans
    rejected in the last 30 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Application Insights supports the logging of small payloads of up to 64 KB at
    a time. If you plan to log more than that, for example, a mini-batch input with
    more than 64 KB of data, you should consider working with the `DataCollector`
    class of the AzureML SDK. This class allows you to log data directly into a storage
    account; however, it is only available if you deploy in AKS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on to the next section, do not forget to delete the deployed
    service to prevent any accidental cost charges for the ACI service. You can delete
    the service from the **Assets** | **Endpoints** list in the studio experience
    or via the code using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this section, you learned how to monitor your real-time endpoint once you
    have it deployed in production. In *Figure 12.12*, you might have noticed that
    there were a couple of `swagger` file. In the next section, you will learn how
    to fix those failed requests and enable rich integration with third-party applications
    that want to consume the results of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with third-party applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you have deployed a web service that accepts an array of arrays as input.
    This is a cryptic input that you need to explain to whoever wants to consume your
    real-time endpoint. In [*Chapter 5*](B16777_05_Final_VK_ePub.xhtml#_idTextAnchor072),
    *Letting the Machines Do the Model Training*, you read about the `swagger` file
    that could be used to generate code to automatically consume your endpoints. To
    produce such a file, you can use the open source `inference-schema` package and
    decorate your code with metadata that will drive the generation of the `swagger.json`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make your model slightly easier to consume by third-party applications,
    you should accept the following payload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you will need to create a new version of the scoring file. Instead of
    cloning and editing the existing scoring file, you can download the modified `score_v2.py`
    version directly from the GitHub page, as mentioned in the *Technical requirements*
    section. In the **Notebooks** section, duplicate the **score.py** file located
    in the **script** folder by right-clicking on it and selecting the **Duplicate**
    command, as shown in *Figure 12.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – Creating the v2 file of the entry script'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.14 – Creating the v2 file of the entry script
  prefs: []
  type: TYPE_NORMAL
- en: 'Name the clone `score_v2.py`, and modify the code to look like the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At the beginning of the script file, you are importing additional helper classes,
    which will be used later in the code. Notice that you will no longer need the
    `json` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You won''t modify the `init` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, you create a `pandas` `DataFrame` that will act
    as the sample for the objects contained in the incoming request''s `data` attribute.
    This `data_sample` object has an `income` feature, which is `float64`, and the
    `credit_cards` and `age` features, which are integers. Similarly, for the output,
    you define `output_sample` as a NumPy array or numeric value. You can use the
    `data_sample` and `output_sample` objects inside the decorators of the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, you use the `data_sample` object with the `@input_schema` decorator. Additionally,
    you use `PandasParameterType`, which indicates that the parameter named `pandas`
    `DataFrame` that follows the schema defined by the `data_sample` example. You
    use the `@output_schema` decorator to specify that your service returns a NumPy
    array as an output, similar to `output_sample`. Once you have configured these
    schemas, you will notice that you do not have to deserialize the incoming payload
    within the `run` method. The `data` object is an already deserialized `pandas`
    `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to process binary files instead of tabular data, for instance, processing
    an image, you can use the `@rawhttp` directive, which will pass the raw HTTP request
    to your `run` method. Working with plain HTTP requests gives you greater flexibility,
    including setting the response headers; this is something required when you configure
    security features such as **Cross-Origin Resource Sharing** (**CORS**). You can
    find resources to learn more about those advanced scenarios in the *Further reading*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the code of the `score_v2.py` script file ready, you need
    to publish the real-time endpoint. To create a real-time endpoint for the new
    scoring function, add the following code inside a cell within your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you append the `inference_schema` pip packages in the
    `myEnv` dependencies, which you defined in the *Understanding the model deployment
    options* section earlier. Note that you are installing that package with the `pandas-support`
    extra, which will include the `pandas` package. The `numpy` dependency that your
    `score_v2.py` file depends upon will automatically be installed by pip since it
    is a dependency of the `pandas` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, you specify that you are using the `score_v2.py` entry script
    and deploy the new service. The new service will have a `swagger.json` file available
    for third-party applications such as Power BI to read and automatically understand
    how to invoke your model. You can get the Swagger URI to point to that file on
    the endpoint''s page, as shown in *Figure 12.11*. On the endpoint''s page, you
    should notice that the **Test** tab has been enhanced to guide you on what fields
    you need to provide to invoke the model. On the code side, you can invoke the
    model with the following payloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Before moving on to the next section, make sure you delete the ACI service
    you just deployed by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: So far, you have been deploying real-time inference endpoints that could fulfill
    ad hoc inference requests through a REST API. In the next section, you will learn
    how to deploy a batch inference pipeline that can process big data in parallel
    using `ParallelRunStep`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a batch inference pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working
    with Pipelines*, you learned how to create pipelines that orchestrate multiple
    steps. These pipelines can be invoked using a REST API, similar to the real-time
    endpoint that you created in the previous section. One key difference is that
    in the real-time endpoint, the infrastructure is constantly on, waiting for a
    request to arrive, while in the published pipelines, the cluster will spin up
    only after the pipeline has been triggered.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could use these pipelines to orchestrate batch inference on top of data
    residing in a dataset. For example, let''s imagine that you just trained the `loans`
    model you have been using in this chapter. You want to run the model against all
    of the pending loan requests and store the results; this is so that you can implement
    an email campaign targeting the customers that might get their loan rejected.
    The easiest approach is to create a single `PythonScriptStep` that will process
    each record sequentially and store the results in the output folder, as you learned
    in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with
    Pipelines*. Instead of doing that, you could break the dataset into multiple batches
    and then have them processed, in parallel, in multiple processes running inside
    each node of your cluster, as displayed in *Figure 12.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – Parallel processing big datasets by splitting them into smaller'
  prefs: []
  type: TYPE_NORMAL
- en: batches and processing them in parallel
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.15 – Parallel processing big datasets by splitting them into smaller
    batches and processing them in parallel
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will create a batch processing pipeline that will be making
    inferences using the `chapter12-loans` model you trained in this chapter. You
    already have a dataset named `loans`, but it is too small to show how `ParallelRunStep`
    can help you speed up by parallelizing the inferences. You will generate a new
    dataset that will be 1,024 times bigger by copying the same DataFrame repeatedly.
    Then, you will create a pipeline similar to the one that you created in [*Chapter
    11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with Pipelines*.
    This time, you will use the `ParallelRunConfig` and the `ParallelRunStep` classes
    to parallelize the processing of the dataset. The configuration class requires
    an entry script, similar to the entry script that you saw in the previous section.
    Additionally, you need to define the following two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init()`: This method loads the model and prepares the process for the incoming
    batches. No output is expected from this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run(mini_batch)`: This method does the actual data processing. This method
    will be invoked multiple times, passing a different `mini_batch` parameter every
    time. You have to return an array containing one row for each item you managed
    to process within this function as an output. For example, if the `mini_batch`
    parameter had 100 rows and you return an array of 98 items, you will indicate
    that you failed to process 2 of those records. The `mini_batch` parameter could
    either be a `pandas` `DataFrame` if you are processing `TabularDataset` or an
    array that contains the file paths you need to process if you are processing a
    `FileDataset`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigate to your `chapter12.ipynb` notebook and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by getting a reference to the workspace, the dataset, and the compute
    cluster you are going to use for your pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code should be self-explanatory, as you used it in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160),
    *Working with Pipelines*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a new, bigger dataset based on the `loans` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you are loading the `loans` `DataFrame` into memory without
    the `approved_loan` column. This dataset only contains 500 rows. Then, you append
    the dataset to itself 10 times. This will create a much bigger dataset containing
    512,000 rows, which you register as `pending-loans`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, it''s time to create the script that will be processing this dataset.
    In the `chapter12` folder, add a `pipeline_step` folder and then add a `tabular_batch.py`
    file with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This script has two methods, as metioned earlier. In the `init` method, you
    use the `get_model_path` method of the `Model` class to get the location of the
    model that you have been using so far. From the script''s perspective, the model
    will reside in a folder on the same computer where the script is running. Then,
    you use `joblib` to load the model inside a `global` variable named `model`. In
    the `run` method, you print the size of the incoming DataFrame, and then you create
    a new column, named *approved*, where you store all of the model inferences. You
    return a list containing a four-element array for each row you processed, similar
    to the following records:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you were to process `FileDataset` instead of the `TabularDataset` that you
    are processing in this section, the corresponding `file_batch.py` file would look
    like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You load your model, as usual, inside the `init` method, for example, a neural
    network that will implement image classification. In the `run` method, the `mini_batch`
    parameter is an array containing the file paths of the files you need to process.
    You can loop through those files and make the inferences using your model. As
    an output, you return the filename and result of the model, as demonstrated in
    the following example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will observe, in *Step 5*, that those results will be aggregated in a single
    file defined in `ParallelRunConfig`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will need to create an environment to execute your pipeline step. Add the
    following code inside a cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You need to install the `scikit-learn` conda package, just as you have been
    doing so far. For `ParallelRunConfig` to work, you will need to include the `azureml-core`
    and `azureml-dataset-runtime[pandas,fuse]` `pip` packages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, create the `ParallelRunConfig` class that configures how the run will
    split the workload and what script to use for data processing. Add the following
    code inside a new notebook cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, you will run the `tabular_batch.py` script located inside the `pipeline_step`
    folder. You are going to split the dataset into smaller batches of, approximately,
    100 KB. If you were processing a `FileDataset`, you would need to specify the
    number of files to put in each batch. Here, `error_threshold` specifies the number
    of record or file failures that should be ignored while processing the data. `-1`
    means you are okay with any number of processing errors. The `output_action` parameter
    accepts either `append_row` values or `summary_only` values. Using the `append_row`
    value, you can request all outputs from the `run` method invocations to be appended
    inside a single output file, the name of which is `parallel_run_step.txt`, unless
    you override it via the `append_row_file_name` parameter, as demonstrated in the
    preceding example. The order of the records in that file is not guaranteed since
    the records are processed in parallel. Usually, you would return the customer
    ID, or the loan application ID, and the model's inference. Using that ID, you
    could link back the original record with the model's prediction. In the current
    example, we don't have any ID; therefore, we return the entire row, just as we
    did in the `tabular_batch.py` script in *Step 3*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Following this, you specify the environment and the cluster where this pipeline
    step will be executed. In the end, you specify that this pipeline step will run
    in a single node, and it will spin up *two* processes per participating node.
    If you used two nodes, you would have four processes running in parallel. In the
    current example, two parallel processes are enough to handle the processing in
    only a couple of minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have a heavy processing script that requires more than 60 seconds to
    process the `mini_batch_size` parameter you specified, you can increase the timeout
    value by setting the `run_invocation_timeout` parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As a next step, you will define the output location of `append_row_file_name`
    that you specified earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will store that aggregation file in the default datastore, underneath the
    `/inferences/loans/` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now it''s time to create the first and only step of the pipeline, that is,
    `ParallelRunStep`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Name that step `chapter12-parallel-loans` and pass the `pending_loans_ds` dataset
    that you registered in *Step 2* earlier. The output is stored in `OutputFileDatasetConfig`,
    which you created in *Step 6*. Specify that this step should not be reused (`allow_reuse`);
    this allows you to trigger the pipeline multiple times to always get the latest
    data in the dataset along with the latest registered model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create and execute a pipeline using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can watch the execution logs by using the `RunDetails` widget with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you can wait for the execution to complete with the following
    code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From that point on, you can publish and even schedule the pipeline, as discussed
    in [*Chapter 11*](B16777_11_Final_VK_ePub.xhtml#_idTextAnchor160), *Working with
    Pipelines*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can visit the pipeline in AzureML studio and observe the outputs and the
    logs it produced, as shown in *Figure 12.16*. Notice that you will find a single
    node and two processes. Each process has multiple `run` method invocations. Each
    time the `run` method was invoked, a DataFrame that required 117.3 KB in memory
    was passed in, which is close to the 100 KB that you requested in *Step 5* earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – The logs from the parallel execution showing the information
    of the mini_batch DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16777_12_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.16 – The logs from the parallel execution showing the information
    of the mini_batch DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to create a batch processing pipeline that
    can process a significant amount of data in parallel. This concludes the operationalization
    options that you need to be aware of for the exam, covering both the real-time
    and batch modes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored various ways in which to use the machine learning
    models that you have been training in this book. You can make either real-time
    inferences or batch process a large number of records in a cost-effective manner.
    You started by registering the model you would use for inferences. From there,
    you can either deploy a real-time endpoint in ACI for testing or in AKS for production
    workloads that require high availability and automatic scaling. You explored how
    to profile your model to determine the recommended container size to host the
    real-time endpoint. Following this, you discovered Application Insights, which
    allows you to monitor production endpoints and identify potential production issues.
    Through Application Insights, you noticed that the real-time endpoint you produced
    wasn't exposing a `swagger.json` file that was needed by third-party applications,
    such as Power BI, to automatically consume your endpoint. You modified the scoring
    function to include metadata regarding your model's inputs and outputs, thus completing
    the real-time inference section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you moved on to the batch inferencing side, where you authored a pipeline
    that can process half a million records, in parallel, in only a few minutes. Combining
    this parallelization with low-priority computes, you can achieve great cost savings
    when inferencing larger data volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have completed your journey of discovering the basic capabilities
    of the AzureML workspace. Now you can now conduct machine learning experiments
    in the workspace, and you can operationalize the resulting models using the option
    that suits the business problem that you are trying to solve. With this knowledge,
    you should be able to pass the *DP-100* exam, *Designing and Implementing a Data
    Science Solution on Azure*, with flying colors.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In each chapter, you will find a number of questions to validate your understanding
    of the topics that have been discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: You want to deploy a real-time endpoint that will handle transactions from a
    live betting website. The traffic from this website will have spikes during games
    and will be very low during the night. Which of the following compute targets
    should you use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. ACI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. A compute instance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. A compute cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. AKS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You want to monitor a real-time endpoint deployed in AKS and determine the average
    response time of the service. Which monitoring solution should you use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. ACI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Azure Container Registry
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Application Insights
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have a computer vision model, and you want to process 100 images in parallel.
    You author a pipeline with a parallel step. You want to process 10 images at a
    time. Which of the following `ParallelRunConfig` parameters should you set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `mini_batch_size=10`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. `error_threshold=10`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. `node_count=10`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. `process_count_per_node=10`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section offers a list of helpful web resources to help you augment your
    knowledge of the AzureML SDK and the various code snippets used in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model persistence guidance from scikit-learn: [https://scikit-learn.org/stable/modules/model_persistence.html](https://scikit-learn.org/stable/modules/model_persistence.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Testing a REST API using Postman: [https://www.postman.com/product/api-client/](https://www.postman.com/product/api-client/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **curl** command-line tool to make web requests: [https://curl.se/](https://curl.se/%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitor Python applications using OpenCensus: [https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python](https://docs.microsoft.com/azure/azure-monitor/app/opencensus-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How to use the inference server to test your entry scripts locally: [https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http](https://docs.microsoft.com/azure/machine-learning/how-to-inference-server-http)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Packaging the model inside an autonomous Docker container: [https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-package-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ONNX machine learning format used to store models that can be loaded in
    multiple platforms: [https://docs.microsoft.com/azure/machine-learning/concept-onnx](https://docs.microsoft.com/azure/machine-learning/concept-onnx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An introduction to Application Insights: [https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An introduction to the Kusto Query Language: [https://docs.microsoft.com/azure/data-explorer/kusto/concepts/](https://docs.microsoft.com/azure/data-explorer/kusto/concepts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The advanced real-time endpoint entry script authoring guide: [https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integrating AzureML models in Power BI: [https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi](https://docs.microsoft.com/power-bi/transform-model/dataflows/dataflows-machine-learning-integration#azure-machine-learning-integration-in-power-bi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the `ParallelRunStep` class to train hundreds of models: [https://github.com/microsoft/solution-accelerator-many-models](https://github.com/microsoft/solution-accelerator-many-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
