<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer057">&#13;
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor797"/>11</h1>&#13;
			<h1 id="_idParaDest-177"><a id="_idTextAnchor798"/>Introduction to Data Science in Python</h1>&#13;
			<p>In recent years, Python has gained a lot of popularity in the data science field. Its very efficient and readable syntax makes the language a very good choice for scientific research, while still being suitable for production workloads; it’s very easy to deploy research projects into real applications that will bring value to users. Thanks to this growing interest, a lot of specialized Python libraries have emerged and are now standards in the industry. In this chapter, we’ll introduce the fundamental concepts of machine learning before diving into the Python libraries used daily by <span class="No-Break">data scientists.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Understanding the basic concepts of <span class="No-Break">machine learning</span></li>&#13;
				<li>Creating and manipulating NumPy arrays and <span class="No-Break">pandas datasets</span></li>&#13;
				<li>Training and evaluating machine learning models <span class="No-Break">with scikit-learn</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-178"><a id="_idTextAnchor799"/>Technical requirements</h1>&#13;
			<p>For this chapter, you’ll require a Python virtual environment, just as we set up in <a href="B19528_01.xhtml#_idTextAnchor024"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Python Development </em><span class="No-Break"><em class="italic">Environment Setup</em></span><span class="No-Break">.</span></p>&#13;
			<p>You’ll find all the code examples for this chapter in the dedicated GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-179"><a id="_idTextAnchor800"/>What is machine learning?</h1>&#13;
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) is <a id="_idIndexMarker782"/>ofte<a id="_idTextAnchor801"/>n seen as a subfield of artificial intelligence. While this categorization is the subject of debate, ML has had a lot of exposure in recent years due to its vast and visible field of applications, such as spam filters, natural language processing, and <span class="No-Break">image generation.</span></p>&#13;
			<p>ML is a<a id="_idIndexMarker783"/> field where we build mathematical models from existing data so that the machine can understand this data by itself. The machine is “learning” in the sense that the developer doesn’t have to program a step-by-step algorithm to solve the problem, which would be impossible for complex tasks. Once a model has been “trained” on existing data, it can be used to predict new data or understand <span class="No-Break">new observations.</span></p>&#13;
			<p>Consider the spam filter example: if we have a sufficiently large collection of emails manually labeled “spam” or “not spam,” we can use ML techniques to build a model that can tell us whether a new incoming email is spam <span class="No-Break">or not.</span></p>&#13;
			<p>In this section, we’ll review the most fundamental concepts <span class="No-Break">of ML<a id="_idTextAnchor802"/>.</span></p>&#13;
			<h2 id="_idParaDest-180"><a id="_idTextAnchor803"/>Supervised versus unsupervised learning</h2>&#13;
			<p>ML techniqu<a id="_idTextAnchor804"/>es can be divid<a id="_idTextAnchor805"/>ed in<a id="_idTextAnchor806"/>to two main categories: <strong class="bold">supervised learning</strong> and <span class="No-Break"><strong class="bold">unsupervised learning</strong></span><span class="No-Break">.</span></p>&#13;
			<p>With <a id="_idIndexMarker784"/>supervised <a id="_idIndexMarker785"/>learning, the existing dataset is already labeled, which means we have both the input (the characteristics of an observation), k<a id="_idTextAnchor807"/>nown as <strong class="bold">features</strong>, and the <a id="_idIndexMarker786"/>output. If we consider the spam filter example here, the features could be the freque<a id="_idTextAnchor808"/>ncies of each word and <a id="_idIndexMarker787"/>the <strong class="bold">label</strong> could be the category – that is, “spam” or “not spam.” Supervised learning is subdivided into <span class="No-Break">two groups:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Classification problems</strong>, to cl<a id="_idTextAnchor809"/>assify <a id="_idIndexMarker788"/>data with a finite set of categories – for example, the <span class="No-Break">spam filter</span></li>&#13;
				<li><strong class="bold">Regression problems</strong>, to <a id="_idTextAnchor810"/>predict<a id="_idIndexMarker789"/> continuous numerical values – for example, the number of rented electric scooters, given the day of the week, the weather, and <span class="No-Break">the location</span></li>&#13;
			</ul>&#13;
			<p>Unsupervised learning, on the <a id="_idIndexMarker790"/>other hand, operates on data without any reference<a id="_idIndexMarker791"/> to a label. The goal here is to discover interesting patterns from the features themselves. The two main problems that unsupervised learning tries to solve are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Cluster<a id="_idTextAnchor811"/>ing</strong>, where we want to find<a id="_idIndexMarker792"/> groups of similar data points – for example, a recommender system to suggest products that you might like, given what other people similar to <span class="No-Break">you like.</span></li>&#13;
				<li><strong class="bold">Dimensionality reduction</strong>, whe<a id="_idTextAnchor812"/>re the <a id="_idIndexMarker793"/>goal is to find a more compact representation of datasets that contain a lot of different features. Doing thi<a id="_idTextAnchor813"/>s will allow us to keep only the most meaningful and discriminant features while working with smaller <span class="No-Break">dataset <a id="_idTextAnchor814"/>dimensions.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-181"><a id="_idTextAnchor815"/>Model validation</h2>&#13;
			<p>On<a id="_idTextAnchor816"/>e<a id="_idIndexMarker794"/> of th<a id="_idTextAnchor817"/>e key aspects of ML is evaluating <a id="_idIndexMarker795"/>whether your model is performing well or not. How can you say that your model will perform well on newly observed data? When building your model, how can you tell whether one algorithm performs better than another? All of these questions can and should be answered with model <span class="No-Break">validation techniques.</span></p>&#13;
			<p>As we mentioned previously, ML methods start with an existing set of data that we’ll use to train <span class="No-Break">a model.</span></p>&#13;
			<p>Intuitively, we may want to use all the data we have to train our model. Once done, what can we do to test it? We could apply our model to the same data and see whether the output was correct... and we would get a surprisingly good result! Here, we are testing the model with the same data we used to train it. Obviously, the model will overperform on this data because it has already seen it. As you may have guessed, this is not a reliable way to measure the accuracy of <span class="No-Break">our model.</span></p>&#13;
			<p>The right way to validate a model is to split the data <a id="_idIndexMarker796"/>into two: we keep one part for training the data and another for testing it<a id="_idTextAnchor818"/>. This is known as the <strong class="bold">holdout set</strong>. This way, we’ll test the model on data that it has never seen before and compare the result that’s predicted by the model with the real value. Hence, the accuracy we are measuring is much <span class="No-Break">more sensible.</span></p>&#13;
			<p>This technique works well; however, it poses a problem: by retaining some data, we are losing precious information that could have helped us build a better model. This is especially true if our initial dataset is small. To<a id="_idTextAnchor819"/> solve this, we<a id="_idIndexMarker797"/> can use <strong class="bold">cross-validation</strong>. With this method, we once again split the data into two sets. This time, we are training the model twice, using each set as training and testing sets. You can see a schematic representation of<a id="_idIndexMarker798"/> this operation in the <span class="No-Break">following diagram:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer054" class="IMG---Figure">&#13;
					<img src="Images/Figure_11.1_B19528.jpg" alt="Figure 11.1 – Two-fold cross-validation" width="1205" height="324"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Two-fold cross-valid<a id="_idTextAnchor820"/>ation</p>&#13;
			<p>At the<a id="_idIndexMarker799"/> end of the ope<a id="_idTextAnchor821"/>ration, we obtain two accuracies, which<a id="_idIndexMarker800"/> will give us a better overview of how our model performs on the whole dataset. This technique can be applied to help us perform more trials with a smaller testing set, as <a id="_idIndexMarker801"/>shown in the <span class="No-Break">following diagram:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer055" class="IMG---Figure">&#13;
					<img src="Images/Figure_11.2_B19528.jpg" alt="Figure 11.2 – Five-fold cross-validation" width="1225" height="974"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Five-fold cross-validation</p>&#13;
			<p>We’ll stop here regarding this very quick introduction to ML. We’ve barely scratched the surface: ML is a vast and complex field, and there are lots of books dedicated to this subject. Still, this <a id="_idTextAnchor822"/>information should be sufficient to help you understand <a id="_idTextAnchor823"/>the basic concepts we’ll show throughout the rest of <span class="No-Break">this chapter.</span></p>&#13;
			<h1 id="_idParaDest-182"><a id="_idTextAnchor824"/>Manipulating arrays with NumPy and pandas</h1>&#13;
			<p>As we said in the<a id="_idIndexMarker802"/> introduction, numerous Python libraries have been developed to help with common data science tasks. The most fundamental ones are probably NumPy and pandas. Their goal is to provide a set of tools to manipulate a big set of data in an efficient way, much more than what we could actually achieve with standard Python, and we’ll show how and why in this section. NumPy and pandas are at the heart of most data science applications in Python; knowing about them is therefore the first step on your journey into Python for <span class="No-Break">data science.</span></p>&#13;
			<p>Before starting to use them, let’s explain why such libraries are needed. In <a href="B19528_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Python Programming Specificities</em>, we stated that Python is a dynamically typed language. This means that the interpreter automatically detects the <a id="_idTextAnchor825"/>type of a variable at runtime, and this type can even change throughout the program. For example, you can do something like this <span class="No-Break">in Python:</span></p>&#13;
			<pre class="source-code">&#13;
$ python&gt;&gt;&gt; x = 1&#13;
&gt;&gt;&gt; type(x)&#13;
&lt;class 'int'&gt;&#13;
&gt;&gt;&gt; x = "hello"&#13;
&gt;&gt;&gt; type(x)&#13;
&lt;class 'str'&gt;</pre>&#13;
			<p>The interpreter was able to determine the type of <strong class="source-inline">x</strong> at <span class="No-Break">each assignation.</span></p>&#13;
			<p>Under the hood, the standard implementation of P<a id="_idTextAnchor826"/>ython, CPython, is written i<a id="_idTextAnchor827"/>n C. The C language is a compiled and statically typed language. This means that the nature of the var<a id="_idTextAnchor828"/>iables is fixed at compile time, and they can’t change during execution. Thus, in the Python implementation, a variable doesn’t only consist of its value: it’s actually a structure containing information about the variable, including its type and size, in addition to <span class="No-Break">its value.</span></p>&#13;
			<p>Thanks to this, we can manipulate variables very dynamically in Python. However, it comes at a cost: each variable has a significantly higher memory footprint to store all its metadata than just the <span class="No-Break">plain value.</span></p>&#13;
			<p>This is particularly true for data structures. Say we consider a simple list <span class="No-Break">like this:</span></p>&#13;
			<pre class="source-code">&#13;
$ python&gt;&gt;&gt; l = [1, 2, 3, 4, 5]</pre>&#13;
			<p>Each item in the list is a Python integer, with all the metadata associated. In a statically typed language such as C, the same list would only be a suite of values in memory sharing the <span class="No-Break">same type.</span></p>&#13;
			<p>Let’s now<a id="_idIndexMarker803"/> imagine a big set of data, like the kind we usually encounter in data science: the cost of storing it in memory would be huge. That’s exactly the purpose of NumPy: to provide a powerful and efficient array structure for manipulating a big set of data. Under the hood, it uses a fixed-type array, meaning all elements of the structure are of the same type, wh<a id="_idTextAnchor829"/>ich allows NumPy to get rid of the costly metadata of every single element. Moreover, common arithmetic operations, such as additions or multiplications, are much faster. In the <em class="italic">Manipulating arrays with NumPy – computation, aggregations, and comparisons</em> section, we’ll make a speed comparison to show you the difference with standard <span class="No-Break">Python lists.</span></p>&#13;
			<h2 id="_idParaDest-183"><a id="_idTextAnchor830"/>Getting started with NumPy</h2>&#13;
			<p>Let’s see how <a id="_idIndexMarker804"/>NumPy works! The first thing is to install it using the <span class="No-Break">following command:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ pip install numpy</pre>			<p>In a Python interpreter, we can now import <span class="No-Break">the library:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ python&gt;&gt;&gt; import numpy as np</pre>&#13;
			<p>Notice that, by convention, <em class="italic">NumPy is generally imported with the alias </em><strong class="source-inline">np</strong>. Let’s now discover<a id="_idTextAnchor831"/> its <span class="No-Break">basic features!</span></p>&#13;
			<h3>Creating arrays</h3>&#13;
			<p>To <a id="_idTextAnchor832"/>create an <a id="_idIndexMarker805"/>array with NumPy, we can simply use the <strong class="source-inline">array</strong> function<a id="_idIndexMarker806"/> and pass it a <span class="No-Break">Python list:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.array([1, 2, 3, 4, 5])array([1, 2, 3, 4, 5])<a id="_idTextAnchor833"/></pre>&#13;
			<p>NumPy will detect the nature of the Python list. However, we can force the resulting type by using the <span class="No-Break"><strong class="source-inline">dtype</strong></span><span class="No-Break"> argument:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.array([1, 2, 3, 4, 5], dtype=np.float64)array([1., 2., 3., 4., 5.])</pre>&#13;
			<p>All elements were upcasted to the specified type. It is key to remember that a <em class="italic">NumPy array is of a fixed type</em>. This means that every element will have the same type and NumPy will silently cast a value to the <strong class="source-inline">array</strong> type. For example, let’s consider an integer list into which we want to insert a <span class="No-Break">floating-point value:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; l = np.array([1, 2, 3, 4, 5])&gt;&gt;&gt; l[0] = 13.37&#13;
&gt;&gt;&gt; l&#13;
array([13,  2,  3,  4,  5])</pre>&#13;
			<p>The <strong class="source-inline">13.37</strong> value has been truncated to fit into <span class="No-Break">an integer.</span></p>&#13;
			<p>If the value cannot be cast to the type of array, an error is raised. For example, let’s try to change the first element with <span class="No-Break">a string:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; l[0] = "a"Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
ValueError: invalid literal for int() with base<a id="_idTextAnchor834"/> 10: 'a'</pre>&#13;
			<p>As we said in the introduction to this section, Python lists are not very efficient for large datasets. This is why it’s ge<a id="_idTextAnchor835"/>nerally more efficient to use NumPy functions to create arrays. The most commonly used ones are generally <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">np.zeros</strong>, to create an array filled <span class="No-Break">with zeros</span></li>&#13;
				<li><strong class="source-inline">np.ones</strong>, to create an array filled <span class="No-Break">with ones</span></li>&#13;
				<li><strong class="source-inline">np.empty</strong>, to create an empty array of the desired size in memory, without initializing <span class="No-Break">the values</span></li>&#13;
				<li><strong class="source-inline">np.arange</strong>, to create an array with a range <span class="No-Break">of elements</span></li>&#13;
			</ul>&#13;
			<p>Let’s see them <span class="No-Break">in action:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.zeros(5)array([0., 0., 0., 0., 0.])&#13;
&gt;&gt;&gt; np.ones(5)&#13;
array([1., 1., 1., 1., 1.])&#13;
&gt;&gt;&gt; np.empty(5)&#13;
array([1., 1., 1., 1., 1.])&#13;
&gt;&gt;&gt; np.arange(5)&#13;
array([0, 1, 2, 3, 4])</pre>&#13;
			<p>Notice that the result of <strong class="source-inline">np.empty</strong> can vary: since the values in the array are not initialized, <em class="italic">they take whatever value there is currently in this memory block</em>. The main motivation behind this function is speed, allowing you to quickly allocate memory, but don’t forget to fill every <span class="No-Break">element after.</span></p>&#13;
			<p>By default, NumPy <a id="_idIndexMarker807"/>creates arrays with a floating-point type (<strong class="source-inline">float64</strong>). Once <a id="_idIndexMarker808"/>again, by using the <strong class="source-inline">dtype</strong> argument, you can force another type to <span class="No-Break">be used:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.ones(5, dtype=np.int32)array([1, 1, 1, 1, 1], dtype=int32)</pre>&#13;
			<p>NumPy provides a wide range of types, allowing you to finely optimize the memory consumption of your program by selecting the right type for your data. You ca<a id="_idTextAnchor836"/>n find the whole list of types supported by <a id="_idIndexMarker809"/>NumPy in the official <span class="No-Break">documentation: </span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases"><span class="No-Break">https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases</span></a><span class="No-Break">.</span></p>&#13;
			<p>NumPy also proposes a function to create an array with <span class="No-Break">random values:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.random.seed(0)  # Set the random seed to make examples reproducible&gt;&gt;&gt; np.random.randint(10, size=5)&#13;
array([<a id="_idTextAnchor837"/>5, 0, 3, 3, 7])</pre>&#13;
			<p>The first argument is the maximum range of the random value, and the <strong class="source-inline">size</strong> argument sets the number of values <span class="No-Break">to generate.</span></p>&#13;
			<p>Until now, we showed how to create one-dimensional arrays. Ho<a id="_idTextAnchor838"/>wever, the great strength of NumPy is that it natively handles multi-dimensional arrays! For example, let’s create a <em class="italic">3 x </em><span class="No-Break"><em class="italic">4</em></span><span class="No-Break"> matrix:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; m = np.ones((3,4))&gt;&gt;&gt; m&#13;
array([[1., 1., 1., 1.],&#13;
       [1., 1., 1., 1.],&#13;
       [1., 1., 1., 1.]])</pre>&#13;
			<p>NumPy did <a id="_idIndexMarker810"/>create an array with three rows and four columns! All we<a id="_idIndexMarker811"/> had to do was to pass a tuple to the NumPy function to specify our dimensions. When having such an array, NumPy gives us access to properties for knowing the number of dimensions, as well as the shape and size <span class="No-Break">of it:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; m.ndim2&#13;
&gt;&gt;&gt; m<a id="_idTextAnchor839"/>.shape&#13;
(3, 4)&#13;
&gt;&gt;&gt; m.size&#13;
12</pre>&#13;
			<h3>Accessing elements and sub-arrays</h3>&#13;
			<p>NumPy arrays closely<a id="_idIndexMarker812"/> follow the standard Python syntax to<a id="_idIndexMarker813"/> ma<a id="_idTextAnchor840"/>nipulate lists. Therefore, to access an element in a one-dimensional array, just do <span class="No-Break">the following:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; l = np.arange(5)&gt;&gt;&gt; l[2]&#13;
2</pre>&#13;
			<p>For multi-dimensional arrays, we just have to add <span class="No-Break">another index:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.random.seed(0)&gt;&gt;&gt; m = np.random.randint(10, size=(3,4))&#13;
&gt;&gt;&gt; m&#13;
array([[5, 0, 3, 3],&#13;
       [7, 9, 3, 5],&#13;
       [2, 4, 7, 6]])&#13;
&gt;&gt;&gt; m[1][2]&#13;
3</pre>&#13;
			<p>Of course, this can be used to <span class="No-Break">re-assign elements:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; m[1][2] = 42&gt;&gt;&gt; m&#13;
array([[ 5,  0,  3,  3],&#13;
       [ 7,  9, 42,  5],&#13;
       [ 2,  4,  7,  6]])</pre>&#13;
			<p>But that’s not all. Thanks to the slicing syntax, we can access sub-arrays wi<a id="_idTextAnchor841"/>th a start index, an end index, and even a step. For example, on a one-dimensional array, we can do <span class="No-Break">the following:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; l = np.arange(5)&gt;&gt;&gt; l&#13;
array([0, 1, 2, 3, 4])&#13;
&gt;&gt;&gt; l[1:4]  # From index 1 (inclusive) to 4 (exclusive)&#13;
array([1, 2, 3])&#13;
&gt;&gt;&gt; l[::2]  # Every second element&#13;
array([0, 2, 4])</pre>&#13;
			<p>This is exactly what we saw for standard Python lists in <a href="B19528_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Python Programming Specificities</em>. Of course, it also works for multi-dimensional arrays, with one slice for <span class="No-Break">each dimension:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.random.seed(0)&gt;&gt;&gt; m = np.random.randint(10, size=(3,4))&#13;
&gt;&gt;&gt; m&#13;
array([[5, 0, 3, 3],&#13;
       [7, 9, 3, 5],&#13;
       [2, 4, 7, 6]])&#13;
&gt;&gt;&gt; m[1:, 0:2]  # From row 1 to end and column 0 to 2&#13;
array([[7, 9],&#13;
       [2, 4]])&#13;
&gt;&gt;&gt; m[::, 3:]  # Every row, only last column&#13;
array([[3],&#13;
       [5],&#13;
       [6]])</pre>&#13;
			<p>You can assign <a id="_idIndexMarker814"/>those sub-arrays to variables. However, for performance reasons, NumPy doesn’t copy the values by default: it’s only a <strong class="bold">view</strong> (or shallow copy), a representation of <a id="_idIndexMarker815"/>the existing data. This is important to bear in mind because if you change a value in the view, it will also change the value in the <span class="No-Break">original array:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; v = m[::, 3:]&gt;&gt;&gt; v[0][0] = 42&#13;
&gt;&gt;&gt; v&#13;
array([[42],&#13;
       [ 5],&#13;
       [ 6]])&#13;
&gt;&gt;&gt; m&#13;
array([[ 5,  0,  3, 42],&#13;
       [ 7,  9,  3,  5],&#13;
       [ 2,  4,  7,  6]])</pre>&#13;
			<p>If you need to really<a id="_idTextAnchor842"/> <strong class="bold">copy</strong> the values in memory, you just have to use the <strong class="source-inline">copy</strong> method on <span class="No-Break">the array:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; m2 = m[::, 3:].copy()</pre>			<p><strong class="source-inline">m2</strong> is now a separate copy of <strong class="source-inline">m</strong>, and changes in its values won’t change the values <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">m</strong></span><span class="No-Break">.</span></p>&#13;
			<p>You now have the basics for handling arrays with NumPy. As we’ve seen, the syntax is very <a id="_idTextAnchor843"/>similar to standard Python. The key points to remember when working with NumPy are <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>NumPy <a id="_idIndexMarker816"/>arrays are of fixed types, meaning all items in the array are of the <span class="No-Break">same type</span></li>&#13;
				<li>NumPy natively handles<a id="_idIndexMarker817"/> multi-dimensional arrays and allows us to subset them using the standard <span class="No-Break">slicing notation</span></li>&#13;
			</ul>&#13;
			<p>Of course, NumPy can do much more than that: actually, it can apply common computations to those<a id="_idTextAnchor844"/> arrays in a very <span class="No-Break">performant way.</span></p>&#13;
			<h2 id="_idParaDest-184"><a id="_idTextAnchor845"/>Manipulating arrays with NumPy – computation, aggregations, and comparisons</h2>&#13;
			<p>As we said, NumPy is all <a id="_idIndexMarker818"/>about manipulating large arrays with great<a id="_idTextAnchor846"/><a id="_idIndexMarker819"/> performance and controlled memory consumpti<a id="_idTextAnchor847"/>on. Let’s say, for example, that we want to compute the double of each element in a large array. In the following example, you can see an implementation of such a function with a standard <span class="No-Break">Python loop:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_compare_operations.py</p>&#13;
			<pre class="source-code">&#13;
import numpy as npnp.random.seed(0)  # Set the random seed to make examples reproducible&#13;
m = np.random.randint(10, size=1000000)  # An array with a million of elements&#13;
def standard_double(array):&#13;
    output = np.empty(array.size)&#13;
    for i in range(array.size):&#13;
        output[i] = array[i] * 2&#13;
    return output</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py</a></p>&#13;
			<p>We instantiate an array with a million random integers. Then, we have our function building an array with the double of each element. Basically, we first instantiate an empty array of the same size before looping over each element to set <span class="No-Break">the double.</span></p>&#13;
			<p>Let’s measure the<a id="_idIndexMarker820"/> performance of this function. In Python, there <a id="_idIndexMarker821"/>is a standard module, <strong class="source-inline">timeit</strong>, dedicated to this purpose. We can use it directly from the command line and pass valid Python statements we want to measure performance for. The following command will measure the performance of <strong class="source-inline">standard_double</strong> with our <span class="No-Break">big array:</span></p>&#13;
			<pre class="source-code">&#13;
python -m timeit "from chapter11.chapter11_compare_operations import m, standard_double; standard_double(m)"1 loop, best of 5: 146 msec per loop</pre>&#13;
			<p>The results will vary depending on your machine, but the magnitude should be equivalent. What <strong class="source-inline">timeit</strong> does is repeat your code a certain number of times and measure its execution time. Here, our function took around 150 milliseconds to compute the double of each element in our array. For such simple computations on a modern computer, that’s not <span class="No-Break">very impressive.</span></p>&#13;
			<p>Let’s compare this with the equivalent operation using NumPy syntax. You can see it in the <span class="No-Break">next sample:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_compare_operations.py</p>&#13;
			<pre class="source-code">&#13;
def numpy_double(array):     return array * 2</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapte<span id="_idTextAnchor848"/>r11_compare_operations.py</a></p>&#13;
			<p>The code is<a id="_idIndexMarker822"/> much shorter<a id="_idTextAnchor849"/>! NumPy implements the basic <a id="_idIndexMarker823"/>arithmetic operations and can apply them to each element of the array. By multiplying the array by a value directly, we implicitly tell NumPy to multiply each element by this value. Let’s measure the performance <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">timeit</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
python -m timeit "from chapter11.chapter11_compare_operations import m, numpy_double; numpy_double(m)"500 loops, best of 5: 611 usec per loop</pre>&#13;
			<p>Here, the best loop achieved the computation in 600 microseconds! That’s 250 times faster than the previous function! How can we explain such a variation? In a standard loop, Python (because of its dynamic nature) has to check for the type of value at each iteration to apply the right function for this type, which adds significant overhead. With NumPy, the operation is deferred to an optimized and compiled loop where types are known ahead of time, which saves a lot of <span class="No-Break">useless checks.</span></p>&#13;
			<p>We once again see here the benefits of NumPy arrays over standard lists when working on a large dataset: it implements operations natively t<a id="_idTextAnchor850"/>o help you make computations <span class="No-Break">very fast.</span></p>&#13;
			<h3>Addi<a id="_idTextAnchor851"/>ng and multiplying arrays</h3>&#13;
			<p>As you saw in the previous example, NumPy supports the arithmetic operators to make operations <span class="No-Break">over arrays.</span></p>&#13;
			<p>This<a id="_idIndexMarker824"/> means that you can operate directly over two arrays of the <span class="No-Break">same dimensions:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.array([1, 2, 3]) + np.array([4, 5, 6])array([5, 7, 9])</pre>&#13;
			<p>In this case, NumPy <a id="_idIndexMarker825"/>applies the operation element-wise. But it also works in certain situations if one of the operands is not of the <span class="No-Break">same shape:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.array([1, 2, 3])<a id="_idTextAnchor852"/> * 2array([<a id="_idTextAnchor853"/>2, 4, 6])</pre>&#13;
			<p>NumPy automatically understands <a id="_idIndexMarker826"/>that it should multiply each element by two. This is called <strong class="bold">broadcasting</strong>: NumPy “expands” the smaller array to match the shape of the <a id="_idIndexMarker827"/>larger array. The previous example is equivalent to <span class="No-Break">this one:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.array([1, 2, 3]) * np.array([2, 2, 2])array([2, 4, 6])</pre>&#13;
			<p>Note that even if those two examples are conceptually equivalent, the first one is more memory-efficient and <a id="_idIndexMarker828"/>computationally efficient: NumPy is smart enough to use only one <strong class="source-inline">2</strong> value, without having to create a full array <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></p>&#13;
			<p>More generally, broadcasting works if the rightmost dimensions of the arrays are of the same size or if one of them is <strong class="source-inline">1</strong>. For example, we can add an array of dimensions <em class="italic">4 x 3</em> to an array of dimensions <em class="italic">1 </em><span class="No-Break"><em class="italic">x 3</em></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; a1 = np.ones((4, 3))&gt;&gt;&gt; a1&#13;
array([[1., 1., 1.],&#13;
       [1., 1., 1.],&#13;
       [1., 1., 1.],&#13;
       [1., 1., 1.]])&#13;
&gt;&gt;&gt; a2 = np.ones((1, 3))&#13;
&gt;&gt;&gt; a2&#13;
array([[1., 1., 1.]])&#13;
&gt;&gt;&gt; a1 + a2&#13;
array([[2., 2., 2.],&#13;
       [2., 2., 2.],&#13;
       [2., 2., 2<a id="_idTextAnchor854"/>.],&#13;
       [2., 2., 2.]])</pre>&#13;
			<p>However, adding an array of dimensions<a id="_idTextAnchor855"/> <em class="italic">4 x 3</em> to an array of dimensions <em class="italic">1 x 4</em> is <span class="No-Break">not possible:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; a3 = np.ones((1, 4))&gt;&gt;&gt; a3&#13;
array([[1., 1., 1., 1.]])&#13;
&gt;&gt;&gt; a1 + a3&#13;
Traceback (most recent call last):&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;&#13;
ValueError: operands could not be broadcast together with shapes (4,3) (1,4)</pre>&#13;
			<p>If this sounds complicated or confusing, that’s normal; it takes time to understand it conceptually, especially in three or more dimensions. For a mo<a id="_idTextAnchor856"/>re detailed explanation of the concept, take time to<a id="_idIndexMarker829"/> read the related article in the official <span class="No-Break">documentation: </span><a href="https://numpy.org/doc/stable/user/basics.broadcasting.html"><span class="No-Break">https://numpy.<span id="_idTextAnchor857"/><span id="_idTextAnchor858"/>org/doc/stable/user/basics.broadcasting.html</span></a><span class="No-Break">.</span></p>&#13;
			<h3>Aggregating arrays – sum, min, max, mean, and so on</h3>&#13;
			<p>When working with arrays, we<a id="_idIndexMarker830"/> often need to<a id="_idTextAnchor859"/> summarize the data to extract some meaningful statistics: the mean, the minimum, the maximum, and so on. Fortunately, NumPy also provides those operations natively. Quite simply, they are provided as methods that you can call directly from <span class="No-Break">an array:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; np.arange(10).mean()<a id="_idTextAnchor860"/>4.5&#13;
&gt;&gt;&gt; np.ones((4,4)).sum()&#13;
16.0</pre>&#13;
			<p>You can find the whole list of aggregating operations<a id="_idIndexMarker831"/> in the official <span class="No-Break">documentation: </span><a href="https://numpy.org/doc/stable/reference/arrays.ndarray.html#calculation"><span class="No-Break">https://numpy.org/doc/st<span id="_idTextAnchor861"/>able/reference/arrays.ndarray.html#calculation</span></a><span class="No-Break">.</span></p>&#13;
			<h3>Comparing arrays</h3>&#13;
			<p>NumPy also<a id="_idIndexMarker832"/> implements the standard comparison<a id="_idTextAnchor862"/> operators to compare arrays. As with arithmetic operators, which we saw in the <em class="italic">Adding and multipl</em><em class="italic">ying</em><em class="italic"> arrays</em> section, broadcasting rules apply. This means that you can compare an array with a <span class="No-Break">single value:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; l = np.array([1, 2, 3, 4])&gt;&gt;&gt; l &lt; 3&#13;
array([ True,  True, False, False])</pre>&#13;
			<p>And you can also compare arrays with arrays, given that they are compatible on the basis of the <span class="No-Break">broadcasting rules:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; m = np.array(    [[1., 5., 9., 13.],&#13;
    [2., 6., 10., 14.],&#13;
    [3., 7., 11., 15.],&#13;
    [4., 8., 12., 16.]]&#13;
)&#13;
&gt;&gt;&gt; m &lt;= np.array([1, 5, 9, 13])&#13;
array([[ True,  True,  True,  True],&#13;
       [False, False, False, False],&#13;
       [False, False, False, False],&#13;
       [False, False, False, False]])</pre>&#13;
			<p>The resulting array is filled with the Boolean result of the comparison for <span class="No-Break">each element.</span></p>&#13;
			<p>That’s it for this very quick <a id="_idIndexMarker833"/>introduction to Num<a id="_idTextAnchor863"/>Py. There is a lot more to know and discover with this library, so we strongly encourage you to read the official user <span class="No-Break">guide: </span><a href="https://numpy.org/doc/stable/user/index.html"><span class="No-Break">https://numpy.org/doc/stable/user/index.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>For the rest of this book, this should be enough for you to understand future examples. Let’s now have a look at a libr<a id="_idTextAnchor864"/>ary often cited and used alongside <span class="No-Break">NumPy: pandas.</span></p>&#13;
			<h2 id="_idParaDest-185"><a id="_idTextAnchor865"/>Getting started with pandas</h2>&#13;
			<p>In the previous section, we introduced <a id="_idIndexMarker834"/>NumPy and its ability to efficiently store and work with a large array of data. We’ll now introduce another<a id="_idTextAnchor866"/> widely used library in data science: pandas. This library is built on top of NumPy to provide convenient data structures able to efficiently store large datasets with <em class="italic">labeled rows and columns</em>. This is, of course, especially handy when working with most datasets representing real-world data that we want to analyze and use in data <span class="No-Break">science projects.</span></p>&#13;
			<p>To get started, we will, of course, install the library with the <span class="No-Break">usual command:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ pip install pandas</pre>			<p>Once done, we can start to use it in a <span class="No-Break">Python interpreter:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ python&gt;&gt;&gt; import pandas as pd</pre>&#13;
			<p>Just like we alias <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, the conv<a id="_idTextAnchor867"/>ention is to alias <strong class="source-inline">pandas</strong> as <strong class="source-inline">pd</strong> when <span class="No-Break">importing it.</span></p>&#13;
			<h3>Using panda<a id="_idTextAnchor868"/>s Series for one-dimensional data</h3>&#13;
			<p>The first pandas data<a id="_idIndexMarker835"/> structure we’ll introduce is <strong class="source-inline">Series</strong>. This <a id="_idIndexMarker836"/>data s<a id="_idTextAnchor869"/>tructure behaves very similarly to a one-dimensional array in NumPy. To create one, we can simply initialize it with a list <span class="No-Break">of values:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5])&gt;&gt;&gt; s&#13;
0    <a id="_idTextAnchor870"/>1&#13;
1    2&#13;
2    3&#13;
3    4&#13;
4    5&#13;
dtype: int64</pre>&#13;
			<p>Under the hood, pandas creates a NumPy array. As such, it uses the same data types to store the data. You can verify this by accessing the <strong class="source-inline">values</strong> property of the <strong class="source-inline">Series</strong> object and checking <span class="No-Break">its type:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; type(s.values)&lt;class 'numpy.ndarray'&gt;</pre>&#13;
			<p>Indexing and slicing work exactly the same way as <span class="No-Break">in NumPy:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s[0]1&#13;
&gt;&gt;&gt; s[1:3]&#13;
1    2&#13;
2    3&#13;
dtype: int64</pre>&#13;
			<p>So far, this is not <a id="_idIndexMarker837"/>very different fro<a id="_idTextAnchor871"/>m a regular NumPy<a id="_idIndexMarker838"/> array. As we said, the main purpose of pandas is to <em class="italic">label the data</em>. To allow this, pandas data structures maintain an index to allow this data labeling. It is accessible through the <span class="No-Break"><strong class="source-inline">index</strong></span><span class="No-Break"> property:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s.indexRangeIndex(start=0, stop=5, step=1)</pre>&#13;
			<p>Here, we have a simple range integer index, but we can actually have any arbitrary index. In the next example, we create the same series, labeling each value with <span class="No-Break">a letter:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s = pd.Series([1, 2, 3, 4, 5], index=["a", "b", "c", "d", "e"])&gt;&gt;&gt; s<a id="_idTextAnchor872"/>&#13;
a    1&#13;
b    2&#13;
c    3&#13;
d    4&#13;
e    5</pre>&#13;
			<p>The <strong class="source-inline">index</strong> argument on the <strong class="source-inline">Series</strong> initializer allows us to set the list of labels. We can now access values with those <span class="No-Break">labels instead:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s["c"]3</pre>&#13;
			<p>Surprisingly, even slicing notation works with those kinds <span class="No-Break">of labels:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s["b":"d"]b    2&#13;
c    3&#13;
d    4&#13;
dtype: int64</pre>&#13;
			<p>Under the hood, pandas<a id="_idIndexMarker839"/> keep the order of the index to allow<a id="_idIndexMarker840"/> such useful notations. Notice, however, that with this notation, the <em class="italic">last index is inclusive</em> (<strong class="source-inline">d</strong> is included in the result), unlike standard index notation, where the last index <span class="No-Break">is exclusive:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s[1:3]b    2&#13;
c    3&#13;
dtype: int64</pre>&#13;
			<p>To avoid confusion between those two styles, pandas exposes two special notations to explicitly indicate which indexing style you wish to use: <strong class="source-inline">loc</strong> (label notation with the last index being inclusive) and <strong class="source-inline">iloc</strong> (s<a id="_idTextAnchor873"/>tandard index notation). You can read more about this in the official<a id="_idIndexMarker841"/> <span class="No-Break">documentation: </span><a href="https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing"><span class="No-Break">https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing</span></a><span class="No-Break">.</span></p>&#13;
			<p><strong class="source-inline">Series</strong> can also be instantiated directly <span class="No-Break">from dictionaries:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; s = pd.Series({"a": 1, "b": 2, "c": 3, "d": 4, "e": 5})&gt;&gt;&gt; s&#13;
a    1&#13;
b    2&#13;
c    3&#13;
d    4&#13;
e    5&#13;
dtype: int64</pre>&#13;
			<p>In this case, the keys of the dictionaries are used <span class="No-Break">as labels.</span></p>&#13;
			<p>Of course, in the<a id="_idIndexMarker842"/> real world, you’ll more likely have to <a id="_idIndexMarker843"/>work with two-dimensional (o<a id="_idTextAnchor874"/><a id="_idTextAnchor875"/>r more!) datasets. This is exactly what DataFrames <span class="No-Break">are for!</span></p>&#13;
			<h3>Using pandas DataFrames for multi-dimensional data</h3>&#13;
			<p>Most of the<a id="_idIndexMarker844"/> time, datasets consist<a id="_idTextAnchor876"/> of<a id="_idIndexMarker845"/> two-dimensional data, where you have several columns for each row, as in a classic spreadsheet application. In Pandas, DataFrames are designed to work with this kind of data. As for <strong class="source-inline">Series</strong>, it can work with a large set of<a id="_idTextAnchor877"/> data that is labeled both by rows <span class="No-Break">and columns.</span></p>&#13;
			<p>The following examples will use a tiny dataset representing the number of tickets (paid and free) delivered in French museums in 2018. Let’s consider we have this data in the form of <span class="No-Break">two dictionaries:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; paid = {"Louvre Museum": 5988065, "Orsay Museum": 1850092, "Pompidou Centre": 2620481, "National Natural History Museum": 404497}&gt;&gt;&gt; free = {"Louvre Museum": 4117897, "Orsay Museum": 1436132, "Pompidou Centre": 1070337, "National Natural History Museum": 344572}</pre>&#13;
			<p>Each key in those dictionaries is a label for a row. We can build a DataFrame directly from those two dictionaries <span class="No-Break">like this:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums = pd.DataFrame({"paid": paid, "free": free})&gt;&gt;&gt; museums&#13;
                                    paid     free&#13;
Louvre Museum                    5988065  4117897&#13;
Orsay Museum                     1850092  1436132&#13;
Pompidou Centre                  2620481  1070337&#13;
National Natural History Museum   404497   344572</pre>&#13;
			<p>The <strong class="source-inline">DataFrame</strong> initializer accepts a dictionary of dic<a id="_idTextAnchor878"/>tionaries, where keys represent the label for <span class="No-Break">the columns.</span></p>&#13;
			<p>We can have a <a id="_idIndexMarker846"/>look at the <strong class="source-inline">index</strong> property, storing <a id="_idIndexMarker847"/>the rows index, and the <strong class="source-inline">columns</strong> property, storing the <span class="No-Break">columns index:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums.indexIndex(['Louvre Museum', 'Orsay Museum', 'Pompidou Centre',&#13;
       'National Natural History Museum'],&#13;
      dtype='object')&#13;
&gt;&gt;&gt; museums<a id="_idTextAnchor879"/>.columns&#13;
Index(['paid', 'free'], dtype='object')</pre>&#13;
			<p>Once again, we can now use indexing and slicing notation to get subsets of columns <span class="No-Break">or rows:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums["free"]Louvre Museum                      4117897&#13;
Orsay Museum                       1436132&#13;
Pompidou Centre                    1070337&#13;
National Natural History Museum     344572&#13;
Name: free, dtype: int64&#13;
&gt;&gt;&gt; museums["Louvre Museum":"Orsay Museum"]&#13;
                  paid     free&#13;
Louvre Museum  5988065  4117897&#13;
Orsay Museum   1850092  1436132&#13;
&gt;&gt;&gt; museums["Louvre Museum":"Orsay Museum"]["paid"]&#13;
Louvre Museum    5988<a id="_idTextAnchor880"/>065&#13;
Orsay Museum     1850092&#13;
Name: paid, dtype: int64</pre>&#13;
			<p>Something that <a id="_idIndexMarker848"/>is even more powerful: you<a id="_idIndexMarker849"/> can write<a id="_idTextAnchor881"/> a Boolean condition<a id="_idIndexMarker850"/> inside the brackets to match some data. This operation is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">masking</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums[museums["paid"] &gt; 2000000]                    paid     free&#13;
Louvre Museum    5<a id="_idTextAnchor882"/>988065  4117897&#13;
Pompidou Centre  2620481  1070337</pre>&#13;
			<p>Finally, you can easily set new columns with this very same <span class="No-Break">indexing notation:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums["total"] = museums["paid"] + museums["free"]&gt;&gt;&gt; museums&#13;
                                    paid     free     total&#13;
Louvre Museum                    5988065  4117897  10105962&#13;
Orsay Museum                     1850092  1436132   3286224&#13;
Pompidou Centre                  2620481  1070337   3690818&#13;
National Natural History Museum   404497   344572    749069</pre>&#13;
			<p>As you can see, just like NumPy arrays, pandas fully supports arithmetic operations over <span class="No-Break">two DataFrames.</span></p>&#13;
			<p>Of course, all the <a id="_idIndexMarker851"/>basic aggregation <a id="_idIndexMarker852"/>operations are supported, including <strong class="source-inline">mean</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">sum</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums["total"].sum()17832073&#13;
&gt;&gt;&gt; museums["total"].mean()&#13;
4458018.25</pre>&#13;
			<p>You can find the whole list of operations<a id="_idIndexMarker853"/> available in the official <span class="No-Break">documentation: </span><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#descriptive-statistics"><span class="No-Break">https://pandas.pydata.<span id="_idTextAnchor883"/><span id="_idTextAnchor884"/>org/pandas-docs/stable/user_guide/basics.html#descriptive-statistics</span></a><span class="No-Break">.</span></p>&#13;
			<h3>Importing and exporting CSV data</h3>&#13;
			<p>One very common way of sharing datasets is through <a id="_idIndexMarker854"/>CSV files. This format is very convenient because it only consists of a simple text file, each line representing a row of data, with each column separat<a id="_idTextAnchor885"/>ed by a comma. Our simple <em class="italic">museums</em> dataset is available in the examples repository as a CSV file, which you can see in the <span class="No-Break">next sample:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">museums.csv</p>&#13;
			<pre class="source-code">&#13;
name,paid,freeLouvre Museum,5988065,4117897&#13;
Orsay Museum,1850092,1436132&#13;
Pompidou Centre,2620481,1070337&#13;
National Natural History Museum,404497,344572</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/museums.csv">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Ed<span id="_idTextAnchor886"/>ition/tree/main/chapter11/museums.csv</a></p>&#13;
			<p>Importing CSV files is so <a id="_idIndexMarker855"/>common that pandas provides a function to load a CSV file into a <span class="No-Break">DataFrame directly:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums = pd.read_csv("./chapter11/museums.csv", index_col=0)&gt;&gt;&gt; museums&#13;
                                    paid     free&#13;
name&#13;
Louvre Museum                    5988065  4117897&#13;
Orsay Museum                     1850092  1436132&#13;
Pompidou Centre                  2620481  1070337&#13;
National Natural History Museum   404497   344572</pre>&#13;
			<p>The function simply expects the<a id="_idIndexMarker856"/><a id="_idTextAnchor887"/> path to the CSV file. Several arguments are available to finely control the operation: here, we used <strong class="source-inline">index_col</strong> to specify the index of the column that should be used as row labels. You can find the whole list of arguments in the official <span class="No-Break">documentation: </span><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"><span class="No-Break">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>Of course, the opposite operation exists to export a DataFrame to a <span class="No-Break">CSV file:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; museums["total"] = museums["paid"] + museu<a id="_idTextAnchor888"/>ms["free"]&gt;&gt;&gt; museums.to_csv("museums_with_total.csv")</pre>&#13;
			<p>We will conclude this very quick introduction to pandas here. Of course, we’ve only <a id="_idTextAnchor889"/>covered the tip of the iceberg, and we recommend that you go through the official user guide to know <span class="No-Break">more: </span><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html"><span class="No-Break">https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>Still, you should now be able to perform basic operations and operate efficiently on large datasets. In the next section, we’ll introduce scikit-learn, one of the fundamental Python toolkits for data science, and you’ll see that it relies a lot on NumPy <span class="No-Break">and pandas.</span></p>&#13;
			<h1 id="_idParaDest-186"><a id="_idTextAnchor890"/>Training models with scikit-learn</h1>&#13;
			<p>scikit-learn is<a id="_idIndexMarker857"/> one of the most widely used Python libraries for data science. It implements dozens of classic ML models, but also numerous tools to help you while training them, such as preprocessing methods and cross-validation. Nowadays, you’ll probably hear about more modern approaches, such as PyTorch, but scikit-learn is still a solid tool for a lot of <span class="No-Break">use cases.</span></p>&#13;
			<p>The first thing you must<a id="_idIndexMarker858"/> do to get started is to install it in your <span class="No-Break">Python</span><span class="No-Break"><a id="_idIndexMarker859"/></span><span class="No-Break"> environment:</span></p>&#13;
			<pre class="source-code">&#13;
(<a id="_idTextAnchor891"/>venv) $ pip install scikit-learn</pre>			<p>We can now start our <span class="No-Break">sci<a id="_idTextAnchor892"/>kit-<a id="_idTextAnchor893"/>learn journ<a id="_idTextAnchor894"/>ey!</span></p>&#13;
			<h2 id="_idParaDest-187"><a id="_idTextAnchor895"/>Training models and predicting</h2>&#13;
			<p>In sc<a id="_idTextAnchor896"/>ikit-l<a id="_idTextAnchor897"/>earn, ML models and algorithms are <a id="_idIndexMarker860"/>called <strong class="bold">estimators</strong>. Each is a Python class that implements the same methods. In particular, we have <strong class="source-inline">fit</strong>, which is used to train a model, and <strong class="source-inline">predict</strong>, which is used to run the trained model on <span class="No-Break">new data.</span></p>&#13;
			<p>To try this, we’ll load a sample dataset. s<a id="_idTextAnchor898"/>cikit-learn comes with a few toy datasets that are very useful for<a id="_idIndexMarker861"/> performing experiments. You can find out more about them in the official <span class="No-Break">documentation: </span><a href="https://scikit-learn.org/stable/datasets.html"><span class="No-Break">https://scikit-learn.org/stable/datasets.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>Here, we’ll use the <em class="italic">digits</em> dataset, a collection of pixel matrices representing handwritten digits. As you may have guessed, the goal of this dataset is to train a model to automatically recognize handwritten digits. The following example shows how to load <span class="No-Break">this dataset:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_load_digits.py</p>&#13;
			<pre class="source-code">&#13;
from sklearn.datasets import load_digitsdigits = load_digits()&#13;
data = digits.data&#13;
targets = digits.target&#13;
print(data[0].reshape((8, 8)))  # First handwritten digit 8 x 8 matrix&#13;
print(targets[0])  # Label of first handwritten digit</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_load_digits.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/<span id="_idTextAnchor899"/>chapter11<span id="_idTextAnchor900"/>/chap<span id="_idTextAnchor901"/>ter11_load_digits.py</a></p>&#13;
			<p><a id="_idTextAnchor902"/>Notice that the toy dataset’s functions are imported from the <strong class="source-inline">datasets</strong> package of scikit-learn. The <strong class="source-inline">load_digits</strong> function returns an object that contains the data and <span class="No-Break">some metadata.</span></p>&#13;
			<p>The most interesting parts of this object are <strong class="source-inline">data</strong>, which contains the handwritten digit pixels matrices, and <strong class="source-inline">targets</strong>, which contains the corresponding label for those digits. Both are <span class="No-Break">NumPy arrays.</span></p>&#13;
			<p>To get a grasp of what this looks like, we will take the first digit in the data and reshape it into an 8 x 8 matrix; this is the size of the source images. Each value represents a pixel on a grayscale, from 0 <span class="No-Break">to 16.</span></p>&#13;
			<p>Then, we print the <a id="_idIndexMarker862"/>label of this first digit, which is <strong class="source-inline">0</strong>. If you run this code, you’ll get <a id="_idIndexMarker863"/>the <span class="No-Break">following output:</span></p>&#13;
			<pre class="source-code">&#13;
[[ 0.  0.  5. 13.  9.  1.  0.  0.] [ 0.  0. 13. 15. 10. 15.  5.  0.]&#13;
 [ 0.  3. 15.  2.  0. 11.  8.  0.]&#13;
 [ 0.  4. 12.  0.  0.  8.  8.  0.]&#13;
 [ 0.  5.  8.  0.  0.  9.  8.  0.]&#13;
 [ 0.  4. 11.  0.  1. 12.  7.  0.]&#13;
 [ 0.  2. 14.  5. 10. 12.  0.  0.]&#13;
 [ 0.  0.  6. 13. 10.  0.  0.  0.]]&#13;
0</pre>&#13;
			<p>Some<a id="_idTextAnchor903"/>how, we <a id="_idTextAnchor904"/>can guess the sh<a id="_idTextAnchor905"/>ape of the z<a id="_idTextAnchor906"/>ero from <span class="No-Break">the matrix.</span></p>&#13;
			<p>Now, let’s try to build a model that recognizes handwritten digits. To start simple, we’ll use a Gaussian Naive Bayes model, a <a id="_idIndexMarker864"/>classic and easy-to-use algorithm that can quickly yield good results. The<a id="_idIndexMarker865"/> following example shows the <span class="No-Break">entire process:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_fit_predict.py</p>&#13;
			<pre class="source-code">&#13;
from sklearn.datasets import load_digitsfrom sklearn.metrics import accuracy_score&#13;
from sklearn.model_selection import train_test_split&#13;
from sklearn.naive_bayes import GaussianNB&#13;
digits = load_digits()&#13;
data = digits.data&#13;
targets = digits.target&#13;
# Split into training and testing sets&#13;
training_data, testing_data, training_targets, testing_targets = train_test_split(&#13;
     data, targets, random_state=0&#13;
)&#13;
# Train the model&#13;
model = GaussianNB()&#13;
model.fit(training_data, training_targets)&#13;
# Run prediction with the testing set&#13;
predicted_targets = model.predict(testing_data)&#13;
# Compute the accuracy&#13;
accuracy = accuracy_score(testing_targets, predicted_targets)&#13;
print(accuracy)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_fit_predict.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI<span id="_idTextAnchor907"/>-Sec<span id="_idTextAnchor908"/>ond-Edition/tree/main/chapter11/ch<span id="_idTextAnchor909"/>apter<span id="_idTextAnchor910"/>11_fit_predict.py</a></p>&#13;
			<p>Now that we’ve <a id="_idIndexMarker866"/>loaded the dataset, you can see that we take care of splitting it into a training and a testing set. As we mentioned in the <em class="italic">Model validation</em> section, this is <a id="_idIndexMarker867"/>essential for computing meaningful accuracy scores to check how our <span class="No-Break">model performs.</span></p>&#13;
			<p>To do this, we can rely on the <strong class="source-inline">train_test_split</strong> function, which is provided in the <strong class="source-inline">model_selection</strong> package. It selects random instances from our dataset to form the two sets. By default, it keeps 25% of the<a id="_idTextAnchor911"/> data to create a testing set, but this can be customized. The <strong class="source-inline">random_state</strong> argument allows us to set the random seed to make the example reproducible. You can find out more about this function in the official <span class="No-Break">documentation: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn-model-selection-train-test-split"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn-model-selection-train-test-split</span></a><span class="No-Break">.</span></p>&#13;
			<p>Then, we must instantiate the <strong class="source-inline">GaussianNB</strong> class. This class is one of the numerous ML estimators that’s implemented in scikit-learn. Each has its own set of parameters, to finely tune the behavior of the algorithm. However, scikit-learn is designed to provide sensible defaults for all the estimators, so it’s usually good to start with the defaults before tinkering <span class="No-Break">with them.</span></p>&#13;
			<p>After that, we must call the <strong class="source-inline">fit</strong> method to train our model. It expects an argument and two arrays: the first one is the actual data, with all its features, while the second one is the corresponding labels. And that’s it! You’ve trained your first <span class="No-Break">ML model!</span></p>&#13;
			<p>Now, let’s see how it behaves: we’ll call <strong class="source-inline">predict</strong> on our model with the testing set so that it automatically classifies the digits of the testing set. The result of this is a new array with the <span class="No-Break">predicted labels.</span></p>&#13;
			<p>All we have to do now is compare it with the actual labels of our testing set. Once again, scikit-le<a id="_idTextAnchor912"/>arn helps<a id="_idTextAnchor913"/> by pro<a id="_idTextAnchor914"/>vidin<a id="_idTextAnchor915"/>g the <strong class="source-inline">accuracy_score</strong> function in the <strong class="source-inline">metrics</strong> package. The first argument is the true labels, while the second is the <span class="No-Break">predicted labels.</span></p>&#13;
			<p>If you run this code, you’ll get an accuracy score of around 83%. That isn’t too bad for a first approach! As you have seen, training and running prediction on an ML model is straightforward <span class="No-Break">with scikit-learn.</span></p>&#13;
			<p>In practice, we <a id="_idIndexMarker868"/>often need to perform preprocessing steps on the <a id="_idIndexMarker869"/>data before feeding it to an estimator. Rather than doing this sequentially by hand, <a id="_idTextAnchor916"/>scikit-learn proposes a convenient feature that can automate this <a id="_idIndexMarker870"/><span class="No-Break">process: </span><span class="No-Break"><strong class="bold">pipelines</strong></span><span class="No-Break">.</span></p>&#13;
			<h2 id="_idParaDest-188">Chai<a id="_idTextAnchor917"/>ning prepr<a id="_idTextAnchor918"/>ocessors and es<a id="_idTextAnchor919"/>timators<a id="_idTextAnchor920"/> with pipelines</h2>&#13;
			<p>Q<a id="_idTextAnchor921"/>uite often, you’ll need to <a id="_idIndexMarker871"/>preprocess your data so that it can be <a id="_idIndexMarker872"/>used by the estimator you wish to use. Typically, you’ll want to transform an image into an array of pixel values or, as we’ll see in the <a id="_idIndexMarker873"/>following example, transform raw text into<a id="_idIndexMarker874"/> numerical values so that we can apply some math <span class="No-Break">to them.</span></p>&#13;
			<p>Rather than writing those st<a id="_idTextAnchor922"/>eps by hand, scikit-learn proposes a feature that can automatically chain preprocessors and estimators: pipelines. Once created, they expose the very same interface as any other estimator, allowing you to run training and prediction in <span class="No-Break">one operation.</span></p>&#13;
			<p>To show you what this looks like, we’ll look at an example of another classic dataset, the <em class="italic">20 newsgroups</em> text dataset. It consists of 18,000 newsgroup articles categorized into 20 topics. The goal of this dataset is to build a model that will automatically categorize an article in one of <span class="No-Break">those topics.</span></p>&#13;
			<p>The following example shows how we can load this data thanks to the <span class="No-Break"><strong class="source-inline">fetch_20newsgroups</strong></span><span class="No-Break"> function:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_pipelines.py</p>&#13;
			<pre class="source-code">&#13;
import pandas as pdfrom sklearn.datasets import fetch_20newsgroups&#13;
from sklearn.feature_extraction.text import TfidfVectorizer&#13;
from sklearn.metrics import accuracy_score, confusion_matrix&#13;
from sklearn.naive_bayes import MultinomialNB&#13;
from sklearn.pipeline import make_pipeline&#13;
# Load some categories of newsgroups dataset&#13;
categories = [&#13;
     "soc.religion.christian",&#13;
     "talk.religion.misc",&#13;
     "comp.sys.mac.hardware",&#13;
     "sci.crypt",&#13;
]&#13;
newsgroups_training = fetch_20newsgroups(&#13;
     subset="train", categories=categories, random_state=0&#13;
)&#13;
newsgroups_testing = fetch_20newsgroups(&#13;
     subset="test", categories=categories, random_state=0&#13;
)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py">https://github.com/PacktPublishing/Building-Data-Scienc<span id="_idTextAnchor923"/>e-Applicatio<span id="_idTextAnchor924"/>ns-with-Fa<span id="_idTextAnchor925"/>stAPI-Seco<span id="_idTextAnchor926"/>nd-Edition/tr<span id="_idTextAnchor927"/>ee/main/chapter11/chapter11_pipelines.py</a></p>&#13;
			<p>Since the <a id="_idIndexMarker875"/>dataset is rather large, we’ll only load a subset of <a id="_idIndexMarker876"/>the categories. Also, notice that it’s already<a id="_idIndexMarker877"/> been <a id="_idIndexMarker878"/>split into training and testing sets, so we only h<a id="_idTextAnchor928"/>ave to load them with the corresponding argument. You can find out more about the functionality of this dataset in the official <span class="No-Break">documentation: </span><a href="https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset"><span class="No-Break">https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset</span></a><span class="No-Break">.</span></p>&#13;
			<p>Before moving on, it’s important to understand what the underlying data is. Actually, this is the raw text of an article. You can check this by printing one of the samples in <span class="No-Break">the data:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; newsgroups_training.data[0]"From: sandvik@newton.apple.com (Kent Sandvik)\nSubject: Re: Ignorance is BLISS, was Is it good that Jesus died?\nOrganization: Cookamunga Tourist Bureau\nLines: 17\n\nIn article &lt;f1682Ap@quack.kfu.com&gt;, pharvey@quack.kfu.com (Paul Harvey)\nwrote:\n&gt; In article &lt;sandvik-170493104859@sandvik-kent.apple.com&gt; \n&gt; sandvik@newton.apple.com (Kent Sandvik) writes:\n&gt; &gt;Ignorance is not bliss!\n \n&gt; Ignorance is STRENGTH!\n&gt; Help spread the TRUTH of IGNORANCE!\n\nHuh, if ignorance is strength, then I won't distribute this piece\nof information if I want to follow your advice (contradiction above).\n\n\nCheers,\nKent\n---\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\n"</pre>&#13;
			<p>So, we <a id="_idIndexMarker879"/>need to extract some features from this text<a id="_idIndexMarker880"/> befor<a id="_idTextAnchor929"/>e feeding it to an estimator. A <a id="_idIndexMarker881"/>common approach for this when working with textual <a id="_idIndexMarker882"/>data is to use the <strong class="bold">Ter<a id="_idTextAnchor930"/>m Frequency-Inverse Document Frequency (TF-IDF)</strong>. Without going into too much detail, this technique will count the <a id="_idIndexMarker883"/>occurrences of each word in all the documents (term frequency), weighted by the importance of this word in every document (inverse document frequency). The idea is to give more weight to rarer words, which<a id="_idTextAnchor931"/> should convey more sense than frequent words such as “the.” You can find out more about this in the scikit-learn<a id="_idIndexMarker884"/> <span class="No-Break">documentation: </span><a href="https://scikit-learn.org/dev/modules/feature_extraction.html#tfidf-term-weighting"><span class="No-Break">https://scikit-learn.org/dev/modules/feature_extraction.html#tfidf-term-weighting</span></a><span class="No-Break">.</span></p>&#13;
			<p>This operation consists of splitting each w<a id="_idTextAnchor932"/>ord in the text samples and counting them. Usually, we apply a lot of techniques to refine this, such as <a id="_idIndexMarker885"/>removing <strong class="bold">stop words</strong> (common words such as “and” or “is” that don’t bring much informatio<a id="_idTextAnchor933"/>n). <a id="_idTextAnchor934"/>Fortunately, scikit-learn provides an<a id="_idTextAnchor935"/> all-in-on<a id="_idTextAnchor936"/>e tool for <span class="No-Break">this: </span><span class="No-Break"><strong class="source-inline">TfidfVectorizer</strong></span><span class="No-Break">.</span></p>&#13;
			<p>This preprocessor can take an array of text, tokenize each word, and compute the TF-IDF for each of them. A lot of options are available for finely tuning its behavior, but the defaults are a good start for English text. The following example shows how to use it with an estimator in <span class="No-Break">a pipeline:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_pipelines.py</p>&#13;
			<pre class="source-code">&#13;
# Make the pipelinemodel = make_pipeline(&#13;
     TfidfVectorizer(),&#13;
     MultinomialNB(),&#13;
)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py">https://github.com/PacktPublishing/Building-Data-Science-Applicat<span id="_idTextAnchor937"/>ions-wit<span id="_idTextAnchor938"/>h-Fa<span id="_idTextAnchor939"/>stAPI-S<span id="_idTextAnchor940"/>econd-Edition/tree/main/chapter11/chapter11_pipelines.py</a></p>&#13;
			<p>The <strong class="source-inline">make_pipeline</strong> function accepts any number of preprocessors and an estimator in its argument. Here, we’re using the Multinomial Naive Bayes classifier, which is suitable for features <span class="No-Break">representing frequency.</span></p>&#13;
			<p>Then, we <a id="_idIndexMarker886"/>can simply train our model and run prediction to <a id="_idIndexMarker887"/>check its accuracy, as we did previously. You <a id="_idIndexMarker888"/>can <a id="_idIndexMarker889"/>see this in the <span class="No-Break">following example:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_pipelines.py</p>&#13;
			<pre class="source-code">&#13;
# Train the modelmodel.fit(newsgroups_training.data, newsgroups_training.target)&#13;
# Run prediction with the testing set&#13;
predicted_targets = model.predict(newsgroups_testing.data)&#13;
# Compute the accuracy&#13;
accuracy = accuracy_score(newsgroups_testing.target, predicted_targets)&#13;
print(accuracy)&#13;
# Show the confusion matrix&#13;
confusion = confusion_matrix(newsgroups_testing.target, predicted_targets)&#13;
confusion_df = pd.DataFrame(&#13;
     confusion,&#13;
     index=pd.Index(newsgroups_testing.target_names, name="True"),&#13;
     columns=pd.Index(newsgroups_testing.target_names, name="Predicted"),&#13;
)&#13;
print(confusion_df)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py">https://github.com/PacktPublishing/Building-Data-Science-Appli<span id="_idTextAnchor941"/>cations-with<span id="_idTextAnchor942"/>-FastAPI-Secon<span id="_idTextAnchor943"/>d-Edition/tree/main/c<span id="_idTextAnchor944"/>hapter11/chapte<span id="_idTextAnchor945"/>r11_pipelines.py</a></p>&#13;
			<p>Notice that<a id="_idIndexMarker890"/> we also printed a confusion matrix, which is <a id="_idIndexMarker891"/>a very convenient representation of the <a id="_idIndexMarker892"/>global results. Scikit-learn has a dedicated<a id="_idIndexMarker893"/> function for this called <strong class="source-inline">confusion_matrix</strong>. Then, we wrap the result in a pandas DataFrame so that we can set the axis labels to improve readability. If you run this example, you’ll get an output similar to what’s shown in the following screenshot. Depending on your machine and system, it could take a couple of minutes <span class="No-Break">to run:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer056" class="IMG---Figure">&#13;
					<img src="Images/Figure_11.3_B19528.jpg" alt="Figure 11.3 – Confusion matrix on the 20 newsgroups dataset" width="1474" height="421"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Confusion matrix on the 20 newsgroups dataset</p>&#13;
			<p>Here, you can see that our results weren’t too bad for our first try. Notice that there is one big area of confusion between the <strong class="source-inline">soc.religion.christian</strong> and <a id="_idTextAnchor946"/><strong class="source-inline">talk.religi<a id="_idTextAnchor947"/>on.misc</strong> categories, <a id="_idTextAnchor948"/>which is<a id="_idTextAnchor949"/> not very surprising, give<a id="_idTextAnchor950"/>n <span class="No-Break">their similarity.</span></p>&#13;
			<p>As you’ve <a id="_idIndexMarker894"/>seen, building a pipeline with a preprocessor is <a id="_idIndexMarker895"/>very straightforward. The nice thing about this is<a id="_idIndexMarker896"/> that it automatically applies it to the <a id="_idIndexMarker897"/>training data, but also when you’re predi<a id="_idTextAnchor951"/>cting <span class="No-Break">the results.</span></p>&#13;
			<p>Before moving on, let’s look at one more important feature of <span class="No-Break">scikit-l<a id="_idTextAnchor952"/>earn: cross-validatio<a id="_idTextAnchor953"/>n.</span></p>&#13;
			<h2 id="_idParaDest-189"><a id="_idTextAnchor954"/>Validating the model with cross-validation</h2>&#13;
			<p>In the <em class="italic">Model validation</em> section, we<a id="_idIndexMarker898"/> introduced the cross-validation <a id="_idIndexMarker899"/>technique, which allows us to use data in training or testing sets. As you may have guessed, this technique is so common that it’s implemented natively <span class="No-Break">in scikit-learn!</span></p>&#13;
			<p>Let’s take another look at the handwritten digit example and <span class="No-Break">apply cross-validation:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter11_cross_validation.py</p>&#13;
			<pre class="source-code">&#13;
from sklearn.datasets import load_digitsfrom sklearn.model_selection import cross_val_score&#13;
from sklearn.naive_bayes import GaussianNB&#13;
digits = load_digits()&#13;
data = digits.data&#13;
targets = digits.target&#13;
# Create the model&#13;
model = GaussianNB()&#13;
# Run cross-validation&#13;
score = cross_val_score(model, data, targets)&#13;
print(score)&#13;
print(score.mean())</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_cross_validation.py">https://github.com/PacktPublishing/Building-Data-Science-<span id="_idTextAnchor955"/>Applicati<span id="_idTextAnchor956"/>ons-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_cross_validation.py</a></p>&#13;
			<p>This time, we <a id="_idIndexMarker900"/>don’t have to split the data ourselves: the <strong class="source-inline">cross_val_score</strong> function performs the folds automatically. In argument, it <a id="_idIndexMarker901"/>expects the estimator, <strong class="source-inline">data</strong>, which contains the handwritten digits’ pixels matrices, an<a id="_idTextAnchor957"/>d <strong class="source-inline">targets</strong>, which contains the corresponding label for those digits. By default, it performs <span class="No-Break">five folds.</span></p>&#13;
			<p>The result of this operation is an array that provides the accuracy score of the five folds. To get a global overview of this result, we can take, for example, the mean. If you run this example, you’ll get the <span class="No-Break">following output:</span></p>&#13;
			<pre class="source-code">&#13;
python chapter11/chapter11_cross_validation.py[0.78055556 0.78333333 0.79387187 0.8718663  0.80501393]&#13;
0.8069281956050759</pre>&#13;
			<p>As you can see, our <a id="_idIndexMarker902"/>mean accuracy is around 80%, which is a<a id="_idIndexMarker903"/> bit lower than the 83% we obtained with single training and testing sets. That’s the main benefit of cross-validation: we obtain a more statistically accurate metric regarding the performance of <span class="No-Break">our model.</span></p>&#13;
			<p>With that, you have learned the basics of working with scikit-learn. It’s obviously a v<a id="_idTextAnchor958"/>ery quick introduction to this vast framework, but it’ll give you the keys to train and evaluate your first <span class="No-Break">ML models.</span></p>&#13;
			<h1 id="_idParaDest-190"><a id="_idTextAnchor959"/>Summary</h1>&#13;
			<p>Congratulations! You’ve discovered the basic concepts of ML and made your first experiments with the fundamental toolkits of the data scientist. Now, you should be able to explore your first data science problems in Python. Of course, this was by no means a complete lesson on ML: the field is vast and there are tons of algorithms and techniques to explore. However, I hope that this has sparked your curiosity and that you’ll deepen your knowledge of <span class="No-Break">this subject.</span></p>&#13;
			<p>Now, it’s time to get back to FastAPI! With our new ML tools at hand, we’ll be able to leverage the power of FastAPI to serve our estimators and propose a reliable and efficient prediction API to <span class="No-Break">our users.</span></p>&#13;
		</div>&#13;
	</div></body></html>