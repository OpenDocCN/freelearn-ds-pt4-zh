- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Data Science in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, Python has gained a lot of popularity in the data science field.
    Its very efficient and readable syntax makes the language a very good choice for
    scientific research, while still being suitable for production workloads; it’s
    very easy to deploy research projects into real applications that will bring value
    to users. Thanks to this growing interest, a lot of specialized Python libraries
    have emerged and are now standards in the industry. In this chapter, we’ll introduce
    the fundamental concepts of machine learning before diving into the Python libraries
    used daily by data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic concepts of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and manipulating NumPy arrays and pandas datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating machine learning models with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you’ll require a Python virtual environment, just as we set
    up in [*Chapter 1*](B19528_01.xhtml#_idTextAnchor024), *Python Development* *Environment
    Setup*.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find all the code examples for this chapter in the dedicated GitHub repository
    at [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is often seen as a subfield of artificial intelligence.
    While this categorization is the subject of debate, ML has had a lot of exposure
    in recent years due to its vast and visible field of applications, such as spam
    filters, natural language processing, and image generation.'
  prefs: []
  type: TYPE_NORMAL
- en: ML is a field where we build mathematical models from existing data so that
    the machine can understand this data by itself. The machine is “learning” in the
    sense that the developer doesn’t have to program a step-by-step algorithm to solve
    the problem, which would be impossible for complex tasks. Once a model has been
    “trained” on existing data, it can be used to predict new data or understand new
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the spam filter example: if we have a sufficiently large collection
    of emails manually labeled “spam” or “not spam,” we can use ML techniques to build
    a model that can tell us whether a new incoming email is spam or not.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll review the most fundamental concepts of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised versus unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML techniques can be divided into two main categories: **supervised learning**
    and **unsupervised learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With supervised learning, the existing dataset is already labeled, which means
    we have both the input (the characteristics of an observation), known as **features**,
    and the output. If we consider the spam filter example here, the features could
    be the frequencies of each word and the **label** could be the category – that
    is, “spam” or “not spam.” Supervised learning is subdivided into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification problems**, to classify data with a finite set of categories
    – for example, the spam filter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression problems**, to predict continuous numerical values – for example,
    the number of rented electric scooters, given the day of the week, the weather,
    and the location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unsupervised learning, on the other hand, operates on data without any reference
    to a label. The goal here is to discover interesting patterns from the features
    themselves. The two main problems that unsupervised learning tries to solve are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**, where we want to find groups of similar data points – for example,
    a recommender system to suggest products that you might like, given what other
    people similar to you like.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**, where the goal is to find a more compact representation
    of datasets that contain a lot of different features. Doing this will allow us
    to keep only the most meaningful and discriminant features while working with
    smaller dataset dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key aspects of ML is evaluating whether your model is performing
    well or not. How can you say that your model will perform well on newly observed
    data? When building your model, how can you tell whether one algorithm performs
    better than another? All of these questions can and should be answered with model
    validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, ML methods start with an existing set of data that
    we’ll use to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we may want to use all the data we have to train our model. Once
    done, what can we do to test it? We could apply our model to the same data and
    see whether the output was correct... and we would get a surprisingly good result!
    Here, we are testing the model with the same data we used to train it. Obviously,
    the model will overperform on this data because it has already seen it. As you
    may have guessed, this is not a reliable way to measure the accuracy of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The right way to validate a model is to split the data into two: we keep one
    part for training the data and another for testing it. This is known as the **holdout
    set**. This way, we’ll test the model on data that it has never seen before and
    compare the result that’s predicted by the model with the real value. Hence, the
    accuracy we are measuring is much more sensible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique works well; however, it poses a problem: by retaining some data,
    we are losing precious information that could have helped us build a better model.
    This is especially true if our initial dataset is small. To solve this, we can
    use **cross-validation**. With this method, we once again split the data into
    two sets. This time, we are training the model twice, using each set as training
    and testing sets. You can see a schematic representation of this operation in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Two-fold cross-validation](img/Figure_11.1_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Two-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the operation, we obtain two accuracies, which will give us a
    better overview of how our model performs on the whole dataset. This technique
    can be applied to help us perform more trials with a smaller testing set, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Five-fold cross-validation](img/Figure_11.2_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Five-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll stop here regarding this very quick introduction to ML. We’ve barely
    scratched the surface: ML is a vast and complex field, and there are lots of books
    dedicated to this subject. Still, this information should be sufficient to help
    you understand the basic concepts we’ll show throughout the rest of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating arrays with NumPy and pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said in the introduction, numerous Python libraries have been developed
    to help with common data science tasks. The most fundamental ones are probably
    NumPy and pandas. Their goal is to provide a set of tools to manipulate a big
    set of data in an efficient way, much more than what we could actually achieve
    with standard Python, and we’ll show how and why in this section. NumPy and pandas
    are at the heart of most data science applications in Python; knowing about them
    is therefore the first step on your journey into Python for data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting to use them, let’s explain why such libraries are needed. In
    [*Chapter 2*](B19528_02.xhtml#_idTextAnchor032), *Python Programming Specificities*,
    we stated that Python is a dynamically typed language. This means that the interpreter
    automatically detects the type of a variable at runtime, and this type can even
    change throughout the program. For example, you can do something like this in
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The interpreter was able to determine the type of `x` at each assignation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, the standard implementation of Python, CPython, is written
    in C. The C language is a compiled and statically typed language. This means that
    the nature of the variables is fixed at compile time, and they can’t change during
    execution. Thus, in the Python implementation, a variable doesn’t only consist
    of its value: it’s actually a structure containing information about the variable,
    including its type and size, in addition to its value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this, we can manipulate variables very dynamically in Python. However,
    it comes at a cost: each variable has a significantly higher memory footprint
    to store all its metadata than just the plain value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is particularly true for data structures. Say we consider a simple list
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Each item in the list is a Python integer, with all the metadata associated.
    In a statically typed language such as C, the same list would only be a suite
    of values in memory sharing the same type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now imagine a big set of data, like the kind we usually encounter in
    data science: the cost of storing it in memory would be huge. That’s exactly the
    purpose of NumPy: to provide a powerful and efficient array structure for manipulating
    a big set of data. Under the hood, it uses a fixed-type array, meaning all elements
    of the structure are of the same type, which allows NumPy to get rid of the costly
    metadata of every single element. Moreover, common arithmetic operations, such
    as additions or multiplications, are much faster. In the *Manipulating arrays
    with NumPy – computation, aggregations, and comparisons* section, we’ll make a
    speed comparison to show you the difference with standard Python lists.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see how NumPy works! The first thing is to install it using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In a Python interpreter, we can now import the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that, by convention, *NumPy is generally imported with the alias* `np`.
    Let’s now discover its basic features!
  prefs: []
  type: TYPE_NORMAL
- en: Creating arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create an array with NumPy, we can simply use the `array` function and pass
    it a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy will detect the nature of the Python list. However, we can force the
    resulting type by using the `dtype` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'All elements were upcasted to the specified type. It is key to remember that
    a *NumPy array is of a fixed type*. This means that every element will have the
    same type and NumPy will silently cast a value to the `array` type. For example,
    let’s consider an integer list into which we want to insert a floating-point value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `13.37` value has been truncated to fit into an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the value cannot be cast to the type of array, an error is raised. For example,
    let’s try to change the first element with a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we said in the introduction to this section, Python lists are not very efficient
    for large datasets. This is why it’s generally more efficient to use NumPy functions
    to create arrays. The most commonly used ones are generally the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`np.zeros`, to create an array filled with zeros'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`np.ones`, to create an array filled with ones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`np.empty`, to create an empty array of the desired size in memory, without
    initializing the values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`np.arange`, to create an array with a range of elements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see them in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the result of `np.empty` can vary: since the values in the array
    are not initialized, *they take whatever value there is currently in this memory
    block*. The main motivation behind this function is speed, allowing you to quickly
    allocate memory, but don’t forget to fill every element after.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, NumPy creates arrays with a floating-point type (`float64`). Once
    again, by using the `dtype` argument, you can force another type to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy provides a wide range of types, allowing you to finely optimize the memory
    consumption of your program by selecting the right type for your data. You can
    find the whole list of types supported by NumPy in the official documentation:
    [https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases](https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases).'
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy also proposes a function to create an array with random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is the maximum range of the random value, and the `size`
    argument sets the number of values to generate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Until now, we showed how to create one-dimensional arrays. However, the great
    strength of NumPy is that it natively handles multi-dimensional arrays! For example,
    let’s create a *3 x* *4* matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy did create an array with three rows and four columns! All we had to do
    was to pass a tuple to the NumPy function to specify our dimensions. When having
    such an array, NumPy gives us access to properties for knowing the number of dimensions,
    as well as the shape and size of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Accessing elements and sub-arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NumPy arrays closely follow the standard Python syntax to manipulate lists.
    Therefore, to access an element in a one-dimensional array, just do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For multi-dimensional arrays, we just have to add another index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, this can be used to re-assign elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'But that’s not all. Thanks to the slicing syntax, we can access sub-arrays
    with a start index, an end index, and even a step. For example, on a one-dimensional
    array, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is exactly what we saw for standard Python lists in [*Chapter 2*](B19528_02.xhtml#_idTextAnchor032),
    *Python Programming Specificities*. Of course, it also works for multi-dimensional
    arrays, with one slice for each dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can assign those sub-arrays to variables. However, for performance reasons,
    NumPy doesn’t copy the values by default: it’s only a **view** (or shallow copy),
    a representation of the existing data. This is important to bear in mind because
    if you change a value in the view, it will also change the value in the original
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to really `copy` method on the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`m2` is now a separate copy of `m`, and changes in its values won’t change
    the values in `m`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You now have the basics for handling arrays with NumPy. As we’ve seen, the
    syntax is very similar to standard Python. The key points to remember when working
    with NumPy are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy arrays are of fixed types, meaning all items in the array are of the same
    type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy natively handles multi-dimensional arrays and allows us to subset them
    using the standard slicing notation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, NumPy can do much more than that: actually, it can apply common
    computations to those arrays in a very performant way.'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating arrays with NumPy – computation, aggregations, and comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we said, NumPy is all about manipulating large arrays with great performance
    and controlled memory consumption. Let’s say, for example, that we want to compute
    the double of each element in a large array. In the following example, you can
    see an implementation of such a function with a standard Python loop:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_compare_operations.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py)'
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate an array with a million random integers. Then, we have our function
    building an array with the double of each element. Basically, we first instantiate
    an empty array of the same size before looping over each element to set the double.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s measure the performance of this function. In Python, there is a standard
    module, `timeit`, dedicated to this purpose. We can use it directly from the command
    line and pass valid Python statements we want to measure performance for. The
    following command will measure the performance of `standard_double` with our big
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The results will vary depending on your machine, but the magnitude should be
    equivalent. What `timeit` does is repeat your code a certain number of times and
    measure its execution time. Here, our function took around 150 milliseconds to
    compute the double of each element in our array. For such simple computations
    on a modern computer, that’s not very impressive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare this with the equivalent operation using NumPy syntax. You can
    see it in the next sample:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_compare_operations.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_compare_operations.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is much shorter! NumPy implements the basic arithmetic operations
    and can apply them to each element of the array. By multiplying the array by a
    value directly, we implicitly tell NumPy to multiply each element by this value.
    Let’s measure the performance with `timeit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, the best loop achieved the computation in 600 microseconds! That’s 250
    times faster than the previous function! How can we explain such a variation?
    In a standard loop, Python (because of its dynamic nature) has to check for the
    type of value at each iteration to apply the right function for this type, which
    adds significant overhead. With NumPy, the operation is deferred to an optimized
    and compiled loop where types are known ahead of time, which saves a lot of useless
    checks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We once again see here the benefits of NumPy arrays over standard lists when
    working on a large dataset: it implements operations natively to help you make
    computations very fast.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding and multiplying arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you saw in the previous example, NumPy supports the arithmetic operators
    to make operations over arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that you can operate directly over two arrays of the same dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, NumPy applies the operation element-wise. But it also works in
    certain situations if one of the operands is not of the same shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy automatically understands that it should multiply each element by two.
    This is called **broadcasting**: NumPy “expands” the smaller array to match the
    shape of the larger array. The previous example is equivalent to this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that even if those two examples are conceptually equivalent, the first
    one is more memory-efficient and computationally efficient: NumPy is smart enough
    to use only one `2` value, without having to create a full array of `2`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More generally, broadcasting works if the rightmost dimensions of the arrays
    are of the same size or if one of them is `1`. For example, we can add an array
    of dimensions *4 x 3* to an array of dimensions *1* *x 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'However, adding an array of dimensions *4 x 3* to an array of dimensions *1
    x 4* is not possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If this sounds complicated or confusing, that’s normal; it takes time to understand
    it conceptually, especially in three or more dimensions. For a more detailed explanation
    of the concept, take time to read the related article in the official documentation:
    [https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating arrays – sum, min, max, mean, and so on
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When working with arrays, we often need to summarize the data to extract some
    meaningful statistics: the mean, the minimum, the maximum, and so on. Fortunately,
    NumPy also provides those operations natively. Quite simply, they are provided
    as methods that you can call directly from an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the whole list of aggregating operations in the official documentation:
    [https://numpy.org/doc/stable/reference/arrays.ndarray.html#calculation](https://numpy.org/doc/stable/reference/arrays.ndarray.html#calculation).'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NumPy also implements the standard comparison operators to compare arrays.
    As with arithmetic operators, which we saw in the *Adding and multipl**ying* *arrays*
    section, broadcasting rules apply. This means that you can compare an array with
    a single value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can also compare arrays with arrays, given that they are compatible
    on the basis of the broadcasting rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The resulting array is filled with the Boolean result of the comparison for
    each element.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it for this very quick introduction to NumPy. There is a lot more to
    know and discover with this library, so we strongly encourage you to read the
    official user guide: [https://numpy.org/doc/stable/user/index.html](https://numpy.org/doc/stable/user/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rest of this book, this should be enough for you to understand future
    examples. Let’s now have a look at a library often cited and used alongside NumPy:
    pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we introduced NumPy and its ability to efficiently
    store and work with a large array of data. We’ll now introduce another widely
    used library in data science: pandas. This library is built on top of NumPy to
    provide convenient data structures able to efficiently store large datasets with
    *labeled rows and columns*. This is, of course, especially handy when working
    with most datasets representing real-world data that we want to analyze and use
    in data science projects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we will, of course, install the library with the usual command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, we can start to use it in a Python interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Just like we alias `numpy` as `np`, the convention is to alias `pandas` as `pd`
    when importing it.
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas Series for one-dimensional data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first pandas data structure we’ll introduce is `Series`. This data structure
    behaves very similarly to a one-dimensional array in NumPy. To create one, we
    can simply initialize it with a list of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, pandas creates a NumPy array. As such, it uses the same data
    types to store the data. You can verify this by accessing the `values` property
    of the `Series` object and checking its type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Indexing and slicing work exactly the same way as in NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, this is not very different from a regular NumPy array. As we said,
    the main purpose of pandas is to *label the data*. To allow this, pandas data
    structures maintain an index to allow this data labeling. It is accessible through
    the `index` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have a simple range integer index, but we can actually have any arbitrary
    index. In the next example, we create the same series, labeling each value with
    a letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index` argument on the `Series` initializer allows us to set the list
    of labels. We can now access values with those labels instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Surprisingly, even slicing notation works with those kinds of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, pandas keep the order of the index to allow such useful notations.
    Notice, however, that with this notation, the *last index is inclusive* (`d` is
    included in the result), unlike standard index notation, where the last index
    is exclusive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid confusion between those two styles, pandas exposes two special notations
    to explicitly indicate which indexing style you wish to use: `loc` (label notation
    with the last index being inclusive) and `iloc` (standard index notation). You
    can read more about this in the official documentation: [https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing](https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing).'
  prefs: []
  type: TYPE_NORMAL
- en: '`Series` can also be instantiated directly from dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the keys of the dictionaries are used as labels.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in the real world, you’ll more likely have to work with two-dimensional
    (or more!) datasets. This is exactly what DataFrames are for!
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas DataFrames for multi-dimensional data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the time, datasets consist of two-dimensional data, where you have several
    columns for each row, as in a classic spreadsheet application. In Pandas, DataFrames
    are designed to work with this kind of data. As for `Series`, it can work with
    a large set of data that is labeled both by rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following examples will use a tiny dataset representing the number of tickets
    (paid and free) delivered in French museums in 2018\. Let’s consider we have this
    data in the form of two dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Each key in those dictionaries is a label for a row. We can build a DataFrame
    directly from those two dictionaries like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `DataFrame` initializer accepts a dictionary of dictionaries, where keys
    represent the label for the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have a look at the `index` property, storing the rows index, and the
    `columns` property, storing the columns index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we can now use indexing and slicing notation to get subsets of
    columns or rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Something that is even more powerful: you can write a Boolean condition inside
    the brackets to match some data. This operation is called **masking**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can easily set new columns with this very same indexing notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, just like NumPy arrays, pandas fully supports arithmetic operations
    over two DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, all the basic aggregation operations are supported, including `mean`
    and `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the whole list of operations available in the official documentation:
    [https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#descriptive-statistics](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#descriptive-statistics).'
  prefs: []
  type: TYPE_NORMAL
- en: Importing and exporting CSV data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One very common way of sharing datasets is through CSV files. This format is
    very convenient because it only consists of a simple text file, each line representing
    a row of data, with each column separated by a comma. Our simple *museums* dataset
    is available in the examples repository as a CSV file, which you can see in the
    next sample:'
  prefs: []
  type: TYPE_NORMAL
- en: museums.csv
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/museums.csv](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/museums.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Importing CSV files is so common that pandas provides a function to load a
    CSV file into a DataFrame directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The function simply expects the path to the CSV file. Several arguments are
    available to finely control the operation: here, we used `index_col` to specify
    the index of the column that should be used as row labels. You can find the whole
    list of arguments in the official documentation: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the opposite operation exists to export a DataFrame to a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will conclude this very quick introduction to pandas here. Of course, we’ve
    only covered the tip of the iceberg, and we recommend that you go through the
    official user guide to know more: [https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Still, you should now be able to perform basic operations and operate efficiently
    on large datasets. In the next section, we’ll introduce scikit-learn, one of the
    fundamental Python toolkits for data science, and you’ll see that it relies a
    lot on NumPy and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Training models with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn is one of the most widely used Python libraries for data science.
    It implements dozens of classic ML models, but also numerous tools to help you
    while training them, such as preprocessing methods and cross-validation. Nowadays,
    you’ll probably hear about more modern approaches, such as PyTorch, but scikit-learn
    is still a solid tool for a lot of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you must do to get started is to install it in your Python
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We can now start our scikit-learn journey!
  prefs: []
  type: TYPE_NORMAL
- en: Training models and predicting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In scikit-learn, ML models and algorithms are called `fit`, which is used to
    train a model, and `predict`, which is used to run the trained model on new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try this, we’ll load a sample dataset. scikit-learn comes with a few toy
    datasets that are very useful for performing experiments. You can find out more
    about them in the official documentation: [https://scikit-learn.org/stable/datasets.html](https://scikit-learn.org/stable/datasets.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’ll use the *digits* dataset, a collection of pixel matrices representing
    handwritten digits. As you may have guessed, the goal of this dataset is to train
    a model to automatically recognize handwritten digits. The following example shows
    how to load this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_load_digits.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_load_digits.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_load_digits.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the toy dataset’s functions are imported from the `datasets` package
    of scikit-learn. The `load_digits` function returns an object that contains the
    data and some metadata.
  prefs: []
  type: TYPE_NORMAL
- en: The most interesting parts of this object are `data`, which contains the handwritten
    digit pixels matrices, and `targets`, which contains the corresponding label for
    those digits. Both are NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: To get a grasp of what this looks like, we will take the first digit in the
    data and reshape it into an 8 x 8 matrix; this is the size of the source images.
    Each value represents a pixel on a grayscale, from 0 to 16.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we print the label of this first digit, which is `0`. If you run this
    code, you’ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Somehow, we can guess the shape of the zero from the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s try to build a model that recognizes handwritten digits. To start
    simple, we’ll use a Gaussian Naive Bayes model, a classic and easy-to-use algorithm
    that can quickly yield good results. The following example shows the entire process:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_fit_predict.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_fit_predict.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_fit_predict.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve loaded the dataset, you can see that we take care of splitting
    it into a training and a testing set. As we mentioned in the *Model validation*
    section, this is essential for computing meaningful accuracy scores to check how
    our model performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we can rely on the `train_test_split` function, which is provided
    in the `model_selection` package. It selects random instances from our dataset
    to form the two sets. By default, it keeps 25% of the data to create a testing
    set, but this can be customized. The `random_state` argument allows us to set
    the random seed to make the example reproducible. You can find out more about
    this function in the official documentation: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn-model-selection-train-test-split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn-model-selection-train-test-split).'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we must instantiate the `GaussianNB` class. This class is one of the numerous
    ML estimators that’s implemented in scikit-learn. Each has its own set of parameters,
    to finely tune the behavior of the algorithm. However, scikit-learn is designed
    to provide sensible defaults for all the estimators, so it’s usually good to start
    with the defaults before tinkering with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we must call the `fit` method to train our model. It expects an
    argument and two arrays: the first one is the actual data, with all its features,
    while the second one is the corresponding labels. And that’s it! You’ve trained
    your first ML model!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how it behaves: we’ll call `predict` on our model with the testing
    set so that it automatically classifies the digits of the testing set. The result
    of this is a new array with the predicted labels.'
  prefs: []
  type: TYPE_NORMAL
- en: All we have to do now is compare it with the actual labels of our testing set.
    Once again, scikit-learn helps by providing the `accuracy_score` function in the
    `metrics` package. The first argument is the true labels, while the second is
    the predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: If you run this code, you’ll get an accuracy score of around 83%. That isn’t
    too bad for a first approach! As you have seen, training and running prediction
    on an ML model is straightforward with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we often need to perform preprocessing steps on the data before
    feeding it to an estimator. Rather than doing this sequentially by hand, scikit-learn
    proposes a convenient feature that can automate this process: **pipelines**.'
  prefs: []
  type: TYPE_NORMAL
- en: Chaining preprocessors and estimators with pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quite often, you’ll need to preprocess your data so that it can be used by the
    estimator you wish to use. Typically, you’ll want to transform an image into an
    array of pixel values or, as we’ll see in the following example, transform raw
    text into numerical values so that we can apply some math to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than writing those steps by hand, scikit-learn proposes a feature that
    can automatically chain preprocessors and estimators: pipelines. Once created,
    they expose the very same interface as any other estimator, allowing you to run
    training and prediction in one operation.'
  prefs: []
  type: TYPE_NORMAL
- en: To show you what this looks like, we’ll look at an example of another classic
    dataset, the *20 newsgroups* text dataset. It consists of 18,000 newsgroup articles
    categorized into 20 topics. The goal of this dataset is to build a model that
    will automatically categorize an article in one of those topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how we can load this data thanks to the `fetch_20newsgroups`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_pipelines.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the dataset is rather large, we’ll only load a subset of the categories.
    Also, notice that it’s already been split into training and testing sets, so we
    only have to load them with the corresponding argument. You can find out more
    about the functionality of this dataset in the official documentation: [https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, it’s important to understand what the underlying data is.
    Actually, this is the raw text of an article. You can check this by printing one
    of the samples in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we need to extract some features from this text before feeding it to an
    estimator. A common approach for this when working with textual data is to use
    the **Term Frequency-Inverse Document Frequency (TF-IDF)**. Without going into
    too much detail, this technique will count the occurrences of each word in all
    the documents (term frequency), weighted by the importance of this word in every
    document (inverse document frequency). The idea is to give more weight to rarer
    words, which should convey more sense than frequent words such as “the.” You can
    find out more about this in the scikit-learn documentation: [https://scikit-learn.org/dev/modules/feature_extraction.html#tfidf-term-weighting](https://scikit-learn.org/dev/modules/feature_extraction.html#tfidf-term-weighting).'
  prefs: []
  type: TYPE_NORMAL
- en: This operation consists of splitting each word in the text samples and counting
    them. Usually, we apply a lot of techniques to refine this, such as removing `TfidfVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This preprocessor can take an array of text, tokenize each word, and compute
    the TF-IDF for each of them. A lot of options are available for finely tuning
    its behavior, but the defaults are a good start for English text. The following
    example shows how to use it with an estimator in a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_pipelines.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The `make_pipeline` function accepts any number of preprocessors and an estimator
    in its argument. Here, we’re using the Multinomial Naive Bayes classifier, which
    is suitable for features representing frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can simply train our model and run prediction to check its accuracy,
    as we did previously. You can see this in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_pipelines.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_pipelines.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we also printed a confusion matrix, which is a very convenient
    representation of the global results. Scikit-learn has a dedicated function for
    this called `confusion_matrix`. Then, we wrap the result in a pandas DataFrame
    so that we can set the axis labels to improve readability. If you run this example,
    you’ll get an output similar to what’s shown in the following screenshot. Depending
    on your machine and system, it could take a couple of minutes to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Confusion matrix on the 20 newsgroups dataset](img/Figure_11.3_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Confusion matrix on the 20 newsgroups dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that our results weren’t too bad for our first try. Notice
    that there is one big area of confusion between the `soc.religion.christian` and
    `talk.religion.misc` categories, which is not very surprising, given their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve seen, building a pipeline with a preprocessor is very straightforward.
    The nice thing about this is that it automatically applies it to the training
    data, but also when you’re predicting the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let’s look at one more important feature of scikit-learn:
    cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Validating the model with cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Model validation* section, we introduced the cross-validation technique,
    which allows us to use data in training or testing sets. As you may have guessed,
    this technique is so common that it’s implemented natively in scikit-learn!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take another look at the handwritten digit example and apply cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter11_cross_validation.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_cross_validation.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter11/chapter11_cross_validation.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we don’t have to split the data ourselves: the `cross_val_score`
    function performs the folds automatically. In argument, it expects the estimator,
    `data`, which contains the handwritten digits’ pixels matrices, and `targets`,
    which contains the corresponding label for those digits. By default, it performs
    five folds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of this operation is an array that provides the accuracy score of
    the five folds. To get a global overview of this result, we can take, for example,
    the mean. If you run this example, you’ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, our mean accuracy is around 80%, which is a bit lower than
    the 83% we obtained with single training and testing sets. That’s the main benefit
    of cross-validation: we obtain a more statistically accurate metric regarding
    the performance of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have learned the basics of working with scikit-learn. It’s obviously
    a very quick introduction to this vast framework, but it’ll give you the keys
    to train and evaluate your first ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Congratulations! You’ve discovered the basic concepts of ML and made your first
    experiments with the fundamental toolkits of the data scientist. Now, you should
    be able to explore your first data science problems in Python. Of course, this
    was by no means a complete lesson on ML: the field is vast and there are tons
    of algorithms and techniques to explore. However, I hope that this has sparked
    your curiosity and that you’ll deepen your knowledge of this subject.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to get back to FastAPI! With our new ML tools at hand, we’ll
    be able to leverage the power of FastAPI to serve our estimators and propose a
    reliable and efficient prediction API to our users.
  prefs: []
  type: TYPE_NORMAL
