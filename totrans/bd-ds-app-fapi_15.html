<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer058">&#13;
			<h1 id="_idParaDest-191" class="chapter-number"><a id="_idTextAnchor960"/>12</h1>&#13;
			<h1 id="_idParaDest-192"><a id="_idTextAnchor961"/>Creating an Efficient Prediction API Endpoint with FastAPI</h1>&#13;
			<p>In the previous chapter, we introduced the most common data science techniques and libraries largely used in the Python community. Thanks to those tools, we can now build machine learning models that can make efficient predictions and classify data. Of course, we now have to think about a convenient interface so that we can take advantage of their intelligence. This way, microservices or frontend applications can ask our model to make predictions to improve the user experience or business operations. In this chapter, we’ll learn how to do that <span class="No-Break">with FastAPI.</span></p>&#13;
			<p>As we’ve seen throughout this book, FastAPI allows us to implement very efficient REST APIs with a clear and lightweight syntax. In this chapter, you’ll learn how to use them as efficiently as possible in order to serve thousands of prediction requests. To help us with this task, we’ll introduce another library, Joblib, which provides tools to help us serialize a trained model and cache <span class="No-Break">predicted results.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Persisting a trained model <span class="No-Break">with Joblib</span></li>&#13;
				<li>Implementing an efficient <span class="No-Break">prediction endpoint</span></li>&#13;
				<li>Caching results <span class="No-Break">with Joblib</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-193"><a id="_idTextAnchor962"/>Technical requirements</h1>&#13;
			<p>For this chapter, you’ll require a Python virtual environment, just as we set up in <a href="B19528_01.xhtml#_idTextAnchor024"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Python Development </em><span class="No-Break"><em class="italic">Environment Setup</em></span><span class="No-Break">.</span></p>&#13;
			<p>You’ll find all the code examples for this chapter in the dedicated GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-194"><a id="_idTextAnchor963"/>Persisting a trained model with Joblib</h1>&#13;
			<p>In the <a id="_idIndexMarker904"/>previous chapter, you<a id="_idTextAnchor964"/> learned how to train an estimator<a id="_idTextAnchor965"/> with scikit-learn. When building such models, you’ll likely obtain a rather complex Python script to load your training data, pre-process it, and train your model with the best set of parameters. However, when deploying your model in a web application such as FastAPI, you don’t want to repeat this script and run all those operations when the server is starting. Instead, you need a ready-to-use representation of your trained model that you can just load <span class="No-Break">and use.</span></p>&#13;
			<p>This is what Joblib does. This <a id="_idIndexMarker905"/>library aims to provide tools for efficiently saving Python objects to disk, such as large arrays of data or function results: this operation is generall<a id="_idTextAnchor966"/>y <a id="_idIndexMarker906"/>called <strong class="bold">dumping</strong>. Joblib is <a id="_idIndexMarker907"/>already a dependency of scikit-learn, so we don’t even need to install it. Actually, scikit-learn itself uses it internally to load the bundled <span class="No-Break">toy datasets.</span></p>&#13;
			<p>As we’ll see, dumping a trained model involves just one line of code <span class="No-Break">with Jobl<a id="_idTextAnchor967"/>ib.</span></p>&#13;
			<h2 id="_idParaDest-195"><a id="_idTextAnchor968"/>Dumping a trained model</h2>&#13;
			<p>In this<a id="_idIndexMarker908"/> example, w<a id="_idTextAnchor969"/>e’re using the newsgroups example we saw in the <em class="italic">Chaining preprocessors and estimators with pipelines</em> section of <a href="B19528_11.xhtml#_idTextAnchor797"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Introduction to Data Science in Python</em>. As a reminder, we load 4 of the 20 categories in the <strong class="source-inline">newsgroups</strong> dataset and build a model to automatically categorize news articles into those categories. Once we’ve done this, we dump the model into a file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">newsgroups_model.joblib</strong></span><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_dump_joblib.py</p>&#13;
			<pre class="source-code">&#13;
# Make the pipelinemodel = make_pipeline(&#13;
           TfidfVectorizer(),&#13;
           MultinomialNB(),&#13;
)&#13;
# Train the model&#13;
model.fit(newsgroups_training.data, newsgroups_training.target)&#13;
# Serialize the model and the target names&#13;
model_file = "newsgroups_model.joblib"&#13;
model_targets_tuple = (model, newsgroups_training.target_names)&#13;
joblib.dump(model_targets_tuple, model_file)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_dump_joblib.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_dump_joblib.py</a></p>&#13;
			<p>As <a id="_idIndexMarker909"/>you can see, J<a id="_idTextAnchor970"/>oblib exposes a function called <strong class="source-inline">dump</strong>, which simply expects two arguments: the Python object to save and the path of <span class="No-Break">the file.</span></p>&#13;
			<p>Notice that we don’t dump the <strong class="source-inline">model</strong> variable alone: instead, we wrap it in a tuple, along with the name of the categories, <strong class="source-inline">target_names</strong>. This allows us to retrieve the actual name of the category after the prediction has been made without us having to reload the <span class="No-Break">training dataset.</span></p>&#13;
			<p>If you run this script, you’ll see that the <strong class="source-inline">newsgroups_model.joblib</strong> file <span class="No-Break">was created:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ python chapter12/chapter12_dump_joblib.py$ ls -lh *.joblib&#13;
-rw-r--r--    1 fvoron    staff       3,0M 10 jan 08:27 newsgroups_model.joblib</pre>&#13;
			<p>Notice that this file is rather large: it’s more than 3 MB! It stores all the probabilities of each word in each category, as computed by the multinomial Naive <span class="No-Break">Bayes model.</span></p>&#13;
			<p>That’s all we need to do. This file now contains a static representation of our Python model, which will be easy to store, share, and load. Now, let’s learn how to load it and check that we can run predictions <span class="No-Break">o<a id="_idTextAnchor971"/>n it.</span></p>&#13;
			<h2 id="_idParaDest-196"><a id="_idTextAnchor972"/>Loading a dumped model</h2>&#13;
			<p>Now th<a id="_idTextAnchor973"/>at we <a id="_idIndexMarker910"/>have our dumped model file, let’s learn how to load it again using Joblib and check that everything is working. In the following example, we’re loading the Joblib dump present in the <strong class="source-inline">chapter12</strong> directory of the examples repository and running <span class="No-Break">a prediction:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_load_joblib.py</p>&#13;
			<pre class="source-code">&#13;
import osimport joblib&#13;
from sklearn.pipeline import Pipeline&#13;
# Load the model&#13;
model_file = os.path.join(os.path.dirname(__file__), "newsgroups_model.joblib")&#13;
loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file)&#13;
model, targets = loaded_model&#13;
# Run a prediction&#13;
p = model.predict(["computer cpu memory ram"])&#13;
print(targets[p[0]])</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_load_joblib.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_load_joblib.py</a></p>&#13;
			<p>All we need to do here is call the <strong class="source-inline">load</strong> function from Joblib and pass it as a valid path to a dump file. The result of this function is the very same Python object we dumped. Here, it’s a tuple composed of the scikit-learn estimator and a list <span class="No-Break">of categories.</span></p>&#13;
			<p>Notice that we added some type hints: while not necessary, it helps mypy or whichever IDE you use identify the nature of the objects you loaded and benefit from type-checking <span class="No-Break">and auto-completion.</span></p>&#13;
			<p>Finally, we<a id="_idTextAnchor974"/> <a id="_idIndexMarker911"/>run a prediction on the model: it’s a true scikit-learn estimator, with all the necessary <span class="No-Break">training parameters.</span></p>&#13;
			<p>That’s it! As you’ve seen, Joblib is straightforward to use. Nevertheless, it’s an essential tool for exporting your scikit-learn models and being able to use them in external services without repeating the training phase. Now, we can use those dump files in <span class="No-Break">FastAPI pr<a id="_idTextAnchor975"/>ojects.</span></p>&#13;
			<h1 id="_idParaDest-197"><a id="_idTextAnchor976"/>Implementing an efficient prediction endpoint</h1>&#13;
			<p>Now that we<a id="_idTextAnchor977"/> <a id="_idIndexMarker912"/>have a way to save and load our machine learning models, it’s time to use them in a FastAPI project. As you’ll see, the implementation shouldn’t be too much of a surprise if you’ve followed this book. The main part of the implementation is the class dependency, which will take care of loading the model and making predictions. If you need a refresher on class dependencies, check out <a href="B19528_05.xhtml#_idTextAnchor285"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Dependency Injection </em><span class="No-Break"><em class="italic">in FastAPI</em></span><span class="No-Break">.</span></p>&#13;
			<p>Let’s go! Our example will be based on the <strong class="source-inline">newgroups</strong> model we dumped in the previous section. We’ll start by showing you how to implement the class dependency, which will take care of loading the model and <span class="No-Break">making predictions:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_prediction_endpoint.py</p>&#13;
			<pre class="source-code">&#13;
class PredictionInput(BaseModel):           text: str&#13;
class PredictionOutput(BaseModel):&#13;
           category: str&#13;
class NewsgroupsModel:&#13;
           model: Pipeline | None = None&#13;
           targets: list[str] | None = None&#13;
           def load_model(self) -&gt; None:&#13;
                         """Loads the model"""&#13;
                         model_file = os.path.join(os.path.dirname(__file__), "newsgroups_model.joblib")&#13;
                         loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file)&#13;
                         model, targets = loaded_model&#13;
                         self.model = model&#13;
                         self.targets = targets&#13;
           async def predict(self, input: PredictionInput) -&gt; PredictionOutput:&#13;
                         """Runs a prediction"""&#13;
                         if not self.model or not self.targets:&#13;
                                       raise RuntimeError("Model is not loaded")&#13;
                         prediction = self.model.predict([input.text])&#13;
                         category = self.targets[prediction[0]]&#13;
                         return PredictionOutput(category=category)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_prediction_endpoint.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_prediction_endpoint.py</a></p>&#13;
			<p>First, we <a id="_idTextAnchor978"/>start by defining two Pydantic models: <strong class="source-inline">PredictionInput</strong> and <strong class="source-inline">PredictionOutput</strong>. In a pure FastAPI philosophy, they will help us validate the request payload and return a structured JSON response. Here, as input, we simply expect a <strong class="source-inline">text</strong> property containing the text we want to classify. As output, we expect a <strong class="source-inline">category</strong> property containing the <span class="No-Break">predicted category.</span></p>&#13;
			<p>The <a id="_idIndexMarker913"/>most interesting part of this extract is the <strong class="source-inline">NewsgroupsModel</strong> class. It implements two methods: <strong class="source-inline">load_model</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">predict</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The <strong class="source-inline">load_model</strong> method loads the model using Joblib, as we saw in the previous section, and stores the model and targets in class properties. Hence, they will be available to use in the <span class="No-Break"><strong class="source-inline">predict</strong></span><span class="No-Break"> method.</span></p>&#13;
			<p>On the other hand, the <strong class="source-inline">predict</strong> method will be injected into the path operation functio<a id="_idTextAnchor979"/>n. As you can see, it directly accepts <strong class="source-inline">PredictionInput</strong>, which will be injected by FastAPI. Inside this method, we are making a prediction, as we usually do with scikit-learn. We return a <strong class="source-inline">PredictionOutput</strong> object with the category <span class="No-Break">we predicted.</span></p>&#13;
			<p>You may have noticed that, first, we check whether the model and its targets have been assigned in the class properties before performing the prediction. Of course, we need to ensure <strong class="source-inline">load_model</strong> was called at some point before making a prediction. You may be wondering why we are not putting this logic in an initializer, <strong class="source-inline">__init__</strong>, so that we can ensure the model is loaded at class instantiation. This would work perfectly fine; however, it would cause some issues. As we’ll see, we are instantiating a <strong class="source-inline">NewsgroupsModel</strong> instance right after FastAPI so that we can use it in our routes. If the loading logic was in <strong class="source-inline">__init__</strong>, the model would be loaded whenever we imported some variables (such as the <strong class="source-inline">app</strong> instance) from this file, such as in unit tests. In most cases, this would incur unnecessary I/O operations and memory consumption. As we’ll see, it’s better to use the lifespan handler of FastAPI to load the model when the app <span class="No-Break">is run.</span></p>&#13;
			<p>The following extract shows the rest of the implementation, along with the actual FastAPI route for <span class="No-Break">handling predictions:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_prediction_endpoint.py</p>&#13;
			<pre class="source-code">&#13;
newgroups_model = NewsgroupsModel()@contextlib.asynccontextmanager&#13;
async def lifespan(app: FastAPI):&#13;
           newgroups_model.load_model()&#13;
           yield&#13;
app = FastAPI(lifespan=lifespan)&#13;
@app.post("/prediction")&#13;
async def prediction(&#13;
           output: PredictionOutput = Depends(newgroups_model.predict),&#13;
) -&gt; PredictionOutput:&#13;
           return output</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_prediction_endpoint.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_prediction_endpoint.py</a></p>&#13;
			<p>As we<a id="_idIndexMarker914"/> mentioned previously, we are creating an instance of <strong class="source-inline">NewsgroupsModel</strong> so t<a id="_idTextAnchor980"/>hat we can inject it into our path operation function. Moreover, we are implementing a lifespan handler to call <strong class="source-inline">load_model</strong>. This way, we are making sure that the model is loaded during application startup and is ready <span class="No-Break">to use.</span></p>&#13;
			<p>The prediction endpoint is quite straightforward: as you can see, we directly depend on the <strong class="source-inline">predict</strong> method, which will take care of injecting the payload and validating it. We only have to return <span class="No-Break">the output.</span></p>&#13;
			<p>That’s it! Once again, FastAPI makes our life very easy by allowing us to write very simple and readable code, even for complex tasks. We can run this application using Uvicorn, <span class="No-Break">as usual:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ uvicorn chapter12.chapter12_prediction_endpoint:app</pre>			<p>Now, we can try to run some predictions <span class="No-Break">with HTTPie:</span></p>&#13;
			<pre class="source-code">&#13;
$ http POST http://localhost:8000/prediction text="computer cpu memory ram"HTTP/1.1 200 OK&#13;
content-length: 36&#13;
content-type: application/json&#13;
date: Tue, 10 Jan 2023 07:37:22 GMT&#13;
server: uvicorn&#13;
{&#13;
           "category": "comp.sys.mac.hardware"&#13;
}</pre>&#13;
			<p>O<a id="_idTextAnchor981"/>ur <a id="_idIndexMarker915"/>machine learning classifier is alive! To push this further, let’s see how we can implement a simple caching mechanism <span class="No-Break">us<a id="_idTextAnchor982"/>ing Joblib.</span></p>&#13;
			<h1 id="_idParaDest-198"><a id="_idTextAnchor983"/>Caching results with Joblib</h1>&#13;
			<p>If your<a id="_idIndexMarker916"/><a id="_idTextAnchor984"/> model takes time to make predictions, it may be in<a id="_idTextAnchor985"/>teresting to <a id="_idIndexMarker917"/>cache the results: if the prediction for a particular input has already been done, it makes sense to return the same result we saved on disk rather than running the computations again. In this section, we’ll learn how to do this with the help <span class="No-Break">of Joblib.</span></p>&#13;
			<p>Joblib provides us with a very convenient and easy-to-use tool to do this, so the implementation is quite straightforward. The main concern will be about whether we should choose standard or async functions to implement the endpoints and dependencies. This will allow us to explain some of the technical details of FastAPI in <span class="No-Break">more detail.</span></p>&#13;
			<p>We’ll build upon the example we provided in the previous section. The first thing we must do is initialize a Joblib <strong class="source-inline">Memory</strong> class, which is the helper for caching function results. Then, we can add a decorator to the functions we want to cache. You can see this in the <span class="No-Break">following example:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_caching.py</p>&#13;
			<pre class="source-code">&#13;
memory = joblib.Memory(location="cache.joblib")@memory.cache(ignore=["model"])&#13;
def predict(model: Pipeline, text: str) -&gt; int:&#13;
           prediction = model.predict([text])&#13;
           return prediction[0]</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py</a></p>&#13;
			<p>When<a id="_idIndexMarker918"/> initializi<a id="_idTextAnchor986"/>ng <strong class="source-inline">memory</strong>, the main argument is <strong class="source-inline">location<a id="_idTextAnchor987"/></strong>, which is the directory path where Joblib will store the results. Joblib automatically saves cached results on the <span class="No-Break">hard disk.</span></p>&#13;
			<p>Then, you<a id="_idIndexMarker919"/> can see that we implemented a <strong class="source-inline">predict</strong> function, which accepts our scikit-learn model and some text input and then returns the predicted category index. This is the same prediction operation we’ve seen so far. Here, we extracted it from the <strong class="source-inline">NewsgroupsModel</strong> dependency class because Joblib caching is primarily designed to work with regular functions. Caching class methods is not recommended. As you can see, we simply have to add a <strong class="source-inline">@memory.cache</strong> decorator on top of this function to enable <span class="No-Break">Joblib caching.</span></p>&#13;
			<p>Whenever this function is called, Joblib will check whether it has the result on disk for the same arguments. If it does, it returns it directly. Otherwise, it proceeds with the regular <span class="No-Break">function call.</span></p>&#13;
			<p>As you can see, we added an <strong class="source-inline">ignore</strong> argument to the decorator, which allows us to tell Joblib to not take into account some arguments in the caching mechanism. Here, we excluded the <strong class="source-inline">model</strong> argument. Joblib cannot dump complex objects, such as scikit-learn estimators. This isn’t a problem, though: the model doesn’t change between several predictions, so we don’t care about having it cached. If we make improvements to our model and deploy a new one, all we have to do is clear the whole cache so that older predictions are made again with the <span class="No-Break">new model.</span></p>&#13;
			<p>Now, we can tweak the <strong class="source-inline">NewsgroupsModel</strong> dependency class so that it works with this new <strong class="source-inline">predict</strong> function. You<a id="_idIndexMarker920"/> can see this in the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker921"/></span><span class="No-Break"> example:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_caching.py</p>&#13;
			<pre class="source-code">&#13;
class NewsgroupsModel:           model: Pipeline | None = None&#13;
           targets: list[str] | None = None&#13;
           def load_model(self) -&gt; None:&#13;
                         """Loads the model"""&#13;
                         model_file = os.path.join(os.path.dirname(__file__), "newsgroups_model.joblib")&#13;
                         loaded_model: tuple[Pipeline, list[str]] = joblib.load(model_file)&#13;
                         model, targets = loaded_model&#13;
                         self.model = model&#13;
                         self.targets = targets&#13;
           def predict(self, input: PredictionInput) -&gt; PredictionOutput:&#13;
                         """Runs a prediction"""&#13;
                         if not self.model or not self.targets:&#13;
                                       raise RuntimeError("Model is not loaded")&#13;
                         prediction = predict(self.model, input.text)&#13;
                         category = self.targets[prediction]&#13;
                         return PredictionOutput(category=category)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py</a></p>&#13;
			<p>In the <strong class="source-inline">p<a id="_idTextAnchor988"/>redict</strong> method, we <a id="_idIndexMarker922"/>are calling the externa<a id="_idTextAnchor989"/>l <strong class="source-inline">predict</strong> function<a id="_idIndexMarker923"/> instead of doing so directly inside the method, taking care to pass the model and the input text as arguments. All we have to do after that is retrieve the corresponding category name and build a <span class="No-Break"><strong class="source-inline">PredictionOutput</strong></span><span class="No-Break"> object.</span></p>&#13;
			<p>Finally, we have the REST API endpoints. Here, we added a <strong class="source-inline">delete/cache</strong> route so that we <a id="_idTextAnchor990"/>can clear the whole Joblib cache with an HTTP r<a id="_idTextAnchor991"/>equest. This can be seen in the <span class="No-Break">following example:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_caching.py</p>&#13;
			<pre class="source-code">&#13;
@app.post("/prediction")def prediction(&#13;
           output: PredictionOutput = Depends(newgroups_model.predict),&#13;
) -&gt; PredictionOutput:&#13;
           return output&#13;
@app.delete("/cache", status_code=status.HTTP_204_NO_CONTENT)&#13;
def delete_cache():&#13;
           memory.clear()</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_caching.py</a></p>&#13;
			<p>The <strong class="source-inline">clear</strong> method on the <strong class="source-inline">memory</strong> object removes all the Joblib cache files on <span class="No-Break">the disk.</span></p>&#13;
			<p>Our FastAPI application <a id="_idIndexMarker924"/>is now caching prediction results. If you <a id="_idIndexMarker925"/>make a request with the same input twice, the second response will show you the cached result. In this example, our model is fast, so you won’t notice a difference in terms of execution time; however, this could be interesting with<a id="_idTextAnchor992"/> more <span class="No-Break">complex models.</span></p>&#13;
			<h2 id="_idParaDest-199">Choosing between standard or async f<a id="_idTextAnchor993"/>unctions</h2>&#13;
			<p>You may have no<a id="_idTextAnchor994"/>ticed that we changed the <strong class="source-inline">predict</strong> method and the <strong class="source-inline">prediction</strong> and <strong class="source-inline">delete_cache</strong> path operation functions so that they’re <em class="italic">standard, </em><span class="No-Break"><em class="italic">non-async</em></span><span class="No-Break"> functions.</span></p>&#13;
			<p>Since the <a id="_idIndexMarker926"/>beginning of this book, we’ve shown you how FastAPI completely embraces asynchronous I/O and why it’s good for the performance of your applications. We’ve also recommended libraries that work asynchronously, such as database drivers, to leverage <span class="No-Break">that power.</span></p>&#13;
			<p>In some case<a id="_idTextAnchor995"/>s, however, that’s not always possible. I<a id="_idTextAnchor996"/>n this case, Joblib is implemented to work synchronously. Nevertheless, it’s performing long I/O operations: it reads and writes cache files on the hard disk. Hence, it will block the process and won’t be able to answer other requests while this is happening, as we explained in the <em class="italic">Asynchronous I/O</em> section of <a href="B19528_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Python </em><span class="No-Break"><em class="italic">Programming Specificities</em></span><span class="No-Break">.</span></p>&#13;
			<p>To solve this, FastAPI implements a neat mechanism: <em class="italic">if you define a path operation function or a dependency as a standard, non-async function, it’ll run it in a separate thread</em>. This means that blocking operations, such as synchronous file reading, won’t block the main process. In a sense, we could say that it mimics an <span class="No-Break">asynchronous operation.</span></p>&#13;
			<p>To understand this, we’ll perform a simple experiment. In the following example, we are building a dummy FastAPI application with <span class="No-Break">three endpoints:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">/fast</strong>, which directly returns <span class="No-Break">a response</span></li>&#13;
				<li><strong class="source-inline">/slow-async</strong>, a path operation defined as <strong class="source-inline">async</strong>, which creates a synchronous blocking operation that takes 10 seconds <span class="No-Break">to run</span></li>&#13;
				<li><strong class="source-inline">/slow-sync</strong>, a path operation that’s defined as a standard method, which creates a synchronous blocking operation that takes 10 seconds <span class="No-Break">to run</span></li>&#13;
			</ul>&#13;
			<p>You can read <a id="_idIndexMarker927"/>the corresponding <span class="No-Break">code here:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter12_async_not_async.py</p>&#13;
			<pre class="source-code">&#13;
import timefrom fastapi import FastAPI&#13;
app = FastAPI()&#13;
@app.get("/fast")&#13;
async def fast():&#13;
           return {"endpoint": "fast"}&#13;
@app.get("/slow-async")&#13;
async def slow_async():&#13;
           """Runs in the main process"""&#13;
           time.sleep(10)    # Blocking sync operation&#13;
           return {"endpoint": "slow-async"}&#13;
@app.get("/slow-sync")&#13;
def slow_sync():&#13;
           """Runs in a thread"""&#13;
           time.sleep(10)    # Blocking sync operation&#13;
           return {"endpoint": "slow-sync"}</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_async_not_async.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter12/chapter12_async<span id="_idTextAnchor997"/>_not_async.py</a></p>&#13;
			<p>With this<a id="_idIndexMarker928"/> simple application, the <a id="_idTextAnchor998"/>goal is to see how those blocking operations block the main process. Let’s run this application <span class="No-Break">with Uvicorn:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ uvicorn chapter12.chapter12_async_not_async:app</pre>			<p>Next, open two new terminals. In the first one, make a request to the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">slow-async</strong></span><span class="No-Break"> endpoint:</span></p>&#13;
			<pre class="source-code">&#13;
$ http GET http://localhost:8000/slow-async</pre>			<p>Without waiting for the response, in the second terminal, make a request to the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">fast</strong></span><span class="No-Break"> endpoint:</span></p>&#13;
			<pre class="source-code">&#13;
$ http GET http://localhost:8000/fast</pre>			<p>You’ll see that you have to wait 10 seconds before you get the response for the <strong class="source-inline">/fast</strong> endpoint. This means that <strong class="source-inline">/slow-async</strong> blocked the process and prevented the server from answering the other request while this <span class="No-Break">was happening.</span></p>&#13;
			<p>Now, let’s perform the same experiment with the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">slow-sync</strong></span><span class="No-Break"> endpoint:</span></p>&#13;
			<pre class="source-code">&#13;
$ http GET http://localhost:8000/slow-sync</pre>			<p>And again, run the <span class="No-Break">following command:</span></p>&#13;
			<pre class="source-code">&#13;
$ http GET http://lo<a id="_idTextAnchor999"/>calhost:8000/fast</pre>			<p>You’ll immediately get the r<a id="_idTextAnchor1000"/>esponse of <strong class="source-inline">/fast</strong> without having to wait for <strong class="source-inline">/slow-sync</strong> to finish. Since it’s defined as a standard, non-async function, FastAPI will run it in a thread to prevent blocking. However, bear in mind that sending the task to a separate thread implies a small overhead, so it’s important to think about the best approach to your <span class="No-Break">current problem.</span></p>&#13;
			<p>So, when developing with FastAPI, how can you choose between standard or async functions for path operations and dependencies? The rules of thumb for this are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>If the functions don’t involve long I/O operations (file reading, network requests, and so on), define them <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">async</strong></span><span class="No-Break">.</span></li>&#13;
				<li>If they involve I/O operations, see <span class="No-Break">the following:</span><ul><li>Try to choose libraries that are compatible with asynchronous I/O, as we saw for databases or HTTP clients. In this case, your functions will <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">async</strong></span><span class="No-Break">.</span></li><li>If it’s not possible, which is the case for Joblib caching, define them as standard functions. FastAPI will run them in a <span class="No-Break">separate thread.</span></li></ul></li>&#13;
			</ul>&#13;
			<p>Since Joblib is <a id="_idIndexMarker929"/>completely synchronous at making I/O operations, we switched the path operations and the dependency method so that they were synchronous, <span class="No-Break">standard meth<a id="_idTextAnchor1001"/>ods.</span></p>&#13;
			<p>In this example, the difference is no<a id="_idTextAnchor1002"/>t very noticeable because the I/O operations are small and fast. However, it’s good to keep this in mind if you have to implement slower operations, such as for performing file <a id="_idTextAnchor1003"/>uploads to <span class="No-Break">cloud storage.</span></p>&#13;
			<h1 id="_idParaDest-200"><a id="_idTextAnchor1004"/>Summary</h1>&#13;
			<p>Congratulations! You’re now able to build a fast and efficient REST API to serve your machine learning models. Thanks to Joblib, you learned how to dump a trained scikit-learn estimator into a file that’s easy to load and use inside your application. We also saw an approach to caching prediction results using Joblib. Finally, we discussed how FastAPI handles synchronous operations by sending them to a separate thread to prevent blocking. While this was a bit technical, it’s important to bear this aspect in mind when dealing with blocking <span class="No-Break">I/O operations.</span></p>&#13;
			<p>We’re near the end of our FastAPI journey. Before letting you build awesome data science applications by yourself, we will provide three more chapters to push this a bit further and study more complex use cases. We’ll start with an application that can perform real-time object detection, thanks to WebSockets and a computer <span class="No-Break">vision model.</span></p>&#13;
		</div>&#13;
	</div></body></html>