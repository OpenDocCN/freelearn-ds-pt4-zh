<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer063">&#13;
			<h1 id="_idParaDest-201" class="chapter-number"><a id="_idTextAnchor1005"/>13</h1>&#13;
			<h1 id="_idParaDest-202"><a id="_idTextAnchor1006"/>Implementing a Real-Time Object Detection System Using WebSockets with FastAPI</h1>&#13;
			<p>In the previous chapter, you learned how to create efficient REST API endpoints to make predictions with trained machine learning models. This approach covers a lot of use cases, given that we have a single observation we want to work on. In some cases, however, we may need to continuously perform predictions on a stream of input – for instance, an object detection system that works in real time with video input. This is exactly what we’ll build in this chapter. How? If you remember, besides HTTP endpoints, FastAPI also has the ability to handle WebSockets endpoints, which allow us to send and receive streams of data. In this case, the browser will send into the WebSocket a stream of images from the webcam, and our application will run an object detection algorithm and send back the coordinates and label of each detected object in the image. For this task, we’ll rely on <strong class="bold">Hugging Face</strong>, which is <a id="_idIndexMarker930"/>both a set of tools and a library of pretrained <span class="No-Break">AI models.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Using a computer vision model with Hugging <span class="No-Break">Face libraries</span></li>&#13;
				<li>Implementing an HTTP endpoint to perform object detection on a <span class="No-Break">single image</span></li>&#13;
				<li>Sending a stream of images from the browser in <span class="No-Break">a WebSocket</span></li>&#13;
				<li>Showing the object detection results in <span class="No-Break">a browser</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-203"><a id="_idTextAnchor1007"/>Technical requirements</h1>&#13;
			<p>For this chapter, you’ll require a Python virtual environment, just as we set up in <a href="B19528_01.xhtml#_idTextAnchor024"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Python Development </em><span class="No-Break"><em class="italic">Environment Setup</em></span><span class="No-Break">.</span></p>&#13;
			<p>You’ll find all the code examples for this chapter in the dedicated GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-204"><a id="_idTextAnchor1008"/>Using a computer vision model with Hugging Face</h1>&#13;
			<p>Computer vision is a <a id="_idIndexMarker931"/>field of study and technology that focuses on enabling computers to extract meaningful information from digital images or videos, simulating human vision capabilities. It involves developing algorithms based on statistical methods or machine learning that allow machines to understand, analyze, and interpret visual data. A typical example of computer vision’s application is object detection: a system able to detect and recognize objects in an image. This is the kind of system we’ll build in <span class="No-Break">this chapter.</span></p>&#13;
			<p>To help us in this task, we’ll use a set of tools provided by Hugging Face. Hugging Face is a company whose goal is to allow developers to use the most recent and powerful AI models quickly and easily. For this, it has built <span class="No-Break">two things:</span></p>&#13;
			<ul>&#13;
				<li>A set of open source Python tools built on top of machine learning libraries such as PyTorch and TensorFlow. We’ll use some of them in <span class="No-Break">this chapter.</span></li>&#13;
				<li>An online library to share and download pretrained models for various machine learning tasks, such as computer vision or <span class="No-Break">image generation.</span></li>&#13;
			</ul>&#13;
			<p>You can read more <a id="_idIndexMarker932"/>about what it's doing on its official <span class="No-Break">website: </span><a href="https://huggingface.co/"><span class="No-Break">https://huggingface.co/</span></a><span class="No-Break">.</span></p>&#13;
			<p>You’ll see that it’ll greatly help us build a powerful and accurate object detection system in no time! To begin with, we’ll install all the libraries we need for <span class="No-Break">this project:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ pip install "transformers[torch]" Pillow</pre>			<p>The <strong class="source-inline">transformers</strong> library<a id="_idIndexMarker933"/> from Hugging Face will allow us to download and run pretrained machine learning models. Notice that we install it with the optional <strong class="source-inline">torch</strong> dependency. Hugging Face tools can be used either with PyTorch or TensorFlow, which are both very powerful ML frameworks. Here, we chose to use PyTorch. Pillow is a widely used Python library for working with images. We’ll see why we need <span class="No-Break">it soon.</span></p>&#13;
			<p>Before starting to work with FastAPI, let’s implement a simple script to run an object detection algorithm. It consists of four <span class="No-Break">main steps:</span></p>&#13;
			<ol>&#13;
				<li>Load <a id="_idIndexMarker934"/>an image from the disk <span class="No-Break">using</span><span class="No-Break"><a id="_idIndexMarker935"/></span><span class="No-Break"> Pillow.</span></li>&#13;
				<li>Load a pretrained object <span class="No-Break">detection model.</span></li>&#13;
				<li>Run the model on <span class="No-Break">our image.</span></li>&#13;
				<li>Display the results by drawing rectangles around the <span class="No-Break">detected objects.</span></li>&#13;
			</ol>&#13;
			<p>We’ll go step by step through <span class="No-Break">the implementation:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_object_detection.py</p>&#13;
			<pre class="source-code">&#13;
from pathlib import Pathimport torch&#13;
from PIL import Image, ImageDraw, ImageFont&#13;
from transformers import YolosForObjectDetection, YolosImageProcessor&#13;
root_directory = Path(__file__).parent.parent&#13;
picture_path = root_directory / "assets" / "coffee-shop.jpg"&#13;
image = Image.open(picture_path)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py</a></p>&#13;
			<p>As you can see, the first step is to load our image from the disk. For this example, we use the image named <strong class="source-inline">coffee-shop.jpg</strong>, which is available in our examples repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg"><span class="No-Break">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/blob/main/assets/coffee-shop.jpg</span></a><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_object_detection.py</p>&#13;
			<pre class="source-code">&#13;
image_processor = YolosImageProcessor.from_pretrained("hustvl/yolos-tiny")model = YolosForObjectDetection.from_pretrained("hustvl/yolos-tiny")</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py</a></p>&#13;
			<p>Next, we<a id="_idIndexMarker936"/> load a model from Hugging Face. For<a id="_idIndexMarker937"/> this example, we chose the YOLOS model. It’s a cutting-edge approach to object detection that has been trained on 118K annotated images. You can read more about the technical approach in the following Hugging Face article: <a href="https://huggingface.co/docs/transformers/model_doc/yolos">https://huggingface.co/docs/transformers/model_doc/yolos</a>. To limit the download size and preserve your computer disk space, we chose here to use the tiny version, which is a lighter version of the original model that can be run on an average machine while maintaining good accuracy. This particular version is described here on Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/hustvl/yolos-tiny"><span class="No-Break">https://huggingface.co/hustvl/yolos-tiny</span></a><span class="No-Break">.</span></p>&#13;
			<p>Notice that we<a id="_idIndexMarker938"/> instantiate two things: an <strong class="bold">image processor</strong> and a <strong class="bold">model</strong>. If you remember<a id="_idIndexMarker939"/> what we said in <a href="B19528_11.xhtml#_idTextAnchor797"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Introduction to Data Science in Python</em>, you know that we need to have a set of features that will feed our ML algorithm. Hence, the role of the image processor is to transform a raw image into a set of characteristics that are meaningful to <span class="No-Break">the model.</span></p>&#13;
			<p>And that’s exactly what we’re doing in the following lines: we create an <strong class="source-inline">inputs</strong> variable by calling <strong class="source-inline">image_processor</strong> on our image. Notice that the <strong class="source-inline">return_tensors</strong> argument is set to <strong class="source-inline">pt</strong> for PyTorch since we chose to go with PyTorch as our underlying ML framework. Then, we can feed this <strong class="source-inline">inputs</strong> variable to our model to <span class="No-Break">get </span><span class="No-Break"><strong class="source-inline">outputs</strong></span><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_object_detection.py</p>&#13;
			<pre class="source-code">&#13;
inputs = image_processor(images=image, return_tensors="pt")outputs = model(**inputs)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py</a></p>&#13;
			<p>You might<a id="_idIndexMarker940"/> think that this is it for the prediction phase <a id="_idIndexMarker941"/>and that we could now display the results. However, that’s not the case. The result of such algorithms is a set of multi-dimensional matrices, the<a id="_idIndexMarker942"/> famous <strong class="bold">tensors</strong>, which don’t really make sense to us as humans. That’s why we need to revert those tensors into something that makes sense for the input image. That’s the purpose of the <strong class="source-inline">post_process_object_detection</strong> operation provided <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">image_processor</strong></span><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_object_detection.py</p>&#13;
			<pre class="source-code">&#13;
target_sizes = torch.tensor([image.size[::-1]])results = image_processor.post_process_object_detection(&#13;
    outputs, target_sizes=target_sizes&#13;
)[0]</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py</a></p>&#13;
			<p>The result of this operation is a dictionary with <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">labels</strong>: The list of labels of each <span class="No-Break">detected object</span></li>&#13;
				<li><strong class="source-inline">boxes</strong>: The coordinates of the bounding box of each <span class="No-Break">detected object</span></li>&#13;
				<li><strong class="source-inline">scores</strong>: The confidence score of the algorithm for each <span class="No-Break">detected object</span></li>&#13;
			</ul>&#13;
			<p>All we need to do then is to iterate over them so we can draw the rectangle and the corresponding label thanks to Pillow. We just show the resulting image at the end. Notice that we only consider objects with a score greater than <strong class="source-inline">0.7</strong> to limit the number of <span class="No-Break">false positives:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_object_detection.py</p>&#13;
			<pre class="source-code">&#13;
draw = ImageDraw.Draw(image)font_path = root_directory / "assets" / "OpenSans-ExtraBold.ttf"&#13;
font = ImageFont.truetype(str(font_path), 24)&#13;
for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):&#13;
    if score &gt; 0.7:&#13;
        box_values = box.tolist()&#13;
        label = model.config.id2label[label.item()]&#13;
        draw.rectangle(box_values, outline="red", width=5)&#13;
        draw.text(box_values[0:2], label, fill="red", font=font)&#13;
image.show()</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_object_detection.py</a></p>&#13;
			<p>Thanks to <a id="_idIndexMarker943"/>Pillow, we’re able to draw rectangles and <a id="_idIndexMarker944"/>add a label above the detected objects. Notice that we loaded a custom font, Open Sans, which is an open font available on the web: <a href="https://fonts.google.com/specimen/Open+Sans">https://fonts.google.com/specimen/Open+Sans</a>. Let’s try to run this script and see <span class="No-Break">the result:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ python chapter13/chapter13_object_detection.py</pre>			<p>The first time it’ll run, you’ll see the model being downloaded. The prediction can then take a few seconds to run depending on your computer. When it’s done, the resulting image should automatically open, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer059" class="IMG---Figure">&#13;
					<img src="Images/Figure_13.1_B19528.jpg" alt="Figure 13.1 – Object detection result on a sample image" width="1221" height="945"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Object detection result on a sample image</p>&#13;
			<p>You can see <a id="_idIndexMarker945"/>that the model detected several <a id="_idIndexMarker946"/>persons in the image, along with various objects such as the couch and a chair. And that’s it! Less than 30 lines of code to have a working object detection script! Hugging Face lets us harness all the power of the latest AI advances <span class="No-Break">very efficiently.</span></p>&#13;
			<p>Of course, our goal in this chapter is to put all this intelligence on a remote server so that we can serve this experience to thousands of users. Once again, FastAPI will be<a id="_idTextAnchor1009"/> our <span class="No-Break">ally here.</span></p>&#13;
			<h1 id="_idParaDest-205"><a id="_idTextAnchor1010"/>Implementing a REST endpoint to perform object detection on a single image</h1>&#13;
			<p>Before<a id="_idIndexMarker947"/> working with WebSockets, we’ll start simple and implement, using FastAPI, a clas<a id="_idTextAnchor1011"/>sic HTTP endpoin<a id="_idTextAnchor1012"/>t to accept image uploads and perform object detection on them. As you’ll see, the main difference from the previous example is in how we acquire the image: instead of reading it from the disk, we get it from a file upload that we have to convert into a Pillow <span class="No-Break">image object.</span></p>&#13;
			<p>Besides, we’ll also use the exact same pattern we saw in <a href="B19528_12.xhtml#_idTextAnchor960"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">Creating an Efficient Prediction API Endpoint with FastAPI</em> – that is, having a dedicated class for our prediction model, which will be loaded during the <span class="No-Break">lifespan handler.</span></p>&#13;
			<p>The first thing we do in this implementation is to define Pydantic models in order to properly structure the output of our prediction model. You can see this <span class="No-Break">as follows:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_api.py</p>&#13;
			<pre class="source-code">&#13;
class Object(BaseModel):    box: tuple[float, float, float, float]&#13;
    label: str&#13;
class Objects(BaseModel):&#13;
    objects: list[Object]</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py</a></p>&#13;
			<p>We have a model for a single detected object, which consists of <strong class="source-inline">box</strong>, a tuple of four numbers describing the coordinates of the bounding box, and <strong class="source-inline">label</strong>, which corresponds to the type of detected object. The <strong class="source-inline">Objects</strong> model is a simple structure bearing a list <span class="No-Break">of objects.</span></p>&#13;
			<p>We won’t go through the model prediction class, as it’s very similar to what we saw in the previous chapter and section. Instead, let’s directly focus on the FastAPI <span class="No-Break">endpoint implementation:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">chapter13_api.py</p>&#13;
			<pre class="source-code">&#13;
object_detection = ObjectDetection()@contextlib.asynccontextmanager&#13;
async def lifespan(app: FastAPI):&#13;
    object_detection.load_model()&#13;
    yield&#13;
app = FastAPI(lifespan=lifespan)&#13;
@app.post("/object-detection", response_model=Objects)&#13;
async def post_object_detection(image: UploadFile = File(...)) -&gt; Objects:&#13;
    image_object = Image.open(image.file)&#13;
    return object_detection.predict(image_object)</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/chapter13_api.py</a></p>&#13;
			<p>Nothing very <a id="_idIndexMarker948"/>surprising here! The main point of attention is to correctly use the <strong class="source-inline">UploadFile</strong> and <strong class="source-inline">File</strong> dependencies so we get the uploaded file. If you need a refresher on this, you can check the <em class="italic">Form data and file uploads</em> section from <a href="B19528_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Developing a RESTful API with FastAPI</em>. All we need to do then is to instantiate it as a proper Pillow image object and call our <span class="No-Break">prediction model.</span></p>&#13;
			<p>As we said, we don’t forget to load the model inside the <span class="No-Break">lifespan handler.</span></p>&#13;
			<p>You can run this example using the usual <span class="No-Break">Uvicorn command:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ uvicorn chapter13.chapter13_api:app</pre>			<p>We’ll<a id="_idIndexMarker949"/> use the same coffee shop picture we already saw in the previous section. Let’s upload it on our endpoint <span class="No-Break">with HTTPie:</span></p>&#13;
			<pre class="source-code">&#13;
$ http --form POST http://localhost:8000/object-detection image@./assets/coffee-shop.jpg{&#13;
    "objects": [&#13;
        {&#13;
            "box": [659.8709716796875, 592.8882446289062, 792.0460815429688, 840.2132568359375],&#13;
            "label": "person"&#13;
        },&#13;
        {&#13;
            "box": [873.5499267578125, 875.7918090820312, 1649.1378173828125, 1296.362548828125],&#13;
            "label": "couch"&#13;
        }&#13;
    ]&#13;
}</pre>&#13;
			<p>We correctly get the list of detected objects, each one with its bounding box and label. Great! Our obje<a id="_idTextAnchor1013"/>ct detection syst<a id="_idTextAnchor1014"/>em is now available as a web server. However, our goal is still to make a real-time system: thanks to WebSockets, we’ll be able to handle a<a id="_idTextAnchor1015"/> stream <span class="No-Break">of images.</span></p>&#13;
			<h1 id="_idParaDest-206"><a id="_idTextAnchor1016"/>Implementing a WebSocket to perform object detection on a stream of images</h1>&#13;
			<p>One of the main benefits of WebSockets, as we saw in <a href="B19528_08.xhtml#_idTextAnchor551"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Defining WebSockets for Two-Way Interactive Communication in FastAPI</em>, is that it opens a full-duplex communication channel between the client and the server. Once the connection is established, messages can be passed quickly without having to go through all the steps of the HTTP protocol. Therefore, it’s much more suited to sending a lot of data in <span class="No-Break">real time.</span></p>&#13;
			<p>The point here w<a id="_idTextAnchor1017"/>ill be to implement a WebSocket en<a id="_idTextAnchor1018"/>dpoint that is able to both accept image data and run object detection on it. The main challenge her<a id="_idTextAnchor1019"/>e will be to handle a phenomenon known as <strong class="bold">backpressure</strong>. Put simply, we’ll receive more images from the browser than the server is able to <a id="_idIndexMarker950"/>handle because of the time needed to run the detection algorithm. Thus, we’ll have to work with a queue (or buffer) of limited size and drop some images along the way to handle the stream in near <span class="No-Break">real time.</span></p>&#13;
			<p>We’ll go step by step through <span class="No-Break">the implementation:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">app.py</p>&#13;
			<pre class="source-code">&#13;
async def receive(websocket: WebSocket, queue: asyncio.Queue):    while True:&#13;
        bytes = await websocket.receive_bytes()&#13;
        try:&#13;
            queue.put_nowait(bytes)&#13;
        except asyncio.QueueFull:&#13;
            pass&#13;
async def detect(websocket: WebSocket, queue: asyncio.Queue):&#13;
    while True:&#13;
        bytes = await queue.get()&#13;
        image = Image.open(io.BytesIO(bytes))&#13;
        objects = object_detection.predict(image)&#13;
        await websocket.send_json(objects.dict())</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py</a></p>&#13;
			<p>We defined two tasks: <strong class="source-inline">receive</strong> and <strong class="source-inline">detect</strong>. The first one is waiting for raw bytes from the WebSock<a id="_idTextAnchor1020"/>et, while the second one is performing the detection and sending the result, exactly as we saw in the <span class="No-Break">last section.</span></p>&#13;
			<p>The key here is to use the <strong class="source-inline">asyncio.Queue</strong> object. This is a convenient structure allowing u<a id="_idTextAnchor1021"/>s to queue some data in memory and retrieve it in a <strong class="bold">first in, first out</strong> (<strong class="bold">FIFO</strong>) strategy. We are <a id="_idIndexMarker951"/>able to set a limit on the number of elements we store in the queue: this is how we’ll be able to limit the number of images <span class="No-Break">we handle.</span></p>&#13;
			<p>The <strong class="source-inline">receive</strong> function receives data and puts it at the end of the queue. When working with <strong class="source-inline">asyncio.Queue</strong>, we have two methods to put a new element in the queue: <strong class="source-inline">put</strong> and <strong class="source-inline">put_nowait</strong>. If the queue is full, the first one will wait until there is room in the queue. This is not what we want here: we want to drop images that we won’t be able to handle in time. With <strong class="source-inline">put_nowait</strong>, the <strong class="source-inline">QueueFull</strong> exception is raised if the queue is full. In this case, we just pass and drop <span class="No-Break">the data.</span></p>&#13;
			<p>On the other hand, the <strong class="source-inline">detect</strong> function pulls the first message from the queue and runs its detection before sending the result. Notice that since we get raw image bytes directly, we have to wrap them with <strong class="source-inline">io.BytesIO</strong> to make it acceptable <span class="No-Break">for Pillow.</span></p>&#13;
			<p>The WebSocket implementation in itself is similar to what we saw in <a href="B19528_08.xhtml#_idTextAnchor551"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Defining WebSockets for Two-Way Interactive Communication in FastAPI</em>. We are scheduling both tasks and waiting until one of them has stopped. Since they both run an infinite loop, this will happen when the WebSocket <span class="No-Break">is disconnected:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">app.py</p>&#13;
			<pre class="source-code">&#13;
@app.websocket("/object-detection")async def ws_object_detection(websocket: WebSocket):&#13;
    await websocket.accept()&#13;
    queue: asyncio.Queue = asyncio.Queue(maxsize=1)&#13;
    receive_task = asyncio.create_task(receive(websocket, queue))&#13;
    detect_task = asyncio.create_task(detect(websocket, queue))&#13;
    try:&#13;
        done, pending = await asyncio.wait(&#13;
            {receive_task, detect_task},&#13;
            return_when=asyncio.FIRST_COMPLETED,&#13;
        )&#13;
        for task in pending:&#13;
            task.cancel()&#13;
        for task in done:&#13;
            task.result()&#13;
    except WebSocketDisconnect:&#13;
        pass</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/app.py</a></p>&#13;
			<p class="callout-heading">Serving static files</p>&#13;
			<p class="callout">If you look at the full implementation of the preceding example, you’ll notice that we defined two more things in our server: an <strong class="source-inline">index</strong> endpoint, which just returns the <strong class="source-inline">index.html</strong> file, and a <strong class="source-inline">StaticFiles</strong> app, which is mounted under the <strong class="source-inline">/assets</strong> path. Both of them are here to allow our FastAPI application to directly serve our HTML and JavaScript code. This way, browsers will be able to query those files on the <span class="No-Break">same server.</span></p>&#13;
			<p class="callout">The key takeaway of this is that even though FastAPI was designed to build REST APIs, it’s also perfectly able to serve HTML and <span class="No-Break">static files.</span></p>&#13;
			<p>Our backend is now ready! Let’s now see how to use its p<a id="_idTextAnchor1022"/>ower from <span class="No-Break">a browser.</span></p>&#13;
			<h1 id="_idParaDest-207"><a id="_idTextAnchor1023"/>Sending a stream of images from the browser in  a WebSocket</h1>&#13;
			<p>In this section, we’ll see <a id="_idIndexMarker952"/>how you can capture images from the webcam i<a id="_idTextAnchor1024"/>n the browser and send them through a WebS<a id="_idTextAnchor1025"/>ocket. Since it mainly involves JavaScript code, it’s admittedly a bit beyond the scope of this book, but it’s necessary to make the application <span class="No-Break">work fully.</span></p>&#13;
			<p>The first step is to enable a camera input in the browser, open the WebSocket connection, pick a camera image, and send it through the WebSocket. Basically, it’ll work like this: thanks to the <strong class="source-inline">MediaDevices</strong> browser API, we’ll be able to list all the camera inputs available on the device. With this, we’ll build a selection form so the user can select the camera they want to use. You can see t<a id="_idTextAnchor1026"/>he concrete JavaScript implementation in the <span class="No-Break">following code:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
window.addEventListener('DOMContentLoaded', (event) =&gt; {  const video = document.getElementById('video');&#13;
  const canvas = document.getElementById('canvas');&#13;
  const cameraSelect = document.getElementById('camera-select');&#13;
  let socket;&#13;
  // List available cameras and fill select&#13;
  navigator.mediaDevices.getUserMedia({ audio: true, video: true }).then(() =&gt; {&#13;
    navigator.mediaDevices.enumerateDevices().then((devices) =&gt; {&#13;
      for (const device of devices) {&#13;
        if (device.kind === 'videoinput' &amp;&amp; device.deviceId) {&#13;
          const deviceOption = document.createElement('option');&#13;
          deviceOption.value = device.deviceId;&#13;
          deviceOption.innerText = device.label;&#13;
          cameraSelect.appendChild(deviceOption);&#13;
        }&#13;
      }&#13;
    });&#13;
  });</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js</a></p>&#13;
			<p>Once the<a id="_idIndexMarker953"/> user submits the form, we call a <strong class="source-inline">startObjectDetection</strong> function with the selected camera. Most of the actual detection logic is implemented in <span class="No-Break">this function:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
  // Start object detection on the selected camera on submit  document.getElementById('form-connect').addEventListener('submit', (event) =&gt; {&#13;
    event.preventDefault();&#13;
    // Close previous socket is there is one&#13;
    if (socket) {&#13;
      socket.close();&#13;
    }&#13;
    const deviceId = cameraSelect.selectedOptions[0].value;&#13;
    socket = startObjectDetection(video, canvas, deviceId);&#13;
  });&#13;
});</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js</a></p>&#13;
			<p>Let’s have a<a id="_idIndexMarker954"/> look at the <strong class="source-inline">startObjectDetection</strong> function in the following code bl<a id="_idTextAnchor1027"/>ock. First, we est<a id="_idTextAnchor1028"/>ablish a connection with the WebSocket. Once it’s opened, we can start to get an image stream from the selected camera. For this, we use the <strong class="source-inline">MediaDevices</strong> API to start capturing video and display the output in an HTML <strong class="source-inline">&lt;video&gt;</strong> element. You can read all the details about the <strong class="source-inline">MediaDevices</strong> API in the MDN <span class="No-Break">documentation: </span><a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices"><span class="No-Break">https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices</span></a><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
const startObjectDetection = (video, canvas, deviceId) =&gt; {  const socket = new WebSocket(`ws://${location.host}/object-detection`);&#13;
  let intervalId;&#13;
  // Connection opened&#13;
  socket.addEventListener('open', function () {&#13;
    // Start reading video from device&#13;
    navigator.mediaDevices.getUserMedia({&#13;
      audio: false,&#13;
      video: {&#13;
        deviceId,&#13;
        width: { max: 640 },&#13;
        height: { max: 480 },&#13;
      },&#13;
    }).then(function (stream) {&#13;
      video.srcObject = stream;&#13;
      video.play().then(() =&gt; {&#13;
        // Adapt overlay canvas size to the video size&#13;
        canvas.width = video.videoWidth;&#13;
        canvas.height = video.videoHeight;</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js</a></p>&#13;
			<p>Then, as shown<a id="_idIndexMarker955"/> in the next code block, we launch a repetitive task that captures an image from the video input and sends it to the server. To do this, we have to use a <strong class="source-inline">&lt;canvas&gt;</strong> element, an HTML tag dedicated to graphics drawing. It comes with a complete JavaScript API so that we can programmatically draw images in it. There, we’ll be able to draw the current video image and convert it into valid JPEG bytes. If you want to know more about this, MDN gives a very detailed tutorial on <strong class="source-inline">&lt;</strong><span class="No-Break"><strong class="source-inline">canvas&gt;</strong></span><span class="No-Break">: </span><a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial"><span class="No-Break">https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial</span></a><span class="No-Break">:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
        // Send an image in the WebSocket every 42 ms        intervalId = setInterval(() =&gt; {&#13;
          // Create a virtual canvas to draw current video image&#13;
          const canvas = document.createElement('canvas');&#13;
          const ctx = canvas.getContext('2d');&#13;
          canvas.width = video.videoWidth;&#13;
          canvas.height = video.videoHeight;&#13;
          ctx.drawImage(video, 0, 0);&#13;
          // Convert it to JPEG and send it to the WebSocket&#13;
          canvas.toBlob((blob) =&gt; socket.send(blob), 'image/jpeg');&#13;
        }, IMAGE_INTERVAL_MS);&#13;
      });&#13;
    });&#13;
  });</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js</a></p>&#13;
			<p>Notice that we<a id="_idIndexMarker956"/> limit the size of the video input to 640 by 480 pixels, so that we don’t blow up the server with images that are too big. Besides, we set the interval to run every 42 milliseconds (the value is set in the <strong class="source-inline">IMAGE_INTERVAL_MS</strong> constant), which is roughly equivalent to 24 images <span class="No-Break">per second.</span></p>&#13;
			<p>Finally, we wire the event listener to handle the messages received from the WebSocket. It calls the <strong class="source-inline">drawObjects</strong> function, which we’ll detail in the <span class="No-Break">next section:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
  // Listen for messages  socket.addEventListener('message', function (event) {&#13;
    drawObjects(video, canvas, JSON.parse(event.data));&#13;
  });&#13;
  // Stop the interval and video reading on close&#13;
  socket.addEventListener('close', function () {&#13;
    window.clearInterval(intervalId);&#13;
    video.pause();&#13;
  });&#13;
  return socket;&#13;
};</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_d<span id="_idTextAnchor1029"/>etection/assets/script.js</a></p>&#13;
			<h1 id="_idParaDest-208"><a id="_idTextAnchor1030"/>Showing the object detection results in the browser</h1>&#13;
			<p>No<a id="_idTextAnchor1031"/>w that we are able to send<a id="_idIndexMarker957"/> input images to the server, we have to show the result of the detection in the browser. In a similar way to what we showed in the <em class="italic">Using a computer vision model with Hugging Face</em> section, we’ll draw a green rectangle around the detected objects, along with their label. Thus, we have to find a way to take the rectangle coordinates sent by the server and draw them in <span class="No-Break">the brow<a id="_idTextAnchor1032"/>ser.</span></p>&#13;
			<p>To do this, we’ll once again use a <strong class="source-inline">&lt;canvas&gt;</strong> element. This time, it’ll be visible to the user and we’ll draw the rectangles using it. The trick here is to use CSS so that this element overlays the video: this way, the rectangles will be shown right on top of the video and the <a id="_idIndexMarker958"/>corresponding objects. You can see the HTML <span class="No-Break">code here:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">index.html</p>&#13;
			<pre class="source-code">&#13;
&lt;body&gt;  &lt;div class="container"&gt;&#13;
    &lt;h1 class="my-3"&gt;Chapter 13 - Real time object detection&lt;/h1&gt;&#13;
    &lt;form id="form-connect"&gt;&#13;
      &lt;div class="input-group mb-3"&gt;&#13;
        &lt;select id="camera-select"&gt;&lt;/select&gt;&#13;
        &lt;button class="btn btn-success" type="submit" id="button-start"&gt;Start&lt;/button&gt;&#13;
      &lt;/div&gt;&#13;
    &lt;/form&gt;&#13;
    &lt;div class="position-relative" style="width: 640px; height: 480px;"&gt;&#13;
      &lt;video id="video"&gt;&lt;/video&gt;&#13;
      &lt;canvas id="canvas" class="position-absolute top-0 start-0"&gt;&lt;/canvas&gt;&#13;
    &lt;/div&gt;&#13;
  &lt;/div&gt;&#13;
  &lt;script src="/assets/script.js"&gt;&lt;/script&gt;&#13;
&lt;/body&gt;</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/index.html</a></p>&#13;
			<p>We are using <a id="_idIndexMarker959"/>CSS classes from Bootstrap, a very common CSS library with a lot of helpers like this. Basically, we set the canvas with absolute positioning and put it at the top left so that it cove<a id="_idTextAnchor1033"/>rs the <span class="No-Break">video element.</span></p>&#13;
			<p>The key<a id="_idTextAnchor1034"/> now is to use the Canvas API to draw the rectangles according to the received coordinates. This is the purpose of the <strong class="source-inline">drawObjects</strong> function, which is shown in the next sample <span class="No-Break">code block:</span></p>&#13;
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">script.js</p>&#13;
			<pre class="source-code">&#13;
const drawObjects = (video, canvas, objects) =&gt; {  const ctx = canvas.getContext('2d');&#13;
  ctx.width = video.videoWidth;&#13;
  ctx.height = video.videoHeight;&#13;
  ctx.beginPath();&#13;
  ctx.clearRect(0, 0, ctx.width, ctx.height);&#13;
  for (const object of objects.objects) {&#13;
    const [x1, y1, x2, y2] = object.box;&#13;
    const label = object.label;&#13;
    ctx.strokeStyle = '#49fb35';&#13;
    ctx.beginPath();&#13;
    ctx.rect(x1, y1, x2 - x1, y2 - y1);&#13;
    ctx.stroke();&#13;
    ctx.font = 'bold 16px sans-serif';&#13;
    ctx.fillStyle = '#ff0000';&#13;
    ctx.fillText(label, x1 - 5 , y1 - 5);&#13;
  }&#13;
};</pre>&#13;
			<p class="SC---Link" lang="en-US" xml:lang="en-US"><a href="https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js">https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter13/websocket_object_detection/assets/script.js</a></p>&#13;
			<p>With the <strong class="source-inline">&lt;canvas&gt;</strong> element, we <a id="_idIndexMarker960"/>can use a 2D context to draw things in the object. Notice that we first clean everything to remove the rectangles from the previous detection. Then, we loop through all the detected objects and draw a rectangle with the given coordinates: <strong class="source-inline">x1</strong>, <strong class="source-inline">y1</strong>, <strong class="source-inline">x2</strong>, and <strong class="source-inline">y2</strong>. Finally, we take care of drawing the label slightly above <span class="No-Break">the rectangle.</span></p>&#13;
			<p>Our system is now complete! <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.2</em> gives you an overview of the file structure <span class="No-Break">we’ve implemented.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer060" class="IMG---Figure">&#13;
					<img src="Images/Figure_13.2_B19528.jpg" alt="Figure 13.2 – Object detection application structure" width="708" height="501"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Object detection ap<a id="_idTextAnchor1035"/>plication structure</p>&#13;
			<p>It’s time to give it a try! We <a id="_idIndexMarker961"/>can start it using the usual <span class="No-Break">Uvicorn command:</span></p>&#13;
			<pre class="source-code">&#13;
(venv) $ uvicorn chapter13.websocket_object_detection.app:app</pre>			<p>You can access the application in your browser with the address <strong class="source-inline">http://localhost:8000</strong>. As we said in the previous section, the <strong class="source-inline">index</strong> endpoint will be called and will return our <span class="No-Break"><strong class="source-inline">index.html</strong></span><span class="No-Break"> file.</span></p>&#13;
			<p>You’ll see an interface <a id="_idIndexMarker962"/>inviting you to choose the camera you want to use, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer061" class="IMG---Figure">&#13;
					<img src="Images/Figure_13.3_B19528.jpg" alt="Figure 13.3 – Webcam selection for the object detection web application" width="1233" height="526"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Webcam selection for the object detection web<a id="_idTextAnchor1036"/> application</p>&#13;
			<p>Select the webcam <a id="_idIndexMarker963"/>you wish to us<a id="_idTextAnchor1037"/>e and click on <strong class="bold">Start</strong>. The video output will show up, object detection will start via the WebSocket, and green rectangles will be drawn around the detected objects. We show this in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer062" class="IMG---Figure">&#13;
					<img src="Images/Figure_13.4_B19528.jpg" alt="Figure 13.4 – Running the object detection web application" width="1178" height="1049"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Running the object detection web ap<a id="_idTextAnchor1038"/>plication</p>&#13;
			<p>It works! We brought <a id="_idTextAnchor1039"/><a id="_idIndexMarker964"/>the intelligence of our Python system right to the user’s web browser. This is just an example of what you<a id="_idIndexMarker965"/> could achieve using WebSockets and ML algorithms, but this definitely enables you to create near real-time experiences for <span class="No-Break">your users.</span></p>&#13;
			<h1 id="_idParaDest-209"><a id="_idTextAnchor1040"/>Summary</h1>&#13;
			<p>In this chapter, we showed how WebSockets can help us bring a more interactive experience to users. Thanks to the pretrained models provided by the Hugging Face community, we were able to quickly implement an object detection system. Then, we integrated it into a WebSocket endpoint with the help of FastAPI. Finally, by using a modern JavaScript API, we sent video input and displayed algorithm results directly in the browser. All in all, a project like this might sound complex to make at first, but we saw that powerful tools such as FastAPI enable us to get results in a very short time and with very comprehensible <span class="No-Break">source code.</span></p>&#13;
			<p>Until now, in our different examples and projects, we assumed the ML model we used was fast enough to be run directly in an API endpoint or a WebSocket task. However, that’s not always the case. In some cases, the algorithm is so complex it takes a couple of minutes to run. If we run this kind of algorithm directly inside an API endpoint, the user would have to wait a long time before getting a response. Not only would this be strange for them but this would also quickly block the whole server, preventing other users from using the API. To solve this, we’ll need a companion for our API server: <span class="No-Break">a worker.</span></p>&#13;
			<p>In the next chapter, we’ll study a concrete example of this challenge: we’ll build our very own AI system to generate images from a <span class="No-Break">text prompt!</span></p>&#13;
		</div>&#13;
	</div></body></html>