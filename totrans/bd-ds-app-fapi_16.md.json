["```py\n\n(venv) $ pip install \"transformers[torch]\" Pillow\n```", "```py\n\nfrom pathlib import Pathimport torch\nfrom PIL import Image, ImageDraw, ImageFont\nfrom transformers import YolosForObjectDetection, YolosImageProcessor\nroot_directory = Path(__file__).parent.parent\npicture_path = root_directory / \"assets\" / \"coffee-shop.jpg\"\nimage = Image.open(picture_path)\n```", "```py\n\nimage_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")model = YolosForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n```", "```py\n\ninputs = image_processor(images=image, return_tensors=\"pt\")outputs = model(**inputs)\n```", "```py\n\ntarget_sizes = torch.tensor([image.size[::-1]])results = image_processor.post_process_object_detection(\n    outputs, target_sizes=target_sizes\n)[0]\n```", "```py\n\ndraw = ImageDraw.Draw(image)font_path = root_directory / \"assets\" / \"OpenSans-ExtraBold.ttf\"\nfont = ImageFont.truetype(str(font_path), 24)\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    if score > 0.7:\n        box_values = box.tolist()\n        label = model.config.id2label[label.item()]\n        draw.rectangle(box_values, outline=\"red\", width=5)\n        draw.text(box_values[0:2], label, fill=\"red\", font=font)\nimage.show()\n```", "```py\n\n(venv) $ python chapter13/chapter13_object_detection.py\n```", "```py\n\nclass Object(BaseModel):    box: tuple[float, float, float, float]\n    label: str\nclass Objects(BaseModel):\n    objects: list[Object]\n```", "```py\n\nobject_detection = ObjectDetection()@contextlib.asynccontextmanager\nasync def lifespan(app: FastAPI):\n    object_detection.load_model()\n    yield\napp = FastAPI(lifespan=lifespan)\n@app.post(\"/object-detection\", response_model=Objects)\nasync def post_object_detection(image: UploadFile = File(...)) -> Objects:\n    image_object = Image.open(image.file)\n    return object_detection.predict(image_object)\n```", "```py\n\n(venv) $ uvicorn chapter13.chapter13_api:app\n```", "```py\n\n$ http --form POST http://localhost:8000/object-detection image@./assets/coffee-shop.jpg{\n    \"objects\": [\n        {\n            \"box\": [659.8709716796875, 592.8882446289062, 792.0460815429688, 840.2132568359375],\n            \"label\": \"person\"\n        },\n        {\n            \"box\": [873.5499267578125, 875.7918090820312, 1649.1378173828125, 1296.362548828125],\n            \"label\": \"couch\"\n        }\n    ]\n}\n```", "```py\n\nasync def receive(websocket: WebSocket, queue: asyncio.Queue):    while True:\n        bytes = await websocket.receive_bytes()\n        try:\n            queue.put_nowait(bytes)\n        except asyncio.QueueFull:\n            pass\nasync def detect(websocket: WebSocket, queue: asyncio.Queue):\n    while True:\n        bytes = await queue.get()\n        image = Image.open(io.BytesIO(bytes))\n        objects = object_detection.predict(image)\n        await websocket.send_json(objects.dict())\n```", "```py\n\n@app.websocket(\"/object-detection\")async def ws_object_detection(websocket: WebSocket):\n    await websocket.accept()\n    queue: asyncio.Queue = asyncio.Queue(maxsize=1)\n    receive_task = asyncio.create_task(receive(websocket, queue))\n    detect_task = asyncio.create_task(detect(websocket, queue))\n    try:\n        done, pending = await asyncio.wait(\n            {receive_task, detect_task},\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n        for task in pending:\n            task.cancel()\n        for task in done:\n            task.result()\n    except WebSocketDisconnect:\n        pass\n```", "```py\n\nwindow.addEventListener('DOMContentLoaded', (event) => {  const video = document.getElementById('video');\n  const canvas = document.getElementById('canvas');\n  const cameraSelect = document.getElementById('camera-select');\n  let socket;\n  // List available cameras and fill select\n  navigator.mediaDevices.getUserMedia({ audio: true, video: true }).then(() => {\n    navigator.mediaDevices.enumerateDevices().then((devices) => {\n      for (const device of devices) {\n        if (device.kind === 'videoinput' && device.deviceId) {\n          const deviceOption = document.createElement('option');\n          deviceOption.value = device.deviceId;\n          deviceOption.innerText = device.label;\n          cameraSelect.appendChild(deviceOption);\n        }\n      }\n    });\n  });\n```", "```py\n\n  // Start object detection on the selected camera on submit  document.getElementById('form-connect').addEventListener('submit', (event) => {\n    event.preventDefault();\n    // Close previous socket is there is one\n    if (socket) {\n      socket.close();\n    }\n    const deviceId = cameraSelect.selectedOptions[0].value;\n    socket = startObjectDetection(video, canvas, deviceId);\n  });\n});\n```", "```py\n\nconst startObjectDetection = (video, canvas, deviceId) => {  const socket = new WebSocket(`ws://${location.host}/object-detection`);\n  let intervalId;\n  // Connection opened\n  socket.addEventListener('open', function () {\n    // Start reading video from device\n    navigator.mediaDevices.getUserMedia({\n      audio: false,\n      video: {\n        deviceId,\n        width: { max: 640 },\n        height: { max: 480 },\n      },\n    }).then(function (stream) {\n      video.srcObject = stream;\n      video.play().then(() => {\n        // Adapt overlay canvas size to the video size\n        canvas.width = video.videoWidth;\n        canvas.height = video.videoHeight;\n```", "```py\n\n        // Send an image in the WebSocket every 42 ms        intervalId = setInterval(() => {\n          // Create a virtual canvas to draw current video image\n          const canvas = document.createElement('canvas');\n          const ctx = canvas.getContext('2d');\n          canvas.width = video.videoWidth;\n          canvas.height = video.videoHeight;\n          ctx.drawImage(video, 0, 0);\n          // Convert it to JPEG and send it to the WebSocket\n          canvas.toBlob((blob) => socket.send(blob), 'image/jpeg');\n        }, IMAGE_INTERVAL_MS);\n      });\n    });\n  });\n```", "```py\n\n  // Listen for messages  socket.addEventListener('message', function (event) {\n    drawObjects(video, canvas, JSON.parse(event.data));\n  });\n  // Stop the interval and video reading on close\n  socket.addEventListener('close', function () {\n    window.clearInterval(intervalId);\n    video.pause();\n  });\n  return socket;\n};\n```", "```py\n\n<body>  <div class=\"container\">\n    <h1 class=\"my-3\">Chapter 13 - Real time object detection</h1>\n    <form id=\"form-connect\">\n      <div class=\"input-group mb-3\">\n        <select id=\"camera-select\"></select>\n        <button class=\"btn btn-success\" type=\"submit\" id=\"button-start\">Start</button>\n      </div>\n    </form>\n    <div class=\"position-relative\" style=\"width: 640px; height: 480px;\">\n      <video id=\"video\"></video>\n      <canvas id=\"canvas\" class=\"position-absolute top-0 start-0\"></canvas>\n    </div>\n  </div>\n  <script src=\"img/script.js\"></script>\n</body>\n```", "```py\n\nconst drawObjects = (video, canvas, objects) => {  const ctx = canvas.getContext('2d');\n  ctx.width = video.videoWidth;\n  ctx.height = video.videoHeight;\n  ctx.beginPath();\n  ctx.clearRect(0, 0, ctx.width, ctx.height);\n  for (const object of objects.objects) {\n    const [x1, y1, x2, y2] = object.box;\n    const label = object.label;\n    ctx.strokeStyle = '#49fb35';\n    ctx.beginPath();\n    ctx.rect(x1, y1, x2 - x1, y2 - y1);\n    ctx.stroke();\n    ctx.font = 'bold 16px sans-serif';\n    ctx.fillStyle = '#ff0000';\n    ctx.fillText(label, x1 - 5 , y1 - 5);\n  }\n};\n```", "```py\n\n(venv) $ uvicorn chapter13.websocket_object_detection.app:app\n```"]