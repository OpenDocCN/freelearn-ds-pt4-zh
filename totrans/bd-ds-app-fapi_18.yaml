- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring the Health and Performance of a Data Science System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the extra mile so you are able to build robust,
    production-ready systems. One of the most important aspects to achieve this is
    to have all the data we need to ensure the system is operating correctly and detect
    as soon as possible when something goes wrong so we can take corrective actions.
    In this chapter, we’ll see how to set up a proper logging facility and how we
    can monitor the performance and health of our software in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re near the end of our journey into FastAPI for data science. Until now,
    we’ve mainly focused on the functionality of the programs we implemented. However,
    there is another aspect that is often overlooked by developers but is actually
    very important: *assessing whether the system is functioning correctly and reliably
    in production* and being warned as soon as possible when that’s not the case.'
  prefs: []
  type: TYPE_NORMAL
- en: For this, lot of tools and techniques exist so we can gather the maximum amount
    of data about how our program is performing. That’s what we’ll review in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and using a logging facility with Loguru
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Prometheus metrics and monitoring them in Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Sentry for reporting errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you’ll require a Python virtual environment, just as we set
    up in [*Chapter 1*](B19528_01.xhtml#_idTextAnchor024), *Python Development* *Environment
    Setup*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run a Dramatiq worker, you’ll need a running Redis server on your local
    computer. The easiest way is to run it as a Docker container. If you’ve never
    used Docker before, we recommend you read the *Getting started* tutorial in the
    official documentation at [https://docs.docker.com/get-started/](https://docs.docker.com/get-started/).
    Once done, you’ll be able to run a Redis server with this simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You’ll find all the code examples of this chapter in the dedicated GitHub repository
    at [https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15).
  prefs: []
  type: TYPE_NORMAL
- en: A note about the screenshots
  prefs: []
  type: TYPE_NORMAL
- en: 'In the course of this chapter, we’ll present several screenshots, in particular
    of the Grafana interface. Their goal is to show you the general layout of the
    UI to help you identify its different parts. Don’t worry if you struggle to read
    the actual content: the explanations around them will explain where to look at
    and what to interact with.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and using a logging facility with Loguru
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In software development, logs are probably the simplest but most powerful way
    to control the behavior of a system. They usually consist of lines of plain text
    that are printed at specific points of a program. By reading them chronologically,
    we are able to trace the behavior of the program and check that everything goes
    well. Actually, we’ve already seen log lines in this book. When you run a FastAPI
    app with Uvicorn and make some requests, you’ll see these lines in the console
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Those are the logs generated by Uvicorn, which tell us when it has started and
    when it has handled a request. As you can see, logs can help us to know what happened
    in our program and what actions it performed. They can also tell us when something
    goes wrong, which could be a bug that needs to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding log levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Notice that before each log line, we have the `INFO` keyword. This is what
    we call the **log level**. It’s a way to classify the importance of this log.
    In general, the following levels are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DEBUG`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`INFO`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WARNING`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERROR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can consider this the *level of importance*: `DEBUG` is really specific
    information about what the program does, which could help you to debug the code,
    while `ERROR` means that something bad happened in your program, which probably
    requires action on your part. The good thing about those levels is that we can
    *configure the minimum level* that should be output by the logger. The actual
    call to the log function is still there in the code, but it’s ignored by the logger
    if it doesn’t match the minimum level.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we can set the `DEBUG` level in local development so we have all
    the information to help us develop and fix our program. On the other hand, we
    can set the level to `INFO` or `WARNING` in production so we have only the most
    important messages.
  prefs: []
  type: TYPE_NORMAL
- en: Adding logs with Loguru
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding your own logs to a Python program can be fairly easy using the `logging`
    module available in the standard library. You could do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it’s just a function call with a string in the argument. Typically,
    logging modules expose the different levels as methods, as you see here with `warning`.
  prefs: []
  type: TYPE_NORMAL
- en: The standard `logging` module is really powerful and allows you to finely customize
    how your logs are handled, printed, and formatted. If you go through the logging
    tutorials in the official documentation, [https://docs.python.org/3/howto/logging.html](https://docs.python.org/3/howto/logging.html),
    you’ll see it can quickly become really complex, even for simple cases.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why Python developers usually use libraries wrapping the `logging` module
    and exposing much more friendly functions and interfaces. In this chapter, we’ll
    review how to use and configure **Loguru**, a modern yet simple approach to logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, the first thing to do is to install it in our Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can try it right away in a Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You may think that’s not very different from what we did with the standard
    `logging` module. However, notice the resulting log already includes the timestamp,
    the level, and the position of the function call in the code. That’s one of the
    main benefits of Loguru: it comes with sensible defaults working out of the box.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see it in action in a more complete script. We’ll define a simple function
    to check whether an integer, `n`, is odd or not. We’ll add a debug line to let
    us know the function starts its logic. Then, before computing the result, we’ll
    first check whether `n` truly is an integer and log an error if not. The implementation
    of this function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_logs_01.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_01.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it’s really simple to use: we just have to import `logger`
    and call it wherever we need to log something. Notice also how we can add variables
    to format our string: we just need to add a placeholder around curly braces inside
    the string and then map each placeholder to its value with keyword arguments.
    This syntax is actually similar to the standard `str.format` method. You can read
    more about it in the official Python documentation: [https://docs.python.org/fr/3/library/stdtypes.html#str.format](https://docs.python.org/fr/3/library/stdtypes.html#str.format).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run this simple script, we’ll see our log lines in the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our log lines are correctly added to the output before the actual exception
    is raised. Notice how Loguru is able to precisely tell us where the log call comes
    from in the code: we have the function’s name and line.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and configuring sinks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve seen that, by default, logs are added to the console output. By default,
    Loguru defines a **sink** targeted at a standard error. A sink is a concept introduced
    by Loguru to define how log lines should be handled by the logger. We’re not limited
    to console output: we can also save them to a file, or a database, or even send
    them to a web service!'
  prefs: []
  type: TYPE_NORMAL
- en: The good thing is that you’re not limited to only one sink; you can have as
    many as you need! Then, each log call will be processed through each sink accordingly.
    You can see a schematic representation of this approach in *Figure 15**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Schema of Loguru sinks](img/Figure_15.01_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Schema of Loguru sinks
  prefs: []
  type: TYPE_NORMAL
- en: 'Each *sink is associated* *with* *a log level*. This means that we could have
    different log levels depending on the sink. For example, we could choose to output
    all logs to a file and keep only the most important warning and error logs in
    the console. Let’s again take our previous example and configure Loguru with this
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_logs_02.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_02.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The `remove` method of `logger` is helpful for removing a previously defined
    sink. When calling it like this with no parameter, all the defined sinks are removed.
    By doing this, we start fresh without the default sink.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we call `add` to define new sinks. The first parameter, like `sys.stdout`
    or `file.log` here, defines how the log calls should be handled. This parameter
    can be many things, such as a callable function, but Loguru allows us, for convenience,
    to directly pass file-like objects, such as `sys.stdout`, or strings, which will
    be interpreted as filenames. Several arguments are accepted to customize all the
    aspects of the sink and, in particular, the level.
  prefs: []
  type: TYPE_NORMAL
- en: As we said, the standard output sink will only log messages with at least a
    `WARNING` level, while the file sink will log all messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice also that we added a `rotation` parameter for the file sink. Since logs
    will continuously be appended to a file, it can quickly grow in size during the
    lifetime of your application. That’s why we have access to a couple of options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“Rotate” the file**: This means that the current file will be renamed, and
    new logs will be added to a new file. This operation can be configured so it happens
    after a certain amount of time (for example, every day, as in our example) or
    when it reaches a certain size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remove older files**: After a certain amount of time, it’s probably not very
    useful to keep older logs that take up unnecessary space on your disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read all the details about these features in the official documentation
    for Loguru: [https://loguru.readthedocs.io/en/stable/api/logger.html#file](https://loguru.readthedocs.io/en/stable/api/logger.html#file).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run this example, we’ll see this in the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DEBUG` logs don’t appear anymore. However, if we read the `file.log` file,
    we’ll have both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! Sinks are really useful for routing our logs to different places
    depending on their nature or importance.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring logs and adding context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In their simplest form, logs consist of free-form text. While convenient, we’ve
    seen that we usually need to log variable values to better understand what’s going
    on. With only strings, this usually ends up in a messy string consisting of multiple
    concatenated values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better approach to handle this is to adopt **structured logging**. The goal
    is to have a clear and proper structure for each log line, so we can embed all
    the information we need without sacrificing readability. Loguru supports this
    approach natively, thanks to contexts. The next example shows you how to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_logs_03.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_03.py)'
  prefs: []
  type: TYPE_NORMAL
- en: We once again took the same example as before. As you can see, we use the `bind`
    method of logger to retain extra information. Here, we set the `n` variable. This
    method returns a new instance of our logger with those attributes attached. Then,
    we can use this instance normally to log things. We don’t need to add `n` in the
    formatted string anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you try this example directly, you won’t see the value of `n` in
    the logs. That’s normal: by default, Loguru doesn’t add context information to
    the formatted log line. We need to customize it! Let’s see how:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_logs_04.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_04.py)'
  prefs: []
  type: TYPE_NORMAL
- en: To format log output, we have to use the `format` parameter when configuring
    a sink. It expects a template string. Here, we copied and pasted the default Loguru
    format and added a part with the `extra` variable. `extra` is a dictionary where
    Loguru stores all the values you added in context. Here, we just output it directly
    so we can see all variables.
  prefs: []
  type: TYPE_NORMAL
- en: Format syntax and available variables
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the available variables you can output in the format string,
    such as `extra` or `level`, in the Loguru documentation: [https://loguru.readthedocs.io/en/stable/api/logger.html#record](https://loguru.readthedocs.io/en/stable/api/logger.html#record).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The format string supports standard formatting directives, which are useful
    for retrieving values, format numbers, pad strings, and so on. You can read more
    about it in the Python documentation: [https://docs.python.org/3/library/string.html#format-string-syntax](https://docs.python.org/3/library/string.html#format-string-syntax).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, Loguru adds special markup so you can color the output. You can read
    more about it here: [https://loguru.readthedocs.io/en/stable/api/logger.html#color](https://loguru.readthedocs.io/en/stable/api/logger.html#color).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, if you run this example, you’ll see the extra context added to the
    log lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach is very convenient and powerful: if you want to keep track of
    a value you care about across logs, you just have to add it once.'
  prefs: []
  type: TYPE_NORMAL
- en: Logs as JSON objects
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach to structured logging is to serialize all the data of a log
    into a JSON object. This can be enabled easily with Loguru by setting `serialize=True`
    when configuring the sink. This approach can be interesting if you plan to use
    a log ingestion service such as Logstash or Datadog: they will be able to parse
    the JSON data and make it available for querying.'
  prefs: []
  type: TYPE_NORMAL
- en: You now have the basics of adding and configuring logs with Loguru. Let’s now
    see how we can leverage them in a FastAPI application.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Loguru as the central logger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding logs to your FastAPI application can be really useful to know what’s
    happening in your different routes and dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example from [*Chapter 5*](B19528_05.xhtml#_idTextAnchor285),
    where we added a global dependency to check for a secret value that should be
    set in the header. In this new version, we’ll add a debug log to trace when the
    `secret_header` dependency is called and a warning log to inform us when this
    secret is missing or invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_logs_05.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_logs_05.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s nothing really surprising if you have followed us so far! Now, let’s
    run this application with Uvicorn and make a request with an invalid header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Our own logs are here, but there is a problem: Uvicorn also adds its own logs,
    but it doesn’t follow our format! Actually, that’s expected: other libraries,
    such as Uvicorn, may have their own logs with their own settings. As such, they
    won’t follow what we defined with Loguru. It’s a bit annoying because if we have
    a complex, well-thought-out setup, we would like every log to follow it. Fortunately,
    there are ways to configure this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we’ll create a module named `logger.py`, where we’ll put all
    our logger configurations. It’s a good practice in your project to have this module
    so your configuration is centralized in one place. The first thing we do in this
    file is to configure Loguru:'
  prefs: []
  type: TYPE_NORMAL
- en: logger.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  prefs: []
  type: TYPE_NORMAL
- en: As we did in the previous section, we removed the default handler and defined
    our own. Notice that we set the level thanks to a constant named `LOG_LEVEL`.
    We hardcoded it here, but a better way would be to take the value from a `Settings`
    object, as we showed in [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694). This
    way, we could directly set the level from environment variables!
  prefs: []
  type: TYPE_NORMAL
- en: After that, we have a quite complex piece of code in the class named `InterceptHandler`.
    It’s a custom handler for the standard logging module that will forward every
    standard log call to Loguru. This code is directly taken from the Loguru documentation.
    We won’t go into much detail about its functioning but just know that it’ll retrieve
    the log level and go through the call stack to retrieve the original caller and
    forward this information to Loguru.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important part, however, is how we use this class. Let’s see this
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: logger.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The trick here is to call the `basicConfig` method from the standard logging
    module to set our custom interception handler. This way, every log call made with
    the root logger, even ones from external libraries, will go through it and be
    handled by Loguru.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, however, this configuration is not sufficient. Some libraries
    define their own loggers with their own handlers, so they won’t use the root configuration.
    That’s the case for Uvicorn, which defines two main loggers: `uvicorn.error` and
    `uvicorn.access`. By retrieving those loggers and changing their handler, we force
    them to go through Loguru as well.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use other libraries that define their own loggers like Uvicorn does,
    you’ll probably need to apply the same technique. All you need to determine is
    the name of their logger, which should be quite easy to find in the library’s
    source code.
  prefs: []
  type: TYPE_NORMAL
- en: It works out of the box with Dramatiq
  prefs: []
  type: TYPE_NORMAL
- en: If you implement a worker with Dramatiq, as we showed in [*Chapter 14*](B19528_14.xhtml#_idTextAnchor1041),
    you’ll see that, if you use the `logger` module, the default logs of Dramatiq
    will be correctly handled by Loguru.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we take care of setting the `__all__` variable at the end of the module:'
  prefs: []
  type: TYPE_NORMAL
- en: logger.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '`__all__` is a special variable telling Python which variables should be made
    publicly available when importing this module. Here, we’ll expose `logger` from
    Loguru, so we can easily import it everywhere we need in our project.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bear in mind that it’s not strictly necessary to use `__all__`: we could very
    well import `logger` without it, but it’s a clean way to hide other things we
    want to keep private, such as `InterceptHandler`, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use it as we saw previously in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: logger.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/logger.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run it with Uvicorn, you’ll now see that all our logs are formatted the
    same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Great! Now, whenever you need to add logs in your app, all you need to do is
    to import `logger` from your `logger` module.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the basics to add logs to your application, with plenty of options
    to fine-tune how and where you output them. Logs are very useful for monitoring
    what your application is doing at a micro-level, operation per operation. Another
    important aspect of monitoring is to have information at a more general level
    in order to have big figures and quickly detect if something goes wrong. That’s
    what we’ll see now with metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Prometheus metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how logs can help us understand what our program
    is doing by finely tracing the operations it does over time. However, most of
    the time, you can’t afford to keep an eye on the logs all day: they are useful
    for understanding and debugging a particular situation but way less useful for
    getting global insights to alert you when something goes wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this, we’ll see in this section how to add **metrics** to our application.
    Their role is to measure things that matter in the execution of our program: the
    number of requests made, the time taken to give a response, the number of pending
    tasks in the worker queue, the accuracy of our ML predictions… Anything that we
    could easily monitor over time – usually, with charts and graphs – so we can easily
    monitor the health of our system. We say that we **instrument** our application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this task, we’ll use two widely used technologies in the industry:
    Prometheus and Grafana.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Prometheus and the different metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prometheus is a technology to help you instrument your application. It consists
    of three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries for a wide range of programming languages, including Python, to add
    metrics to an application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A server to aggregate and store those metrics over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A query language, PromQL, so we can pull data from those metrics into visualization
    tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus has very precise guidelines and conventions about how to define metrics.
    Actually, it defines four different types of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The counter metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The counter metric is a way to measure a *value that goes up over time*. For
    example, this could be the number of requests answered or the number of predictions
    done. This will not be used for values that can go down. For that, there is the
    gauge metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Possible representation of a counter](img/Figure_15.02_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Possible representation of a counter
  prefs: []
  type: TYPE_NORMAL
- en: The gauge metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The gauge metric is a way to measure a *value that can go up or down over time*.
    For example, this could be the current memory usage or the number of pending tasks
    in a worker queue.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – Possible representation of a gauge](img/Figure_15.03_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – Possible representation of a gauge
  prefs: []
  type: TYPE_NORMAL
- en: The histogram metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to counters and gauges, a histogram will *measure values and count
    them in buckets*. Typically, if we want to measure the response time of our API,
    we can count the number of requests that have been processed in less than 10 milliseconds,
    less than 100 milliseconds, and less than 1 second. Doing this is much more insightful
    than getting a simple average or median, for example.
  prefs: []
  type: TYPE_NORMAL
- en: When using a histogram, it’s our responsibility to define the buckets we want
    with their value threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – Possible representation of a histogram](img/Figure_15.04_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – Possible representation of a histogram
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus defines a fourth type of metric, a summary. It’s quite similar to
    the histogram metric, but it works with sliding quantiles instead of defined buckets.
    We won’t go through it since it has quite limited support in Python. Besides,
    we’ll see in the Grafana section of this chapter that we’ll be able to compute
    quantiles with the histogram metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more details about those metrics in the official Prometheus documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring and exposing metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the metrics have been defined, we can start to measure things during the
    lifetime of our program. Similar to what we do with logs, metrics expose methods
    so we can store values during the execution of the application. Prometheus will
    then retain those values in memory to build the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: But then, how can we access those metrics so we can actually analyze and monitor
    them? Quite simply, apps using Prometheus usually expose an HTTP endpoint called
    `/metrics`, which will return the current values of all metrics in a specific
    format. You can see what it looks like in *Figure 15**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5 – Output of a Prometheus metrics endpoint](img/Figure_15.05_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 – Output of a Prometheus metrics endpoint
  prefs: []
  type: TYPE_NORMAL
- en: This endpoint can then be polled at regular intervals by a Prometheus server,
    which will store those metrics over time and make them available through PromQL.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics are reset when your application restarts
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that every time you restart your application, like your FastAPI
    server, metric values are lost, and you start from zero. It may be a bit surprising,
    but it’s key to understand that metric values are only stored in memory in your
    app. The responsibility for properly storing them permanently belongs to the Prometheus
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good idea of how they work, let’s see how to add metrics
    to FastAPI and Dramatiq applications.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Prometheus metrics to FastAPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we said, Prometheus maintains official libraries for various languages, including
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: We could very well use it on its own and manually define various metrics to
    monitor our FastAPI app. We would also need to come up with some logic to hook
    into a FastAPI request handler so we could measure things such as the requests
    count, response time, payload size, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'While definitely doable, we’ll take a shortcut and rely once again on the open
    source community, which proposes a ready-to-use library for integrating Prometheus
    into a FastAPI project: `/``metrics` endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing is, of course, to install it with `pip`. Run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we’ve implemented a very simple FastAPI app and enabled
    the instrumentator:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_metrics_01.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_01.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling the instrumentator consists of three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate the `Instrumentator` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable the default metrics proposed by the library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wire it to our FastAPI `app` and expose the `/``metrics` endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! FastAPI is instrumented with Prometheus!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run this app with Uvicorn and access the `hello` endpoint. Internally,
    Prometheus will measure things about this request. Let’s now access `/metrics`
    to see the result. If you scroll down this big list of metrics, you should come
    across these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This is the metrics counting the number of requests. We see that we have one
    request in total, which corresponds to our call to `hello`. Notice that the instrumentator
    is smart enough to label the metrics by path, method, and even status code. This
    is very convenient, as it’ll enable us to pull interesting figures depending on
    the characteristics of the request.
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The built-in metrics are a good start, but we’ll likely need to come up with
    our own to measure things specific to our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to implement a function that rolls a dice with six faces
    and exposes it via a REST API. We want to define a metric allowing us to count
    the number of times each face has appeared. For this task, a counter is a good
    match. Let’s see how to declare it in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_metrics_02.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
  prefs: []
  type: TYPE_NORMAL
- en: We have to instantiate a `Counter` object. The two first arguments are, respectively,
    the name and description of the metric. The name will be used by Prometheus to
    uniquely identify this metric. Since we want to count the rolls per face, we also
    add a single label named `face`. Every time we count a roll of the dice, we’ll
    have to set this label to the corresponding result face.
  prefs: []
  type: TYPE_NORMAL
- en: Conventions for metric names
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus defines very precise conventions for naming your metrics. In particular,
    it should start with the domain the metrics belong to, such as `http_` or `app_`,
    and should end with the unit, such as `_seconds`, `_bytes`, or `_total` if this
    is just a value count. We strongly recommend you read the Prometheus guidelines:
    [https://prometheus.io/docs/practices/naming/](https://prometheus.io/docs/practices/naming/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use this metric in our code. In the following snippet, you’ll see
    the implementation of the `roll_dice` function:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_metrics_02.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_02.py)'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that we directly use the metrics instance, `DICE_COUNTER`, and first
    call the `labels` method to set the face, and then `inc` to actually increment
    the counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s all we need to do: our metric is automatically registered in the Prometheus
    client and will start to be exposed by the `/metrics` endpoint. In *Figure 15**.6*,
    you can see a possible visualization of this metric in Grafana.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.6 – Representation of the dice roll metric in Grafana](img/Figure_15.06_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.6 – Representation of the dice roll metric in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, declaring and using a new metric is quite straightforward:
    we can just call it directly in the code we want to monitor.'
  prefs: []
  type: TYPE_NORMAL
- en: Handling multiple processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694), we mentioned in the *Adding
    Gunicorn as a server process for deployment* section that, in a production deployment,
    FastAPI apps are usually run with several workers. Basically, it spawns several
    processes of the same application and balances the incoming requests between them.
    This allows us to serve more requests concurrently and avoid blocks if one of
    the operations is blocking the process.
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse Gunicorn workers and Dramatiq workers
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about workers in the context of a Gunicorn deployment for FastAPI,
    we are referring to the fact that we are spawning multiple processes that’ll be
    able to serve our API requests concurrently. We are not talking about workers
    in the context of Dramatiq that are processing tasks in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple processes for the same application is a bit problematic for
    Prometheus metrics. Indeed, as we mentioned before, those metrics are only stored
    in memory and exposed through a `/``metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: If we have several processes answering requests, each one will have its own
    set of metrics values. Then, when the Prometheus server asks for `/metrics`, we’ll
    get the values of the process that answered our request but not the ones of the
    others. And it may change in the next poll! Obviously, this will totally defeat
    our initial goal.
  prefs: []
  type: TYPE_NORMAL
- en: To circumvent this, the Prometheus client has a special multiprocess mode. Basically,
    instead of storing the values in memory, it’ll store them in files in a dedicated
    folder. When calling `/metrics`, it’ll take care of loading all the files and
    reconciling the values of all processes together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling this mode requires us to set the environment variable called `PROMETHEUS_MULTIPROC_DIR`.
    It should point to a valid folder in your filesystem where the metrics files will
    be stored. Here is a command example of how to set this variable and start Gunicorn
    with four workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Of course, in a production deployment, you would set the environment variable
    globally on your platform, as we explained in [*Chapter 10*](B19528_10.xhtml#_idTextAnchor694).
  prefs: []
  type: TYPE_NORMAL
- en: If you try this command, you’ll see that Prometheus will start to store some
    `.db` files inside the folder, each one corresponding to a metric and a process.
    The side effect is that *metrics won’t be cleared when restarting the process*.
    It can lead to unexpected behaviors if you change your metrics definition or if
    you run a completely different application. Make sure to choose a dedicated folder
    for each of your apps and clean it up when you run a new version.
  prefs: []
  type: TYPE_NORMAL
- en: We are now able to precisely instrument a FastAPI app. However, we saw in the
    previous chapter that data science applications can be constituted of a separate
    worker process, where a lot of logic and intelligence is run. Thus, it’s also
    crucial to instrument this part of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Prometheus metrics to Dramatiq
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 14*](B19528_14.xhtml#_idTextAnchor1041), we implemented a complex
    application with a distinct worker process that was in charge of loading and executing
    the Stable Diffusion model to generate images. Hence, this part of the architecture
    is critical and needs to be monitored to be sure everything is going well.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll see how to add Prometheus metrics to a Dramatiq worker.
    The good news is that Dramatiq already comes with built-in metrics and exposes
    the `/metrics` endpoint by default. Really, there is nothing much to do!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a very basic example of a Dramatiq worker with a dummy task:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_metrics_03.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_03.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you probably understand by now, Dramatiq is by nature a multiprocessing
    program: it spawns several workers to handle tasks concurrently. As such, we need
    to make sure Prometheus is in multiprocessing mode, as we mentioned in the *Handling
    multiple processes* section. Thus, we’ll need to set the `PROMETHEUS_MULTIPROC_DIR`
    environment variable, as we explained earlier, but also `dramatiq_prom_db`. Indeed,
    Dramatiq implements its own mechanism to enable Prometheus’s multiprocessing mode,
    which should work out of the box, but it turns out, in our experience, that it’s
    better to be explicit about it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command shows you how to start our worker with `PROMETHEUS_MULTIPROC_DIR`
    and `dramatiq_prom_db` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To allow you to schedule a task easily in this worker, we’ve added a small
    `__name__ == "__main__"` instruction. In another terminal, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It’ll schedule a task in the worker. You’ll probably see it being executed in
    the worker logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, try to open the following URL in your browser: `http://localhost:9191/metrics`.
    You’ll see a result similar to what we show in *Figure 15**.7*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint](img/Figure_15.07_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.7 – Output of a Dramatiq Prometheus metrics endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'We already see several metrics, including a counter for the total number of
    messages processed by Dramatiq, a histogram to measure the execution time of our
    tasks, and a gauge to measure the number of tasks currently in progress. You can
    review the complete list of metrics included by Dramatiq in its official documentation:
    [https://dramatiq.io/advanced.html#prometheus-metrics](https://dramatiq.io/advanced.html#prometheus-metrics).'
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Of course, as for FastAPI, we would probably like to add our own metrics to
    the Dramatiq worker. Actually, this is very similar to what we saw in the previous
    section. Let’s again take the dice roll example:'
  prefs: []
  type: TYPE_NORMAL
- en: chapter15_metrics_04.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/chapter15_metrics_04.py)'
  prefs: []
  type: TYPE_NORMAL
- en: All we needed to do was to create our `Counter` object, as we did before, and
    use it in our task. If you try to run the worker and request the `/metrics` endpoint,
    you’ll see this new metric appear.
  prefs: []
  type: TYPE_NORMAL
- en: We are now able to instrument our FastAPI and Dramatiq apps. As we have already
    mentioned several times, we now need to aggregate those metrics in a Prometheus
    server and visualize them in Grafana. That’s what we’ll look at in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring metrics in Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having metrics is nice, but being able to visualize them is better! In this
    section, we’ll see how we can collect Prometheus metrics, send them to Grafana,
    and create dashboards to monitor them.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana is an open source web application for data visualization and analytics.
    It’s able to connect to various data sources, such as timeseries databases and,
    of course, Prometheus. Its powerful query and graph builder allows us to create
    detailed dashboards where we can monitor our data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Grafana to collect metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since it’s open source, you can run it from your own machine or server. Detailed
    instructions are available in the official documentation: [https://grafana.com/docs/grafana/latest/setup-grafana/installation/](https://grafana.com/docs/grafana/latest/setup-grafana/installation/).
    However, to speed things up and get you started quickly, we’ll rely here on Grafana
    Cloud, an official hosting platform. It offers a free plan, which should be enough
    for you to get started. You can create your account here: [https://grafana.com/auth/sign-up/create-user](https://grafana.com/auth/sign-up/create-user).
    Once done, you’ll be asked to create your own instance, a “Grafana Stack,” by
    choosing a subdomain and a data center region, as you can see in *Figure 15**.8*.
    Choose a region close to your geographic location.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.8 – Instance creation on Grafana Cloud](img/Figure_15.08_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.8 – Instance creation on Grafana Cloud
  prefs: []
  type: TYPE_NORMAL
- en: You’ll then be presented with a set of common actions to get started with Grafana.
    The first thing we’ll do is add Prometheus metrics. Click on **Scale and centralize
    existing data**, then **Hosted Prometheus metrics**. You’ll be taken to a page
    to configure a Prometheus metrics collection. Click on the tab named **Configuration
    Details** at the top. The page will look like the one shown in *Figure 15**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.9 – Hosted Prometheus metrics configuration on Grafana](img/Figure_15.09_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.9 – Hosted Prometheus metrics configuration on Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'You see that we have two ways to forward metrics: via Grafana Agent or via
    a Prometheus server.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, a Prometheus server is responsible for collecting
    metrics for all our apps and storing the data in a database. It’s the standard
    way to do it. You can find instructions on how to install it in the official documentation:
    [https://prometheus.io/docs/prometheus/latest/installation/](https://prometheus.io/docs/prometheus/latest/installation/).
    Bear in mind, though, that it’s a dedicated application server that’ll need proper
    backups, as it’ll store all your metrics data.'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way is to use Grafana Agent. It consists of a small
    command-line program with a single configuration file. When it runs, it’ll poll
    the metrics of each of your apps and send the data to Grafana Cloud. All the data
    is stored on Grafana Cloud, so nothing is lost, even if you stop or delete the
    agent. This is what we’ll use here.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana shows you commands on the page to download, unzip, and execute the Grafana
    Agent program. Execute those commands so you have it at the root of your project.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the last step, you’ll have to create an API token so Grafana Agent
    can send data to your instance. Give it a name and click on **Create API Token**.
    A new text area will appear with a new command to create the agent’s configuration
    file, as you can see in *Figure 15**.10*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.10 – Command to create Grafana Agent configuration](img/Figure_15.10_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.10 – Command to create Grafana Agent configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the `./grafana-agent-linux-amd64 –config.file=agent-config.yaml` command.
    A file named `agent-config.yaml` will be created in your project. We now have
    to edit it so we can configure our actual FastAPI and Dramatiq applications. You
    can see the result in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: agent-config.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml](https://github.com/PacktPublishing/Building-Data-Science-Applications-with-FastAPI-Second-Edition/tree/main/chapter15/agent-config.yaml)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s a YAML configuration file where we can set the various options for Grafana
    Agent. The most important part is the `scrape_configs` key. As you can see, we
    can define the list of all the apps we want to gather the metrics for and specify
    their hostname, the “target”: `localhost:8000` for the FastAPI app and `localhost:9191`
    for the Dramatiq worker. Of course, this configuration is valid for local development,
    but you’ll have to adapt it with the proper hostnames of your apps in a production
    deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to start Grafana Agent and collect the metrics! Make sure
    your FastAPI and Dramatiq apps are running, and then run Grafana Agent. Depending
    on your system, the name of the executable will vary, but it’ll look similar to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Grafana Agent will start and will collect the metrics at regular intervals before
    sending them to Grafana. We’re now ready to plot some data!
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing metrics in Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our metrics data is now sent to Grafana. We’re ready to query it and build some
    graphs. The first step is to create a new **dashboard**, a place where you’ll
    be able to create and organize multiple graphs. Click on the plus button at the
    top right and then **New dashboard**.
  prefs: []
  type: TYPE_NORMAL
- en: A new blank dashboard will appear, as you can see in *Figure 15**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.11 – Create a new dashboard in Grafana](img/Figure_15.11_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.11 – Create a new dashboard in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Add a new panel**. The interface to build a new graph will appear.
    There are three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The graph preview at the top left. When starting, it’s empty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The query builder at the bottom left. This is where we’ll query the metrics
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The graph settings on the right. This is where we’ll choose the type of graph
    and finely configure its look and feel, similar to what we have in spreadsheet
    software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s try to create a graph for the duration of HTTP requests in our FastAPI
    app. In the select menu called **Metric**, you’ll have access to all the Prometheus
    metrics that have been reported by our apps. Select **http_request_duration_seconds_bucket**.
    This is the histogram metric defined by default by Prometheus FastAPI Instrumentator
    to measure the response time of our endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Then, click on **Run queries**. Under the hood, Grafana will build and execute
    PromQL queries to retrieve the data.
  prefs: []
  type: TYPE_NORMAL
- en: At the top right of the graph, let’s select a shorter time span, such as **Last
    15 minutes**. Since we do not have much data yet, we’ll have a clearer view if
    we look at only a few minutes of data instead of hours. You should see a graph
    similar to the one in *Figure 15**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.12 – Basic plot of a histogram metric in Grafana](img/Figure_15.12_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.12 – Basic plot of a histogram metric in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'Grafana has plotted several series: for each `handler` (which corresponds to
    the endpoint pattern), we have several buckets, `le`. Each line roughly represents
    *the number of times we answered “handler” in less than “**le” seconds*.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the raw representation of the metric. However, you probably see that
    it’s not very convenient to read and analyze. It would be better if we could look
    at this data another way, in terms of response time, arranged by quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, PromQL includes some math operations so we can arrange the raw
    data. The part below the **Metric** menu allows us to add those operations. We
    can even see that Grafana suggests we use **add histogram_quantile**. If you click
    on this blue button, Grafana will automatically add three operations: a *Rate*,
    a *Sum by le*, and finally, a *Histogram quantile*, set by default to *0.95*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing this, we’ll now have a view of the evolution of our response time:
    95% of the time, we answer in less than *x* seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: The default *y* axis unit is not very convenient. Since we know we work with
    seconds, let’s select this unit in the graph options. On the right, look for the
    **Standard options** part and, in the **Unit** menu, look for **seconds (s)**
    under the **Time** group. Your graph will now look like *Figure 15**.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.13 – Quantile representation of a histogram metric in Grafana](img/Figure_15.13_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.13 – Quantile representation of a histogram metric in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it’s much more insightful: we can see that we answer nearly all our requests
    (95%) in under 100 milliseconds. If our server starts to slow down, we’ll immediately
    see an increase in our graph, which could alert us that something has gone wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to have other quantiles on the same graph, we can duplicate this
    query by clicking on the **Duplicate** button right above **Run queries**. Then,
    all we have to do is to select another quantile. We show the result with quantiles
    *0.95*, *0.90*, and *0.50* in *Figure 15**.14*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.14 – Several quantiles on the same graph in Grafana](img/Figure_15.14_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.14 – Several quantiles on the same graph in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: The legend can be customized
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the name of the series in the legend can be customized. Under the
    **Options** part of each query, you can customize it at will. You can even include
    dynamic values coming from the query, such as metrics labels.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can give a name to our graph by setting **Panel title**, in the
    right column. Now that we’re happy with our graph, we can click on **Apply** at
    the top right to add it to our dashboard, as we see in *Figure 15**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.15 – Grafana dashboard](img/Figure_15.15_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.15 – Grafana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! We can start to monitor our application. You can resize and position
    each panel at will. You can set the query time span you want to look at and even
    enable auto-refresh so the data gets updated in real time! Don’t forget to click
    on the **Save** button to save your dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: We can build a similar graph with the exact same configuration to monitor the
    time needed to execute tasks in Dramatiq, thanks to the metric named `dramatiq_message_duration_milliseconds_bucket`.
    Notice that this one is expressed in milliseconds instead of seconds, so you should
    be careful when selecting the unit of your graph. We see here one of the benefits
    of the Prometheus naming convention for metrics!
  prefs: []
  type: TYPE_NORMAL
- en: Adding a bar chart graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a lot of different types of graphs available in Grafana. For example,
    we could plot our dice roll metric in the form of a bar chart, where each bar
    represents the number of times a face has been seen. Let’s try it: add a new panel
    and select the `app_dice_rolls_total` metric. You’ll see something similar to
    what is shown in *Figure 15**.6*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.16 – Default representation of a counter metric with a bar chart
    in Grafana](img/Figure_15.16_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.16 – Default representation of a counter metric with a bar chart in
    Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'We do have a bar for each face, but there is something strange: there are bars
    for each point in time. That’s a key thing to understand with Prometheus metrics
    and PromQL: all metrics are stored as *time series*. This allows us to go back
    in time and see the evolution of the metrics over time.'
  prefs: []
  type: TYPE_NORMAL
- en: However, for some representations, like the one shown here, it’s not really
    insightful. For this case, it would be better to show us the latest values for
    the time span we selected. We can do this by setting **Type** to **Instant** under
    the **Options** part of the metric panel. We’ll see that we now have a single
    graph with a single point in time, as you can see in *Figure 15**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.17 – Counter metric configured as Instant in Grafana](img/Figure_15.17_B19528.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.17 – Counter metric configured as Instant in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: It’s better, but we can go further. Typically, we would like the *x* axis to
    show the face labels instead of the point in time. First, let’s customize the
    legend with a `{{face}}`. The legend will now only show the `face` label.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll transform the data so the *x* axis is the `face` label. Click on
    the **Transform** tab. You’ll see a list of functions that can be applied by Grafana
    to your data before visualizing it. For our case here, we’ll choose **Reduce**.
    The effect of this function is to take each series, take a specific value from
    it, and plot it on the *x* axis. By default, Grafana will take the maximum value,
    **Max**, but there are other choices, such as **Last**, **Mean**, or **StdDev**.
    In this context, they won’t make a difference since we already queried the instant
    value.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! Our graph now shows the number of times we’ve seen a face. This is
    the one we showed in *Figure 15**.6* earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Congratulations! You are now able to report metrics and build your own dashboards
    in Grafana to monitor your data science applications. Over time, don’t hesitate
    to add new metrics or complete your dashboards if you notice some blind spots:
    the goal is to be able to watch over every important part at a glance so you can
    quickly take corrective actions. Those metrics can also be used to drive the evolution
    of your work: by monitoring the performance and accuracy of your ML models, you
    can track the effects of your changes and see whether you are going in the right
    direction.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of this book and our FastAPI journey. We sincerely hope that
    you liked it and that you learned a lot along the way. We’ve covered many subjects,
    sometimes just by scratching the surface, but you should now be ready to build
    your own projects with FastAPI and serve smart data science algorithms. Be sure
    to check all the external resources we proposed along the way, as they will give
    you all the insights you need to master them.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, Python has gained a lot of popularity, especially in data science
    communities, and the FastAPI framework, even though still very young, is already
    a game-changer and has seen an unprecedented adoption rate. It’ll likely be at
    the heart of many data science systems in the coming years... And as you read
    this book, you’ll probably be one of the developers behind them. Cheers!
  prefs: []
  type: TYPE_NORMAL
