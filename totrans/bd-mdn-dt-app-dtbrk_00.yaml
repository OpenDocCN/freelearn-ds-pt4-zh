- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As datasets have exploded in size with the introduction of cheap cloud storage
    and processing data in near real time has become an industry standard, many organizations
    have turned to the lakehouse architecture, which combines the fast BI speeds of
    a traditional data warehouse with the scalable ETL processing of big data in the
    cloud. The Databricks Data Intelligence Platform – built upon several open source
    technologies, including Apache Spark, Delta Lake, MLflow, and Unity Catalog –
    eliminates friction points and accelerates the design and deployment of modern
    data applications built for the l akehouse.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, you’ll start with an overview of the Delta Lake format, cover
    core concepts of the Databricks Data Intelligence Platform, and master building
    data pipelines using the Delta Live Tables framework. We’ll dive into applying
    data transformations, how to implement the Databricks medallion architecture,
    and how to continuously monitor the quality of data landing in your lakehouse.
    You’ll learn how to react to incoming data using the Databricks Auto Loader feature
    and automate real-time data processing using Databricks workflows. You’ll learn
    how to use CI/CD tools such as **Terraform** and **Databricks Asset Bundles**
    ( **DABs** ) to deploy data pipeline changes automatically across deployment environments,
    as well as monitor, control, and optimize cloud costs along the way. By the end
    of this book, you will have mastered building a production-ready, modern data
    application using the Databricks Data Intelligence Platform.
  prefs: []
  type: TYPE_NORMAL
- en: With Databricks recently named a Leader in the 2024 Gartner Magic Quadrant for
    Data Science and Machine Learning Platforms, the demand for mastering a skillset
    in the Databricks Data Intelligence Platform is only expected to grow in the coming
    years.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is for data engineers, data scientists, and data stewards tasked with
    enterprise data processing for their organizations. This book will simplify learning
    advanced data engineering techniques on Databricks, making implementing a cutting-edge
    lakehouse accessible to individuals with varying technical expertise. However,
    beginner-level knowledge of Apache Spark and Python is needed to make the most
    out of the code examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B22011_01.xhtml#_idTextAnchor014) , *An Introduction to Delta
    Live Tables* , discusses building near-real-time data pipelines using the Delta
    Live Tables framework. It covers the fundamentals of pipeline design as well as
    the core concepts of the Delta Lake format. The chapter concludes with a simple
    example of building a Delta Live Table pipeline from start to finish.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B22011_02.xhtml#_idTextAnchor052) , *Applying Data Transformations
    Using Delta Live Tables* , explores data transformations using Delta Live Tables,
    guiding you through the process of cleaning, refining, and enriching data to meet
    specific business requirements. You will learn how to use Delta Live Tables to
    ingest data from a variety of input sources, register datasets in Unity Catalog,
    and effectively apply changes to downstream tables.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B22011_03.xhtml#_idTextAnchor079) , *Managing Data Quality Using
    Delta Live Tables* , introduces several techniques for enforcing data quality
    requirements on newly arriving data. You will learn how to define data quality
    constraints using Expectations in the Delta Live Tables framework, as well as
    monitor the data quality of a pipeline in near real time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B22011_04.xhtml#_idTextAnchor103) , *Scaling DLT Pipelines* ,
    explains how to scale a **Delta Live Tables** ( **DLT** ) pipeline to handle the
    unpredictable demands of a typical production environment. You will take a deep
    dive into configuring pipeline settings using the DLT UI and Databricks Pipeline
    REST API. You will also gain a better understanding of the daily DLT maintenance
    tasks that are run in the background and how to optimize table layouts to improve
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B22011_05.xhtml#_idTextAnchor126) , *Mastering Data Governance
    in the Lakehouse* *with* *Unity Catalog* , provides a comprehensive guide to enhancing
    data governance and compliance of your lakehouse using Unity Catalog. You will
    learn how to enable Unity Catalog on a Databricks workspace, enable data discovery
    using metadata tags, and implement fine-grained row and column-level access control
    of datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22011_06.xhtml#_idTextAnchor148) , *Managing Data Locations
    in Unity Catalog* , explores how to effectively manage storage locations using
    Unity Catalog. You will learn how to govern data access across various roles and
    departments within an organization while ensuring security and auditability with
    the Databricks Data Intelligence Platform.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22011_07.xhtml#_idTextAnchor165) , *Viewing Data Lineage using
    Unity Catalog* , discusses tracing data origins, visualizing data transformations,
    and identifying upstream and downstream dependencies by tracing data lineage in
    Unity Catalog. By the end of the chapter, You will be equipped with the skills
    needed to validate that data is coming from trusted sources.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22011_08.xhtml#_idTextAnchor185) , *Deploy* *ing* *, Maintain*
    *ing* *, and Administrat* *ing* *DLT* *Pipelines Using Terraform* , covers deploying
    DLT pipelines using the Databricks Terraform provider. You will learn how to set
    up a local development environment and automate a continuous build and deployment
    pipeline, along with best practices and future considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) , *Leveraging Databricks Asset
    Bundles to Streamline Data Pipeline Deployment* , explores how DABs can be used
    to streamline the deployment of data analytics projects and improve cross-team
    collaboration. You will gain an understanding of the practical use of DABs through
    several hands-on examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B22011_10.xhtml#_idTextAnchor249) *,* *Monitoring Data Pipelines
    in Production* , delves into the crucial task of monitoring data pipelines in
    Databricks. You will learn various mechanisms for tracking pipeline health, performance,
    and data quality within the Databricks Data Intelligence Platform.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While not a mandatory requirement, to get the most out of this book, it’s recommended
    that you have beginner-level knowledge of Python and Apache Spark, and at least
    some knowledge of navigating around the Databricks Data Intelligence Platform.
    It’s also recommended to have the following dependencies installed locally in
    order to follow along with the hands-on exercises and code examples throughout
    the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Software/hardware covered in** **the book** | **Operating** **system requirements**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Python 3.6+ | Windows, macOS, or Linux |'
  prefs: []
  type: TYPE_TB
- en: '| Databricks CLI 0.205+ |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, it’s recommended that you have a Databricks account and workspace
    to log in, import notebooks, create clusters, and create new data pipelines. If
    you do not have a Databricks account, you can sign up for a free trial on the
    Databricks website [https://www.databricks.com/try-databricks](https://www.databricks.com/try-databricks)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book’s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting** **of code.**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse)
    . If there’s an update to the code, it will be updated in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)
    . Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Code in text** : Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “The result of the data generator notebook should
    be three tables in total: **youtube_channels** , **youtube_channel_artists** ,
    and **combined_table** .”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold** : Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold** . Here is an example:
    “Click the **Run all** button at the top right of the Databricks workspace to
    execute all the notebook cells, verifying that all cells execute successfully.”'
  prefs: []
  type: TYPE_NORMAL
- en: Tips or important notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback** : If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](mailto:customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata** : Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy** : If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packtpub.com](mailto:copyright@packtpub.com)
    with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author** : If there is a topic that
    you have expertise in and you are interested in either writing or contributing
    to a book, please visit [authors.packtpub.com](http://authors.packtpub.com) .'
  prefs: []
  type: TYPE_NORMAL
- en: Share Your Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Building Modern Data Applications Using Databricks Lakehouse*
    , we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1-801-07323-6) for this book and share your
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
- en: Download a free PDF copy of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks for purchasing this book!
  prefs: []
  type: TYPE_NORMAL
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  prefs: []
  type: TYPE_NORMAL
- en: Is your eBook purchase not compatible with the device of your choice?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  prefs: []
  type: TYPE_NORMAL
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these simple steps to get the benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link below
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![img](img/B22011_QR_Free_PDF.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/free-ebook/978-1-80107-323-3](https://packt.link/free-ebook/978-1-80107-323-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Submit your proof of purchase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 1:Near-Real-Time Data Pipelines for the Lakehouse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first part of the book, we’ll introduce the core concepts of the **Delta
    Live Tables** ( **DLT** ) framework. We’ll cover how to ingest data from a variety
    of input sources and apply the latest changes to downstream tables. We’ll also
    explore how to enforce requirements on incoming data so that your data teams can
    be alerted of potential data quality issues that might contaminate your l akehouse.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B22011_01.xhtml#_idTextAnchor014) , *An Introduction to Delta
    Live Tables*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B22011_02.xhtml#_idTextAnchor052) , *Applying Data Transformations
    Using Delta Live Tables*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B22011_03.xhtml#_idTextAnchor079) , *Managing Data Quality Using
    Delta Live Tables*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B22011_04.xhtml#_idTextAnchor103) , *Scaling DLT Pipelines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
