<html><head></head><body>
  <div id="_idContainer018">
   <h1 class="chapter-number" id="_idParaDest-15">
    <a id="_idTextAnchor014">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     1
    </span>
   </h1>
   <h1 id="_idParaDest-16">
    <a id="_idTextAnchor015">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     An Introduction to Delta Live Tables
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In this chapter, we will examine how the data industry has evolved over the last several decades.
    </span>
    <span class="koboSpan" id="kobo.3.2">
     We’ll also look at why real-time data processing has significant ties to how a business can react to the latest signals in data.
    </span>
    <span class="koboSpan" id="kobo.3.3">
     We’ll address why trying to build your own streaming solution from scratch may not be sustainable, and why the maintenance does not easily scale over time.
    </span>
    <span class="koboSpan" id="kobo.3.4">
     By the end of the chapter, you should completely understand the types of problems
    </span>
    <a id="_idIndexMarker000">
    </a>
    <span class="koboSpan" id="kobo.4.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.5.1">
      Delta Live Tables
     </span>
    </strong>
    <span class="koboSpan" id="kobo.6.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.7.1">
      DLT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.8.1">
     ) framework solves and the value the framework brings to data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.9.1">
      engineering teams.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.10.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.11.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.12.1">
      The emergence of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.13.1">
       the
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.14.1">
       l
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.15.1">
       akehouse
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.16.1">
      The importance of real-time data in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.17.1">
       the
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.18.1">
       l
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.19.1">
       akehouse
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.20.1">
      The maintenance predicament of a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.21.1">
       streaming application
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.22.1">
      What is the Delta Live
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.23.1">
       Tables framework?
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.24.1">
      How are Delta Live Tables related to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.25.1">
       Delta Lake?
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.26.1">
      An introduction to Delta Live
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.27.1">
       Tables concepts
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.28.1">
      A quick Delta
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.29.1">
       Lake primer
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.30.1">
      A hands-on example – creating your first Delta Live
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.31.1">
       Tables pipeline
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-17">
    <a id="_idTextAnchor016">
    </a>
    <span class="koboSpan" id="kobo.32.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.33.1">
     It’s recommended to have access to
    </span>
    <a id="_idTextAnchor017">
    </a>
    <span class="koboSpan" id="kobo.34.1">
     a Databricks premium workspace to follow along with the code examples at the end of the chapter.
    </span>
    <span class="koboSpan" id="kobo.34.2">
     It’s also recommended to have Databricks workspace permissions to create an all-purpose cluster and a DLT pipeline using a cluster policy.
    </span>
    <span class="koboSpan" id="kobo.34.3">
     Users will create and attach a notebook to a cluster and execute the notebook cells.
    </span>
    <span class="koboSpan" id="kobo.34.4">
     All code samples can be downloaded from this chapter’s GitHub repository, located at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01">
     <span class="koboSpan" id="kobo.35.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter01
     </span>
    </a>
    <span class="koboSpan" id="kobo.36.1">
     .
    </span>
    <span class="koboSpan" id="kobo.36.2">
     This chapter will create and run a new DLT pipeline using the Core product edition.
    </span>
    <span class="koboSpan" id="kobo.36.3">
     As a result, the pipeline is estimated to consume
    </span>
    <a id="_idIndexMarker001">
    </a>
    <span class="koboSpan" id="kobo.37.1">
     around 5–10
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.38.1">
      Databricks
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.39.1">
       Units
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.40.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.41.1">
       DBUs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.42.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-18">
    <a id="_idTextAnchor018">
    </a>
    <span class="koboSpan" id="kobo.43.1">
     The emergence of the lakehouse
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.44.1">
     During the early 1980s, the data
    </span>
    <a id="_idIndexMarker002">
    </a>
    <span class="koboSpan" id="kobo.45.1">
     warehouse was a great tool for processing structured data.
    </span>
    <span class="koboSpan" id="kobo.45.2">
     Combined with the right indexing methods, data warehouses allowed us to serve
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.46.1">
      business intelligence
     </span>
    </strong>
    <span class="koboSpan" id="kobo.47.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.48.1">
      BI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.49.1">
     ) reports
    </span>
    <a id="_idIndexMarker003">
    </a>
    <span class="koboSpan" id="kobo.50.1">
     at blazing speeds.
    </span>
    <span class="koboSpan" id="kobo.50.2">
     However, after the turn of the century, data warehouses could not keep up with newer data formats such as JSON, as well as new data modalities such as audio and video.
    </span>
    <span class="koboSpan" id="kobo.50.3">
     Simply put, data warehouses struggled to process semi-structured and unstructured data that most businesses used.
    </span>
    <span class="koboSpan" id="kobo.50.4">
     Additionally, data warehouses struggled to scale to millions or billions of rows, common in the new information era of the early 2000s.
    </span>
    <span class="koboSpan" id="kobo.50.5">
     Overnight, batch data processing jobs soon ran into BI reports scheduled to refresh during the early morning
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.51.1">
      business hours.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.52.1">
     At the same time, cloud computing became a popular choice among organizations because it provided enterprises with an elastic computing capacity that could quickly grow or shrink, based on the current computing demand, without having to deal with the upfront costs of provisioning and installing additional
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.53.1">
      hardware on-premises.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.54.1">
     Modern
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.55.1">
      extract, transform, and load
     </span>
    </strong>
    <span class="koboSpan" id="kobo.56.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.57.1">
      ETL
     </span>
    </strong>
    <span class="koboSpan" id="kobo.58.1">
     ) processing
    </span>
    <a id="_idIndexMarker004">
    </a>
    <span class="koboSpan" id="kobo.59.1">
     engines such as Apache Hadoop and Apache Spark™ addressed the performance problem of processing big data ETL pipelines, ushering in a new concept, a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.60.1">
      data lake
     </span>
    </strong>
    <span class="koboSpan" id="kobo.61.1">
     .
    </span>
    <span class="koboSpan" id="kobo.61.2">
     Conversely, data
    </span>
    <a id="_idIndexMarker005">
    </a>
    <span class="koboSpan" id="kobo.62.1">
     lakes were terrible for serving BI reports and oftentimes offered degrading performance experiences for many concurrent user sessions.
    </span>
    <span class="koboSpan" id="kobo.62.2">
     Furthermore, data lakes had poor data governance.
    </span>
    <span class="koboSpan" id="kobo.62.3">
     They were prone to sloppy data wrangling patterns, leading to many expensive copies of the same datasets that frequently diverged from the source of truth.
    </span>
    <span class="koboSpan" id="kobo.62.4">
     As a result, these data lakes quickly earned the nickname of
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.63.1">
      data swamps
     </span>
    </em>
    <span class="koboSpan" id="kobo.64.1">
     .
    </span>
    <span class="koboSpan" id="kobo.64.2">
     The big data industry needed a change.
    </span>
    <span class="koboSpan" id="kobo.64.3">
     The lakehouse pattern was this change and aimed to combine the best of both worlds – fast BI reports and fast ETL processing of structured, semi-structured, and unstructured data in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.65.1">
      the cloud.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-19">
    <a id="_idTextAnchor019">
    </a>
    <span class="koboSpan" id="kobo.66.1">
     The Lambda architectural pattern
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.67.1">
     In the early 2010s, data
    </span>
    <a id="_idIndexMarker006">
    </a>
    <span class="koboSpan" id="kobo.68.1">
     streaming took a foothold in the data
    </span>
    <a id="_idIndexMarker007">
    </a>
    <span class="koboSpan" id="kobo.69.1">
     industry, and many enterprises needed a way to support both batch ETL processing and append-only streams of data.
    </span>
    <span class="koboSpan" id="kobo.69.2">
     Furthermore, data architectures with many concurrent ETL processes needed to simultaneously read and change the underlying data.
    </span>
    <span class="koboSpan" id="kobo.69.3">
     It was not uncommon for organizations to experience frequent conflicting write failures that led to data corruption and even data loss.
    </span>
    <span class="koboSpan" id="kobo.69.4">
     As a result, in many early data architectures, a two-pronged Lambda architecture was built to provide a layer of isolation between
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.70.1">
      these processes.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer008">
     <span class="koboSpan" id="kobo.71.1">
      <img alt="Figure 1.1 – A Lambda architecture was oftentimes created to support both real-time streaming workloads and batch processes such as BI reports" src="image/B22011_01_001.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.72.1">
     Figure 1.1 – A Lambda architecture was oftentimes created to support both real-time streaming workloads and batch processes such as BI reports
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.73.1">
     Using the Lambda architecture, downstream processes such as BI reports or
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.74.1">
      Machine Learning
     </span>
    </strong>
    <span class="koboSpan" id="kobo.75.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.76.1">
      ML
     </span>
    </strong>
    <span class="koboSpan" id="kobo.77.1">
     ) model
    </span>
    <a id="_idIndexMarker008">
    </a>
    <span class="koboSpan" id="kobo.78.1">
     training could execute calculations on a snapshot of data, while streaming processes could apply near real-time data changes in isolation.
    </span>
    <span class="koboSpan" id="kobo.78.2">
     However, these Lambda architectures duplicated data to support concurrent batch and streaming workloads, leading to inconsistent data changes that needed to be reconciled at the end of each
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.79.1">
      business day.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-20">
    <a id="_idTextAnchor020">
    </a>
    <span class="koboSpan" id="kobo.80.1">
     Introducing the medallion architecture
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.81.1">
     In an effort to
    </span>
    <a id="_idIndexMarker009">
    </a>
    <span class="koboSpan" id="kobo.82.1">
     clean up data lakes and prevent bad data practices, data
    </span>
    <a id="_idIndexMarker010">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     lake architects needed a data processing pattern that would meet the high demands of modern-day ETL processing.
    </span>
    <span class="koboSpan" id="kobo.83.2">
     In addition, organizations needed a simplified architecture for batch and streaming workloads, easy data rollbacks, good data auditing, and strong data isolation, while scaling to process terabytes or even petabytes of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.84.1">
      data daily.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.85.1">
     As a result, a design pattern within the lakehouse emerged, commonly referred to as the medallion architecture.
    </span>
    <span class="koboSpan" id="kobo.85.2">
     This data processing pattern physically isolates data processing and improves data quality by applying business-level transformations in successive data hops, also
    </span>
    <a id="_idIndexMarker011">
    </a>
    <span class="koboSpan" id="kobo.86.1">
     called
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.87.1">
       data layers
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.88.1">
      .
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer009">
     <span class="koboSpan" id="kobo.89.1">
      <img alt="Figure 1.2 – The lakehouse medallion architecture" src="image/B22011_01_002.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.90.1">
     Figure 1.2 – The lakehouse medallion architecture
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.91.1">
     A typical design pattern for organizing data within a lakehouse (as shown in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.92.1">
       Figure 1
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.93.1">
      .2
     </span>
    </em>
    <span class="koboSpan" id="kobo.94.1">
     ) includes three distinct data layers – a bronze layer, a silver layer, and finally, a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.95.1">
      gold layer:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.96.1">
      The bronze layer serves as a landing zone for raw,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.97.1">
       unprocessed data
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.98.1">
       .
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.99.1">
      Filtered, cleaned, and augmented data with a defined structure and enforced schema will be stored in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.100.1">
       silver layer
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.101.1">
       .
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.102.1">
      Lastly, a refined, or gold layer, will deliver pristine, business-level aggregations ready to be consumed by downstream BI and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.103.1">
       ML systems
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.104.1">
       .
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.105.1">
     Moreover, this simplified data architecture unifies batch and streaming workloads, by storing datasets in a big data format that supports concurrent batch and streaming
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.106.1">
      data operations.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-21">
    <a id="_idTextAnchor021">
    </a>
    <span class="koboSpan" id="kobo.107.1">
     The Databricks lakehouse
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.108.1">
     The Databricks
    </span>
    <a id="_idIndexMarker012">
    </a>
    <span class="koboSpan" id="kobo.109.1">
     lakehouse combines the processing
    </span>
    <a id="_idIndexMarker013">
    </a>
    <span class="koboSpan" id="kobo.110.1">
     power of a new high-performance processing engine, called the Photon Engine, with the augmentation of Apache Spark.
    </span>
    <span class="koboSpan" id="kobo.110.2">
     Combined with open data formats for data storage, and support for a wide range of data types, including structured, semi-structured, and unstructured data, the Photon engine can process a wide variety of workloads using a single, consistent snapshot of the data in cheap and resilient cloud storage.
    </span>
    <span class="koboSpan" id="kobo.110.3">
     I
    </span>
    <a id="_idTextAnchor022">
    </a>
    <span class="koboSpan" id="kobo.111.1">
     n addition, the Databricks lakehouse simplifies data architecture by unifying batch and streaming processing with a single API – the Spark DataFrame API.
    </span>
    <span class="koboSpan" id="kobo.111.2">
     Lastly, the Databricks lakehouse was built with data governance and data security in mind, allowing organizations to centrally define data access patterns and consistently apply them across
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.112.1">
      their businesses.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.113.1">
     In this book, we’ll cover
    </span>
    <a id="_idIndexMarker014">
    </a>
    <span class="koboSpan" id="kobo.114.1">
     three major features that the Databricks lakehouse is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.115.1">
      anchored in:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.116.1">
      The Delta
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.117.1">
       Lake format
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.118.1">
      The
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.119.1">
       Photon Engine
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.120.1">
       Unity Catalog
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.121.1">
     While Delta Lake can be used to process both batch and streaming workloads concurrently, most data teams choose to implement their ETL pipelines using a batch execution model, mainly for simplicity’s sake.
    </span>
    <span class="koboSpan" id="kobo.121.2">
     Let’s look at why that might be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.122.1">
      the case.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-22">
    <a id="_idTextAnchor023">
    </a>
    <span class="koboSpan" id="kobo.123.1">
     The maintenance predicament of a streaming application
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.124.1">
     Spark Structured
    </span>
    <a id="_idIndexMarker015">
    </a>
    <span class="koboSpan" id="kobo.125.1">
     Streaming provides near-real-time stream processing with fault tolerance, and exactly-once processing guarantees through the use of a DataFrame API that is near-identical to batch processing in Spark.
    </span>
    <span class="koboSpan" id="kobo.125.2">
     As a result of a common DataFrame API, data engineering teams can convert existing batch Spark workloads to streaming with minimal effort.
    </span>
    <span class="koboSpan" id="kobo.125.3">
     However, as the volume of data increases and the number of ingestion sources and data pipelines naturally grows over time, data engineering teams face the burden of augmenting existing data pipelines to keep up with new data transformations or changing business logic.
    </span>
    <span class="koboSpan" id="kobo.125.4">
     In addition, Spark Streaming comes with additional configuration maintenance such as updating checkpoint locations, managing watermarks and triggers, and even backfilling tables when a significant data change or data correction occurs.
    </span>
    <span class="koboSpan" id="kobo.125.5">
     Advanced data engineering teams may even be expected to build data validation and system monitoring capabilities, adding even more custom pipeline features to maintain.
    </span>
    <span class="koboSpan" id="kobo.125.6">
     Over time, data pipeline complexity will grow, and data engineering teams will spend most of their time maintaining the operation of data pipelines in production and less time gleaning insights from their enterprise data.
    </span>
    <span class="koboSpan" id="kobo.125.7">
     It’s evident that a framework is needed that allows data engineers to quickly declare data transformations, manage data quality, and rapidly deploy changes
    </span>
    <a id="_idIndexMarker016">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     to production where they can monitor pipeline operations from a UI or other
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.127.1">
      notification systems.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-23">
    <a id="_idTextAnchor024">
    </a>
    <span class="koboSpan" id="kobo.128.1">
     What is the DLT framework?
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.129.1">
     DLT
    </span>
    <a id="_idIndexMarker017">
    </a>
    <span class="koboSpan" id="kobo.130.1">
     is a declarative framework that aims to simplify the development and maintenance operations of a data pipeline by abstracting away a lot of the boilerplate complexities.
    </span>
    <span class="koboSpan" id="kobo.130.2">
     For example, rather than declaring how to transform, enrich, and validate data, data engineers can declare what transformations to apply to newly arriving data.
    </span>
    <span class="koboSpan" id="kobo.130.3">
     Furthermore, DLT provides support to enforce data quality, preventing a data lake from becoming a data swamp.
    </span>
    <span class="koboSpan" id="kobo.130.4">
     DLT gives data teams the ability to choose how to handle poor-quality data, whether that means printing a warning message to the system logs, dropping invalid data, or failing a data pipeline run altogether.
    </span>
    <span class="koboSpan" id="kobo.130.5">
     Lastly, DLT automatically handles the mundane data engineering tasks of maintaining optimized data file sizes of the underlying tables, as well as cleaning up obsolete data files that are no longer present in the Delta transaction log (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.131.1">
      Optimize
     </span>
    </strong>
    <span class="koboSpan" id="kobo.132.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.133.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.134.1">
     operations are covered later in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.135.1">
      A quick Delta Lake primer
     </span>
    </em>
    <span class="koboSpan" id="kobo.136.1">
     section).
    </span>
    <span class="koboSpan" id="kobo.136.2">
     DLT aims to ease the maintenance and operational burden on data engineering teams so that they can focus their time on uncovering business value from the data stored in their lakehouse, rather than spending time managing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.137.1">
      operational complexities.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-24">
    <a id="_idTextAnchor025">
    </a>
    <span class="koboSpan" id="kobo.138.1">
     How is DLT related to Delta Lake?
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.139.1">
     The DLT
    </span>
    <a id="_idIndexMarker018">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     framework relies heavily on the Delta Lake format to incrementally process data at every step of the way.
    </span>
    <span class="koboSpan" id="kobo.140.2">
     For example, streaming tables and materialized views defined in a DLT pipeline are backed by a Delta table.
    </span>
    <span class="koboSpan" id="kobo.140.3">
     Features that make Delta Lake an ideal storage format for a streaming pipeline include support for
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.141.1">
      Atomicity, Consistency, Isolation, and Durability
     </span>
    </strong>
    <span class="koboSpan" id="kobo.142.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.143.1">
      ACID
     </span>
    </strong>
    <span class="koboSpan" id="kobo.144.1">
     ) transactions so that concurrent data modifications such as
    </span>
    <a id="_idIndexMarker019">
    </a>
    <span class="koboSpan" id="kobo.145.1">
     inserts, updates, and deletions can be incrementally applied to a streaming table.
    </span>
    <span class="koboSpan" id="kobo.145.2">
     Plus, Delta Lake features scalable metadata handling, allowing Delta Lake to easily scale to petabytes and beyond.
    </span>
    <span class="koboSpan" id="kobo.145.3">
     If there is incorrect data computation, Delta Lake offers time travel – the ability to restore a copy of a table to a previous snapshot.
    </span>
    <span class="koboSpan" id="kobo.145.4">
     Lastly, Delta Lake inherently tracks audit information in each table’s transaction log.
    </span>
    <span class="koboSpan" id="kobo.145.5">
     Provenance information such as what type of operation modified the table, by what cluster, by which user, and at what precise timestamp are all captured alongside the data files.
    </span>
    <span class="koboSpan" id="kobo.145.6">
     Let’s look at how DLT leverages Delta tables to quickly and efficiently define data pipelines
    </span>
    <a id="_idIndexMarker020">
    </a>
    <span class="koboSpan" id="kobo.146.1">
     that can scale
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.147.1">
      over time.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-25">
    <a id="_idTextAnchor026">
    </a>
    <span class="koboSpan" id="kobo.148.1">
     Introducing DLT concepts
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.149.1">
     The DLT
    </span>
    <a id="_idIndexMarker021">
    </a>
    <span class="koboSpan" id="kobo.150.1">
     framework automatically manages task orchestration, cluster creation, and exception handling, allowing data engineers to focus on defining transformations, data enrichment, and data validation logic.
    </span>
    <span class="koboSpan" id="kobo.150.2">
     Data engineers will define a data pipeline using one or more dataset types.
    </span>
    <span class="koboSpan" id="kobo.150.3">
     Under the hood, the DLT system will determine how to keep these datasets up to date.
    </span>
    <span class="koboSpan" id="kobo.150.4">
     A data pipeline using the DLT framework is made up of the streaming tables, materialized views, and views dataset types, which we’ll discuss in detail in the following sections.
    </span>
    <span class="koboSpan" id="kobo.150.5">
     We’ll also briefly discuss how to visualize the pipeline, view its triggering method, and look at the entire pipeline data flow from a bird’s-eye view.
    </span>
    <span class="koboSpan" id="kobo.150.6">
     We’ll also briefly understand the different types of Databricks compute and runtime, and Unity Catalog.
    </span>
    <span class="koboSpan" id="kobo.150.7">
     Let’s go ahead and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.151.1">
      get started.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-26">
    <a id="_idTextAnchor027">
    </a>
    <span class="koboSpan" id="kobo.152.1">
     Streaming tables
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.153.1">
     Streaming tables leverage
    </span>
    <a id="_idIndexMarker022">
    </a>
    <span class="koboSpan" id="kobo.154.1">
     the benefits of Delta Lake and Spark Structured Streaming to incrementally process new data as it arrives.
    </span>
    <span class="koboSpan" id="kobo.154.2">
     This dataset type is useful when data must be ingested, transformed, or enriched at a high throughput and low latency.
    </span>
    <span class="koboSpan" id="kobo.154.3">
     Streaming tables were designed specifically for data sources that append new data only and do not include data modification, such as updates or deletes.
    </span>
    <span class="koboSpan" id="kobo.154.4">
     As a result, this type of dataset can scale to large data volumes, since it can incrementally apply data transformations as soon as new data arrives and
    </span>
    <a id="_idIndexMarker023">
    </a>
    <span class="koboSpan" id="kobo.155.1">
     does not need to recompute the entire table history during a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.156.1">
      pipeline update.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-27">
    <a id="_idTextAnchor028">
    </a>
    <span class="koboSpan" id="kobo.157.1">
     Materialized views
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.158.1">
     Materialized views
    </span>
    <a id="_idIndexMarker024">
    </a>
    <span class="koboSpan" id="kobo.159.1">
     leverage Delta Lake to compute the latest changes to a dataset and materialize the results in cloud storage.
    </span>
    <span class="koboSpan" id="kobo.159.2">
     This dataset type is great when the data source includes data modifications such as updates and deletions, or a data aggregation must be performed.
    </span>
    <span class="koboSpan" id="kobo.159.3">
     Under the hood, the DLT framework will perform the calculations to recompute the latest data changes to the dataset, using the full table’s history.
    </span>
    <span class="koboSpan" id="kobo.159.4">
     The output of this calculation is stored in cloud storage so that future queries can reference the pre-computed results, as opposed to re-performing the full calculations each time the table is queried.
    </span>
    <span class="koboSpan" id="kobo.159.5">
     As a result, this type of dataset will incur additional storage and compute costs each time the materialized view is updated.
    </span>
    <span class="koboSpan" id="kobo.159.6">
     Furthermore, materialized views can be published to Unity Catalog, so the results can be queried outside of the DLT data pipeline.
    </span>
    <span class="koboSpan" id="kobo.159.7">
     This is great when you need to share the output of a query across multiple
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.160.1">
      data pipelines.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-28">
    <a id="_idTextAnchor029">
    </a>
    <span class="koboSpan" id="kobo.161.1">
     Views
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.162.1">
     Views also recompute
    </span>
    <a id="_idIndexMarker025">
    </a>
    <span class="koboSpan" id="kobo.163.1">
     the latest results of a particular query but do not materialize the results to cloud storage, which helps save on storage costs.
    </span>
    <span class="koboSpan" id="kobo.163.2">
     This dataset type is great when you want to quickly check the intermediate result of data transformations in a data pipeline or apply other ad hoc data validations.
    </span>
    <span class="koboSpan" id="kobo.163.3">
     Furthermore, the results of this dataset type cannot be published to Unity Catalog and are only available within the context of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.164.1">
      data pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.165.1">
     The following table summarizes the differences between the different dataset types in the DLT framework and when it’s appropriate to use one dataset type versus
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.166.1">
      the other:
     </span>
    </span>
   </p>
   <table class="T---Table _idGenTablePara-1" id="table001-1">
    <colgroup>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="T---Table">
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.167.1">
           Dataset type
          </span>
         </strong>
        </span>
       </p>
      </td>
      <td class="T---Table T---Body T---Body">
       <p>
        <strong class="bold">
         <span class="koboSpan" id="kobo.168.1">
          When to
         </span>
        </strong>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.169.1">
           use it
          </span>
         </strong>
        </span>
       </p>
      </td>
     </tr>
     <tr class="T---Table">
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.170.1">
          Streaming
         </span>
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.171.1">
          t
         </span>
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.172.1">
          able
         </span>
        </span>
       </p>
      </td>
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="koboSpan" id="kobo.173.1">
         Ingestion workloads, when you need to continuously append new data to a target table with high throughput and
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.174.1">
          low latency.
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="T---Table">
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.175.1">
          Materialized
         </span>
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.176.1">
          v
         </span>
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.177.1">
          iew
         </span>
        </span>
       </p>
      </td>
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="koboSpan" id="kobo.178.1">
         Data operations that include data modifications, such as updates and deletions, or you need to perform aggregations on the full
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.179.1">
          table history.
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="T---Table">
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.180.1">
          View
         </span>
        </span>
       </p>
      </td>
      <td class="T---Table T---Body T---Body">
       <p>
        <span class="koboSpan" id="kobo.181.1">
         When you need to query intermediate data without publishing the results to Unity Catalog (e.g., perform data quality checks on
        </span>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.182.1">
          intermediate transformations)
         </span>
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.183.1">
     Table 1.1 – Each dataset type in DLT serves a different purpose
    </span>
   </p>
   <h2 id="_idParaDest-29">
    <a id="_idTextAnchor030">
    </a>
    <span class="koboSpan" id="kobo.184.1">
     Pipeline
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.185.1">
     A DLT
    </span>
    <a id="_idIndexMarker026">
    </a>
    <span class="koboSpan" id="kobo.186.1">
     pipeline is the logical data processing graph of one or more streaming tables, materialized views, or views.
    </span>
    <span class="koboSpan" id="kobo.186.2">
     The DLT framework will take dataset declarations, using either the Python API or SQL API, and infer the dependencies between each dataset.
    </span>
    <span class="koboSpan" id="kobo.186.3">
     Once a pipeline update runs, the DLT framework will update the datasets in the correct order using a dependency graph, called
    </span>
    <a id="_idIndexMarker027">
    </a>
    <span class="koboSpan" id="kobo.187.1">
     a
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.188.1">
       dataflow graph
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.189.1">
      .
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-30">
    <a id="_idTextAnchor031">
    </a>
    <span class="koboSpan" id="kobo.190.1">
     Pipeline triggers
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.191.1">
     A pipeline will be
    </span>
    <a id="_idIndexMarker028">
    </a>
    <span class="koboSpan" id="kobo.192.1">
     executed based on some triggering event.
    </span>
    <span class="koboSpan" id="kobo.192.2">
     DLT offers three types of triggers – manual, scheduled, and continuous triggers.
    </span>
    <span class="koboSpan" id="kobo.192.3">
     Once triggered, the pipeline will initialize and execute the dataflow graph, updating each of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.193.1">
      dataset states.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-31">
    <a id="_idTextAnchor032">
    </a>
    <span class="koboSpan" id="kobo.194.1">
     Workflow
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.195.1">
     Databricks workflows is
    </span>
    <a id="_idIndexMarker029">
    </a>
    <span class="koboSpan" id="kobo.196.1">
     a managed orchestration feature
    </span>
    <a id="_idIndexMarker030">
    </a>
    <span class="koboSpan" id="kobo.197.1">
     of the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.198.1">
      Databricks Data Intelligence Platform
     </span>
    </strong>
    <span class="koboSpan" id="kobo.199.1">
     that allows data engineers to chain together one or more dependent data processing tasks.
    </span>
    <span class="koboSpan" id="kobo.199.2">
     For more complex data processing use cases, it may be necessary to build a data pipeline using multiple, nested DLT pipelines.
    </span>
    <span class="koboSpan" id="kobo.199.3">
     For those use cases, Databricks workflows can simplify the
    </span>
    <a id="_idIndexMarker031">
    </a>
    <span class="koboSpan" id="kobo.200.1">
     orchestration of these data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.201.1">
      processing tasks.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-32">
    <a id="_idTextAnchor033">
    </a>
    <span class="koboSpan" id="kobo.202.1">
     Types of Databricks compute
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.203.1">
     There are four
    </span>
    <a id="_idIndexMarker032">
    </a>
    <span class="koboSpan" id="kobo.204.1">
     types of computational resources available to Databricks users from the Databricks Data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.205.1">
      Intelligence Platform.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.206.1">
     Job computes
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     A job compute is
    </span>
    <a id="_idIndexMarker033">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     an ephemeral collection
    </span>
    <a id="_idIndexMarker034">
    </a>
    <span class="koboSpan" id="kobo.209.1">
     of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.210.1">
      virtual machines
     </span>
    </strong>
    <span class="koboSpan" id="kobo.211.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.212.1">
      VMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.213.1">
     ) with
    </span>
    <a id="_idIndexMarker035">
    </a>
    <span class="koboSpan" id="kobo.214.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.215.1">
      Databricks Runtime
     </span>
    </strong>
    <span class="koboSpan" id="kobo.216.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.217.1">
      DBR
     </span>
    </strong>
    <span class="koboSpan" id="kobo.218.1">
     ) installed that are dynamically provisioned for the duration of a scheduled job.
    </span>
    <span class="koboSpan" id="kobo.218.2">
     Once the job is complete, the VMs are immediately released back to the cloud provider.
    </span>
    <span class="koboSpan" id="kobo.218.3">
     Since job clusters do not utilize the UI components of the Databricks Data Intelligence Platform (e.g., notebooks and the query editor), job clusters assess a lower
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.219.1">
      Databricks Unit
     </span>
    </strong>
    <span class="koboSpan" id="kobo.220.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.221.1">
      DBU
     </span>
    </strong>
    <span class="koboSpan" id="kobo.222.1">
     ) for the
    </span>
    <a id="_idIndexMarker036">
    </a>
    <span class="koboSpan" id="kobo.223.1">
     entirety of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.224.1">
      their execution.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.225.1">
     All-purpose computes
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.226.1">
     An all-purpose
    </span>
    <a id="_idIndexMarker037">
    </a>
    <span class="koboSpan" id="kobo.227.1">
     compute is a collection of ephemeral VMs with the DBR installed that is dynamically provisioned by a user, directly from the Databricks UI via a button click, or via the Databricks REST API (using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.228.1">
      /api/2.0/clusters/create
     </span>
    </strong>
    <span class="koboSpan" id="kobo.229.1">
     endpoint, for example), and they remain running until a user, or an expiring auto-termination timer, terminates the cluster.
    </span>
    <span class="koboSpan" id="kobo.229.2">
     Upon termination, the VMs are returned to the cloud provider, and Databricks stops assessing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.230.1">
      additional DBUs.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.231.1">
     Instance pools
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.232.1">
     Instance pools are
    </span>
    <a id="_idIndexMarker038">
    </a>
    <span class="koboSpan" id="kobo.233.1">
     a feature in Databricks that helps reduce the time it takes to provision additional VMs and install the DBR.
    </span>
    <span class="koboSpan" id="kobo.233.2">
     Instance pools will pre-provision VMs from the cloud provider and hold them in a logical container, similar to a valet keeping your car running in a valet
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.234.1">
      parking lot.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.235.1">
     For some cloud providers, it can take 15 minutes or more to provision an additional VM, leading to longer troubleshooting cycles or ad hoc development tasks, such as log inspection or rerunning failed notebook cells during the development of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.236.1">
      new features.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.237.1">
     Additionally, instance pools improve efficiency when many jobs are scheduled to execute closely together or with overlapping schedules.
    </span>
    <span class="koboSpan" id="kobo.237.2">
     For example, as one job finishes, rather than releasing the VMs back to the cloud provider, the job cluster can place the VMs into the instance pool to be reused by the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.238.1">
      next job.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.239.1">
     Before returning the VMs to the instance pool, the Databricks container installed on the VM is destroyed, and a new container is installed on the VM containing the DBR when the next scheduled job requests
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.240.1">
      the VM.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.241.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.242.1">
     Databricks will not assess additional DBUs while VM(s) are up and running.
    </span>
    <span class="koboSpan" id="kobo.242.2">
     However, the cloud provider will continue to charge for as long as the VMs are held in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.243.1">
      instance pools.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.244.1">
     To help control costs, instance pools provide an autoscaling feature that allows the size of the pool to grow
    </span>
    <a id="_idIndexMarker039">
    </a>
    <span class="koboSpan" id="kobo.245.1">
     and shrink, in response to demand.
    </span>
    <span class="koboSpan" id="kobo.245.2">
     For example, the instance pool might grow to 10 VMs during peak hours but shrink back to 1 or 2 during lulls in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.246.1">
      processing demand.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.247.1">
     Databricks SQL warehouses
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.248.1">
     The last type of
    </span>
    <a id="_idIndexMarker040">
    </a>
    <span class="koboSpan" id="kobo.249.1">
     computational resource featured in the Databricks Data Intelligence Platform is
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.250.1">
      Databricks SQL
     </span>
    </strong>
    <span class="koboSpan" id="kobo.251.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.252.1">
      DBSQL
     </span>
    </strong>
    <span class="koboSpan" id="kobo.253.1">
     ) warehouses.
    </span>
    <span class="koboSpan" id="kobo.253.2">
     DBSQL warehouses are designed to run SQL workloads such
    </span>
    <a id="_idIndexMarker041">
    </a>
    <span class="koboSpan" id="kobo.254.1">
     as queries, reports, and dashboards.
    </span>
    <span class="koboSpan" id="kobo.254.2">
     Furthermore, DBSQL warehouses are pre-configured computational resources designed to limit the configuration that a data analyst or SQL analyst would need to optimize for ad hoc data exploration and query execution.
    </span>
    <span class="koboSpan" id="kobo.254.3">
     DBSQL warehouses are preconfigured with the latest DBRs, leverage the Databricks Photon engine, and have advanced Spark configuration settings preconfigured to optimize performance.
    </span>
    <span class="koboSpan" id="kobo.254.4">
     A DBSQL warehouse also includes additional performance features such as results caching and disk caching, which can accelerate workloads by moving data closer to the hardware performing the query calculations.
    </span>
    <span class="koboSpan" id="kobo.254.5">
     Combined with the processing speed of the Photon engine, the DBSQL warehouse achieves cloud warehouse speeds that Apache Spark once struggled
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.255.1">
      to meet.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-33">
    <a id="_idTextAnchor034">
    </a>
    <span class="koboSpan" id="kobo.256.1">
     Databricks Runtime
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.257.1">
     The Databricks Runtime
    </span>
    <a id="_idIndexMarker042">
    </a>
    <span class="koboSpan" id="kobo.258.1">
     is a set of libraries pre-installed
    </span>
    <a id="_idIndexMarker043">
    </a>
    <span class="koboSpan" id="kobo.259.1">
     on the driver and worker nodes of a cluster during cluster initialization.
    </span>
    <span class="koboSpan" id="kobo.259.2">
     These libraries include popular Java, R, and Python libraries to assist end users with ad hoc data wrangling or other development tasks.
    </span>
    <span class="koboSpan" id="kobo.259.3">
     The libraries include core components that interface with the Databricks backend services to support rich platform features, such as collaborative notebooks, workflows, and cluster metrics.
    </span>
    <span class="koboSpan" id="kobo.259.4">
     Furthermore, DBR includes other performance features such as data file caching (known as disk caching), the Databricks Photon engine for accelerated Spark processing, and other computational speed-ups.
    </span>
    <span class="koboSpan" id="kobo.259.5">
     DBR comes in two varieties, Standard and ML, which are tailored to assist with the workloads anticipated to be run, based on the end user persona.
    </span>
    <span class="koboSpan" id="kobo.259.6">
     For example, DBR for ML would have popular Python libraries such as TensorFlow and scikit-learn pre-installed to assist end users with the training of ML models, feature engineering, and other ML
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.260.1">
      development tasks.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-34">
    <a id="_idTextAnchor035">
    </a>
    <span class="koboSpan" id="kobo.261.1">
     Unity Catalog
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.262.1">
     As the name
    </span>
    <a id="_idIndexMarker044">
    </a>
    <span class="koboSpan" id="kobo.263.1">
     suggests, Unity Catalog is a centralized
    </span>
    <a id="_idIndexMarker045">
    </a>
    <span class="koboSpan" id="kobo.264.1">
     governance store that is intended to span multiple Databricks workspaces.
    </span>
    <span class="koboSpan" id="kobo.264.2">
     Rather than repeatedly defining the data governance policies for users and groups within each Databricks workspace, Unity Catalog allows data administrators to define access policies once in a centralized location.
    </span>
    <span class="koboSpan" id="kobo.264.3">
     As a result, Unity Catalog acts as a single source of truth for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.265.1">
      data governance.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.266.1">
     In addition to data access policies, Unity Catalog also features data auditing, data lineage, data discovery, and data sharing capabilities, which will be covered in
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.267.1">
      Chapters 5
     </span>
    </em>
    <span class="koboSpan" id="kobo.268.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.269.1">
      6
     </span>
    </em>
    <span class="koboSpan" id="kobo.270.1">
     ,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.271.1">
      and
     </span>
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.272.1">
       7
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.273.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.274.1">
     Unity Catalog is tightly integrated into the Databricks lakehouse, making it easy to build near-real-time data pipelines with strong data security in mind, using an open lakehouse storage format such as
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.275.1">
      Delta Lake.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-35">
    <a id="_idTextAnchor036">
    </a>
    <span class="koboSpan" id="kobo.276.1">
     A quick Delta Lake primer
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.277.1">
     Delta Lake is a big data
    </span>
    <a id="_idIndexMarker046">
    </a>
    <span class="koboSpan" id="kobo.278.1">
     file protocol built around a multi-version transaction log that provides features such as ACID transactions, schema enforcement, time travel, data file management, and other performance features on top of existing data files in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.279.1">
      a
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.280.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.281.1">
      akehouse.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.282.1">
     Originally, big data architectures had many concurrent processes that both read and modified data, leading to data corruption and even data loss.
    </span>
    <span class="koboSpan" id="kobo.282.2">
     As previously mentioned, a two-pronged Lambda architecture was created, providing a layer of isolation between processes that applied streaming updates to data and downstream processes that needed a consistent snapshot of the data, such as BI workloads that generated daily reports or refreshed dashboards.
    </span>
    <span class="koboSpan" id="kobo.282.3">
     However, these Lambda architectures duplicated data to support these batch and streaming workloads, leading to inconsistent data changes that needed to be reconciled at the end of each
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.283.1">
      business day.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.284.1">
     Fortunately, the Delta Lake format provides a common storage layer for a lakehouse across disparate workloads and unifies both batch and streaming workloads.
    </span>
    <span class="koboSpan" id="kobo.284.2">
     As such, a Delta table serves as the foundation for a Delta Live Table.
    </span>
    <span class="koboSpan" id="kobo.284.3">
     Under the hood, a Delta Live Table is backed by a Delta table that is added to a dataflow graph, and whose state is updated by the DLT system whenever a DLT pipeline update
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.285.1">
      is executed.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-36">
    <a id="_idTextAnchor037">
    </a>
    <span class="koboSpan" id="kobo.286.1">
     The architecture of a Delta table
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.287.1">
     The Delta transaction
    </span>
    <a id="_idIndexMarker047">
    </a>
    <span class="koboSpan" id="kobo.288.1">
     log is a key piece of the architecture for this big data format.
    </span>
    <span class="koboSpan" id="kobo.288.2">
     Each Delta table contains a transaction log, which is a directory name,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.289.1">
      _delta_log
     </span>
    </strong>
    <span class="koboSpan" id="kobo.290.1">
     , located at a ta
    </span>
    <a id="_idTextAnchor038">
    </a>
    <span class="koboSpan" id="kobo.291.1">
     ble’s root directory.
    </span>
    <span class="koboSpan" id="kobo.291.2">
     The transaction log is a multi-version system of records that keeps track of the table’s state over a linear period
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.292.1">
      of time.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer010">
     <span class="koboSpan" id="kobo.293.1">
      <img alt="Figure 1.3 – The Delta transaction log sits alongside the partition directories and data files in a separate directory titled _delta_log" src="image/B22011_01_003.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.294.1">
     Figure 1.3 – The Delta transaction log sits alongside the partition directories and data files in a separate directory titled _delta_log
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.295.1">
     The transaction log informs
    </span>
    <a id="_idIndexMarker048">
    </a>
    <span class="koboSpan" id="kobo.296.1">
     the Delta Lake engine which data files to read to answer a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.297.1">
      particular query.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.298.1">
     Within each transaction log directory, there will be one or more files stored in the JSON format, as well as other metadata information to help quickly and efficiently calculate the Delta table’s state (covered in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.299.1">
      following section).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.300.1">
     As new data is appended, updated, or even deleted from a Delta table, these changes are recorded, or committed, to this directory as metadata information, stored as atomic JSON files.
    </span>
    <span class="koboSpan" id="kobo.300.2">
     The JSON files are named using an ordered integer, starting with
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.301.1">
      00…0.json
     </span>
    </strong>
    <span class="koboSpan" id="kobo.302.1">
     and incrementing by one after each successful
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.303.1">
      transaction commit.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.304.1">
     If a Delta table is partitioned, there will be one or more subdirectories containing the partitioning column information within the table’s root directory.
    </span>
    <span class="koboSpan" id="kobo.304.2">
     Hive-style table partitioning is a very common performance technique that can speed up a query by collocating similar data within the same directory.
    </span>
    <span class="koboSpan" id="kobo.304.3">
     The data is collocated by a particular column’s value (e.g.,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.305.1">
      "date=2024-01-05"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.306.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.306.2">
     Optionally, there can be even more subdirectories nested within these partition directories, depending upon how many columns a table is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.307.1">
      partitioned by.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.308.1">
     Within these partition subdirectories are one or more data files, stored using the Apache Parquet format.
    </span>
    <span class="koboSpan" id="kobo.308.2">
     Apache Parquet is a popular columnar storage format, with efficient data compression and encoding schemes that yield fast data storage and retrieval for big data workloads.
    </span>
    <span class="koboSpan" id="kobo.308.3">
     As a result, this open format was chosen as a foundation to store
    </span>
    <a id="_idIndexMarker049">
    </a>
    <span class="koboSpan" id="kobo.309.1">
     the data files that make up a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.310.1">
      Delta table.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-37">
    <a id="_idTextAnchor039">
    </a>
    <span class="koboSpan" id="kobo.311.1">
     The contents of a transaction commit
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.312.1">
     As mentioned earlier, the
    </span>
    <a id="_idIndexMarker050">
    </a>
    <span class="koboSpan" id="kobo.313.1">
     transaction log is the single source of truth for a Delta table.
    </span>
    <span class="koboSpan" id="kobo.313.2">
     Each committed transaction (the JSON file under the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.314.1">
      _delta_log
     </span>
    </strong>
    <span class="koboSpan" id="kobo.315.1">
     directory) will contain metadata information about the operation, or action, being applied to a particular Delta table.
    </span>
    <span class="koboSpan" id="kobo.315.2">
     These JSON files can be viewed as a set of actions.
    </span>
    <span class="koboSpan" id="kobo.315.3">
     Although there can be many concurrent transactions, the history of transaction commits is replayed in a linear order by table readers, and the result is the latest state of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.316.1">
      Delta table.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.317.1">
     Each JSON file could contain any of the following actions, as outlined by the Delta
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.318.1">
      Lake protocol:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.319.1">
       Change metadata
      </span>
     </strong>
     <span class="koboSpan" id="kobo.320.1">
      : This type
     </span>
     <a id="_idIndexMarker051">
     </a>
     <span class="koboSpan" id="kobo.321.1">
      of action is used to update the name, schema, or partitioning information of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.322.1">
       a table.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.323.1">
       Add file
      </span>
     </strong>
     <span class="koboSpan" id="kobo.324.1">
      : Perhaps the most frequent action applied, this action adds a new data file to a table along with statistical information about the first 32 columns of a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.325.1">
       Delta table.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.326.1">
       Remove file
      </span>
     </strong>
     <span class="koboSpan" id="kobo.327.1">
      : This action will logically delete a particular data file.
     </span>
     <span class="koboSpan" id="kobo.327.2">
      Note that the physical data file will remain in cloud storage even after this transaction is committed (there’s more about this topic in the
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.328.1">
       Tombstoned data
      </span>
     </em>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.329.1">
        files
       </span>
      </em>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.330.1">
       section).
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.331.1">
       Add Change Data Capture (CDC) information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.332.1">
      : This action is used to add a CDC file that will contain all the data that has changed as a result of a particular
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.333.1">
       table transaction.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.334.1">
       Transaction identifiers
      </span>
     </strong>
     <span class="koboSpan" id="kobo.335.1">
      : This action is used for Structured Streaming workloads and will contain the unique identifier for a particular stream, as well as the epoch identifier for the most recently committed Structured
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.336.1">
       Streaming micro-batch.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.337.1">
       Protocol evolution
      </span>
     </strong>
     <span class="koboSpan" id="kobo.338.1">
      : Provides backward compatibility and ensures that old Delta Lake table readers can read the metadata information within the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.339.1">
       transaction log.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.340.1">
       Commit provenance information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.341.1">
      : This type of action will conta
     </span>
     <a id="_idTextAnchor040">
     </a>
     <span class="koboSpan" id="kobo.342.1">
      in information about the process of committing a particular data transaction to a table.
     </span>
     <span class="koboSpan" id="kobo.342.2">
      This will include information including the timestamp, the operation type, cluster identifier, and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.343.1">
       user information.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.344.1">
       Domain metadata
      </span>
     </strong>
     <span class="koboSpan" id="kobo.345.1">
      : This type of action sets the configuration for a particular domain.
     </span>
     <span class="koboSpan" id="kobo.345.2">
      There are two types of domain metadata – system domain and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.346.1">
       user-controlled domain.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.347.1">
       Sidecar file information
      </span>
     </strong>
     <span class="koboSpan" id="kobo.348.1">
      : This type of action will commit a separate metadata file to the transaction
     </span>
     <a id="_idIndexMarker052">
     </a>
     <span class="koboSpan" id="kobo.349.1">
      log, which contains summary information about the checkpoint file that was created (checkpoints are covered in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.350.1">
       following section).
      </span>
     </span>
    </li>
   </ul>
   <h2 id="_idParaDest-38">
    <a id="_idTextAnchor041">
    </a>
    <span class="koboSpan" id="kobo.351.1">
     Supporting concurrent table reads and writes
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.352.1">
     There are two
    </span>
    <a id="_idIndexMarker053">
    </a>
    <span class="koboSpan" id="kobo.353.1">
     types of concurrency control methods in storage systems – pessimistic concurrency control and optimistic concurrency control.
    </span>
    <span class="koboSpan" id="kobo.353.2">
     A pessimistic concurrency control will attempt to thwart possible table conflicts by locking an entire table until an ongoing transaction has been completed.
    </span>
    <span class="koboSpan" id="kobo.353.3">
     Conversely, optimistic concurrency control does not lock a table and will permit potential transaction conflicts
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.354.1">
      to happen.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.355.1">
     The authors of the Delta Lake protocol chose to implement the Delta Lake format using optimistic concurrency control.
    </span>
    <span class="koboSpan" id="kobo.355.2">
     The reason why this design choice was made is that most big data workloads will append new data to an existing table, as opposed to modifying
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.356.1">
      existing data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.357.1">
     As an example, let’s look at how Delta Lake will deal with a concurrent write conflict between two table writers – Table Writer A and Table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.358.1">
      Writer B:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer011">
     <span class="koboSpan" id="kobo.359.1">
      <img alt="Figure 1.4 – Delta Lake implements an optimistic concurrency scheme to handle concurrent write conflicts" src="image/B22011_01_004.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.360.1">
     Figure 1.4 – Delta Lake implements an optimistic concurrency scheme to handle concurrent write conflicts
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.361.1">
     Imagine that the two table
    </span>
    <a id="_idIndexMarker054">
    </a>
    <span class="koboSpan" id="kobo.362.1">
     writers modify the same data files and attempt to commit the data changes that conflict with one another.
    </span>
    <span class="koboSpan" id="kobo.362.2">
     Let’s look at how Delta Lake handles this type
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.363.1">
      of scenario.
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.364.1">
      Writer A will first record the starting version identifier of the transaction that it will attempt to commit to the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.365.1">
       transaction log.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.366.1">
      Writer A will then write all the data files for the transaction that it would like
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.367.1">
       to commit.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.368.1">
      Next, Writer A will attempt to commit the transaction to the Delta
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.369.1">
       transaction log.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.370.1">
      At the same time, Writer B has already committed their transaction using the same table
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.371.1">
       version identifier.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.372.1">
      Writer A detects Writer B’s commit and replays the commit information to determine whether any of the underlying data files have changed (e.g., the data has
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.373.1">
       been updated).
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.374.1">
      If no data has changed (for example, both Writer A and Writer B commit append-only operations to the transaction log), then Writer A will increment the version identifier by 1 and attempt to recommit the transaction to the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.375.1">
       transaction log.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.376.1">
      If the data has changed,
     </span>
     <a id="_idTextAnchor042">
     </a>
     <span class="koboSpan" id="kobo.377.1">
      then Writer A will need to recompute the transaction from scratch, increment
     </span>
     <a id="_idIndexMarker055">
     </a>
     <span class="koboSpan" id="kobo.378.1">
      the version identifier, and attempt to recommit
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.379.1">
       the transaction.
      </span>
     </span>
    </li>
   </ol>
   <h2 id="_idParaDest-39">
    <a id="_idTextAnchor043">
    </a>
    <span class="koboSpan" id="kobo.380.1">
     Tombstoned data files
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.381.1">
     When an update is
    </span>
    <a id="_idIndexMarker056">
    </a>
    <span class="koboSpan" id="kobo.382.1">
     applied to a Delta table that requires the data within a file to be updated, a new file using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.383.1">
      AddFile
     </span>
    </strong>
    <span class="koboSpan" id="kobo.384.1">
     operation will be created.
    </span>
    <span class="koboSpan" id="kobo.384.2">
     Similarly, the file containing the out-of-date data will be logically deleted, using a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.385.1">
      RemoveFile
     </span>
    </strong>
    <span class="koboSpan" id="kobo.386.1">
     operation.
    </span>
    <span class="koboSpan" id="kobo.386.2">
     Then, both actions will be committed to the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.387.1">
      transaction log.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.388.1">
     By default, Delta Lake will retain the table metadat
    </span>
    <a id="_idTextAnchor044">
    </a>
    <span class="koboSpan" id="kobo.389.1">
     a (transaction log data) for 30 days before being automatically removed from cloud storage.
    </span>
    <span class="koboSpan" id="kobo.389.2">
     When a particular data file is removed from the Delta transaction log, this is often referred to as a
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.390.1">
       tombstoned file
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.391.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.392.1">
     To help control cloud storage costs, these tombstoned files, or files that no longer make up the latest Delta table state and are no longer referenced in the transaction log, can be removed from cloud storage altogether.
    </span>
    <span class="koboSpan" id="kobo.392.2">
     A separate Delta Lake file management utility, called the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.393.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.394.1">
     command, can be run as a separate process to identify all the tombstoned data files and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.395.1">
      remove them.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.396.1">
     Furthermore, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.397.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.398.1">
     command is configurable, and the length of time to remove the table files can be specified as an optional input parameter.
    </span>
    <span class="koboSpan" id="kobo.398.2">
     For example, the following code snippet will execute the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.399.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.400.1">
     command on the Delta table,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.401.1">
      yellow_taxi
     </span>
    </strong>
    <span class="koboSpan" id="kobo.402.1">
     , remov
    </span>
    <a id="_idTextAnchor045">
    </a>
    <span class="koboSpan" id="kobo.403.1">
     ing data files from the last 14 days of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.404.1">
      table history:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.405.1">
%sql
-- Vacuums the Delta table `yellow_taxi`
-- and retains 14 days of table history
VACUUM yellow_taxi RETAIN 336 HOURS</span></pre>
   <p>
    <span class="koboSpan" id="kobo.406.1">
     As we’ll see in the
    </span>
    <a id="_idIndexMarker057">
    </a>
    <span class="koboSpan" id="kobo.407.1">
     upcoming chapter, this process is automatically run and managed for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.408.1">
      DLT pipelines.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-40">
    <a id="_idTextAnchor046">
    </a>
    <span class="koboSpan" id="kobo.409.1">
     Calculating Delta table state
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.410.1">
     As alluded to in
    </span>
    <a id="_idIndexMarker058">
    </a>
    <span class="koboSpan" id="kobo.411.1">
     the previous section, Delta Lake will automatically compact metadata in a transaction log.
    </span>
    <span class="koboSpan" id="kobo.411.2">
     As you can imagine, in big data workloads with thousands or even millions of transactions each day, a Delta table can rapidly grow in size.
    </span>
    <span class="koboSpan" id="kobo.411.3">
     Similarly, the commit information in the transaction log will also
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.412.1">
      grow comparatively.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.413.1">
     For every 10th commit, Delta Lake will create a checkpoint file, using the Apache Parquet format, that contains the latest table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.414.1">
      state information.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer012">
     <span class="koboSpan" id="kobo.415.1">
      <img alt="Figure 1.5 – For every 10th commit, Delta Lake will write a checkpoint file" src="image/B22011_01_005.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.416.1">
     Figure 1.5 – For every 10th commit, Delta Lake will write a checkpoint file
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.417.1">
     Under the hood, a Delta Lake reader creates a separate Apache Spark Job to efficiently read the Delta table’s commit logs.
    </span>
    <span class="koboSpan" id="kobo.417.2">
     For example, to calculate the latest table state, a Delta Lake reader will begin by reading the latest checkpoint file and applying the transaction commits that may have occurred after the file
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.418.1">
      was created.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.419.1">
     Storing the table state information in checkpoint files alongside the data files in cloud storage was another pivotal design choice for the Delta Lake format.
    </span>
    <span class="koboSpan" id="kobo.419.2">
     By using this method, calculating a table’s state could scale much better than other methods, such as using the Hive Metastore to serve table metadata information.
    </span>
    <span class="koboSpan" id="kobo.419.3">
     Traditional big data metastores, such as the Hive Metastore, struggle to scale when many large, heavily active tables are queried concurrently and the table metadata information needs to be retrieved to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.420.1">
      answer queries.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.421.1">
     To further speed up queries, Delta Lake readers will also cache the table state in local memory; that way, table
    </span>
    <a id="_idIndexMarker059">
    </a>
    <span class="koboSpan" id="kobo.422.1">
     readers can calculate which data files will answer a particular table query
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.423.1">
      much faster.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-41">
    <a id="_idTextAnchor047">
    </a>
    <span class="koboSpan" id="kobo.424.1">
     Time travel
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.425.1">
     Another file management
    </span>
    <a id="_idIndexMarker060">
    </a>
    <span class="koboSpan" id="kobo.426.1">
     utility in Delta Lake is the time travel feature that allows end users to query a table’s state from a previous version.
    </span>
    <span class="koboSpan" id="kobo.426.2">
     Time travel offers two methods to specify table state – using the table version number assigned in the transaction log or by using
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.427.1">
      a timestamp.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.428.1">
     Users can query a previous Delta table’s state directly using the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.429.1">
      SQL syntax:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.430.1">
%sql
SELECT *
  FROM yellow_taxi
  TIMESTAMP AS OF '2023-12-31'</span></pre>
   <p>
    <span class="koboSpan" id="kobo.431.1">
     Similarly, Python users on the Databricks Data Intelligence Platform can use the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.432.1">
      Python API:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.433.1">
%py
display(
    (spark.read
        .format("delta")
        .option("timestampAsOf", "2023-12-31")
        .load("s3a://my-data-lake/yellow_taxi/"))
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.434.1">
     It’s important to note that, by default, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.435.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.436.1">
     utility will remove all data files from a particular Delta table, from the last seven days of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.437.1">
      table versions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.438.1">
     As a result, if the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.439.1">
      Vacuum
     </span>
    </strong>
    <span class="koboSpan" id="kobo.440.1">
     command is run and a user attempts to query table history beyond the last seven days, the end user will receive a runtime exception, specifying that the data referenced in the transaction log no
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.441.1">
      longer exists.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.442.1">
     Furthermore, Delta Lake’s time travel feature was designed to correct recent data issues, so it should not be used for long-term data storage requirements, such as implementing an
    </span>
    <a id="_idIndexMarker061">
    </a>
    <span class="koboSpan" id="kobo.443.1">
     auditing system with a history
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.444.1">
      spanning years.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-42">
    <a id="_idTextAnchor048">
    </a>
    <span class="koboSpan" id="kobo.445.1">
     Tracking table changes using change data feed
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.446.1">
     Delta Lake’s
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.447.1">
      Change Data Feed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.448.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.449.1">
      CDF
     </span>
    </strong>
    <span class="koboSpan" id="kobo.450.1">
     ) feature
    </span>
    <a id="_idIndexMarker062">
    </a>
    <span class="koboSpan" id="kobo.451.1">
     tracks row-level
    </span>
    <a id="_idIndexMarker063">
    </a>
    <span class="koboSpan" id="kobo.452.1">
     changes that have been made to a Delta table, as well as metadata about those changes.
    </span>
    <span class="koboSpan" id="kobo.452.2">
     For example, CDF will capture information about the operation type, the timestamp confirming that the change was made, and other provenance information such as cluster identification and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.453.1">
      user information.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.454.1">
     For update operations, CDF will capture a snapshot of the row before an update, as well as a snapshot of the row after the update has
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.455.1">
      been applied.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer013">
     <span class="koboSpan" id="kobo.456.1">
      <img alt="Figure 1.6 – CDF captures the operation type, commit version, and timestamp" src="image/B22011_01_006.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.457.1">
     Figure 1.6 – CDF captures the operation type, commit version, and timestamp
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.458.1">
     This feature is not enabled by default but can be configured by updating a Delta table’s properties.
    </span>
    <span class="koboSpan" id="kobo.458.2">
     For example, CDF can be enabled on an existing table by altering the table, using a SQL
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.459.1">
       ALTER
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.460.1">
      statement:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.461.1">
%sql
ALTER TABLE yellow_taxi
SET TBLPROPERTIES (delta.enableChangeDataFeed = true)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.462.1">
     Similarly, CDF can also be enabled when a table is created by including the table property as a part of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.463.1">
      CREATE
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.464.1">
       TABLE
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.465.1">
      statement:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.466.1">
%sql
CREATE TABLE IF NOT EXISTS yellow_taxi
TBLPROPERTIES (delta.enableChangeDataFeed = true)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.467.1">
     As we’ll see in the next chapter, this feature is important in how DLT can efficiently apply changes
    </span>
    <a id="_idIndexMarker064">
    </a>
    <span class="koboSpan" id="kobo.468.1">
     from a source
    </span>
    <a id="_idIndexMarker065">
    </a>
    <span class="koboSpan" id="kobo.469.1">
     table to downstream
    </span>
    <a id="_idIndexMarker066">
    </a>
    <span class="koboSpan" id="kobo.470.1">
     datasets and implement
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.471.1">
      slowly changing
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.472.1">
       dimensions
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.473.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.474.1">
       SCDs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.475.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-43">
    <a id="_idTextAnchor049">
    </a>
    <span class="koboSpan" id="kobo.476.1">
     A hands-on example – creating your first Delta Live Tables pipeline
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.477.1">
     In this section, we’ll
    </span>
    <a id="_idIndexMarker067">
    </a>
    <span class="koboSpan" id="kobo.478.1">
     use a NYC taxi sample dataset to declare a data pipeline, using the DLT framework, and apply a basic transformation to enrich
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.479.1">
      the data.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.480.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.481.1">
     To get the most value out of this section, it’s recommended to have Databricks workspace permissions to create an all-purpose cluster and a DLT pipeline, using a cluster policy.
    </span>
    <span class="koboSpan" id="kobo.481.2">
     In this section, you will attach a notebook to a cluster, execute notebook cells, as well as create and run a new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.482.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.483.1">
     Let’s start by creating a new all-purpose cluster.
    </span>
    <span class="koboSpan" id="kobo.483.2">
     Navigate to the Databricks Compute UI by selecting the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.484.1">
      Compute
     </span>
    </strong>
    <span class="koboSpan" id="kobo.485.1">
     button from the sidebar navigation on the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.486.1">
      left side.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer014">
     <span class="koboSpan" id="kobo.487.1">
      <img alt="Figure 1.7 – Navigate to the Compute UI from the left-hand sidebar" src="image/B22011_01_007.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.488.1">
     Figure 1.7 – Navigate to the Compute UI from the left-hand sidebar
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.489.1">
     Click the button titled
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.490.1">
      Create compute
     </span>
    </strong>
    <span class="koboSpan" id="kobo.491.1">
     at the top right.
    </span>
    <span class="koboSpan" id="kobo.491.2">
     Next, provide a name for the cluster.
    </span>
    <span class="koboSpan" id="kobo.491.3">
     For this exercise, the cluster can be a small, single-node cluster.
    </span>
    <span class="koboSpan" id="kobo.491.4">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.492.1">
      Single node
     </span>
    </strong>
    <span class="koboSpan" id="kobo.493.1">
     radio button for
    </span>
    <a id="_idIndexMarker068">
    </a>
    <span class="koboSpan" id="kobo.494.1">
     the cluster type.
    </span>
    <span class="koboSpan" id="kobo.494.2">
     Select the latest DBR in the runtime dropdown.
    </span>
    <span class="koboSpan" id="kobo.494.3">
     Accept the defaults and click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.495.1">
      Create compute
     </span>
    </strong>
    <span class="koboSpan" id="kobo.496.1">
     button
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.497.1">
      once again.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.498.1">
     Now that we have a cluster up and running, we can begin the development of our very first
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.499.1">
      data pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.500.1">
     Let’s first start by creating a new Databricks notebook under your workspace home directory.
    </span>
    <span class="koboSpan" id="kobo.500.2">
     Create a new notebook by clicking the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.501.1">
      Workspace
     </span>
    </strong>
    <span class="koboSpan" id="kobo.502.1">
     button on the left sidebar, clicking on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.503.1">
      Add
     </span>
    </strong>
    <span class="koboSpan" id="kobo.504.1">
     dropdown, and selecting
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.505.1">
      Notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.506.1">
     .
    </span>
    <span class="koboSpan" id="kobo.506.2">
     Give the notebook a meaningful name, such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.507.1">
      My First DLT Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.508.1">
     .
    </span>
    <span class="koboSpan" id="kobo.508.2">
     This new notebook is where we will declare the datasets and dependencies that will make up our Delta Live
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.509.1">
      Table pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.510.1">
     All Databricks workspaces come with a set of sample datasets, located at
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.511.1">
      /databricks-datasets
     </span>
    </strong>
    <span class="koboSpan" id="kobo.512.1">
     in the Databricks FileSystem.
    </span>
    <span class="koboSpan" id="kobo.512.2">
     You can browse the list of available datasets by listing the directory contents, using the Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.513.1">
      FileSystem utility:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.514.1">
%py
display(
    dbutils.fs.ls('/databricks-datasets')
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.515.1">
     Next, we need to import the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.516.1">
      dlt
     </span>
    </strong>
    <span class="koboSpan" id="kobo.517.1">
     Python module.
    </span>
    <span class="koboSpan" id="kobo.517.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.518.1">
      dlt
     </span>
    </strong>
    <span class="koboSpan" id="kobo.519.1">
     module contains function decorators that will instruct the DLT system on how to build our data pipeline, the dependencies, and an internal data processing graph, called a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.520.1">
      dataflow graph.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.521.1">
     Add the following line to a new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.522.1">
      notebook cell:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.523.1">
import dlt</span></pre>
   <p>
    <span class="koboSpan" id="kobo.524.1">
     DLT is built on top of PySpark, so we can leverage Spark DataFrames to define how to ingest
    </span>
    <a id="_idIndexMarker069">
    </a>
    <span class="koboSpan" id="kobo.525.1">
     data from cloud storage and how to apply data transformations.
    </span>
    <span class="koboSpan" id="kobo.525.2">
     Let’s start by defining a function that will use Spark to read the NYC taxi sample dataset from the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.526.1">
      /
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.527.1">
       databricks-datasets
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.528.1">
      directory:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.529.1">
def yellow_taxi_raw():
    path = "/databricks-datasets/nyctaxi/tripdata/yellow"
    return (spark.readStream
        .schema(schema)
        .format("csv")
        .option("header", True)
        .load(path))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.530.1">
     In this example, we’ve declared a simple function with a meaningful name, and when invoked, the function will use Spark to read the raw data stored in the yellow taxi dataset and return it as a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.531.1">
      streaming DataFrame.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.532.1">
     Now, we need to tell the DLT framework that we should use this declared function as a part of a data pipeline.
    </span>
    <span class="koboSpan" id="kobo.532.2">
     We can do this by adding the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.533.1">
      @dlt.table()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.534.1">
     function decorator.
    </span>
    <span class="koboSpan" id="kobo.534.2">
     This function decorator will create a Delta Live Table from the function and add it to the pipeline’s dataflow graph.
    </span>
    <span class="koboSpan" id="kobo.534.3">
     Let’s also add some descriptive text to the optional
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.535.1">
      comment
     </span>
    </strong>
    <span class="koboSpan" id="kobo.536.1">
     parameter of this
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.537.1">
      function decorator:
     </span>
    </span>
   </p>
   <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.538.1">@dlt.table(</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.539.1">    comment="The raw NYC taxi cab trip dataset located in `/databricks-datasets/`"</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.540.1">)</span></strong><span class="koboSpan" id="kobo.541.1">
def yellow_taxi_raw():
    path = "/databricks-datasets/nyctaxi/tripdata/yellow"
    return (spark.readStream
        .schema(schema)
        .format("csv")
        .option("header", True)
        .load(path))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.542.1">
     After executing the
    </span>
    <a id="_idIndexMarker070">
    </a>
    <span class="koboSpan" id="kobo.543.1">
     notebook cell, the Databricks Data Intelligence Platform will detect a DLT table, print the output schema, and prompt you to create a new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.544.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer015">
     <span class="koboSpan" id="kobo.545.1">
      <img alt="Figure 1.8 – Databricks will parse the DLT table declaration and print the output schema" src="image/B22011_01_008.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.546.1">
     Figure 1.8 – Databricks will parse the DLT table declaration and print the output schema
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.547.1">
     Let’s click
    </span>
    <a id="_idIndexMarker071">
    </a>
    <span class="koboSpan" id="kobo.548.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.549.1">
      Create Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.550.1">
     button to generate a new DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.550.2">
     Give the data pipeline a meaningful name, such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.551.1">
      Yellow Taxi Cab Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.552.1">
     .
    </span>
    <span class="koboSpan" id="kobo.552.2">
     Select
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.553.1">
      Core
     </span>
    </strong>
    <span class="koboSpan" id="kobo.554.1">
     as the product edition and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.555.1">
      Triggered
     </span>
    </strong>
    <span class="koboSpan" id="kobo.556.1">
     as the pipeline
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.557.1">
      execution mode.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer016">
     <span class="koboSpan" id="kobo.558.1">
      <img alt="Figure 1.9 – Create a new DLT pipeline using the Core product edition" src="image/B22011_01_009.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.559.1">
     Figure 1.9 – Create a new DLT pipeline using the Core product edition
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.560.1">
     Next, under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.561.1">
      Target Location
     </span>
    </strong>
    <span class="koboSpan" id="kobo.562.1">
     settings, select the Unity Catalog radio button, and specify the target catalog and schema where you would like to store the dataset.
    </span>
    <span class="koboSpan" id="kobo.562.2">
     Under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.563.1">
      Compute
     </span>
    </strong>
    <span class="koboSpan" id="kobo.564.1">
     settings, set
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.565.1">
      Min workers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.566.1">
     to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.567.1">
      1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.568.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.569.1">
      Max workers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.570.1">
     to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.571.1">
      1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.572.1">
     .
    </span>
    <span class="koboSpan" id="kobo.572.2">
     Then, accept the defaults by clicking the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.573.1">
      Create
     </span>
    </strong>
    <span class="koboSpan" id="kobo.574.1">
     button.
    </span>
    <span class="koboSpan" id="kobo.574.2">
     Finally, click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.575.1">
      Start
     </span>
    </strong>
    <span class="koboSpan" id="kobo.576.1">
     button to execute the data pipeline.
    </span>
    <span class="koboSpan" id="kobo.576.2">
     You will be
    </span>
    <a id="_idIndexMarker072">
    </a>
    <span class="koboSpan" id="kobo.577.1">
     taken to a visual representation of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.578.1">
      dataflow graph.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer017">
     <span class="koboSpan" id="kobo.579.1">
      <img alt="Figure 1.10 – The dataflow graph will contain the streaming table we declared in our notebook" src="image/B22011_01_010.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.580.1">
     Figure 1.10 – The dataflow graph will contain the streaming table we declared in our notebook
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.581.1">
     As you can see, our dataflow graph consists of a single streaming table, which is a new dataset that will ingest raw NYC taxi trip data from the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.582.1">
      /databricks-datasets/
     </span>
    </strong>
    <a id="_idTextAnchor050">
    </a>
    <span class="koboSpan" id="kobo.583.1">
     location on the Databricks FileSystem.
    </span>
    <span class="koboSpan" id="kobo.583.2">
     While a trivial example, this example shows the declarative nature of DLT framework, as well as how quickly we can declare a data pipeline using the familiar PySpark API.
    </span>
    <span class="koboSpan" id="kobo.583.3">
     Furthermore, you should now have a feel for how we can monitor and
    </span>
    <a id="_idIndexMarker073">
    </a>
    <span class="koboSpan" id="kobo.584.1">
     view the latest state of our data pipeline from the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.585.1">
      DLT UI.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-44">
    <a id="_idTextAnchor051">
    </a>
    <span class="koboSpan" id="kobo.586.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.587.1">
     In this chapter, we examined how and why the data industry has settled on a lakehouse architecture, which aims to merge the scalability of ETL processing and the fast data warehousing speeds for BI workloads under a single, unified architecture.
    </span>
    <span class="koboSpan" id="kobo.587.2">
     We learned how real-time data processing is essential to uncovering value from the latest data as soon as it arrives, but real-time data pipelines can halt the productivity of data engineering teams as complexity grows over time.
    </span>
    <span class="koboSpan" id="kobo.587.3">
     Finally, we learned the core concepts of the Delta Live Tables framework and how, with just a few lines of PySpark code and function decorators, we can quickly declare a real-time data pipeline that is capable of incrementally processing data with high throughput and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.588.1">
      low latency.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.589.1">
     In the next chapter, we’ll take a deep dive into the advanced settings of Delta Live Tables pipelines and how the framework will optimize the underlying datasets for us.
    </span>
    <span class="koboSpan" id="kobo.589.2">
     Then, we’ll look at more advanced data transformations, using a real-world use case to develop a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.590.1">
      data pipeline.
     </span>
    </span>
   </p>
  </div>
 </body></html>