<html><head></head><body>
  <div id="_idContainer026">
   <h1 class="chapter-number" id="_idParaDest-45">
    <a id="_idTextAnchor052">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     2
    </span>
   </h1>
   <h1 id="_idParaDest-46">
    <a id="_idTextAnchor053">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Applying Data Transformations Using Delta Live Tables
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In this chapter, we’ll dive straight into how
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      Delta Live Tables
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      DLT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     ) makes ingesting data from a variety of input sources simple and straightforward, whether it’s files landing in cloud storage or connecting to an external storage system, such as a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.8.1">
      relational database management system
     </span>
    </strong>
    <span class="koboSpan" id="kobo.9.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.10.1">
      RDBMS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.11.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.11.2">
     Then, we’ll take a look at how we can efficiently and accurately apply changes from our input data sources to downstream datasets, using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.12.1">
      APPLY CHANGES
     </span>
    </strong>
    <span class="koboSpan" id="kobo.13.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.13.2">
     Lastly, we’ll conclude the chapter with a deep dive into the advanced data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.14.1">
      pipeline settings.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.15.1">
     To summarize, in this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.16.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.17.1">
      Ingesting data from
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.18.1">
       input sources
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.19.1">
      Applying changes to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.20.1">
       downstream tables
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.21.1">
      Publishing datasets to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.22.1">
       Unity Catalog
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.23.1">
      Data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.24.1">
       pipeline settings
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.25.1">
      Hands-on exercise – applying SCD Type
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.26.1">
       2 changes
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-47">
    <a id="_idTextAnchor054">
    </a>
    <span class="koboSpan" id="kobo.27.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.28.1">
     To follow along in this chapter, it’s recommended to have Databricks workspace permissions to create an all-purpose cluster and a DLT pipeline, using a cluster policy.
    </span>
    <span class="koboSpan" id="kobo.28.2">
     It’s also recommended to have Unity Catalog permissions to create and use catalogs, schemas, and tables.
    </span>
    <span class="koboSpan" id="kobo.28.3">
     All code samples can be downloaded from the chapter’s GitHub repository, located at https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter02.
    </span>
    <span class="koboSpan" id="kobo.28.4">
     This chapter will create and run several new notebooks and a DLT pipeline using the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.29.1">
      Core
     </span>
    </strong>
    <span class="koboSpan" id="kobo.30.1">
     product edition.
    </span>
    <span class="koboSpan" id="kobo.30.2">
     As a result, the pipelines are estimated to consume around 10–15
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.31.1">
      Databricks
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.32.1">
       Units
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.34.1">
       DBUs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.35.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-48">
    <a id="_idTextAnchor055">
    </a>
    <span class="koboSpan" id="kobo.36.1">
     Ingesting data from input sources
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.37.1">
     DLT makes ingesting data from a variety of input sources simple.
    </span>
    <span class="koboSpan" id="kobo.37.2">
     For example, DLT can efficiently process new
    </span>
    <a id="_idIndexMarker074">
    </a>
    <span class="koboSpan" id="kobo.38.1">
     files landing in a cloud storage location throughout the day, ingest structured data by connecting to an external storage system, such as a relational database, or read static reference tables that can be cached into memory.
    </span>
    <span class="koboSpan" id="kobo.38.2">
     Let’s look at how we can use DLT to incrementally ingest new data that arrives in a cloud
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.39.1">
      storage location.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-49">
    <a id="_idTextAnchor056">
    </a>
    <span class="koboSpan" id="kobo.40.1">
     Ingesting data using Databricks Auto Loader
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.41.1">
     One of the key features of the Databricks Data Intelligence Platform is a feature called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.42.1">
      Auto Loader
     </span>
    </strong>
    <span class="koboSpan" id="kobo.43.1">
     , which is a simple yet powerful ingestion
    </span>
    <a id="_idIndexMarker075">
    </a>
    <span class="koboSpan" id="kobo.44.1">
     mechanism for efficiently reading
    </span>
    <a id="_idIndexMarker076">
    </a>
    <span class="koboSpan" id="kobo.45.1">
     input files from cloud storage.
    </span>
    <span class="koboSpan" id="kobo.45.2">
     Auto Loader can be referenced in a DataFrame definition by using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.46.1">
      cloudFiles
     </span>
    </strong>
    <span class="koboSpan" id="kobo.47.1">
     data source.
    </span>
    <span class="koboSpan" id="kobo.47.2">
     For example, the following code snippet will use the Databricks Auto Loader feature to ingest newly arriving JSON files from a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.48.1">
      storage container:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.49.1">
df = (spark.readStream
      .format("cloudFiles")
      .option("cloudFiles.format", "json")
      .option("cloudFiles.schemaLocation", schema_path)
      .load(raw_data_path))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.50.1">
     Auto Loader can scale to process billions of files in cloud storage efficiently.
    </span>
    <span class="koboSpan" id="kobo.50.2">
     Databricks Auto Loader supports ingesting files stored in the CSV, JSON, XML, Apache Parquet, Apache Avro, and Apache Orc file formats, as well as text and binary files.
    </span>
    <span class="koboSpan" id="kobo.50.3">
     Furthermore, one thing that you may have noticed in the preceding code snippet is that a schema definition was not specified for the input stream but, rather, a target schema location.
    </span>
    <span class="koboSpan" id="kobo.50.4">
     That is because Auto Loader will automatically infer the data source schema and keep track of the changes to the schema definition in a separate storage location.
    </span>
    <span class="koboSpan" id="kobo.50.5">
     Behind the scenes, Auto Loader will sample up to the first 1,000 cloud file objects to infer the schema structure for a cloud file source.
    </span>
    <span class="koboSpan" id="kobo.50.6">
     For semi-structured formats such as JSON, where the schema can change
    </span>
    <a id="_idIndexMarker077">
    </a>
    <span class="koboSpan" id="kobo.51.1">
     over time, this can alleviate a huge burden on data engineering teams by them not having to maintain an up-to-date definition of the latest
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.52.1">
      schema definition.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-50">
    <a id="_idTextAnchor057">
    </a>
    <span class="koboSpan" id="kobo.53.1">
     Scalability challenge in structured streaming
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.54.1">
     Traditionally, data pipelines that used Spark Structured Streaming to ingest new files, where files were appended to a cloud storage location, struggled to scale as data volumes grew into GB or even TB.
    </span>
    <span class="koboSpan" id="kobo.54.2">
     As new files were written to the cloud storage container, Structured Streaming would perform a directory listing.
    </span>
    <span class="koboSpan" id="kobo.54.3">
     For large datasets, (i.e., datasets comprised
    </span>
    <a id="_idIndexMarker078">
    </a>
    <span class="koboSpan" id="kobo.55.1">
     of millions of files or more), the directory listing process alone would take a lengthy amount of time.
    </span>
    <span class="koboSpan" id="kobo.55.2">
     In addition, the cloud provider would assess API fees for these directory listing calls, adding to the overall cloud provider fees.
    </span>
    <span class="koboSpan" id="kobo.55.3">
     For files that have already been processed, this directory listing was both expensive
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.56.1">
      and inefficient.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.57.1">
     Databricks Auto Loader supports two types of cloud file detection modes – notification mode and legacy directory listing mode.
    </span>
    <span class="koboSpan" id="kobo.57.2">
     In notification mode, Auto Loader bypasses this expensive directory listing process entirely by automatically deploying a more scalable architecture under the hood.
    </span>
    <span class="koboSpan" id="kobo.57.3">
     With just a few lines of Python code, Databricks pre-provisions backend cloud services that will automatically keep track of new files that have landed in cloud storage, as well as files that have already
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.58.1">
      been processed.
     </span>
    </span>
   </p>
   <p class="IMG---Figure">
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer019">
     <span class="koboSpan" id="kobo.59.1">
      <img alt="Figure 2.1 – In notification mode, Databricks Auto Loader uses an event stream to keep track of new, unprocessed files in cloud storage" src="image/B22011_02_001.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.60.1">
     Figure 2.1 – In notification mode, Databricks Auto Loader uses an event stream to keep track of new, unprocessed files in cloud storage
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.61.1">
     Let’s walk through an example of how the Auto Loader feature, configured in notification mode, will efficiently process newly arriving cloud file
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.62.1">
      objects together:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.63.1">
      The process begins with the Databricks Auto Loader listening to a particular cloud storage path for new file object creation events, also referred to as
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.64.1">
       PUT
      </span>
     </strong>
     <span class="koboSpan" id="kobo.65.1">
      events, named
     </span>
     <a id="_idIndexMarker079">
     </a>
     <span class="koboSpan" id="kobo.66.1">
      after the HTTP verb used to create
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.67.1">
       the object.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.68.1">
      When a new file object has been created, the metadata about this new file is persisted to a key-value store, which serves as a checkpoint location if there are
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.69.1">
       system failures.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.70.1">
      Next, the information pertaining to the file object, or file objects, will then be published to an event stream that the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.71.1">
       cloudFiles
      </span>
     </strong>
     <span class="koboSpan" id="kobo.72.1">
      data source
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.73.1">
       reads from.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.74.1">
      Upon reading from the event stream, the Auto Loader process in Databricks will fetch the data pertaining only to those new, unprocessed file objects in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.75.1">
       cloud storage.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.76.1">
      Lastly, the Auto Loader process will update the key-value store, marking the new files as processed in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.77.1">
       the system.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.78.1">
     This implementation of notification-based file processing avoids the expensive and inefficient directory listing process, ensuring that the process can recover from failures and that files are
    </span>
    <a id="_idIndexMarker080">
    </a>
    <span class="koboSpan" id="kobo.79.1">
     processed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.80.1">
      exactly once.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-51">
    <a id="_idTextAnchor058">
    </a>
    <span class="koboSpan" id="kobo.81.1">
     Using Auto Loader with DLT
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.82.1">
     Databricks Auto Loader can be used to create a streaming table in a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.82.2">
     Now that we know what’s going on behind the scenes, building a robust, scalable streaming table that can scale to
    </span>
    <a id="_idIndexMarker081">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     billions of files can be done with just a few lines of Python code.
    </span>
    <span class="koboSpan" id="kobo.83.2">
     In fact, for data sources that append new files to cloud
    </span>
    <a id="_idIndexMarker082">
    </a>
    <span class="koboSpan" id="kobo.84.1">
     storage, it’s recommended to always use Auto Loader to ingest data.
    </span>
    <span class="koboSpan" id="kobo.84.2">
     Let’s take a streaming DataFrame definition from the preceding section and combine it with the DLT dataset annotation to define a new data stream in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.85.1">
      our pipeline:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.86.1">
@dlt.table(
    comment="Raw cloud files stream of completed taxi trips"
)
def yellow_taxi_events_raw():
    return (spark.readStream
            .format("cloudFiles")
            .option("cloudFiles.format", "json")
            .option("cloudFiles.path", schema_path)
            .load(raw_landing_zone_path))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.87.1">
     One thing to note is that in the preceding code snippet, we’ve provided two cloud storage paths.
    </span>
    <span class="koboSpan" id="kobo.87.2">
     The first storage path,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.88.1">
      schema_path
     </span>
    </strong>
    <span class="koboSpan" id="kobo.89.1">
     , refers to the cloud storage path where schema information and the key-value store will be written.
    </span>
    <span class="koboSpan" id="kobo.89.2">
     The second storage location,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.90.1">
      raw_landing_zone_path
     </span>
    </strong>
    <span class="koboSpan" id="kobo.91.1">
     , points to the location where new, unprocessed files will be written by the external
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.92.1">
      data source.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.93.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.94.1">
     It’s recommended to use an external location governed by Unity Catalog so that you can enforce fine-grained data access across different users and groups within your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.95.1">
      Databricks workspace.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.96.1">
     Now that we have a reliable and efficient way of ingesting raw data from cloud storage input sources, we’ll want
    </span>
    <a id="_idIndexMarker083">
    </a>
    <span class="koboSpan" id="kobo.97.1">
     to transform the data and apply the output
    </span>
    <a id="_idIndexMarker084">
    </a>
    <span class="koboSpan" id="kobo.98.1">
     to downstream datasets in
    </span>
    <a id="_idIndexMarker085">
    </a>
    <span class="koboSpan" id="kobo.99.1">
     our data pipeline.
    </span>
    <span class="koboSpan" id="kobo.99.2">
     Let’s look at how the DLT framework makes applying downstream changes simple
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.100.1">
      and straightforward.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-52">
    <a id="_idTextAnchor059">
    </a>
    <span class="koboSpan" id="kobo.101.1">
     Applying changes to downstream tables
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.102.1">
     Traditionally, Delta Lake offered a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.103.1">
      MERGE INTO
     </span>
    </strong>
    <span class="koboSpan" id="kobo.104.1">
     command, allowing change data capture to be merged into target tables by matching on a particular condition.
    </span>
    <span class="koboSpan" id="kobo.104.2">
     However, if the new data happened to
    </span>
    <a id="_idIndexMarker086">
    </a>
    <span class="koboSpan" id="kobo.105.1">
     be out of order, the merged changes would result in incorrect results, leading to an inaccurate and misleading output.
    </span>
    <span class="koboSpan" id="kobo.105.2">
     To remediate this problem, data engineering teams would need to build complex reconciliation processes to handle out-of-order data, adding yet another layer to a data pipeline to manage
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.106.1">
      and maintain.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-53">
    <a id="_idTextAnchor060">
    </a>
    <span class="koboSpan" id="kobo.107.1">
     APPLY CHANGES command
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.108.1">
     DLT offers a new API to automatically apply changes to downstream tables, even handling out-of-order data based on a set of one
    </span>
    <a id="_idIndexMarker087">
    </a>
    <span class="koboSpan" id="kobo.109.1">
     or more sequence columns.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.110.1">
      Slowly
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.111.1">
      c
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.112.1">
      hanging
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.113.1">
      d
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.114.1">
      imensions
     </span>
    </strong>
    <span class="koboSpan" id="kobo.115.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.116.1">
      SCDs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.117.1">
     ) are dimensions in traditional
    </span>
    <a id="_idIndexMarker088">
    </a>
    <span class="koboSpan" id="kobo.118.1">
     data warehousing that allow the current and historical snapshot of data to be tracked over time.
    </span>
    <span class="koboSpan" id="kobo.118.2">
     DLT allows data engineering teams to
    </span>
    <a id="_idIndexMarker089">
    </a>
    <span class="koboSpan" id="kobo.119.1">
     update downstream datasets in a data pipeline with changes in the upstream data source.
    </span>
    <span class="koboSpan" id="kobo.119.2">
     For example, DLT allows users to capture SCD Type 1 (which does not preserve previous row history) and SCD Type 2 (which preserves historical versions
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.120.1">
      of rows).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.121.1">
     DLT offers a Python API as well as SQL syntax to apply change
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.122.1">
      data captures:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.123.1">
       APPLY CHANGES
      </span>
     </strong>
     <span class="koboSpan" id="kobo.124.1">
      – for pipelines written using
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.125.1">
       SQL syntax
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.126.1">
       apply_changes()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.127.1">
      – for pipelines written
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.128.1">
       using Python
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.129.1">
     Let’s imagine that we have a table that will record room temperatures published from smart thermostats throughout the day, and it’s important to preserve a history of temperature updates.
    </span>
    <span class="koboSpan" id="kobo.129.2">
     The
    </span>
    <a id="_idIndexMarker090">
    </a>
    <span class="koboSpan" id="kobo.130.1">
     following code
    </span>
    <a id="_idIndexMarker091">
    </a>
    <span class="koboSpan" id="kobo.131.1">
     snippet will apply SCD Type 2 changes to an output table in our data pipeline, using the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.132.1">
       apply_changes()
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.133.1">
      API:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.134.1">
import dlt 
import pyspark.sql.functions as F
dlt.create_streaming_table("iot_device_temperatures")
dlt.apply_changes(
    target = "iot_device_temperatures",
    source = "smart_thermostats",
    keys = ["device_id"],
    sequence_by = F.col("sequence_num"),
    apply_as_deletes = F.expr("operation = 'DELETE'"),
    except_column_list = ["operation", "sequence_num"],
    stored_as_scd_type = "2"
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.135.1">
     Furthermore, DLT will capture high-level operational metrics about the data changes that are applied during the completion of an
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.136.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.137.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.137.2">
     For instance, the DLT system will track the number of rows that were updated, inserted, or deleted for each execution of the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.138.1">
       apply_changes()
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.139.1">
      command.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-54">
    <a id="_idTextAnchor061">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     The DLT reconciliation process
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.141.1">
     Behind the scenes, DLT will create two dataset objects to accurately apply table changes to pipeline datasets.
    </span>
    <span class="koboSpan" id="kobo.141.2">
     The first data object is a hidden, backend Delta table that contains the full history of changes.
    </span>
    <span class="koboSpan" id="kobo.141.3">
     This
    </span>
    <a id="_idIndexMarker092">
    </a>
    <span class="koboSpan" id="kobo.142.1">
     dataset is used to perform a reconciliation process that is capable of handling out-of-order row updates that are
    </span>
    <a id="_idIndexMarker093">
    </a>
    <span class="koboSpan" id="kobo.143.1">
     processed.
    </span>
    <span class="koboSpan" id="kobo.143.2">
     Furthermore, this backend table will be named using the provided name parameter in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.144.1">
      APPLY CHANGES
     </span>
    </strong>
    <span class="koboSpan" id="kobo.145.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.146.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.147.1">
     function call, concatenated with the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.148.1">
       __apply_changes_storage_
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.149.1">
      string.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.150.1">
     For example, if the name of the table was
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.151.1">
      iot_readings
     </span>
    </strong>
    <span class="koboSpan" id="kobo.152.1">
     , it would result in a backend table being created with the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.153.1">
      name
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.154.1">
       __apply_changes_storage_iot_readings
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.155.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.156.1">
     This particular table will only be visible if the DLT pipeline publishes the dataset to the legacy Hive Metastore.
    </span>
    <span class="koboSpan" id="kobo.156.2">
     However, Unity Catalog will abstract these low-level details away from end users, and the dataset will not be visible from the Catalog Explorer UI.
    </span>
    <span class="koboSpan" id="kobo.156.3">
     However, the table can
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.157.1">
      still
     </span>
    </em>
    <span class="koboSpan" id="kobo.158.1">
     be queried using a notebook or from a query executed on a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.159.1">
      SQL warehouse.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.160.1">
     Secondly, the DLT system will create another dataset – a view using the name provided for the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.161.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.162.1">
     function.
    </span>
    <span class="koboSpan" id="kobo.162.2">
     This view will contain the latest snapshot of a table with all the changes applied.
    </span>
    <span class="koboSpan" id="kobo.162.3">
     The view will use a column, or combination of columns, specified as table keys to uniquely identify each row within the backend table.
    </span>
    <span class="koboSpan" id="kobo.162.4">
     Then, DLT uses the column or sequence of columns specified in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.163.1">
      sequence_by
     </span>
    </strong>
    <span class="koboSpan" id="kobo.164.1">
     parameter of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.165.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.166.1">
     function to order the table changes for each unique row, picking out the latest row change to calculate the result set for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.167.1">
      the view.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer020">
     <span class="koboSpan" id="kobo.168.1">
      <img alt="Figure 2.2 – DLT creates a backend table to apply table changes, as well as a view to query the latest data" src="image/B22011_02_002.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.169.1">
     Figure 2.2 – DLT creates a backend table to apply table changes, as well as a view to query the latest data
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.170.1">
     As you can see, DLT makes
    </span>
    <a id="_idIndexMarker094">
    </a>
    <span class="koboSpan" id="kobo.171.1">
     it extremely simple to keep downstream data sources in line with the data changes occurring in the source.
    </span>
    <span class="koboSpan" id="kobo.171.2">
     With just a few parameter changes, you can use the powerful
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.172.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.173.1">
     API to apply
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.174.1">
      SCD data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.175.1">
     Now that we understand how
    </span>
    <a id="_idIndexMarker095">
    </a>
    <span class="koboSpan" id="kobo.176.1">
     we can leverage the DLT framework to define data transformations and apply changes to downstream tables, let’s turn our attention to how we can add strong data governance on top of our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.177.1">
      pipeline datasets.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-55">
    <a id="_idTextAnchor062">
    </a>
    <span class="koboSpan" id="kobo.178.1">
     Publishing datasets to Unity Catalog
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.179.1">
     DLT offers two methods for storing datasets in the Databricks Data Intelligence Platform – the legacy Hive Metastore and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.180.1">
      Unity Catalog.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.181.1">
     As described in
    </span>
    <a href="B22011_01.xhtml#_idTextAnchor014">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.182.1">
        Chapter 1
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.183.1">
     , Unity Catalog is a centralized governance store that spans all of your Databricks workspaces
    </span>
    <a id="_idIndexMarker096">
    </a>
    <span class="koboSpan" id="kobo.184.1">
     within a particular global region.
    </span>
    <span class="koboSpan" id="kobo.184.2">
     As a result, data access policies can be defined once in a centralized location and will be consistently applied across
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.185.1">
      your organization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.186.1">
     However, within the context of a DLT pipeline, these two methods of storing the output datasets are mutually exclusive to one another – that is, a particular DLT pipeline cannot store some datasets in the Unity Catalog and others in the Hive Metastore.
    </span>
    <span class="koboSpan" id="kobo.186.2">
     You must choose a single metastore location for the entire data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.187.1">
      pipeline output.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-56">
    <a id="_idTextAnchor063">
    </a>
    <span class="koboSpan" id="kobo.188.1">
     Why store datasets in Unity Catalog?
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.189.1">
     Unity Catalog is the new best-of-breed method for storing data and querying datasets in the lakehouse.
    </span>
    <span class="koboSpan" id="kobo.189.2">
     You might choose landing data in a data pipeline into Unity Catalog over the Hive Metastore for several
    </span>
    <a id="_idIndexMarker097">
    </a>
    <span class="koboSpan" id="kobo.190.1">
     reasons, including
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.191.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.192.1">
      The data is secured
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.193.1">
       by default
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.194.1">
       .
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.195.1">
      There is a consistent definition of access policies across groups and users versus defining data access policies for every
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.196.1">
       individual workspace
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.197.1">
       .
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.198.1">
      Open source technology with no risk of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.199.1">
       vendor lock-in
      </span>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.200.1">
       .
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.201.1">
     Furthermore, Unity Catalog offers a Hive-compatible API, allowing third-party tools to integrate with a Unity Catalog metastore as if it were the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.202.1">
      Hive Metastore.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-57">
    <a id="_idTextAnchor064">
    </a>
    <span class="koboSpan" id="kobo.203.1">
     Creating a new catalog
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.204.1">
     One major difference
    </span>
    <a id="_idIndexMarker098">
    </a>
    <span class="koboSpan" id="kobo.205.1">
     between Unity Catalog and the Hive Metastore is that the former introduces a three-level namespace when defining tables.
    </span>
    <span class="koboSpan" id="kobo.205.2">
     The parent namespace will refer to the catalog object.
    </span>
    <span class="koboSpan" id="kobo.205.3">
     A catalog is a logical container that will hold one to many schemas,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.206.1">
      or databases.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     One of the first steps in building a new DLT pipeline is to define a centralized location to store the output datasets.
    </span>
    <span class="koboSpan" id="kobo.207.2">
     Creating a new catalog in Unity Catalog is simple.
    </span>
    <span class="koboSpan" id="kobo.207.3">
     It can be done using a variety of methods, such as through the Catalog Explorer UI, using SQL statements executed from within a notebook, or using the Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.208.1">
      REST API.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.209.1">
     We’ll use the Databricks
    </span>
    <a id="_idIndexMarker099">
    </a>
    <span class="koboSpan" id="kobo.210.1">
     Catalog Explorer UI to create a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.211.1">
      new Catalog:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.212.1">
      First, navigate to the Catalog Explorer by clicking on the
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.213.1">
       Catalog Explorer
      </span>
     </strong>
     <span class="koboSpan" id="kobo.214.1">
      tab in the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.215.1">
       navigation sidebar.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.216.1">
      Next, click the
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.217.1">
       Create
      </span>
     </strong>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.218.1">
        Catalog
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.219.1">
       button.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.220.1">
      Give the catalog a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.221.1">
       meaningful name.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.222.1">
      Select
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.223.1">
       Standard
      </span>
     </strong>
     <span class="koboSpan" id="kobo.224.1">
      as the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.225.1">
       catalog type.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.226.1">
      Finally, click on the
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.227.1">
       Create
      </span>
     </strong>
     <span class="koboSpan" id="kobo.228.1">
      button to create the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.229.1">
       new catalog.
      </span>
     </span>
    </li>
   </ol>
   <h2 id="_idParaDest-58">
    <a id="_idTextAnchor065">
    </a>
    <span class="koboSpan" id="kobo.230.1">
     Assigning catalog permissions
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.231.1">
     As previously mentioned, one
    </span>
    <a id="_idIndexMarker100">
    </a>
    <span class="koboSpan" id="kobo.232.1">
     of the benefits of using Unity Catalog is that your data is secured by default.
    </span>
    <span class="koboSpan" id="kobo.232.2">
     In other words, access to data stored in the Unity Catalog is denied by default unless explicitly permitted.
    </span>
    <span class="koboSpan" id="kobo.232.3">
     To create new tables in the newly created catalog, we’ll need to grant permission to create and manipulate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.233.1">
      new tables.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.234.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.235.1">
     If you are the creator and owner of a target catalog and schema objects, as well as the creator and owner of a DLT pipeline, then you do not need to execute the following
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.236.1">
      GRANT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.237.1">
     statements.
    </span>
    <span class="koboSpan" id="kobo.237.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.238.1">
      GRANT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.239.1">
     statements are meant to demonstrate the types of permissions needed to share data assets across multiple groups and users in a typical Unity
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.240.1">
      Catalog metastore.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.241.1">
     First, let’s grant access to use the catalog.
    </span>
    <span class="koboSpan" id="kobo.241.2">
     From a new notebook, execute the following SQL syntax to grant access to use the newly created catalog, where
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.242.1">
      my_user
     </span>
    </strong>
    <span class="koboSpan" id="kobo.243.1">
     is the name of a Databricks user and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.244.1">
      chp2_transforming_data
     </span>
    </strong>
    <span class="koboSpan" id="kobo.245.1">
     is the name of the catalog created in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.246.1">
      previous example:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.247.1">
%sql
GRANT USE CATALOG, CREATE SCHEMA ON CATALOG `chp2_transforming_data` TO `my_user`;</span></pre>
   <p>
    <span class="koboSpan" id="kobo.248.1">
     Next, we’ll need to create a
    </span>
    <a id="_idIndexMarker101">
    </a>
    <span class="koboSpan" id="kobo.249.1">
     schema that will hold the output datasets from our DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.249.2">
     From the same notebook, execute the following SQL statement to create a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.250.1">
      new schema:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.251.1">
%sql
USE CATALOG `chp2_transforming_data`;
CREATE SCHEMA IF NOT EXISTS `ride_hailing`;
USE SCHEMA `ride_hailing`;</span></pre>
   <p>
    <span class="koboSpan" id="kobo.252.1">
     Execute the following statement to grant permission to create materialized views within the newly
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.253.1">
      created schema:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.254.1">
%sql
GRANT USE SCHEMA, CREATE TABLE, CREATE MATERIALIZED VIEW ON SCHEMA `ride_hailing` TO `my_user`;</span></pre>
   <p>
    <span class="koboSpan" id="kobo.255.1">
     By now, you should see how simple yet powerful Unity Catalog makes applying consistent data security to your data pipeline datasets, providing data stewards with a variety of options to enforce dataset permissions across their organization.
    </span>
    <span class="koboSpan" id="kobo.255.2">
     Let’s turn our attention to how we can configure some of the advanced features and settings of a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.256.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-59">
    <a id="_idTextAnchor066">
    </a>
    <span class="koboSpan" id="kobo.257.1">
     Data pipeline settings
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.258.1">
     Up until now, we’ve only discussed how to use the DLT framework to declare tables, views, and transformations on the arriving data.
    </span>
    <span class="koboSpan" id="kobo.258.2">
     However, the computational resources that execute a particular
    </span>
    <a id="_idIndexMarker102">
    </a>
    <span class="koboSpan" id="kobo.259.1">
     data pipeline also play a major role in landing the latest data in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.260.1">
      a
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.261.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.262.1">
      akehouse.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.263.1">
     In this section, we’re going to discuss the different data pipeline settings and how you can control computational resources, such as the cluster,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.264.1">
      at runtime.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.265.1">
     The following pipeline settings can be configured directly from the DLT UI or using the Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.266.1">
      REST API.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-60">
    <a id="_idTextAnchor067">
    </a>
    <span class="koboSpan" id="kobo.267.1">
     The DLT product edition
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.268.1">
     The data pipeline product edition tells the DLT framework what set of features your data pipeline will utilize.
    </span>
    <span class="koboSpan" id="kobo.268.2">
     Higher product editions will contain more features, and as a result, Databricks will assess a
    </span>
    <a id="_idIndexMarker103">
    </a>
    <span class="koboSpan" id="kobo.269.1">
     higher price (
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.270.1">
      a DBU).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.271.1">
     Databricks offers three types of product editions for DLT pipelines, ranked in order of feature set, from the least
    </span>
    <a id="_idIndexMarker104">
    </a>
    <span class="koboSpan" id="kobo.272.1">
     features to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.273.1">
      the most:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.274.1">
       Core
      </span>
     </strong>
     <span class="koboSpan" id="kobo.275.1">
      :
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.276.1">
       Core
      </span>
     </strong>
     <span class="koboSpan" id="kobo.277.1">
      is the base product edition.
     </span>
     <span class="koboSpan" id="kobo.277.2">
      This product edition is meant for streaming workloads that only append
     </span>
     <a id="_idIndexMarker105">
     </a>
     <span class="koboSpan" id="kobo.278.1">
      new data to streaming tables.
     </span>
     <span class="koboSpan" id="kobo.278.2">
      Data expectations (data quality enforcement is discussed in the next chapter) and utilities to apply change data capture are not available in this
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.279.1">
       product edition.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.280.1">
       Pro
      </span>
     </strong>
     <span class="koboSpan" id="kobo.281.1">
      : The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.282.1">
       Pro
      </span>
     </strong>
     <span class="koboSpan" id="kobo.283.1">
      product edition is the next edition above Core.
     </span>
     <span class="koboSpan" id="kobo.283.2">
      This production edition is designed for streaming
     </span>
     <a id="_idIndexMarker106">
     </a>
     <span class="koboSpan" id="kobo.284.1">
      workloads that append new data to streaming tables and apply updates and deletes using
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.285.1">
       APPLY
      </span>
     </strong>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.286.1">
       CHANGES
      </span>
     </strong>
     <span class="koboSpan" id="kobo.287.1">
      command.
     </span>
     <span class="koboSpan" id="kobo.287.2">
      However, data quality expectations are not available in this
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.288.1">
       product edition.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.289.1">
       Advanced
      </span>
     </strong>
     <span class="koboSpan" id="kobo.290.1">
      : The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.291.1">
       Advanced
      </span>
     </strong>
     <span class="koboSpan" id="kobo.292.1">
      product edition is the most feature-rich product edition.
     </span>
     <span class="koboSpan" id="kobo.292.2">
      Data quality
     </span>
     <a id="_idIndexMarker107">
     </a>
     <span class="koboSpan" id="kobo.293.1">
      expectations are available in this production edition, as well as support for appending new data to streaming tables and applying inserts, updates, and deletes that have occurred in the upstream
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.294.1">
       data sources.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.295.1">
     There may be times when your requirements change over time.
    </span>
    <span class="koboSpan" id="kobo.295.2">
     For example, you might require strict data quality enforcement to prevent downstream failures in third-party
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.296.1">
      business intelligence
     </span>
    </strong>
    <span class="koboSpan" id="kobo.297.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.298.1">
      BI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.299.1">
     ) reporting
    </span>
    <a id="_idIndexMarker108">
    </a>
    <span class="koboSpan" id="kobo.300.1">
     tools.
    </span>
    <span class="koboSpan" id="kobo.300.2">
     In scenarios like these, you can update the product edition of an existing DLT pipeline at any time, allowing your data pipeline to adapt to changes in your feature requirements
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.301.1">
      and budget.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-61">
    <a id="_idTextAnchor068">
    </a>
    <span class="koboSpan" id="kobo.302.1">
     Pipeline execution mode
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.303.1">
     DLT offers a way to inform a system that the changes to a data pipeline are experimental.
    </span>
    <span class="koboSpan" id="kobo.303.2">
     This feature is called data pipeline environment mode.
    </span>
    <span class="koboSpan" id="kobo.303.3">
     There are two available environment
    </span>
    <a id="_idIndexMarker109">
    </a>
    <span class="koboSpan" id="kobo.304.1">
     modes –
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.305.1">
      development
     </span>
    </strong>
    <span class="koboSpan" id="kobo.306.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.307.1">
      production
     </span>
    </strong>
    <span class="koboSpan" id="kobo.308.1">
     .
    </span>
    <span class="koboSpan" id="kobo.308.2">
     The main difference is the behavior of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.309.1">
      computational resource.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.310.1">
     In development
    </span>
    <a id="_idIndexMarker110">
    </a>
    <span class="koboSpan" id="kobo.311.1">
     environment mode, a data flow task will not be automatically retried if a failure is encountered.
    </span>
    <span class="koboSpan" id="kobo.311.2">
     This allows a data engineer to intervene and correct any programmatic errors during ad hoc
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.312.1">
      development cycles.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.313.1">
     Furthermore, during a failure in development mode, the cluster executing the data pipeline updates will remain up.
    </span>
    <span class="koboSpan" id="kobo.313.2">
     This allows a data engineer to view the driver logs and cluster metrics of the cluster, and it also prevents a lengthy cluster re-provisioning and re-initialization of the cluster runtime for each pipeline execution, which, depending upon the cloud provider, could take 10 to 15 minutes to complete.
    </span>
    <span class="koboSpan" id="kobo.313.3">
     It’s expected to have short and iterative development and testing cycles, which assist data engineers in their development life cycle by keeping the cluster up
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.314.1">
      and running.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.315.1">
     The data pipeline environment mode can be set from the DLT UI by clicking the environment mode toggle switch at the very top navigation bar of a data pipeline in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.316.1">
      DLT UI.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer021">
     <span class="koboSpan" id="kobo.317.1">
      <img alt="Figure 2.3 – DLT pipeline execution mode can be set using the toggle switch from the UI" src="image/B22011_02_003.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.318.1">
     Figure 2.3 – DLT pipeline execution mode can be set using the toggle switch from the UI
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.319.1">
     Alternatively, the environment mode can also be set using the Databricks REST API.
    </span>
    <span class="koboSpan" id="kobo.319.2">
     In the following code snippet, we’ll use the Python
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.320.1">
      requests
     </span>
    </strong>
    <span class="koboSpan" id="kobo.321.1">
     library to send a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.322.1">
      PUT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.323.1">
     request to the Databricks DLT pipelines REST API that will set the development mode of a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.323.2">
     Note that the endpoint URL will change, depending on your Databricks workspace deployment, and the code snippet is just
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.324.1">
      an example:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.325.1">
import requests
response = requests.put(
    "https://&lt;your_databricks_workspace&gt;/api/2.0/pipelines/1234",
    headers={"Authorization": f"Bearer {api_token}"},
    json={
        "id": "1234",
        "name": "Clickstream Pipeline",
        "storage": "/Volumes/clickstream/data",
        "clusters": [{
            "label": "default",
            "autoscale": {
                "min_workers": 1,
                "max_workers": 3,
                "mode": "ENHANCED"}
        }],
        </span><strong class="bold"><span class="koboSpan" id="kobo.326.1">"development": True,</span></strong><span class="koboSpan" id="kobo.327.1">
        "target": "clickstream_data",
        "continuous": False
    }
)</span></pre>
   <h2 id="_idParaDest-62">
    <a id="_idTextAnchor069">
    </a>
    <span class="koboSpan" id="kobo.328.1">
     Databricks runtime
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.329.1">
     DLT is a version-less product feature on the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.329.2">
     In other words, Databricks
    </span>
    <a id="_idIndexMarker111">
    </a>
    <span class="koboSpan" id="kobo.330.1">
     manages the underlying
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.331.1">
      Databricks Runtime
     </span>
    </strong>
    <span class="koboSpan" id="kobo.332.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.333.1">
      DBR
     </span>
    </strong>
    <span class="koboSpan" id="kobo.334.1">
     ) that a
    </span>
    <a id="_idIndexMarker112">
    </a>
    <span class="koboSpan" id="kobo.335.1">
     data pipeline uses for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.336.1">
      its cluster.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.337.1">
     Furthermore, Databricks will automatically upgrade a data pipeline cluster to use the latest stable runtime release.
    </span>
    <span class="koboSpan" id="kobo.337.2">
     Runtime upgrades are important because they introduce bug fixes, new performance features, and other enhancements.
    </span>
    <span class="koboSpan" id="kobo.337.3">
     This can mean that your data pipelines will execute faster, translating to less time and money spent to transform the latest data in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.338.1">
      your
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.339.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.340.1">
      akehouse.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.341.1">
     You might even be eager to test out the latest performance features.
    </span>
    <span class="koboSpan" id="kobo.341.2">
     Each DLT pipeline has a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.342.1">
      Channel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.343.1">
     setting that allows data engineers
    </span>
    <a id="_idIndexMarker113">
    </a>
    <span class="koboSpan" id="kobo.344.1">
     to select one of two channel options –
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.345.1">
      Current
     </span>
    </strong>
    <span class="koboSpan" id="kobo.346.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.347.1">
      Preview
     </span>
    </strong>
    <span class="koboSpan" id="kobo.348.1">
     .
    </span>
    <span class="koboSpan" id="kobo.348.2">
     The Preview channel
    </span>
    <a id="_idIndexMarker114">
    </a>
    <span class="koboSpan" id="kobo.349.1">
     allows data engineers to configure a data pipeline to execute using the latest, experimental runtime that contains the new performance features and other enhancements.
    </span>
    <span class="koboSpan" id="kobo.349.2">
     However, since this is an experimental runtime, it’s not recommended that data pipelines running in production should use a Preview channel of the Databricks runtime.
    </span>
    <span class="koboSpan" id="kobo.349.3">
     Instead, it’s recommended to use the former option, Current, which
    </span>
    <a id="_idIndexMarker115">
    </a>
    <span class="koboSpan" id="kobo.350.1">
     selects the latest stable release of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.351.1">
      Databricks runtime.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.352.1">
     Furthermore, the DLT system will proactively catch runtime exceptions for data pipelines deployed in production mode.
    </span>
    <span class="koboSpan" id="kobo.352.2">
     For example, if a new runtime release introduces a runtime bug, also referred to as a runtime regression, or a library version conflict, DLT will attempt to downgrade the cluster to a lower runtime that was known to execute data pipelines successfully, and it will retry executing the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.353.1">
      pipeline update.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.354.1">
     The following diagram illustrates the automatic runtime upgrade
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.355.1">
      exception handling.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer022">
     <span class="koboSpan" id="kobo.356.1">
      <img alt="Figure 2.4 – In production mode, DLT will attempt to rerun a failed data pipeline execution using a lower Databricks runtime release" src="image/B22011_02_004.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.357.1">
     Figure 2.4 – In production mode, DLT will attempt to rerun a failed data pipeline execution using a lower Databricks runtime release
    </span>
   </p>
   <h2 id="_idParaDest-63">
    <a id="_idTextAnchor070">
    </a>
    <span class="koboSpan" id="kobo.358.1">
     Pipeline cluster types
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.359.1">
     Each data pipeline will have
    </span>
    <a id="_idIndexMarker116">
    </a>
    <span class="koboSpan" id="kobo.360.1">
     two associated clusters – one to perform the dataset updates and one to perform the table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.361.1">
      maintenance tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.362.1">
     The settings for these two types of clusters are expressed in the pipeline settings of a pipeline, using a JSON cluster configuration definition.
    </span>
    <span class="koboSpan" id="kobo.362.2">
     There are three types of cluster configurations that can be expressed in the pipeline settings – the update cluster configuration, the maintenance cluster configuration, and a third option that acts as a default cluster configuration, applying generalized settings to both update and maintenance clusters.
    </span>
    <span class="koboSpan" id="kobo.362.3">
     The schema for this JSON configuration closely follows that of the Databricks Clusters
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.363.1">
      REST API.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.364.1">
     In addition to configuring the physical attributes of a cluster, such as the number of worker nodes and virtual machine instance types, the cluster configuration can also contain advanced Spark configurations.
    </span>
    <span class="koboSpan" id="kobo.364.2">
     Let’s walk through a sample cluster
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.365.1">
      configuration together.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.366.1">
     The following example
    </span>
    <a id="_idIndexMarker117">
    </a>
    <span class="koboSpan" id="kobo.367.1">
     contains two separate cluster configurations – a default cluster configuration that will be applied to both update and maintenance DLT clusters, as well as another cluster configuration that will be applied only to the update
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.368.1">
      DLT cluster.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.369.1">
     In the first cluster configuration, we’ll specify that the cluster configuration will be the default cluster configuration using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.370.1">
      label
     </span>
    </strong>
    <span class="koboSpan" id="kobo.371.1">
     attribute.
    </span>
    <span class="koboSpan" id="kobo.371.2">
     This means that the cluster configuration will be applied to DLT clusters used to update the datasets and for clusters created to run table maintenance tasks.
    </span>
    <span class="koboSpan" id="kobo.371.3">
     Then, we’ll enable autoscaling for our DLT clusters, specifying that all clusters will begin provisioning a cluster with a single virtual machine but can grow up to five virtual machines in total as processing demands increase.
    </span>
    <span class="koboSpan" id="kobo.371.4">
     We’ll also specify that an enhanced version of the cluster autoscaling algorithm should
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.372.1">
      be used.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.373.1">
     In the second set of cluster configurations, we’ll specify that the cluster configuration should be applied only to DLT update clusters using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.374.1">
      label
     </span>
    </strong>
    <span class="koboSpan" id="kobo.375.1">
     attribute again.
    </span>
    <span class="koboSpan" id="kobo.375.2">
     Then, we’ll specify which instance types to provision for the update cluster driver and worker nodes.
    </span>
    <span class="koboSpan" id="kobo.375.3">
     For the driver node, which orchestrates tasks, we’ll specify that the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.376.1">
      i4i.2xlarge
     </span>
    </strong>
    <span class="koboSpan" id="kobo.377.1">
     EC2 instance type should be used, while all worker nodes should use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.378.1">
      i4i.xlarge
     </span>
    </strong>
    <span class="koboSpan" id="kobo.379.1">
     EC2 instances.
    </span>
    <span class="koboSpan" id="kobo.379.2">
     Lastly, we’ll also enable a Databricks Runtime performance feature, called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.380.1">
      Auto-Optimized Shuffle
     </span>
    </strong>
    <span class="koboSpan" id="kobo.381.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.382.1">
      AOS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.383.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.383.2">
     AOS will automatically size the number of Spark shuffle partitions
    </span>
    <a id="_idIndexMarker118">
    </a>
    <span class="koboSpan" id="kobo.384.1">
     at runtime, which can improve performance during wide Spark transformations such as joins, aggregations, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.385.1">
      merge operations.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.386.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.387.1">
     In the following example, we’ve chosen to illustrate the cluster configuration settings using virtual machine instances for the AWS cloud.
    </span>
    <span class="koboSpan" id="kobo.387.2">
     However, if your workspace is in a different cloud provider, we’d suggest using Delta cache accelerated VM instances of similar sizes – eight cores for the driver node and four cores for the worker
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.388.1">
      nodes (
     </span>
    </span>
    <a href="https://docs.databricks.com/en/optimizations/disk-cache.html">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.389.1">
       https://docs.databricks.com/en/optimizations/disk-cache.html
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.390.1">
      ):
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.391.1">
{
    "clusters": [{
        "label": "default",
        "autoscale": {
            "min_workers": 1,
            "max_workers": 5,
            "mode": "ENHANCED"}
    },
    {
        "label": "updates",
        "node_type_id": "i4i.xlarge",
        "driver_node_type_id": "i4i.2xlarge",
        "spark_conf": {"spark.sql.suffle.partitions": "auto"}
    }]
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.392.1">
     As you can see, cluster configurations are a powerful tool that provides data engineers with the ability to
    </span>
    <a id="_idIndexMarker119">
    </a>
    <span class="koboSpan" id="kobo.393.1">
     apply either generalized cluster settings, target specific cluster settings, or do a combination of both.
    </span>
    <span class="koboSpan" id="kobo.393.2">
     This is a great way to tune clusters for specific workloads and yield additional performance for your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.394.1">
      DLT pipelines.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-64">
    <a id="_idTextAnchor071">
    </a>
    <span class="koboSpan" id="kobo.395.1">
     A serverless compute versus a traditional compute
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.396.1">
     Data pipelines can be executed using clusters configured with a traditional compute or a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.397.1">
      serverless compute.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.398.1">
     A traditional compute gives a
    </span>
    <a id="_idIndexMarker120">
    </a>
    <span class="koboSpan" id="kobo.399.1">
     user the most control over computational resources.
    </span>
    <span class="koboSpan" id="kobo.399.2">
     However, with a traditional compute, the user will need to manage several aspects of the underlying cluster.
    </span>
    <span class="koboSpan" id="kobo.399.3">
     For example, data engineering teams will need to configure cluster attributes such as the auto-scaling behavior, whether the pipeline should be executed using the Photon engine or the legacy Catalyst engine in Spark, as well as optional c
    </span>
    <a id="_idTextAnchor072">
    </a>
    <span class="koboSpan" id="kobo.400.1">
     luster tagging.
    </span>
    <span class="koboSpan" id="kobo.400.2">
     Furthermore, traditional compute allows the user to have full control over the VM instance types that are selected for the driver and worker nodes of the cluster.
    </span>
    <span class="koboSpan" id="kobo.400.3">
     As we saw in the previous section, the VM instance types can be specified under the pipeline settings by listing specific instance types within the JSON configuration.
    </span>
    <span class="koboSpan" id="kobo.400.4">
     For example, the following cluster configuration specifies the i4i.xlarge and i4i.2xlarge EC2 instance types for all update clusters within a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.401.1">
      DLT pipeline:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.402.1">
{
    "clusters": [{
        "label": "updates",
        "node_type_id": "i4i.xlarge",
        "driver_node_type_id": "i4i.2xlarge"
    }]
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.403.1">
     However, DLT pipelines configured to use a serverless compute will abstract away all the underlying cluster settings, such as the cluster VM instance types, the number of worker nodes, and the autoscaling settings.
    </span>
    <span class="koboSpan" id="kobo.403.2">
     As the name
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.404.1">
      serverless compute
     </span>
    </em>
    <span class="koboSpan" id="kobo.405.1">
     suggests, the computational resources will be provisioned and managed by Databricks in the Databricks cloud provider account.
    </span>
    <span class="koboSpan" id="kobo.405.2">
     Behind the scenes, Databricks will maintain a pool of pre-provisioned computational resources so that cluster provisioning is fast.
    </span>
    <span class="koboSpan" id="kobo.405.3">
     As soon as an update for a data pipeline is triggered, the DLT system will create a logical network inside of the Databricks cloud provider account and initialize a cluster to execute the pipeline’s data flow graph.
    </span>
    <span class="koboSpan" id="kobo.405.4">
     Databricks will automatically select the VM instance type, the Photon execution engine, and the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.406.1">
      autoscaling behavior.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.407.1">
     As an added layer of security, there is no communication permitted between logical networks or from the external internet, and the computational resources are never reused across serverless workloads.
    </span>
    <span class="koboSpan" id="kobo.407.2">
     When the data pipeline processing has been completed and the cluster has been terminated, the computational resources are released back to the cloud provider
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.408.1">
      and destroyed.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.409.1">
     You might choose a serverless compute to remove the infrastructure overhead of maintaining and updating multiple cluster policies, as well as to also take advantage of fast cluster provisioning
    </span>
    <a id="_idIndexMarker121">
    </a>
    <span class="koboSpan" id="kobo.410.1">
     when reacting to spikes in processing demands is critical.
    </span>
    <span class="koboSpan" id="kobo.410.2">
     Plus, serverless execution enables other platform features, such as updating materialized views in continuous processing mode (processing modes are covered in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.411.1">
      next section).
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-65">
    <a id="_idTextAnchor073">
    </a>
    <span class="koboSpan" id="kobo.412.1">
     Loading external dependencies
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.413.1">
     Data pipelines may need to
    </span>
    <a id="_idIndexMarker122">
    </a>
    <span class="koboSpan" id="kobo.414.1">
     load external dependencies, such as helper utilities or third-party libraries.
    </span>
    <span class="koboSpan" id="kobo.414.2">
     As such, DLT offers three ways to install runtime dependencies for a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.415.1">
      data pipeline:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.416.1">
      From a notebook cell, using the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.417.1">
       %pip
      </span>
     </strong>
     <span class="koboSpan" id="kobo.418.1">
      magic
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.419.1">
       command (
      </span>
     </span>
     <a href="https://docs.databricks.com/en/notebooks/notebooks-code.html#mix-languages">
      <span class="No-Break">
       <span class="koboSpan" id="kobo.420.1">
        https://docs.databricks.com/en/notebooks/notebooks-code.html#mix-languages
       </span>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.421.1">
       )
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.422.1">
      Loading a module from a workspace file or
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.423.1">
       Databricks repo
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.424.1">
      Using a cluster
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.425.1">
       initialization script
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.426.1">
     The popular Python package manager
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.427.1">
      pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.428.1">
     can be used to install Python modules from any notebook in a data pipeline’s source code, via the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.429.1">
      %pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.430.1">
     Databricks magic command.
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.431.1">
      %pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.432.1">
     is the simplest method for installing library dependencies in a data pipeline.
    </span>
    <span class="koboSpan" id="kobo.432.2">
     At runtime, the DLT system will detect all notebook cells containing
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.433.1">
      %pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.434.1">
     magic commands and execute these cells first, before performing any pipeline updates.
    </span>
    <span class="koboSpan" id="kobo.434.2">
     Furthermore, all notebooks for a declared data pipeline’s source code will share a single virtual environment, so the library dependencies will be installed together in an isolated environment and be globally available to all notebooks in a data pipeline’s source code.
    </span>
    <span class="koboSpan" id="kobo.434.3">
     Conversely, the notebooks for a data pipeline cannot install different versions of the same Python library.
    </span>
    <span class="koboSpan" id="kobo.434.4">
     For example, the following code sample will use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.435.1">
      pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.436.1">
     package manager to install the popular libraries
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.437.1">
      numpy
     </span>
    </strong>
    <span class="koboSpan" id="kobo.438.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.439.1">
      pandas
     </span>
    </strong>
    <span class="koboSpan" id="kobo.440.1">
     , and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.441.1">
      scikit-learn
     </span>
    </strong>
    <span class="koboSpan" id="kobo.442.1">
     , as well as a custom Python library from a Databricks
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.443.1">
       Volumes
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.444.1">
      location:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.445.1">
%pip install numpy pandas scikit-learn /Volumes/tradingutils/tech-analysis-utils-v1.whl</span></pre>
   <p>
    <span class="koboSpan" id="kobo.446.1">
     As a best practice, these dependency installation statements should be placed at the very top of a notebook, so that it's easier to reference
    </span>
    <a id="_idIndexMarker123">
    </a>
    <span class="koboSpan" id="kobo.447.1">
     pipeline
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.448.1">
      dependencies quickly.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.449.1">
     Alternatively, library dependencies can also be installed as a Python module.
    </span>
    <span class="koboSpan" id="kobo.449.2">
     In this scenario, the library can be installed and loaded in a DLT pipeline as either a workspace file or from a Databricks Repo, if the module is version-controlled using a Git provider, such as GitHub
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.450.1">
      or Bitbucket.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.451.1">
     Lastly, cluster initialization scripts can also be used to install external dependencies.
    </span>
    <span class="koboSpan" id="kobo.451.2">
     These scripts are run after a cluster has provisioned the VMs and installed the Databricks runtime, but before the data pipeline begins execution.
    </span>
    <span class="koboSpan" id="kobo.451.3">
     For example, this type of dependency installation might be applicable in a scenario where firmwide libraries need to be consistently installed across all data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.452.1">
      engineering platforms.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.453.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.454.1">
     You may have noticed that the preceding options only covered installing Python dependencies.
    </span>
    <span class="koboSpan" id="kobo.454.2">
     DLT does not support installing JVM libraries, since it only offers the Python and SQL
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.455.1">
      programming interfaces.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-66">
    <a id="_idTextAnchor074">
    </a>
    <span class="koboSpan" id="kobo.456.1">
     Data pipeline processing modes
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.457.1">
     The data pipeline
    </span>
    <a id="_idIndexMarker124">
    </a>
    <span class="koboSpan" id="kobo.458.1">
     processing mode determines how frequently the tables and materialized views within a pipeline are updated.
    </span>
    <span class="koboSpan" id="kobo.458.2">
     DLT offers two
    </span>
    <a id="_idIndexMarker125">
    </a>
    <span class="koboSpan" id="kobo.459.1">
     types of pipeline
    </span>
    <a id="_idIndexMarker126">
    </a>
    <span class="koboSpan" id="kobo.460.1">
     processing modes –
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.461.1">
      triggered
     </span>
    </strong>
    <span class="koboSpan" id="kobo.462.1">
     processing mode and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.463.1">
      continuous
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.464.1">
      processing mode.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.465.1">
     Triggered processing mode will update the datasets contained within a pipeline once and then immediately terminate the cluster that was provisioned to run the pipeline, releasing the computational resources back to the cloud provider and thereby terminating additional cloud costs from being assessed.
    </span>
    <span class="koboSpan" id="kobo.465.2">
     As the name suggests, triggered processing mode can be run in an ad hoc manner and will execute immediately from a triggering event, such as
    </span>
    <a id="_idIndexMarker127">
    </a>
    <span class="koboSpan" id="kobo.466.1">
     the event of a button clicked on the DLT UI by a user or an invocation to the Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.467.1">
      REST API.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer023">
     <span class="koboSpan" id="kobo.468.1">
      <img alt="Figure 2.5 – A triggered data pipeline will refresh each dataset and then immediately terminate the cluster" src="image/B22011_02_005.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.469.1">
     Figure 2.5 – A triggered data pipeline will refresh each dataset and then immediately terminate the cluster
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.470.1">
     Triggered processing mode can also be triggered to run using a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.471.1">
      cron
     </span>
    </strong>
    <span class="koboSpan" id="kobo.472.1">
     schedule, which can be configured from the UI or via the REST API.
    </span>
    <span class="koboSpan" id="kobo.472.2">
     As shown in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.473.1">
       Figure 2
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.474.1">
      .6
     </span>
    </em>
    <span class="koboSpan" id="kobo.475.1">
     , a recurring schedule can be created by clicking on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.476.1">
      Schedule
     </span>
    </strong>
    <span class="koboSpan" id="kobo.477.1">
     drop-down button in the DLT UI, clicking the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.478.1">
      Add schedule
     </span>
    </strong>
    <span class="koboSpan" id="kobo.479.1">
     button, and finally, selecting the desired time to trigger a pipeline update.
    </span>
    <span class="koboSpan" id="kobo.479.2">
     Each day, the datasets within the pipeline will be refreshed at the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.480.1">
      scheduled time.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer024">
     <span class="koboSpan" id="kobo.481.1">
      <img alt="Figure 2.6 – DLT pipelines can be scheduled to refresh datasets based on a repeating schedule" src="image/B22011_02_006.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.482.1">
     Figure 2.6 – DLT pipelines can be scheduled to refresh datasets based on a repeating schedule
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.483.1">
     Conversely, continuous processing mode will provision computational resources to refresh datasets within a pipeline but will continue to execute indefinitely, processing data and refreshing the
    </span>
    <a id="_idIndexMarker128">
    </a>
    <span class="koboSpan" id="kobo.484.1">
     tables and materialized views as data arrives from the source.
    </span>
    <span class="koboSpan" id="kobo.484.2">
     A continuous processing pipeline will keep the computational resources running and will continue to incur cloud costs, with the trade-off of minimal data staleness.
    </span>
    <span class="koboSpan" id="kobo.484.3">
     This type of pipeline mode should be selected when data latency is prioritized over cloud compute costs for a particular
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.485.1">
      data pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.486.1">
     Fortunately, the pipeline processing mode and other pipeline settings can be updated throughout the lifecycle of a data pipeline, allowing the pipeline to be flexible around processing latency and compute costs.
    </span>
    <span class="koboSpan" id="kobo.486.2">
     For example, an economic downturn may force an organization to prioritize cost savings over latency but may again emphasize latency later down
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.487.1">
      the road.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.488.1">
     Let’s use everything that we’ve learned together in this chapter and build a DLT pipeline that will apply SCD Type 2 changes to downstream datasets in our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.489.1">
      data pipeline.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-67">
    <a id="_idTextAnchor075">
    </a>
    <span class="koboSpan" id="kobo.490.1">
     Hands-on exercise – applying SCD Type 2 changes
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.491.1">
     In this hands-on exercise, we’ll use Databricks Auto Loader to incrementally load JSON files that are written to
    </span>
    <a id="_idIndexMarker129">
    </a>
    <span class="koboSpan" id="kobo.492.1">
     a raw landing zone in a cloud storage account.
    </span>
    <span class="koboSpan" id="kobo.492.2">
     Next, we’ll transform downstream columns and join data ingested from an external
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.493.1">
      Postgres database.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.494.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.495.1">
     Reading data from a remote Postgres database is optional.
    </span>
    <span class="koboSpan" id="kobo.495.2">
     This step is intended to demonstrate the flexibility of the Databricks Data Intelligence Platform, showing you how easy it is to read structured data from a remote RDBMS and combine it with semi-structured data.
    </span>
    <span class="koboSpan" id="kobo.495.3">
     If you do not have a Postgres database, a static DataFrame containing the taxi driver information is provided
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.496.1">
      for you.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.497.1">
     If you haven’t done so, you will need to clone the accompanying notebooks from this chapter’s GitHub repo, located
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.498.1">
      at
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter02">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.499.1">
       https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter02
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.500.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.501.1">
     Let’s start by importing the data generator notebook, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.502.1">
      Generate Mock Taxi Trip Data
     </span>
    </strong>
    <span class="koboSpan" id="kobo.503.1">
     .
    </span>
    <span class="koboSpan" id="kobo.503.2">
     This notebook will create a mock dataset containing fictitious information about taxi trips.
    </span>
    <span class="koboSpan" id="kobo.503.3">
     Once the mock dataset has been generated, this notebook will store the taxi trip dataset as multiple JSON files in our cloud storage account, which will be later ingested by our DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.503.4">
     Attach the taxi trip data generator notebook to an all-purpose cluster and execute all the cells to generate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.504.1">
      mock data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.505.1">
     Next, let’s create our DLT pipeline definition.
    </span>
    <span class="koboSpan" id="kobo.505.2">
     Create a new notebook by clicking the workspace table on the left sidebar, clicking on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.506.1">
      Add
     </span>
    </strong>
    <span class="koboSpan" id="kobo.507.1">
     dropdown, and selecting
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.508.1">
      Notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.509.1">
     .
    </span>
    <span class="koboSpan" id="kobo.509.2">
     Rename the notebook with a meaningful name, such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.510.1">
      Taxi Trips DLT Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.511.1">
     .
    </span>
    <span class="koboSpan" id="kobo.511.2">
     We’ll declare the datasets and transformations for our DLT pipeline in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.512.1">
      this notebook.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.513.1">
     Next, import the DLT Python module to access the DLT function decorators to define datasets and dependencies, as well as the PySpark
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.514.1">
      functions module:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.515.1">
import dlt
import pyspark.sql.functions as F</span></pre>
   <p>
    <span class="koboSpan" id="kobo.516.1">
     We’ll need to create a streaming table that will ingest taxi trip JSON data that has been written to a landing zone on cloud storage.
    </span>
    <span class="koboSpan" id="kobo.516.2">
     Let’s start by defining a new streaming table that uses the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.517.1">
      cloudFiles
     </span>
    </strong>
    <span class="koboSpan" id="kobo.518.1">
     data
    </span>
    <a id="_idIndexMarker130">
    </a>
    <span class="koboSpan" id="kobo.519.1">
     source to listen for new file events in the raw
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.520.1">
      landing zone:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.521.1">
# This location keeps track of schema changes
SCHEMA_LOCATION = "/tmp/chp_02/taxi_data_chkpnt"
# This location contains the raw, unprocessed trip data
RAW_DATA_LOCATION = "/tmp/chp_02/taxi_data/"
@dlt.table(
    name="raw_taxi_trip_data",
    comment="Raw taxi trip data generated by the data generator notebook"
)
def raw_taxi_trip_data():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", SCHEMA_LOCATION)
        .load(RAW_DATA_LOCATION) )</span></pre>
   <p>
    <span class="koboSpan" id="kobo.522.1">
     As new taxi trip data arrives, our DLT pipeline will efficiently load the data using Auto Loader, fetching only the information pertaining to the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.523.1">
      unprocessed files.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.524.1">
     Now that we’ve ingested the raw taxi trip data, we can begin applying the recorded changes to downstream tables.
    </span>
    <span class="koboSpan" id="kobo.524.2">
     Let’s first define a target streaming table to apply SCD Type 2 changes that have been reported by the mock taxi trip
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.525.1">
      data source:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.526.1">
# Define a new streaming table to apply SCD Type 2 changes
dlt.create_streaming_table("taxi_trip_data_merged")</span></pre>
   <p>
    <span class="koboSpan" id="kobo.527.1">
     Next, we’ll leverage the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.528.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.529.1">
     function covered earlier to instruct the DLT system on how changes should be applied, which columns to omit in the downstream
    </span>
    <a id="_idIndexMarker131">
    </a>
    <span class="koboSpan" id="kobo.530.1">
     table, and which SCD type to use.
    </span>
    <span class="koboSpan" id="kobo.530.2">
     Add the following function call to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.531.1">
      the notebook:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.532.1">
dlt.apply_changes(
    target="taxi_trip_data_merged",
    source="raw_taxi_trip_data",
    keys = ["trip_id"],
    sequence_by = F.col("sequence_num"),
    apply_as_deletes = F.expr("op_type = 'D'"),
    except_column_list = ["op_type", "op_date", "sequence_num"],
    stored_as_scd_type = 2
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.533.1">
     For our last step, we’ll transform a few of the columns in our upstream tables, such as rounding columns with a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.534.1">
      float
     </span>
    </strong>
    <span class="koboSpan" id="kobo.535.1">
     data type to two decimal places, as well as splitting the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.536.1">
      trip_distance
     </span>
    </strong>
    <span class="koboSpan" id="kobo.537.1">
     column into a column with miles as the unit of measurement and another column with kilometers as the unit of measurement.
    </span>
    <span class="koboSpan" id="kobo.537.2">
     Next, we’ll connect to a remote Postgres database and read the latest taxi driver information.
    </span>
    <span class="koboSpan" id="kobo.537.3">
     If you have access to a Postgres database, you can import the notebook, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.538.1">
      Generate Postgres Table
     </span>
    </strong>
    <span class="koboSpan" id="kobo.539.1">
     , and execute the cells to generate a table to test with.
    </span>
    <span class="koboSpan" id="kobo.539.2">
     Our final streaming table, which enriches our data and joins the latest taxi driver reference data, will look like
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.540.1">
      the following:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.541.1">
@dlt.table(
    name="raw_driver_data",
    comment="Dataset containing info about the taxi drivers"
)
def raw_driver_data():
    postgresdb_url = f"jdbc:postgresql://{POSTGRES_HOSTNAME}:{POSTGRES_PORT}/{POSTGRES_DB}"
    conn_props = {
        "user": POSTGRES_USERNAME,
        "password": POSTGRES_PW,
        "driver": "org.postgresql.Driver",
        "fetchsize": "1000"
    }
    return (
        spark.read
            .jdbc(postgresdb_url,
                  table=POSTGRES_TABLENAME,
                  properties=conn_props))
@dlt.table(
    name="taxi_trip_silver",
    comment="Taxi trip data with transformed columns"
)
def taxi_trip_silver():
    return (
        dlt.read("taxi_trip_data_merged")
            .withColumn("fare_amount_usd",
                        F.round(F.col("trip_amount"), 2))
            .withColumn("taxes_amount_usd",
                        F.round(F.col("trip_amount") * 0.05, 2))
            .withColumn("trip_distance_miles",
                        F.round(F.col("trip_distance"), 2))
            .withColumn("trip_distance_km",
                        F.round(F.col("trip_distance")
                        * 1.60934, 2)) # 1 mile = 1.60934 km
    ).join(
        dlt.read("raw_driver_data"),
        on="taxi_number",
        how="left"
    )</span></pre>
   <p>
    <span class="koboSpan" id="kobo.542.1">
     In this last function definition, we make use of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.543.1">
      dlt.read()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.544.1">
     function to retrieve earlier dataset
    </span>
    <a id="_idIndexMarker132">
    </a>
    <span class="koboSpan" id="kobo.545.1">
     declarations.
    </span>
    <span class="koboSpan" id="kobo.545.2">
     Behind the scenes, the DLT framework will add the datasets to the dataflow graph, creating the dependencies between the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.546.1">
      taxi_trip_data_merged
     </span>
    </strong>
    <span class="koboSpan" id="kobo.547.1">
     and
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.548.1">
       taxi_trip_silver
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.549.1">
      datasets.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.550.1">
     Now, it’s time to create our DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.550.2">
     Attach the notebook in the previous step to an all-purpose cluster and execute the notebook cells.
    </span>
    <span class="koboSpan" id="kobo.550.3">
     When prompted, click on the blue
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.551.1">
      Create Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.552.1">
     button to open the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.553.1">
      Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.554.1">
     UI page.
    </span>
    <span class="koboSpan" id="kobo.554.2">
     Give the pipeline a meaningful name, such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.555.1">
      Taxi Trip Data Pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.556.1">
     .
    </span>
    <span class="koboSpan" id="kobo.556.2">
     Since we are leveraging the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.557.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.558.1">
     function, we will need to select the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.559.1">
      Advanced
     </span>
    </strong>
    <span class="koboSpan" id="kobo.560.1">
     product edition.
    </span>
    <span class="koboSpan" id="kobo.560.2">
     Ensure that the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.561.1">
      Triggered
     </span>
    </strong>
    <span class="koboSpan" id="kobo.562.1">
     processing mode radio button is selected.
    </span>
    <span class="koboSpan" id="kobo.562.2">
     To view the backend table that is created by the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.563.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.564.1">
     function, select the Hive Metastore for the storage location, and provide a target schema to store the pipeline datasets.
    </span>
    <span class="koboSpan" id="kobo.564.2">
     Accept the remainder default values, and then click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.565.1">
      Create
     </span>
    </strong>
    <span class="koboSpan" id="kobo.566.1">
     button to create
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.567.1">
      the pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.568.1">
     Finally, run the newly created pipeline by clicking the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.569.1">
      Start
     </span>
    </strong>
    <span class="koboSpan" id="kobo.570.1">
     button in the DLT UI.
    </span>
    <span class="koboSpan" id="kobo.570.2">
     Soon, you will see a data flow graph that ingests changes from the raw JSON data, enriches downstream columns, and joins the remote structured data from the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.571.1">
      Postgres database.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer025">
     <span class="koboSpan" id="kobo.572.1">
      <img alt="Figure 2.7 – A data flow graph generated by the DLT system" src="image/B22011_02_007.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.573.1">
     Figure 2.7 – A data flow graph generated by the DLT system
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.574.1">
     As data is changed in the mock taxi trip data source, the full history of DML changes is picked and applied to a target
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.575.1">
      taxi_trip_data_merged
     </span>
    </strong>
    <span class="koboSpan" id="kobo.576.1">
     table.
    </span>
    <span class="koboSpan" id="kobo.576.2">
     The output of our data pipeline will be
    </span>
    <a id="_idIndexMarker133">
    </a>
    <span class="koboSpan" id="kobo.577.1">
     a curated streaming table that contains information about the taxi cab ride, as well as information about the taxi driver and taxi vehicle.
    </span>
    <span class="koboSpan" id="kobo.577.2">
     Best of all, with just a few lines of code, we deployed a fully scalable, cost-efficient data pipeline that can easily process billions
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.578.1">
      of files.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-68">
    <a id="_idTextAnchor076">
    </a>
    <span class="koboSpan" id="kobo.579.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.580.1">
     In this chapter, we looked at how DLT can simplify our data pipelines by abstracting away many of the low-level details of processing data in Spark.
    </span>
    <span class="koboSpan" id="kobo.580.2">
     We saw how Databricks Auto Loader solves the scalability problem of stream processing files from cloud storage.
    </span>
    <span class="koboSpan" id="kobo.580.3">
     With just a few lines of code, we deployed a scalable backend system to efficiently read new files as soon as they appear in a cloud storage location.
    </span>
    <span class="koboSpan" id="kobo.580.4">
     When it came to applying data changes to downstream datasets within our pipeline, the DLT framework once again simplified data reconciliation when data events were published late or out of order.
    </span>
    <span class="koboSpan" id="kobo.580.5">
     We also saw how we could apply slowly changing dimensions with just a few parameter changes in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.581.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.582.1">
     API.
    </span>
    <span class="koboSpan" id="kobo.582.2">
     Finally, we uncovered the details of data pipeline settings, optimizing the pipeline compute based on the computational requirements and
    </span>
    <a id="_idTextAnchor077">
    </a>
    <span class="koboSpan" id="kobo.583.1">
     DLT feature set that we needed in the data pipeline.
    </span>
    <span class="koboSpan" id="kobo.583.2">
     We also saw how DLT can automatically handle pipeline failures for us and proactively take action and attempt to fix certain
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.584.1">
      runtime exceptions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.585.1">
     In the next chapter, we’ll look at how we can use
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.586.1">
      expectations
     </span>
    </em>
    <span class="koboSpan" id="kobo.587.1">
     in DLT to enforce data quality rul
    </span>
    <a id="_idTextAnchor078">
    </a>
    <span class="koboSpan" id="kobo.588.1">
     es on data throughout each hop in our data pipeline, taking action whenever the data quality rules have
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.589.1">
      been violated.
     </span>
    </span>
   </p>
  </div>
 </body></html>