["```py\ndf = (spark.readStream\n      .format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .option(\"cloudFiles.schemaLocation\", schema_path)\n      .load(raw_data_path))\n```", "```py\n@dlt.table(\n    comment=\"Raw cloud files stream of completed taxi trips\"\n)\ndef yellow_taxi_events_raw():\n    return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"json\")\n            .option(\"cloudFiles.path\", schema_path)\n            .load(raw_landing_zone_path))\n```", "```py\nimport dlt \nimport pyspark.sql.functions as F\ndlt.create_streaming_table(\"iot_device_temperatures\")\ndlt.apply_changes(\n    target = \"iot_device_temperatures\",\n    source = \"smart_thermostats\",\n    keys = [\"device_id\"],\n    sequence_by = F.col(\"sequence_num\"),\n    apply_as_deletes = F.expr(\"operation = 'DELETE'\"),\n    except_column_list = [\"operation\", \"sequence_num\"],\n    stored_as_scd_type = \"2\"\n)\n```", "```py\n%sql\nGRANT USE CATALOG, CREATE SCHEMA ON CATALOG `chp2_transforming_data` TO `my_user`;\n```", "```py\n%sql\nUSE CATALOG `chp2_transforming_data`;\nCREATE SCHEMA IF NOT EXISTS `ride_hailing`;\nUSE SCHEMA `ride_hailing`;\n```", "```py\n%sql\nGRANT USE SCHEMA, CREATE TABLE, CREATE MATERIALIZED VIEW ON SCHEMA `ride_hailing` TO `my_user`;\n```", "```py\nimport requests\nresponse = requests.put(\n    \"https://<your_databricks_workspace>/api/2.0/pipelines/1234\",\n    headers={\"Authorization\": f\"Bearer {api_token}\"},\n    json={\n        \"id\": \"1234\",\n        \"name\": \"Clickstream Pipeline\",\n        \"storage\": \"/Volumes/clickstream/data\",\n        \"clusters\": [{\n            \"label\": \"default\",\n            \"autoscale\": {\n                \"min_workers\": 1,\n                \"max_workers\": 3,\n                \"mode\": \"ENHANCED\"}\n        }],\n        \"development\": True,\n        \"target\": \"clickstream_data\",\n        \"continuous\": False\n    }\n)\n```", "```py\n{\n    \"clusters\": [{\n        \"label\": \"default\",\n        \"autoscale\": {\n            \"min_workers\": 1,\n            \"max_workers\": 5,\n            \"mode\": \"ENHANCED\"}\n    },\n    {\n        \"label\": \"updates\",\n        \"node_type_id\": \"i4i.xlarge\",\n        \"driver_node_type_id\": \"i4i.2xlarge\",\n        \"spark_conf\": {\"spark.sql.suffle.partitions\": \"auto\"}\n    }]\n}\n```", "```py\n{\n    \"clusters\": [{\n        \"label\": \"updates\",\n        \"node_type_id\": \"i4i.xlarge\",\n        \"driver_node_type_id\": \"i4i.2xlarge\"\n    }]\n}\n```", "```py\n%pip install numpy pandas scikit-learn /Volumes/tradingutils/tech-analysis-utils-v1.whl\n```", "```py\nimport dlt\nimport pyspark.sql.functions as F\n```", "```py\n# This location keeps track of schema changes\nSCHEMA_LOCATION = \"/tmp/chp_02/taxi_data_chkpnt\"\n# This location contains the raw, unprocessed trip data\nRAW_DATA_LOCATION = \"/tmp/chp_02/taxi_data/\"\n@dlt.table(\n    name=\"raw_taxi_trip_data\",\n    comment=\"Raw taxi trip data generated by the data generator notebook\"\n)\ndef raw_taxi_trip_data():\n    return (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION)\n        .load(RAW_DATA_LOCATION) )\n```", "```py\n# Define a new streaming table to apply SCD Type 2 changes\ndlt.create_streaming_table(\"taxi_trip_data_merged\")\n```", "```py\ndlt.apply_changes(\n    target=\"taxi_trip_data_merged\",\n    source=\"raw_taxi_trip_data\",\n    keys = [\"trip_id\"],\n    sequence_by = F.col(\"sequence_num\"),\n    apply_as_deletes = F.expr(\"op_type = 'D'\"),\n    except_column_list = [\"op_type\", \"op_date\", \"sequence_num\"],\n    stored_as_scd_type = 2\n)\n```", "```py\n@dlt.table(\n    name=\"raw_driver_data\",\n    comment=\"Dataset containing info about the taxi drivers\"\n)\ndef raw_driver_data():\n    postgresdb_url = f\"jdbc:postgresql://{POSTGRES_HOSTNAME}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n    conn_props = {\n        \"user\": POSTGRES_USERNAME,\n        \"password\": POSTGRES_PW,\n        \"driver\": \"org.postgresql.Driver\",\n        \"fetchsize\": \"1000\"\n    }\n    return (\n        spark.read\n            .jdbc(postgresdb_url,\n                  table=POSTGRES_TABLENAME,\n                  properties=conn_props))\n@dlt.table(\n    name=\"taxi_trip_silver\",\n    comment=\"Taxi trip data with transformed columns\"\n)\ndef taxi_trip_silver():\n    return (\n        dlt.read(\"taxi_trip_data_merged\")\n            .withColumn(\"fare_amount_usd\",\n                        F.round(F.col(\"trip_amount\"), 2))\n            .withColumn(\"taxes_amount_usd\",\n                        F.round(F.col(\"trip_amount\") * 0.05, 2))\n            .withColumn(\"trip_distance_miles\",\n                        F.round(F.col(\"trip_distance\"), 2))\n            .withColumn(\"trip_distance_km\",\n                        F.round(F.col(\"trip_distance\")\n                        * 1.60934, 2)) # 1 mile = 1.60934 km\n    ).join(\n        dlt.read(\"raw_driver_data\"),\n        on=\"taxi_number\",\n        how=\"left\"\n    )\n```"]