["```py\n%sql\nCREATE CATALOG IF NOT EXISTS yellow_taxi_catalog;\nCREATE SCHEMA IF NOT EXISTS yellow_taxi_catalog.yellow_taxi;\nCREATE TABLE yellow_taxi_catalog.yellow_taxi.drivers(\n    driver_id INTEGER NOT NULL,\n    first_name STRING,\n    last_name STRING,\n    CONSTRAINT drivers_pk PRIMARY KEY(driver_id));\n```", "```py\n%sql\nCREATE TABLE yellow_taxi_catalog.yellow_taxi.rides(\n    ride_id INTEGER NOT NULL,\n    driver_id INTEGER,\n    passenger_count INTEGER,\n    total_amount DOUBLE,\n    CONSTRAINT rides_pk PRIMARY KEY (ride_id),\n    CONSTRAINT drivers_fk FOREIGN KEY (driver_id)\n    REFERENCES yellow_taxi_catalog.yellow_taxi.drivers);\n```", "```py\nCREATE VIEW yellow_taxi_catalog.yellow_taxi.rides_pk_validation_vw AS\nSELECT *\nFROM (\n    SELECT count(*) AS num_occurrences\n    FROM  yellow_taxi_catalog.yellow_taxi.rides\n    GROUP BY ride_id\n) WHERE num_occurrences > 1\n```", "```py\nSELECT count(*) AS num_invalid_pks\n  FROM yellow_taxi_catalog.yellow_taxi.rides_pk_validation_vw;\n```", "```py\n%pip install dbldatagen==0.4.0\n```", "```py\ndef generate_taxi_trip_data():\n    \"\"\"Generates random taxi trip data\"\"\"\n    import dbldatagen as dg\n    from pyspark.sql.types import (\n        IntegerType, StringType, FloatType, DateType\n    )\n    ds = (\n        dg.DataGenerator(spark, name=\"random_taxi_trip_dataset\",\n                         rows=100000, partitions=8)\n        .withColumn(\"trip_id\", IntegerType(),\n                    minValue=1000000, maxValue=2000000)\n        .withColumn(\"taxi_number\", IntegerType(),\n                    uniqueValues=10000, random=True)\n        .withColumn(\"passenger_count\", IntegerType(),\n                    minValue=1, maxValue=4)\n        .withColumn(\"trip_amount\", FloatType(), minValue=-100.0,\n                    maxValue=1000.0, random=True)\n        .withColumn(\"trip_distance\", FloatType(),\n                    minValue=0.1, maxValue=1000.0)\n        .withColumn(\"trip_date\", DateType(),\n                    uniqueValues=300, random=True))\n    return ds.build()\n```", "```py\ndbutils.fs.mkdirs(\"/tmp/chp_03/taxi_data\")\n```", "```py\nimport random\nmax_num_files = 100\nfor i in range(int(max_num_files)):\n    df = generate_taxi_trip_data()\n    file_name = f\"/tmp/chp_03/taxi_data/taxi_data_{random.randint(1, 1000000)}.json\"\n    df.write.mode(\"append\").json(file_name)\n```", "```py\nimport dlt\nfrom pyspark.sql.functions import *\n```", "```py\n@dlt.table(\n    comment=\"The randomly generated taxi trip dataset\"\n)\ndef yellow_taxi_raw():\n    path = \"/tmp/chp_03/taxi_data\"\n    schema = \"trip_id INT, taxi_number INT, passenger_count INT, trip_amount FLOAT, trip_distance FLOAT, trip_date DATE\"\n    return (spark.readStream\n                 .schema(schema)\n                 .format(\"json\")\n                 .load(path))\n```", "```py\n@dlt.table(name=\"trip_data_financials\",\n           comment=\"Financial information from incoming taxi trips.\")\n@dlt.expect(\"valid_total_amount\", \"trip_amount > 0.0\")\ndef trip_data_financials():\n    return (dlt.readStream(\"yellow_taxi_raw\")\n               .withColumn(\"driver_payment\",\n                           expr(\"trip_amount * 0.40\"))\n               .withColumn(\"vehicle_maintenance_fee\",\n                           expr(\"trip_amount * 0.05\"))\n               .withColumn(\"adminstrative_fee\",\n                           expr(\"trip_amount * 0.1\"))\n               .withColumn(\"potential_profits\",\n                           expr(\"trip_amount * 0.45\")))\n```", "```py\n@dlt.expect(\"valid_total_amount\", \"trip_amount > 0.0\")\n```", "```py\n@dlt.expect_or_fail(\"valid_total_amount\", \"trip_amount > 0.0\")\n```", "```py\n@dlt.table(\n    name=\"trip_data_financials\",\n    comment=\"Financial information from completed taxi trips.\"\n)\n@dlt.expect_or_fail(\"valid_total_amount\", \"trip_amount > 0.0\")\ndef trip_data_financials():\n    return (\n        dlt.readStream(\"yellow_taxi_raw\")\n           .withColumn(\"driver_payment\",\n                       expr(\"trip_amount*0.40\"))\n           .withColumn(\"vehicle_maintenance_fee\",\n                       expr(\"trip_amount*0.05\"))\n           .withColumn(\"adminstrative_fee\",\n                       expr(\"trip_amount*0.1\"))\n           .withColumn(\"potential_profits\",\n                       expr(\"trip_amount*0.45\")))\n```", "```py\nassertions = {\n    \"total_amount_constraint\": \"trip_amount > 0.0\",\n    \"passenger_count\": \"passenger_count >= 1\"\n}\n@dlt.table(\n    name=\"yellow_taxi_validated\",\n    comment=\"A dataset containing trip data that has been validated.\")\n@dlt.expect_all_or_drop(assertions)\ndef yellow_taxi_validated():\n    return (\n        dlt.readStream(\"yellow_taxi_raw\")\n           .withColumn(\"nyc_congestion_tax\",\n                       expr(\"trip_amount * 0.05\")))\n```", "```py\n%sql\nCREATE TABLE IF NOT EXISTS<catalog_name>.<schema_name>.data_quality_rules\n(rule_name STRING, rule_expression STRING, dataset_name STRING)\nUSING DELTA\n```", "```py\n%sql\nINSERT INTO\n    data_quality_rules\nVALUES\n    (\n        'valid_total_amount',\n        'trip_amount > 0.0',\n        'yellow_taxi_raw'\n    ),(\n        'valid_passenger_count',\n        'passenger_count > 0',\n        'yellow_taxi_raw'\n    );\n```", "```py\ndef compile_data_quality_rules(rules_table_name, dataset_name):\n    \"\"\"A helper function that reads from the data_quality_rules table and coverts to a format interpreted by a DLT Expectation.\"\"\"\n    rules = spark.sql(f\"\"\"SELECT * FROM {rules_table_name} WHERE dataset_name='{dataset_name}'\"\"\").collect()\n    rules_dict = {}\n    # Short circuit if there are no rules found\n    if len(rules) == 0:\n        raise Exception(f\"No rules found for dataset '{dataset_name}'\")\n    for rule in rules:\n        rules_dict[rule.rule_name] = rule.rule_expression\n    return rules_dict\n```", "```py\nimport dlt\nfrom pyspark.sql.functions import *\nRULES_TABLE = \"<catalog_name>.<schema_name>.data_quality_rules\"\nDATASET_NAME = \"yellow_taxi_raw\"\n@dlt.table(\n    comment=\"Randomly generated taxi trip data.\"\n)\ndef yellow_taxi_raw():\n    path = \"/tmp/chp_03/taxi_data\"\n    schema = \"trip_id INT, taxi_number INT, passenger_count INT, trip_amount FLOAT, trip_distance FLOAT, trip_date DATE\"\n    return (spark.readStream\n                 .schema(schema)\n                 .format(\"json\")\n                 .load(path))\n@dlt.table(\n    name=\"yellow_taxi_validated\",\n    comment=\"A dataset containing trip data that has been validated.\")\n@dlt.expect_all(compile_data_quality_rules(RULES_TABLE, DATASET_NAME))\ndef yellow_taxi_validated():\n    return (\n        dlt.readStream(\"yellow_taxi_raw\")\n           .withColumn(\"nyc_congestion_tax\",\n                       expr(\"trip_amount * 0.05\"))\n    )\n```", "```py\n%py\nimport dlt\nfrom pyspark.sql.functions import *\n@dlt.table(\n    name=\"yellow_taxi_raw\",\n    comment=\"The randomly generated taxi trip dataset\"\n)\ndef yellow_taxi_raw():\n    path = \"/tmp/chp_03/taxi_data\"\n    schema = \"trip_id INT, taxi_number INT, passenger_count INT, trip_amount FLOAT, trip_distance FLOAT, trip_date DATE\"\n    return (spark.readStream\n                 .schema(schema)\n                 .format(\"json\")\n                 .load(path))\n```", "```py\ndata_quality_rules = {\n    \"total_amount_assertion\": \"trip_amount > 0.0\",\n    \"passenger_count\": \"passenger_count >= 1\"\n}\n```", "```py\n@dlt.table(\n    name=\"yellow_taxi_validated\",\n    comment=\"Validation table that applies data quality rules to the incoming data\"\n)\ndef yellow_taxi_validated():\n    return (\n        dlt.readStream(\"yellow_taxi_raw\")\n           .withColumn(\"is_valid\",\n                when(expr(\" AND \".join(data_quality_rules.values())),\n                lit(True)).otherwise(lit(False)))\n    )\n```", "```py\n@dlt.table(\n    name=\"yellow_taxi_quarantine\",\n    comment=\"A quarantine table for incoming data that has not met the validation criteria\"\n)\ndef yellow_taxi_quarantine():\n    return (\n        dlt.readStream(\"yellow_taxi_validated\")\n           .where(expr(\"is_valid == False\"))\n    )\n@dlt.table(\n    name=\"yellow_taxi_passing\"\n)\ndef yellow_taxi_passing():\n    return (\n        dlt.readStream(\"yellow_taxi_validated\")\n           .where(expr(\"is_valid == True\"))\n    )\n```"]