- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Data Quality Using Delta Live Tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces several techniques for managing the data quality of
    datasets in a data pipeline. We’ll introduce **expectations** in **Delta Live
    Tables** ( **DLT** ), which is a way to enforce certain data quality constraints
    on arriving data before merging the data into downstream tables. Later in the
    chapter, we’ll look at more advanced techniques such as quarantining bad data
    for human intervention. Next, we’ll also see how we can decouple constraints so
    that they can be managed separately by non-technical personas within your organization.
    By the end of the chapter, you should have a firm understanding of how you can
    take measures to ensure the data integrity of datasets in your lakehouse and how
    to take appropriate action on data not meeting the expected criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining data constraints in Delta Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using temporary datasets to validate data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hands-on exercise: writing your first data quality expectation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking action on failed expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying multiple data quality expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoupling expectations from a DLT pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercise – quarantining poor-quality data for correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with this chapter, it’s recommended to have Databricks workspace
    permissions to create an all-purpose cluster and a DLT pipeline using a cluster
    policy. It’s also recommended to have Unity Catalog permissions to create and
    use catalogs, schemas, and tables. All code samples can be downloaded from this
    chapter’s GitHub repository, located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter03](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter03)
    . We’ll be using the NYC yellow taxi dataset, which can be found on the Databricks
    FileSystem at **/databricks-datasets/nyctaxi/tripdata/yellow** . This chapter
    will create and run several new notebooks and DLT pipelines using the **Advanced**
    product edition. As a result, the pipelines are estimated to consume around 10-20
    **Databricks** **Units** ( **DBUs** ).
  prefs: []
  type: TYPE_NORMAL
- en: Defining data constraints in Delta Lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data constraints are an effective way of defining criteria that incoming data
    must satisfy before being inserted into a Delta table. Constraints are defined
    per column in a Delta table and are stored as additional table metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four different types of constraints available within the Databricks
    Data Intelligence Platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOT NULL** : Ensures that the data for a particular column in a table is
    not null. The **NOT NULL** constraint was first introduced in the **StructField**
    class definition of Apache Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHECK** : A Boolean expression that must evaluate to **True** for each row
    before being inserted. Check constraints allow data engineers to enforce complex
    validation logic that a particular column must satisfy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PRIMARY KEY** : Establishes uniqueness for a particular column across all
    the rows in a table. The **PRIMARY KEY** constraint is a special kind of constraint
    as it is purely informative and is not enforced on the incoming data. As we’ll
    see in the following example, a **NOT NULL** constraint must accompany a **PRIMARY**
    **KEY** constraint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FOREIGN KEY** : Establishes a relationship between a particular column and
    another table. Like the **PRIMARY KEY** constraint, a **FOREIGN KEY** constraint
    is also purely informative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, only the **NOT NULL** and **CHECK** constraints are enforced on
    the incoming data.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Constraint** | **Enforced** | **Informative** |'
  prefs: []
  type: TYPE_TB
- en: '| **NOT NULL** | ✔️ | ✖️ |'
  prefs: []
  type: TYPE_TB
- en: '| **CHECK** | ✔️ | ✖️ |'
  prefs: []
  type: TYPE_TB
- en: '| **PRIMARY KEY** | ✖️ | ✔️ |'
  prefs: []
  type: TYPE_TB
- en: '| **FOREIGN KEY** | ✖️ | ✔️ |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Data quality constraints can either be enforced or not enforced
    on the Databricks Data Intelligence Platform
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The **PRIMARY KEY** constraint and the **FOREIGN KEY** constraint require the
    Delta tables to be stored in Unity Catalog, otherwise a runtime error will be
    thrown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how we can use constraints to define a hierarchical relationship
    between two Delta tables in our lakehouse. First, create a new SQL-based notebook
    within your Databricks notebook. Let’s start by defining a child table that will
    contain data about the taxicab drivers, called **drivers** , with a primary key
    defined on the **driver_id** column. Add the following code snippet to a new notebook
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define a parent table, **rides** , having a primary key defined
    for the **ride_id** column and a foreign key that references the **drivers** table.
    Add the following code snippet below the first notebook cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Attach the newly created notebook to an all-purpose cluster and execute the
    notebook cells to create the parent and child tables. Finally, let’s navigate
    to the newly defined tables in Catalog Explorer to generate an **Entity Relationship
    Diagram** ( **ERD** ) directly from the Databricks Data Intelligence Platform.
    From our Databricks workspace, click on **Catalog Explorer** on the left sidebar.
    Navigate to the **yellow_taxi_catalog** catalog in Unity Catalog, in the preceding
    example. Click on the defined schema and, finally, click on the parent table.
    A side pane will expand, displaying metadata about our Delta table. Click on the
    button titled **View Relationships** to view the ERD.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Data constraints can be used to define primary key and foreign
    key relationships between Delta tables](img/B22011_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Data constraints can be used to define primary key and foreign
    key relationships between Delta tables
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, the primary key and foreign key constraints are purely
    informative and are not enforced on the incoming data. Instead, it’s recommended
    to implement additional safeguards to ensure the data integrity of a primary key
    column in a Delta table. Let’s look at a few effective strategies we can employ
    to maintain the integrity of primary key columns defined in our lakehouse tables.
  prefs: []
  type: TYPE_NORMAL
- en: Using temporary datasets to validate data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ll see in this section, creating a view is an effective method for validating
    the uniqueness of a primary key column. Additionally, we can also define alerts
    in the Databricks Data Intelligence Platform to notify the data stewards of potential
    data quality issues so that they can take appropriate measures to correct the
    data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage a view to validate the uniqueness of the primary key column.
    Recall the **rides** and **drivers** tables we defined in the previous section.
    In this example, we’re going to define a view on the incoming data to ensure the
    uniqueness of a primary key column across the **rides** Delta table. Create a
    new query in Databricks by navigating back to your workspace and right-clicking
    to open a dialog box. Select **New** | **Query** to open a new query in the editor.
    Next, rename the query with a meaningful name, such as **rides_pk_validation_vw**
    . Finally, add the following query text to the open query and click the **Run**
    button to validate that the query runs as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As it turns out, primary key uniqueness is essential in downstream reports for
    the Yellow Taxi Corporation. Let’s create a new alert in the Databricks Data Intelligence
    Platform to alert our data stewards of possible data corruption so that they can
    take appropriate action when a duplicate primary key is inserted.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a query that will be run by our alert. From the sidebar,
    click on the **Queries** button and click the **Create query** button, which will
    take us to the query editor in the Databricks Data Intelligence Platform. Rename
    the query to something meaningful, such as **Rides Primary Key Uniqueness** .
    Enter the following SQL text as the query body, click the **Save** button, and
    select a workspace folder to save the query. Click the **Run** button and ensure
    that the query runs successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, from the sidebar, click on the **Alerts** button to navigate to the **Alerts**
    UI. Then, click on the **Create alert** button to begin creating a new alert and
    enter a descriptive name in the **Alert name** textbox, such as **Invalid Primary
    Key on Rides Table** . In the **Query** dropdown, select the query we just created.
    Click the **Send notification** checkbox and accept the default settings by clicking
    the **Create alert** button. In a real-world scenario, this could be an email
    chain for on-call data engineers or other popular notification destinations such
    as Slack or Microsoft Teams.
  prefs: []
  type: TYPE_NORMAL
- en: This example is quite practical in real-world data pipelines. However, views
    require the latest table state to be calculated each time a pipeline is run, as
    well as the maintenance overhead of having to configure the notification alerts.
    That’s a lot of configuration to maintain, which simply won’t scale as we add
    more tables to our pipelines. What if there’s an easier way to declare data quality
    as a part of our DLT pipeline declaration?
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to expectations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expectations are data quality rules defined alongside a dataset definition in
    a DLT pipeline. The data quality rule is a Boolean expression applied to each
    record passing through a particular dataset definition. The expression must evaluate
    to **True** for the record to be marked as passing, else it will result in a failed
    record indicating that the record has not passed data quality validation.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the DLT pipeline will record data quality metrics for each row
    that gets processed in a data pipeline. For example, DLT will record the number
    of records that have passed data quality validation, as well as the number of
    records that have not.
  prefs: []
  type: TYPE_NORMAL
- en: Expectation composition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each expectation is comprised of three major components: a description, a Boolean
    expression, and an action to take.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The main components of a DLT expectation](img/B22011_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The main components of a DLT expectation
  prefs: []
  type: TYPE_NORMAL
- en: An expectation is declared using a DLT function decorator. The function decorator
    specifies the type of action that should be taken whenever a particular constraint
    or set of constraints eval uates to **False** . Additionally, the function decorator
    accepts two input parameters, a short description that describes the data quality
    constraint and a Boolean expression that must evaluate to **True** for a row to
    be marked as passing validation.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – writing your first data quality expectation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get a feel for the DLT syntax, let’s work through a real-world example of
    writing a data pipeline for a New York City cab company called the Yellow Taxi
    Corporation. We’ll write a simple data pipeline, enforcing a data quality constraint
    that can be applied to our incoming NYC Taxi data and warn us when there are records
    that don’t adhere to our data quality specifications. In this scenario, we want
    to ensure that the incoming trip data does not have any trips with a negative
    total amount, since it would not be possible for our cab drivers to owe the riders
    any money.
  prefs: []
  type: TYPE_NORMAL
- en: Generating taxi trip data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin by logging into our Databricks workspace. For this exercise, you
    will need to use the accompanying NYC Yellow Taxi trip data generator, which can
    be downloaded from the chapter’s GitHub repo. Either import the data generator
    notebook into your Databricks workspace or create a new Python notebook with the
    following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll need to download the **dbldatagen** Python library, which will
    help us randomly generate new taxi trip data. Add the following code snippet to
    your notebook, which uses the **%pip** magic command to download the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the library has been installed, let’s define a Python function for
    generating new taxi trip data according to our schema. We’ll specify columns for
    typical taxi trip details, including the number of passengers, the fare amount,
    the trip distance, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve defined a way to randomly generate new trip data, we’ll need
    to define a location to store the new data so that it can be processed by a DLT
    pipeline. In a new notebook cell, let’s create an empty directory on the **Databricks
    File System** ( **DBFS** ) for storing our trip data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we’ll need a way to tie everything together. In a new notebook cell,
    add the following **for** loop, which will call the **generate_taxi_trip_data**
    function and write the data to the DBFS location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, create an all-purpose cluster to execute the trip data generator notebook.
    Once the all-purpose cluster has been created, navigate to the new notebook and
    click the cluster dropdown in the top navigation bar of the Databricks Data Intelligence
    Platform. Select the name of the cluster you created and select **Attach** to
    attach the trip data generator notebook to the cluster and execute all the cells.
    The taxi trip data generator will append several new JSON files containing the
    randomly generated trip data to the DBFS location.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new DLT pipeline definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve generated new data, let’s create another new notebook for our
    DLT pipeline definition. Navigate to the workspace tab on the sidebar, drill down
    to your user’s home directory, and create a new notebook by right-clicking and
    selecting **Add Notebook** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Give the new notebook a meaningful name such as **Chapter 3** **– Enforcing
    Data Quality** . Begin by importing the DLT Python module as well as the PySpark
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s define a bronze table, **yellow_taxi_raw** , that will ingest the
    taxi trip data that was written to the DBFS location by our taxi trip data generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For the next layer of our data pipeline, the stakeholders within our organization
    have asked us to provide a way for their business to report real-time financial
    analytics of our incoming trip data. As a result, let’s add a silver table that
    will transform the incoming stream of trip data, calculating the expected profits
    and losses of our cab company, Yellow Taxi Corporation. In this example, we’re
    going to take the total amount that was paid by the passengers and begin to calculate
    how that money is allocated to fund different parts of the business and calculate
    potential profits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define our silver table definition, **trip_data_financials** . The table
    definition begins just like any normal streaming table definition. We begin by
    defining a Python function that returns a streaming table. Next, we use the DLT
    function annotations to declare this function as a streaming table with an optional
    name, **trip_data_financials** , as well as a comment with descriptive text about
    the streaming table. Create a new notebook cell, adding the following DLT dataset
    definition for the silver table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One thing that you may have noticed in our silver table declaration is a new
    function decorator for enforcing a data quality constraint. In this case, we want
    to ensure that the total amount reported in our trip data is greater than zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'When our data pipeline is triggered to run and update the bronze and silver
    datasets, the DLT system will inspect each row that is processed and evaluate
    whether the Boolean expression for our data quality constraint evaluates to **True**
    for the row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Within the body of the function definition, we are using the built-in PySpark
    **withColumn()** and **expr()** functions to add four new columns to the output
    of our bronze table – **driver_payment** , **vehicle_maintenance_fee** , **adminstrative_fee**
    , and **potential_profits** . These columns are calculated by taking a percentage
    of the original **trip_amount** column. In business terms, we are splitting the
    total amount that was collected from the passengers into the driver’s payment,
    fees collected to run the company, and potential profits for the company.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we’ll look at the different types of actions that
    the DLT system will take if an expectation Boolean expression is evaluated to
    **False** . By default, the DLT system will simply record that the row failed
    the Boolean expression for a particular row in the system logs and record the
    data quality metrics in the system. In our silver table declaration, let’s assume
    the default behavior of logging a warning message.
  prefs: []
  type: TYPE_NORMAL
- en: Running the data pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s create a new data pipeline from our dataset declarations in our notebook.
    Execute the notebook cells and ensure that there are no syntax errors. Next, the
    Databricks Data Intelligence Platform will prompt you to create a new data pipeline.
    Click the **Create pipeline** button to create a new DLT data pipeline. Next,
    under the **Destination** settings, select a catalog and schema in Unity Catalog
    where you would like to store the pipeline datasets. Under the **Compute** settings,
    set **Min workers** to **1** and **Max workers** to **2** . Accept the defaults
    by clicking the **Create** button. Finally, click the **Start** button to execute
    the data pipeline. You will be taken to a visual representation of the dataflow
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The dataflow graph for our NYC Yellow Taxi Corp. pipeline](img/B22011_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – The dataflow graph for our NYC Yellow Taxi Corp. pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, the DLT system will begin by creating and initializing a
    new Databricks cluster and begin parsing the dataset definitions in our notebook
    into a dataflow graph. As you can see, the DLT system will ingest the raw trip
    data files from our DBFS location into the streaming table, **yellow_taxi_raw**
    . Next, the system detects the dependency of our silver table, **trip_data_financials**
    , and will immediately begin calculating our additional four columns in our silver
    table. Along the way, our data quality constraint is being evaluated on the incoming
    data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the data quality in real time. Click on the silver table, and
    the DLT UI will expand a pane on the right-hand side summarizing the silver table.
    Click on the **Data quality** tab to view the data quality metrics. Notice that
    the graph is being updated in real time as our data is processed. Of all the data
    that has been processed by the data pipeline, you’ll notice that around 10% has
    failed the **valid_total_amount** expectation – which is expected. The data generator
    notebook will purposely publish records with a negative total amount to our cloud
    storage location. We can easily see how much of our data is validating against
    our defined data quality criteria and how much is not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The DLT UI will summarize the data quality metrics of our data
    pipeline in real time](img/B22011_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The DLT UI will summarize the data quality metrics of our data
    pipeline in real time
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve written your first data quality constraint in Delta
    Live Tables. By now, you should see just how easy yet powerful the DLT framework
    is. In just a few lines of code, we’re able to enforce data quality constraints
    on our incoming data, as well as to monitor the data quality in real time. This
    gives data engineering teams more control over their data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see how data engineering teams can leverage DLT expectations
    to react to potential data quality issues before leading to potential data corruption.
  prefs: []
  type: TYPE_NORMAL
- en: Acting on failed expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three types of actions that DLT can take when a particular record
    violates the data constraints defined on a DLT dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warn** : When DLT encounters an expression violation, the record will be
    recorded as a metric and will continue to be written to the downstream target
    dataset .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drop** : When DLT encounters an expression violation, the record will be
    recorded as a metric and will be prevented from entering the downstream target
    dataset .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fail** : When DLT encounters an expression violation, the pipeline update
    will fail entirely until a data engineering team member can investigate and correct
    the data violation or possible data corruption .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should always choose one of the actions based on the individual use case
    and on how you want to handle data that does not meet data quality rules. For
    example, there may be times when data does not meet the defined data quality constraints
    but logging the violating rows in the DLT system and monitoring the data quality
    meets the requirements for a particular use case. On the other hand, there may
    be scenarios where specific data quality constraints must be met, otherwise the
    incoming data will break downstream processes. In that scenario, more aggressive
    action such as failing the data pipeline run and rolling back transactions is
    the appropriate behavior. In either scenario, the Delta Live Tables framework
    gives data engineering teams full control to decide the fate of violating rows
    and the power to define how the system should react.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on example – failing a pipeline run due to poor data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There may be scenarios when you want to immediately halt the execution of a
    data pipeline update to intervene and correct the data, for example. In this case,
    DLT expectations offer the ability to immediately fail a data pipeline run using
    the **@dlt.expect_or_fail()** function decorator.
  prefs: []
  type: TYPE_NORMAL
- en: If the operation is a table update, the transaction is immediately rolled back
    to prevent contamination of bad data. Furthermore, DLT will track additional metadata
    about processed records so that data engineering teams can pinpoint which record
    in the dataset caused the failure.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we can update the earlier example of our Yellow Taxi Corporation
    data pipeline. In this scenario, having a negative total amount would break downstream
    financial reports. In this case, rather than simply record the rows that violate
    the expectation, we’d like to fail the pipeline run, so that our data engineering
    team can investigate potential issues in the data and take appropriate action
    such as the manual correction of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Delta Live Tables framework, adjusting the behavior of our data pipeline
    is as simple as updating the function decorator of our silver table definition.
    Let’s update the expectation with the **expect_or_fail** action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The full dataset definition for the silver table, **trip_data_financials**
    , should look like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s rerun the trip data generator to append additional files to the
    raw landing zone in the Databricks file system. Once the trip data generator has
    finished, navigate back to the Yellow Taxi Corporation data pipeline created earlier
    and click the **Start** button to trigger another execution of the data pipeline.
    For this chapter’s examples, the trip data generator will randomly generate trip
    data with negative total amounts.
  prefs: []
  type: TYPE_NORMAL
- en: You should observe for this run of the data pipeline that the data pipeline
    update failed with an error status.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – The dataflow graph will update to display an error when the
    data quality constraint is violated](img/B22011_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – The dataflow graph will update to display an error when the data
    quality constraint is violated
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the failure message, you can see that the cause of the pipeline failure
    was a violation of the expectation constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – The data pipeline logs will display the failed update due to
    a violated expectation check](img/B22011_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – The data pipeline logs will display the failed update due to a
    violated expectation check
  prefs: []
  type: TYPE_NORMAL
- en: Applying multiple data quality expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There may be times when a dataset author may want to apply more than one business
    rule or data quality constraint on each row of a dataset. In that event, DLT provides
    a special set of function decorators for specifying multiple data quality constraint
    d efinitions.
  prefs: []
  type: TYPE_NORMAL
- en: The **@dlt.expect_all()** function decorator can be used to combine more than
    one data quality constraint for a particular dataset. Similarly, **expect_all_or_drop()**
    can be specified when incoming data should be dropped from entering a target table
    unless all the criteria in the set of data quality constraints are satisfied.
    Lastly, **expect_all_or_fail()** will fail a run of a data pipeline if any of
    the criteria in a set of data quality constraints are not met by the incoming
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how we might drop invalid taxicab trip data entries from entering
    downstream datasets in our pipeline when the values don’t pass the validation
    criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we’ve defined a set of data constraints using the
    expectations function decorators and we are applying them collectively to the
    incoming data. Let’s imagine that losing a few records of the taxicab trip data
    will not pose a threat to downstream processes. As a result, we’ve decided to
    drop records that don’t pass the validation step in our expectation declaration.
    With just a few extra lines of configuration, our data pipeline can enforce data
    quality constraints on the incoming data and automatically react to data that
    doesn’t pass the defined criteria.
  prefs: []
  type: TYPE_NORMAL
- en: While we’ve only looked at data within the context of our DLT data pipeline,
    let’s see how the DLT framework can validate data across multiple systems of data.
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling expectations from a DLT pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we’ve only worked with defining data quality constraints within
    the table definition. However, there may be scenarios when you’d like to decouple
    the data quality constraints from data pipeline definitions, allowing the data
    engineering teams to work separately from the data analyst teams. This is especially
    useful when a group of non-technical individuals determine the data quality criteria.
    Furthermore, this design also provides even more flexibility to maintain and change
    business rules as the business changes. For example, a real-world example would
    be validating seasonal discount codes that change over time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that we have a group of non-technical business analysts who would
    like to interact with the data quality constraints using a UI such as a web portal
    in a browser window. In that case, we can load and save our data quality constraints
    into a separate Delta table and then dynamically load the data quality constraints
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by defining a data quality rules table. We’ll introduce three columns:
    a column for the rule name, a column defining the data quality rule expression,
    and a column identifying the dataset name – everything needed to create an expectation
    using DLT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s revisit the previous example for specifying multiple expectations using
    a Python dictionary. In that example, we defined a **dict** data structure called
    **assertions** . In this example, let’s convert it into a tabular format, inserting
    the entries into our Delta table. Add the following SQL statement to a new notebook
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, within the data pipeline notebook, we can create a helper function that
    will read directly from our data quality rules table and translate each row to
    a format that the DLT Expectation can interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a Delta table that our non-technical data analysts can update using
    a UI separate from our data pipeline, and we also have a helper function that
    can read from the Delta table and translate the entries into a format that a DLT
    expectation can interpret. Let’s see how these pieces tie together to create a
    new dataset in our pipeline that dynamically loads the data quality requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This design pattern provides the flexibility to maintain the data quality rules
    separately from the data pipeline definition so that non-technical individuals
    determine the data quality criteria. But what if we have a technical group of
    individuals who want to stay involved in the quality of the data passing through
    our data pipeline? Moreover, what if this group of individuals needs to be notified
    of poor-quality data so that they can intervene and even manually correct the
    data for the downstream processes to function? Let’s take a look at how we might
    implement such a recovery process in the next hands-on exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – quarantining bad data for correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we’re going to build a conditional data flow for data that
    doesn’t meet our data quality requirements. This will allow us to isolate the
    data that violates our data quality rules so that we can take appropriate action
    later or even report on the data that violates the data quality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use the same Yellow Taxi Corporation example to illustrate building a
    data quarantine zone concept. Let’s start off with a bronze table that ingests
    the raw JSON data written to the DBFS location by the trip data generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s begin by defining a few data quality rules on incoming data. Let’s
    make sure that the trip data published to our DBFS location is sensible. We’ll
    ensure that the total fare amount is greater than $ **0** and that the ride has
    at least **1** passenger, otherwise, we’ll quarantine the trip data for further
    review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s apply the two data quality rules to the incoming data by creating
    another dataset with a calculated column, **is_valid** . This column will contain
    the results of the data quality rules evaluated for each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can use the **is_valid** calculated column to split the streaming
    table into two data flows – a data flow for all incoming data that has passed
    the data quality assertions and a separate data flow for the incoming data that
    has not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a quarantine table in our data pipeline that will route the data
    according to the evaluated data quality rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Finally, create a new DLT pipeline using the new notebook as the source. Provide
    a meaningful name for the pipeline, such as **Chapter 3** **Quarantining Invalid
    Data** . Select **Core** as the product edition and **Triggered** as the execution
    mode. Next, select a target catalog and schema in Unity Catalog to store the pipeline
    datasets. Accept the remaining default values and click the **Create** button
    to create the new DLT pipeline. Finally, click on the **Start** button to trigger
    a new pipeline execution run. Notice how the data is split into two downstream
    tables – one table containing the rows that passed the data quality rules, and
    a quarantine table containing the rows that have failed the data quality rules.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Data that fails data quality rules is split into a quarantine
    table](img/B22011_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Data that fails data quality rules is split into a quarantine table
  prefs: []
  type: TYPE_NORMAL
- en: By implementing a quarantine table, we can report on the real-time metrics so
    that stakeholders within our organization can be kept up to date on the quality
    of our incoming data. Furthermore, the data stewards of our lakehouse can review
    the data that has not passed the validation logic and even take appropriate action,
    such as manually correcting the invalid data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a lot of topics surrounding the data quality of
    the data in our lakehouse. We learned how the integrity of a table can be enforced
    using **NOT NULL** and **CHECK** constraints in Delta Lake. We also defined relationships
    between the tables in our lakehouse using **PRIMARY KEY** and **FOREIGN KEY**
    constraints. Next, we saw how we could enforce primary key uniqueness across our
    Delta tables using views to validate the data in our tables. We also saw just
    how easy it was to update the behavior of our data pipeline when incoming rows
    violated data quality constraints, allowing data engineering teams to react to
    downstream processes that have the potential to break from poor-quality data.
    Finally, we saw a practical example of how we can use expectations to create a
    conditional data flow in our pipeline, allowing our data stewards to quarantine
    and correct data that doesn’t meet the expected data quality.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to get into more advanced topics of maintaining
    data pipelines in production. We’ll see how we can tune many different aspects
    of data pipelines to scale to large volumes of data and meet real-time stream
    processing demands such as high throughput and low latency.
  prefs: []
  type: TYPE_NORMAL
