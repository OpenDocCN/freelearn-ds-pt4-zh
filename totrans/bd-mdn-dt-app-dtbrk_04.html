<html><head></head><body>
  <div id="_idContainer044">
   <h1 class="chapter-number" id="_idParaDest-83">
    <a id="_idTextAnchor103">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     4
    </span>
   </h1>
   <h1 id="_idParaDest-84">
    <a id="_idTextAnchor104">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Scaling DLT Pipelines
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In this chapter, we’re going to look at several methods for scaling your
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      Delta Live Tables
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      DLT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     ) pipelines
    </span>
    <a id="_idIndexMarker176">
    </a>
    <span class="koboSpan" id="kobo.8.1">
     to handle the processing demands of a typical production environment.
    </span>
    <span class="koboSpan" id="kobo.8.2">
     We’ll cover several aspects of tuning your DLT pipelines, from optimizing the DLT cluster settings so that your pipelines can quickly scale to handle the spikes of heavy processing demand to looking at ways we can optimize the data layout of the underlying tables in cloud storage.
    </span>
    <span class="koboSpan" id="kobo.8.3">
     By the end of this chapter, you should have mastered how DLT clusters can automatically scale out to handle demand.
    </span>
    <span class="koboSpan" id="kobo.8.4">
     You should also have a good understanding of the impact that table maintenance tasks, which are automatically run in the background by the DLT system, have on the performance of your data pipelines.
    </span>
    <span class="koboSpan" id="kobo.8.5">
     Lastly, you should understand how to leverage Delta Lake optimization techniques to further improve the execution performance of your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.9.1">
      DLT pipelines.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.10.1">
     We’re going to cover the following main topics in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.11.1">
      this chapter:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.12.1">
      Scaling compute to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.13.1">
       handle demand
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.14.1">
      Hands-on example – setting autoscaling properties using the Databricks
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.15.1">
       REST API
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.16.1">
      Automated table
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.17.1">
       maintenance tasks
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.18.1">
      Optimizing table layouts for faster
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.19.1">
       table updates
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.20.1">
      Serverless
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.21.1">
       DLT pipelines
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.22.1">
      Introducing Enzyme, a performance
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.23.1">
       optimization layer
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-85">
    <a id="_idTextAnchor105">
    </a>
    <span class="koboSpan" id="kobo.24.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.25.1">
     To follow along with this chapter, you will need Databricks workspace permissions to create and start an all-purpose cluster, as well as access to create a new DLT pipeline using at least a cluster policy.
    </span>
    <span class="koboSpan" id="kobo.25.2">
     All code samples can be downloaded from this chapter’s GitHub repository located at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter04">
     <span class="koboSpan" id="kobo.26.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter04
     </span>
    </a>
    <span class="koboSpan" id="kobo.27.1">
     .
    </span>
    <span class="koboSpan" id="kobo.27.2">
     This chapter will create and run several new notebooks, as well as a new DLT pipeline using the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.28.1">
      Core
     </span>
    </strong>
    <span class="koboSpan" id="kobo.29.1">
     product edition.
    </span>
    <span class="koboSpan" id="kobo.29.2">
     As a result, the code samples in this chapter are estimated to consume
    </span>
    <a id="_idIndexMarker177">
    </a>
    <span class="koboSpan" id="kobo.30.1">
     around 10-15
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.31.1">
      Databricks
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.32.1">
       Units
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.34.1">
       DBUs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.35.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-86">
    <a id="_idTextAnchor106">
    </a>
    <span class="koboSpan" id="kobo.36.1">
     Scaling compute to handle demand
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.37.1">
     Different portions
    </span>
    <a id="_idIndexMarker178">
    </a>
    <span class="koboSpan" id="kobo.38.1">
     of a data pipeline may involve heavy computation as calculations are performed, while other sections of the pipeline don’t require as much processing power.
    </span>
    <span class="koboSpan" id="kobo.38.2">
     To yield the best performance while simultaneously optimizing costs, it’s important for any data pipeline to be able to add additional processing power when needed, as well as release computational resources when processing demands shrink over time.
    </span>
    <span class="koboSpan" id="kobo.38.3">
     Fortunately, Databricks features a built-in autoscaling feature for DLT pipelines, so that
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.39.1">
      virtual machines
     </span>
    </strong>
    <span class="koboSpan" id="kobo.40.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.41.1">
      VMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.42.1">
     ) can be
    </span>
    <a id="_idIndexMarker179">
    </a>
    <span class="koboSpan" id="kobo.43.1">
     added to and removed from a pipeline cluster to match the processing demands of a data pipeline during its
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.44.1">
      execution period.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.45.1">
     In fact, Databricks offers two types of cluster autoscaling modes for DLT pipelines: legacy and enhanced.
    </span>
    <span class="koboSpan" id="kobo.45.2">
     Both autoscaling modes will automatically add or remove VMs as processing demands increase or decrease throughout a pipeline run.
    </span>
    <span class="koboSpan" id="kobo.45.3">
     However,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.46.1">
      when
     </span>
    </em>
    <span class="koboSpan" id="kobo.47.1">
     the VMs are added and removed differs between
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.48.1">
      the two.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.49.1">
     With legacy autoscaling mode, a pipeline cluster will add additional VMs when there has been an increase in processing demand over a sustained period of time.
    </span>
    <span class="koboSpan" id="kobo.49.2">
     Furthermore, in legacy autoscaling mode, a pipeline cluster will scale down only when VMs are left idle for a period of time and they have no currently executing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.50.1">
      Spark tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.51.1">
     On the other hand, with enhanced autoscaling mode, the DLT system will only add additional VMs if the system
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.52.1">
      predicts
     </span>
    </em>
    <span class="koboSpan" id="kobo.53.1">
     that adding additional compute resources would speed up the execution of the pipeline update – for example, if the Spark jobs are limited by the number of available CPU cores and would benefit from having additional CPUs to execute a large amount of Spark tasks in parallel.
    </span>
    <span class="koboSpan" id="kobo.53.2">
     In addition, the enhanced autoscaling feature will proactively look for opportunities for the pipeline cluster to scale down, evicting running Spark tasks and reducing cloud operational costs.
    </span>
    <span class="koboSpan" id="kobo.53.3">
     During the eviction process, the enhanced autoscaling mode will ensure that evicted Spark tasks are recovered successfully on the remaining, running VMs before terminating the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.54.1">
      over-provisioned VMs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.55.1">
     Lastly, enhanced autoscaling is only available for clusters used in pipeline update tasks, while the legacy autoscaling mode is used by the DLT system to execute
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.56.1">
      maintenance tasks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.57.1">
     The following table outlines the differences between the two types of autoscaling modes available for
    </span>
    <a id="_idIndexMarker180">
    </a>
    <span class="koboSpan" id="kobo.58.1">
     DLT pipeline clusters, as well as which DLT tasks are available for each of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.59.1">
      autoscaling modes.
     </span>
    </span>
   </p>
   <table class="No-Table-Style _idGenTablePara-1" id="table001-3">
    <colgroup>
     <col/>
     <col/>
     <col/>
     <col/>
     <col/>
    </colgroup>
    <tbody>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.60.1">
           Autoscaling Mode
          </span>
         </strong>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <strong class="bold">
         <span class="koboSpan" id="kobo.61.1">
          Predictive
         </span>
        </strong>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.62.1">
           Autoscaling
          </span>
         </strong>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <strong class="bold">
         <span class="koboSpan" id="kobo.63.1">
          Proactive
         </span>
        </strong>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.64.1">
           Down Scaling
          </span>
         </strong>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.65.1">
           Update Tasks
          </span>
         </strong>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <strong class="bold">
          <span class="koboSpan" id="kobo.66.1">
           Maintenance Tasks
          </span>
         </strong>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.67.1">
          Legacy
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.68.1">
          ✖️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.69.1">
          ✖️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.70.1">
          ✔️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.71.1">
          ✔️
         </span>
        </span>
       </p>
      </td>
     </tr>
     <tr class="No-Table-Style">
      <td class="No-Table-Style">
       <p>
        <span class="No-Break">
         <span class="koboSpan" id="kobo.72.1">
          Enhanced
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.73.1">
          ✔️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.74.1">
          ✔️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.75.1">
          ✔️
         </span>
        </span>
       </p>
      </td>
      <td class="No-Table-Style">
       <p>
        <span lang="en-US" xml:lang="en-US">
         <span class="koboSpan" id="kobo.76.1">
          ✖️
         </span>
        </span>
       </p>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.77.1">
     Table 4.1 – The differences between autoscaling modes available on DLT pipeline clusters
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.78.1">
     You can configure the cluster autoscaling mode from either the DLT UI or the Databricks REST API.
    </span>
    <span class="koboSpan" id="kobo.78.2">
     In the
    </span>
    <a id="_idIndexMarker181">
    </a>
    <span class="koboSpan" id="kobo.79.1">
     next section, let’s use the Databricks REST API to update the autoscaling mode of an existing data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.80.1">
      pipeline cluster.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-87">
    <a id="_idTextAnchor107">
    </a>
    <span class="koboSpan" id="kobo.81.1">
     Hands-on example – setting autoscaling properties using the Databricks REST API
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.82.1">
     In this section, you’ll
    </span>
    <a id="_idIndexMarker182">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     need to download the code samples from this chapter’s GitHub repository located at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter04">
     <span class="koboSpan" id="kobo.84.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter04
     </span>
    </a>
    <span class="koboSpan" id="kobo.85.1">
     .
    </span>
    <span class="koboSpan" id="kobo.85.2">
     Within the chapter’s GitHub repository is a helper notebook titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.86.1">
      Random Taxi Trip Data Generator.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.87.1">
     , which we’ll use to generate random bursts of data to a cloud storage landing zone, simulating the unpredictable behavior you could expect in a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.88.1">
      production environment.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.89.1">
     First, let’s begin by importing this chapter’s pipeline definition notebook, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.90.1">
      Taxi Trip Data Pipeline.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.91.1">
     , into your Databricks workspace and opening
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.92.1">
      the notebook.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.93.1">
     You will notice that we’ve defined two datasets within our data pipeline.
    </span>
    <span class="koboSpan" id="kobo.93.2">
     The first dataset uses the Databricks Auto Loader feature to ingest new JSON files as they arrive in our raw landing zone.
    </span>
    <span class="koboSpan" id="kobo.93.3">
     Once the data has been ingested, a second dataset – our silver table – will contain the result of the transformed taxi trip data with additional columns containing the financial analytics of the taxi
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.94.1">
      trip data:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.95.1">
@dlt.table(
    name="random_trip_data_raw",
    comment="The raw taxi trip data ingested from a landing zone.",
    table_properties={
        "quality": "bronze"
    }
)
def random_trip_data_raw():
    raw_trip_data_schema = StructType([
        StructField('Id', IntegerType(), True),
        StructField('driver_id', IntegerType(), True),
        StructField('Trip_Pickup_DateTime',
                    TimestampType(), True),
        StructField('Trip_Dropoff_DateTime',
                    TimestampType(), True),
        StructField('Passenger_Count', IntegerType(), True),
        StructField('Trip_Distance', DoubleType(), True),
        StructField('Start_Lon', DoubleType(), True),
        StructField('Start_Lat', DoubleType(), True),
        StructField('Rate_Code', StringType(), True),
        StructField('store_and_forward', IntegerType(), True),
        StructField('End_Lon', DoubleType(), True),
        StructField('End_Lat', DoubleType(), True),
        StructField('Payment_Type', StringType(), True),
        StructField('Fare_Amt', DoubleType(), True),
        StructField('surcharge', DoubleType(), True),
        StructField('mta_tax', StringType(), True),
        StructField('Tip_Amt', DoubleType(), True),
        StructField('Tolls_Amt', DoubleType(), True),
        StructField('Total_Amt', DoubleType(), True)
    ])
    return (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .schema(raw_trip_data_schema)
        .load(raw_landing_zone))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.96.1">
     Next, attach the
    </span>
    <a id="_idIndexMarker183">
    </a>
    <span class="koboSpan" id="kobo.97.1">
     notebook to an all-purpose cluster and execute all the notebook cells.
    </span>
    <span class="koboSpan" id="kobo.97.2">
     Ensure that all the notebook cells are executed successfully.
    </span>
    <span class="koboSpan" id="kobo.97.3">
     When prompted, create a new DLT pipeline using the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.98.1">
      Core
     </span>
    </strong>
    <span class="koboSpan" id="kobo.99.1">
     product edition.
    </span>
    <span class="koboSpan" id="kobo.99.2">
     Select
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.100.1">
      Continuous
     </span>
    </strong>
    <span class="koboSpan" id="kobo.101.1">
     processing mode as the pipeline execution mode.
    </span>
    <span class="koboSpan" id="kobo.101.2">
     (If you need a refresher, please consult the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.102.1">
      Data pipeline settings
     </span>
    </em>
    <span class="koboSpan" id="kobo.103.1">
     section of
    </span>
    <a href="B22011_02.xhtml#_idTextAnchor052">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.104.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.105.1">
     in this book.) Next, select a target Unity Catalog destination to store the output of the data pipeline datasets and accept all the remaining default values.
    </span>
    <span class="koboSpan" id="kobo.105.2">
     Finally, note the pipeline ID of the newly created data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.106.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.107.1">
     For the next part of this exercise, we’ll use a popular Python library,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.108.1">
      requests
     </span>
    </strong>
    <span class="koboSpan" id="kobo.109.1">
     , to interact with the Databricks REST API.
    </span>
    <span class="koboSpan" id="kobo.109.2">
     Create a new notebook within your Databricks workspace and begin by importing the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.110.1">
      requests
     </span>
    </strong>
    <span class="koboSpan" id="kobo.111.1">
     library in the first cell of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.112.1">
      our notebook:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.113.1">
import requests</span></pre>
   <p>
    <span class="koboSpan" id="kobo.114.1">
     Next, let’s create a new request to the Databricks REST API for updating the cluster settings of our data pipeline.
    </span>
    <span class="koboSpan" id="kobo.114.2">
     Within the request payload, we’ll specify the autoscaling mode, the minimum number of worker nodes for our pipeline cluster, as well as the maximum number of worker nodes.
    </span>
    <span class="koboSpan" id="kobo.114.3">
     As per the public Databricks documentation, we’ll also need to use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.115.1">
      PUT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.116.1">
     verb for updating the settings of our DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.116.2">
     Add the following code
    </span>
    <a id="_idIndexMarker184">
    </a>
    <span class="koboSpan" id="kobo.117.1">
     snippet to the newly created notebook, updating the variables with your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.118.1">
      environment-specific values:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.119.1">
databricks_workspace_url = "&lt;your_databricks_workspace&gt;"
pipeline_id = "&lt;your_pipeline_id&gt;"
pat_token = "&lt;your_api_token&gt;"
response = requests.put(
    f"{databricks_workspace_url}/api/2.0/pipelines/{pipeline_id}",
    hea</span><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.120.1">ders={"Authentication": pat_token},
    json={
        ...
</span><span class="koboSpan" id="kobo.120.2">        "clusters":[</span><strong class="bold"><span class="koboSpan" id="kobo.121.1">{</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.122.1">            "autoscale": {</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.123.1">                "min_workers": 2,</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.124.1">                "max_workers": 5,</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.125.1">                "mode": "ENHANCED"</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.126.1">            }</span></strong><span class="koboSpan" id="kobo.127.1">
        }]
        ...
</span><span class="koboSpan" id="kobo.127.2">    }
)
print(response.json())</span></pre>
   <p>
    <span class="koboSpan" id="kobo.128.1">
     Alternatively, you can update the autoscaling mode to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.129.1">
      ENHANCED
     </span>
    </strong>
    <span class="koboSpan" id="kobo.130.1">
     for the pipeline by navigating to the pipeline settings from the DLT UI.
    </span>
    <span class="koboSpan" id="kobo.130.2">
     Now that we’ve updated our DLT pipeline to use enhanced autoscaling, let’s execute a pipeline update.
    </span>
    <span class="koboSpan" id="kobo.130.3">
     Navigate to the data pipeline UI of the newly created data pipeline.
    </span>
    <span class="koboSpan" id="kobo.130.4">
     At the top right, select the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.131.1">
      Start
     </span>
    </strong>
    <span class="koboSpan" id="kobo.132.1">
     button to trigger a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.133.1">
      pipeline update.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.134.1">
     Meanwhile, let’s
    </span>
    <a id="_idIndexMarker185">
    </a>
    <span class="koboSpan" id="kobo.135.1">
     also simulate spikes in processing demand using a random data generator.
    </span>
    <span class="koboSpan" id="kobo.135.2">
     Import the data generator notebook, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.136.1">
      Random Taxi Trip Data Generator.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.137.1">
     , from the chapter’s GitHub repository.
    </span>
    <span class="koboSpan" id="kobo.137.2">
     As the name suggests,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.138.1">
      Random Taxi Trip Data Generator.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.139.1">
     will randomly generate new taxi trip data with varying degrees of volume and frequency, simulating a typical workload in a production environment.
    </span>
    <span class="koboSpan" id="kobo.139.2">
     Attach the notebook to an all-purpose cluster and click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.140.1">
      Run all
     </span>
    </strong>
    <span class="koboSpan" id="kobo.141.1">
     button to execute all the cells.
    </span>
    <span class="koboSpan" id="kobo.141.2">
     Ensure that the notebook cells have all
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.142.1">
      completed successfully.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.143.1">
     Next, switch back to the DLT UI for the pipeline we created.
    </span>
    <span class="koboSpan" id="kobo.143.2">
     We’ll monitor the event log of our pipeline to ensure that our DLT cluster will automatically increase the number of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.144.1">
      worker instances.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer035">
     <span class="koboSpan" id="kobo.145.1">
      <img alt="Figure 4.1 – Autoscaling events will be recorded in the event log from the DLT UI" src="image/B22011_04_001.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.146.1">
     Figure 4.1 – Autoscaling events will be recorded in the event log from the DLT UI
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.147.1">
     Similarly, monitor the event log to ensure that the DLT update cluster scales back down after the flow of additional data terminates and processing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.148.1">
      demand dwindles.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.149.1">
     By now, you should have a strong foundation in understanding how DLT clusters scale up and down to accommodate the peaks and valleys in processing demands.
    </span>
    <span class="koboSpan" id="kobo.149.2">
     As you can see, our DLT pipeline will only provision the compute it needs to efficiently keep our datasets up to date and then release additional compute instances to minimize our operational
    </span>
    <a id="_idIndexMarker186">
    </a>
    <span class="koboSpan" id="kobo.150.1">
     costs.
    </span>
    <span class="koboSpan" id="kobo.150.2">
     Let’s turn our attention to other efficiencies that the DLT system does automatically for us, such as how the DLT system will automatically maintain the optimal state of our underlying
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.151.1">
      Delta tables.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-88">
    <a id="_idTextAnchor109">
    </a>
    <span class="koboSpan" id="kobo.152.1">
     Automated table maintenance tasks
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.153.1">
     As mentioned in
    </span>
    <a id="_idIndexMarker187">
    </a>
    <span class="koboSpan" id="kobo.154.1">
     previous chapters, each
    </span>
    <a id="_idIndexMarker188">
    </a>
    <span class="koboSpan" id="kobo.155.1">
     DLT pipeline will be associated with two clusters – one cluster for performing updates to each of the datasets in a pipeline definition, as well as another cluster for performing maintenance activities to each dataset.
    </span>
    <span class="koboSpan" id="kobo.155.2">
     These maintenance tasks include executing the Delta
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.156.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.157.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.158.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.159.1">
     operations for each Delta table contained within a data pipeline definition.
    </span>
    <span class="koboSpan" id="kobo.159.2">
     Previously, data engineers would be responsible for creating and maintaining a separate Databricks workflow that would execute the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.160.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.161.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.162.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.163.1">
     commands for each Delta table, typically scheduled to run nightly.
    </span>
    <span class="koboSpan" id="kobo.163.2">
     As you can imagine, as you begin to add more and more tables to a pipeline, this can turn out to be quite a cumbersome task.
    </span>
    <span class="koboSpan" id="kobo.163.3">
     Fortunately, the DLT framework does this heavy lifting for us right out of the box.
    </span>
    <span class="koboSpan" id="kobo.163.4">
     Furthermore, each
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.164.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.165.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.166.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.167.1">
     maintenance activity is executed within 24 hours of the last pipeline
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.168.1">
      execution run.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.169.1">
     Let’s look at each
    </span>
    <a id="_idIndexMarker189">
    </a>
    <span class="koboSpan" id="kobo.170.1">
     operation individually to understand what overall benefit the maintenance tasks have on the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.171.1">
      underlying datasets.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-89">
    <a id="_idTextAnchor110">
    </a>
    <span class="koboSpan" id="kobo.172.1">
     Why auto compaction is important
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.173.1">
     During each new
    </span>
    <a id="_idIndexMarker190">
    </a>
    <span class="koboSpan" id="kobo.174.1">
     update run for a particular DLT pipeline, the DLT pipeline will initialize a dataflow graph and perform the underlying calculations spelled out in each dataset definition.
    </span>
    <span class="koboSpan" id="kobo.174.2">
     As a result, new data is either appended or merged into a particular Delta table.
    </span>
    <span class="koboSpan" id="kobo.174.3">
     Each time the data is written, Apache Spark will distribute the write operation out to the executors, potentially generating many small files as a result.
    </span>
    <span class="koboSpan" id="kobo.174.4">
     As more updates are executed, more of these small files are created on cloud storage.
    </span>
    <span class="koboSpan" id="kobo.174.5">
     As downstream processes read these Delta tables, they will need to expend a single Spark task for each unique file that answers a particular table query.
    </span>
    <span class="koboSpan" id="kobo.174.6">
     More files will result in more Spark tasks – or better put, more work that needs to be done by the Spark engine.
    </span>
    <span class="koboSpan" id="kobo.174.7">
     This is commonly referred to as the “small files problem,” as tables that experience heavy volumes of new data result in many small files, slowing down overall query performance.
    </span>
    <span class="koboSpan" id="kobo.174.8">
     As a remediation, it would be better to consolidate these small files into larger ones, a process referred to as
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.175.1">
      file compaction.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.176.1">
     Fortunately, as data engineers, we don’t need to write our own utility for combining smaller files into larger ones.
    </span>
    <span class="koboSpan" id="kobo.176.2">
     In fact, Delta Lake features a helpful command called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.177.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.178.1">
     for doing such maintenance tasks.
    </span>
    <span class="koboSpan" id="kobo.178.2">
     By default, the Delta Lake
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.179.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.180.1">
     command will attempt to coalesce smaller files into larger, 1
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.181.1">
      Gigabyte files.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer036">
     <span class="koboSpan" id="kobo.182.1">
      <img alt="Figure 4.2 – DLT will automatically run the OPTIMIZE command on Delta tables, coalescing smaller files into larger 1 GB files" src="image/B22011_04_002.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.183.1">
     Figure 4.2 – DLT will automatically run the OPTIMIZE command on Delta tables, coalescing smaller files into larger 1 GB files
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.184.1">
     Of course, you could choose
    </span>
    <a id="_idIndexMarker191">
    </a>
    <span class="koboSpan" id="kobo.185.1">
     to disable the auto-optimize feature by disabling the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.186.1">
      autoOptimize
     </span>
    </strong>
    <span class="koboSpan" id="kobo.187.1">
     table property in the table definition of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.188.1">
      DLT pipeline:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.189.1">
@dlt.table(
    name="random_trip_data_raw",
    comment="The raw taxi trip data ingested from a landing zone.",
    table_properties={
        "quality": "bronze",
        </span><strong class="bold"><span class="koboSpan" id="kobo.190.1">"pipelines.autoOptimize.managed": "false"</span></strong><span class="koboSpan" id="kobo.191.1">
    }
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.192.1">
     There might be certain scenarios when you would want to override the default behavior, such as implementing your own table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.193.1">
      optimization workflow.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.194.1">
     As each
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.195.1">
      OPTIMIZE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.196.1">
     maintenance activity is performed, it too will generate additional files for each Delta table.
    </span>
    <span class="koboSpan" id="kobo.196.2">
     To prevent cloud storage costs from ballooning out of control, we must also take
    </span>
    <a id="_idIndexMarker192">
    </a>
    <span class="koboSpan" id="kobo.197.1">
     care of removing obsolete table files so that as an organization, we aren’t paying for unnecessary
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.198.1">
      cloud storage.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-90">
    <a id="_idTextAnchor111">
    </a>
    <span class="koboSpan" id="kobo.199.1">
     Vacuuming obsolete table files
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.200.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.201.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.202.1">
     operation is
    </span>
    <a id="_idIndexMarker193">
    </a>
    <span class="koboSpan" id="kobo.203.1">
     designed to
    </span>
    <a id="_idIndexMarker194">
    </a>
    <span class="koboSpan" id="kobo.204.1">
     remove table files from previous versions of a Delta table that are no longer in the latest table snapshot and are older than the retention threshold property.
    </span>
    <span class="koboSpan" id="kobo.204.2">
     By default, the retention threshold for all Delta tables is seven days, meaning that the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.205.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.206.1">
     operation will remove obsolete table files that are older than seven days from the current snapshot date.
    </span>
    <span class="koboSpan" id="kobo.206.2">
     At runtime, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.207.1">
      VACUUM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.208.1">
     utility will search the Delta table’s root directory as well as all of the subdirectories, removing table files older than the retention threshold from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.209.1">
      cloud storage.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.210.1">
     This is a great way to balance both cloud storage costs with the ability to maintain and view older snapshots of a particular Delta table.
    </span>
    <span class="koboSpan" id="kobo.210.2">
     As mentioned in
    </span>
    <a href="B22011_01.xhtml#_idTextAnchor014">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.211.1">
        Chapter 1
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.212.1">
     , the time travel feature of Delta Lake relies upon the table history to query previous versions of a Delta table.
    </span>
    <span class="koboSpan" id="kobo.212.2">
     However, this feature was not designed to support long-term archival use cases, but rather shorter-term table history.
    </span>
    <span class="koboSpan" id="kobo.212.3">
     So, it’s reasonable to expect that we don’t need to store all the history of a Delta table and pay the associated storage costs, which could become
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.213.1">
      quite expensive.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.214.1">
     Like the auto-optimize feature, a Delta table’s history retention threshold is determined by a table property and can be specified in the table definition using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.215.1">
      deletedFileRetentionDuration
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.216.1">
      table property:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.217.1">
@dlt.table(
    name="random_trip_data_silver",
    comment="Taxi trip data transformed with financial data.",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.zOrderCols": "driver_id",
        </span><strong class="bold"><span class="koboSpan" id="kobo.218.1">"delta.deletedFileRetentionDuration": "INTERVAL 14 days"</span></strong><span class="koboSpan" id="kobo.219.1">
    }
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.220.1">
     Similarly, the Delta
    </span>
    <a id="_idIndexMarker195">
    </a>
    <span class="koboSpan" id="kobo.221.1">
     transaction logs – the metadata files that
    </span>
    <a id="_idIndexMarker196">
    </a>
    <span class="koboSpan" id="kobo.222.1">
     record details about each committed table transaction (covered in
    </span>
    <a href="B22011_01.xhtml#_idTextAnchor014">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.223.1">
        Chapter 1
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.224.1">
     ) – can also lead to unnecessary storage costs.
    </span>
    <span class="koboSpan" id="kobo.224.2">
     However, these log files are automatically removed during log checkpoint operations (every tenth transaction commit).
    </span>
    <span class="koboSpan" id="kobo.224.3">
     By default, Delta Lake will retain
    </span>
    <a id="_idTextAnchor112">
    </a>
    <span class="koboSpan" id="kobo.225.1">
     a maximum of 30 days’ worth of table history in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.226.1">
      transaction logs.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer037">
     <span class="koboSpan" id="kobo.227.1">
      <img alt="Figure 4.3 – DLT will automatically run a VACUUM operation on all Delta tables" src="image/B22011_04_003.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.228.1">
     Figure 4.3 – DLT will automatically run a VACUUM operation on all Delta tables
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.229.1">
     Since the transaction
    </span>
    <a id="_idIndexMarker197">
    </a>
    <span class="koboSpan" id="kobo.230.1">
     log files contain only metadata
    </span>
    <a id="_idIndexMarker198">
    </a>
    <span class="koboSpan" id="kobo.231.1">
     information, they are small, containing only a few megabytes of information.
    </span>
    <span class="koboSpan" id="kobo.231.2">
     However, this history retention can also be configured by setting the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.232.1">
      logRetentionDuration
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.233.1">
      table property:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.234.1">
@dlt.table(
    name="random_trip_data_silver",
    comment="Taxi trip data transformed with financial data.",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.zOrderCols": "driver_id",
        "delta.deletedFileRetentionDuration": "INTERVAL 9 days",
        </span><strong class="bold"><span class="koboSpan" id="kobo.235.1">"delta.logRetentionDuration": "INTERVAL 35 days"</span></strong><span class="koboSpan" id="kobo.236.1">
    }
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.237.1">
     Removing obsolete cloud files is a great way to control cloud costs and prevent your organization from paying for unnecessary cloud storage charges.
    </span>
    <span class="koboSpan" id="kobo.237.2">
     Let’s look at how we might be able to
    </span>
    <a id="_idIndexMarker199">
    </a>
    <span class="koboSpan" id="kobo.238.1">
     optimize
    </span>
    <a id="_idIndexMarker200">
    </a>
    <span class="koboSpan" id="kobo.239.1">
     other aspects of our DLT pipelines to improve operating efficiency while continuing to drive down
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.240.1">
      operating costs.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-91">
    <a id="_idTextAnchor113">
    </a>
    <span class="koboSpan" id="kobo.241.1">
     Moving compute closer to the data
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.242.1">
     One of the simplest
    </span>
    <a id="_idIndexMarker201">
    </a>
    <span class="koboSpan" id="kobo.243.1">
     methods for ensuring that your data pipelines will execute efficiently is to ensure that the DLT pipeline clusters are launched within the same global region as the data that is being processed.
    </span>
    <span class="koboSpan" id="kobo.243.2">
     This is an age-old tuning concept of moving the hardware closer to the data to minimize network latencies during data processing.
    </span>
    <span class="koboSpan" id="kobo.243.3">
     For example, you wouldn’t want your DLT pipeline cluster to execute in, say, the US West Region of a cloud provider, yet the data is stored in a completely different geographical location, such as the US East Region of the same cloud provider.
    </span>
    <span class="koboSpan" id="kobo.243.4">
     As a result, this will introduce a considerable amount of network latency to transfer the data across geographical regions, process the data transformations or other calculations, and then store the result back in the original geographical region.
    </span>
    <span class="koboSpan" id="kobo.243.5">
     Furthermore, most cloud providers will assess data egress and ingress charges associated with the geographical
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.244.1">
      data transfer.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer038">
     <span class="koboSpan" id="kobo.245.1">
      <img alt="Figure 4.4 – The geographical locations of your DLT cluster and storage container can introduce significant network latencies" src="image/B22011_04_004.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.246.1">
     Figure 4.4 – The geographical locations of your DLT cluster and storage container can introduce significant network latencies
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.247.1">
     The geographical region of a DLT cluster can be set by defining the cloud zone location in the pipeline cluster policy definition.
    </span>
    <span class="koboSpan" id="kobo.247.2">
     For example, the following code snippet defines a cluster policy that could be used to configure DLT pipeline clusters to launch in the US East Region of the
    </span>
    <a id="_idIndexMarker202">
    </a>
    <span class="koboSpan" id="kobo.248.1">
     AWS
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.249.1">
      cloud provider:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.250.1">
{
    ...
</span><span class="koboSpan" id="kobo.250.2">    </span><strong class="bold"><span class="koboSpan" id="kobo.251.1">"aws_attributes.zone_id": "us-east-1",</span></strong><span class="koboSpan" id="kobo.252.1">
    "custom_tags.lob": {
        "type": "fixed",
        "value": "ad analytics team"
    }
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.253.1">
     By ensuring that your DLT clusters are provisioned in the same geographical region as your organization data, you can make certain that you will be getting the best operating performance out of your pipeline clusters.
    </span>
    <span class="koboSpan" id="kobo.253.2">
     At the same time, since your pipelines run faster and utilize cloud resources for less time, this translates to dollars saved for your organization.
    </span>
    <span class="koboSpan" id="kobo.253.3">
     Along with optimizing the computational resources of our data pipelines, we can also organize our table data efficiently to further improve the performance of our data pipeline updates.
    </span>
    <span class="koboSpan" id="kobo.253.4">
     Let’s look at a few other techniques for improving the processing efficiency
    </span>
    <a id="_idIndexMarker203">
    </a>
    <span class="koboSpan" id="kobo.254.1">
     of our DLT pipelines by optimizing the data layouts of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.255.1">
      our tables.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-92">
    <a id="_idTextAnchor114">
    </a>
    <span class="koboSpan" id="kobo.256.1">
     Optimizing table layouts for faster table updates
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.257.1">
     A typical DLT
    </span>
    <a id="_idIndexMarker204">
    </a>
    <span class="koboSpan" id="kobo.258.1">
     pipeline might include one or more datasets that append new data and update existing data with either new values or even delete rows altogether.
    </span>
    <span class="koboSpan" id="kobo.258.2">
     Let’s take an in-depth look into this latter scenario and analyze what happens “under the hood” so that we can optimize our DLT datasets for faster performance as we add new data to our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.259.1">
      DLT tables.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-93">
    <a id="_idTextAnchor115">
    </a>
    <span class="koboSpan" id="kobo.260.1">
     Rewriting table files during updates
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.261.1">
     During a table
    </span>
    <a id="_idIndexMarker205">
    </a>
    <span class="koboSpan" id="kobo.262.1">
     update, the DLT engine will perform two scans to identify all the rows that match a particular update condition and rewrite the changed table data accordingly.
    </span>
    <span class="koboSpan" id="kobo.262.2">
     During the first table scan, the DLT engine will identify all table files that contain rows that match a predicate clause in an
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.263.1">
      apply_changes()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.264.1">
     (or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.265.1">
      APPLY CHANGES
     </span>
    </strong>
    <span class="koboSpan" id="kobo.266.1">
     if using SQL) expression,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.267.1">
      for example.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer039">
     <span class="koboSpan" id="kobo.268.1">
      <img alt="Figure 4.5 – DLT will apply changes to the target DLT table by identifying matching rows using a matching operation" src="image/B22011_4_5.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.269.1">
     Figure 4.5 – DLT will apply changes to the target DLT table by identifying matching rows using a matching operation
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.270.1">
     Next, the DLT engine will compile a list of all table files that contain these rows.
    </span>
    <span class="koboSpan" id="kobo.270.2">
     Using this list of table files, the DLT engine will rewrite each of these files containing the newly updated row(s) in a second table scanning operation.
    </span>
    <span class="koboSpan" id="kobo.270.3">
     As you can imagine, as you add more data to a DLT
    </span>
    <a id="_idIndexMarker206">
    </a>
    <span class="koboSpan" id="kobo.271.1">
     table, the process of locating these matching rows and identifying the list of files to rewrite can get quite expensive over time.
    </span>
    <span class="koboSpan" id="kobo.271.2">
     Fortunately, Delta Lake has a few features up its sleeves that we can use to optimize this search process and speed up the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.272.1">
      matching process.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-94">
    <a id="_idTextAnchor116">
    </a>
    <span class="koboSpan" id="kobo.273.1">
     Data skipping using table partitioning
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.274.1">
     One way to speed up
    </span>
    <a id="_idIndexMarker207">
    </a>
    <span class="koboSpan" id="kobo.275.1">
     this search process is to limit the search space for the DLT engine.
    </span>
    <span class="koboSpan" id="kobo.275.2">
     One such technique is to use Hive-style table partitioning.
    </span>
    <span class="koboSpan" id="kobo.275.3">
     Table partitioning organizes related data into physically separate subdirectories within a table’s root storage location.
    </span>
    <span class="koboSpan" id="kobo.275.4">
     The subdirectories correspond to one or more
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.276.1">
      table columns.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.277.1">
     During the matching process, the DLT engine can eliminate entire subdirectories that don’t match the predicate condition, removing the need to scan
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.278.1">
      unnecessary data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.279.1">
     Partitioning a table with
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.280.1">
      MERGE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.281.1">
     columns, the columns used to apply data changes to the table, can dramatically boost the performance of the update process.
    </span>
    <span class="koboSpan" id="kobo.281.2">
     On the other hand, since table partitioning creates physically separate directories, table partitioning can be difficult to get correct and expensive to change, requiring the entire table to be rewritten to adjust the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.282.1">
      partitioning scheme.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.283.1">
     Another challenge is identifying a table partitioning scheme that will result in partition directories that are evenly balanced with the same amount of data.
    </span>
    <span class="koboSpan" id="kobo.283.2">
     It’s quite easy to end up partitioning a table by
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.284.1">
      MERGE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.285.1">
     columns, but then end up in a scenario where some partition directories contain small amounts of data, while other partition directories contain massive amounts of data.
    </span>
    <span class="koboSpan" id="kobo.285.2">
     This is commonly referred to as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.286.1">
      data skew
     </span>
    </strong>
    <span class="koboSpan" id="kobo.287.1">
     .
    </span>
    <span class="koboSpan" id="kobo.287.2">
     Still, table
    </span>
    <a id="_idIndexMarker208">
    </a>
    <span class="koboSpan" id="kobo.288.1">
     partitioning is a powerful tool in your data pipeline tuning arsenal.
    </span>
    <span class="koboSpan" id="kobo.288.2">
     Let’s look at how we
    </span>
    <a id="_idIndexMarker209">
    </a>
    <span class="koboSpan" id="kobo.289.1">
     might be able to combine table partitioning with another table optimization technique to further boost our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.290.1">
      pipeline performance.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-95">
    <a id="_idTextAnchor117">
    </a>
    <span class="koboSpan" id="kobo.291.1">
     Delta Lake Z-ordering on MERGE columns
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.292.1">
     One way to optimize
    </span>
    <a id="_idIndexMarker210">
    </a>
    <span class="koboSpan" id="kobo.293.1">
     the table layout of a
    </span>
    <a id="_idIndexMarker211">
    </a>
    <span class="koboSpan" id="kobo.294.1">
     Delta table is to organize the data
    </span>
    <a id="_idIndexMarker212">
    </a>
    <span class="koboSpan" id="kobo.295.1">
     within each of the table files so that it can be read efficiently during file-scanning operations.
    </span>
    <span class="koboSpan" id="kobo.295.2">
     This is commonly referred to as data clustering.
    </span>
    <span class="koboSpan" id="kobo.295.3">
     Fortunately, Delta Lake features a data clustering algorithm known as Z-order clustering.
    </span>
    <span class="koboSpan" id="kobo.295.4">
     Z-order clustering will write the table data by clustering relevant data together, forming a “Z”-shaped pattern.
    </span>
    <span class="koboSpan" id="kobo.295.5">
     Storing the table data according to this pattern will improve the probability that the DLT engine will skip past irrelevant data within a table file and only read data that matches merge conditions during the update
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.296.1">
      matching process.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.297.1">
     Traditionally, without Z-order clustering, Delta Lake will store the data in a linear pattern.
    </span>
    <span class="koboSpan" id="kobo.297.2">
     As a result, during the update matching process, Delta Lake will need to open each file of the table and scan each of the rows in a linear sorting order.
    </span>
    <span class="koboSpan" id="kobo.297.3">
     Sometimes, only a single row might match the merge condition.
    </span>
    <span class="koboSpan" id="kobo.297.4">
     In turn, the DLT engine will read all the unnecessary rows that do not match, only to find maybe 1 or 2 rows that do match the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.298.1">
      update condition.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.299.1">
     By clustering the data within a file using the Z-order clustering technique, the DLT engine can pinpoint where in a particular file the relevant data exists, limiting the amount of data that it must scan.
    </span>
    <span class="koboSpan" id="kobo.299.2">
     For large tables that require a lot of scanning, this can improve the update process of a DLT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.300.1">
      pipeline dramatically.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer040">
     <span class="koboSpan" id="kobo.301.1">
      <img alt="Figure 4.6 – Z-order clustering data within table files can be visualized as data clusters forming “Z” shapes" src="image/B22011_04_006.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.302.1">
     Figure 4.6 – Z-order clustering data within table files can be visualized as data clusters forming “Z” shapes
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.303.1">
     Z-order clustering can be enabled on DLT datasets by setting the appropriate table property within the dataset definition.
    </span>
    <span class="koboSpan" id="kobo.303.2">
     Let’s look at how we might configure the Z
    </span>
    <a id="_idTextAnchor118">
    </a>
    <span class="koboSpan" id="kobo.304.1">
     -order clustering for our silver table,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.305.1">
      yellow_taxi_transformed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.306.1">
     , which receives many updates throughout
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.307.1">
      the day.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.308.1">
     We first begin by defining the dataset like any dataset within our DLT pipelines.
    </span>
    <span class="koboSpan" id="kobo.308.2">
     You’ll notice that
    </span>
    <a id="_idIndexMarker213">
    </a>
    <span class="koboSpan" id="kobo.309.1">
     we’ve
    </span>
    <a id="_idIndexMarker214">
    </a>
    <span class="koboSpan" id="kobo.310.1">
     included
    </span>
    <a id="_idIndexMarker215">
    </a>
    <span class="koboSpan" id="kobo.311.1">
     a name for the dataset,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.312.1">
      yellow_taxi_transormed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.313.1">
     , as well as a comment, which adds some descriptive text about the table.
    </span>
    <span class="koboSpan" id="kobo.313.2">
     However, within the DLT function annotation, we’ve added a couple more parameters where we can set the table properties for this dataset.
    </span>
    <span class="koboSpan" id="kobo.313.3">
     In the table properties parameter, we’ve added a couple of attributes that will describe our dataset to the DLT engine.
    </span>
    <span class="koboSpan" id="kobo.313.4">
     First, we’ve added a table property describing the quality of this dataset, which is a silver table in our medallion architecture.
    </span>
    <span class="koboSpan" id="kobo.313.5">
     Next, we’ve also added another table property that specifies which table columns we would like to apply a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.314.1">
      Z-ord
     </span>
     <a id="_idTextAnchor119">
     </a>
     <span class="koboSpan" id="kobo.315.1">
      er clustering:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.316.1">
import dlt
@dlt.table(
    name="yellow_taxi_transformed",
    comment="Taxi cab trip data containing additional columns about the financial</span><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.317.1"> data.",
    table_properties={
        "quality": "silver",
        </span><strong class="bold"><span class="koboSpan" id="kobo.318.1">"pipelines.autoOptimize.zOrderCols": "zip_code, driver_id"</span></strong><span class="koboSpan" id="kobo.319.1">
    }
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.320.1">
     During the execution of our daily maintenance tasks, the maintenance task will dynamically parse these Z-order columns and will run the following Z-order command on the underlying Delta table behind this
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.321.1">
      DLT dataset:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.322.1">
OPTIMIZE yellow_taxi_transformed
    ZORDER BY (zip_code, driver_id)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.323.1">
     So, which table
    </span>
    <a id="_idIndexMarker216">
    </a>
    <span class="koboSpan" id="kobo.324.1">
     columns should you Z-order
    </span>
    <a id="_idIndexMarker217">
    </a>
    <span class="koboSpan" id="kobo.325.1">
     your
    </span>
    <a id="_idIndexMarker218">
    </a>
    <span class="koboSpan" id="kobo.326.1">
     DLT tables by and how many columns should you specify?
    </span>
    <span class="koboSpan" id="kobo.326.2">
     A good range is anywhere from 1 to 3 columns, but no more than 5 columns.
    </span>
    <span class="koboSpan" id="kobo.326.3">
     As you add more columns, it will complicate the data clustering within the table files, diminishing the returns on any possible data skipping that
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.327.1">
      could occur.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.328.1">
     Furthermore, you should strive to choose columns that are numerical in data type.
    </span>
    <span class="koboSpan" id="kobo.328.2">
     The reason for this is that whenever new data is written to a Delta table, the Delta engine will capture statistical information about the first 32 columns – column information such as the minimum value, maximum value, and number of nulls.
    </span>
    <span class="koboSpan" id="kobo.328.3">
     This statistical information will be used during the update searching process to effectively locate which rows match the update predicate.
    </span>
    <span class="koboSpan" id="kobo.328.4">
     For data types such as strings, this statistical information does not provide very useful information, since there cannot be an average string, for example.
    </span>
    <span class="koboSpan" id="kobo.328.5">
     However, there can be an average for a column with a float data type,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.329.1">
      for instance.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.330.1">
     In addition, columns that are used in
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.331.1">
      APPLY CHANGES
     </span>
    </strong>
    <span class="koboSpan" id="kobo.332.1">
     predicates, join columns, and columns where aggregations are performed all serve as ideal Z-order candidates.
    </span>
    <span class="koboSpan" id="kobo.332.2">
     Lastly, these columns should have a higher cardinality than the columns used to create a table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.333.1">
      partitioning scheme.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.334.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.335.1">
     There may be times when you may want to experiment with different columns or a different set of columns to Z-order your table by.
    </span>
    <span class="koboSpan" id="kobo.335.2">
     Changing this Z-order scheme is trivial – it’s as simple as updating the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.336.1">
      table_properties
     </span>
    </strong>
    <span class="koboSpan" id="kobo.337.1">
     parameter in the DLT pipeline definition.
    </span>
    <span class="koboSpan" id="kobo.337.2">
     However, it’s important to note that the new Z-order clustering will take effect only on new data that is written to the table.
    </span>
    <span class="koboSpan" id="kobo.337.3">
     To apply the new Z-order clustering to existing data, the entire table would need to be fully refreshed so that the table files can be reorganized according to the clustering pattern.
    </span>
    <span class="koboSpan" id="kobo.337.4">
     As a result, you may want to balance the time and cost it will take to rewrite the table data with the performance benefits that you may get from the table
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.338.1">
      Z-order optimization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.339.1">
     As you can see by now, Z-order optimization is a great way to optimize the layout of your DLT tables to boost the performance of your data pipelines.
    </span>
    <span class="koboSpan" id="kobo.339.2">
     Having an effective data layout can improve the data skipping of the DLT engine and limit the amount of data that the DLT engine needs to scan to apply updates to target tables within your pipelines.
    </span>
    <span class="koboSpan" id="kobo.339.3">
     Combined with Hive-style table partitioning, this is a great way to ensure you are squeezing the best performance out of your data pipelines, leading to shorter execution times and less time and money spent keeping update clusters up
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.340.1">
      and running.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.341.1">
     However, what if you are only updating a small amount of data within a particular table file?
    </span>
    <span class="koboSpan" id="kobo.341.2">
     That translates to
    </span>
    <a id="_idIndexMarker219">
    </a>
    <span class="koboSpan" id="kobo.342.1">
     rewriting an entire
    </span>
    <a id="_idIndexMarker220">
    </a>
    <span class="koboSpan" id="kobo.343.1">
     file
    </span>
    <a id="_idIndexMarker221">
    </a>
    <span class="koboSpan" id="kobo.344.1">
     for the sake of updating maybe 1 or 2 rows, for example.
    </span>
    <span class="koboSpan" id="kobo.344.2">
     Let’s look at how we might be able to optimize the performance of our DLT pipelines further to avoid this
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.345.1">
      costly operation.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-96">
    <a id="_idTextAnchor121">
    </a>
    <span class="koboSpan" id="kobo.346.1">
     Improving write performance using deletion vectors
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.347.1">
     During a table
    </span>
    <a id="_idIndexMarker222">
    </a>
    <span class="koboSpan" id="kobo.348.1">
     update, the
    </span>
    <a id="_idIndexMarker223">
    </a>
    <span class="koboSpan" id="kobo.349.1">
     DLT engine applies the update by rewriting the matched file with the newly changed rows in the new, target file.
    </span>
    <span class="koboSpan" id="kobo.349.2">
     In this type of table update strategy, known as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.350.1">
      Copy-on-Write
     </span>
    </strong>
    <span class="koboSpan" id="kobo.351.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.352.1">
      COW
     </span>
    </strong>
    <span class="koboSpan" id="kobo.353.1">
     ), the rows
    </span>
    <a id="_idIndexMarker224">
    </a>
    <span class="koboSpan" id="kobo.354.1">
     not receiving any updates need to be copied over to the new file, as the name suggests.
    </span>
    <span class="koboSpan" id="kobo.354.2">
     For table updates that require only a few rows to change across many files, this can be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.355.1">
      largely inefficient.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.356.1">
     A better optimization technique would be to keep track of all the rows that have changed in a separate data structure and write the newly updated rows into separate file(s).
    </span>
    <span class="koboSpan" id="kobo.356.2">
     Then, during a table query, the table client can use this data structure to filter out any of the updated rows.
    </span>
    <span class="koboSpan" id="kobo.356.3">
     This technique is
    </span>
    <a id="_idIndexMarker225">
    </a>
    <span class="koboSpan" id="kobo.357.1">
     called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.358.1">
      Merge-on-Read
     </span>
    </strong>
    <span class="koboSpan" id="kobo.359.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.360.1">
      MOR
     </span>
    </strong>
    <span class="koboSpan" id="kobo.361.1">
     ) and is implemented in Delta Lake using a feature called
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.362.1">
      deletion vectors.
     </span>
    </span>
   </p>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.363.1">
      Deletion vectors
     </span>
    </strong>
    <span class="koboSpan" id="kobo.364.1">
     are
    </span>
    <a id="_idIndexMarker226">
    </a>
    <span class="koboSpan" id="kobo.365.1">
     a special data structure that keeps track of all the row IDs that are updated during an
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.366.1">
      UPDATE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.367.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.368.1">
      MERGE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.369.1">
     operation on a Delta table.
    </span>
    <span class="koboSpan" id="kobo.369.2">
     Deletion vectors can be enabled by setting a table property of the underlying Delta table.
    </span>
    <span class="koboSpan" id="kobo.369.3">
     Like the statistical information regarding the Delta table columns, deletion vectors are stored alongside the table data on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.370.1">
      cloud storage.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer041">
     <span class="koboSpan" id="kobo.371.1">
      <img alt="Figure 4.7 – Delta Lake tables will keep track of the row IDs of each row in a separate data structure" src="image/B22011_04_007.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.372.1">
     Figure 4.7 – Delta Lake tables will keep track of the row IDs of each row in a separate data structure
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.373.1">
     Furthermore, deletion
    </span>
    <a id="_idIndexMarker227">
    </a>
    <span class="koboSpan" id="kobo.374.1">
     vectors can
    </span>
    <a id="_idIndexMarker228">
    </a>
    <span class="koboSpan" id="kobo.375.1">
     be automatically enabled by default for all new tables created within a Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.375.2">
     A workspace administrator can enable or disable this behavior from the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.376.1">
      Advanced
     </span>
    </strong>
    <span class="koboSpan" id="kobo.377.1">
     tab of the workspace admin
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.378.1">
      settings UI.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer042">
     <span class="koboSpan" id="kobo.379.1">
      <img alt="Figure 4.8 – Deletion vectors can be automatically enabled in the ﻿Databricks Data Intelligence Platform" src="image/B22011_04_008.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.380.1">
     Figure 4.8 – Deletion vectors can be automatically enabled in the Databricks Data Intelligence Platform
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.381.1">
     Deletion vectors
    </span>
    <a id="_idIndexMarker229">
    </a>
    <span class="koboSpan" id="kobo.382.1">
     can be explicitly
    </span>
    <a id="_idIndexMarker230">
    </a>
    <span class="koboSpan" id="kobo.383.1">
     set on a dataset by setting the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.384.1">
      enableDeletionVectors
     </span>
    </strong>
    <span class="koboSpan" id="kobo.385.1">
     table property in the DLT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.386.1">
      table definition:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.387.1">
@dlt.table(
    name="random_trip_data_silver",
    comment="Taxi trip data transformed with financial data.",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.zOrderCols": "driver_id",
        "delta.enableDeletionVectors": "true"
    }
)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.388.1">
     In addition, deletion vectors unlock a new class of update performance features on the Databricks Data Intelligence Platform, collectively referred
    </span>
    <a id="_idIndexMarker231">
    </a>
    <span class="koboSpan" id="kobo.389.1">
     to as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.390.1">
      Predictive I/O
     </span>
    </strong>
    <span class="koboSpan" id="kobo.391.1">
     .
    </span>
    <span class="koboSpan" id="kobo.391.2">
     Predictive I/O uses deep learning and file statistics to accurately predict the location of rows within files that match an update condition.
    </span>
    <span class="koboSpan" id="kobo.391.3">
     As a result, the time it takes to scan matching files and rewrite data during updates, merges, and deletes is
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.392.1">
      drastically reduced.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.393.1">
     Hive-style table partitioning, Z-order data clustering, and deletion vectors are all great optimization techniques for efficiently storing our table data and improving the speed of our pipeline updates.
    </span>
    <span class="koboSpan" id="kobo.393.2">
     Let’s turn our attention back to the computational resources of our data pipelines and analyze yet another technique for improving the performance of our DLT pipelines
    </span>
    <a id="_idIndexMarker232">
    </a>
    <span class="koboSpan" id="kobo.394.1">
     in production, particularly
    </span>
    <a id="_idIndexMarker233">
    </a>
    <span class="koboSpan" id="kobo.395.1">
     in times when the processing demands may spike and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.396.1">
      become unpredictable.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-97">
    <a id="_idTextAnchor122">
    </a>
    <span class="koboSpan" id="kobo.397.1">
     Serverless DLT pipelines
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.398.1">
     In
    </span>
    <a href="B22011_02.xhtml#_idTextAnchor052">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.399.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.400.1">
     , we briefly
    </span>
    <a id="_idIndexMarker234">
    </a>
    <span class="koboSpan" id="kobo.401.1">
     described what serverless DLT clusters were and how they can quickly and efficiently scale computational resources up to handle spikes in demand, as well as scale down to save cloud costs.
    </span>
    <span class="koboSpan" id="kobo.401.2">
     While we won’t cover the architecture of serverless clusters again, we will cover how serverless DLT clusters can help organizations scale their data pipelines as more and more data pipelines
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.402.1">
      are added.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.403.1">
     With serverless DLT clusters, the cluster infrastructure and settings are automatically handled by the Databricks cloud provider account.
    </span>
    <span class="koboSpan" id="kobo.403.2">
     This translates to removing the burden of having to select VM instance types to balance performance with costs.
    </span>
    <span class="koboSpan" id="kobo.403.3">
     The costs for serverless compute are at a fixed, flat rate, making the costs predictable.
    </span>
    <span class="koboSpan" id="kobo.403.4">
     In addition, since the computational resources are managed by the Databricks cloud provider account, Databricks can reserve a large amount of VM instances at a discounted price by each cloud provider.
    </span>
    <span class="koboSpan" id="kobo.403.5">
     These discounted rates can then be passed along to the DLT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.404.1">
      serverless consumers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.405.1">
     Furthermore, serverless DLT clusters
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.406.1">
      simplify data pipeline maintenance
     </span>
    </em>
    <span class="koboSpan" id="kobo.407.1">
     by reducing the amount of configuration that’s needed per data pipeline.
    </span>
    <span class="koboSpan" id="kobo.407.2">
     With less configuration, data engineer teams can focus less on the maintenance of their data pipelines and more on what matters to the business, such as changing business logic, data validation, and adding more data pipelines to name a few.
    </span>
    <span class="koboSpan" id="kobo.407.3">
     In addition, as your data pipelines grow over time and dataset volumes increase over time, you may need to provision more VM instances.
    </span>
    <span class="koboSpan" id="kobo.407.4">
     Eventually, you may hit the cloud provider limits for certain instance types, which requires an additional process to have these limits increased by the cloud provider.
    </span>
    <span class="koboSpan" id="kobo.407.5">
     With serverless DLT compute, these limits have already been negotiated with the cloud provider, meaning that the DLT serverless consumers need not be concerned with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.408.1">
      this burden.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.409.1">
     Serverless data
    </span>
    <a id="_idIndexMarker235">
    </a>
    <span class="koboSpan" id="kobo.410.1">
     pipelines can also help
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.411.1">
      reduce costs
     </span>
    </em>
    <span class="koboSpan" id="kobo.412.1">
     for data pipelines.
    </span>
    <span class="koboSpan" id="kobo.412.2">
     For example, with traditional, customer-managed compute, a cluster can only add additional VM instances as quickly as the cloud provider can provision additional instances and routinely run the diagnostic checks.
    </span>
    <span class="koboSpan" id="kobo.412.3">
     Plus, the Databricks runtime container and user libraries need to be installed on the additional instances, which takes even more time.
    </span>
    <span class="koboSpan" id="kobo.412.4">
     This can translate to many minutes – sometimes 15 minutes or more depending on the cloud provider – before a DLT cluster can scale out to handle the unpredictable spikes in computational demand.
    </span>
    <span class="koboSpan" id="kobo.412.5">
     As a result, DLT pipelines running on traditional compute will take much longer to execute as compared to the serverless DLT clusters.
    </span>
    <span class="koboSpan" id="kobo.412.6">
     With serverless DLT clusters, the VM instances are pre-provisioned with the latest Databricks runtime container already installed and started in a pre-allocated instance pool.
    </span>
    <span class="koboSpan" id="kobo.412.7">
     During spikes in processing demand, the DLT pipeline can respond with additional resources to meet the demand on the order of seconds rather than minutes.
    </span>
    <span class="koboSpan" id="kobo.412.8">
     These minutes can add up over many data pipeline runs and over the course of a cloud billing cycle.
    </span>
    <span class="koboSpan" id="kobo.412.9">
     By driving down the time it takes to scale out with additional resources and being able to aggressively scale down with enhanced autoscaling, serverless DLT pipelines can drastically reduce operational costs while simultaneously improving the efficiency of the ETL processing in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.413.1">
      your lakehouse.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.414.1">
     Removing the infrastructure burden of managing compute settings for data pipelines as well as controlling cloud costs are great motivating factors behind choosing serverless DLT pipelines over traditional, customer-managed compute.
    </span>
    <span class="koboSpan" id="kobo.414.2">
     However, let’s look at another motivation
    </span>
    <a id="_idIndexMarker236">
    </a>
    <span class="koboSpan" id="kobo.415.1">
     for selecting serverless DLT clusters, such as the performance features that come with this type of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.416.1">
      computational resource.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-98">
    <a id="_idTextAnchor123">
    </a>
    <span class="koboSpan" id="kobo.417.1">
     Introducing Enzyme, a performance optimization layer
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.418.1">
     There may be certain
    </span>
    <a id="_idIndexMarker237">
    </a>
    <span class="koboSpan" id="kobo.419.1">
     scenarios where a data pipeline has been deployed into a production environment.
    </span>
    <span class="koboSpan" id="kobo.419.2">
     However, down the road, there may be significant changes in the business requirements, requiring the datasets to be recomputed from scratch.
    </span>
    <span class="koboSpan" id="kobo.419.3">
     In these scenarios, recomputing the historical data of these datasets could be
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.420.1">
      cost prohibitive.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.421.1">
     Enzyme, a brand-new optimization layer that is only available for serverless DLT pipelines, aims to reduce ETL costs by dynamically calculating a cost model for keeping the materialized results of a dataset up to date.
    </span>
    <span class="koboSpan" id="kobo.421.2">
     Like the cost model in Spark query planning, Enzyme calculates a cost model between several ETL techniques from a traditional materialized view in DLT to a Delta streaming table to another Delta streaming table, or a manual ETL technique.
    </span>
    <span class="koboSpan" id="kobo.421.3">
     For example, the Enzyme engine might model the cost to refresh a dataset using a materialization technique, translating to 10 Spark jobs, each with 200 Spark tasks.
    </span>
    <span class="koboSpan" id="kobo.421.4">
     This cost model might save two Spark jobs and shave five minutes off the overall execution time as predicted by another modeled ETL technique, so the Enzyme engine will choose the first
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.422.1">
      technique instead.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer043">
     <span class="koboSpan" id="kobo.423.1">
      <img alt="Figure 4.9 – The Enzyme optimization layer will automatically select the most cost-efficient ETL refresh technique using a cost model" src="image/B22011_04_009.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.424.1">
     Figure 4.9 – The Enzyme optimization layer will automatically select the most cost-efficient ETL refresh technique using a cost model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.425.1">
     The Enzyme layer will dynamically choose the most efficient and cost-effective method for recomputing the results for a given dataset at runtime.
    </span>
    <span class="koboSpan" id="kobo.425.2">
     Since Enzyme is a serverless DLT feature, it is already enabled by default, removing the need for DLT admins to manage pipeline
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.426.1">
      cluster settings.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.427.1">
     By now, you should
    </span>
    <a id="_idIndexMarker238">
    </a>
    <span class="koboSpan" id="kobo.428.1">
     understand the powerful features that come with serverless DLT pipelines, such as the Enzyme optimization layer, as well as the infrastructure management and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.429.1">
      cost-saving benefits.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-99">
    <a id="_idTextAnchor124">
    </a>
    <span class="koboSpan" id="kobo.430.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.431.1">
     In this chapter, we looked at various methods for scaling our data pipelines to handle large volumes of data and perform well under periods of high and unpredictable processing demand.
    </span>
    <span class="koboSpan" id="kobo.431.2">
     We looked at two attributes of scaling our DLT pipelines – compute and data layout.
    </span>
    <span class="koboSpan" id="kobo.431.3">
     We examined the enhanced autoscaling feature of the Databricks Data Intelligence Platform to automatically scale the computational resources that the data pipelines execute on.
    </span>
    <span class="koboSpan" id="kobo.431.4">
     We also looked at optimizing how the underlying table data was stored, clustering relevant data within table files and leading to faster table queries and shorter pipeline processing times.
    </span>
    <span class="koboSpan" id="kobo.431.5">
     Furthermore, we also looked at regular maintenance activities to maintain high-performing table queries, as well as prevent ballooning cloud storage costs from obsolete
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.432.1">
      data files.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.433.1">
     Data security is of the utmost importance and is often overlooked until the end of a lakehouse implementation.
    </span>
    <span class="koboSpan" id="kobo.433.2">
     However, this could mean the difference between a successful lakehouse and making the front page of a newspaper – and not for a good reason.
    </span>
    <span class="koboSpan" id="kobo.433.3">
     In the next chapter, we’ll be taking a look at how we can effectively implement strong data governance across our lakehouse, whether it’s within a single geographical region or across a fail-over region on a different part of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.434.1">
      the globe.
     </span>
    </span>
   </p>
  </div>
 

  <div class="Content" id="_idContainer045">
   <h1 id="_idParaDest-100" lang="en-US" xml:lang="en-US">
    <a id="_idTextAnchor125">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     Part 2:Securing the Lakehouse Using the Unity Catalog
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.2.1">
     In this part, we’ll explore how to implement an effective data governance strategy using the Unity Catalog in the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.2.2">
     We’ll look at how you can enforce fine-grained data access policies across various roles and departments in your organization.
    </span>
    <span class="koboSpan" id="kobo.2.3">
     Lastly, we’ll look at how you trace the origins of data assets in Unity Catalog, ensuring that data is coming from
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.3.1">
      trusted sources.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.4.1">
     This part contains the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.5.1">
      following chapters:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <a href="B22011_05.xhtml#_idTextAnchor126">
      <em class="italic">
       <span class="koboSpan" id="kobo.6.1">
        Chapter 5
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.7.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.8.1">
       Mastering Data Governance in the Lakehouse
      </span>
     </em>
     <em class="italic">
      <span class="koboSpan" id="kobo.9.1">
       with
      </span>
     </em>
     <em class="italic">
      <span class="koboSpan" id="kobo.10.1">
       Unity Catalog
      </span>
     </em>
    </li>
    <li>
     <a href="B22011_06.xhtml#_idTextAnchor148">
      <em class="italic">
       <span class="koboSpan" id="kobo.11.1">
        Chapter 6
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.12.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.13.1">
       Managing Data Locations in Unity Catalog
      </span>
     </em>
    </li>
    <li>
     <a href="B22011_07.xhtml#_idTextAnchor165">
      <em class="italic">
       <span class="koboSpan" id="kobo.14.1">
        Chapter 7
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.15.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.16.1">
       Viewing Data Lineage Using Unity Catalog
      </span>
     </em>
    </li>
   </ul>
  </div>
  <div>
   <div id="_idContainer046">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer047">
   </div>
  </div>
 </body></html>