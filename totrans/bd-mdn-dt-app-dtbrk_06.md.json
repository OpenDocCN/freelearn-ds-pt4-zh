["```py\nGRANT CREATE CATALOG ON METASTORE TO `jane.smith@example.com`;\n```", "```py\nGRANT CREATE VOLUME\n    ON CATALOG development_catalog\n    TO `jane.smith@example.com`;\n```", "```py\nimport requests\ncatalog = \"testing_catalog\"\nresponse = requests.patch(\n    f\"https://{workspace_name}/api/2.1/unity-catalog/catalogs/{catalog}\",\n    headers = {\"Authorization\": f\"Bearer {api_token}\"},\n    json = {\"isolation_mode\": \"ISOLATED\"}\n)\nprint(response.json())\n```", "```py\nresponse = requests.patch(\n    f\"https://{workspace_name}/api/2.1/unity-catalog/bindings/catalog/{catalog}\",\n    headers = {\"Authorization\": f\"Bearer {api_token}\"},\n    json = {\"add\": [{\n        \"workspace_id\": <production_workspace_id>,\n        \"binding_type\": \"BINDING_TYPE_READ_ONLY\"}]\n    }\n)\nprint(response.json())\n```", "```py\n# Connect to data stored in an ADLS Gen2 container\naccount_name = \"some_storage_account\"\nspark.conf.set(f\"fs.azure.account.auth.type.{account_name}.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(f\"fs.azure.sas.token.provider.type.{account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n# Use a Databricks Secret to safely store and retrieve a SAS key for authenticating with ADLS service\nspark.conf.set(\n    f\"fs.azure.sas.fixed.token.{account_name}.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"sas_token_scope\", \n    key=\"sas_ token_key\"))\n```", "```py\ndatabricks storage-credentials create \\\n  --json '{\"name\": \"my_storage_cred\", ' \\\n  '\"aws_iam_role\": {\"role_arn\": ' \\\n  '\"arn:aws:iam::<role_identifier>:role/<account_name>\"}}'\n```", "```py\n-- Grant access to create an external location using the storage cred\nGRANT CREATE EXTERNAL LOCATION\n    ON STORAGE CREDENTIAL my_s3_bucket_cred\n    TO `data-science`;\n```", "```py\nCREATE CONNECTION my_mysql_connection TYPE mysql\nOPTIONS (\n    host '<fully_qualified_hostname>',\n    port '3306',\n    user secret('mysql_scope', 'mysql_username'),\n    password secret('mysql_scope', 'mysql_password')\n)\n```", "```py\n%pip install faker reportlab\n```", "```py\n    from shutil import copyfile\n    def save_doc_as_text(file_name, save_path, paragraph):\n        \"\"\"Helper function that saves a paragraph of text as a text file\"\"\"\n        tmp_path = f\"/local_disk0/tmp/{file_name}\"\n        volume_path = f\"{save_path}/{file_name}\"\n        print(f\"Saving text file at : {tmp_path}\")\n        txtfile = open(tmp_path, \"a\")\n        txtfile.write(paragraph)\n        txtfile.close()\n        copyfile(tmp_path, volume_path)\n    ```", "```py\n    def save_doc_as_pdf(file_name, save_path, paragraph):\n        \"\"\"Helper function that saves a paragraph of text as a PDF file\"\"\"\n        from reportlab.pdfgen.canvas import Canvas\n        from reportlab.lib.pagesizes import letter\n        from reportlab.lib.units import cm\n        tmp_path = f\"/local_disk0/tmp/{file_name}\"\n        volume_path = f\"{save_path}/{file_name}\"\n        canvas = Canvas(tmp_path, pagesize=letter)\n        lines = paragraph.split(\".\")\n        textobject = canvas.beginText(5*cm, 25*cm)\n        for line in lines:\n            textobject.textLine(line)\n            canvas.drawText(textobject)\n        canvas.save()\n        print(f\"Saving PDF file at : {tmp_path}\")\n        copyfile(tmp_path, volume_path)\n    ```", "```py\n    def save_doc_as_csv(file_name, save_path, paragraph):\n        \"\"\"Helper function that saves a paragraph of text as a CSV file\"\"\"\n        import csv\n        tmp_path = f\"/local_disk0/tmp/{file_name}\"\n        volume_path = f\"{save_path}/{file_name}\"\n        print(f\"Saving CSV file at : {tmp_path}\")\n        with open(tmp_path, 'w', newline='') as file:\n            writer = csv.writer(file)\n            writer.writerow([\"Id\", \"Sentence\"])\n            i = 1\n            for line in paragraph.split(\".\"):\n                writer.writerow([i, line])\n                i = i + 1\n        copyfile(tmp_path, volume_path)\n    ```", "```py\nfrom faker import Faker\nimport time\nimport random\nFaker.seed(631)\nfake = Faker()\n# Randomly generate documents\nnum_docs = 5\nnum_sentences_per_doc = 100\ndoc_types = [\"txt\", \"pdf\", \"csv\"]\nvolume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n```", "```py\nfor _ in range(num_docs):\n    paragraph = fake.paragraph(nb_sentences=num_sentences_per_doc)\n```", "```py\n    # Randomly choose a document format type\n    doc_type = doc_types[random.randrange(2)]\n    print(doc_type)\n    if doc_type == \"txt\":\n        doc_name = f\"{fake.pystr()}.txt\"\n        save_doc_as_text(doc_name, volume_path, paragraph)\n    elif doc_type == \"pdf\":\n        doc_name = f\"{fake.pystr()}.pdf\"\n        save_doc_as_pdf(doc_name, volume_path, paragraph)\n    elif doc_type == \"csv\":\n        doc_name = f\"{fake.pystr()}.csv\"\n        save_doc_as_csv(doc_name, volume_path, paragraph)\n```", "```py\n    # Sleep for a random interval\n    sleep_time = random.randint(3, 30)\n    print(f\"Sleeping for {sleep_time} seconds...\\n\\n\")\n    time.sleep(sleep_time)\n```", "```py\n@dlt.table(\n    name=\"text_docs_silver\",\n    comment=\"Combined textual documents for Generative AI pipeline.\"\n)\ndef text_docs_silver():\n    text_docs_df = dlt.read(\"text_docs_raw\").withColumn(\n        \"type\", F.lit(\"text\"))\n    csv_docs_df = dlt.read(\"csv_docs_raw\").withColumn(\n        \"type\", F.lit(\"csv\"))\n    pdf_docs_df = dlt.read(\"pdf_docs_raw\").withColumn(\n        \"type\", F.lit(\"pdf\"))\n    combined_df = text_docs_df.union(csv_docs_df).union(\n        pdf_docs_df)\n    return combined_df\n```"]