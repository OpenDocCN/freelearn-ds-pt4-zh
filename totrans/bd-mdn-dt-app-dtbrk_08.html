<html><head></head><body>
  <div id="_idContainer091">
   <h1 class="chapter-number" id="_idParaDest-146">
    <a id="_idTextAnchor185">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     8
    </span>
   </h1>
   <h1 id="_idParaDest-147">
    <a id="_idTextAnchor186">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Deploying, Maintaining, and Administrating DLT Pipelines Using Terraform
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In this chapter, we’re going to explore how an automation tool such as Terraform can be used to express data pipelines as code, commonly referred to as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.4.1">
      Infrastructure as Code
     </span>
    </strong>
    <span class="koboSpan" id="kobo.5.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.6.1">
      IAC
     </span>
    </strong>
    <span class="koboSpan" id="kobo.7.1">
     ), in
    </span>
    <a id="_idIndexMarker463">
    </a>
    <span class="koboSpan" id="kobo.8.1">
     Databricks.
    </span>
    <span class="koboSpan" id="kobo.8.2">
     We’ll look at how to set up a local Terraform development environment using popular code editors such as VS Code so that we can experiment with deploying different resources to a Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.8.3">
     Next, we’ll dive into how to represent data pipelines using Terraform and how to configure different aspects
    </span>
    <a id="_idIndexMarker464">
    </a>
    <span class="koboSpan" id="kobo.9.1">
     of a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.10.1">
      Delta Live Tables
     </span>
    </strong>
    <span class="koboSpan" id="kobo.11.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.12.1">
      DLT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.13.1">
     ) pipeline.
    </span>
    <span class="koboSpan" id="kobo.13.2">
     We’ll also look at how we can automate the validation and deployment of IaC to different Databricks workspaces, including a production workspace.
    </span>
    <span class="koboSpan" id="kobo.13.3">
     Lastly, we’ll examine industry best practices and future considerations along
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.14.1">
      the way.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.15.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.16.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.17.1">
      Introducing the Databricks provider
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.18.1">
       for Terraform
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.19.1">
      Setting up a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.20.1">
       local environment
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.21.1">
      Configuring DLT pipelines
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.22.1">
       using Terraform
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.23.1">
      Automating DLT
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.24.1">
       pipeline deployment
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.25.1">
      Hands-on exercise – deploying a DLT pipeline using
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.26.1">
       VS Code
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-148">
    <a id="_idTextAnchor187">
    </a>
    <span class="koboSpan" id="kobo.27.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.28.1">
     To follow along with the examples provided in this chapter, you’ll need Databricks workspace permissions to create and start an all-purpose cluster so that you can import and execute the chapter’s accompanying notebooks.
    </span>
    <span class="koboSpan" id="kobo.28.2">
     All code samples can be downloaded from this chapter’s GitHub repository located at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08">
     <span class="koboSpan" id="kobo.29.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08
     </span>
    </a>
    <span class="koboSpan" id="kobo.30.1">
     .
    </span>
    <span class="koboSpan" id="kobo.30.2">
     This chapter will create and run several new notebooks, as well as run a new DLT pipeline using the product’s
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.31.1">
      Advanced
     </span>
    </strong>
    <span class="koboSpan" id="kobo.32.1">
     edition, estimated to consume around 10—15
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.33.1">
      Databricks
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.34.1">
       Units
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.35.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.36.1">
       DBUs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.37.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-149">
    <a id="_idTextAnchor188">
    </a>
    <span class="koboSpan" id="kobo.38.1">
     Introducing the Databricks provider for Terraform
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.39.1">
     Terraform
    </span>
    <a id="_idIndexMarker465">
    </a>
    <span class="koboSpan" id="kobo.40.1">
     is an open source deployment automation tool that can be used to automate
    </span>
    <a id="_idIndexMarker466">
    </a>
    <span class="koboSpan" id="kobo.41.1">
     the deployment of cloud infrastructure in a repeatable and predictable manner.
    </span>
    <span class="koboSpan" id="kobo.41.2">
     One reason Terraform is such a popular deployment
    </span>
    <a id="_idIndexMarker467">
    </a>
    <span class="koboSpan" id="kobo.42.1">
     tool is that it supports deploying infrastructure to the three major cloud providers:
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.43.1">
      Amazon Web Services
     </span>
    </strong>
    <span class="koboSpan" id="kobo.44.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.45.1">
      AWS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.46.1">
     ),
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.47.1">
      Azure
     </span>
    </strong>
    <span class="koboSpan" id="kobo.48.1">
     , and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.49.1">
      Google Cloud Platform
     </span>
    </strong>
    <span class="koboSpan" id="kobo.50.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.51.1">
      GCP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.52.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.52.2">
     Terraform
    </span>
    <a id="_idIndexMarker468">
    </a>
    <span class="koboSpan" id="kobo.53.1">
     is centered around the concept of
    </span>
    <a id="_idIndexMarker469">
    </a>
    <span class="koboSpan" id="kobo.54.1">
     defining IaC where, rather than manually deploying cloud components such as network objects, virtual machines, or storage containers, they are expressed using code files.
    </span>
    <span class="koboSpan" id="kobo.54.2">
     Furthe
    </span>
    <a id="_idTextAnchor189">
    </a>
    <span class="koboSpan" id="kobo.55.1">
     rmore, Terraform
    </span>
    <a id="_idIndexMarker470">
    </a>
    <span class="koboSpan" id="kobo.56.1">
     files are configuration-driven.
    </span>
    <span class="koboSpan" id="kobo.56.2">
     Rather than expressing
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.57.1">
      how
     </span>
    </em>
    <span class="koboSpan" id="kobo.58.1">
     to deploy the infrastructure, cloud administrators focus on expressing
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.59.1">
      what
     </span>
    </em>
    <span class="koboSpan" id="kobo.60.1">
     changes between environments through configuration.
    </span>
    <span class="koboSpan" id="kobo.60.2">
     Lastly, Terraform maintains the state of your architecture, meaning that the tool will keep track of the state of cloud resources and will update the state accordingly for each new execution of a Terraform
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.61.1">
      configuration file.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.62.1">
     Lastly, Terraform files can be executed directly from your local machine, allowing you to interact with cloud
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.63.1">
      resources remotely.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer085">
     <span class="koboSpan" id="kobo.64.1">
      <img alt="Figure 8.1 – Terraform will reflect environment changes using configuration files" src="image/B22011_08_1.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.65.1">
     Figure 8.1 – Terraform will reflect environment changes using configuration files
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.66.1">
     Terraform
    </span>
    <a id="_idIndexMarker471">
    </a>
    <span class="koboSpan" id="kobo.67.1">
     configuration files define the cloud infrastructure that is applied in the cloud provider, and the infrastructure state is synced back to the local environment.
    </span>
    <span class="koboSpan" id="kobo.67.2">
     Furthermore, Databricks provides a Terraform provider for deploying Databricks workspaces and workspace objects to the major
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.68.1">
      cloud providers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.69.1">
     A Terraform provider
    </span>
    <a id="_idIndexMarker472">
    </a>
    <span class="koboSpan" id="kobo.70.1">
     is a plugin for the Terraform tool that enables users to interact with specific APIs.
    </span>
    <span class="koboSpan" id="kobo.70.2">
     In this case, the Terraform provider interacts with the Databricks REST API, allowing workspace administrators to automate the deployment of even the most complex data
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.71.1">
      processing environments.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.72.1">
     There are
    </span>
    <a id="_idIndexMarker473">
    </a>
    <span class="koboSpan" id="kobo.73.1">
     many advantages to using Terraform to automate the deployment of data pipelines within your organization, including
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.74.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.75.1">
      It is easy to deploy infrastructure between the major cloud providers, making it trivial to migrate between clouds if
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.76.1">
       need be.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.77.1">
      It is easy to scale to hundreds of data pipelines by focusing on defining configuration rather than manually deploying and maintaining
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.78.1">
       data pipelines.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.79.1">
      Pipeline definition is concise, allowing cloud administrators to focus on expressing what should change, rather than how to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.80.1">
       deploy infrastructure.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.81.1">
     Let’s look at how easy it is to get started defining Databricks resources and applying them to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.82.1">
      targeted workspaces.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-150">
    <a id="_idTextAnchor190">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     Setting up a local Terraform environment
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.84.1">
     Before
    </span>
    <a id="_idIndexMarker474">
    </a>
    <span class="koboSpan" id="kobo.85.1">
     we
    </span>
    <a id="_idIndexMarker475">
    </a>
    <span class="koboSpan" id="kobo.86.1">
     can begin deploying data pipeline objects to our Databricks workspace, we need to install the
    </span>
    <a id="_idIndexMarker476">
    </a>
    <span class="koboSpan" id="kobo.87.1">
     Terraform
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.88.1">
      command-line interface
     </span>
    </strong>
    <span class="koboSpan" id="kobo.89.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.90.1">
      CLI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.91.1">
     ) tool.
    </span>
    <span class="koboSpan" id="kobo.91.2">
     If you haven’t already done so, you will need to download the Terraform CLI, which can be downloaded for free from the HashiCorp
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.92.1">
      website (
     </span>
    </span>
    <a href="https://developer.hashicorp.com/terraform/install">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.93.1">
       https://developer.hashicorp.com/terraform/install
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.94.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.95.1">
     Next, we want to organize the Terraform configuration files into a single directory.
    </span>
    <span class="koboSpan" id="kobo.95.2">
     Let’s create a new directory
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.96.1">
      called
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.97.1">
       chp8_databricks_terraform
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.98.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.99.1">
     Within the newly created directory, let’s create a brand new Terraform configuration file where we will define our data pipeline and other related workspace objects.
    </span>
    <span class="koboSpan" id="kobo.99.2">
     Create a new file, naming
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.100.1">
      it
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.101.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.102.1">
      .
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.103.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.104.1">
     Terraform configuration files use the Terraform language and end with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.105.1">
      .
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.106.1">
       tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.107.1">
      extension.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-151">
    <a id="_idTextAnchor191">
    </a>
    <span class="koboSpan" id="kobo.108.1">
     Importing the Databricks Terraform provider
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.109.1">
     The first step
    </span>
    <a id="_idIndexMarker477">
    </a>
    <span class="koboSpan" id="kobo.110.1">
     to using Terraform to deploy Databricks workspace objects is to import the Databricks Terraform provider.
    </span>
    <span class="koboSpan" id="kobo.110.2">
     If it’s your first time using the Terraf
    </span>
    <a id="_idTextAnchor192">
    </a>
    <span class="koboSpan" id="kobo.111.1">
     orm provider, Terraform will
    </span>
    <a id="_idIndexMarker478">
    </a>
    <span class="koboSpan" id="kobo.112.1">
     take care of downloading the Databricks provider from the Terraform Registry.
    </span>
    <span class="koboSpan" id="kobo.112.2">
     The Terraform Registry is a public hub for downloading third-party providers, modules, and security policies that aid in developing Terraform configuration files to deploy your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.113.1">
      cloud infrastructure.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.114.1">
     Add the following code snippet at the top of the new Terraform configuration
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.115.1">
      file,
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.116.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.117.1">
      :
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.118.1">
terraform {
  required_providers {
    databricks = {
    source = "databricks/databricks"
    }
  }
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.119.1">
     This
    </span>
    <a id="_idIndexMarker479">
    </a>
    <span class="koboSpan" id="kobo.120.1">
     code snippet will instruct the Terraform CLI tool to download and import a Terraform provider called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.121.1">
      databricks
     </span>
    </strong>
    <span class="koboSpan" id="kobo.122.1">
     that has been published to the Terraform Registry by the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.123.1">
      Databricks organization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.124.1">
     Now that we’ve imported the Databricks Terraform provider, we can begin deploying data pipeline objects to our Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.124.2">
     But before we can do that, we must first authenticate with our Databricks workspace to make changes, such as creating a new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.125.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-152">
    <a id="_idTextAnchor193">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     Configuring workspace authentication
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.127.1">
     If you recall, the
    </span>
    <a id="_idIndexMarker480">
    </a>
    <span class="koboSpan" id="kobo.128.1">
     Databricks Terraform provider will interact with the Databricks REST API behind the scenes.
    </span>
    <span class="koboSpan" id="kobo.128.2">
     As a result, the same authentication mechanisms that are used to authenticate with the Databricks REST API and make workspace changes can be applied
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.129.1">
      using Terraform.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.130.1">
     In total, there are about nine supported methods for authenticating with a Databricks workspace using the Terraform provider (the latest list can be found here:
    </span>
    <a href="https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication">
     <span class="koboSpan" id="kobo.131.1">
      https://registry.terraform.io/providers/databricks/databricks/latest/docs#authentication
     </span>
    </a>
    <span class="koboSpan" id="kobo.132.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.132.2">
     A few of the popular authentication methods include
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.133.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.134.1">
      Using a workspace administrator username
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.135.1">
       and password
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.136.1">
      Using a
     </span>
     <a id="_idIndexMarker481">
     </a>
     <span class="koboSpan" id="kobo.137.1">
      Databricks
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.138.1">
       Personal Access
      </span>
     </strong>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.139.1">
        Token
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.140.1">
       (
      </span>
     </span>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.141.1">
        PAT
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.142.1">
       )
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.143.1">
      Using the Azure CLI or Google
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.144.1">
       Cloud CLI
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.145.1">
      If using the Azure cloud provider, using a service principal or managed
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.146.1">
       service identity
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.147.1">
      Using the Databricks CLI (
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.148.1">
       user-to-machine authentication)
      </span>
     </span>
    </li>
   </ul>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.149.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.150.1">
     Since we are doing local development and testing, in the following example, we’ll be generating an OAuth token using the Databricks CLI and logging in to our Databricks workspace manually.
    </span>
    <span class="koboSpan" id="kobo.150.2">
     However, for production deployments, it’s recommended to securely store workspace credentials in a secrets manager such as Azure Key Vault, AWS Secrets Manager, or HashiCorp Vault, to name
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.151.1">
      a few.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.152.1">
     There
    </span>
    <a id="_idIndexMarker482">
    </a>
    <span class="koboSpan" id="kobo.153.1">
     are a couple of options for storing authentication tokens that are used with Terraform – directly within the Terraform configuration file as a part of the Databricks provider import, or on the local machine within a configuration file.
    </span>
    <span class="koboSpan" id="kobo.153.2">
     We would recommend the latter option to avoid accidental exposure of credentials when checking in code artifacts to your code repository.
    </span>
    <span class="koboSpan" id="kobo.153.3">
     The easiest method for populating this configuratio
    </span>
    <a id="_idTextAnchor194">
    </a>
    <span class="koboSpan" id="kobo.154.1">
     n file is by using the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.155.1">
      Databricks CLI.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.156.1">
     The Databricks CLI supports Windows, Linux, or macOS operating systems, making it a cross-platform-compatible and versatile tool.
    </span>
    <span class="koboSpan" id="kobo.156.2">
     If your local machine uses macOS or Linux operating systems, you can download the Databricks CLI using the Homebrew package manager using a shell prompt.
    </span>
    <span class="koboSpan" id="kobo.156.3">
     Or you can easily upgrade the version of an existing Databricks CLI installation.
    </span>
    <span class="koboSpan" id="kobo.156.4">
     For example, the following commands will install or upgrade an existing Databricks CLI installation using Homebrew on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.157.1">
      a Mac:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.158.1">
$ brew tap databricks/tap
$ brew install databricks</span></pre>
   <p>
    <span class="koboSpan" id="kobo.159.1">
     On a Windows machine, you can install the Databricks CLI using the popular package manager,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.160.1">
      winget
     </span>
    </strong>
    <span class="koboSpan" id="kobo.161.1">
     (
    </span>
    <a href="https://learn.microsoft.com/windows/package-manager/winget/">
     <span class="koboSpan" id="kobo.162.1">
      https://learn.microsoft.com/windows/package-manager/winget/
     </span>
    </a>
    <span class="koboSpan" id="kobo.163.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.163.2">
     The following commands will download and install the Databricks CLI using the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.164.1">
       winget
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.165.1">
      utility:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.166.1">
$ winget search databricks
$ winget install Databricks.DatabricksCLI</span></pre>
   <p>
    <span class="koboSpan" id="kobo.167.1">
     Once downloaded, you can configure authentication by executing the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.168.1">
      configure
     </span>
    </strong>
    <span class="koboSpan" id="kobo.169.1">
     command in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.170.1">
      Databricks CLI:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.171.1">
$ databricks configure</span></pre>
   <p>
    <span class="koboSpan" id="kobo.172.1">
     When
    </span>
    <a id="_idIndexMarker483">
    </a>
    <span class="koboSpan" id="kobo.173.1">
     applying a Terraform configuration file to a target environment, the Terraform CLI will first check to see whether authentication details are provided directly within the configuration file.
    </span>
    <span class="koboSpan" id="kobo.173.2">
     Otherwise, the Terraform CLI will look for the local Databricks configuration file, which gets stored in a special hidden file called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.174.1">
      .databrickscfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.175.1">
     under your user’s
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.176.1">
      home folder.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.177.1">
     You can also specify a profile name, which is helpful when you have
    </span>
    <a id="_idTextAnchor195">
    </a>
    <span class="koboSpan" id="kobo.178.1">
     multiple Databricks workspaces and you need to deploy infrastructure components between the different workspaces.
    </span>
    <span class="koboSpan" id="kobo.178.2">
     Using the profiles, you can store authentication details separately and easily reference them during deployment.
    </span>
    <span class="koboSpan" id="kobo.178.3">
     You can learn more about creating/testing a profile
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.179.1">
      here:
     </span>
    </span>
    <a href="https://docs.databricks.com/dev-tools/cli/profiles.html">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.180.1">
       https://docs.databricks.com/dev-tools/cli/profiles.html
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.181.1">
      .
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-153">
    <a id="_idTextAnchor196">
    </a>
    <span class="koboSpan" id="kobo.182.1">
     Defining a DLT pipeline source notebook
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.183.1">
     In the
    </span>
    <a id="_idIndexMarker484">
    </a>
    <span class="koboSpan" id="kobo.184.1">
     next example, we’re
    </span>
    <a id="_idIndexMarker485">
    </a>
    <span class="koboSpan" id="kobo.185.1">
     going to define a notebook that will contain the start of a simple DLT pipeline and deploy the notebook to your user’s workspace directory in a target
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.186.1">
      Databricks workspace.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.187.1">
     To construct the workspace location of where to deploy the notebook, we’ll need to get your current user in Databricks.
    </span>
    <span class="koboSpan" id="kobo.187.2">
     Rather than hardcoding this value, we can use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.188.1">
      databricks_current_user
     </span>
    </strong>
    <span class="koboSpan" id="kobo.189.1">
     data source, which retrieves the current user’s Databricks username at deployment time.
    </span>
    <span class="koboSpan" id="kobo.189.2">
     Add the following configuration block to the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.190.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.191.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.192.1">
data "databricks_current_user" "my_user" {}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.193.1">
     Next, we’ll use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.194.1">
      databricks_notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.195.1">
     resource to define a new Python notebook, using the previous data source to construct the notebook path.
    </span>
    <span class="koboSpan" id="kobo.195.2">
     Since the notebook is fairly simple, containing only a single DLT dataset definition, we’ll define the notebook contents inline.
    </span>
    <span class="koboSpan" id="kobo.195.3">
     Add the following configuration block to the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.196.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.197.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.198.1">
resource "databricks_notebook" "dlt_pipeline_notebook" {
  path = "${data.databricks_current_user.my_user.home}/chp_8_terraform/my_simple_dlt_pipeline.py"
  language = "PYTHON"
  content_base64 = base64encode(&lt;&lt;-EOT
    import dlt
    @dlt.table(
        comment="The raw NYC taxi cab trip dataset located in `/databricks-datasets/`"
    )
    def yellow_taxi_raw():
        path = "/databricks-datasets/nyctaxi/tripdata/yellow"
        schema = "vendor_id string, pickup_datetime timestamp, dropoff_datetime timestamp, passenger_count integer, trip_distance float, pickup_longitude float, pickup_latitude float, rate_code integer, store_and_fwd_flag integer, dropoff_longitude float, dropoff_lattitude float, payment_type string, fare_amount float, surcharge float, mta_tax float, tip_amount float, tolls_amount float, total_amount float"
        return (spark.readStream
                    .schema(schema)
                    .format("csv")
                    .option("header", True)
                    .load(path))
    EOT
  )
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.199.1">
     Finally, let’s
    </span>
    <a id="_idIndexMarker486">
    </a>
    <span class="koboSpan" id="kobo.200.1">
     add one last block to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.201.1">
      main.tf
     </span>
    </strong>
    <span class="koboSpan" id="kobo.202.1">
     configuration that prints the URL to the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.203.1">
      deployed notebook:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.204.1">
output "notebook_url" {
  value = databricks_notebook.dlt_pipeline_notebook.url
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.205.1">
     Click
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.206.1">
      Save
     </span>
    </strong>
    <span class="koboSpan" id="kobo.207.1">
     to
    </span>
    <a id="_idIndexMarker487">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     save the configuration file.
    </span>
    <span class="koboSpan" id="kobo.208.2">
     In a terminal window, navigate to the directory containing the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.209.1">
      main.tf
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.210.1">
      configuration file.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-154">
    <a id="_idTextAnchor197">
    </a>
    <span class="koboSpan" id="kobo.211.1">
     Applying workspace changes
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.212.1">
     The first
    </span>
    <a id="_idIndexMarker488">
    </a>
    <span class="koboSpan" id="kobo.213.1">
     command that should be run is the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.214.1">
      terraform init
     </span>
    </strong>
    <span class="koboSpan" id="kobo.215.1">
     command, which executes several initialization steps to prepare the current working directory to deploy cloud resources using Terraform.
    </span>
    <span class="koboSpan" id="kobo.215.2">
     Execute the following command from a terminal window or
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.216.1">
      shell prompt:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.217.1">
terraform init</span></pre>
   <p>
    <span class="koboSpan" id="kobo.218.1">
     Next, the Terraform CLI provides a way for us to validate the effects of a Terraform configuration file before applying the changes.
    </span>
    <span class="koboSpan" id="kobo.218.2">
     Execute the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.219.1">
       validate
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.220.1">
      com
     </span>
     <a id="_idTextAnchor198">
     </a>
     <span class="koboSpan" id="kobo.221.1">
      mand:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.222.1">
terraform validate</span></pre>
   <p>
    <span class="koboSpan" id="kobo.223.1">
     Lastly, we can view the proposed infrastructure changes by listing all of the planned changes in the Terraform plan.
    </span>
    <span class="koboSpan" id="kobo.223.2">
     Execute the following command to view the proposed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.224.1">
      Terraform plan:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.225.1">
terraform plan</span></pre>
   <p>
    <span class="koboSpan" id="kobo.226.1">
     You’ll notice that there will be a single resource defined in the plan.
    </span>
    <span class="koboSpan" id="kobo.226.2">
     In this case, it will be the new Databricks notebook containing our DLT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.227.1">
      dataset definition.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.228.1">
     Once we validate that the plan looks good, we can then apply the changes to the target Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.228.2">
     Apply the Terraform plan by executing the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.229.1">
       apply
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.230.1">
      command:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.231.1">
terraform apply</span></pre>
   <p>
    <span class="koboSpan" id="kobo.232.1">
     The output will be the full notebook URL to the newly created notebook.
    </span>
    <span class="koboSpan" id="kobo.232.2">
     Copy the output URL and paste it into a browser window.
    </span>
    <span class="koboSpan" id="kobo.232.3">
     Verify that there is a new notebook with Python set as the default programming language, containing a single notebook cell with the definition of a single DLT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.233.1">
      dataset,
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.234.1">
       yellow_taxi_raw
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.235.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.236.1">
     Congratulations!
    </span>
    <span class="koboSpan" id="kobo.236.2">
     You’ve written your first Terraform configuration file and you are well on your way to automating the deployment of your Databricks assets across environments.
    </span>
    <span class="koboSpan" id="kobo.236.3">
     In the next section, we’ll expand on the previous example to see how the Databricks Terraform provider can be used to deploy DLT pipelines
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.237.1">
      to workspaces.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-155">
    <a id="_idTextAnchor199">
    </a>
    <span class="koboSpan" id="kobo.238.1">
     Configuring DLT pipelines using Terraform
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.239.1">
     We will use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.240.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.241.1">
     resource in the Databricks Terraform provider to deploy a DLT pipeline to a target Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.241.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.242.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.243.1">
     resource is the main building block for our Terraform configuration files.
    </span>
    <span class="koboSpan" id="kobo.243.2">
     Within
    </span>
    <a id="_idIndexMarker489">
    </a>
    <span class="koboSpan" id="kobo.244.1">
     this Terraform resource, we can specify many different configuration options that will affect the deployment of our DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.244.2">
     For example, we can configure the DLT production edition, a target Unity Catalog location, library dependencies, update cluster sizes, and more.
    </span>
    <span class="koboSpan" id="kobo.244.3">
     Let’s dive into the exact configurations to get a better idea of the type of control you have over the DLT pipeline that
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.245.1">
      gets deployed.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.246.1">
     There are several arguments used to define the configuration and behavior of a DLT pipeline using the Databricks provider for Terraform.
    </span>
    <span class="koboSpan" id="kobo.246.2">
     To get a better picture of the types of arguments, the following sections cover all the available arguments in the Databricks provider for Terraform (the latest version can be found
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.247.1">
      here:
     </span>
    </span>
    <a href="https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.248.1">
       https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/pipeline
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.249.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.250.1">
     Generally speaking, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.251.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.252.1">
     arguments can be thought about as falling into one of three
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.253.1">
      possible categories:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.254.1">
       Runtime configuration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.255.1">
      :
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.256.1">
       name
      </span>
     </strong>
     <span class="koboSpan" id="kobo.257.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.258.1">
       channel
      </span>
     </strong>
     <span class="koboSpan" id="kobo.259.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.260.1">
       development
      </span>
     </strong>
     <span class="koboSpan" id="kobo.261.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.262.1">
       continuous
      </span>
     </strong>
     <span class="koboSpan" id="kobo.263.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.264.1">
       edition
      </span>
     </strong>
     <span class="koboSpan" id="kobo.265.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.266.1">
       photon
      </span>
     </strong>
     <span class="koboSpan" id="kobo.267.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.268.1">
       configuration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.269.1">
      ,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.270.1">
       and
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.271.1">
        library
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.272.1">
       Pipeline compute
      </span>
     </strong>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.273.1">
        configuration
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.274.1">
       :
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.275.1">
        cluster
       </span>
      </strong>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.276.1">
       Pipeline dataset storage configuration
      </span>
     </strong>
     <span class="koboSpan" id="kobo.277.1">
      :
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.278.1">
       catalog
      </span>
     </strong>
     <span class="koboSpan" id="kobo.279.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.280.1">
       target
      </span>
     </strong>
     <span class="koboSpan" id="kobo.281.1">
      ,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.282.1">
       and
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.283.1">
        storage
       </span>
      </strong>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.284.1">
     Let’s go
    </span>
    <a id="_idIndexMarker490">
    </a>
    <span class="koboSpan" id="kobo.285.1">
     through each argument in greater detail to get a better understanding of the effect that our Terraform configuration can have on a target
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.286.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-156">
    <a id="_idTextAnchor200">
    </a>
    <span class="koboSpan" id="kobo.287.1">
     name
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.288.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.289.1">
      name
     </span>
    </strong>
    <span class="koboSpan" id="kobo.290.1">
     argument assigns
    </span>
    <a id="_idIndexMarker491">
    </a>
    <span class="koboSpan" id="kobo.291.1">
     an alphanumeric name to identify a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.291.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.292.1">
      name
     </span>
    </strong>
    <span class="koboSpan" id="kobo.293.1">
     argument should be a String that can contain mixed-case characters, numbers, spaces, and special characters (including emoji characters).
    </span>
    <span class="koboSpan" id="kobo.293.2">
     Furthermore, the pipeline
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.294.1">
      name
     </span>
    </strong>
    <span class="koboSpan" id="kobo.295.1">
     argument doesn’t necessarily need to be unique; name uniqueness is not enforced by the Databricks Terraform provider.
    </span>
    <span class="koboSpan" id="kobo.295.2">
     Upon creation of a DLT pipeline, the Databricks Data Intelligence Platform will assign a unique pipeline identifier to each pipeline, so the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.296.1">
      name
     </span>
    </strong>
    <span class="koboSpan" id="kobo.297.1">
     argument is solely used as a convenient way for data engineers to distinguish their DLT pipelines from other pipelines from the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.298.1">
      DLT UI.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-157">
    <a id="_idTextAnchor201">
    </a>
    <span class="koboSpan" id="kobo.299.1">
     notification
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.300.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.301.1">
      notification
     </span>
    </strong>
    <span class="koboSpan" id="kobo.302.1">
     argument is
    </span>
    <a id="_idIndexMarker492">
    </a>
    <span class="koboSpan" id="kobo.303.1">
     used to specify a list of email recipients who will receive an email notification during specific pipeline events.
    </span>
    <span class="koboSpan" id="kobo.303.2">
     The types of DLT pipeline events that will trigger a notification include
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.304.1">
      on-update-success
     </span>
    </strong>
    <span class="koboSpan" id="kobo.305.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.306.1">
      on-update-failure
     </span>
    </strong>
    <span class="koboSpan" id="kobo.307.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.308.1">
      on-update-fatal-failure
     </span>
    </strong>
    <span class="koboSpan" id="kobo.309.1">
     ,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.310.1">
      and
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.311.1">
       on-flow-failure
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.312.1">
      .
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-158">
    <a id="_idTextAnchor202">
    </a>
    <span class="koboSpan" id="kobo.313.1">
     channel
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.314.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.315.1">
      channel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.316.1">
     argument
    </span>
    <a id="_idIndexMarker493">
    </a>
    <span class="koboSpan" id="kobo.317.1">
     controls the type of Databricks runtime a DLT pipeline update cluster should use.
    </span>
    <span class="koboSpan" id="kobo.317.2">
     There are only two options to choose from:
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.318.1">
      CURRENT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.319.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.320.1">
      PREVIEW
     </span>
    </strong>
    <span class="koboSpan" id="kobo.321.1">
     .
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.322.1">
      CURRENT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.323.1">
     selects the latest stable Databricks runtime release and is the default option.
    </span>
    <span class="koboSpan" id="kobo.323.2">
     You may want to choose
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.324.1">
      PREVIEW
     </span>
    </strong>
    <span class="koboSpan" id="kobo.325.1">
     if your DLT pipeline is operating in a development environment, and you’d like to experiment with upcoming performance features and optimizations that haven’t made their way into the current Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.326.1">
      runtime yet.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-159">
    <a id="_idTextAnchor203">
    </a>
    <span class="koboSpan" id="kobo.327.1">
     development
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.328.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.329.1">
      development
     </span>
    </strong>
    <span class="koboSpan" id="kobo.330.1">
     argument is a Boolean flag that controls whether you want to execute
    </span>
    <a id="_idIndexMarker494">
    </a>
    <span class="koboSpan" id="kobo.331.1">
     your DLT pipeline in Development mode or not.
    </span>
    <span class="koboSpan" id="kobo.331.2">
     When set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.332.1">
      true
     </span>
    </strong>
    <span class="koboSpan" id="kobo.333.1">
     , Terraform will deploy a DLT pipeline, with the pipeline mode set to
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.334.1">
      Development
     </span>
    </strong>
    <span class="koboSpan" id="kobo.335.1">
     .
    </span>
    <span class="koboSpan" id="kobo.335.2">
     This will also be reflected on the DLT UI by a toggle button at the top right of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.336.1">
      DLT UI.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer086">
     <span class="koboSpan" id="kobo.337.1">
      <img alt="Figure 8.2 – Development mode is visible from the DLT UI as a toggle button" src="image/B22011_8_2.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.338.1">
     Figure 8.2 – Development mode is visible from the DLT UI as a toggle button
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.339.1">
     Similarly, when this argument is set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.340.1">
      false
     </span>
    </strong>
    <span class="koboSpan" id="kobo.341.1">
     , Terraform will set the pipeline mode to
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.342.1">
      Production
     </span>
    </strong>
    <span class="koboSpan" id="kobo.343.1">
     .
    </span>
    <span class="koboSpan" id="kobo.343.2">
     If you recall from
    </span>
    <a href="B22011_02.xhtml#_idTextAnchor052">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.344.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.345.1">
     , we mentioned that in Development mode, DLT will not retry pipeline updates in the event of a runtime exception and will also keep the update cluster up and running to help data engineers triage and fix bugs, thereby shortening
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.346.1">
      debugging cycles.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-160">
    <a id="_idTextAnchor204">
    </a>
    <span class="koboSpan" id="kobo.347.1">
     continuous
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.348.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.349.1">
      continuous
     </span>
    </strong>
    <span class="koboSpan" id="kobo.350.1">
     argument is
    </span>
    <a id="_idIndexMarker495">
    </a>
    <span class="koboSpan" id="kobo.351.1">
     a Boolean flag that controls the frequency of pipeline update executions.
    </span>
    <span class="koboSpan" id="kobo.351.2">
     When set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.352.1">
      true
     </span>
    </strong>
    <span class="koboSpan" id="kobo.353.1">
     , Terraform will deploy a DLT pipeline that will continuously update datasets within a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.353.2">
     Similarly, when set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.354.1">
      false
     </span>
    </strong>
    <span class="koboSpan" id="kobo.355.1">
     , the DLT pipeline will be deployed with a triggered execution mode.
    </span>
    <span class="koboSpan" id="kobo.355.2">
     In this type of execution mode, a data engineer will need to trigger the start of a pipeline update either by clicking the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.356.1">
      Start
     </span>
    </strong>
    <span class="koboSpan" id="kobo.357.1">
     button on the DLT UI or by invoking the Pipelines REST API to start a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.358.1">
      pipeline update.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-161">
    <a id="_idTextAnchor205">
    </a>
    <span class="koboSpan" id="kobo.359.1">
     edition
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.360.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.361.1">
      edition
     </span>
    </strong>
    <span class="koboSpan" id="kobo.362.1">
     argument
    </span>
    <a id="_idIndexMarker496">
    </a>
    <span class="koboSpan" id="kobo.363.1">
     selects which product edition you would like to use when deploying a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.363.2">
     There are only three options to choose from:
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.364.1">
      CORE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.365.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.366.1">
      PRO
     </span>
    </strong>
    <span class="koboSpan" id="kobo.367.1">
     , and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.368.1">
      ADVANCED
     </span>
    </strong>
    <span class="koboSpan" id="kobo.369.1">
     .
    </span>
    <span class="koboSpan" id="kobo.369.2">
     If you recall from
    </span>
    <a href="B22011_02.xhtml#_idTextAnchor052">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.370.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.371.1">
     , the product edition selects the feature set that you would like to enable when running a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.371.2">
     As a result, the pipeline pricing is reflected in the number of features enabled with an edition.
    </span>
    <span class="koboSpan" id="kobo.371.3">
     For example, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.372.1">
      PRO
     </span>
    </strong>
    <span class="koboSpan" id="kobo.373.1">
     product edition will enable data engineers to use expectations to enforce data quality but will also incur the highest operational pricing.
    </span>
    <span class="koboSpan" id="kobo.373.2">
     On the other hand, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.374.1">
      CORE
     </span>
    </strong>
    <span class="koboSpan" id="kobo.375.1">
     product edition may be used to append incoming data to streaming tables and will incur the least amount of operation charges
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.376.1">
      to update.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-162">
    <a id="_idTextAnchor206">
    </a>
    <span class="koboSpan" id="kobo.377.1">
     photon
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.378.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.379.1">
      photon
     </span>
    </strong>
    <span class="koboSpan" id="kobo.380.1">
     argument
    </span>
    <a id="_idIndexMarker497">
    </a>
    <span class="koboSpan" id="kobo.381.1">
     is a Boolean flag that controls whether to use a Photon processing engine to update a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.381.2">
     When set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.382.1">
      true
     </span>
    </strong>
    <span class="koboSpan" id="kobo.383.1">
     , Terraform will deploy a DLT pipeline with an update cluster having the Photon engine enabled.
    </span>
    <span class="koboSpan" id="kobo.383.2">
     During the dataset updates, your DLT pipeline can take advantage of this fast, vectorized processing engine that makes joins, aggregations, windows, and sorting much faster than the default cluster.
    </span>
    <span class="koboSpan" id="kobo.383.3">
     When set to
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.384.1">
      false
     </span>
    </strong>
    <span class="koboSpan" id="kobo.385.1">
     , DLT will create an update cluster using the traditional Catalyst engine in Spark.
    </span>
    <span class="koboSpan" id="kobo.385.2">
     Due to faster processing and improved performance, enabling Photon execution will incur higher
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.386.1">
      DBU pricing.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-163">
    <a id="_idTextAnchor207">
    </a>
    <span class="koboSpan" id="kobo.387.1">
     configuration
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.388.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.389.1">
      configuration
     </span>
    </strong>
    <span class="koboSpan" id="kobo.390.1">
     argument
    </span>
    <a id="_idIndexMarker498">
    </a>
    <span class="koboSpan" id="kobo.391.1">
     allows data engineers to deploy a DLT pipeline with optional runtime configuration.
    </span>
    <span class="koboSpan" id="kobo.391.2">
     The configuration argument is an optional list of key-value pairs.
    </span>
    <span class="koboSpan" id="kobo.391.3">
     This argument can be used to populate environment variables, cloud storage locations, or cluster shutdown settings,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.392.1">
      for example.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-164">
    <a id="_idTextAnchor208">
    </a>
    <span class="koboSpan" id="kobo.393.1">
     library
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.394.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.395.1">
      library
     </span>
    </strong>
    <span class="koboSpan" id="kobo.396.1">
     argument
    </span>
    <a id="_idIndexMarker499">
    </a>
    <span class="koboSpan" id="kobo.397.1">
     can be used to install cluster libraries that a DLT pipeline update might depend on in order to apply updates
    </span>
    <a id="_idTextAnchor209">
    </a>
    <span class="koboSpan" id="kobo.398.1">
     to a pipeline.
    </span>
    <span class="koboSpan" id="kobo.398.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.399.1">
      library
     </span>
    </strong>
    <span class="koboSpan" id="kobo.400.1">
     argument also adds support for referencing local notebook or arbitrary file dependencies, if data engineers wish to include dependent code artifacts using local files as opposed to build artifacts.
    </span>
    <span class="koboSpan" id="kobo.400.2">
     For example, the following
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.401.1">
      library
     </span>
    </strong>
    <span class="koboSpan" id="kobo.402.1">
     block could be used to include a custom date utility defined as a Python file in the user’s home directory in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.403.1">
      their workspace:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.404.1">
library {
  notebook {
    path = "/Users/&lt;username&gt;/common/utils/date_utils.py"
  }
}</span></pre>
   <h2 id="_idParaDest-165">
    <a id="_idTextAnchor210">
    </a>
    <span class="koboSpan" id="kobo.405.1">
     cluster
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.406.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.407.1">
      cluster
     </span>
    </strong>
    <span class="koboSpan" id="kobo.408.1">
     argument
    </span>
    <a id="_idIndexMarker500">
    </a>
    <span class="koboSpan" id="kobo.409.1">
     controls what cluster is used by the pipeline during an update, maintenance activities, or the default cluster type to use in both update and maintenance tasks.
    </span>
    <span class="koboSpan" id="kobo.409.2">
     If no
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.410.1">
      cluster
     </span>
    </strong>
    <span class="koboSpan" id="kobo.411.1">
     block is specified, DLT will create a default cluster to use to apply updates to a pipeline’s datasets.
    </span>
    <span class="koboSpan" id="kobo.411.2">
     Furthermore, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.412.1">
      cluster
     </span>
    </strong>
    <span class="koboSpan" id="kobo.413.1">
     argument will also contain a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.414.1">
      mode
     </span>
    </strong>
    <span class="koboSpan" id="kobo.415.1">
     parameter where you can specify what type of autoscaling to use.
    </span>
    <span class="koboSpan" id="kobo.415.2">
     If you recall, in
    </span>
    <a href="B22011_02.xhtml#_idTextAnchor052">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.416.1">
        Chapter 2
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.417.1">
     , we described two autoscaling modes in DLT:
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.418.1">
      LEGACY
     </span>
    </strong>
    <span class="koboSpan" id="kobo.419.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.420.1">
      ENHANCED
     </span>
    </strong>
    <span class="koboSpan" id="kobo.421.1">
     .
    </span>
    <span class="koboSpan" id="kobo.421.2">
     For example, the following configuration will create an update cluster that will autoscale from a minimum of three worker nodes to a maximum of eight nodes using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.422.1">
      ENHANCED
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.423.1">
      autoscaling algorithm:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.424.1">
cluster {
  node_type_id = "i3.xlarge"
  autoscale {
    min_workers = 3
    max_workers = 8
    mode = "ENHANCED"
  }
}</span></pre>
   <h2 id="_idParaDest-166">
    <a id="_idTextAnchor211">
    </a>
    <span class="koboSpan" id="kobo.425.1">
     catalog
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.426.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.427.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.428.1">
     argument
    </span>
    <a id="_idIndexMarker501">
    </a>
    <span class="koboSpan" id="kobo.429.1">
     determines the catalog to store the output datasets of a DLT pipeline in Unity Catalog.
    </span>
    <span class="koboSpan" id="kobo.429.2">
     As a DLT pipeline executes the definitions for datasets outlined in a DLT pipeline, these datasets need to have some destination location specified.
    </span>
    <span class="koboSpan" id="kobo.429.3">
     You can specify a combination of a catalog and a schema (covered in the next section,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.430.1">
      target
     </span>
    </em>
    <span class="koboSpan" id="kobo.431.1">
     ) or you can specify a cloud storage location – but not both.
    </span>
    <span class="koboSpan" id="kobo.431.2">
     This argument is mutually
    </span>
    <a id="_idIndexMarker502">
    </a>
    <span class="koboSpan" id="kobo.432.1">
     exclusive to the following argument, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.433.1">
      storage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.434.1">
     argument.
    </span>
    <span class="koboSpan" id="kobo.434.2">
     Alternatively, data engineers can continue to store a DLT pipeline’s datasets in the legacy Hive Metastore specifying the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.435.1">
      following configuration:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.436.1">
catalog {
  name = "hive_metastore"
}</span></pre>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.437.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.438.1">
     If either the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.439.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.440.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.441.1">
      storage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.442.1">
     arguments are changed in a Terraform configuration file and the changes are applied, Terraform will recreate the entire DLT pipeline with the new changes.
    </span>
    <span class="koboSpan" id="kobo.442.2">
     These values cannot be updated in the original DLT pipeline
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.443.1">
      once deployed.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-167">
    <a id="_idTextAnchor212">
    </a>
    <span class="koboSpan" id="kobo.444.1">
     target
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.445.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.446.1">
      target
     </span>
    </strong>
    <span class="koboSpan" id="kobo.447.1">
     argument
    </span>
    <a id="_idIndexMarker503">
    </a>
    <span class="koboSpan" id="kobo.448.1">
     specifies the schema in which to store the output datasets defined in a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.448.2">
     This argument, combined with the previous
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.449.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.450.1">
     argument, specifies a fully qualified schema in Unity Catalog or the legacy Hive Metastore.
    </span>
    <span class="koboSpan" id="kobo.450.2">
     Data engineers may choose to use the values set in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.451.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.452.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.453.1">
      target
     </span>
    </strong>
    <span class="koboSpan" id="kobo.454.1">
     arguments as a convenient means for querying the intermediate datasets of a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.454.2">
     This may be for common tasks such as data validation, debugging, or general
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.455.1">
      data wrangling.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-168">
    <a id="_idTextAnchor213">
    </a>
    <span class="koboSpan" id="kobo.456.1">
     storage
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.457.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.458.1">
      storage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.459.1">
     argument
    </span>
    <a id="_idIndexMarker504">
    </a>
    <span class="koboSpan" id="kobo.460.1">
     can be used to specify a cloud storage location to store the output datasets and other related metadata for a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.460.2">
     It’s important to keep in mind that this argument is mutually exclusive to the preceding argument, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.461.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.462.1">
     argument.
    </span>
    <span class="koboSpan" id="kobo.462.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.463.1">
      storage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.464.1">
     argument may contain a fully qualified storage location path, a volumes location, or a location in
    </span>
    <a id="_idIndexMarker505">
    </a>
    <span class="koboSpan" id="kobo.465.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.466.1">
      Databricks File System
     </span>
    </strong>
    <span class="koboSpan" id="kobo.467.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.468.1">
      DBFS
     </span>
    </strong>
    <span class="koboSpan" id="kobo.469.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.469.2">
     For example, the following configuration block would create a DLT pipeline whose output datasets would be stored in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.470.1">
      the DBFS:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.471.1">
storage {
  path = "/pipelines/my-dlt-pipeline-output/"
}</span></pre>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.472.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.473.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.474.1">
      storage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.475.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.476.1">
      catalog
     </span>
    </strong>
    <span class="koboSpan" id="kobo.477.1">
     arguments are mutually exclusive to one another.
    </span>
    <span class="koboSpan" id="kobo.477.2">
     You may only specify one when authoring a Terraform
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.478.1">
      configuration file.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.479.1">
     By now, you
    </span>
    <a id="_idIndexMarker506">
    </a>
    <span class="koboSpan" id="kobo.480.1">
     should feel confident in using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.481.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.482.1">
     resource to declare DLT pipelines using the Databricks provider for Terraform.
    </span>
    <span class="koboSpan" id="kobo.482.2">
     You should also have a greater understanding of the different types of configuration options available to customize a target DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.482.3">
     In the next section, we’ll look at how we can automate the deployment of DLT pipelines using existing version control systems so that the latest changes are synchronized across target workspaces as soon as they are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.483.1">
      made available.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-169">
    <a id="_idTextAnchor214">
    </a>
    <span class="koboSpan" id="kobo.484.1">
     Automating DLT pipeline deployment
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.485.1">
     Terraform
    </span>
    <a id="_idIndexMarker507">
    </a>
    <span class="koboSpan" id="kobo.486.1">
     can be combined with automated
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.487.1">
      Continuous Integration/Continuous Deployment
     </span>
    </strong>
    <span class="koboSpan" id="kobo.488.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.489.1">
      CI/CD
     </span>
    </strong>
    <span class="koboSpan" id="kobo.490.1">
     ) tools, such as
    </span>
    <a id="_idIndexMarker508">
    </a>
    <span class="koboSpan" id="kobo.491.1">
     GitHub Actions or Azure DevOps Pipelines to automatically deploy code changes to your Databricks workspaces.
    </span>
    <span class="koboSpan" id="kobo.491.2">
     Since Terraform is cross-platform, the target Databricks workspace can be in one of the major cloud providers: GCP, AWS, or Azure.
    </span>
    <span class="koboSpan" id="kobo.491.3">
     This allows your development team to maintain infrastructure in a single set of code artifacts while also being agile enough to apply the same resources to alternate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.492.1">
      cloud providers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.493.1">
     Let’s walk through a typical CI/CD process that uses the Databricks provider for Terraform to deploy Databricks resources to target workspaces.
    </span>
    <span class="koboSpan" id="kobo.493.2">
     The CI/CD process will contain two automated build pipelines – one that will be used to validate changes made in feature branches, and a second that will be used to deploy changes that have been approved and merged into the main code branch to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.494.1">
      Databricks workspaces.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.495.1">
     First, a team member creates a new feature branch to track changes to their organization’s IaC code base.
    </span>
    <span class="koboSpan" id="kobo.495.2">
     Once finished, the engineer will open a pull request, requesting one or more peers from their team to review the changes, leave feedback, and approve
    </span>
    <a id="_idIndexMarker509">
    </a>
    <span class="koboSpan" id="kobo.496.1">
     or reject the changes.
    </span>
    <span class="koboSpan" id="kobo.496.2">
     Upon opening a pull request, the build pipeline will be triggered to run, which will check out the feature branch, initialize the current working directory using the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.497.1">
      init
     </span>
    </strong>
    <span class="koboSpan" id="kobo.498.1">
     command, validate the Terraform plan using the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.499.1">
      validate
     </span>
    </strong>
    <span class="koboSpan" id="kobo.500.1">
     command, and generate an output in the form of a Terraform plan.
    </span>
    <span class="koboSpan" id="kobo.500.2">
     Optionally, this Terraform plan can be automatically included in the pull request as a comment for peers
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.501.1">
      to review.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.502.1">
     When the pull request has been approved by their team members, the feature branch can be merged into the main code repository branch – or the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.503.1">
      main
     </span>
    </strong>
    <span class="koboSpan" id="kobo.504.1">
     branch,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.505.1">
      for short.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.506.1">
     Once the feature branch has been merged into the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.507.1">
      main
     </span>
    </strong>
    <span class="koboSpan" id="kobo.508.1">
     branch, the build release pipeline is triggered to run.
    </span>
    <span class="koboSpan" id="kobo.508.2">
     The build release pipeline will check out the latest copy of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.509.1">
      main
     </span>
    </strong>
    <span class="koboSpan" id="kobo.510.1">
     branch and apply the changes using the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.511.1">
      apply
     </span>
    </strong>
    <span class="koboSpan" id="kobo.512.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.512.2">
     Upon applying the Terraform plan, new changes to the organization’s infrastructure will be reflected in the target
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.513.1">
      Databricks workspace.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer087">
     <span class="koboSpan" id="kobo.514.1">
      <img alt="Figure 8.3 – Automatic deployment of Databricks resources using build tools" src="image/B22011_08_3.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.515.1">
     Figure 8.3 – Automatic deployment of Databricks resources using build tools
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.516.1">
     By now, you
    </span>
    <a id="_idIndexMarker510">
    </a>
    <span class="koboSpan" id="kobo.517.1">
     should have a complete understanding of how to design an automatic Databricks deployment using tools such as Azure DevOps to synchronize infrastructure changes through Terraform.
    </span>
    <span class="koboSpan" id="kobo.517.2">
     Let’s combine everything that we’ve learned in the preceding sections to deploy our very own DLT pipeline to a target Databricks workspace using a typical
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.518.1">
      development environment.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-170">
    <a id="_idTextAnchor215">
    </a>
    <span class="koboSpan" id="kobo.519.1">
     Hands-on exercise – deploying a DLT pipeline using VS Code
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.520.1">
     In this hands-on
    </span>
    <a id="_idIndexMarker511">
    </a>
    <span class="koboSpan" id="kobo.521.1">
     exercise, we’ll be using the popular code editor,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.522.1">
      Visual Studio Code
     </span>
    </strong>
    <span class="koboSpan" id="kobo.523.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.524.1">
      VS Code
     </span>
    </strong>
    <span class="koboSpan" id="kobo.525.1">
     ), to author new Terraform configuration files for deploying a DLT pipeline to a target Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.525.2">
     VS Code has gained immense popularity over the years due to its ease of use, light memory footprint, friendly code navigation, syntax highlighting, and code refactoring, as well as a great community of extensions.
    </span>
    <span class="koboSpan" id="kobo.525.3">
     Plus, VS Code is built around an open source community, meaning it’s free to download and use.
    </span>
    <span class="koboSpan" id="kobo.525.4">
     Furthermore, VS Code is a cross-platform code editor, supporting Windows, macOS, and Linux operating systems.
    </span>
    <span class="koboSpan" id="kobo.525.5">
     In this hands-on exercise, we’ll
    </span>
    <a id="_idIndexMarker512">
    </a>
    <span class="koboSpan" id="kobo.526.1">
     be using one of the community extensions, the Terraform plugin for VS Code, which is authored by HashiCorp to help assist in the development of Terraform configuration files.
    </span>
    <span class="koboSpan" id="kobo.526.2">
     For example, the Terraform plugin for VS Code features Terraform syntax highlighting, auto-completion, code formatting, access to Terraform commands from the VS Code command palette, and overall, provides an easy experience navigating Terraform configuration files for deploying Databricks
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.527.1">
      workspace objects.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-171">
    <a id="_idTextAnchor216">
    </a>
    <span class="koboSpan" id="kobo.528.1">
     Setting up VS Code
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.529.1">
     VS C
    </span>
    <a href="https://code.visualstudio.com/download">
     <span class="koboSpan" id="kobo.530.1">
      ode can be
     </span>
     <span id="_idIndexMarker513">
     </span>
     <span class="koboSpan" id="kobo.531.1">
      downloaded from its websit
     </span>
    </a>
    <span class="koboSpan" id="kobo.532.1">
     e located at
    </span>
    <a href="https://code.visualstudio.com/download">
     <span class="koboSpan" id="kobo.533.1">
      https://code.visualstudio.com/download
     </span>
    </a>
    <span class="koboSpan" id="kobo.534.1">
     .
    </span>
    <span class="koboSpan" id="kobo.534.2">
     If you haven’t installed VS Code yet, select the installer download for the operating system that matches your local machine.
    </span>
    <span class="koboSpan" id="kobo.534.3">
     The installer may take a few minutes to download, depending on your network connection speed.
    </span>
    <span class="koboSpan" id="kobo.534.4">
     Once the installer has been downloaded, unzip the ZIP file to reveal the downloaded contents.
    </span>
    <span class="koboSpan" id="kobo.534.5">
     Next, double-click the application file,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.535.1">
      Visual Studio Code
     </span>
    </strong>
    <span class="koboSpan" id="kobo.536.1">
     , to launch the code editor.
    </span>
    <span class="koboSpan" id="kobo.536.2">
     Alternatively, you can move the application file to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.537.1">
      Applications
     </span>
    </strong>
    <span class="koboSpan" id="kobo.538.1">
     directory of your local
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.539.1">
      operating system.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.540.1">
     Next, let’s install the Terraform extension by HashiCorp.
    </span>
    <span class="koboSpan" id="kobo.540.2">
     In a web browser window, navigate to the Terraform extension in the Visual Studio Marketplace website located at
    </span>
    <a href="https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform">
     <span class="koboSpan" id="kobo.541.1">
      https://marketplace.visualstudio.com/items?itemName=HashiCorp.terraform
     </span>
    </a>
    <span class="koboSpan" id="kobo.542.1">
     .
    </span>
    <span class="koboSpan" id="kobo.542.2">
     Or you can search for the extension in the Marketplace search box in VS Code.
    </span>
    <span class="koboSpan" id="kobo.542.3">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.543.1">
      Install
     </span>
    </strong>
    <span class="koboSpan" id="kobo.544.1">
     button to download and install the Terraform extension for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.545.1">
      VS Code.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer088">
     <span class="koboSpan" id="kobo.546.1">
      <img alt="Figure 8.4 – The Terraform extension for VS Code by HashiCorp can be installed from the Visual Studio Marketplace" src="image/B22011_08_4.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.547.1">
     Figure 8.4 – The Terraform extension for VS Code by HashiCorp can be installed from the Visual Studio Marketplace
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.548.1">
     You may be
    </span>
    <a id="_idIndexMarker514">
    </a>
    <span class="koboSpan" id="kobo.549.1">
     prompted to allow your web browser to open the VS Code application on your local machine.
    </span>
    <span class="koboSpan" id="kobo.549.2">
     If so, click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.550.1">
      Allow
     </span>
    </strong>
    <span class="koboSpan" id="kobo.551.1">
     button to open VS Code and install the extension.
    </span>
    <span class="koboSpan" id="kobo.551.2">
     The extension will be downloaded and installed in just a few minutes.
    </span>
    <span class="koboSpan" id="kobo.551.3">
     Once the installation has been completed, you should now see menu items for HashiCorp Terraform on the left-hand side navigation bar of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.552.1">
      VS Code.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer089">
     <span class="koboSpan" id="kobo.553.1">
      <img alt="Figure 8.5 – The HashiCorp Terraform extension will create new menu items in the left-hand side navigation bar" src="image/B22011_08_5.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.554.1">
     Figure 8.5 – The HashiCorp Terraform extension will create new menu items in the left-hand side navigation bar
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.555.1">
     Now that the Terraform extension has been successfully installed, the extension will automatically be activated when the code editor detects a Terraform file.
    </span>
    <span class="koboSpan" id="kobo.555.2">
     You can verify that the extension is activated by a Terraform logo, which will appear in the bottom right-hand corner of the opened
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.556.1">
      Terraform file.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-172">
    <a id="_idTextAnchor217">
    </a>
    <span class="koboSpan" id="kobo.557.1">
     Creating a new Terraform project
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.558.1">
     Let’s create
    </span>
    <a id="_idIndexMarker515">
    </a>
    <span class="koboSpan" id="kobo.559.1">
     a new directory for our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.560.1">
      hands-on exercise:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.561.1">
$ mkdir chapter_8_hands_on
$ cd chapter_8_hands_on</span></pre>
   <p>
    <span class="koboSpan" id="kobo.562.1">
     Create an empty Terraform configuration file, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.563.1">
      main.tf
     </span>
    </strong>
    <span class="koboSpan" id="kobo.564.1">
     , either from a shell prompt or using
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.565.1">
      VS Code:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.566.1">
$ touch main.tf</span></pre>
   <p>
    <span class="koboSpan" id="kobo.567.1">
     Optionally, you can clone the sample project from this chapter’s GitHub repo, located at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08">
     <span class="koboSpan" id="kobo.568.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter08
     </span>
    </a>
    <span class="koboSpan" id="kobo.569.1">
     .
    </span>
    <span class="koboSpan" id="kobo.569.2">
     Next, open the directory in VS Code by selecting
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.570.1">
      File
     </span>
    </strong>
    <span class="koboSpan" id="kobo.571.1">
     |
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.572.1">
      Open Folder
     </span>
    </strong>
    <span class="koboSpan" id="kobo.573.1">
     and navigating to the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.574.1">
      directory’s location.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-173">
    <a id="_idTextAnchor218">
    </a>
    <span class="koboSpan" id="kobo.575.1">
     Defining the Terraform resources
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.576.1">
     Let’s start by
    </span>
    <a id="_idIndexMarker516">
    </a>
    <span class="koboSpan" id="kobo.577.1">
     expanding the Terraform example introduced in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.578.1">
      Setting up a local Terraform environment
     </span>
    </em>
    <span class="koboSpan" id="kobo.579.1">
     section at the beginning of this chapter.
    </span>
    <span class="koboSpan" id="kobo.579.2">
     Either copy the existing
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.580.1">
      main.tf
     </span>
    </strong>
    <span class="koboSpan" id="kobo.581.1">
     file or feel free to directly edit the body of the existing
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.582.1">
      main.tf
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.583.1">
      configuration file.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.584.1">
     First, let’s begin by adding a second dataset to the DLT pipeline definition in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.585.1">
      databricks_notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.586.1">
     resource definition (the code from the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.587.1">
      Defining a DLT pipeline source notebook
     </span>
    </em>
    <span class="koboSpan" id="kobo.588.1">
     section has been truncated for brevity in the following code block).
    </span>
    <span class="koboSpan" id="kobo.588.2">
     We will now have a data pipeline containing two datasets – a bronze layer followed by a silver layer.
    </span>
    <span class="koboSpan" id="kobo.588.3">
     Update the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.589.1">
      databricks_notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.590.1">
     resource definition in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.591.1">
      main.tf
     </span>
    </strong>
    <span class="koboSpan" id="kobo.592.1">
     file with the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.593.1">
      following definition:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.594.1">
resource "databricks_notebook" "dlt_pipeline_notebook" {
  path = "${data.databricks_current_user.my_user.home}/chp_8_terraform/taxi_trips_pipeline.py"
...
</span><span class="koboSpan" id="kobo.594.2">                    .load(path))
    @dlt.table(
        name="yellow_taxi_silver",
        comment="Financial information from incoming taxi trips."
</span><span class="koboSpan" id="kobo.594.3">    )
    @dlt.expect_or_fail("valid_total_amount", "total_amount &gt; 0.0")
    def yellow_taxi_silver():
        return (dlt.readStream("yellow_taxi_bronze")
                    .withColumn("driver_payment",
                                F.expr("total_amount * 0.40"))
                    .withColumn("vehicle_maintenance_fee",
                                F.expr("total_amount * 0.05"))
                    .withColumn("adminstrative_fee",
                                F.expr("total_amount * 0.1"))
                    .withColumn("potential_profits",
                                F.expr("total_amount * 0.45")))
    EOT
  )
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.595.1">
     Next, before we can create a new DLT pipeline, we’ll want to define a location in Unity Catalog in which to store the pipeline datasets.
    </span>
    <span class="koboSpan" id="kobo.595.2">
     Add the following catalog and schema resource definitions
    </span>
    <a id="_idIndexMarker517">
    </a>
    <span class="koboSpan" id="kobo.596.1">
     to the bottom of the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.597.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.598.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.599.1">
resource "databricks_catalog" "dlt_target_catalog" {
  name = "chp8_deploying_pipelines_w_terraform"
  comment = "The target catalog for Taxi Trips DLT pipeline"
}
resource "databricks_schema" "dlt_target_schema" {
  catalog_name = databricks_catalog.dlt_target_catalog.id
  name = "terraform_demo"
  comment = "The target schema for Taxi Trips DLT pipeline"
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.600.1">
     Now that we have an updated source notebook containing the definition of our DLT pipeline, as well as a location to store the pipeline datasets, we can define a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.600.2">
     Add the following pipeline definition to the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.601.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.602.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.603.1">
resource "databricks_pipeline" "taxi_trips_pipeline" {
  name = "Taxi Trips Pipeline"
  library {
    notebook {
      path = "${data.databricks_current_user.my_user.home}/chp_8_terraform/taxi_trips_pipeline.py"
    }
  }
  cluster {
    label = "default"
    num_workers = 2
    autoscale {
      min_workers = 2
      max_workers = 4
      mode = "ENHANCED"
    }
    driver_node_type_id = "i3.2xlarge"
    node_type_id = "i3.xlarge"
  }
  continuous = false
  development = true
  photon = false
  serverless = false
  catalog = databricks_catalog.dlt_target_catalog.name
  target = databricks_schema.dlt_target_schema.name
  edition = "ADVANCED"
  channel = "CURRENT"
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.604.1">
     You’ll
    </span>
    <a id="_idIndexMarker518">
    </a>
    <span class="koboSpan" id="kobo.605.1">
     notice that we’ve defined the location for the notebook containing the DLT pipeline definition, a default cluster to use for pipeline updates and maintenance tasks, as well as other runtime settings such as the Development mode, product edition, channel,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.606.1">
      and more.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.607.1">
     Next, we’ll want to orchestrate the updates to our DLT pipeline so that we can trigger runs on a repeated schedule, configure alert notifications, or set timeout thresholds.
    </span>
    <span class="koboSpan" id="kobo.607.2">
     Add the following workflow definition to the bottom of the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.608.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.609.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.610.1">
resource "databricks_job" "taxi_trips_pipeline_job" {
  name = "Taxi Trips Pipeline Update Job"
  description = "Databricks Workflow that executes a pipeline update of the Taxi Trips DLT pipeline."
</span><span class="koboSpan" id="kobo.610.2">  job_cluster {
    job_cluster_key = "taxi_trips_pipeline_update_job_cluster"
    new_cluster {
      num_workers = 2
      spark_version = "15.4.x-scala2.12"
      node_type_id  = "i3.xlarge"
      driver_node_type_id = "i3.2xlarge"
    }
  }
  task {
    task_key = "update_taxi_trips_pipeline"
    pipeline_task {
      pipeline_id = databricks_pipeline.taxi_trips_pipeline.id
    }
  }
  trigger {
    pause_status = "PAUSED"
    periodic {
      interval = "1"
      unit = "HOURS"
    }
  }
}</span></pre>
   <p>
    <span class="koboSpan" id="kobo.611.1">
     Lastly, we’ll
    </span>
    <a id="_idIndexMarker519">
    </a>
    <span class="koboSpan" id="kobo.612.1">
     want to output the workflow URL of the deployed resource so that we can open the workflow UI easily from a browser.
    </span>
    <span class="koboSpan" id="kobo.612.2">
     Add the following output definition to the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.613.1">
       main.tf
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.614.1">
      file:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.615.1">
output "workflow_url" {
  value = databricks_job.taxi_trips_pipeline_job.url
}</span></pre>
   <h2 id="_idParaDest-174">
    <a id="_idTextAnchor219">
    </a>
    <span class="koboSpan" id="kobo.616.1">
     Deploying the Terraform project
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.617.1">
     Before we
    </span>
    <a id="_idIndexMarker520">
    </a>
    <span class="koboSpan" id="kobo.618.1">
     can begin deploying new resources, the first step is to initialize the Terraform project.
    </span>
    <span class="koboSpan" id="kobo.618.2">
     Execute the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.619.1">
      init
     </span>
    </strong>
    <span class="koboSpan" id="kobo.620.1">
     command in the parent directory either from the VS Code command palette or from a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.621.1">
      shell prompt:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.622.1">
$ terraform init</span></pre>
   <p>
    <span class="koboSpan" id="kobo.623.1">
     Next, preview the changes in the Terraform file by executing the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.624.1">
      plan
     </span>
    </strong>
    <span class="koboSpan" id="kobo.625.1">
     command to view the proposed
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.626.1">
      infrastructure changes:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.627.1">
$ terraform plan</span></pre>
   <p>
    <span class="koboSpan" id="kobo.628.1">
     In total, there should be five new resources created, including the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.629.1">
      databricks_notebook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.630.1">
     resource, which represents the notebook containing the DLT pipeline definition, the target Unity Catalog’s catalog, the target Unity Catalog schema, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.631.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.632.1">
     resource, which represents our DLT pipeline, and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.633.1">
      databricks_job
     </span>
    </strong>
    <span class="koboSpan" id="kobo.634.1">
     resource, which represents the workflow that will trigger
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.635.1">
      pipeline updates.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.636.1">
     After we’ve validated the plan, we can now deploy our DLT pipeline to a Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.636.2">
     Next, execute the Terraform
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.637.1">
      apply
     </span>
    </strong>
    <span class="koboSpan" id="kobo.638.1">
     command to deploy the new infrastructure changes to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.639.1">
      our workspace:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.640.1">
$ terraform apply</span></pre>
   <p>
    <span class="koboSpan" id="kobo.641.1">
     Once all resource changes have been applied, you should expect Terraform to output the URL to the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.642.1">
      Databricks workflow.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.643.1">
     Copy and
    </span>
    <a id="_idIndexMarker521">
    </a>
    <span class="koboSpan" id="kobo.644.1">
     paste the workflow URL into a browser window and ensure that the address resolves to the newly created workflow in the target workspace.
    </span>
    <span class="koboSpan" id="kobo.644.2">
     You’ll notice that the new workflow contains a single task for updating the DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.644.3">
     The workflow is paused, as outlined in the Terraform configuration.
    </span>
    <span class="koboSpan" id="kobo.644.4">
     Optionally, you can click the blue
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.645.1">
      Run now
     </span>
    </strong>
    <span class="koboSpan" id="kobo.646.1">
     button to trigger a new, immediate run of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.647.1">
      the workflow.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer090">
     <span class="koboSpan" id="kobo.648.1">
      <img alt="Figure 8.6 – Terraform will output the ﻿workflow URL for updating the DLT pipeline" src="image/B22011_08_6.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.649.1">
     Figure 8.6 – Terraform will output the workflow URL for updating the DLT pipeline
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.650.1">
     As simple as it was to deploy our changes to the target Databricks workspace, it’s just as easy to undeploy the changes.
    </span>
    <span class="koboSpan" id="kobo.650.2">
     Execute the following command to remove all the resource changes from the target
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.651.1">
      Databricks workspace:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.652.1">
$ terraform destroy</span></pre>
   <p>
    <span class="koboSpan" id="kobo.653.1">
     Confirm the decision by entering the word
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.654.1">
      yes
     </span>
    </strong>
    <span class="koboSpan" id="kobo.655.1">
     .
    </span>
    <span class="koboSpan" id="kobo.655.2">
     It may take a few minutes to fully undeploy all of the resources from your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.656.1">
      Databricks workspace.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.657.1">
     As you saw, with
    </span>
    <a id="_idIndexMarker522">
    </a>
    <span class="koboSpan" id="kobo.658.1">
     just a few keystrokes and a few clicks of the button, it was fast and easy to provision and deprovision resources in a Databricks workspace using the Databricks Terraform provider.
    </span>
    <span class="koboSpan" id="kobo.658.2">
     Rather than instructing Terraform how to deploy the resources to our target Databricks workspace, we focused on what changes to make through configuration and let the Terraform tool handle the heavy lifting
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.659.1">
      for us.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-175">
    <a id="_idTextAnchor220">
    </a>
    <span class="koboSpan" id="kobo.660.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.661.1">
     In this chapter, we covered how to use the Databricks provider for Terraform to implement a CI/CD process for deploying data pipelines across workspaces.
    </span>
    <span class="koboSpan" id="kobo.661.2">
     We saw how easy it was to set up a local development environment for working with Terraform configuration files and how easy it was to test our Terraform plans before applying them to a target environment.
    </span>
    <span class="koboSpan" id="kobo.661.3">
     We also installed the Databricks Terraform provider from the Terraform Registry and imported the provider into Terraform configuration files.
    </span>
    <span class="koboSpan" id="kobo.661.4">
     Next, we dove into the details of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.662.1">
      databricks_pipeline
     </span>
    </strong>
    <span class="koboSpan" id="kobo.663.1">
     resource, which is used by the Databricks Terraform provider to deploy a DLT pipeline to a target workspace.
    </span>
    <span class="koboSpan" id="kobo.663.2">
     We inspected each argument in the resource specification and saw how we coul
    </span>
    <a id="_idTextAnchor221">
    </a>
    <span class="koboSpan" id="kobo.664.1">
     d control the DLT pipeline runtime configuration, the compute settings, and even the location of the output datasets from our pipeline.
    </span>
    <span class="koboSpan" id="kobo.664.2">
     Lastly, we saw how easy it was to automate our Terraform configuration files by storing them in a version control system such as GitHub and automating the deployment using a build tool such as Azure DevOps Pipelines.
    </span>
    <span class="koboSpan" id="kobo.664.3">
     We concluded the chapter with a hands-on example of using the Terraform extension with the popular code editor VS Code to deploy a DLT pipeline from your local
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.665.1">
      development environment.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.666.1">
     However, Terraform isn’t for everyone and it may be the case that it’s too complex or too difficult to use for your use case.
    </span>
    <span class="koboSpan" id="kobo.666.2">
     In the next chapter, we’ll dive into
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.667.1">
      Databricks Asset Bundles
     </span>
    </strong>
    <span class="koboSpan" id="kobo.668.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.669.1">
      DABs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.670.1">
     ), which is another CI/CD tool that makes it simple to package and deploy Databricks code artifacts for data and machine
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.671.1">
      learning workloads.
     </span>
    </span>
   </p>
  </div>
 </body></html>