["```py\nterraform {\n  required_providers {\n    databricks = {\n    source = \"databricks/databricks\"\n    }\n  }\n}\n```", "```py\n$ brew tap databricks/tap\n$ brew install databricks\n```", "```py\n$ winget search databricks\n$ winget install Databricks.DatabricksCLI\n```", "```py\n$ databricks configure\n```", "```py\ndata \"databricks_current_user\" \"my_user\" {}\n```", "```py\nresource \"databricks_notebook\" \"dlt_pipeline_notebook\" {\n  path = \"${data.databricks_current_user.my_user.home}/chp_8_terraform/my_simple_dlt_pipeline.py\"\n  language = \"PYTHON\"\n  content_base64 = base64encode(<<-EOT\n    import dlt\n    @dlt.table(\n        comment=\"The raw NYC taxi cab trip dataset located in `/databricks-datasets/`\"\n    )\n    def yellow_taxi_raw():\n        path = \"/databricks-datasets/nyctaxi/tripdata/yellow\"\n        schema = \"vendor_id string, pickup_datetime timestamp, dropoff_datetime timestamp, passenger_count integer, trip_distance float, pickup_longitude float, pickup_latitude float, rate_code integer, store_and_fwd_flag integer, dropoff_longitude float, dropoff_lattitude float, payment_type string, fare_amount float, surcharge float, mta_tax float, tip_amount float, tolls_amount float, total_amount float\"\n        return (spark.readStream\n                    .schema(schema)\n                    .format(\"csv\")\n                    .option(\"header\", True)\n                    .load(path))\n    EOT\n  )\n}\n```", "```py\noutput \"notebook_url\" {\n  value = databricks_notebook.dlt_pipeline_notebook.url\n}\n```", "```py\nterraform init\n```", "```py\nterraform validate\n```", "```py\nterraform plan\n```", "```py\nterraform apply\n```", "```py\nlibrary {\n  notebook {\n    path = \"/Users/<username>/common/utils/date_utils.py\"\n  }\n}\n```", "```py\ncluster {\n  node_type_id = \"i3.xlarge\"\n  autoscale {\n    min_workers = 3\n    max_workers = 8\n    mode = \"ENHANCED\"\n  }\n}\n```", "```py\ncatalog {\n  name = \"hive_metastore\"\n}\n```", "```py\nstorage {\n  path = \"/pipelines/my-dlt-pipeline-output/\"\n}\n```", "```py\n$ mkdir chapter_8_hands_on\n$ cd chapter_8_hands_on\n```", "```py\n$ touch main.tf\n```", "```py\nresource \"databricks_notebook\" \"dlt_pipeline_notebook\" {\n  path = \"${data.databricks_current_user.my_user.home}/chp_8_terraform/taxi_trips_pipeline.py\"\n...\n                    .load(path))\n    @dlt.table(\n        name=\"yellow_taxi_silver\",\n        comment=\"Financial information from incoming taxi trips.\"\n    )\n    @dlt.expect_or_fail(\"valid_total_amount\", \"total_amount > 0.0\")\n    def yellow_taxi_silver():\n        return (dlt.readStream(\"yellow_taxi_bronze\")\n                    .withColumn(\"driver_payment\",\n                                F.expr(\"total_amount * 0.40\"))\n                    .withColumn(\"vehicle_maintenance_fee\",\n                                F.expr(\"total_amount * 0.05\"))\n                    .withColumn(\"adminstrative_fee\",\n                                F.expr(\"total_amount * 0.1\"))\n                    .withColumn(\"potential_profits\",\n                                F.expr(\"total_amount * 0.45\")))\n    EOT\n  )\n}\n```", "```py\nresource \"databricks_catalog\" \"dlt_target_catalog\" {\n  name = \"chp8_deploying_pipelines_w_terraform\"\n  comment = \"The target catalog for Taxi Trips DLT pipeline\"\n}\nresource \"databricks_schema\" \"dlt_target_schema\" {\n  catalog_name = databricks_catalog.dlt_target_catalog.id\n  name = \"terraform_demo\"\n  comment = \"The target schema for Taxi Trips DLT pipeline\"\n}\n```", "```py\nresource \"databricks_pipeline\" \"taxi_trips_pipeline\" {\n  name = \"Taxi Trips Pipeline\"\n  library {\n    notebook {\n      path = \"${data.databricks_current_user.my_user.home}/chp_8_terraform/taxi_trips_pipeline.py\"\n    }\n  }\n  cluster {\n    label = \"default\"\n    num_workers = 2\n    autoscale {\n      min_workers = 2\n      max_workers = 4\n      mode = \"ENHANCED\"\n    }\n    driver_node_type_id = \"i3.2xlarge\"\n    node_type_id = \"i3.xlarge\"\n  }\n  continuous = false\n  development = true\n  photon = false\n  serverless = false\n  catalog = databricks_catalog.dlt_target_catalog.name\n  target = databricks_schema.dlt_target_schema.name\n  edition = \"ADVANCED\"\n  channel = \"CURRENT\"\n}\n```", "```py\nresource \"databricks_job\" \"taxi_trips_pipeline_job\" {\n  name = \"Taxi Trips Pipeline Update Job\"\n  description = \"Databricks Workflow that executes a pipeline update of the Taxi Trips DLT pipeline.\"\n  job_cluster {\n    job_cluster_key = \"taxi_trips_pipeline_update_job_cluster\"\n    new_cluster {\n      num_workers = 2\n      spark_version = \"15.4.x-scala2.12\"\n      node_type_id  = \"i3.xlarge\"\n      driver_node_type_id = \"i3.2xlarge\"\n    }\n  }\n  task {\n    task_key = \"update_taxi_trips_pipeline\"\n    pipeline_task {\n      pipeline_id = databricks_pipeline.taxi_trips_pipeline.id\n    }\n  }\n  trigger {\n    pause_status = \"PAUSED\"\n    periodic {\n      interval = \"1\"\n      unit = \"HOURS\"\n    }\n  }\n}\n```", "```py\noutput \"workflow_url\" {\n  value = databricks_job.taxi_trips_pipeline_job.url\n}\n```", "```py\n$ terraform init\n```", "```py\n$ terraform plan\n```", "```py\n$ terraform apply\n```", "```py\n$ terraform destroy\n```"]