<html><head></head><body>
  <div id="_idContainer111">
   <h1 class="chapter-number" id="_idParaDest-193">
    <a id="_idTextAnchor249">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     10
    </span>
   </h1>
   <h1 id="_idParaDest-194">
    <a id="_idTextAnchor250">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Monitoring Data Pipelines in Production
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.3.1">
     In the previous chapters, we learned how to build, configure, and deploy data pipelines using the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.3.2">
     To round off managing data pipelines for the lakehouse, in this final chapter of the book, we’ll dive into the crucial task of monitoring data pipelines in production.
    </span>
    <span class="koboSpan" id="kobo.3.3">
     We’ll learn how to leverage comprehensive monitoring techniques directly from the Databricks Data Intelligence Platform to track pipeline health, pipeline performance, and data quality, to name a few.
    </span>
    <span class="koboSpan" id="kobo.3.4">
     We will also implement a few real-world examples through hands-on exercises.
    </span>
    <span class="koboSpan" id="kobo.3.5">
     Lastly, we’ll look at the best practices for ensuring that your data pipelines run smoothly, enabling timely issue detection and resolution, and ensuring the delivery of reliable and accurate data for your analytics and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.4.1">
      business needs.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.5.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.6.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.7.1">
      Introduction to data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.8.1">
       pipeline monitoring
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.9.1">
      Pipeline health and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.10.1">
       performance monitoring
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.11.1">
      Data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.12.1">
       quality monitoring
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.13.1">
      Best practices for production
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.14.1">
       failure resolution
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.15.1">
      Hands-on exercise – setting up a webhook alert when a job runs longer
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.16.1">
       than expected
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-195">
    <a id="_idTextAnchor251">
    </a>
    <span class="koboSpan" id="kobo.17.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.18.1">
     To follow along with the examples provided in this chapter, you’ll need Databricks workspace permissions to create and start an all-purpose cluster so that you can import and execute the chapter’s accompanying notebooks.
    </span>
    <span class="koboSpan" id="kobo.18.2">
     It’s also recommended that your Databricks user be elevated to a workspace administrator so that you can create and edit alert destinations.
    </span>
    <span class="koboSpan" id="kobo.18.3">
     All code samples can be downloaded from this chapter’s GitHub repository at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10">
     <span class="koboSpan" id="kobo.19.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10
     </span>
    </a>
    <span class="koboSpan" id="kobo.20.1">
     .
    </span>
    <span class="koboSpan" id="kobo.20.2">
     This chapter will create and run several new notebooks, estimated to consume
    </span>
    <a id="_idIndexMarker571">
    </a>
    <span class="koboSpan" id="kobo.21.1">
     around 10-15
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.22.1">
      Databricks
     </span>
    </strong>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.23.1">
       Units
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.24.1">
      (
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.25.1">
       DBUs
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.26.1">
      ).
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-196">
    <a id="_idTextAnchor252">
    </a>
    <span class="koboSpan" id="kobo.27.1">
     Introduction to data pipeline monitoring
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.28.1">
     As data teams
    </span>
    <a id="_idIndexMarker572">
    </a>
    <span class="koboSpan" id="kobo.29.1">
     deploy data pipelines into production environments, being able to detect processing errors, delays, or data quality issues as soon as they happen can make a huge impact on catching and correcting issues before they have a chance to cascade to downstream systems and processes.
    </span>
    <span class="koboSpan" id="kobo.29.2">
     As such, the environment that data teams build and deploy their pipelines in should be able to monitor them and alert them when
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.30.1">
      problems arise.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-197">
    <a id="_idTextAnchor253">
    </a>
    <span class="koboSpan" id="kobo.31.1">
     Exploring ways to monitor data pipelines
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.32.1">
     There are several
    </span>
    <a id="_idIndexMarker573">
    </a>
    <span class="koboSpan" id="kobo.33.1">
     ways that data teams can monitor their data pipelines in production from within the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.33.2">
     For example, data teams can manually observe updates regarding their data pipeline by doing
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.34.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.35.1">
      Viewing pipeli
     </span>
     <a id="_idTextAnchor254">
     </a>
     <span class="koboSpan" id="kobo.36.1">
      ne status from
     </span>
     <a id="_idIndexMarker574">
     </a>
     <span class="koboSpan" id="kobo.37.1">
      the
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.38.1">
       Delta Live Tables
      </span>
     </strong>
     <span class="koboSpan" id="kobo.39.1">
      (
     </span>
     <span class="No-Break">
      <strong class="bold">
       <span class="koboSpan" id="kobo.40.1">
        DLT
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.41.1">
       ) UI
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.42.1">
      Querying pipeline information from the DLT
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.43.1">
       event log
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.44.1">
     While these manual means provide a way to quickly view the latest status of a data pipeline in an ad hoc manner, it’s certainly not a scalable solution, particularly as your data team adds more and more pipelines.
    </span>
    <span class="koboSpan" id="kobo.44.2">
     Instead, organizations turn to more automated mechanisms.
    </span>
    <span class="koboSpan" id="kobo.44.3">
     For instance, many organizations choose to leverage the built-in notification system in the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.44.4">
     Notifications are prevalent in many objects within the platform.
    </span>
    <span class="koboSpan" id="kobo.44.5">
     For example, data administrators can configure notifications in the following scenarios to alert data teams about a change in status pertaining to a particular
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.45.1">
      Databricks resource:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.46.1">
      DLT pipeline (either on update or
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.47.1">
       on flow)
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.48.1">
      Databricks workflow (at the top-most
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.49.1">
       job level)
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.50.1">
      Databricks
     </span>
     <a id="_idIndexMarker575">
     </a>
     <span class="koboSpan" id="kobo.51.1">
      workflow task (finer-grained notification than the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.52.1">
       preceding option)
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.53.1">
     While these notifications can be helpful to alert teams about events or status changes during data processing, data teams also need mechanisms for alerting each other about issues in the contents of the data landing into the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.54.1">
      enterprise
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.55.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.56.1">
      akehouse.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-198">
    <a id="_idTextAnchor255">
    </a>
    <span class="koboSpan" id="kobo.57.1">
     Using DBSQL Alerts to notify data validity
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.58.1">
     The Databricks Data Intelligence Platform can create alert notifications driven by a query within the
    </span>
    <a id="_idIndexMarker576">
    </a>
    <span class="koboSpan" id="kobo.59.1">
     DBSQL portion of the platform called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.60.1">
      DBSQL Alerts
     </span>
    </strong>
    <span class="koboSpan" id="kobo.61.1">
     .
    </span>
    <span class="koboSpan" id="kobo.61.2">
     DBSQL Alerts can be a useful tool to alert data teams about the data landing in their enterprise lakehouse.
    </span>
    <span class="koboSpan" id="kobo.61.3">
     DBSQL Alerts operate by specifying a particular query outcome condition that must be met for the data to be considered valid.
    </span>
    <span class="koboSpan" id="kobo.61.4">
     However, if a particular condition in an Alert is violated, such as an order amount crossing above a certain dollar amount threshold, for example, then the system will trigger a notification to send to an alert destination.
    </span>
    <span class="koboSpan" id="kobo.61.5">
     The following diagram depicts a DBSQL Alert that notifies recipients via email when there are sales orders exceeding a specific dollar amount – in this case, that’s $10,000.
    </span>
    <span class="koboSpan" id="kobo.61.6">
     In this example, the query is a max aggregation, the triggering condition is when the max aggregation exceeds $10,000, and the alert destination is an
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.62.1">
      email address.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer099">
     <span class="koboSpan" id="kobo.63.1">
      <img alt="Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email" src="image/B22011_10_001.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.64.1">
     Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.65.1">
     Furthermore, DBSQL Alerts can be scheduled to execute on a repeated schedule, for example once every hour.
    </span>
    <span class="koboSpan" id="kobo.65.2">
     This is an excellent way to automate
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.66.1">
      data validation
     </span>
    </em>
    <span class="koboSpan" id="kobo.67.1">
     checks on the contents of your datasets using the built-in mechanism from within the Databricks Data Intelligence Platform.
    </span>
    <span class="koboSpan" id="kobo.67.2">
     The following screenshot is an example of how alerts can be used to schedule a data validation query on a repeated schedule and notify data teams when a particular
    </span>
    <a id="_idIndexMarker577">
    </a>
    <span class="koboSpan" id="kobo.68.1">
     condition or set of conditions has
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.69.1">
      been violated.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer100">
     <span class="koboSpan" id="kobo.70.1">
      <img alt="Figure 10.2 – Configuration of an Alert triggering condition" src="image/B22011_10_002.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.71.1">
     Figure 10.2 – Configuration of an Alert triggering condition
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.72.1">
     Another mechanism for monitoring data pipelines in production is through workflow notifications.
    </span>
    <span class="koboSpan" id="kobo.72.2">
     Within the Databricks Data Intelligence Platform, notification messages can be delivered to enterprise messaging platforms, such as Slack or Microsoft Teams, or to incident management systems such as PagerDuty.
    </span>
    <span class="koboSpan" id="kobo.72.3">
     Later in the chapter, we’ll explore how to implement an HTTP webhook-based delivery destination, which is popular in web services
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.73.1">
      architecture environments.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.74.1">
     There are two types of notifications that can be sent from within a particular workflow – job status and task status.
    </span>
    <span class="koboSpan" id="kobo.74.2">
     Job status notifications are high-level statuses about the overall success or failure of a particular workflow.
    </span>
    <span class="koboSpan" id="kobo.74.3">
     However, you can also configure notifications to be sent to monitoring destinations at the task level, such as if you’d like to monitor when
    </span>
    <a id="_idIndexMarker578">
    </a>
    <span class="koboSpan" id="kobo.75.1">
     tasks within a workflow
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.76.1">
      are retried.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer101">
     <span class="koboSpan" id="kobo.77.1">
      <img alt="Figure 10.3 – Configuring job- and task-level notifications" src="image/B22011_10_003.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.78.1">
     Figure 10.3 – Configuring job- and task-level notifications
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.79.1">
     While alert notifications are a great way to automate the notification of team members when problems arise, data teams also need to monitor the health of data pipelines in a periodic and ad hoc manner.
    </span>
    <span class="koboSpan" id="kobo.79.2">
     We will discuss this in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.80.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-199">
    <a id="_idTextAnchor256">
    </a>
    <span class="koboSpan" id="kobo.81.1">
     Pipeline health and performance monitoring
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.82.1">
     The Databricks Data
    </span>
    <a id="_idIndexMarker579">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     Intelligence Platform provides a location for data teams to
    </span>
    <a id="_idIndexMarker580">
    </a>
    <span class="koboSpan" id="kobo.84.1">
     query the status of data pipelines called the event log.
    </span>
    <span class="koboSpan" id="kobo.84.2">
     The
    </span>
    <a id="_idIndexMarker581">
    </a>
    <span class="koboSpan" id="kobo.85.1">
     event log contains a history of all events that pertain to a particular DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.85.2">
     In particular, the event log will contain an event feed with a list of event objects with recorded metadata about
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.86.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.87.1">
      What type of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.88.1">
       event occurred
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.89.1">
      A unique identifier of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.90.1">
       the event
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.91.1">
      Timestamps of when the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.92.1">
       event occurred
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.93.1">
      A high-level description of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.94.1">
       the event
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.95.1">
      Fine-grained details about
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.96.1">
       the event
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.97.1">
      An event-level indication (
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.98.1">
       INFO
      </span>
     </strong>
     <span class="koboSpan" id="kobo.99.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.100.1">
       WARN
      </span>
     </strong>
     <span class="koboSpan" id="kobo.101.1">
      ,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.102.1">
       ERROR
      </span>
     </strong>
     <span class="koboSpan" id="kobo.103.1">
      ,
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.104.1">
       or
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.105.1">
        METRICS
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.106.1">
       )
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.107.1">
      The origin source of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.108.1">
       the event
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.109.1">
     Unlike scalar functions, which return a
    </span>
    <a id="_idIndexMarker582">
    </a>
    <span class="koboSpan" id="kobo.110.1">
     single value,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.111.1">
      Table Valued Functions
     </span>
    </strong>
    <span class="koboSpan" id="kobo.112.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.113.1">
      TVFs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.114.1">
     ) are functions that return a table as the result.
    </span>
    <span class="koboSpan" id="kobo.114.2">
     For DLT pipelines that publish to a catalog and schema within Unity Catalog, the Databricks Data Intelligence Platform offers a special TVF called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.115.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.116.1">
     to query comprehensive information regarding a given DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.116.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.117.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.118.1">
     function can take one of two arguments as input: a fully qualified table name of a pipeline
    </span>
    <a id="_idIndexMarker583">
    </a>
    <span class="koboSpan" id="kobo.119.1">
     dataset or a
    </span>
    <a id="_idIndexMarker584">
    </a>
    <span class="koboSpan" id="kobo.120.1">
     pipeline ID as
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.121.1">
      an argument.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer102">
     <span class="koboSpan" id="kobo.122.1">
      <img alt="Figure 10.4 – The event_log() TVF returns a list of events that occurred" src="image/B22011_10_004.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.123.1">
     Figure 10.4 – The event_log() TVF returns a list of events that occurred
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.124.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.125.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.126.1">
     function will retrieve information about a given DLT pipeline, including
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.127.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.128.1">
      Outcomes of data quality
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.129.1">
       checks (expectations)
      </span>
     </span>
    </li>
    <li>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.130.1">
       Auditing information
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.131.1">
      Pipeline
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.132.1">
       update status
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.133.1">
      Data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.134.1">
       lineage information
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.135.1">
     A common
    </span>
    <a id="_idIndexMarker585">
    </a>
    <span class="koboSpan" id="kobo.136.1">
     approach to make it easier for data stewards to query events for
    </span>
    <a id="_idIndexMarker586">
    </a>
    <span class="koboSpan" id="kobo.137.1">
     a particular DLT pipeline is to register a view alongside the datasets for a particular pipeline.
    </span>
    <span class="koboSpan" id="kobo.137.2">
     This allows users to conveniently reference the event log results in subsequent queries.
    </span>
    <span class="koboSpan" id="kobo.137.3">
     The following SQL
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.138.1">
      Data Definition Language
     </span>
    </strong>
    <span class="koboSpan" id="kobo.139.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.140.1">
      DDL
     </span>
    </strong>
    <span class="koboSpan" id="kobo.141.1">
     ) statement
    </span>
    <a id="_idIndexMarker587">
    </a>
    <span class="koboSpan" id="kobo.142.1">
     will create a view that retrieves the event log for a DLT pipeline with the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.143.1">
       my_dlt_pipeline_id
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.144.1">
      ID:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.145.1">
CREATE VIEW my_pipeline_event_log_vw AS
SELECT
  *
FROM
  event_log('&lt;my_dlt_pipeline_id&gt;');</span></pre>
   <p>
    <span class="koboSpan" id="kobo.146.1">
     Sometimes, the event log for a particular pipeline can grow too large, making it difficult for data stewards to quickly summarize the latest status updates.
    </span>
    <span class="koboSpan" id="kobo.146.2">
     Instead, data teams can narrow the event log feed even further to a particular dataset within a DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.146.3">
     For example, data teams can create a view on top o
    </span>
    <a id="_idTextAnchor257">
    </a>
    <span class="koboSpan" id="kobo.147.1">
     f a specific dataset to capture all the events using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.148.1">
      table()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.149.1">
     function and provide a fully-qualified table name as an argument to the function.
    </span>
    <span class="koboSpan" id="kobo.149.2">
     The following SQL DDL statement will create a view that retrieves the event log for a dataset
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.150.1">
      called
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.151.1">
       my_gold_table
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.152.1">
      :
     </span>
    </span>
    <a id="_idTextAnchor258">
    </a>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.153.1">
CREATE VIEW my_gold_table_event_log_vw AS
SELECT
  *
FROM
  event_log(table(my_catalog.my_schema.my_gold_table));</span></pre>
   <p>
    <span class="koboSpan" id="kobo.154.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.155.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.156.1">
     TVF function
    </span>
    <a id="_idIndexMarker588">
    </a>
    <span class="koboSpan" id="kobo.157.1">
     provides data teams with great visibility into the actions performed on a particular DLT pipeline and dataset
    </span>
    <a id="_idIndexMarker589">
    </a>
    <span class="koboSpan" id="kobo.158.1">
     making it easy to implement end-to-end observability
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.159.1">
      and auditability.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.160.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.161.1">
     Presently, if a DLT pipeline is configured to publish output datasets to Unity Catalog, then only the owner of a particular DLT pipeline can query the views.
    </span>
    <span class="koboSpan" id="kobo.161.2">
     To share access to the event logs, the pipeline owner must save a copy of the event log feed to another table within Unity Catalog and grant access to other users
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.162.1">
      or groups.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.163.1">
     Let’s look at how we might leverage the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.164.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.165.1">
     function to query the data quality events for a particular
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.166.1">
      DLT pipeline.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-200">
    <a id="_idTextAnchor259">
    </a>
    <span class="koboSpan" id="kobo.167.1">
     Hands-on exercise – querying data quality events for a dataset
    </span>
   </h1>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.168.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.169.1">
     For the following exercise, you will need to use a shared, all-purpose cluster or a Databricks SQL warehouse to query the event log.
    </span>
    <span class="koboSpan" id="kobo.169.2">
     Furthermore, the event log is only available to query DLT pipelines that have been configured to store datasets in Unity Catalog.
    </span>
    <span class="koboSpan" id="kobo.169.3">
     The event log will not be found for DLT pipelines that have been configured to store datasets in the legacy
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.170.1">
      Hive Metastore.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.171.1">
     Data quality metrics are
    </span>
    <a id="_idIndexMarker590">
    </a>
    <span class="koboSpan" id="kobo.172.1">
     stored in the event log as a serialized JSON string.
    </span>
    <span class="koboSpan" id="kobo.172.2">
     We’ll need to parse the JSON string into a different data structure so that we can easily query data quality events from the event log.
    </span>
    <span class="koboSpan" id="kobo.172.3">
     Let’s use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.173.1">
      from_json()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.174.1">
     SQL function to parse the serialized JSON string for our data quality expectations.
    </span>
    <span class="koboSpan" id="kobo.174.2">
     We’ll need to specify a schema as an argument to instruct Spark how to parse the JSON string into a deserialized data structure – specifically, an array of structs that contain information about the expectation name, dataset name, number of passing records, and number of failing records.
    </span>
    <span class="koboSpan" id="kobo.174.3">
     Lastly, we’ll use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.175.1">
      explode()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.176.1">
     SQL function to transform the array of expectation structs into a new row for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.177.1">
      each expectation.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.178.1">
     We can leverage the previously defined views to monitor the ongoing data quality of the datasets within our
    </span>
    <a id="_idIndexMarker591">
    </a>
    <span class="koboSpan" id="kobo.179.1">
     DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.179.2">
     Let’s create another view pertaining to the data quality of our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.180.1">
      DLT pipeline:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.181.1">
CREATE OR REPLACE TEMPORARY VIEW taxi_trip_pipeline_data_quality_vw AS
SELECT
  timestamp,
  event_type,
  message,
  data_quality.dataset,
  data_quality.name AS expectation_name,
  data_quality.passed_records AS num_passed_records,
  data_quality.failed_records AS num_failed_records
FROM
  (
    SELECT
      event_type,
      message,
      timestamp,
      explode(
        from_json(
          details :flow_progress.data_quality.expectations,
          "ARRAY&lt;
            STRUCT&lt;
              name: STRING,
              dataset: STRING,
              passed_records: INT,
              failed_records: INT
            &gt;
          &gt;"
        )
      ) AS data_quality
    FROM
      my_table_event_log_vw
  );</span></pre>
   <p>
    <span class="koboSpan" id="kobo.182.1">
     Several common
    </span>
    <a id="_idIndexMarker592">
    </a>
    <span class="koboSpan" id="kobo.183.1">
     questions that are posed by data teams include: “How many records were processed?”, “How many records failed data quality validation?”, or “What was the percentage of passing records versus failing records?”. Let’s take the previous example a step further and summarize the high-level data quality metrics per dataset in our pipeline.
    </span>
    <span class="koboSpan" id="kobo.183.2">
     Let’s count the total number of rows having an expectation applied, as well as the percentage of passing records versus failing records for each of the datasets in our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.184.1">
      DLT pipeline:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.185.1">
SELECT
  timestamp,
  dataset,
  sum(num_passed_records + num_failed_records)
    AS total_expectations_evaluated,
  avg(
    num_passed_records /
    (num_passed_records + num_failed_records)
  ) * 100 AS avg_pass_rate,
  avg(
    num_failed_records /
    (num_passed_records + num_failed_records)
  ) * 100 AS avg_fail_rate
FROM
  taxi_trip_pipeline_data_quality_vw
GROUP BY
  timestamp,
  dataset;</span></pre>
   <p>
    <span class="koboSpan" id="kobo.186.1">
     We get the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.187.1">
      following output:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer103">
     <span class="koboSpan" id="kobo.188.1">
      <img alt="Figure 10.5 – Events captured in the DLT event log" src="image/B22011_10_005.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.189.1">
     Figure 10.5 – Events captured in the DLT event log
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.190.1">
     As you can
    </span>
    <a id="_idIndexMarker593">
    </a>
    <span class="koboSpan" id="kobo.191.1">
     see, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.192.1">
      event_log()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.193.1">
     function makes it simple for data teams to query comprehensive information regarding a given DLT pipeline.
    </span>
    <span class="koboSpan" id="kobo.193.2">
     Not only can data teams query the status of a pipeline update but they can also query the status of the quality data landing into their lakehouse.
    </span>
    <span class="koboSpan" id="kobo.193.3">
     Still, data teams need a way to automate the notification of failing data quality checks at runtime, as is the scenario when the data accuracy of downstream reports is critical to the business.
    </span>
    <span class="koboSpan" id="kobo.193.4">
     Let’s look closer at this in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.194.1">
      following section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-201">
    <a id="_idTextAnchor260">
    </a>
    <span class="koboSpan" id="kobo.195.1">
     Data quality monitoring
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.196.1">
     Ongoing monitoring of the data quality of
    </span>
    <a id="_idIndexMarker594">
    </a>
    <span class="koboSpan" id="kobo.197.1">
     datasets within your lakehouse is critical for the success of business-critical data applications deployed to production.
    </span>
    <span class="koboSpan" id="kobo.197.2">
     Take, for example, the impact that a sudden ingestion of null values on a joined column might have on downstream reports that rely on joining together upstream datasets.
    </span>
    <span class="koboSpan" id="kobo.197.3">
     Suddenly,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.198.1">
      business intelligence
     </span>
    </strong>
    <span class="koboSpan" id="kobo.199.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.200.1">
      BI
     </span>
    </strong>
    <span class="koboSpan" id="kobo.201.1">
     ) reports
    </span>
    <a id="_idIndexMarker595">
    </a>
    <span class="koboSpan" id="kobo.202.1">
     might refresh, but the data may appear stale or outdated.
    </span>
    <span class="koboSpan" id="kobo.202.2">
     By automatically detecting data quality issues as soon as they arise, your data team can be alerted of potential issues and take immediate action to intervene and correct possible data corruption or even
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.203.1">
      data loss.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer104">
     <span class="koboSpan" id="kobo.204.1">
      <img alt="Figure 10.6 – Detecting issues early is important to ensure the quality of downstream processes" src="image/B22011_10_006.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.205.1">
     Figure 10.6 – Detecting issues early is important to ensure the quality of downstream processes
    </span>
   </p>
   <h2 id="_idParaDest-202">
    <a id="_idTextAnchor261">
    </a>
    <span class="koboSpan" id="kobo.206.1">
     Introducing Lakehouse Monitoring
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.207.1">
     Lakehouse Monitoring, a recent
    </span>
    <a id="_idIndexMarker596">
    </a>
    <span class="koboSpan" id="kobo.208.1">
     feature of the Databricks Data Intelligence Platform, gives data teams the ability to track and monitor the data quality of data and other assets in the lakehouse.
    </span>
    <span class="koboSpan" id="kobo.208.2">
     Data teams can automatically measure the statistical distribution of data across columns, number of null values, minimum, maximum, median column values, and other statistical properties.
    </span>
    <span class="koboSpan" id="kobo.208.3">
     With Lakehouse Monitoring, data teams can automatically detect major problems in datasets such as data skews or missing values, and alert team members of issues so that they can take
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.209.1">
      appropriate action.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.210.1">
     Lakehouse Monitoring is most useful when used to monitor the data quality of Delta tables, views, materialized views, and streaming tables.
    </span>
    <span class="koboSpan" id="kobo.210.2">
     It can even be used in
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.211.1">
      Machine Learning
     </span>
    </strong>
    <span class="koboSpan" id="kobo.212.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.213.1">
      ML
     </span>
    </strong>
    <span class="koboSpan" id="kobo.214.1">
     ) pipelines, measuring
    </span>
    <a id="_idIndexMarker597">
    </a>
    <span class="koboSpan" id="kobo.215.1">
     the statistical summaries of datasets and triggering alert notifications as soon as data drift is detected.
    </span>
    <span class="koboSpan" id="kobo.215.2">
     Furthermore, Lakehouse Monitoring can be customized to be fine- or coarse-grained in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.216.1">
      monitoring metrics.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.217.1">
     Lakehouse Monitoring
    </span>
    <a id="_idIndexMarker598">
    </a>
    <span class="koboSpan" id="kobo.218.1">
     begins with the creation of a monitor object, which is then attached to a data asset such as a Delta table in your lakehouse.
    </span>
    <span class="koboSpan" id="kobo.218.2">
     Behind the scenes, the monitor object will create two additional tables inside your lakehouse to capture statistical measures of the corresponding Delta table or other
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.219.1">
      data assets.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.220.1">
     The monitoring tables are then used to power a Dashboard, which can be used by data teams and other stakeholders to get a view into the real-time data insights of the quality of your data in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.221.1">
      the
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.222.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.223.1">
      akehouse.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer105">
     <span class="koboSpan" id="kobo.224.1">
      <img alt="Figure 10.7 – A lakehouse monitor will create two metrics tables for the monitored data asset" src="image/B22011_10_007.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.225.1">
     Figure 10.7 – A lakehouse monitor will create two metrics tables for the monitored data asset
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.226.1">
     A lakehouse monitor can be configured to measure different aspects of a data asset, which is also referred to as a profile type.
    </span>
    <span class="koboSpan" id="kobo.226.2">
     There are three
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.227.1">
      monitor profile types
     </span>
    </em>
    <span class="koboSpan" id="kobo.228.1">
     that can
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.229.1">
      be created:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.230.1">
       Snapshot
      </span>
     </strong>
     <span class="koboSpan" id="kobo.231.1">
      : This is a generic, yet robust monitor.
     </span>
     <span class="koboSpan" id="kobo.231.2">
      It’s useful to monitor data quality and other
     </span>
     <a id="_idIndexMarker599">
     </a>
     <span class="koboSpan" id="kobo.232.1">
      metrics of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.233.1">
       a table.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.234.1">
       Time series
      </span>
     </strong>
     <span class="koboSpan" id="kobo.235.1">
      : It’s
     </span>
     <a id="_idIndexMarker600">
     </a>
     <span class="koboSpan" id="kobo.236.1">
      useful for time series datasets.
     </span>
     <span class="koboSpan" id="kobo.236.2">
      It’s used to monitor the data quality over time
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.237.1">
       period windows.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.238.1">
       Inference
      </span>
     </strong>
     <span class="koboSpan" id="kobo.239.1">
      : It’s useful to
     </span>
     <a id="_idIndexMarker601">
     </a>
     <span class="koboSpan" id="kobo.240.1">
      compare the quality of an ML model inference versus the input over a window of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.241.1">
       time periods.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.242.1">
     In this chapter, we’ll only be covering the time series and snapshot types.
    </span>
    <span class="koboSpan" id="kobo.242.2">
     Discussing inference is out of the scope of this book, but you are encouraged to explore how Lakehouse Monitoring can be helpful for ML use
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.243.1">
      cases (
     </span>
    </span>
    <a href="https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.244.1">
       https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.245.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.246.1">
     Monitors can also be
    </span>
    <a id="_idIndexMarker602">
    </a>
    <span class="koboSpan" id="kobo.247.1">
     created that compare the statistical metrics of a table versus a baseline table.
    </span>
    <span class="koboSpan" id="kobo.247.2">
     This can be useful in scenarios such as comparing the relative humidity of smart thermostat devices for this week as compared to last week, or comparing the number of recorded sales in a particular dataset for a monthly sales report versus last month’s report,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.248.1">
      for example.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.249.1">
     Let’s look at a practical example of using a lakehouse monitor in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.250.1">
      a
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.251.1">
      l
     </span>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.252.1">
      akehouse.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-203">
    <a id="_idTextAnchor262">
    </a>
    <span class="koboSpan" id="kobo.253.1">
     Hands-on exercise – creating a lakehouse monitor
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.254.1">
     In this hands-on exercise, we’re going to create a lakehouse monitor for measuring the data quality of a target
    </span>
    <a id="_idIndexMarker603">
    </a>
    <span class="koboSpan" id="kobo.255.1">
     Delta table.
    </span>
    <span class="koboSpan" id="kobo.255.2">
     Although our Delta table does contain timestamp information, we’ll choose a
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.256.1">
      snapshot profile
     </span>
    </em>
    <span class="koboSpan" id="kobo.257.1">
     to monitor the data quality of a target Delta table in our lakehouse.
    </span>
    <span class="koboSpan" id="kobo.257.2">
     Recall that the snapshot profile is a generic lakehouse monitor that also proves to be quite versatile, as mentioned earlier.
    </span>
    <span class="koboSpan" id="kobo.257.3">
     The snapshot profiler will allow us to measure standard summary metrics about our dataset or insert custom business calculations around the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.258.1">
      data quality.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.259.1">
     Like many resources in the Databricks Data Intelligence Platform, there are a variety of ways that you can create a new lakehouse monitor.
    </span>
    <span class="koboSpan" id="kobo.259.2">
     For example, you can use the Databricks UI, the Databricks REST API, the Databricks CLI (covered in
    </span>
    <a href="B22011_09.xhtml#_idTextAnchor222">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.260.1">
        Chapter 9
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.261.1">
     ), or automation tools such as Terraform, to name a few.
    </span>
    <span class="koboSpan" id="kobo.261.2">
     Perhaps the simplest mechanism for creating a new monitor is through the UI.
    </span>
    <span class="koboSpan" id="kobo.261.3">
     In this hands-on exercise, we’re going to use the Databricks UI to create the lakehouse monitor.
    </span>
    <span class="koboSpan" id="kobo.261.4">
     This is a great way to get started experimenting with Lakehouse Monitoring and with different data quality metrics to measure your datasets.
    </span>
    <span class="koboSpan" id="kobo.261.5">
     However, it’s recommended in production scenarios to migrate your lakehouse monitors to an automated build tool such
    </span>
    <a id="_idIndexMarker604">
    </a>
    <span class="koboSpan" id="kobo.262.1">
     as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.263.1">
      Databricks Asset Bundles
     </span>
    </strong>
    <span class="koboSpan" id="kobo.264.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.265.1">
      DABs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.266.1">
     ) (covered in
    </span>
    <a href="B22011_09.xhtml#_idTextAnchor222">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.267.1">
        Chapter 9
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.268.1">
     ) or Terraform (covered in
    </span>
    <a href="B22011_08.xhtml#_idTextAnchor185">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.269.1">
        Chapter 8
       </span>
      </em>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.270.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.271.1">
     If you haven’t done so already, you can clone the accompanying code resources for this chapter
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.272.1">
      at
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.273.1">
       https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.274.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.275.1">
     The first step is to generate a target Delta table, which we would like to monitor the data quality.
    </span>
    <span class="koboSpan" id="kobo.275.2">
     Clone or import the data generator notebook or create a new notebook with the following code generator
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.276.1">
      source code.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.277.1">
     In the first cell of the
    </span>
    <a id="_idIndexMarker605">
    </a>
    <span class="koboSpan" id="kobo.278.1">
     notebook, we’ll leverage the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.279.1">
      %pip
     </span>
    </strong>
    <span class="koboSpan" id="kobo.280.1">
     magic command to download and install the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.281.1">
      dbldatagen
     </span>
    </strong>
    <span class="koboSpan" id="kobo.282.1">
     Python library, which is used to generate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.283.1">
      sample data:
     </span>
    </span>
   </p>
   <pre class="console"><span class="koboSpan" id="kobo.284.1">
%pip install dbldatagen==0.4.0</span></pre>
   <p>
    <span class="koboSpan" id="kobo.285.1">
     Next, we’ll define a helper function for generating a synthetic dataset containing smart thermostat readings
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.286.1">
      over time:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.287.1">
import dbldatagen as dg
from pyspark.sql.types import IntegerType, FloatType, TimestampType
def generate_smart_thermostat_readings():
    """Generates synthetics thermostat readings"""
    ds = (
        dg.DataGenerator(
            spark,
            name="smart_thermostat_dataset",
            rows=10000,
            partitions=4)
        .withColumn("device_id", IntegerType(),
                    minValue=1000000, maxValue=2000000)
        .withColumn("temperature", FloatType(),
                    minValue=10.0, maxValue=1000.0)
        .withColumn("humidity", FloatType(),
                    minValue=0.1, maxValue=1000.0)
        .withColumn("battery_level", FloatType(),
                    minValue=-50.0, maxValue=150.0)
        .withColumn("reading_ts", TimestampType(), random=False)
    )
    return ds.build()
# Generate the data using dbldatagen
df = generate_smart_thermostat_readings()
df.display()</span></pre>
   <p>
    <span class="koboSpan" id="kobo.288.1">
     Finally, we’ll save the
    </span>
    <a id="_idIndexMarker606">
    </a>
    <span class="koboSpan" id="kobo.289.1">
     newly created dataset as a Delta table in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.290.1">
      Unity Catalog:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.291.1">
(df.write
    .format("delta")
    .mode("overwrite")
    .saveAsTable(FULLY_QUALIFIED_TABLE_NAME))</span></pre>
   <p>
    <span class="koboSpan" id="kobo.292.1">
     Now that our Delta table has been created in our lakehouse, let’s use the UI in the Catalog Explo
    </span>
    <a id="_idTextAnchor263">
    </a>
    <span class="koboSpan" id="kobo.293.1">
     rer to create a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.294.1">
      new monitor.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.295.1">
     From the left-side navigation bar, click on the Catalog Explorer icon.
    </span>
    <span class="koboSpan" id="kobo.295.2">
     Next, navigate to the catalog created for this chapter by expanding the list of catalogs or using the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.296.1">
      Search
     </span>
    </strong>
    <span class="koboSpan" id="kobo.297.1">
     field to filter the results.
    </span>
    <span class="koboSpan" id="kobo.297.2">
     Click on the schema that was created for this chapter.
    </span>
    <span class="koboSpan" id="kobo.297.3">
     Finally, click on the Delta
    </span>
    <a id="_idIndexMarker607">
    </a>
    <span class="koboSpan" id="kobo.298.1">
     table that was created earlier by our data generator notebook.
    </span>
    <span class="koboSpan" id="kobo.298.2">
     Click on the data quality tab that is appropriately
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.299.1">
      titled
     </span>
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.300.1">
       Quality
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.301.1">
      .
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer106">
     <span class="koboSpan" id="kobo.302.1">
      <img alt="Figure 10.8 – A new monitor can be created directly from the Databricks UI" src="image/B22011_10_008.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.303.1">
     Figure 10.8 – A new monitor can be created directly from the Databricks UI
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.304.1">
     Next, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.305.1">
      Get started
     </span>
    </strong>
    <span class="koboSpan" id="kobo.306.1">
     button to begin creating a new monitor.
    </span>
    <span class="koboSpan" id="kobo.306.2">
     A pop-up dialog will open, prompting you to select the profile type for the monitor, as well as advanced configuration options such as the schedule, notification delivery, and workspace directory for storing the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.307.1">
      generated dashboard.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.308.1">
     Click the dropdown for the profile type and select the option for generating a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.309.1">
      snapshot profile.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.310.1">
     Next, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.311.1">
      Advanced Options
     </span>
    </strong>
    <span class="koboSpan" id="kobo.312.1">
     section to expand the dialog form.
    </span>
    <span class="koboSpan" id="kobo.312.2">
     The UI will allow users to capture dataset metrics, either manually or by defining a cron schedule for executing the metrics calculations on a repeated schedule.
    </span>
    <span class="koboSpan" id="kobo.312.3">
     You’ll notice that the dialog provides the flexibility to define the schedule using a traditional cron syntax, or by selecting the date and time drop-down menus in the dialog form.
    </span>
    <span class="koboSpan" id="kobo.312.4">
     For this hands-on exercise, we’ll choose the former option and refresh the monitoring metrics manually through the click of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.313.1">
      a button.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.314.1">
     Optionally, you can choose to have notifications about the success or failure of monitoring metrics calculations sent via email to a list of email recipients.
    </span>
    <span class="koboSpan" id="kobo.314.2">
     You can add up to five email addresses for notifications to be delivered to.
    </span>
    <span class="koboSpan" id="kobo.314.3">
     Ensure that your user email address is listed in the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.315.1">
      Notifications
     </span>
    </strong>
    <span class="koboSpan" id="kobo.316.1">
     section and that the checkbox is checked to receive a notification for failures during the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.317.1">
      metrics collection.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.318.1">
     If you recall from earlier, a lakehouse monitor will create two metrics tables.
    </span>
    <span class="koboSpan" id="kobo.318.2">
     We’ll need to provide a location in Unity Catalog to store these metrics tables.
    </span>
    <span class="koboSpan" id="kobo.318.3">
     Under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.319.1">
      Metrics
     </span>
    </strong>
    <span class="koboSpan" id="kobo.320.1">
     section, add the catalog and schema name created for this chapter’s hands-on exercise.
    </span>
    <span class="koboSpan" id="kobo.320.2">
     For example,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.321.1">
      enter
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.322.1">
       chp10.monitor_demo
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.323.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.324.1">
     The last item that we need to specify is a workspace location for storing the generated dashboard for our lakehouse monitor.
    </span>
    <span class="koboSpan" id="kobo.324.2">
     By default, the generated assets will be stored under the user’s home directory, for example,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.325.1">
      /Users/&lt;user_email_address&gt;/databricks_lakehouse_monitoring
     </span>
    </strong>
    <span class="koboSpan" id="kobo.326.1">
     .
    </span>
    <span class="koboSpan" id="kobo.326.2">
     For this hands-on exercise, we’ll accept the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.327.1">
      default location.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.328.1">
     We’re ready to create our monitor!
    </span>
    <span class="koboSpan" id="kobo.328.2">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.329.1">
      Create
     </span>
    </strong>
    <span class="koboSpan" id="kobo.330.1">
     button to create the lakehouse monitor for our
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.331.1">
      Delta table.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.332.1">
     Since we haven’t configured a schedule for our lakehouse monitor, we’ll need to manually execute a metrics collection.
    </span>
    <span class="koboSpan" id="kobo.332.2">
     Back in the Catalog Explorer, under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.333.1">
      Quality
     </span>
    </strong>
    <span class="koboSpan" id="kobo.334.1">
     tab of our Delta table, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.335.1">
      Refresh metrics
     </span>
    </strong>
    <span class="koboSpan" id="kobo.336.1">
     button to manually trigger a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.337.1">
      metrics
     </span>
    </span>
    <span class="No-Break">
     <a id="_idIndexMarker608">
     </a>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.338.1">
      collection.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer107">
     <span class="koboSpan" id="kobo.339.1">
      <img alt="Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog Explorer UI" src="image/B22011_10_009.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.340.1">
     Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog Explorer UI
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.341.1">
     An update of the table metrics will be triggered to execute and will take up to a few minutes to complete.
    </span>
    <span class="koboSpan" id="kobo.341.2">
     Once the update has completed, click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.342.1">
      View dashboard
     </span>
    </strong>
    <span class="koboSpan" id="kobo.343.1">
     button to view the metrics captured.
    </span>
    <span class="koboSpan" id="kobo.343.2">
     Congratulations!
    </span>
    <span class="koboSpan" id="kobo.343.3">
     You’ve created your first lakehouse monitor and you’re well on your way to implementing robust and automated data quality observability for your
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.344.1">
      data team.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.345.1">
     Now that we have an idea of how to alert our team members when production issues arise, let’s turn our attention to a few approaches to resolve failures in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.346.1">
      production deployments.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-204">
    <a id="_idTextAnchor264">
    </a>
    <span class="koboSpan" id="kobo.347.1">
     Best practices for production failure resolution
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.348.1">
     The DLT framework
    </span>
    <a id="_idIndexMarker609">
    </a>
    <span class="koboSpan" id="kobo.349.1">
     was designed with failure resolution in mind.
    </span>
    <span class="koboSpan" id="kobo.349.2">
     For example, DLT will automatically respond to three types of common
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.350.1">
      pipeline failures:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.351.1">
      Databricks Runtime regressions (covered in
     </span>
     <a href="B22011_02.xhtml#_idTextAnchor052">
      <span class="No-Break">
       <em class="italic">
        <span class="koboSpan" id="kobo.352.1">
         Chapter 2
        </span>
       </em>
      </span>
     </a>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.353.1">
       )
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.354.1">
      Update
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.355.1">
       processing failures
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.356.1">
      Data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.357.1">
       transaction failure
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.358.1">
     Let’s look at update failures and data transaction failures in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.359.1">
      greater detail.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-205">
    <a id="_idTextAnchor265">
    </a>
    <span class="koboSpan" id="kobo.360.1">
     Handling pipeline update failures
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.361.1">
     The DLT framework was
    </span>
    <a id="_idIndexMarker610">
    </a>
    <span class="koboSpan" id="kobo.362.1">
     designed with robust error handling in mind.
    </span>
    <span class="koboSpan" id="kobo.362.2">
     During a pipeline update, the framework will attempt to apply the most recent updates to tables defined in the dataflow graph.
    </span>
    <span class="koboSpan" id="kobo.362.3">
     If a processing error occurs, the framework will classify the error as either a retriable error or a non-retriable error.
    </span>
    <span class="koboSpan" id="kobo.362.4">
     A retriable error means that the framework has classified the runtime error as likely an issue caused by the current set of conditions.
    </span>
    <span class="koboSpan" id="kobo.362.5">
     For example, a system error would not be considered a retriable error, since it relates to the runtime environment that execution retries will not solve.
    </span>
    <span class="koboSpan" id="kobo.362.6">
     However, a network timeout would be a retriable error, since it could be impacted by the temporary set of network environment conditions.
    </span>
    <span class="koboSpan" id="kobo.362.7">
     By default, the DLT framework retries a pipeline update twice if it detects a
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.363.1">
       retriable
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.364.1">
      error.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-206">
    <a id="_idTextAnchor266">
    </a>
    <span class="koboSpan" id="kobo.365.1">
     Recovering from table transaction failure
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.366.1">
     Due to the nature of
    </span>
    <a id="_idIndexMarker611">
    </a>
    <span class="koboSpan" id="kobo.367.1">
     the Delta Lake transaction log, changes to a dataset are atomic, meaning that they can only happen if a table transaction (such as a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.368.1">
      Data Manipulation Language
     </span>
    </strong>
    <span class="koboSpan" id="kobo.369.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.370.1">
      DML
     </span>
    </strong>
    <span class="koboSpan" id="kobo.371.1">
     ) statement) is
    </span>
    <a id="_idIndexMarker612">
    </a>
    <span class="koboSpan" id="kobo.372.1">
     committed to the transaction log.
    </span>
    <span class="koboSpan" id="kobo.372.2">
     As a result, if a transaction fails in the middle of its execution, then the entire transaction is abandoned, thereby preventing the dataset from entering a non-deterministic state requiring data teams to intervene and manually reverse the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.373.1">
      data changes.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.374.1">
     Now that we understand how to handle pipeline failures in production, let’s cement the topics from this chapter through a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.375.1">
      real-world example.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-207">
    <a id="_idTextAnchor267">
    </a>
    <span class="koboSpan" id="kobo.376.1">
     Hands-on exercise – setting up a webhook alert when a job runs longer than expected
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.377.1">
     In this hands-on exercise, we’ll be creating a custom HTTP webhook that will notify an HTTP endpoint about the timeout status of a scheduled job
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.378.1">
      in Databricks.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.379.1">
     A webhook alert
    </span>
    <a id="_idIndexMarker613">
    </a>
    <span class="koboSpan" id="kobo.380.1">
     is a notification mechanism in the Databricks Data Intelligence Platform that enables data teams to monitor their data pipeline by automatically publishing the outcome of a particular job execution run.
    </span>
    <span class="koboSpan" id="kobo.380.2">
     For example, you can receive notifications about the successful run, execution state, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.381.1">
      run failures.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.382.1">
     Why are we using a workflow rather than a DLT pipeline directly?
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.383.1">
     In practice, a DLT pipeline will often be just one of many dependencies in a complete data product.
    </span>
    <span class="koboSpan" id="kobo.383.2">
     Databricks workflows are a popular orchestration tool that can prepare dependencies, run one or more DLT pipelines, and execute downstream tasks as well.
    </span>
    <span class="koboSpan" id="kobo.383.3">
     In this exercise, we’ll be configuring notifications from a Databricks workflow, as opposed to notifications directly from a DLT pipeline, to simulate a typical
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.384.1">
      production scenario.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.385.1">
     Let’s start by
    </span>
    <a id="_idIndexMarker614">
    </a>
    <span class="koboSpan" id="kobo.386.1">
     navigating to your Databricks workspace and logging into your workspace.
    </span>
    <span class="koboSpan" id="kobo.386.2">
     Next, let’s create a new workflow.
    </span>
    <span class="koboSpan" id="kobo.386.3">
     We’ll start by navigating to the Workflow UI by clicking on the workflows icon from the workspace navigation bar on the left-hand side.
    </span>
    <span class="koboSpan" id="kobo.386.4">
     Give the workflow a meaningful name, such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.387.1">
      Production
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.388.1">
       Monitoring Demo
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.389.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.390.1">
     If you haven’t done so already, you can download the sample notebooks for this chapter’s exercise at
    </span>
    <a href="https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10">
     <span class="koboSpan" id="kobo.391.1">
      https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10
     </span>
    </a>
    <span class="koboSpan" id="kobo.392.1">
     .
    </span>
    <span class="koboSpan" id="kobo.392.2">
     We’ll be using the IoT device data generator notebook, titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.393.1">
      04a-IoT Device Data Generator.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.394.1">
     , and the IoT Device DLT pipeline definition notebook, which is titled
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.395.1">
      04b-IoT Device
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.396.1">
       Data Pipeline.py
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.397.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.398.1">
     In the Workflow UI, create a new workflow with two tasks.
    </span>
    <span class="koboSpan" id="kobo.398.2">
     The first task will prepare an input dataset using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.399.1">
      04a-IoT Device Data Generator.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.400.1">
     notebook; the second task will
    </span>
    <a id="_idIndexMarker615">
    </a>
    <span class="koboSpan" id="kobo.401.1">
     execute a DLT pipeline that reads the generated data using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.402.1">
      04b-IoT Device Data
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.403.1">
       Pipeline.py
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.404.1">
      notebook.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer108">
     <span class="koboSpan" id="kobo.405.1">
      <img alt="Figure 10.10 – The ﻿workflow will generate IoT device data and execute a DLT pipeline update" src="image/B22011_10_010.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.406.1">
     Figure 10.10 – The workflow will generate IoT device data and execute a DLT pipeline update
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.407.1">
     Now that our workflow has been created, let’s imagine, for example, that our pipeline is taking longer than expected to execute.
    </span>
    <span class="koboSpan" id="kobo.407.2">
     Wouldn’t it be helpful to be notified if there are potential processing delays, so that your data team can investigate immediately or prevent a long-running job from running up a large cloud bill due to a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.408.1">
      processing mishap?
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.409.1">
     Fortunately, the Databricks Data Intelligence Platform makes it simple to configure this type of notification.
    </span>
    <span class="koboSpan" id="kobo.409.2">
     Let’s create a timeout threshold for our workflow.
    </span>
    <span class="koboSpan" id="kobo.409.3">
     This will automatically notify our HTTP webhook endpoint that our workflow is taking longer than expected to execute.
    </span>
    <span class="koboSpan" id="kobo.409.4">
     Once our workflow has exceeded this timeout threshold, the current execution run is stopped, and the run is marked as
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.410.1">
      failed
     </span>
    </em>
    <span class="koboSpan" id="kobo.411.1">
     .
    </span>
    <span class="koboSpan" id="kobo.411.2">
     We would like to be notified of this type of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.412.1">
      failure scenario.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.413.1">
     From the Workflow UI, click on the newly created workflow,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.414.1">
      Production Monitoring Demo
     </span>
    </strong>
    <span class="koboSpan" id="kobo.415.1">
     , to reveal the details.
    </span>
    <span class="koboSpan" id="kobo.415.2">
     Under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.416.1">
      Job notifications
     </span>
    </strong>
    <span class="koboSpan" id="kobo.417.1">
     section, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.418.1">
      Add metric thresholds
     </span>
    </strong>
    <span class="koboSpan" id="kobo.419.1">
     button to add a new run duration threshold.
    </span>
    <span class="koboSpan" id="kobo.419.2">
     Let’s add a maximum duration of 120 minutes to the maximum threshold.
    </span>
    <span class="koboSpan" id="kobo.419.3">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.420.1">
      Save
     </span>
    </strong>
    <span class="koboSpan" id="kobo.421.1">
     button.
    </span>
    <span class="koboSpan" id="kobo.421.2">
     Next, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.422.1">
      + Add notification
     </span>
    </strong>
    <span class="koboSpan" id="kobo.423.1">
     button to add a new notification.
    </span>
    <span class="koboSpan" id="kobo.423.2">
     Expand the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.424.1">
      Destination
     </span>
    </strong>
    <span class="koboSpan" id="kobo.425.1">
     drop-down menu to reveal the choices and select
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.426.1">
      + Add new system destination
     </span>
    </strong>
    <span class="koboSpan" id="kobo.427.1">
     .
    </span>
    <span class="koboSpan" id="kobo.427.2">
     A new browser tab will open, presenting the workspace administration settings for your Databricks workspace.
    </span>
    <span class="koboSpan" id="kobo.427.3">
     Under the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.428.1">
      Notifications
     </span>
    </strong>
    <span class="koboSpan" id="kobo.429.1">
     section, click on the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.430.1">
      Manage
     </span>
    </strong>
    <span class="koboSpan" id="kobo.431.1">
     button.
    </span>
    <span class="koboSpan" id="kobo.431.2">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.432.1">
      Add destination
     </span>
    </strong>
    <span class="koboSpan" id="kobo.433.1">
     button.
    </span>
    <span class="koboSpan" id="kobo.433.2">
     Select
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.434.1">
      Webhook
     </span>
    </strong>
    <span class="koboSpan" id="kobo.435.1">
     for the destination type, provide a meaningful name for the destination, enter the endpoint URL for which the notifications should be sent, and enter the username and password information if your endpoint uses basic
    </span>
    <a id="_idIndexMarker616">
    </a>
    <span class="koboSpan" id="kobo.436.1">
     HTTP authentication.
    </span>
    <span class="koboSpan" id="kobo.436.2">
     Click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.437.1">
      Create
     </span>
    </strong>
    <span class="koboSpan" id="kobo.438.1">
     button to create the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.439.1">
      Webhook destination.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer109">
     <span class="koboSpan" id="kobo.440.1">
      <img alt="Figure 10.11 – Creating a new Webhook destination" src="image/B22011_10_011.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.441.1">
     Figure 10.11 – Creating a new Webhook destination
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.442.1">
     Finally, click the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.443.1">
      Save
     </span>
    </strong>
    <span class="koboSpan" id="kobo.444.1">
     button to finalize the metric
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.445.1">
      threshold notification.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer110">
     <span class="koboSpan" id="kobo.446.1">
      <img alt="Figure 10.12 – Execution duration thresholds can be set on a ﻿workflow’s tasks" src="image/B22011_10_012.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.447.1">
     Figure 10.12 – Execution duration thresholds can be set on a workflow’s tasks
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.448.1">
     Now that we’ve established a run duration threshold, whenever our workflow runs for longer than 120 minutes, our workflow will be stopped and a notification message with a status message of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.449.1">
      Timed Out
     </span>
    </strong>
    <span class="koboSpan" id="kobo.450.1">
     will be sent to our HTTP
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.451.1">
      webhook destination.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.452.1">
     Congratulations!
    </span>
    <span class="koboSpan" id="kobo.452.2">
     You’ve
    </span>
    <a id="_idIndexMarker617">
    </a>
    <span class="koboSpan" id="kobo.453.1">
     now automated the monitoring of your data pipelines in production, allowing your team to be automatically notified whenever failure conditions arise.
    </span>
    <span class="koboSpan" id="kobo.453.2">
     This means that your teams can step in and correct data processing issues as soon as they happen and minimize
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.454.1">
      potential downtime.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-208">
    <a id="_idTextAnchor268">
    </a>
    <span class="koboSpan" id="kobo.455.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.456.1">
     In this chapter, we covered several techniques for implementing pipeline and data quality observability so that data teams can react as soon as problems arise and thwart major downstream disruptions.
    </span>
    <span class="koboSpan" id="kobo.456.2">
     One of the major keys to becoming a successful data team is being able to react to issues quickly.
    </span>
    <span class="koboSpan" id="kobo.456.3">
     We saw how alert notifications are built into many aspects of the Databricks Data Intelligence Platform and how we can configure different types of alert destinations to send notifications when conditions are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.457.1">
      not met.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.458.1">
     We covered monitoring capabilities built into the Databricks platform, such as the pipeline event log that makes it easy for pipeline owners to query the data pipeline’s health, auditability, and performance, as well as data quality, in real time.
    </span>
    <span class="koboSpan" id="kobo.458.2">
     We also saw how Lakehouse Monitoring is a robust and versatile feature that allows data teams to automatically monitor the statistical metrics of datasets and notify team members when thresholds have been crossed.
    </span>
    <span class="koboSpan" id="kobo.458.3">
     We also covered techniques to evaluate data quality throughout the pipeline, preventing downstream errors
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.459.1">
      and inaccuracies.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.460.1">
     Lastly, we concluded the chapter with a real-world exercise for automatically alerting data teams in the event of a real and all-too-common problem – when a scheduled job runs for longer
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.461.1">
      than expected.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.462.1">
     Congratulations on reaching the end of this book!
    </span>
    <span class="koboSpan" id="kobo.462.2">
     Thank you for taking this journey with me through each chapter.
    </span>
    <span class="koboSpan" id="kobo.462.3">
     We’ve covered a lot of topics, but you should feel proud of your accomplishments thus far.
    </span>
    <span class="koboSpan" id="kobo.462.4">
     By now, you should have a well-rounded foundation of the lakehouse on which you can continue to build.
    </span>
    <span class="koboSpan" id="kobo.462.5">
     In fact, I hope that this book has filled you with inspiration to continue your lakehouse journey and to build modern data applications that do great things.
    </span>
    <span class="koboSpan" id="kobo.462.6">
     I wish you the best of luck and encourage you to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.463.1">
      keep learning!
     </span>
    </span>
   </p>
  </div>
 </body></html>