- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring Data Pipelines in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to build, configure, and deploy data
    pipelines using the Databricks Data Intelligence Platform. To round off managing
    data pipelines for the lakehouse, in this final chapter of the book, we’ll dive
    into the crucial task of monitoring data pipelines in production. We’ll learn
    how to leverage comprehensive monitoring techniques directly from the Databricks
    Data Intelligence Platform to track pipeline health, pipeline performance, and
    data quality, to name a few. We will also implement a few real-world examples
    through hands-on exercises. Lastly, we’ll look at the best practices for ensuring
    that your data pipelines run smoothly, enabling timely issue detection and resolution,
    and ensuring the delivery of reliable and accurate data for your analytics and
    business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data pipeline monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline health and performance monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for production failure resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercise – setting up a webhook alert when a job runs longer than expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the examples provided in this chapter, you’ll need Databricks
    workspace permissions to create and start an all-purpose cluster so that you can
    import and execute the chapter’s accompanying notebooks. It’s also recommended
    that your Databricks user be elevated to a workspace administrator so that you
    can create and edit alert destinations. All code samples can be downloaded from
    this chapter’s GitHub repository at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    . This chapter will create and run several new notebooks, estimated to consume
    around 10-15 **Databricks** **Units** ( **DBUs** ).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data pipeline monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As data teams deploy data pipelines into production environments, being able
    to detect processing errors, delays, or data quality issues as soon as they happen
    can make a huge impact on catching and correcting issues before they have a chance
    to cascade to downstream systems and processes. As such, the environment that
    data teams build and deploy their pipelines in should be able to monitor them
    and alert them when problems arise.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ways to monitor data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several ways that data teams can monitor their data pipelines in
    production from within the Databricks Data Intelligence Platform. For example,
    data teams can manually observe updates regarding their data pipeline by doing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Viewing pipeli ne status from the **Delta Live Tables** ( **DLT** ) UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying pipeline information from the DLT event log
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While these manual means provide a way to quickly view the latest status of
    a data pipeline in an ad hoc manner, it’s certainly not a scalable solution, particularly
    as your data team adds more and more pipelines. Instead, organizations turn to
    more automated mechanisms. For instance, many organizations choose to leverage
    the built-in notification system in the Databricks Data Intelligence Platform.
    Notifications are prevalent in many objects within the platform. For example,
    data administrators can configure notifications in the following scenarios to
    alert data teams about a change in status pertaining to a particular Databricks
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: DLT pipeline (either on update or on flow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks workflow (at the top-most job level)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks workflow task (finer-grained notification than the preceding option)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these notifications can be helpful to alert teams about events or status
    changes during data processing, data teams also need mechanisms for alerting each
    other about issues in the contents of the data landing into the enterprise l akehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Using DBSQL Alerts to notify data validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Databricks Data Intelligence Platform can create alert notifications driven
    by a query within the DBSQL portion of the platform called **DBSQL Alerts** .
    DBSQL Alerts can be a useful tool to alert data teams about the data landing in
    their enterprise lakehouse. DBSQL Alerts operate by specifying a particular query
    outcome condition that must be met for the data to be considered valid. However,
    if a particular condition in an Alert is violated, such as an order amount crossing
    above a certain dollar amount threshold, for example, then the system will trigger
    a notification to send to an alert destination. The following diagram depicts
    a DBSQL Alert that notifies recipients via email when there are sales orders exceeding
    a specific dollar amount – in this case, that’s $10,000. In this example, the
    query is a max aggregation, the triggering condition is when the max aggregation
    exceeds $10,000, and the alert destination is an email address.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email](img/B22011_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Configuration of a DBSQL Alert notifying recipients via email
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, DBSQL Alerts can be scheduled to execute on a repeated schedule,
    for example once every hour. This is an excellent way to automate *data validation*
    checks on the contents of your datasets using the built-in mechanism from within
    the Databricks Data Intelligence Platform. The following screenshot is an example
    of how alerts can be used to schedule a data validation query on a repeated schedule
    and notify data teams when a particular condition or set of conditions has been
    violated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Configuration of an Alert triggering condition](img/B22011_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Configuration of an Alert triggering condition
  prefs: []
  type: TYPE_NORMAL
- en: Another mechanism for monitoring data pipelines in production is through workflow
    notifications. Within the Databricks Data Intelligence Platform, notification
    messages can be delivered to enterprise messaging platforms, such as Slack or
    Microsoft Teams, or to incident management systems such as PagerDuty. Later in
    the chapter, we’ll explore how to implement an HTTP webhook-based delivery destination,
    which is popular in web services architecture environments.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of notifications that can be sent from within a particular
    workflow – job status and task status. Job status notifications are high-level
    statuses about the overall success or failure of a particular workflow. However,
    you can also configure notifications to be sent to monitoring destinations at
    the task level, such as if you’d like to monitor when tasks within a workflow
    are retried.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Configuring job- and task-level notifications](img/B22011_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Configuring job- and task-level notifications
  prefs: []
  type: TYPE_NORMAL
- en: While alert notifications are a great way to automate the notification of team
    members when problems arise, data teams also need to monitor the health of data
    pipelines in a periodic and ad hoc manner. We will discuss this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline health and performance monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Databricks Data Intelligence Platform provides a location for data teams
    to query the status of data pipelines called the event log. The event log contains
    a history of all events that pertain to a particular DLT pipeline. In particular,
    the event log will contain an event feed with a list of event objects with recorded
    metadata about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of event occurred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unique identifier of the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamps of when the event occurred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high-level description of the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-grained details about the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An event-level indication ( **INFO** , **WARN** , **ERROR** , or **METRICS**
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The origin source of the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unlike scalar functions, which return a single value, **Table Valued Functions**
    ( **TVFs** ) are functions that return a table as the result. For DLT pipelines
    that publish to a catalog and schema within Unity Catalog, the Databricks Data
    Intelligence Platform offers a special TVF called **event_log()** to query comprehensive
    information regarding a given DLT pipeline. The **event_log()** function can take
    one of two arguments as input: a fully qualified table name of a pipeline dataset
    or a pipeline ID as an argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The event_log() TVF returns a list of events that occurred](img/B22011_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The event_log() TVF returns a list of events that occurred
  prefs: []
  type: TYPE_NORMAL
- en: 'The **event_log()** function will retrieve information about a given DLT pipeline,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Outcomes of data quality checks (expectations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auditing information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline update status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lineage information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A common approach to make it easier for data stewards to query events for a
    particular DLT pipeline is to register a view alongside the datasets for a particular
    pipeline. This allows users to conveniently reference the event log results in
    subsequent queries. The following SQL **Data Definition Language** ( **DDL** )
    statement will create a view that retrieves the event log for a DLT pipeline with
    the **my_dlt_pipeline_id** ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, the event log for a particular pipeline can grow too large, making
    it difficult for data stewards to quickly summarize the latest status updates.
    Instead, data teams can narrow the event log feed even further to a particular
    dataset within a DLT pipeline. For example, data teams can create a view on top
    o f a specific dataset to capture all the events using the **table()** function
    and provide a fully-qualified table name as an argument to the function. The following
    SQL DDL statement will create a view that retrieves the event log for a dataset
    called **my_gold_table** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The **event_log()** TVF function provides data teams with great visibility into
    the actions performed on a particular DLT pipeline and dataset making it easy
    to implement end-to-end observability and auditability.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Presently, if a DLT pipeline is configured to publish output datasets to Unity
    Catalog, then only the owner of a particular DLT pipeline can query the views.
    To share access to the event logs, the pipeline owner must save a copy of the
    event log feed to another table within Unity Catalog and grant access to other
    users or groups.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we might leverage the **event_log()** function to query the
    data quality events for a particular DLT pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – querying data quality events for a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For the following exercise, you will need to use a shared, all-purpose cluster
    or a Databricks SQL warehouse to query the event log. Furthermore, the event log
    is only available to query DLT pipelines that have been configured to store datasets
    in Unity Catalog. The event log will not be found for DLT pipelines that have
    been configured to store datasets in the legacy Hive Metastore.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality metrics are stored in the event log as a serialized JSON string.
    We’ll need to parse the JSON string into a different data structure so that we
    can easily query data quality events from the event log. Let’s use the **from_json()**
    SQL function to parse the serialized JSON string for our data quality expectations.
    We’ll need to specify a schema as an argument to instruct Spark how to parse the
    JSON string into a deserialized data structure – specifically, an array of structs
    that contain information about the expectation name, dataset name, number of passing
    records, and number of failing records. Lastly, we’ll use the **explode()** SQL
    function to transform the array of expectation structs into a new row for each
    expectation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage the previously defined views to monitor the ongoing data quality
    of the datasets within our DLT pipeline. Let’s create another view pertaining
    to the data quality of our DLT pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Several common questions that are posed by data teams include: “How many records
    were processed?”, “How many records failed data quality validation?”, or “What
    was the percentage of passing records versus failing records?”. Let’s take the
    previous example a step further and summarize the high-level data quality metrics
    per dataset in our pipeline. Let’s count the total number of rows having an expectation
    applied, as well as the percentage of passing records versus failing records for
    each of the datasets in our DLT pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Events captured in the DLT event log](img/B22011_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Events captured in the DLT event log
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the **event_log()** function makes it simple for data teams
    to query comprehensive information regarding a given DLT pipeline. Not only can
    data teams query the status of a pipeline update but they can also query the status
    of the quality data landing into their lakehouse. Still, data teams need a way
    to automate the notification of failing data quality checks at runtime, as is
    the scenario when the data accuracy of downstream reports is critical to the business.
    Let’s look closer at this in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ongoing monitoring of the data quality of datasets within your lakehouse is
    critical for the success of business-critical data applications deployed to production.
    Take, for example, the impact that a sudden ingestion of null values on a joined
    column might have on downstream reports that rely on joining together upstream
    datasets. Suddenly, **business intelligence** ( **BI** ) reports might refresh,
    but the data may appear stale or outdated. By automatically detecting data quality
    issues as soon as they arise, your data team can be alerted of potential issues
    and take immediate action to intervene and correct possible data corruption or
    even data loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Detecting issues early is important to ensure the quality of
    downstream processes](img/B22011_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Detecting issues early is important to ensure the quality of downstream
    processes
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Lakehouse Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lakehouse Monitoring, a recent feature of the Databricks Data Intelligence Platform,
    gives data teams the ability to track and monitor the data quality of data and
    other assets in the lakehouse. Data teams can automatically measure the statistical
    distribution of data across columns, number of null values, minimum, maximum,
    median column values, and other statistical properties. With Lakehouse Monitoring,
    data teams can automatically detect major problems in datasets such as data skews
    or missing values, and alert team members of issues so that they can take appropriate
    action.
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouse Monitoring is most useful when used to monitor the data quality of
    Delta tables, views, materialized views, and streaming tables. It can even be
    used in **Machine Learning** ( **ML** ) pipelines, measuring the statistical summaries
    of datasets and triggering alert notifications as soon as data drift is detected.
    Furthermore, Lakehouse Monitoring can be customized to be fine- or coarse-grained
    in the monitoring metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouse Monitoring begins with the creation of a monitor object, which is
    then attached to a data asset such as a Delta table in your lakehouse. Behind
    the scenes, the monitor object will create two additional tables inside your lakehouse
    to capture statistical measures of the corresponding Delta table or other data
    assets.
  prefs: []
  type: TYPE_NORMAL
- en: The monitoring tables are then used to power a Dashboard, which can be used
    by data teams and other stakeholders to get a view into the real-time data insights
    of the quality of your data in the l akehouse.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – A lakehouse monitor will create two metrics tables for the
    monitored data asset](img/B22011_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – A lakehouse monitor will create two metrics tables for the monitored
    data asset
  prefs: []
  type: TYPE_NORMAL
- en: 'A lakehouse monitor can be configured to measure different aspects of a data
    asset, which is also referred to as a profile type. There are three *monitor profile
    types* that can be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Snapshot** : This is a generic, yet robust monitor. It’s useful to monitor
    data quality and other metrics of a table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series** : It’s useful for time series datasets. It’s used to monitor
    the data quality over time period windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference** : It’s useful to compare the quality of an ML model inference
    versus the input over a window of time periods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we’ll only be covering the time series and snapshot types.
    Discussing inference is out of the scope of this book, but you are encouraged
    to explore how Lakehouse Monitoring can be helpful for ML use cases ( [https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html](https://docs.databricks.com/en/lakehouse-monitoring/fairness-bias.html)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Monitors can also be created that compare the statistical metrics of a table
    versus a baseline table. This can be useful in scenarios such as comparing the
    relative humidity of smart thermostat devices for this week as compared to last
    week, or comparing the number of recorded sales in a particular dataset for a
    monthly sales report versus last month’s report, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a practical example of using a lakehouse monitor in a l akehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – creating a lakehouse monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this hands-on exercise, we’re going to create a lakehouse monitor for measuring
    the data quality of a target Delta table. Although our Delta table does contain
    timestamp information, we’ll choose a *snapshot profile* to monitor the data quality
    of a target Delta table in our lakehouse. Recall that the snapshot profile is
    a generic lakehouse monitor that also proves to be quite versatile, as mentioned
    earlier. The snapshot profiler will allow us to measure standard summary metrics
    about our dataset or insert custom business calculations around the data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Like many resources in the Databricks Data Intelligence Platform, there are
    a variety of ways that you can create a new lakehouse monitor. For example, you
    can use the Databricks UI, the Databricks REST API, the Databricks CLI (covered
    in [*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) ), or automation tools such
    as Terraform, to name a few. Perhaps the simplest mechanism for creating a new
    monitor is through the UI. In this hands-on exercise, we’re going to use the Databricks
    UI to create the lakehouse monitor. This is a great way to get started experimenting
    with Lakehouse Monitoring and with different data quality metrics to measure your
    datasets. However, it’s recommended in production scenarios to migrate your lakehouse
    monitors to an automated build tool such as **Databricks Asset Bundles** ( **DABs**
    ) (covered in [*Chapter 9*](B22011_09.xhtml#_idTextAnchor222) ) or Terraform (covered
    in [*Chapter 8*](B22011_08.xhtml#_idTextAnchor185) ).
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t done so already, you can clone the accompanying code resources
    for this chapter at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to generate a target Delta table, which we would like to monitor
    the data quality. Clone or import the data generator notebook or create a new
    notebook with the following code generator source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first cell of the notebook, we’ll leverage the **%pip** magic command
    to download and install the **dbldatagen** Python library, which is used to generate
    sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll define a helper function for generating a synthetic dataset containing
    smart thermostat readings over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’ll save the newly created dataset as a Delta table in Unity Catalog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that our Delta table has been created in our lakehouse, let’s use the UI
    in the Catalog Explo rer to create a new monitor.
  prefs: []
  type: TYPE_NORMAL
- en: From the left-side navigation bar, click on the Catalog Explorer icon. Next,
    navigate to the catalog created for this chapter by expanding the list of catalogs
    or using the **Search** field to filter the results. Click on the schema that
    was created for this chapter. Finally, click on the Delta table that was created
    earlier by our data generator notebook. Click on the data quality tab that is
    appropriately titled **Quality** .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – A new monitor can be created directly from the Databricks UI](img/B22011_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – A new monitor can be created directly from the Databricks UI
  prefs: []
  type: TYPE_NORMAL
- en: Next, click on the **Get started** button to begin creating a new monitor. A
    pop-up dialog will open, prompting you to select the profile type for the monitor,
    as well as advanced configuration options such as the schedule, notification delivery,
    and workspace directory for storing the generated dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Click the dropdown for the profile type and select the option for generating
    a snapshot profile.
  prefs: []
  type: TYPE_NORMAL
- en: Next, click on the **Advanced Options** section to expand the dialog form. The
    UI will allow users to capture dataset metrics, either manually or by defining
    a cron schedule for executing the metrics calculations on a repeated schedule.
    You’ll notice that the dialog provides the flexibility to define the schedule
    using a traditional cron syntax, or by selecting the date and time drop-down menus
    in the dialog form. For this hands-on exercise, we’ll choose the former option
    and refresh the monitoring metrics manually through the click of a button.
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can choose to have notifications about the success or failure
    of monitoring metrics calculations sent via email to a list of email recipients.
    You can add up to five email addresses for notifications to be delivered to. Ensure
    that your user email address is listed in the **Notifications** section and that
    the checkbox is checked to receive a notification for failures during the metrics
    collection.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall from earlier, a lakehouse monitor will create two metrics tables.
    We’ll need to provide a location in Unity Catalog to store these metrics tables.
    Under the **Metrics** section, add the catalog and schema name created for this
    chapter’s hands-on exercise. For example, enter **chp10.monitor_demo** .
  prefs: []
  type: TYPE_NORMAL
- en: The last item that we need to specify is a workspace location for storing the
    generated dashboard for our lakehouse monitor. By default, the generated assets
    will be stored under the user’s home directory, for example, **/Users/<user_email_address>/databricks_lakehouse_monitoring**
    . For this hands-on exercise, we’ll accept the default location.
  prefs: []
  type: TYPE_NORMAL
- en: We’re ready to create our monitor! Click the **Create** button to create the
    lakehouse monitor for our Delta table.
  prefs: []
  type: TYPE_NORMAL
- en: Since we haven’t configured a schedule for our lakehouse monitor, we’ll need
    to manually execute a metrics collection. Back in the Catalog Explorer, under
    the **Quality** tab of our Delta table, click on the **Refresh metrics** button
    to manually trigger a metrics collection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog
    Explorer UI](img/B22011_10_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Monitoring metrics can be manually triggered from the Catalog
    Explorer UI
  prefs: []
  type: TYPE_NORMAL
- en: An update of the table metrics will be triggered to execute and will take up
    to a few minutes to complete. Once the update has completed, click the **View
    dashboard** button to view the metrics captured. Congratulations! You’ve created
    your first lakehouse monitor and you’re well on your way to implementing robust
    and automated data quality observability for your data team.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea of how to alert our team members when production issues
    arise, let’s turn our attention to a few approaches to resolve failures in production
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for production failure resolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The DLT framework was designed with failure resolution in mind. For example,
    DLT will automatically respond to three types of common pipeline failures:'
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Runtime regressions (covered in [*Chapter 2*](B22011_02.xhtml#_idTextAnchor052)
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update processing failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transaction failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at update failures and data transaction failures in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Handling pipeline update failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DLT framework was designed with robust error handling in mind. During a
    pipeline update, the framework will attempt to apply the most recent updates to
    tables defined in the dataflow graph. If a processing error occurs, the framework
    will classify the error as either a retriable error or a non-retriable error.
    A retriable error means that the framework has classified the runtime error as
    likely an issue caused by the current set of conditions. For example, a system
    error would not be considered a retriable error, since it relates to the runtime
    environment that execution retries will not solve. However, a network timeout
    would be a retriable error, since it could be impacted by the temporary set of
    network environment conditions. By default, the DLT framework retries a pipeline
    update twice if it detects a *retriable* error.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering from table transaction failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the nature of the Delta Lake transaction log, changes to a dataset are
    atomic, meaning that they can only happen if a table transaction (such as a **Data
    Manipulation Language** ( **DML** ) statement) is committed to the transaction
    log. As a result, if a transaction fails in the middle of its execution, then
    the entire transaction is abandoned, thereby preventing the dataset from entering
    a non-deterministic state requiring data teams to intervene and manually reverse
    the data changes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to handle pipeline failures in production, let’s
    cement the topics from this chapter through a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – setting up a webhook alert when a job runs longer than expected
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this hands-on exercise, we’ll be creating a custom HTTP webhook that will
    notify an HTTP endpoint about the timeout status of a scheduled job in Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: A webhook alert is a notification mechanism in the Databricks Data Intelligence
    Platform that enables data teams to monitor their data pipeline by automatically
    publishing the outcome of a particular job execution run. For example, you can
    receive notifications about the successful run, execution state, and run failures.
  prefs: []
  type: TYPE_NORMAL
- en: Why are we using a workflow rather than a DLT pipeline directly?
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a DLT pipeline will often be just one of many dependencies in a
    complete data product. Databricks workflows are a popular orchestration tool that
    can prepare dependencies, run one or more DLT pipelines, and execute downstream
    tasks as well. In this exercise, we’ll be configuring notifications from a Databricks
    workflow, as opposed to notifications directly from a DLT pipeline, to simulate
    a typical production scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by navigating to your Databricks workspace and logging into your
    workspace. Next, let’s create a new workflow. We’ll start by navigating to the
    Workflow UI by clicking on the workflows icon from the workspace navigation bar
    on the left-hand side. Give the workflow a meaningful name, such as **Production**
    **Monitoring Demo** .
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t done so already, you can download the sample notebooks for this
    chapter’s exercise at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter10)
    . We’ll be using the IoT device data generator notebook, titled **04a-IoT Device
    Data Generator.py** , and the IoT Device DLT pipeline definition notebook, which
    is titled **04b-IoT Device** **Data Pipeline.py** .
  prefs: []
  type: TYPE_NORMAL
- en: In the Workflow UI, create a new workflow with two tasks. The first task will
    prepare an input dataset using the **04a-IoT Device Data Generator.py** notebook;
    the second task will execute a DLT pipeline that reads the generated data using
    the **04b-IoT Device Data** **Pipeline.py** notebook.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 10.10 – The \uFEFFworkflow will generate IoT device data and execute\
    \ a DLT pipeline update](img/B22011_10_010.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The workflow will generate IoT device data and execute a DLT
    pipeline update
  prefs: []
  type: TYPE_NORMAL
- en: Now that our workflow has been created, let’s imagine, for example, that our
    pipeline is taking longer than expected to execute. Wouldn’t it be helpful to
    be notified if there are potential processing delays, so that your data team can
    investigate immediately or prevent a long-running job from running up a large
    cloud bill due to a processing mishap?
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the Databricks Data Intelligence Platform makes it simple to configure
    this type of notification. Let’s create a timeout threshold for our workflow.
    This will automatically notify our HTTP webhook endpoint that our workflow is
    taking longer than expected to execute. Once our workflow has exceeded this timeout
    threshold, the current execution run is stopped, and the run is marked as *failed*
    . We would like to be notified of this type of failure scenario.
  prefs: []
  type: TYPE_NORMAL
- en: From the Workflow UI, click on the newly created workflow, **Production Monitoring
    Demo** , to reveal the details. Under the **Job notifications** section, click
    on the **Add metric thresholds** button to add a new run duration threshold. Let’s
    add a maximum duration of 120 minutes to the maximum threshold. Click the **Save**
    button. Next, click on the **+ Add notification** button to add a new notification.
    Expand the **Destination** drop-down menu to reveal the choices and select **+
    Add new system destination** . A new browser tab will open, presenting the workspace
    administration settings for your Databricks workspace. Under the **Notifications**
    section, click on the **Manage** button. Click the **Add destination** button.
    Select **Webhook** for the destination type, provide a meaningful name for the
    destination, enter the endpoint URL for which the notifications should be sent,
    and enter the username and password information if your endpoint uses basic HTTP
    authentication. Click the **Create** button to create the Webhook destination.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Creating a new Webhook destination](img/B22011_10_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – Creating a new Webhook destination
  prefs: []
  type: TYPE_NORMAL
- en: Finally, click the **Save** button to finalize the metric threshold notification.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 10.12 – Execution duration thresholds can be set on a \uFEFFworkflow’s\
    \ tasks](img/B22011_10_012.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Execution duration thresholds can be set on a workflow’s tasks
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve established a run duration threshold, whenever our workflow runs
    for longer than 120 minutes, our workflow will be stopped and a notification message
    with a status message of **Timed Out** will be sent to our HTTP webhook destination.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You’ve now automated the monitoring of your data pipelines
    in production, allowing your team to be automatically notified whenever failure
    conditions arise. This means that your teams can step in and correct data processing
    issues as soon as they happen and minimize potential downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered several techniques for implementing pipeline and
    data quality observability so that data teams can react as soon as problems arise
    and thwart major downstream disruptions. One of the major keys to becoming a successful
    data team is being able to react to issues quickly. We saw how alert notifications
    are built into many aspects of the Databricks Data Intelligence Platform and how
    we can configure different types of alert destinations to send notifications when
    conditions are not met.
  prefs: []
  type: TYPE_NORMAL
- en: We covered monitoring capabilities built into the Databricks platform, such
    as the pipeline event log that makes it easy for pipeline owners to query the
    data pipeline’s health, auditability, and performance, as well as data quality,
    in real time. We also saw how Lakehouse Monitoring is a robust and versatile feature
    that allows data teams to automatically monitor the statistical metrics of datasets
    and notify team members when thresholds have been crossed. We also covered techniques
    to evaluate data quality throughout the pipeline, preventing downstream errors
    and inaccuracies.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we concluded the chapter with a real-world exercise for automatically
    alerting data teams in the event of a real and all-too-common problem – when a
    scheduled job runs for longer than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on reaching the end of this book! Thank you for taking this
    journey with me through each chapter. We’ve covered a lot of topics, but you should
    feel proud of your accomplishments thus far. By now, you should have a well-rounded
    foundation of the lakehouse on which you can continue to build. In fact, I hope
    that this book has filled you with inspiration to continue your lakehouse journey
    and to build modern data applications that do great things. I wish you the best
    of luck and encourage you to keep learning!
  prefs: []
  type: TYPE_NORMAL
