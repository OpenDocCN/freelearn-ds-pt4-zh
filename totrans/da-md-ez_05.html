<html><head></head><body>
		<div id="_idContainer268">
			<h1 class="chapterNumber">5</h1>
			<h1 id="_idParaDest-84" class="chapterTitle" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor085"/>Applying Machine Learning at Work</h1>
			<p class="normal">You've heard a lot about creating business value with intelligent algorithms: it's finally time to roll up our sleeves and make it happen. In this chapter, we are going to experience what it means to apply machine learning to tangible cases by going through a few step-by-step tutorials. Our companion KNIME is back on stage: we will learn how to build workflows for implementing machine learning models using real-world data. We are going to meet a few specific algorithms and learn the intuitive mechanisms behind how they operate. We'll glimpse into their underlying mathematical models, focusing on the basics to comprehend their results and leverage them in our work.</p>
			<p class="normal">This practical chapter will answer several questions, including:</p>
			<ul>
				<li class="bullet">How do I make predictions using supervised machine learning algorithms in KNIME?</li>
				<li class="bullet">How can I check whether a model is performing well?</li>
				<li class="bullet">How do we avoid the risk of overfitting?</li>
				<li class="bullet">What techniques can I use to improve the performance of a model?</li>
				<li class="bullet">How can I group similar elements together using clustering algorithms?</li>
			</ul>
			<p class="normal">The tutorials included in this chapter cover three of the most recurrent cases when you can rely on machine learning as part of your work: predicting numbers, classifying entities, and grouping elements. Think of them as "templates" that you can widely reapply after you reach the last page of the chapter and that you are likely to keep using as a reference. The steps of the tutorials are also organized in the same order they would unfold in everyday practice, including the "back and forth" iterations required for improving the performance of your model. This will prepare you to face the actual use of real-life machine learning, which often follows a circuitous route made of trial and error attempts.</p>
			<p class="normal">Within each tutorial, you will encounter one or two machine learning algorithms (specifically, <strong class="keyword">linear regression</strong> in the first, <strong class="keyword">decision tree</strong> and <strong class="keyword">random forest</strong> in the second, and <strong class="keyword">k-means </strong>in the third) that will be introduced and explained before being seen in action. Let's get started with some first predictions!</p>
			<h1 id="_idParaDest-85" class="title"><a id="_idTextAnchor086"/>Predicting numbers through regressions</h1>
			<p class="normal">For this <a id="_idIndexMarker383"/>tutorial, you will assume the—somewhat—enviable role of a real estate agent based in Rome, Italy. The company you work for owns multiple agencies specialized in rentals of properties located in the broader metropolitan area of the Eternal City. Your passion for data analytics got you noticed by the CEO: she asked you to figure out a way to support agents in objectively evaluating the fair monthly rent of a property based on its features. She noticed that the business greatly suffers when the rent set for a property is not aligned with the market. In fact, if the rent is too low, the agency fee (which is a fixed percentage of the agreed rent) will end up being lower than what it could have been, leaving profit on the table. On the other hand, if the ask is too high, revenues for the agency will take longer to materialize, causing a substantial impact on the cash flow. The traditional approach to set the monthly rent for new properties is a "negotiation" between owners and agents, who will use their market understanding (and sometimes the benchmark of similar properties) to convince the owners about the right rent to ask for.</p>
			<p class="normal">You are sure that machine learning has the potential to make a difference, and you are resolute in finding an ML way to improve this business process. The idea that comes to mind is to use the database of the monthly rent of previously rented properties (for which we have available their full description) to predict the right monthly rent of future properties based on their objective characteristics. Such a data-driven approach, if well communicated, can ease the price-setting process and result in a mutual advantage for all the parties involved: the landlord and the agency will get a quick and profitable transaction, and the tenant will obtain a fair rent.</p>
			<p class="normal">The prospect of building a machine able to predict rental prices is exhilarating and makes you impatient to start. You manage to obtain an extraction of the last 4,000 rental agreements signed at the agency (<code class="Code-In-Text--PACKT-">RomeHousing-History.xlsx</code>). The table contains, for each property:</p>
			<ul>
				<li class="bullet"><em class="italic">House_ID</em>: a unique identifier of the property.</li>
				<li class="bullet"><em class="italic">Neighborhood</em>: the name of the area where the property lies, ranging from the fancy surroundings of <code class="Code-In-Text--PACKT-">Piazza</code> <code class="Code-In-Text--PACKT-">Navona</code> to the tranquil, lakeside towns of <code class="Code-In-Text--PACKT-">Castelli</code> <code class="Code-In-Text--PACKT-">Romani</code>. <em class="italic">Figure 5.1</em> shows a map of the Rome area with some of these neighborhoods.</li>
				<li class="bullet"><em class="italic">Property_type</em>: a string clarifying if the property is a <code class="Code-In-Text--PACKT-">flat</code>, a <code class="Code-In-Text--PACKT-">house</code>, a <code class="Code-In-Text--PACKT-">villa</code>, or a <code class="Code-In-Text--PACKT-">penthouse</code>.</li>
				<li class="bullet"><em class="italic">Rooms</em>: the number of available rooms in the property, including bathrooms. </li>
				<li class="bullet"><em class="italic">Surface</em>: the usable floor area of the property in square meters.</li>
				<li class="bullet"><em class="italic">Elevator</em>: a binary category indicating if an elevator is available (<code class="Code-In-Text--PACKT-">1</code>) or not (<code class="Code-In-Text--PACKT-">0</code>).</li>
				<li class="bullet"><em class="italic">Floor_type</em>: a<a id="_idIndexMarker384"/> category showing if the property is on a <code class="Code-In-Text--PACKT-">Mezzanine</code>, a <code class="Code-In-Text--PACKT-">Ground</code> floor, or an <code class="Code-In-Text--PACKT-">Upper</code> level.</li>
				<li class="bullet"><em class="italic">Floor_number</em>: the floor number on which the property is situated, based on the European convention (<code class="Code-In-Text--PACKT-">0</code> is for the ground floor, <code class="Code-In-Text--PACKT-">0.5</code> is the mezzanine, <code class="Code-In-Text--PACKT-">1</code> is for the first level above the ground, and so on).</li>
				<li class="bullet"><em class="italic">Rent</em>: the all-inclusive, monthly rent in euros on the final rental agreement. <figure class="mediaobject"><img src="image/B17125_05_01.png" alt="Map&#10;&#10;Description automatically generated"/></figure></li>
			</ul>
			<p class="packt_figref">Figure 5.1: The Rome neighborhoods covered by our real estate. Have you visited any of these places already?</p>
			<p class="normal">Before building the model, you wisely stop for a second and think through the ways you are going to practically leverage it once ready. You realize that the potential business value for completing this endeavor is two-fold:</p>
			<ol>
				<li class="numbered">First, by interpreting how the model works, you can find out some insightful evidence on the market price formation mechanisms. You might be able to find answers to the questions: <em class="italic">what features really do make a difference in the pricing?</em>, <em class="italic">does the floor number impact the value greatly?</em>, and <em class="italic">which neighborhoods prove to be most expensive ones, at parity of all other characteristics of the property?</em>. Some of the answers will reinforce the market understanding that your agency already has, adding the benefit of making this knowledge explicit and formally described. More interestingly, other findings might be truly unexpected and unveil original dynamics you did not know about.</li>
				<li class="numbered">Second, your model can be used to generate data-based recommendations on the rent to <a id="_idIndexMarker385"/>be set for new properties as they go on the market and enter the portfolio of the agency. To make things more interesting on this front, the owner shares with you a list (<code class="Code-In-Text--PACKT-">RomeHousing-NewProperties.xlsx</code>) of 10 incoming properties for which the rental price has not been fixed yet, using the same features (such as <em class="italic">Neighborhood</em>, <em class="italic">Property_type</em>, and so on) available in the historical database. Once ready, you will apply your model to these sample properties as an illustration of how it works.</li>
			</ol>
			<p class="normal">You are now clear on what the business requires, and you can finally translate it into definite machine learning terms, building on what we have learned in the previous chapter. You need to build a machine that predicts "unknown" rental prices by learning from some "known" examples: the database of previously rented properties is your <em class="italic">labeled</em> dataset, as it has examples of your target variable, in this case, the <em class="italic">Rent</em>. Going through the catalog of machine learning algorithms (<em class="italic">Figure 4.5</em>), you realize we are clearly in the category of <em class="italic">supervised</em> machine learning. More specifically, you need to predict numbers (rent in euros), so you definitely need to leverage an algorithm for doing a <em class="italic">regression</em>. </p>
			<p class="normal">The ML way to solve this business opportunity is now clear in front of your eyes: you can finally get KNIME started and create a new workflow (<strong class="screenText">File</strong> | <strong class="screenText">New…</strong> | <strong class="screenText">New KNIME Workflow</strong>)<a id="_idTextAnchor087"/>:</p>
			<ol>
				<li class="numbered" value="1">As a very first step, you load your labeled dataset by dragging and dropping the file<em class="italic"> </em>(<code class="Code-In-Text--PACKT-">RomeHousing-History.xlsx</code>) into your blank workflow or by implementing the<em class="italic"> </em><strong class="keyword">Excel Reader</strong><em class="italic"> </em>node. In <a id="_idIndexMarker386"/>either case, KNIME will have recognized the structure of the file, and you just need to accept its default configuration. After running the node, you obtain the dataset shown in <em class="italic">Figure 5.2</em>, where you find the nine columns you expected:<figure class="mediaobject"><img src="image/B17125_05_02.png" alt=""/></figure><p class="packt_figref">Figure 5.2: Historical rental data loaded into KNIME: 4,000 properties to learn from</p>
</li>
</ol>

<p class="normal">When you<a id="_idIndexMarker387"/> build a machine learning model, you will interact in various ways with the columns of your data table. It is sensible to get an understanding of what you are going to deal with by exploring the columns right at the beginning. Fortunately, the <strong class="keyword">Statistics</strong> node helps as it displays at once the most important things you need to know about your columns.</p>
<h2 id="_idParaDest-86" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor088"/><img src="image/NEW_statistics_node.png" alt=""/> </span><em class="bold-italic">Statistics</em></h2><p class="normal">This node (<strong class="screenText">Analytics &gt; Statistics</strong>) calculates<a id="_idIndexMarker388"/> summary<a id="_idIndexMarker389"/> statistics for each column available in the input table. The checkbox appearing at the top of its configuration dialog (<em class="italic">Figure 5.3</em>) lets you decide whether to <strong class="screenText">Calculate median values</strong> of the numeric columns: this calculation might be computationally expensive for large datasets, so you will tick it only if necessary. The column selector in the middle lets you decide which columns should be treated as <strong class="screenText">Nominal</strong>. For these columns, the node will count the number of instances of each unique value: this is useful for categorical columns when you want to quickly assess the relative footprint of every category in a table. The main summary metrics calculated by the node are minimum (<strong class="screenText">Min</strong>), average (<strong class="screenText">Mean</strong>), <strong class="screenText">Median</strong>, maximum (<strong class="screenText">Max</strong>), standard deviations (<strong class="screenText">Std. Dev.</strong>), <strong class="screenText">Skewness</strong>, <strong class="screenText">Kurtosis</strong>, count of non-numeric values such as missing values (<strong class="screenText">No. Missing</strong>), and plus or minus infinite (<strong class="screenText">No. +∞</strong>, <strong class="screenText">No. –∞</strong>). The node will also output the histograms showing the distributions of the values and, for nominal columns, the list of the most and least numerous categories identified in the dataset:</p><div id="_idContainer158" class="note"><p class="Information-Box--PACKT-"><strong class="keyword">Skewness</strong> and <strong class="keyword">Kurtosis</strong> are <a id="_idIndexMarker390"/>certainly<a id="_idIndexMarker391"/> the least known summary statistics among the ones mentioned above. However, they are useful in telling you quickly how much the shape of a distribution differs from the iconic bell-shaped curve of a pure Gaussian distribution. Skewness tells you about the symmetry of the distribution: if it has a positive value, it is skewed on the left while if it has a negative value, it is skewed on the right. Kurtosis tells you about the flatness of the distribution: if <a id="_idIndexMarker392"/>negative it is flatter than a bell curve, while if positive it shows <a id="_idIndexMarker393"/>a sharper peak.</p></div><figure class="mediaobject"><img src="image/B17125_05_03.png" alt=""/></figure><p class="packt_figref">Figure 5.3: Configuration of Statistics: explore the data with its summary statistics</p>
<ol>
				<li class="numbered" value="2">Implement the <strong class="keyword">Statistics</strong> node and connect it with the previous one. When configuring it, check the first box so we can have a look at the median values of the numeric columns. In the selector of the nominal values, keep only the string-typed columns (<em class="italic">Neighborhood</em>, <em class="italic">Property_type</em>, and <em class="italic">Floor_type</em>) plus <em class="italic">Elevator</em>. Although formally numeric, this latter column splits our samples into two categories, the properties equipped with the elevator and the ones missing it: it will be interesting to read a count of how many properties fall into each category, so we shall treat this column as nominal. If you run the node and display its main output (just press <em class="italic">Shift</em> + <em class="italic">10</em> or, after you execute the node, right-click on it and select <strong class="screenText">View: Statistics View</strong>) you will obtain a window with three useful tabs. The first one (<em class="italic">Figure 5.4</em>) gives you all the highlights on the numeric columns: we learn that the average rent of the properties in our database is slightly above €1,000 and that the median floor surface is around 70 square meters. We also learn that there are no missing values: this is good news as <a id="_idIndexMarker394"/>we don't need to <a id="_idIndexMarker395"/>engage in clean up chores:<figure class="mediaobject"><img src="image/B17125_05_04.png" alt="A picture containing table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.4: Numeric panel within the Statistics output: how are my numeric features distributed?</p><p class="bullet-para">The second and third (<em class="italic">Figure 5.5</em>) tabs tell you about the nominal columns: we learn that some neighborhoods (such as <code class="Code-In-Text--PACKT-">Magliana</code> and <code class="Code-In-Text--PACKT-">Portuense</code>) are much less represented in our dataset than others. By looking at the values in the <em class="italic">Property_type</em> column, we also learn that the vast majority of our rented properties have been flats:</p><figure class="mediaobject"><img src="image/B17125_05_05.png" alt=""/></figure><p class="packt_figref">Figure 5.5: Top/bottom panel within the Statistics output: check the values of your categorical columns</p>
</li>
</ol>

<p class="normal">Now that <a id="_idIndexMarker396"/>we have explored the dataset<a id="_idIndexMarker397"/> and have become acquainted with the main characteristics of its columns, we can proceed with the fun part and design our model. To build a robust supervised machine learning model, we need to rely on the typical flow that we encountered in the previous chapter. Let's refresh our memory on this critical point: in order to stay away from the trap of overfitting, we need to partition our labeled data into training and test sets, learn on the training set, predict on the test set, and—finally—assess the expected performance of the model by scoring the predicted values. You can go back to <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>, and check <em class="italic">Figure 4.13</em> out to see once again the full process: we are always required to follow this approach when implementing a machine that can predict something useful. So, the very first step is to randomly partition all our labeled data rows into two separate subsets. This is exactly the "specialty" of our next node: <strong class="keyword">Partitioning</strong>.</p>
<h2 id="_idParaDest-87" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor089"/><img src="image/image007.png"/> </span><em class="bold-italic">Partitioning</em></h2><p class="normal">This node (<strong class="screenText">Manipulation &gt; Row &gt; Transform</strong>) performs a row-wise split of the input table into<a id="_idIndexMarker398"/> two tables corresponding <a id="_idIndexMarker399"/>to the upper (first partition) and lower (second partition) output ports. The selector at the top of its configuration window (<em class="italic">Figure 5.6</em>) lets you set the size of the first partition (upper output port). You can either specify the number of rows to be included (<strong class="screenText">Absolute</strong>) or the relative size of the partition in percentage points (<strong class="screenText">Relative[%]</strong>). The second selector specifies the method used for splitting the rows into the two partitions:</p><ul><li class="bullet"><strong class="screenText">Take from top</strong>: if you select this option, the split will happen according to the current sorting order. The top rows of the input table will end up in the first partition while all others, after a certain threshold, will go to the second. The position of the threshold depends on the size of the partition that you have already decided above.</li><li class="bullet"><strong class="screenText">Linear sampling</strong>: also, in this case, the order of the input table rows is preserved: every <em class="italic">n</em><sup class="italic">th</sup> row will go to an output port, alternating regularly across the two partitions. If, for instance, you run a linear sampling for creating two equally sized partitions (each having half of the original rows), you will end up with all the odd rows in a partition and all the even ones in the other. If, instead, the split is one-third and two-thirds, you will have every third row in the first partition and all others in the second one. This is particularly useful when your dataset is a time series, with records sorted chronologically.</li><li class="bullet"><strong class="screenText">Draw randomly</strong>: if you go for this option, you obtain a random sampling. The only thing you can be sure of is that the number of rows in the first partition will be exactly what you have set in the first selector.</li><li class="bullet"><strong class="screenText">Stratified sampling</strong>: in this case, you also run a random sampling but, you force the distribution of a nominal column to be preserved in both output partitions. For example: if you have an input table describing 1,000 patients, out of which 90% are labeled as <code class="Code-In-Text--PACKT-">negative</code> and 10% as <code class="Code-In-Text--PACKT-">positive</code>, you can use stratified sampling to retain the ratio between positive and negative patients in each partition. In this case, if you want to have 700 rows to go to the first partition, you will end up with exactly 630 negative patients and 70 positive ones: the proportion is kept.</li></ul>

			<p class="normal">If you have selected a splitting method based on a random selection (the last two options in the list above), you can protect the reproducibility of your workflow by ticking the <strong class="screenText">Use random seed</strong> optional box. When you specify a constant number for initializing the random sampling, you are "fixing" the random behavior: as a result, you will always obtain the same partitions every time you execute the node. This is handy when you want to keep the partitioning constant as you go back and forth in the construction of your workflow or <a id="_idIndexMarker400"/>when you want other people<a id="_idIndexMarker401"/> to get the same partitioning on their machines:</p>
			<figure class="mediaobject"><img src="image/B17125_05_06.png" alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.6: Configuration dialog of Partitioning: how do you want to split your dataset?</p>
			<div>
				<div id="_idContainer164" class="note">
					<p class="Information-Box--PACKT-">One thing that computers really struggle with is behaving randomly and doing anything "unexpected" as they are built and programmed to follow a deterministic set of steps. For this reason, computers leverage special algorithms for <a id="_idIndexMarker402"/>generating sequences of <strong class="keyword">pseudo-random numbers</strong> that "look" as if they are truly random. Notably, the starting point of <a id="_idIndexMarker403"/>these sequences (the <strong class="keyword">random seed</strong>) can determine the full progression of numbers. When needed, a computer can still generate a random seed by looking at a quickly changing state (like the number of clock cycles of the CPU from the last boot) or by measuring some microscopic physical quantities (like a voltage on a port) that are affected by uncontrollable phenomena, such as thermal noise and other quantic effects. It's interesting how computers struggle with what would take us just a flip of a coin!</p>
				</div>
			</div>
			<p class="normal">Let's start our supervised learning typical flow and split our full housing dataset into training and test subsets:</p>
			<ol>
				<li class="numbered" value="3">Let's implement the <strong class="keyword">Partitioning</strong> node and connect it with the output of the <strong class="keyword">Excel Reader</strong> output (you can keep the <strong class="keyword">Statistics </strong>node unhooked as we don't need to use its outputs). In the configuration dialog, let's make sure that we select the <strong class="screenText">Relative[%]</strong> option with the value <code class="Code-In-Text--PACKT-">70</code>. This means that, out of the 4,000 properties available at the inputs, 70% of them will be used for training (which is a fair thing to do since, as anticipated in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>, the training set should normally cover between 70% and 80% of the total full dataset). We<a id="_idIndexMarker404"/> want the partitioning to happen randomly. In the previous step, we noticed that some nominal columns (like <em class="italic">Neighborhood</em>) display an unbalanced distribution across their values. This means that we have the risk of having the very few properties in a smaller neighborhood (like the 26 rows referring to <code class="Code-In-Text--PACKT-">Magliana</code>) ending up solely in a partition. Although this is not strictly required, we better avoid any unbalance that can affect our learning and select <strong class="screenText">Stratified sampling</strong> on <em class="italic">Neighborhood</em> in the dialog. You can also click on the bottom tick box and, on the right, type in a random seed, like <code class="Code-In-Text--PACKT-">12345</code>, so that you can count on the same partitioning over and over. When you run the node, you find that in the upper output port (right-click on the node and select <strong class="screenText">First partition</strong>) you find 2,800 rows that are exactly 70% of the original dataset. This is a good sign <a id="_idIndexMarker405"/>and we can move ahead with the learning step.
</li>
</ol>

<p class="normal">At this point, we need to add the nodes (both learner and predictor) that implement the specific machine learning algorithm we want to use. The simplest algorithm for predicting numbers is <strong class="keyword">linear regression, </strong>which is what we are going to use in this tutorial. It's worth introducing first the underlying mathematical model so that we can get ready to interpret its results.</p>


<h2 id="_idParaDest-88" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor090"/>Linear regression algorithm </h2>
<p class="normal">The<a id="_idIndexMarker406"/> linear regression<a id="_idIndexMarker407"/> model is a generalization of the simple regression we have used to predict second-hand car prices in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>. In that case, we modeled the price as a straight line, following the simple equation:</p><figure class="mediaobject"><img src="image/B17125_05_001.png" style="height: 1.3em"/></figure><p class="normal">where <em class="italic">y</em> was the dependent variable, so the target of our prediction (the price of the car), <em class="italic">x</em>, was the only independent variable (in that case, the age of the car in years) and <img src="image/B17125_05_002.png" style="height: 1.0em"/> and <span class="mediaobject"><img src="image/B17125_05_003.png" style="height: 1.0em"/></span> were the parameters of the model, defining the <em class="italic">height</em> of the line (also known as the <em class="italic">offset</em> or <em class="italic">intercept</em>) and its <em class="italic">slope</em>, respectively:</p><figure class="mediaobject"><img src="image/B17125_05_07.png" alt=""/></figure><p class="packt_figref">Figure 5.7: Linear regression of car prices: the line shows the prediction as the age varies</p><p class="normal">Specifically, as <a id="_idIndexMarker408"/>you <a id="_idIndexMarker409"/>can see in <em class="italic">Figure 5.7</em>, we have <img src="image/B17125_05_004.png" style="height: 1.1em"/> (it's where the model line encounters the vertical axis) and <img src="image/B17125_05_005.png" style="height: 1.1em"/> so the price of the car is predicted through the simple model:</p><figure class="mediaobject"><img src="image/B17125_05_006.png" style="height: 1.3em"/></figure><p class="normal">The price of a 2-year-old car will be estimated to be $12,600, since:</p><figure class="mediaobject"><img src="image/B17125_05_007.png" style="height: 1.3em"/></figure><p class="normal">The purpose of the <em class="italic">learner </em>algorithm of a simple linear regression is to find the right parameters (<img src="image/B17125_05_008.png" style="height: 1.0em"/> and <img src="image/B17125_05_009.png" style="height: 1.0em"/>) that minimize the error of a prediction, while the <em class="italic">predictor</em> algorithm will just apply the model on new numbers, like we did when we came up with the estimated price of a 2-year-old car.</p><p class="normal">Linear regression is a generalization of the simple model that we have just seen in action. Its <a id="_idIndexMarker410"/>underlying <a id="_idIndexMarker411"/>mathematical description is:</p><figure class="mediaobject"><img src="image/B17125_05_010.png" style="height: 1.3em"/></figure><p class="normal">where <em class="italic">y</em> is still the (single) target variable that we are trying to predict, the various <em class="italic">x</em><sub class="italic">i</sub> values represent the (many) independent variables that correspond to the features we have available, and the <img src="image/B17125_05_011.png" style="height: 0.8em"/> values are the parameters of the model that define its "shape." Since we have several independent variables this time (for this reason, we call it a <strong class="keyword">multivariate model</strong>), we <a id="_idIndexMarker412"/>cannot "visualize" it any longer with a simple line on a 2D chart. Still, its underlying mathematical model is quite simple because it assumes that every feature is "linearly" connected with the target variable. Here you go: you have just<a id="_idIndexMarker413"/> met the multivariate linear regression model.</p><p class="normal">If we apply this model to the prediction of the rental prices, our target variable is represented by the column <em class="italic">Rent</em> while the features (independent variables) are all the other columns, like <em class="italic">Rooms</em>, <em class="italic">Surface</em>, and so on. The multivariate linear regression model will look like:</p><figure class="mediaobject"><img src="image/B17125_05_012.png" style="height: 1.3em"/></figure><p class="normal">and the aim of the learner algorithm implementing this model will be to find the "best" values of <img src="image/B17125_05_013.png" style="height: 1.0em"/>, <span class="mediaobject"><img src="image/B17125_05_014.png" style="height: 1.0em"/></span>, <span class="mediaobject"><img src="image/B17125_05_015.png" style="height: 1.0em"/></span>, and so on that minimize the error produced on the training set.</p><div id="_idContainer182" class="note"><p class="Information-Box--PACKT-">There are ways to find analytically (meaning through a set of given formulas, nothing overly complex) the set of parameters <img src="image/B17125_05_016.png" style="height: 1.0em"/> that minimize the error of a linear regression model. The <a id="_idIndexMarker414"/>simplest one is called <strong class="keyword">Ordinary Least Squares (OLS)</strong>: it minimizes the sum of the squared errors of a linear regression. Do you remember the <strong class="keyword">Root Mean Squared Error</strong> (<strong class="keyword">RMSE</strong>) metric<a id="_idIndexMarker415"/> introduced in <em class="italic">Chapter 4</em>? By using the ordinary least squares procedure, we are going to minimize the RMSE, which is exactly what we need to do here.</p></div><p class="normal">The model <a id="_idIndexMarker416"/>above expects<a id="_idIndexMarker417"/> every independent variable to be a number. So, how do we deal with the nominal features we have in our dataset like <em class="italic">Floor_type</em>? We can solve this apparent limitation with a common trick used in machine learning: creating the<a id="_idIndexMarker418"/> so-called <strong class="keyword">dummy variables</strong>. The idea is very simple: we transform every nominal variable into multiple numerical variables. Let's take the example of <em class="italic">Floor_type</em>: this is a categorical variable whose value can be either <code class="Code-In-Text--PACKT-">Upper</code>, <code class="Code-In-Text--PACKT-">Mezzanine</code>, or <code class="Code-In-Text--PACKT-">Ground floor</code>. In this case we would replace this categorical variable by creating three numeric dummy variables: <em class="italic">Floor_type=Upper</em>, <em class="italic">Floor_type=Mezzanine</em>, and <em class="italic">Floor_type=Ground</em>. The dummy variables will take as values either <code class="Code-In-Text--PACKT-">1</code> or <code class="Code-In-Text--PACKT-">0</code>, depending on the category: for a given row, only one dummy variable will take <code class="Code-In-Text--PACKT-">1</code> and all others will take <code class="Code-In-Text--PACKT-">0</code>. For example, if a row refers to an <code class="Code-In-Text--PACKT-">Upper</code> floor property, the dummy variable <em class="italic">Floor_type=Upper </em>will be <code class="Code-In-Text--PACKT-">1</code> and the other two will be <code class="Code-In-Text--PACKT-">0</code>.</p><p class="normal">Thanks to this trick, we can apply a linear regression model on any categorical variables as well; we just need to "convert" them into multiple additional dummy variables.</p><p class="normal">We have all we need to give the linear regression model a try by introducing the KNIME node that implements its learning algorithm.</p>


<h2 id="_idParaDest-89" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor091"/><img src="image/image024.png" alt="A picture containing logo&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Linear Regression Learner</em></h2>
<p class="normal">This <a id="_idIndexMarker419"/>node (<strong class="screenText">Analytics &gt; Mining &gt; Linear/Polynomial Regression</strong>) trains<a id="_idIndexMarker420"/> a multivariate linear regression model for predicting a numeric quantity. For its configuration (see <em class="italic">Figure 5.8</em>) you will have to specify the numeric column to be predicted by picking it in the <strong class="screenText">Target </strong>drop-down menu at the top:</p><figure class="mediaobject"><img src="image/B17125_05_08.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.8: Configuration dialog of Linear Regression Learner: choose what to predict and the features to use</p><p class="normal">Then, in the central box, you can select which columns should be used as features: only the columns that appear on the green box on the right will be considered as independent variables in the model. The nominal columns, such as strings, will be automatically converted by the node into dummy variables.</p><div id="_idContainer185" class="note"><p class="Information-Box--PACKT-">If a nominal column (like <em class="italic">Type</em>) admits <em class="italic">N</em> unique values (like <code class="Code-In-Text--PACKT-">A</code>, <code class="Code-In-Text--PACKT-">B</code>, and <code class="Code-In-Text--PACKT-">C</code>), this node will actually create not <em class="italic">N</em>, but <em class="italic">N-1</em> dummy variables (<em class="italic">Type=A</em> and <em class="italic">Type=B</em>). In fact, one of the nominal values can be covered by the combination of all zeros: in our case, if <em class="italic">Type</em> is <code class="Code-In-Text--PACKT-">C</code>, both <em class="italic">Type=A</em> and <em class="italic">Type=B</em> will be zero, implying that the only possible value for that row is <code class="Code-In-Text--PACKT-">C</code>. In this way, we make the model simpler and avoid the <a id="_idIndexMarker421"/>so-called dummy variable trap, which might make our model parameters impossible to calculate. The node takes care of this automatically, so you don't have to worry about it: just keep this in mind when reading the model parameters related to dummy variables.</p></div><p class="normal">By clicking on the <strong class="screenText">Predefined Offset Value</strong> tick box, you can "force" the offset value of the linear <a id="_idIndexMarker422"/>regression model (we<a id="_idIndexMarker423"/> also called it <img src="image/B17125_05_017.png" style="height: 1.0em"/> or intercept earlier) to a certain value or remove it, by setting it to zero. This reduces the "flexibility" of the model to minimize the error so it will reduce its accuracy. However, this trick might be helpful when you are trying to reduce the complexity of the model and improve its explain ability, as we have one less parameter to interpret. By default, this node will fail if there are some missing values in the input data. To manage this, you can either manage them earlier in the workflow, using<a id="_idIndexMarker424"/> the <strong class="keyword">Missing Value</strong> node, or select the <strong class="screenText">Ignore rows with missing value</strong> option at the bottom-left corner of the configuration dialog.</p><p class="normal">Once executed, the node will return at its first output port the regression model, which can then be used by a predictor node for making predictions. The second output is a table (<em class="italic">Figure 5.9</em>) that contains a summary view of the regression model parameters, where for each variable (including the dummy ones) you can find:</p><ul><li class="bullet"><strong class="screenText">Coeff.</strong>: this is the <strong class="keyword">parameter</strong> (also called coefficient) of the variable. This is the <span class="mediaobject"><img src="image/B17125_05_018.png" style="height: 1.0em"/></span> parameter we have seen in the regression model formula.</li><li class="bullet"><strong class="screenText">Std. Err.</strong>: this is <strong class="keyword">the standard deviation of the error</strong> expected for this parameter. If you compare it with the value of the parameter, you get a rough idea of how "precise" the estimation of that parameter can be. You can use it also to get a rough confidence interval for the given parameter as we did in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>, when talking about RMSE. In the case of the car price regression, if the parameter for the variable <em class="italic">Age </em>is -1.7 and the standard error is 0.1, you can say that 95% of the time, the price of a car declines by $M 1.7 ± 0.2 (2 times the standard error) every year.</li><li class="bullet"><strong class="screenText">t-value</strong> and <strong class="screenText">P&gt;|t|</strong>: these are two summary statistics (<strong class="keyword">t-value</strong> and <strong class="keyword">p-value</strong>) generated by the<a id="_idIndexMarker425"/> application <a id="_idIndexMarker426"/>of the Student test, which clarifies how significant a variable is for the model. The smaller the p-value, the more confident you can be in rejecting the possibility that that parameter looks significant just "by chance" (it's<a id="_idIndexMarker427"/> called <strong class="keyword">null hypothesis</strong>). As a general rule of thumb, when the p-value (the last column in this table) is above 0.05, you should remove that variable <a id="_idIndexMarker428"/>from <a id="_idIndexMarker429"/>the model, as it is likely insignificant:<figure class="mediaobject"><img src="image/B17125_05_09.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.9: The summary output of the Linear Regression Learner node: find out what the parameters of the regression are and if they turn out significant or not</p></li></ul>
			<p class="normal">If you right-click on the node after it is executed, you can open an additional graphical view (select <strong class="screenText">View: Linear Regression Scatterplot View</strong>) where you can visually compare the individual features against the target to look for steep slopes and other patterns.</p>
			<p class="normal">Let's now put this node to work with our Rome properties and see what it's got:</p>
			<ol>
				<li class="numbered" value="4">Implement the <strong class="keyword">Linear Regression Learner</strong> node and connect it with the upper output of the <strong class="keyword">Partitioning</strong> node, which is the training set (a 70% random sample of the historical database of rents). In the configuration window, double-check that <em class="italic">Rent</em> is set as the <strong class="screenText">Target</strong> variable on top. Feature-wise, at this point, we can keep all of them to see if they are significant or not. However, we can already remove one, <em class="italic">House_ID</em>, as we already know we don't want it to be used. We don't want to make use of the unique identifier of the property to infer the rental price. That number has been assigned artificially when the property was added to <a id="_idIndexMarker430"/>the<a id="_idIndexMarker431"/> database, and it is not connected with features of the property itself, so we don't want to consider it in a predictive model. Run the model and open the second output port to obtain the summary view of the model parameters.
</li>
</ol>

<p class="normal">This summary view will look similar to what is displayed in <em class="italic">Figure 5.9</em>, although the numbers could differ given that the random partitioning might have generated in your case different partitions: welcome to the world of probabilistic models! However, we can already notice that some parameters display a p-value (the last column of the table, <strong class="screenText">P&gt;|t|</strong>) higher than 0.05. This means we can come back to this step later and do some cleaning and improve the performance of the model. For now, let's proceed further so that we can make some predictions and score the model.</p>


<h2 id="_idParaDest-90" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor092"/><img src="image/image028.png" alt="Icon&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Regression Predictor</em></h2>
<p class="normal">This <a id="_idIndexMarker432"/>node (<strong class="screenText">Analytics &gt; Mining &gt; Linear/Polynomial Regression</strong>) applies a regression model (given <a id="_idIndexMarker433"/>as an input in the first blue port on the left) to a dataset (second port) and returns the result of the prediction for each input row. The node does not require any configuration and can be used in conjunction with either the <strong class="keyword">Linear Regression Learner</strong> node, introduced above, or the <strong class="keyword">Polynomial Regression</strong> <strong class="keyword">Learner</strong> node: you can check this one out by yourself if you want to build linear regressions on different polynomial degrees as we did in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em> (have a look at <em class="italic">Figure 4.9</em>).</p>
 
<ol>
				<li class="numbered" value="5">Let's add the <strong class="keyword">Regression Predictor</strong> node to the workflow and make the connections: link the blue square output of the <strong class="keyword">Linear Regression Learner</strong> to the upper input port of the predictor and connect the bottom output port of the <strong class="keyword">Partitioning</strong> (the test set) to the second input port. No configuration is needed so you can execute the node and look at the output, which is similar to what you find in <em class="italic">Figure 5.10</em>:<figure class="mediaobject"><img src="image/B17125_05_10.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.10: Output of the Regression Predictor node: we finally have a prediction of the rental price.</p>
</li>
</ol>

<p class="normal">You can <a id="_idIndexMarker434"/>look<a id="_idIndexMarker435"/> with pride at the last column on the right, called <em class="italic">Prediction (Rent)</em>: for each row in the test set (which has not been "seen" by the learner node) the node has generated a prediction of the rent. This prediction was obtained by just "applying" the parameters of the regression model to the values of the rows in the test set. Let's see how this works with an example: consider the parameters in <em class="italic">Figure 5.9</em>. In this case the intercept (last row) is 569.9, the parameter of <em class="italic">Rooms</em> is around 25.8, the one for <em class="italic">Surface</em> is 9.6, the parameter of the dummy variable associated with the <code class="Code-In-Text--PACKT-">Collatino</code> neighborhood (<em class="italic">Neighborhood=Collatino</em>) is -561.9, and so on. When the predictor had to come up with a prediction for the first row in the test set (see the first line in <em class="italic">Figure 5.10</em>), it had to just apply the formula of the regression model, with the parameters found by the learner, to this property (with 3 rooms, 80 square meters, based in <code class="Code-In-Text--PACKT-">Collatino</code>, and so on). Hence, the resulting calculation for the <strong class="keyword">Regression Predictor</strong> node is:</p><figure class="mediaobject"><img src="image/B17125_05_019.png" style="height: 1.2em"/></figure><p class="normal">In this specific case, if you add all the other features that were not reported in the preceding formula, we come up with a final prediction of €896.4, making around €50 of error versus the actual rental price, which we know is €950: not bad for our first prediction! To have a complete view of the performance of the current model, we would need to check the <a id="_idIndexMarker436"/>difference between <a id="_idIndexMarker437"/>predicted and real rents for all rows in the test set, using the <strong class="keyword">Numeric Scorer</strong> node.</p>


<h2 id="_idParaDest-91" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor093"/><img src="image/image032.png" alt="A picture containing text&#10;&#10;Description automatically generated"/></span><em class="bold-italic"> Numeric Scorer</em></h2><p class="normal">This node (<strong class="screenText">Analytics &gt; Mining &gt; Scoring</strong>) calculates the summary performance metrics of a <a id="_idIndexMarker438"/>regression <a id="_idIndexMarker439"/>by comparing two numeric columns. Its only required configuration (<em class="italic">Figure 5.11</em>) is the selection of the two columns to be compared: you can select the target column of the regression, containing the actual values, in the <strong class="screenText">Reference column </strong>dropdown, and the predictions in the next one, labeled as <strong class="screenText">Predicted column</strong>. If you want to output the performance scores as variables as well (this is useful when doing hyperparameter optimization), you need to tick the <strong class="screenText">Output scores as flow variables</strong> box at the bottom. The node outputs the most popular scoring metrics of a regression, including the <strong class="keyword">Coefficient of Determination</strong>, <strong class="keyword">R</strong><sup class="bold">2</sup>, and the <strong class="keyword">RMSE</strong>:</p><figure class="mediaobject"><img src="image/B17125_05_11.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.11: Configuration dialog of the Numeric Scorer node: select the columns to compare. </p>
<ol>
				<li class="numbered" value="6">Implement<a id="_idIndexMarker440"/> the <strong class="keyword">Numeric Scorer</strong> node (watch<a id="_idIndexMarker441"/> out: don't get confused with the <strong class="keyword">Scorer</strong> node, which is used for classifications) and connect the output of the <strong class="keyword">Regression Predictor</strong> with its input port. For its configuration, just double-check that you have <em class="italic">Rent</em> and <em class="italic">Prediction (Rent)</em> in the drop-down menus at the top and run the node. Its output (<em class="italic">Figure 5.12</em>) is very encouraging (of course, you can get slightly different results from what you find in these figures and that's normal):<figure class="mediaobject"><img src="image/B17125_05_12.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.12: Performance metrics as returned by the Numeric Scorer node:not bad for your first regression</p>
</li>
</ol>

<p class="normal">We obtained<a id="_idIndexMarker442"/> an R<sup class="Superscript--PACKT-">2</sup> of 0.92, which means<a id="_idIndexMarker443"/> that our current model accounts for around 92% of the full variability of rental prices in Rome. Considering the limited sample and the few features available, this looks quite good already. Also, the RMSE is €110, which means that 68% of the time (one standard deviation) we will make a prediction error that is, in absolute terms, below €110, and 95% of the time our error will be below €220 (two times<a id="_idIndexMarker444"/> the RMSE). The last performance metric, <strong class="keyword">Mean Absolute Percentage Error</strong> (<strong class="keyword">MAPE</strong>) tells us that, on average, our predicted rent will differ from the actual rent by around 10%: again, not bad at all.</p><p class="normal">Still, we strive for the best and question ourselves if we can do anything to improve the model. The simplest thing to do will be to consider whether we can improve the selection of features. Let's go back and have a look at the parameters obtained by the regression (<em class="italic">Figure 5.9</em>) and if we can remove some unneeded (or damaging) features. When we remove excess features from a model, we obtain at least two advantages: first, we make the model simpler and more explanatory to other human beings, as we have fewer parameters to explain. Secondly, we reduce the possibility for the model to overfit on the training set and, so, we increase its general robustness.</p><div id="_idContainer195" class="note"><p class="Information-Box--PACKT-">Another reason for removing features is to avoid the risk of <strong class="keyword">multicollinearity</strong>, which happens when features are correlated with each other. Correlated features are redundant: they can produce degradation of the predictive performance of your <a id="_idIndexMarker445"/>model and should be removed. The <strong class="keyword">Linear Correlation</strong> node can help you calculate the correlation across all pairs of numeric columns in a table. As an alternative, you can use the <strong class="keyword">Variance Inflation Filter</strong> (<strong class="keyword">VIF</strong>) component, available in the KNIME Hub: as a rule of thumb, all <a id="_idIndexMarker446"/>variables showing a VIF higher than 5 should be removed.</p></div><p class="normal">Let's have a look at the p-values (last column of the table) and see if we can unveil some opportunities. Remember, the higher they are, the less meaningful their associated features proved to be. For sure we notice that the feature <em class="italic">Elevator</em> should be removed: its p-value is way above the thumb-rule threshold of 0.05 so we can go ahead and remove it. Also, the variable <em class="italic">Property_type</em> shall be removed: the p-values of their dummy variables are high, with the exception of <em class="italic">Property_type=Penthouse</em> (indicating that <code class="Code-In-Text--PACKT-">Penthouse</code> is the only type that seems to be significant in affecting the value of the rent). Still, considering <a id="_idIndexMarker447"/>how few penthouses<a id="_idIndexMarker448"/> we have in the dataset, it's worth removing this feature and further simplifying the model. Let's give this simplification a try and see what happens:</p>
<ol>
				<li class="numbered">Open the configuration dialog of the <strong class="keyword">Linear Regression Learner</strong> node and move <em class="italic">Elevator</em> and <em class="italic">Property_type</em> to the left box of the column selector, so as to remove them as features of the model.</li>
				<li class="numbered">Now let's run the full model and see if something changed. To do so, it will be enough to execute the <strong class="keyword">Numeric Scorer</strong> node: all previous nodes will be forced to run as well.<p class="normal">By removing these two features (see the updated results in <em class="italic">Figure 5.13</em>), we managed to keep the same performance levels, proving that they were unneeded. Actually, the performance has marginally increased (notice the lower RMSE), probably showing that we were slightly overfitting because of these uninformative variables. Additionally, we simplified the model, making it simpler to explain. Now we can predict the rental price of a property in Rome by knowing only the neighborhood, the number of rooms, the surface, and its floor (number and type):</p><figure class="mediaobject"><img src="image/B17125_05_13.png" alt="Graphical user interface, table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.13: Updated parameters and performance scores after the removal of two features: every little helps</p>
</li>
</ol>

<p class="normal">These<a id="_idIndexMarker449"/> last two steps have shown us<a id="_idIndexMarker450"/> the value of selecting features wisely. As anticipated in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>, feature selection is an important practice in machine learning, indeed.</p><div id="_idContainer197" class="packt_tip"><p class="Tip--PACKT-">In this case, we applied feature selection "by hand," checking the parameters manually and selecting the least meaningful ones. There are more systemic and semi-automated techniques to find out the best subset of features to use in a machine learning model. If you are curious, check the KNIME nodes for <strong class="keyword">Feature Selection</strong> loops and have a look at the sample workflow available on the KNIME Hub called <strong class="screenText">Performing a Forward Feature Selection</strong>.</p></div><p class="normal">Before concluding, we need to do one last thing: it's time to apply our model to the 10 incoming properties for which the rental price is not available yet. This will be a way to illustrate our findings to the owner of the company. It will also be an opportunity for us to understand how predictive models are used in real life after they are built. In fact, once models are constructed (and validated against overfitting, as we did through the partitioning in training and test sets, and so on) they are <strong class="keyword">operationalized</strong> in a way that they can be applied to future samples (in this case, the 10 new properties) whenever a prediction is needed. Let's see this in action with our properties:</p>
<ol>
				<li class="numbered" value="9">Load the Excel file with the new properties <em class="italic">(</em><code class="Code-In-Text--PACKT-">RomeHousing-NewProperties.xlsx</code><em class="italic">) by dragging and dropping it into your workflow or implementing an </em><strong class="keyword">Excel Reader</strong><em class="italic"> node</em>. Once executed, you will find a short table that has exactly the same columns as the historical database, but—of course—lacks the <em class="italic">Rent</em> value.</li>
				<li class="numbered">Implement a new <strong class="keyword">Regression Predictor</strong> node (or copy/paste the existing one) and connect it as displayed in <em class="italic">Figure 5.14</em>. You should link the output of the <strong class="keyword">Linear Regression Learner</strong> (yes—we are going to reuse the model we learned earlier) to <a id="_idIndexMarker451"/>the first input of<a id="_idIndexMarker452"/> the predictor. Then connect the <strong class="keyword">Excel Reader</strong> output (the 10 new properties) to the second input of the predictor. You can now execute the node and have a look at the output:<figure class="mediaobject"><img src="image/B17125_05_14.png" alt=""/></figure></li>
			</ol>
			<p class="packt_figref">Figure 5.14: Full workflow for the Rome rent prediction</p>
			<p class="normal">At this point, you have all you need to go back to the owner of the real estate with your output table (which will have a similar format to what you find in <em class="italic">Figure 5.15</em>) and wait impatiently for her reaction, which turns out to be very positive! She loves it, as she finds that the estimates make, at least at a first glance, a lot of sense:</p>
			<figure class="mediaobject"><img src="image/B17125_05_15.png" alt="Table&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.15: The predicted rental prices on the new properties: do you fancy a 145 square meter flat near Piazza Navona at this price?</p>
			<p class="normal">The<a id="_idIndexMarker453"/> understandable initial stress <a id="_idIndexMarker454"/>turns quickly to a broad sense of enthusiasm. The model you created responds to the initial business objectives. In fact: </p>
			<ul>
				<li class="bullet">The interpretation of the parameters of the model tells us something quite useful about the price formation mechanisms. For instance, you have found that the presence of the elevators and the type of flat doesn't count as much as the surface, the number of rooms, the floors, and, very importantly, the neighborhood to which the property belongs. By looking at the parameters of the neighborhood dummy variables (<em class="italic">Figure 5.13</em>), you find out what additional value each neighborhood brings (of course to be added to the rest of the components of your regression). For instance, Piazza Navona is by far the most expensive area while Castelli Romani seems to offer (at parity of characteristics) the most accessible rent.</li>
				<li class="bullet">On top of this, you now have a simple approach to quickly generate a recommendation of what fair rent looks like, which could be the basis for the discussion with the prospective landlord when fixing the rental price. By having a data-based number to start from, the agents can aim at a smoother negotiation session, which will more likely end up with a quicker and more profitable matching of demand and offer in the housing market.</li>
			</ul>
			<p class="normal">Congratulations on completing your first regression model! It's now time to move on and challenge ourselves with a different undertaking: anticipating consumers' behavior.</p>
			<h1 id="_idParaDest-92" class="title"><a id="_idTextAnchor094"/>Anticipating preferences with classification</h1>
			<p class="normal">In this<a id="_idIndexMarker455"/> tutorial, you will step into the role of a marketing analyst working for a mid-sized national consumer bank, offering services such as accounts, personal loans, and mortgages to around 300,000 customers in the country. The bank is currently trying to launch a new type of low-cost savings account, providing essential services and a pre-paid card that can be fully managed online. The product manager of this new account is not very pleased with how things are going and invites you to join a review meeting. You can see he is tense as he presents the outcome of a pilot telemarketing campaign run to support the launch. As part of this pilot, 10,000 people were randomly selected among the full bank customer base and were phoned by an outbound call center. The outcome was apparently not so bad: 1,870 of the contacted customers (19% of the total) signed up for a new account. However, the <a id="_idIndexMarker456"/>calculation of the <strong class="keyword">Return On Investment</strong> (<strong class="keyword">ROI</strong>) pulled the entire audience back to the unsettling reality. The average cost of attempting to contact a customer through a call center is $15 per person while the incremental revenue resulting from a confirmed sale is estimated to be, on average, $60. The math is simple: the pilot telemarketing campaign cost $150,000 and generated revenues amounting only to $112,200, implying a net loss of $37,800. Now it is clear why the product manager looked disappointed: repeating the same campaign on more customers would be financially devastating.</p>
			<p class="normal">You timidly raise your hand and ask whether the outcomes of the pilot calls could be used to rethink the campaign target and improve the ROI of the marketing efforts. You explain that some machine learning algorithms might be able to predict whether a customer is willing or not to buy a product by learning from previous examples. As it normally happens in these cases, you instantly earn the opportunity to try what you suggested, and your manager asks you to put together a proposal on an ML way to support the launch of the new savings account.</p>
			<p class="normal">You have mixed feelings about what just happened: on one hand, you are wondering whether you were a bit too quick in sharing the idea. On the other hand, you are very excited as you get to try leveraging algorithms to impact the business on such an important case. You are impatient to start and ask for all the available information related to the customers that were involved in the pilot. The file you receive (<code class="Code-In-Text--PACKT-">BankTelemarketing.csv</code>) contains the following columns:</p>
			<ul>
				<li class="bullet"><em class="italic">Age</em>: the age of the customer.</li>
				<li class="bullet"><em class="italic">Job</em>: a string describing the job family of the customer, like <code class="Code-In-Text--PACKT-">blue-collar</code>, <code class="Code-In-Text--PACKT-">management</code>, <code class="Code-In-Text--PACKT-">student</code>, <code class="Code-In-Text--PACKT-">unemployed</code>, and <code class="Code-In-Text--PACKT-">retired</code>.</li>
				<li class="bullet"><em class="italic">Marital</em>: the marital status, which could be <code class="Code-In-Text--PACKT-">married</code>, <code class="Code-In-Text--PACKT-">single</code>, <code class="Code-In-Text--PACKT-">divorced</code>, or <code class="Code-In-Text--PACKT-">unknown</code>.</li>
				<li class="bullet"><em class="italic">Education</em>: the highest education level reached to date by the customer, ranging from <code class="Code-In-Text--PACKT-">illiterate</code> and <code class="Code-In-Text--PACKT-">basic.4y</code> (4 years of basic education in total) to <code class="Code-In-Text--PACKT-">university.degree</code>.</li>
				<li class="bullet"><em class="italic">Default</em>: this tells us whether we know that the customer has defaulted due to extended payment delinquency or not. Only a few customers end up being marked as defaulted (<code class="Code-In-Text--PACKT-">yes</code>): most of them either show a good rating history (<code class="Code-In-Text--PACKT-">no</code>) or do not have enough history to be assigned in a category (<code class="Code-In-Text--PACKT-">unknown</code>).</li>
				<li class="bullet"><em class="italic">Mortgage</em> and<em class="italic"> Loan</em>: tells us whether the user has ever requested a housing mortgage or a personal loan, respectively.</li>
				<li class="bullet"><em class="italic">Contact</em>: indicates if the telephone number provided as a preferred contact method is a <code class="Code-In-Text--PACKT-">landline</code> or a <code class="Code-In-Text--PACKT-">mobile</code> <code class="Code-In-Text--PACKT-">phone</code>.</li>
				<li class="bullet"><em class="italic">Outcome</em>: a<a id="_idIndexMarker457"/> string recording the result of the call center contact during the pilot campaign. It can be <code class="Code-In-Text--PACKT-">yes</code> or <code class="Code-In-Text--PACKT-">no</code>, depending on whether the customer opened the new savings account or decided to decline the offer.</li>
			</ul>
			<p class="normal">Before you get cracking, you have a chat with the product manager to get clear on what would be the most valuable outputs for the business given the situation:</p>
			<ul>
				<li class="bullet">First of all, it would be very useful to understand and document what characteristics make a customer most likely to buy the new banking product. Given its novelty, it is not clear yet who will find its proposition particularly appealing. Having some more clues on this aspect can help to build more tailored campaigns, personalize their content, and—by doing so—transfer the learnings from the call center pilot to other types of media touchpoints.</li>
				<li class="bullet">Given that the pilot covered only a relatively small subset of customers—around 3% of the total—it would be useful to identify "who else" to call within the other 97% to maximize the ROI of the marketing initiative. In fact, we can assume that the same features we found in our pilot dataset—such as age, job, marital status, and so on—are available for the entire customer database. If we were able to <em class="italic">score</em> the remaining customers in terms of their <em class="italic">propensity</em> to buy the product, we would be focusing our efforts on the most inclined ones and greatly improving the campaign's effectiveness. In other words, we should create a <strong class="keyword">propensity model</strong> that will score current (and future) customers to enable a better marketing targeting. We will use the propensity scores to "limit" the next marketing efforts to a selected subset of the total customer base where the percentage of people in the new product is higher than 19% (as it was in our pilot): by doing so, we would increase the ROI of our marketing efforts.</li>
			</ul>
			<p class="normal">From a machine learning standpoint, you need to create a machine able to predict whether a consumer will buy or will not open a savings account before you make the call. This is still a clear case of supervised learning, since you aim at predicting something based on previous examples (the pilot calls). In contrast with the Rome real estate case, where we had to predict a number (the rental price) using <em class="italic">regression</em> algorithms, here we need to predict the value of the categorical column <em class="italic">Outcome</em>. We will then need to implement <em class="italic">classification</em> algorithms, such as decision trees and random forest, which we are going to meet shortly. We are clear on the business need, the available data, and the type of machine learning route we want to take: we have all we need to start getting serious<a id="_idIndexMarker458"/> about this challenge. After creating a new workflow in KNIME, we load the data into it:</p>
			<ol>
				<li class="numbered" value="1">Drag and drop the file <code class="Code-In-Text--PACKT-">BankTelemarketing.csv</code> onto the blank workflow. After the <strong class="keyword">CSV Reader</strong> node <a id="_idIndexMarker459"/>dialog appears, we can quickly check that all is in order and close the window by clicking on <strong class="screenText">OK</strong>. Once executed, the output of the node (<em class="italic">Figure 5.16</em>) confirms that our dataset is ready to go:<figure class="mediaobject"><img src="image/B17125_05_16.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.16: The pilot campaign data: 10,000 customers through 8 features and for which we know the outcome of their call center contact</p></li>
				<li class="numbered">As usual, we implement the node <strong class="keyword">Statistics</strong>, to explore the characteristics of our dataset. After confirming its default configuration, we check the <strong class="screenText">Top/bottom</strong> tab of its main view (press <em class="italic">F10</em> or right-click and select <strong class="screenText">View: Statistics View</strong> to open it). It seems that there are no missing values and that all seems to be in line with what we knew about the pilot campaign: the <em class="italic">Outcome</em> column shows 1,870 rows with <code class="Code-In-Text--PACKT-">yes</code>, which is what the product manager managed in his presentation. We also notice that the <em class="italic">Default</em> column has only one row referring to a defaulted customer. This column might still be useful as it differentiates between customers who never defaulted and ones we don't have any certainty <a id="_idIndexMarker460"/>about, so we decide to keep it and move on:<figure class="mediaobject"><img src="image/B17125_05_17.png" alt="Text&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.17: The Top/bottom output of the Statistics node: only one person in this sample defaulted—good for everyone!</p></li>
				<li class="numbered">Since we are in the supervised learning scenario, we need to implement the usual partitioning/learn/predict/score structure in order to validate against the risk of overfitting. We start by adding the <strong class="keyword">Partitioning</strong> node and connecting it downstream to the <strong class="keyword">CSV Reader</strong> node. In its configuration dialog, we leave the <strong class="screenText">Relative</strong> 70% size for the training partition and we decide to protect the distribution of the target variable <em class="italic">Outcome </em>in both partitions, selecting the <strong class="screenText">Stratified sampling</strong> option. Additionally, we put a static number in the random seed box (you can put <code class="Code-In-Text--PACKT-">12345</code> as you see in <em class="italic">Figure 5.18</em>) and tick the adjacent checkbox:<div id="_idContainer202" class="packt_tip"><p class="Tip--PACKT-">As a general rule, always perform a stratified sampling on the target variable of a classification. This will reduce the impact of imbalanced classes when learning and validating your model. There are other ways to restore a balance in the distribution of classes, such as under-sampling the majority class or over-sampling the minority one. One interesting approach is the creation of synthetic (and realistic) additional samples using algorithms <a id="_idIndexMarker461"/>like the <strong class="keyword">Synthetic Minority Over-sampling Technique</strong>: check out the <strong class="keyword">SMOTE</strong> node <a id="_idIndexMarker462"/>to learn more.</p></div><figure class="mediaobject"><img src="image/B17125_05_18.png" alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.18: Performing a stratified sampling using the Partitioning node: this way, we ensure a fair presence of yes and no customers in each partition</p>
</li>
</ol>


<p class="normal">Now that we have a training and test dataset readily available, we can proceed with implementing our first classification algorithm: <strong class="keyword">decision trees</strong>. Let's get a hint of how it works.</p><h2 id="_idParaDest-93" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor095"/>Decision tree algorithm</h2><p class="normal">Decision<a id="_idIndexMarker463"/> trees <a id="_idIndexMarker464"/>are simple models that describe a decision-making process. Have a look at the tree shown in <em class="italic">Figure 5.19</em> to get an idea of how they work. Their hierarchical structure resembles an upside-down tree. The root on top corresponds to the first question: according to the possible answers, there is a split between two or more subsequent <em class="italic">branches</em>. Every branch can either lead to additional questions (and respective splits into more branches) or terminate in <em class="italic">leaves</em>, indicating the outcome of the decision:</p><figure class="mediaobject"><img src="image/B17125_05_19.png" alt=""/></figure><p class="packt_figref">Figure 5.19: How will you go to work tomorrow? A decision tree can help you make up your mind</p><p class="normal">Decision trees can be used to describe the process that assigns an entity to a class: in this case, we call it a <strong class="keyword">classification tree</strong>. Think about a table where each entity (corresponding to a row) is described by multiple features (columns) and is assigned to one specific class, among different alternatives. For example, a classification tree that assigns consumers to multiple classes will answer the question <em class="italic">to which class does the consumer belong?</em>: every branching will correspond to different outcomes of a test on the features (like <em class="italic">is the age of the consumer higher than 35?</em> or <em class="italic">is the person married?</em>) while each terminal leaf will be one of the possible classes. Once you have defined the decision tree, you can apply it to all consumers (current and future). For every consumer in the table, you follow the decision tree: the features of the consumer will dictate which specific path to follow and result in a single leaf to be assigned as the class of the consumer.</p><p class="normal">There are many tree-based learning algorithms available for classification. They are able to "draw" trees<a id="_idIndexMarker465"/> by <a id="_idIndexMarker466"/>learning from labeled examples. These algorithms can find out the right splits and paths that end up with a decision model able to predict classes of new, unlabeled entities. The simplest version of a decision tree learning algorithm will proceed by iteration, starting from the root of the tree and checking what the "best possible" next split to make is so as to differentiate classes in the least ambiguous way. This concept will become clear by means of a practical example. Let's imagine that we want to build a decision tree in order to predict which drink fast-food customers are going to order (among soda, wine, or beer), based on the food menu they had (the delicious alternatives are pizza, burger, or salad) and the composition of the table (whether it is among kids, couples, or groups of adults). The dataset to learn from will look like the one shown in <em class="italic">Figure 5.20</em>: we have 36 rows, each referring to a previous customer, and three columns, one for each feature (<em class="italic">Menu</em> and <em class="italic">Type</em>) and the target class (the <em class="italic">Favorite drink</em>).</p><figure class="mediaobject"><img src="image/B17125_05_20.png" alt="A picture containing text, electronics&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.20: Drink preferences for 36 fast-food customers: can you predict their preferences based on their food menu and type?</p><p class="normal">Since we<a id="_idIndexMarker467"/> have <a id="_idIndexMarker468"/>only two features, the resulting decision tree can only have two levels, resulting in two alternative shapes: either the first split is by <em class="italic">Menu</em> and the second, at the level below, by <em class="italic">Type</em>, or the other way around. The learning algorithm will pick the split that makes the most sense by looking at the count of the items falling into each branch and checking which splits make the "clearest cut" among classes. </p><p class="normal">In this specific case, the alternative choices for the first split are the ones drawn in <em class="italic">Figure 5.21</em>: you can find the number of customers falling into each branch, separated by alternative class (beer, soda, or wine). Have a look at the number and ask yourself: between the <em class="italic">Menu</em> split on the left and the <em class="italic">Type</em> split on the right, which one is differentiating in the "purest" way among the three classes?</p><figure class="mediaobject"><img src="image/B17125_05_21.png" alt="A picture containing diagram&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.21: Which of these two alternative splits gives you the most help in anticipating the choice of drinks?</p><p class="normal">In this case, it<a id="_idIndexMarker469"/> seems <a id="_idIndexMarker470"/>that the <em class="italic">Type</em> split on the right is a no-brainer: kids are consistently going for sodas (with the exception of 2 customers who—hopefully—got served with alcohol-free beer), groups prefer beers, while couples go mainly with wine. The other alternative (split by <em class="italic">Menu</em>) is messier: for those having salad and, to some extent, burger, there is no such clear cut drinks choice. Our preference for the option on the right is guided by human intuition: for an algorithm, we need to have a more deterministic way to make a decision. Tree learning algorithms use, in fact, metrics to decide which splits are best to pick. One of these metrics is<a id="_idIndexMarker471"/> called <a id="_idIndexMarker472"/>the <strong class="keyword">Gini index</strong>, or <strong class="keyword">Impurity index</strong>. Its formula is quite simple:</p><figure class="mediaobject"><img src="image/B17125_05_020.png" style="height: 4.0em"/></figure><p class="normal">where <em class="italic">f</em><sub class="italic">i</sub> is the relative frequency of <em class="italic">i-n</em><sup class="Superscript--PACKT-">th</sup> class (it's in the <em class="italic">%</em> column in <em class="italic">Figure 5.21</em>), among the <em class="italic">M</em> possible classes. The algorithm will calculate the <em class="italic">I</em><sub class="italic">G</sub> for each possible branching of a split and average the results out. The option with the lowest Gini index (meaning, with the least "impure" cut) will win among the others. In our fast-food case, the overall Gini index for the option on the left will be the average of:</p><figure class="mediaobject"><img src="image/B17125_05_021.png" style="height: 1.2em"/></figure><figure class="mediaobject"><img src="image/B17125_05_022.png" style="height: 1.2em"/></figure><figure class="mediaobject"><img src="image/B17125_05_023.png" style="height: 1.2em"/></figure><p class="normal">By averaging them out, we find that the Gini index for the left option is 0.60 while the one for the right option is 0.38. These metrics are confirming our intuition: the option on the right (the split by <em class="italic">Type</em>) is "purer" as demonstrated by the lower Gini index. Now you have all the elements to see how the decision tree learning algorithm works: it will iteratively<a id="_idIndexMarker473"/> calculate<a id="_idIndexMarker474"/> the average <em class="italic">I</em><sub class="italic">G</sub> for all possible splits (at least one for each available feature), pick the one with the lowest index, and repeat the same at the levels below, for all possible branches, until it is not possible to split further. In the end, the leaves are assigned by just looking at where the majority of the known examples fall. For instance, take the branching on the right in <em class="italic">Figure 5.21</em>: if this was the last level of a tree, kids will be classified with soda, couples with wine, and groups with beer. You can see in <em class="italic">Figure 5.22</em> the resulting full decision tree you would obtain by using the fast-food data we presented above:</p><figure class="mediaobject"><img src="image/B17125_05_22.png" alt=""/></figure><p class="packt_figref">Figure 5.22: Decision tree for classifying fast-food customers according to their favorite drink.In which path would you normally be?</p><p class="normal">By looking at the obtained decision tree, you will notice that not all branches at the top level incur further splits at the level below. Take the example of the <em class="italic">Type</em>=<code class="Code-In-Text--PACKT-">Kids</code> branch on the top left: the vast majority of kids (10 out of 12) go for <code class="Code-In-Text--PACKT-">Soda</code>. There are not enough remaining examples to make a meaningful further split by <em class="italic">Menu</em>, so the tree just stops there. On top of this basic stopping criterion, you can implement additional (and more stringent) conditions that limit the growth of the tree by removing less meaningful branches: these <a id="_idIndexMarker475"/>are <a id="_idIndexMarker476"/>called—quite appropriately, I must say—<strong class="keyword">pruning mechanisms</strong>. By <a id="_idIndexMarker477"/>pruning a decision tree, you end up with a less complex model: this is very handy to use when you want to avoid model overfitting. Think about this: if you have many features and examples, your tree can grow massively. </p><p class="normal">Every combination of values might, in theory, produce a very specific path. Chances are that these small branches cover an insignificant case that just happened to be in the training set but has no general value: this is a typical case of overfitting that we want to avoid as much as possible. That is why, as you will soon see in KNIME, you might need to activate some of the pruning mechanisms to avoid overfitting when growing trees.</p><p class="normal">Let's make another consideration related to numeric features in decision trees. In the fast-food example, we only had nominal features, which make every split quite simple to imagine: every underlying branch covered a possible value of the categorical column. If you have a numeric column to be considered, the algorithm will check what the Gini index would be if you split your samples using a numeric threshold. The algorithm will try multiple thresholds and pick the best split that minimizes impurity. Let's imagine that in our example we had an additional feature, called <em class="italic">Size</em>, that counts the number of people sitting at each table. The algorithm will test multiple thresholds and will check what the Gini index would be if you divided your samples according to these conditions, which are questions like "is <em class="italic">Size</em> &gt; 3?", "is <em class="italic">Size</em> &gt; 5?", and "is <em class="italic">Size</em> &gt; 7?". If one of these conditions is meaningful, the split will be made according to the numeric variable: all samples having <em class="italic">Size</em> lower than the threshold will go to the left branch, and all others to the right branch. The Gini indices resulting from all the thresholds on the numeric features will be compared across all other indices coming from the categorical variables as we saw earlier: at each step, the purest split will win, irrespectively of its type. This is how decision trees can cleverly mix all types of features when classifying samples.</p><div id="_idContainer212" class="note"><p class="Information-Box--PACKT-">Decision tree models can be extended to predict numbers and, so, become <strong class="keyword">regression trees</strong>. In these <a id="_idIndexMarker478"/>trees, each leaf is labeled with a different value of the target variable. Normally, the value of the leaf is just the average of all the samples that ended up in such a leaf node, after going through a construction mechanism similar to the ones for classification trees (using Gini indices and all that). You can build regression trees in KNIME as well: have a<a id="_idIndexMarker479"/> look at the <strong class="keyword">simple regression tree</strong> nodes in the repository.</p></div><p class="normal">Now that we know what decision trees are, let's grow one to classify our bank customers <a id="_idIndexMarker480"/>according to<a id="_idIndexMarker481"/> the outcome of the telemarketing campaign. We'll use a new node for that: the <strong class="keyword">Decision Tree Learner</strong>.</p>

<h2 id="_idParaDest-94" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor096"/><img src="image/image0491.png" alt="Logo&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Decision Tree Learner</em></h2><p class="normal">This <a id="_idIndexMarker482"/>node (<strong class="screenText">Analytics &gt; Mining &gt; Decision tree</strong>) trains <a id="_idIndexMarker483"/>a decision tree model for predicting nominal variables (classification). The most important fields to be set in its configuration dialog (see <em class="italic">Figure 5.23</em>) are:</p><ul><li class="bullet"><strong class="screenText">Class column</strong>: you need to specify your nominal target variable to be predicted.</li><li class="bullet"><strong class="screenText">Quality measure</strong>: this is the metric used to decide how to make the splits. The default value is the <strong class="screenText">Gini index</strong> we have encountered above. You can also select the information for <strong class="screenText">Gain ratio</strong>, which would tend to create more numerous and smaller branches. There is not a good and bad choice, and in most cases both measures generate very similar trees: you can try them both and see which one produces the best results.</li><li class="bullet"><strong class="screenText">Pruning method</strong>: you can use this selector to activate a robust pruning technique<a id="_idIndexMarker484"/> called <strong class="keyword">MDL</strong> (<strong class="keyword">Minimum Description Length</strong>) that removes the less meaningful branches and generates a balanced tree.</li><li class="bullet"><strong class="screenText">Min number records per node</strong>: you can control the tree growth-stopping criterion by setting a minimum number of samples for allowing a further split. By default, this hyperparameter is set to <code class="Code-In-Text--PACKT-">2</code>: this means that no branch will be generated with less than 2 samples. As you increase this number, you will prune more branches and obtain smaller and smaller trees: this is an effective way for tuning the complexity of the trees and obtaining an optimal, well-fitted model. By activating the MDL technique in the earlier selector, you go the "easy way" as it will automatically guess the right level of pruning.<figure class="mediaobject"><img src="image/B17125_05_23.png" alt=""/></figure></li></ul>

			<p class="packt_figref">Figure 5.23: Configuration window of the Decision Tree Learner node: are you up for some pruning today?</p>
			<p class="normal">The output <a id="_idIndexMarker485"/>of the<a id="_idIndexMarker486"/> node is the definition of the tree model, which can be explored by opening its main view (right-click on the node and select <strong class="screenText">View: Decision Tree View</strong>). In <em class="italic">Figure 5.24</em>, you will find the KNIME output of the fast-food classification tree we obtained earlier (see, for comparison, <em class="italic">Figures 5.22 </em>and <em class="italic">5.21</em>): at each node of the tree, you find the number of training samples falling into each value of the class. You can expand and collapse the branches by clicking on the circled <strong class="screenText">+</strong> and <strong class="screenText">–</strong> signs appearing at each split:</p>
			<figure class="mediaobject"><img src="image/B17125_05_24.png" alt=""/></figure>
			<p class="packt_figref">Figure 5.24: The fast-food classification tree, as outputted by the Decision Tree Learner node in KNIME.The gray rows correspond to the majority class</p>
			<ol>
				<li class="numbered" value="4">Drag <a id="_idIndexMarker487"/>and <a id="_idIndexMarker488"/>drop the <strong class="keyword">Decision Tree Learner</strong> node from the repository and connect the upper output of the <strong class="keyword">Partitioning node</strong> (the training set) with it. Let's leave all the default values for now in its configuration (we will have the opportunity for some pruning later): the only selector to double-check is the one setting the <strong class="screenText">Class column</strong> that in our case is <em class="italic">Outcome</em>. If you run the node and open its decision tree view (select the node and press <em class="italic">F10</em>), you will meet the tree you have just grown:<figure class="mediaobject"><img src="image/B17125_05_25.png" alt=""/></figure><p class="packt_figref">Figure 5.25: A first tree classifying bank customers by Outcome: this is just a partial view of the many levels and branches available</p>
</li>
</ol>

<p class="normal">As you<a id="_idIndexMarker489"/> expand<a id="_idIndexMarker490"/> some of the branches, you realize that the tree is very wide and deep: <em class="italic">Figure 5.25</em> shows an excerpt of what the tree might look like (depending on your random partitioning, you might end up with a different tree, which is fine). In this case, we noticed that the top split divided customers into mobile and landline users. This is what happened: the Gini index was calculated across all features and scored the lowest for <em class="italic">Contact</em>, making this the single most important variable to differentiate customers according to their <em class="italic">Outcome</em>. Let's see whether this tree is good enough and predict the outcomes in the test set.</p><h2 id="_idParaDest-95" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor097"/><img src="image/image0531.png" alt="Icon&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Decision Tree Predictor</em></h2>
<p class="normal">This (<strong class="screenText">Analytics &gt; Mining &gt; Decision tree</strong>) applies <a id="_idIndexMarker491"/>a <a id="_idIndexMarker492"/>decision tree model (provided as an input in the first port) to a dataset (second port) and returns the prediction for each input row. This node will not require any configuration and will produce a similar table to the one provided in the input with an additional column that includes the result of the classification.</p>
<ol>
				<li class="numbered" value="5">Let's implement the <strong class="keyword">Decision Tree Predictor</strong> node and wire it in such a way it gets as inputs the tree model outputted by the <strong class="keyword">Decision Tree Learner</strong> node and the second outport of the <strong class="keyword">Partitioning</strong> node, which is our test set. As you execute the node, you will find an output that the precious additional column called <em class="italic">Prediction (Outcome)</em>.
</li>
</ol>

<p class="normal">At this point, we can finally assess the performance of the model by calculating the metrics used for classification. Do you remember the accuracy, precision, sensitivity measures, and confusion matrix we obtained in the cute dog versus muffin example? It's time to <a id="_idIndexMarker493"/>calculate <a id="_idIndexMarker494"/>these metrics by using the right node: <strong class="keyword">Scorer</strong>.</p>

<h2 id="_idParaDest-96" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor098"/><img src="image/image054.png" alt="A picture containing graphical user interface&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Scorer</em></h2>

<p class="normal">This <a id="_idIndexMarker495"/>node (<strong class="screenText">Analytics &gt; Mining &gt; Scoring</strong>) calculates<a id="_idIndexMarker496"/> the summary performance scores of classification by comparing two nominal columns. The only step required for its configuration (<em class="italic">Figure 5.26</em>) is the selection of the columns to be compared: you should select the column carrying the observed (actual) values in the <strong class="screenText">First Column </strong>dropdown, while predictions go in the <strong class="screenText">Second Column</strong> selector. The node outputs the most important metrics for assessing a classification performance, namely: the Confusion Matrix, provided <a id="_idIndexMarker497"/>as a table in the first output (columns will refer to the predictions, while<a id="_idIndexMarker498"/> actual values will go as rows) and summary <a id="_idIndexMarker499"/>metrics <a id="_idIndexMarker500"/>such as <strong class="keyword">Accuracy</strong>, <strong class="keyword">Precision</strong>, and <strong class="keyword">Sensitivity</strong>, which you can find in the second output of the node.</p><div id="_idContainer219" class="note"><p class="Information-Box--PACKT-">Some of the performance metrics for a classification will depend on which class you decide to be considered as <code class="Code-In-Text--PACKT-">Positive</code>: have a look at <em class="italic">Figure 4.8</em> in the previous chapter to get a refresher. In the second output of the Scorer node, you will find one row for every possible class: each row contains the metrics calculated under the assumption that one specific class is labeled as <code class="Code-In-Text--PACKT-">Positive</code> and all the other classes are <code class="Code-In-Text--PACKT-">Negative</code>. </p></div><figure class="mediaobject"><img src="image/B17125_05_26.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.26: The configuration window of the Scorer node: just select the columns to compare across</p>
<ol>
				<li class="numbered" value="6">We can <a id="_idIndexMarker501"/>now <a id="_idIndexMarker502"/>add the <strong class="keyword">Scorer </strong>node (make sure you don't get confused and<a id="_idIndexMarker503"/> pick the <strong class="keyword">Numeric Scorer</strong> node, which can only be used for regressions) to the workflow and connect it downstream to the <strong class="keyword">Decision Tree Predictor</strong>. In the configuration window, we can leave everything as it is, just checking that we have <em class="italic">Outcome</em> as <strong class="screenText">First Column</strong> and <em class="italic">Prediction (Outcome)</em> as <strong class="screenText">Second Column</strong>. Execute the node and open its main view (<em class="italic">F10</em> or right-click and select <strong class="screenText">View: Confusion Matrix</strong>).<p class="normal">The output of the <strong class="keyword">Scorer</strong> node (<em class="italic">Figure 5.27</em>) tells us that we get an accuracy level of 78.3%: out of 100 predictions, 78 of them turn out to be correct. The confusion matrix helps us understand whether the model can bring value to our business case:</p><figure class="mediaobject"><img src="image/B17125_05_27.png" alt=""/></figure><p class="packt_figref">Figure 5.27: The output of the node Scorer after our first classification: 78% accuracy is not bad as a starting point</p>
</li>
</ol>

<p class="normal">In the case shown in <em class="italic">Figure 5.27</em>, we have 450 customers (180 + 270) in the test set that were <a id="_idIndexMarker504"/>predicted as<a id="_idIndexMarker505"/> interested in the account (<em class="italic">Prediction (Outcome) </em>= <code class="Code-In-Text--PACKT-">yes</code>). Out of this, only 180 (40%, which corresponds to the precision of our model) were predicted correctly, meaning that these customers ended up buying the product. The number seems to be low, but it is already encouraging: the algorithm can help to find a subset of customers that are more likely to buy the product. If we indiscriminately called every customer—as we know from the pilot—we would have achieved a success rate of 19% while, by focusing on the (fewer) customers that the algorithm identified as potential (<em class="italic">Prediction (Outcome) </em>= <code class="Code-In-Text--PACKT-">yes</code>), the success rate would double and reach 40%.</p><p class="normal">Let's now think about what we can do to improve the results of the modeling. We remember that our decision tree was deep and wide: some of the branches were leading to very "specific" cases, which interested only a handful of examples in the training set. This doesn't look right: a decision tree that adapted so closely to the training set might produce high errors in future cases as it is not able to comprehend the essential patterns of general validity. We might be overfitting! Let's equip ourselves with a good pair of pruning shears: we can try to fix the overfitting by reducing the complexity of the tree, making some smart cuts here and there:</p><div id="_idContainer222" class="packt_tip"><p class="Tip--PACKT-">Sometimes, the Decision Tree Predictor node generates null predictions (red <code class="Code-In-Text--PACKT-">?</code> in KNIME tables, which caused the warning message you see at the top of <em class="italic">Figure 5.27</em>). This is a sign that the tree might be overfitted: its paths are too "specific" and do not encompass the set of values that require a prediction (this "pathology" is<a id="_idIndexMarker506"/> called <strong class="keyword">No True Child</strong>). Besides taking care of the overfitting, one trick you can apply to solve the missing values is to open the <strong class="screenText">PMMLSettings </strong>panel (second tab in the <strong class="keyword">Decision Tree Learner</strong> configuration) and set <strong class="screenText">No true child strategy</strong> to <strong class="screenText">returnLastPrediction</strong>.</p></div>
<ol>
				<li class="numbered" value="7">Open the configuration dialog of the <strong class="keyword">Decision Tree Learner</strong> and select <strong class="screenText">MDL</strong> as the <strong class="screenText">Pruning method</strong>. This is the simplest and quickest way to prune our tree: we could have also iterated through higher values of <strong class="screenText">Min number records per node</strong> (give it a try to check how it works), but MDL is a safe approach to get quick improvements.</li>
				<li class="numbered">Let's see if it worked. We don't need to change anything else, so let's just execute the <strong class="keyword">Scorer</strong> node and open its main view to see what happened.<p class="normal">When you<a id="_idIndexMarker507"/> look at the<a id="_idIndexMarker508"/> results (<em class="italic">Figure 5.28</em>) you feel a thrill of excitement: things got better. The accuracy raised to 83% and, most importantly, the precision of the model greatly increased. Out of the 175 customers in the test set who are now predicted as <em class="italic">Outcome</em>=<code class="Code-In-Text--PACKT-">yes</code>, 117 would have ended up actually buying the product. If we followed the recommendation of the model (which we can assume will keep a similar predictive performance on customers we didn't call yet—so the remaining 97% of our customer base), the success rate of our marketing campaign will move to 67%, which is more than 3 times better than our initial baseline of 19%!</p><figure class="mediaobject"><img src="image/B17125_05_28.png" alt=""/></figure><p class="packt_figref">Figure 5.28: The output of the node Scorer after our tree pruning: the precision</p>
</li>
</ol>

<p class="normal">The model was previously overfitting and some pruning clearly helped. If you now open the tree view of the <strong class="keyword">Decision Tree Learner</strong> node, you will find a much simpler model that can be explored and, finally, interpreted. You can expand all branches at once by selecting the root node (just left-click on it) and then clicking on <strong class="screenText">Tree</strong> | <strong class="screenText">Expand Selected Branch</strong> from the top menu. By looking at the tree, which might be similar to the one shown in <em class="italic">Figure 5.29</em>, we can finally attempt some interpretation of the model. Look at the different percentages of the <code class="Code-In-Text--PACKT-">yes</code> category within each node: we found some buckets of customers that are disproportionally interested in our product:</p><figure class="mediaobject"><img src="image/B17125_05_29.png" alt=""/></figure><p class="packt_figref">Figure 5.29: An excerpt of the decision tree classifying bank customers by Outcome: students, retired, and 60+ customers using landlines are showing the most interest in our new savings account</p><p class="normal">For<a id="_idIndexMarker509"/> example, we find out that<a id="_idIndexMarker510"/> customers falling into these three segments:</p><ul><li class="bullet">Mobile users who are students</li><li class="bullet">Mobile users who are retired</li><li class="bullet">Landline users who are 60+ years old</li></ul>
			<p class="normal">responded much more to our pilot campaign than all others, having more than 50% of the samples ending up with opening a new savings account. We have a quick chat with the product manager and show these results to him. He is very excited about the findings and, after some thinking, he confirms that what the algorithm spotted makes perfect sense from a business standpoint. The new type of account has less fixed costs than the others, so this explains while its proposition proves more compelling to lower-income customers, such as students and the retired. Additionally, this account includes a free prepaid card, which is a great tool for students, who can get their balance topped up progressively, but also for older customers, who do not fully trust yet the usage of traditional credit cards and prefer keeping the risk of fraud under control. The account manager is very pleased with what you shared with him and does not stop thanking you: by having data-based evidence of the characteristics that make a customer more likely to buy his new product, he can now finetune the marketing concept, highlighting benefits and reinforcing the message to share with prospective customers.</p>
			<p class="normal">The positive feedback you just received was invigorating and you want to quickly move to the second part of the challenge: building a propensity model able to "score" the 97% of the customers that have not been contacted yet. To do so, we will first need to introduce <a id="_idIndexMarker511"/>another classification<a id="_idIndexMarker512"/> algorithm particularly well suited for anticipating propensities: <strong class="keyword">random forest</strong>.</p>
			<h2 id="_idParaDest-97" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor099"/>Random forest algorithm</h2>
			<p class="normal">One<a id="_idIndexMarker513"/> approach <a id="_idIndexMarker514"/>used in machine learning to obtain better performance is <strong class="keyword">ensemble learning</strong>. The<a id="_idIndexMarker515"/> idea behind it is very simple: instead of building a single model, you combine multiple <em class="italic">base</em> models together and obtain an <em class="italic">ensemble</em> model that, collectively, produces stronger results than any of the underlying models. If we apply this concept to decision trees, we will grow multiple models in parallel and obtain… a forest. However, if we run the decision tree algorithm we've seen in the previous pages to the same data set multiple times, we will just obtain "copies" of identical trees. Think about it: the procedure we described earlier (with the calculation of the Gini index and the building of subsequent branches) is completely deterministic and will always produce the same outputs when using the same inputs. To encourage "diversity" across the base models, we need to force some variance in the inputs: one way to do so is to randomly sample subsets of rows and columns of our input dataset, and offer them as different training sets to independently growing base models. Then, we will just need to aggregate the results of the several base models into a single ensemble model. This <a id="_idIndexMarker516"/>is called <strong class="keyword">Bagging</strong>, short <a id="_idIndexMarker517"/>for <strong class="keyword">Bootstrap Aggregation</strong>, which is the secret ingredient that we are going to use to move from decision trees to random forests. </p>
			<p class="normal">To understand how it works, let's visualize it in a practical example: <em class="italic">Figure 5.30</em> shows both a simple decision tree and a random forest (made of four trees) built on our bank telemarketing example:</p>
			<figure class="mediaobject"><img src="image/B17125_05_30.png" alt=""/></figure>
			<p class="packt_figref">Figure 5.30: A decision tree and random forest compared: with the forest you get a propensity score and higher accuracy</p>
			<p class="normal">Thanks to a random <a id="_idIndexMarker518"/>sampling of rows and columns, we managed to grow four different trees, starting <a id="_idIndexMarker519"/>from the same initial dataset. Look at the tree on the bottom left (marked as <em class="italic">#1</em> in the figure): it only had the <em class="italic">Mortgage</em> and the <em class="italic">Contact</em> columns available to learn from, as they were the ones randomly sampled in its case. Given the subset of rows that were offered to it (that were also randomly drawn as part of the bootstrap process), the model applies the decision tree algorithm and produces a tree that differs from all other base models (you can check the four trees at the bottom—they are all different). Given the four trees that make our forest, let's imagine that we want to predict the outcome for a 63-year-old retired customer, who has a mortgage and gets contacted by landline. The <em class="italic">same</em> customer will follow four <em class="italic">different</em> paths (one for each tree), which will lead to different outcomes. In this case, 3 trees out of 4 agree that the prediction should be <code class="Code-In-Text--PACKT-">yes</code>. The resulting ensemble prediction will be made in a very democratic manner, by voting. Since the majority believes that this customer is a <code class="Code-In-Text--PACKT-">yes</code>, the final outcome will be <code class="Code-In-Text--PACKT-">yes</code> with a <strong class="keyword">Propensity score</strong> of 0.75 (3 divided by 4). </p>
			<p class="normal">The assumption we make is that the more trees that are in agreement with a customer being classified as <code class="Code-In-Text--PACKT-">yes</code>, the "closer" the customer is to buying our product. Of course, we normally build many more trees than just four: the diversity of the different branching each tree displays will make our ensemble model more "sensitive" to the smaller nuances of feature combinations that can tell us something useful about the propensity of a customer. Every tree offers a slightly "different" point of view on how to classify a customer: by bringing all these contributions together—in a sort of decisions crowdsourcing—we obtain more robust collective predictions: this is yet another proof of the universal value of diversity in life!</p>
			<div>
				<div id="_idContainer226" class="note">
					<p class="Information-Box--PACKT-">Although the propensity score is related to the probability that a classification is correct, they are not the same thing. We are still in the uncertain world of probabilistic models: even if 100% of the trees agree on a specific classification, you cannot be 100% sure that the classification is right.</p>
				</div>
			</div>
			<p class="normal">Let's get <a id="_idIndexMarker520"/>acquainted <a id="_idIndexMarker521"/>with the KNIME node that can grow forests: meet the <strong class="keyword">Random Forest Learner</strong> node.</p>
			<h2 id="_idParaDest-98" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor100"/><img src="image/image060.png" alt=""/> </span><em class="bold-italic">Random Forest Learner</em></h2>
			<p class="normal">This<a id="_idIndexMarker522"/> node (<strong class="screenText">Analytics &gt; Mining &gt; Decision Tree Ensemble &gt; Random Forest &gt; Classification</strong>) trains a <a id="_idIndexMarker523"/>random forest model for classification. At the top of its configuration window (<em class="italic">Figure 5.31</em>) you can select the nominal column to use as the target of the classification (<strong class="screenText">Target Column</strong>). Then, in the column selector in the middle, you can choose which columns to use as features (the ones appearing on the <strong class="screenText">Include</strong> box on the right): all others will be ignored by the learning algorithm. The option <strong class="screenText">Save target distribution…</strong> will record the number of samples that fell into each leaf of the underlying tree models: although it is memory expensive, it can help to generate more accurate propensity scores, by means of the <strong class="keyword">soft voting</strong> technique, which we will talk about later.</p>
			<p class="normal">Toward the bottom of the window, you will find also a box that lets you choose how many trees you want to grow (<strong class="screenText">Number of models</strong>). Lastly, you can decide to check a tick box (labeled as <strong class="screenText">Use static random seed</strong>) that, similarly to what you found in the <strong class="keyword">Partitioning</strong> node, lets you "fix" the initialization seed of the pseudo-random number generator used for the random sampling of rows and columns: in this case, you will obtain, at parity of input and configuration parameters, always the same forest generated:</p>
			<figure class="mediaobject"><img src="image/B17125_05_31.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.31: Configuration window of the Random Forest Learner node: how many trees you want to see in the forest?</p>
			<ol>
				<li class="numbered" value="9">Let's<a id="_idIndexMarker524"/> implement<a id="_idIndexMarker525"/> the <strong class="keyword">Random Forest Learner</strong> node and connect the training set (the first output port of the <strong class="keyword">Partitioning</strong> node) with its input: there is no harm in reusing the same training and test sets used for the decision tree learner. If we execute the node and open its main view (<em class="italic">F10</em> or right-click and then select <strong class="screenText">View: Tree Views</strong>), we will find a tree-like output, as in the case of the decision trees: however, this time, we have a little selector at the top that lets us scroll across all 100 trees of the forest.
</li>
</ol>

<div id="_idContainer229" class="packt_tip"><p class="Tip--PACKT-">Random forests are <strong class="keyword">black box</strong> models as they are hard to interpret: going through 100 different trees would not offer us a hint for explaining how the predictions are made. However, there is a simple way to check which features proved to be most meaningful. Open the second outport of the <strong class="keyword">Random Forest Learner</strong> node (right-click and click on <strong class="screenText">Attribute statistics</strong>). The first column—called <em class="italic">#splits (level 0)</em>—tells you how many times that feature was selected as the top split of a tree. The higher that number, the more useful that feature has been in <a id="_idIndexMarker526"/>the <a id="_idIndexMarker527"/>learning process of the model.</p></div>


<h2 id="_idParaDest-99" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor101"/><img src="image/image062.png" alt=""/> </span><em class="bold-italic">Random Forest Predictor</em></h2><p class="normal">This<a id="_idIndexMarker528"/> node (<strong class="screenText">Analytics &gt; Mining &gt; Decision Tree Ensemble &gt; Random Forest &gt; Classification</strong>) applies<a id="_idIndexMarker529"/> a random forest model (which needs to be provided in the first gray input port) to a dataset (second port) and returns the ensemble prediction for each input row. As part of its configuration, you can decide whether you want to output the propensity scores for each individual class (<strong class="screenText">Append individual class probabilities</strong>). If you tick the <strong class="screenText">Use soft voting </strong>box, you enable a more accurate estimation of propensity: in this case, the vote of each tree will be weighted by a factor that depends on how many samples fell in each leaf during the learning process. The more samples a leaf has "seen," the more confident we can be about its estimation. To use this feature, you will have to select the option <strong class="screenText">Save target distribution…</strong> in the <strong class="keyword">Random Forest Learning</strong> node, which is upstream.</p><figure class="mediaobject"><img src="image/B17125_05_32.png" alt=""/></figure><p class="packt_figref">Figure 5.32: The configuration dialog of Random Forest Learner node. You can decide whether you want to see propensity scores or not.</p>
<ol>
				<li class="numbered" value="10">Drag <a id="_idIndexMarker530"/>and<a id="_idIndexMarker531"/> drop the <strong class="keyword">Random Forest Predictor</strong> node onto the workflow and connect its inputs with the forest model, outputted by the <strong class="keyword">Random Forest Learner</strong> and the training set, meaning the bottom outport of the <strong class="keyword">Partitioning</strong> node. Configure the node by unticking the <strong class="screenText">Append overall prediction confidence</strong> box, and ticking both the <strong class="screenText">Append individual class probabilities</strong> (we need the propensity score) and the <strong class="screenText">Use soft voting</strong> boxes. After you execute it, you will find at its output the test set enriched with the prediction, <em class="italic">Prediction (Outcome)</em>, and the propensity scores by class. Specifically, the propensity of a customer being interested in our product is <em class="italic">P (Outcome=Yes)</em>.</li>
				<li class="numbered">Implement a new <strong class="keyword">Scorer</strong> node (for simplicity, you can copy/paste the one you used for the decision tree) and connect it downstream to the <strong class="keyword">Random Forest Predictor</strong>. For its configuration, just make sure you select <em class="italic">Outcome </em>and <em class="italic">Prediction (Outcome)</em> in the first two drop-down menus. Execute it and open its main output view (<em class="italic">F10</em>).<p class="normal">The results of <strong class="keyword">Scorer</strong> (<em class="italic">Figure 5.33</em>) confirm that, at least in this case, the ensemble model comes with better performance metrics. Accuracy has increased by a few decimal points and, most importantly (as it directly affects the ROI of our marketing campaigns), precision has reached 72% (open the <strong class="screenText">Accuracy statistics </strong>outport to check it or compute it easily<a id="_idIndexMarker532"/> from<a id="_idIndexMarker533"/> the confusion matrix):</p><figure class="mediaobject"><img src="image/B17125_05_33.png" alt=""/></figure><p class="packt_figref">Figure 5.33: The Scorer node output for our random forest. Both accuracy and precision increased versus the decision tree: diversity helps</p>
</li>
</ol>

<p class="normal">Now that we have confirmation that we have built a robust model at hand, let's concentrate on the propensity score we calculated and see what we can do with it.</p><p class="normal">Open the output of the <strong class="keyword">Random Forest Predictor</strong> node and sort the rows by decreasing level of propensity (click on the header of column <em class="italic">P (Outcome=yes)</em> and then on <strong class="screenText">Sort Descending</strong>): you will obtain a view similar to the one shown in <em class="italic">Figure 5.34</em>:</p><figure class="mediaobject"><img src="image/B17125_05_34.png" alt=""/></figure><p class="packt_figref">Figure 5.34: The predictions generated by the random forest in descending order of propensity, P (Outcome=yes): the more we go down the list, the less interested customers (column Outcome) we find</p><p class="normal">At the top of the list, we have the customers in the test set that most decision trees identified as interested. In fact, if you look at the column <em class="italic">Outcome</em>, we find that most rows show a <code class="Code-In-Text--PACKT-">yes</code>, proving that, indeed, these customers were very interested in the product (when called, they agreed to open the savings account). If you scroll down the list, the propensity will go down and you will start finding increasingly more <code class="Code-In-Text--PACKT-">no</code> values in column <em class="italic">Outcome</em>. Now, let's think about the business case once again: now that we have a model able to predict the level of propensity, we could run it on the other 97% of customers<a id="_idIndexMarker534"/> that<a id="_idIndexMarker535"/> were not contacted as part of the pilot. If we then sorted our customer list by decreasing level of propensity (as we just did on the test set), we will obtain a prioritized list of the next people to call about our product. We will expect that the first calls (the ones directed to the most inclined people) will end up with a very high success rate (like we noticed in the test set). </p><p class="normal">Then, little by little, the success rate will decay: more and more people will start saying <code class="Code-In-Text--PACKT-">no</code> and, at some point, it will start to become counterproductive to make a call. So, the key question becomes: at what point should we "stop" to get the maximum possible ROI from the initiative? How many calls should we make? What is the minimum level of propensity, below which we should avoid attempting to make a sale? The exciting part of propensity modeling is that you can find an answer to these questions before making <em class="italic">any</em> call!</p>

<p class="normal">In fact, if we assume that the customers that were part of the pilot were a fair sample of the total population, then we can use our test set (which has not been "seen" by the training algorithm, so there is no risk of overfitting) as a base for simulating the ROI of a marketing campaign where we call customers by following a decreasing level of propensity. This is exactly what we are going to do right now: we will need to first sort the test set by decreasing level of propensity (the temporary sorting we did earlier did not impact the permanent order of the rows in the underlying table); then, we calculate the cumulative profit we would make by "going through the list," using the cost and revenue estimates shared by the product manager. We check at which level of propensity we maximized our profit, so that we have a good estimate of the number of calls that we will need to make in total to optimize the ROI. Let's get cracking!</p>

<ol>
				<li class="numbered" value="12">Implement a <strong class="keyword">Sorter</strong> node and connect it at the output of the <strong class="keyword">Random Forest Predictor</strong> node. We want to sort the customers in the test set by decreasing level of propensity, so select column <em class="italic">P (Outcome=yes)</em> and go for the <strong class="screenText">Descending </strong>option.</li>
				<li class="numbered">Implement a <strong class="keyword">Rule Engine</strong> node to calculate the marginal profit we make on each individual <a id="_idIndexMarker536"/>customer. We know that every call we make costs us $15, irrespective of its outcome. We also know that every account opening brings an incremental revenue of $60. Hence, every customer that ends up buying the product (<em class="italic">Outcome</em>=<code class="Code-In-Text--PACKT-">Yes</code>) brings $45 of profit while all others hit us by $–15. Let's create a column (we <a id="_idIndexMarker537"/>can<a id="_idIndexMarker538"/> call it <em class="italic">Profit</em>) that implements this simple logic, as shown in <em class="italic">Figure 5.35</em>:<figure class="mediaobject"><img src="image/B17125_05_35.png" alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.35: The Rule Engine node for calculating the marginal profit for each individual customer </p>
</li>
</ol>

<p class="normal">To calculate the cumulative profit we will need to use a new node, called <strong class="keyword">Moving Aggregation</strong>.</p>


<h2 id="_idParaDest-100" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor102"/><img src="image/image067.png" alt="A picture containing text, clipart&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Moving Aggregation</em></h2><p class="normal">As the <a id="_idIndexMarker539"/>name <a id="_idIndexMarker540"/>suggests, this node (<strong class="screenText">Other Data Types &gt; Time Series &gt; Smoothing</strong>) aggregates values on moving windows and calculates cumulative summarizations. To use a moving window, you will have to declare the <strong class="screenText">Window length</strong> in terms of the number of rows to be considered and the <strong class="screenText">Window type </strong>(meaning the direction of movement of the window in the table). For example, if you select <strong class="screenText">3</strong> as the length and <strong class="screenText">Backward</strong> as the type, the previous 3 rows will be aggregated together. If you want to aggregate by cumulating values from the first row to the last, you need to check the <strong class="screenText">Cumulative computation</strong> box. Similarly to a Group By node, the <strong class="screenText">Aggregation settings</strong> tab will let you select which columns should be aggregated and using which method:</p><figure class="mediaobject"><img src="image/B17125_05_36.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.36: Configuration dialog of the Moving Aggregation node: you can aggregate through moving windows or by progressively cumulating</p>

<ol>
				<li class="numbered" value="14">Implement the <strong class="keyword">Moving Aggregation</strong> node and connect it downstream from the <strong class="keyword">Rule Engine</strong>. Check the <strong class="screenText">Cumulative Computation</strong> box, double-click on the <em class="italic">Profit</em> column on the left, and select <strong class="screenText">Sum</strong> as the aggregation method. Execute the node and open its outport view.<p class="normal">The <strong class="keyword">Moving Aggregation</strong> node has cumulated the marginal profit generated by each customer. If we scroll the list (similar to the one displayed in <em class="italic">Figure 5.37</em>) and keep an eye on the last column, <em class="italic">Sum(Profit)</em>, we noticed that the profit peaks when we are slightly below the first third of the full list. When the <em class="italic">P (Outcome=yes)</em> propensity is near 0.23, we obtain a profit of around $8,200. This means that by calling only people above this level <a id="_idIndexMarker541"/>of <a id="_idIndexMarker542"/>propensity (called <a id="_idIndexMarker543"/>the <strong class="keyword">Cutoff</strong> point), we maximize the ROI of our campaign.</p><figure class="mediaobject"><img src="image/B17125_05_37.png" alt=""/></figure><p class="packt_figref">Figure 5.37: The output of the Moving Aggregation node: it seems that we reach maximum profit when we call people having a propensity of around 0.23.</p>
</li>
</ol>


<p class="normal">To make this concept clearer, let's visualize the changing profit by employing a line chart.</p><h2 id="_idParaDest-101" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor103"/><img src="image/image070.png" alt="A picture containing text&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Line Plot (local)</em></h2><p class="normal">This <a id="_idIndexMarker544"/>node (<strong class="screenText">View &gt; Local (Swing)</strong>) generates <a id="_idIndexMarker545"/>a line plot. The only configuration that might be needed is the box labeled <strong class="screenText">No. of rows to display</strong>, which you can use to extend the limit of rows considered for creating the plot.</p>
<ol>
				<li class="numbered" value="15">Implement a <strong class="keyword">Line Plot (local)</strong> node, extend the number of rows to display to at least 3,000 (the size of the test set), execute it, and open its view at once (<em class="italic">Shift</em> + <em class="italic">F10</em>). In the <strong class="screenText">Column Selection </strong>tab, keep only <em class="italic">Sum(Profit) </em>on the right and remove all other columns.</li>
			</ol>
			<p class="normal">The output of the chart (shown in <em class="italic">Figure 5.38</em>) confirms what we noticed in the table and makes it more evident: if we use the propensity score to decide the calling order of customers, our profit will follow the shape of the curve in the figure. We will start with a steep increase of profit (see the first segment on the left), as most of the first people we call (which are top prospects, given their high propensity score) will actually buy the product. Then, at around one-third of the list (when we know that the propensity score is near 0.23), we reach the maximum possible profit. After that, it will drop fast as we will encounter fewer and fewer interested customers. If we called all the people on the list, we will end<a id="_idIndexMarker546"/> up with <a id="_idIndexMarker547"/>a significant loss, as we have painfully learned as part of the pilot campaign:</p>
			<p class="packt_figref"><img src="image/B17125_05_38.png" alt="Graphical user interface, chart&#10;&#10;Description automatically generated"/></p>
			<p class="packt_figref">Figure 5.38: The cumulative profit curve for our machine learning-assisted telemarketing campaign: we maximize the ROI at around one-third of the list sorted by propensity</p>
			<p class="normal">Thanks to this simulation, we have discovered that if we limit our campaign to customers with a propensity score higher than 0.23 (which will be around one-third of the total population), we will maximize our profit. By doing the required proportions (our simulation covered <em class="italic">only</em> the test set, so 3,000 customers in total), we can estimate how much profit we would make if we applied our propensity model to the <em class="italic">entire</em> bank database. In this case, we would use the scores to decide who to call within the remaining 97% of the customer base. The overall "size of the prize" of conducting a mass telemarketing campaign will bring around $800,000 of profit, if we were to call one-third of the bank's customers. Considering that it might not be viable to make so many calls, we might stop earlier in the list: in any case, we will make some considerable profit by following the list that our random forest can now generate. The simulation that we just did can be used as a tool for planning the marketing spend and sizing the right level of investment. The product manager <a id="_idIndexMarker548"/>and your<a id="_idIndexMarker549"/> boss are pleased with the great work you pulled together. You definitely proved that spotting (and following) the ML way can bring sizeable value to the business: in this case, you completely reversed the potential outcome of a marketing campaign. The heavy losses in the pilot can now be transformed into a meaningful value, thanks to data, algorithms, and—most importantly—your expertise in leveraging them. It was a terrific result, and it took only 12 KNIME nodes (<em class="italic">Figure 5.39</em>) to put all of this together!</p>
			<figure class="mediaobject"><img src="image/B17125_05_39.png" alt=""/></figure>
			<p class="packt_figref">Figure 5.39: Full workflow for the bank telemarketing optimization</p>
			<h1 id="_idParaDest-102" class="title"><a id="_idTextAnchor104"/>Segmenting consumers with clustering</h1>
			<p class="normal">In this<a id="_idIndexMarker550"/> tutorial, you will re-enter the shoes of the business analyst working for the online retailer we encountered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Transforming Data</em>. This time, instead of automating the creation of a set of financial reports, you are after a seemingly sexier objective. The <strong class="keyword">Customer Relationship Management</strong> (<strong class="keyword">CRM</strong>) team is looking for a smarter way to communicate <a id="_idIndexMarker551"/>with those customers who opted-in to receive regular newsletters. Instead of sending a weekly email equal for all, the CRM manager asked you to find a data-based approach for creating a few meaningful consumer segments. Once segments are defined, the CRM team can build multiple messages, one for each segment. By doing so, they will offer a more personalized (and engaging) experience for the entire customer base, which will ultimately affect customer loyalty and drive sustainable revenue growth.</p>
			<p class="normal">Unsupervised learning offers a proven methodology that can meet this business need: by using a clustering algorithm, we can create several groups of customers that <em class="italic">look similar</em> in terms of their characteristics (such as age, family composition, and income level) and the consumption patterns they displayed through previous purchases (like the average price of the products they selected, the overall amount of money they spent, or the frequency of their orders). This is the ML way of helping the business: use clustering to segment consumers appropriately.</p>
			<p class="normal">The CRM manager has already initiated the gathering of some basic consumer-level data and obtained a CSV file (<code class="Code-In-Text--PACKT-">eCommerce-CRM.csv</code>), which has 4,157 rows—one for each customer—and four columns:</p>
			<ul>
				<li class="bullet"><em class="italic">Customer_ID</em>: a unique identifier of the customer.</li>
				<li class="bullet"><em class="italic">Average Price</em>: the average unit price for all purchases made by each customer. It gives us a directional view of the "premiumness" of the former shopping choices displayed by the customer.</li>
				<li class="bullet"><em class="italic">Basket Size</em>: the average number of units purchased within any single order created by the customers. This measure indicates whether they prefer to go for "bulk" shopping with fat baskets or smaller, occasion-driven purchase acts. </li>
				<li class="bullet"><em class="italic">Unique Products</em>: the average number of different articles that the customer buys on each occasion. This metric indicates the breadth of the assortment "tried" by each customer. It gives us an idea of the customer's willingness to explore new products versus their preference of "keep buying" the same articles all the time.</li>
			</ul>
			<p class="normal">As we exchange thoughts with the CRM manager about this dataset, she confirms what we had already noticed: the three consumption metrics included in the data (the last three columns) are far from giving us a comprehensive picture of each customer's preferences. If we wanted, we could have generated many more columns by aggregating the transactions history: think about the absolute number of purchases by customer, the total generated value, the "mix" of purchased categories and subcategories, the premiumness of the purchased products within each category and subcategory, and also the customer characteristics, like their age, the average income of the neighborhood they live in, and so on. Still, we decide to go ahead and leverage the power of machine learning on this first dataset: we can always increase the level of the model sophistication later if we want. Now, the<a id="_idIndexMarker552"/> important thing is to "start rocking" and pragmatically prove some first business value from this new way of operating. In terms of deliverables, you align with your business partner the need to assign each customer to a small number of clusters and put together some visualizations to interpret what differentiates clusters.</p>
			<p class="normal">It's time to power KNIME on, create a new workflow, and load our CRM extract into it:</p>
			<ol>
				<li class="numbered" value="1">Load the file <code class="Code-In-Text--PACKT-">eCommerce-CRM.csv</code> onto the workflow editor. As the <strong class="keyword">CSV Reader</strong> node dialog pops up, we can check that all four columns are showing in the preview and click <strong class="screenText">OK</strong> to confirm the default setting. After executing the node, we can look at its output view (<em class="italic">Figure 5.40</em>) and move to the next step:<figure class="mediaobject"><img src="image/B17125_05_40.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.40: The CRM extract once loaded: for every customer, we have three metrics, each one giving us a hint of their shopping habits</p><p class="normal">Creating<a id="_idIndexMarker553"/> homogenous groups of elements, such as customers in our case, requires the use of a clustering algorithm. Let's make acquaintance with possibly the most popular clustering algorithm available today – <strong class="keyword">k-means</strong>.</p>
</li>
</ol>

<h2 id="_idParaDest-103" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor105"/>K-means algorithm</h2><p class="normal">The k-means<a id="_idIndexMarker554"/> algorithm <a id="_idIndexMarker555"/>is perhaps the easiest (and yet probably the most used) approach used for clustering. The big idea is elementary and can be summarized in two lines: each element in a dataset is assigned to the closest cluster. At each step of the process, the position of the clusters gets updated, so they become more and more compact.</p><p class="normal">Let's imagine we want to cluster a set of points displayed on a bi-dimensional scatter plot. Each point is described employing two numbers that represent the horizontal and the vertical coordinates, respectively. The distance between any two points can be easily calculated through the Pythagorean theorem (yes, the same used for calculating the sides of a right triangle—see <em class="italic">Figure 5.41</em> for a refresher):</p><figure class="mediaobject"><img src="image/B17125_05_41.png" alt="Chart, line chart&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.41: Calculating the distance between two points using the Pythagorean theorem: you make the square root of the sum of the squared differences for each coordinate</p><p class="normal">The goal of <a id="_idIndexMarker556"/>the k-means <a id="_idIndexMarker557"/>algorithm is to create a given number (<em class="italic">k</em>) of homogenous groups formed by points that are relatively close to one another. Like many other machine learning algorithms, k-means has an iterative approach: at each iteration, it groups the points based on their proximity to some special points called the <strong class="keyword">centroids</strong> of each cluster. Every point is associated with its closest centroid. The algorithm then updates the position of the centroids iteratively: at each iteration, the groups will tend to be more and more homogenous, meaning that the points forming these clusters will be gradually closer and closer to each other.</p><p class="normal">Let's see in detail the sequence of steps that make the k-means algorithm:</p>
<ol>
<li class="alphabetic-l2"><strong class="keyword">Initialization</strong>: the <a id="_idIndexMarker558"/>first step of the algorithm is making the initial choice of the centroids, one per cluster. There are different ways to make this choice. The simplest way is to randomly select <em class="italic">k</em> points in our dataset.</li><li class="alphabetic-l2"><strong class="keyword">Grouping</strong>: the algorithm now calculates the distance of each point from each centroid (using the Pythagorean theorem), and each point is matched with its closest centroid (the one lying at the smallest distance). In this way, all the points near a centroid are grouped together as they belong to the same cluster.</li><li class="alphabetic-l2"><strong class="keyword">Update</strong>: the algorithm now calculates the centroid of each cluster again by making an average of the coordinates of all the points that belong to the cluster. Basically, the <a id="_idIndexMarker559"/>centroid is updated so that it matches the center of mass of the newly formed group.</li>
</ol>
			<p class="normal">At this point of the process, we return to step <em class="italic">b</em> to start a new iteration and repeat steps <em class="italic">b</em> and <em class="italic">c</em> as long <a id="_idIndexMarker560"/>as it is possible to<a id="_idIndexMarker561"/> improve the centroids. At every iteration, the clusters will converge, meaning that they will become increasingly more meaningful. We will stop when the update step produces no change in the way in which points are assigned to clusters. When this happens, the algorithm terminates: a stable solution has been found, and the current definition of clusters is returned as the resulting output. Should this convergence not take place, the algorithm will stop in any case once a preset number of maximum iterations is reached.</p>
			<p class="normal">This process might still look complicated but let me stress how simple the underlying mathematics is: random draws, averages, squares, and the square roots in the Pythagorean theorem are all the math we need to implement the k-means algorithm.</p>
			<p class="normal">To better understand how the algorithm works, let's go through a concrete example and use some charts to display the evolution of the various iterations graphically. For the sake of simplicity, we will use a simple dataset formed only by two columns: by having only two columns, we can visualize the distances between the various points on 2-dimensional scatter plots (Cartesian diagrams).</p>
			<div>
				<div id="_idContainer243" class="note">
					<p class="Information-Box--PACKT-">When we work with datasets with more than two columns (as is usually the case), the concept of distance becomes more difficult to visualize in our human mind. While, with three columns, we can still imagine the algorithm working on a 3-dimensional space, with 4, 5, or 10 columns, we will necessarily need to delegate the task to machines. Luckily, they are much more at ease than humans when navigating multidimensional spaces. The good news is that the basic formula for calculating distances (the Pythagorean theorem you found in <em class="italic">Figure 5.41</em>) stays the same: you will have to calculate the squares of the distances across <em class="italic">all</em> dimensions—no matter how many they are—and sum them across.</p>
				</div>
			</div>
			<p class="normal">Going back to the real estate sector for a second, let's imagine that we have a dataset describing 16 properties utilizing their price per square meter and their age in years (see <em class="italic">Figure 5.42</em> on the left). We want to cluster these properties in three homogeneous clusters. The business reason we want to create such a cluster is immediate: should a client show interest in any of these properties, we want to immediately recommend considering all other properties in the same cluster since they should exhibit <em class="italic">similar</em> features. This<a id="_idIndexMarker562"/> example looks naïve<a id="_idIndexMarker563"/> with 16 properties: we wouldn't need k-means to identify similarities with so little data involved. However, the beauty of k-means is that it could easily scale to many dimensions and properties, while our human brain would start struggling with a few more data points:</p>
			<p class="normal"><img src="image/B17125_05_42.png" alt="Chart, scatter chart&#10;&#10;Description automatically generated"/></p>
			<p class="packt_figref">Figure 5.42: Kicking k-means off: out of the 16 properties with different prices and ages (left), three are randomly picked and elected as initial centroids (right)</p>
			<p class="normal">The first step to run is the initialization: the algorithm will draw at random three properties, as three is the number of requested clusters (<em class="italic">k=3</em>). The algorithm has randomly extracted properties <strong class="keyword">C</strong>, <strong class="keyword">G</strong>, and <strong class="keyword">I</strong>, as you can see on the right side of <em class="italic">Figure 5.42</em>. As part of the first iteration, the algorithm will proceed with the grouping step: first, it will use the Pythagorean theorem to calculate the distances between each property and each centroid and will associate every property to its closest centroid out of the three. Let's follow how k-means proceeds at each iteration with the help of the figures. As you can see in the left handside of <em class="italic">Figure 5.43</em>, the grouping step has created three first cluster compositions, each one represented by a different color. The blue-colored properties (<strong class="keyword">C</strong>, <strong class="keyword">A</strong>, <strong class="keyword">B</strong>, and <strong class="keyword">D</strong>) are the closest ones to the blue centroid that overlaps with property <strong class="keyword">C</strong>. The ones belonging to the red cluster (<strong class="keyword">G</strong>, <strong class="keyword">E</strong>, <strong class="keyword">F</strong>, and <strong class="keyword">H</strong>) are, instead, closest to the red centroid, <strong class="keyword">G</strong>. Finally, the green cluster is made of the points (<strong class="keyword">I</strong>, <strong class="keyword">L</strong>, <strong class="keyword">M</strong>, <strong class="keyword">N</strong>, <strong class="keyword">O</strong>, <strong class="keyword">P</strong>, <strong class="keyword">Q</strong>, and <strong class="keyword">R</strong>) whose closest centroid is <strong class="keyword">I</strong>. The next step for the algorithm is to update the centroids: considering the points falling into each cluster, it will be enough to calculate the actual center of mass of the cluster by averaging out the prices and the ages of the properties belonging to it. For example, let's look at the green cluster: the properties forming this cluster tend to be older, leading the new centroid to be placed on the right side of the scatter plot. The centroid in the red cluster has instead moved toward the top: indeed, the properties associated with this cluster all have in common a higher price compared to point <strong class="keyword">C</strong> (the old centroid):</p>
			<figure class="mediaobject"><img src="image/B17125_05_43.png" alt="Chart, scatter chart&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.43: The first full iteration of k-means: with the update step, the centroids make a move</p>
			<p class="normal">Now we can <a id="_idIndexMarker564"/>finally start the <a id="_idIndexMarker565"/>second iteration (<em class="italic">Figure 5.44</em>). Once again, we begin by grouping the points using the centroid we have just recalculated. As a consequence of this shift in the centroids, the clusters have changed, and some properties switched color: for instance, property <strong class="keyword">E</strong> used to be red and is now blue as its closest centroid is now the blue one, and no longer the red one. The same applies to points <strong class="keyword">I</strong> and <strong class="keyword">L</strong>, which used to be green and are now red. It could appear that our algorithm has taken the right road as it is converging to a solution that makes sense: after this iteration, the clusters have changed in a way that makes their elements closer to each other. In the second step of the iteration, the algorithm will again update the centroids, taking into account the new compositions of the clusters. The most remarkable change is now in the red cluster, whose centroid has moved toward the bottom (where prices are lower), given the addition of properties <strong class="keyword">I</strong> and <strong class="keyword">L</strong> to the group:</p>
			<figure class="mediaobject"><img src="image/B17125_05_44.png" alt="Chart, scatter chart&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.44: The second iteration of k-means: the groups make more and more sense</p>
			<p class="normal">In the <a id="_idIndexMarker566"/>third iteration (<em class="italic">Figure 5.45</em>), the<a id="_idIndexMarker567"/> algorithm repeats the grouping step, and other properties change color (for instance, <strong class="keyword">M</strong> moves from green to red, and <strong class="keyword">F</strong> becomes blue). However, something new happens: despite having updated the centroids, the composition of the cluster does not change at all. This is the sign that our algorithm has found a stable solution and can be terminated, returning our final cluster composition:</p>
			<figure class="mediaobject"><img src="image/B17125_05_45.png" alt="Chart&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.45: The third and last iteration of k-means: no more updates are possible and the algorithm converges</p>
			<p class="normal">This final cluster composition seems to be making a lot of sense. By looking at the scatter plots, we can also attempt a business interpretation of each cluster:</p>
			<ul>
				<li class="bullet">Blue properties (<strong class="keyword">A</strong>, <strong class="keyword">B</strong>, <strong class="keyword">C</strong>, <strong class="keyword">D</strong>, <strong class="keyword">E</strong>, and <strong class="keyword">F</strong>) are in the top-left corner of our diagram. They were all recently built and, as new properties, they tend to display a higher price than the rest.</li>
				<li class="bullet">Red properties (<strong class="keyword">G</strong>, <strong class="keyword">H</strong>, <strong class="keyword">I</strong>, <strong class="keyword">L</strong>, and <strong class="keyword">M</strong>) are in the bottom central part of the diagram and refer to buildings built in the seventies with lower quality materials; hence, their price is more accessible.</li>
				<li class="bullet">Finally, the green points (<strong class="keyword">N</strong>, <strong class="keyword">O</strong>, <strong class="keyword">P</strong>, <strong class="keyword">Q</strong>, and <strong class="keyword">R</strong>) are associated with older buildings, which tend to be more prestigious and come with a higher price tag.</li>
			</ul>
			<p class="normal">The clusters we obtained after only a handful of iterations of the k-means algorithm can certainly help real estate agents present convincing alternatives to potential buyers. Not bad for an algorithm repeating a set of simple mathematical steps.</p>
			<div>
				<div id="_idContainer248" class="packt_tip">
					<p class="Tip--PACKT-">A natural question that comes to mind when using k-means is: what is the right value of <em class="italic">k</em> or, in other words, how many clusters should I create? Even though there are some numerical techniques (check <a id="_idIndexMarker568"/>out the <strong class="keyword">Elbow method</strong>, for instance) to infer an optimal value for <em class="italic">k</em>, the business practice of machine learning <a id="_idIndexMarker569"/>demands<a id="_idIndexMarker570"/> taking another, less mathematically rigorous approach. When choosing the number of clusters, the advice is to take a step back and think of the actual business utilization of the cluster definitions. The right question to ask becomes: how many clusters shall I create so that the result can be used in practice in my business case? In the example of segmenting consumers for personalizing communication, is it reasonable to create—let's say—100 clusters of consumers if I can only afford to produce three versions of a newsletter at most? We will often use the business constraints for deciding a range of reasonable values of <em class="italic">k</em> and then pick the one that looks most interpretable. The moral of this story is that data analytics is a mix of art and science, and human judgment is often needed to guide algorithms to the right path.</p>
				</div>
			</div>
			<p class="normal">Before moving back to our tutorial flow, let's go through a couple of considerations regarding "what can go wrong" when using a distance-based approach like k-means and how to avoid it:</p>
			<ul>
				<li class="bullet"><strong class="keyword">Outliers can spoil the game</strong>. If some points in your dataset exhibit extreme values, they will naturally "stay apart" from the rest, making the clustering exercise less meaningful. For example, imagine that in our real estate case, we have a single property with a price ten times higher than every other property: this exceptional property will probably make a cluster by itself. Most times, we don't want this to happen, so we remove outliers upfront. The <strong class="keyword">Numeric Outliers</strong> node will do the job for us.</li>
				<li class="bullet"><strong class="keyword">Extreme range differences can make distance calculations unbalanced</strong>. This one is easy to see through an example. Think again about the formula in <em class="italic">Figure 5.41</em> for calculating distances: in the real estate example, it would leverage the differences in house prices (which are in the thousands of dollars) and the age differences (which, instead, vary in the area of dozens of years). The massive gap between the two orders of magnitude becomes even wider when you square them, as the formula provides. This means that the house prices will count disproportionally more than the age, making the latter almost meaningless. To fix this numeric disadvantage, we need to normalize all the measures used in k-means and reduce their scale to a common range (generally from zero to one) while keeping the differences across data points. This is what the <strong class="keyword">Normalizer</strong> node (and its inverse companion, the <strong class="keyword">Denormalizer</strong> node) will do for us in KNIME.</li>
			</ul>
			<p class="normal">To avoid these issues, remember this general advice: always remove outliers and normalize your data before applying k-means. With more practice and expertise, you might be able to "bend" these<a id="_idIndexMarker571"/> rules<a id="_idIndexMarker572"/> to meet your specific business needs at best, but in most cases, these two steps can only improve your clustering results, so they are no-brainers. Let's now see how to apply them in KNIME.</p>
			<h2 id="_idParaDest-104" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor106"/><img src="image/image079.png" alt="A picture containing icon&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Numeric Outliers</em></h2>
			<p class="normal">This <a id="_idIndexMarker573"/>node (<strong class="screenText">Analytics &gt; Statistics</strong>) identifies <a id="_idIndexMarker574"/>outliers in a data table and manages them according to the needs. At the top of its configuration window (<em class="italic">Figure 5.46</em>), you can select which columns to consider in the outliers detection:</p>
			<figure class="mediaobject"><img src="image/B17125_05_46.png" alt="Graphical user interface&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.46: Configuration window of Numeric Outliers node: what do you want to do with your extreme values?</p>
			<p class="normal">In <a id="_idIndexMarker575"/>the <strong class="screenText">Outlier Treatment</strong> panel <a id="_idIndexMarker576"/>on the bottom right, you can decide how to manage outliers once detected. In particular, the <strong class="screenText">Treatment option</strong> drop-down menu lets you choose whether you want to <strong class="screenText">Remove outlier rows</strong> (so as to ignore them in the rest of the workflow), <strong class="screenText">Remove non-outlier rows</strong> (so you keep <em class="italic">only</em> the outliers and study them further), or <strong class="screenText">Replace outliers values</strong> (by either assigning them a missing value status or substituting them with the closest value within the permitted range—you can specify your preference in the <strong class="screenText">Replacement strategy</strong> menu).</p>
			<p class="normal">The key parameter for setting the sensitivity to use in detecting outliers is the <strong class="screenText">Interquartile range multiplier (k)</strong>, which you can set on the bottom-left area of the configuration window. To understand how it works, have a look at the <strong class="keyword">box plot </strong>shown in <em class="italic">Figure 5.47</em>:</p>
			<figure class="mediaobject"><img src="image/B17125_05_47.png" alt="Diagram&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.47: How to interpret a box-and-whisker plot: the box in the middle covers the central 50% of points in a distribution. Beyond the whiskers, you find outliers</p>
			<p class="normal">Box plots <a id="_idIndexMarker577"/>show us at a glance<a id="_idIndexMarker578"/> the key features of a numeric distribution: quartile values (see in the picture <strong class="keyword">Q1</strong>, <strong class="keyword">Q2</strong>, which is the <strong class="keyword">Median</strong>, and <strong class="keyword">Q3</strong>) tell us where we could "cut" a population of sorted numbers so as to get 25% of the values in each slice. Now, look at the central box, whose<a id="_idIndexMarker579"/> length is called <strong class="keyword">Interquartile range</strong> (<strong class="keyword">IQR</strong>): within this range, we will find nearly 50% of the values of the population—this is the <em class="italic">core</em> of our distribution. Keeping this in mind, outliers can be defined as the values that lie <em class="italic">far</em> from this core. Typically, the values that are further than 1.5 times the interquartile range above the third quartile or below the first quartile are<a id="_idIndexMarker580"/> considered <strong class="keyword">mild outliers</strong>. </p>
			<p class="normal">They are represented as circles in <em class="italic">Figure 5.47</em>, while the limit of mild outliers is represented by the dashed "whiskers" you see above and below the central box (this is why box plots are also known as box-and-whisker plots). If you increase the multiplier of the interquartile range to 3.0, you find the <strong class="keyword">extreme outliers</strong>, which<a id="_idIndexMarker581"/> are shown as crosses in the figure. By editing the interquartile range multiplier parameter in the configuration dialog, you can tell the node how "aggressive" it should be in detecting outliers.</p>
			<p class="normal">Let's leverage our new node straight away on the CRM dataset:</p>
			<ol>
				<li class="numbered" value="2">Implement a <strong class="keyword">Numeric Outliers</strong> node and connect it with the output port of the CSV reader. In its configuration window, deselect the column <em class="italic">Customer_ID</em> since we don't want to use it in our clustering. Since we are after extreme outliers, set <code class="Code-In-Text--PACKT-">3.0</code> as the <strong class="screenText">Interquartile range multiplier (k)</strong>, and select <strong class="screenText">Remove outlier rows</strong> as the <strong class="screenText">Treatment option</strong>. Finally, execute the node and have a look at its output ports.<p class="normal">The first output (<strong class="screenText">Treated table</strong>) is the cleaned-up version of the table, showing only 3,772 rows: this <a id="_idIndexMarker582"/>means that we <a id="_idIndexMarker583"/>removed 10% of rows as they were considered outliers according to some columns. We could have played with the IQR multiplier value and increased it to 5.0 or more, so as to focus on more extreme values and remove fewer rows, but for the sake of this exercise, we can carry on with this. The second output of the node (<strong class="screenText">Summary</strong>, shown in <em class="italic">Figure 5.48</em>) tells us the number of rows regarded as outliers according to each individual column (<em class="italic">Basket Size</em> seems to be the one displaying more extreme values):</p><figure class="mediaobject"><img src="image/B17125_05_48.png" alt="Graphical user interface, table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.48: Summary output view of the Numeric Outliers node: which columns are causing most of the outliers?</p>
</li>
</ol>

<p class="normal">Let's proceed with the second preparation step before applying k-means: normalize the data to a set range through the <strong class="keyword">Normalizer</strong> node.</p>

<h2 id="_idParaDest-105" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor107"/><img src="image/image083.png" alt="Diagram, schematic&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Normalizer</em></h2><p class="normal">This <a id="_idIndexMarker584"/>node (<strong class="screenText">Manipulation &gt; Column &gt; Transform</strong> in the<a id="_idIndexMarker585"/> node repository) normalizes all values in selected numerical columns of a dataset. In its configuration window (<em class="italic">Figure 5.49</em>), you first choose which columns to normalize and, then, pick a normalization method. The most useful one (especially indicated in conjunction with distance-based procedures like k-means clustering) is the <strong class="screenText">Min-Max Normalization</strong>, which linearly projects the original range onto a predefined range (usually 0 to 1, but you can manually edit the boundaries using the text boxes provided). With this normalization approach, the original minimum value is transformed to 0, the maximum to 1, and everything in the middle is proportionally assigned to a value within the 0 to 1 range. Another popular normalization method is the <strong class="screenText">Z-Score Normalization (Gaussian)</strong>, also <a id="_idIndexMarker586"/>known as <strong class="keyword">Standardization</strong>. Using this method, each value is transformed into the number of standard deviations by which it is above or below the population's mean. For instance, a Z-score of –3 means that the value is three standard deviations below the population's average. This is useful when you want to assess how much your points deviate from their mean:</p><figure class="mediaobject"><img src="image/B17125_05_49.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.49: Configuration window of the Normalizer node:select the columns to normalize and the method to apply</p><p class="normal">The node <a id="_idIndexMarker587"/>has two outputs: the<a id="_idIndexMarker588"/> upper output port returns the table with normalized values, and the bottom (the cyan square) holds the normalization model. Such a model can restate the original values using the <strong class="keyword">Denormalizer</strong> node, which we will encounter in a few pages.</p><p class="normal">We now have all we need to proceed and normalize our outliers-less CRM data</p>
<ol>
				<li class="numbered" value="3">Pick the <strong class="keyword">Normalizer</strong> node from the repository and connect its input to the first output of <strong class="keyword">Numeric Outliers</strong>. The node configuration is straightforward: exclude the <em class="italic">Customer­­_ID</em> column from the normalization process by double-clicking on it and making sure it appears on the red box on the right. The default settings of the normalization method work well for us: indeed, the<strong class="screenText"> Min-Max Normalization</strong> with a range between 0 and 1 is great for calculating distances with algorithms such as k-means. Finally, click on <strong class="screenText">OK </strong>and execute the node.<p class="normal">If you look at the first output of the <strong class="keyword">Normalizer </strong>node, you will notice how the values of the affected columns are now falling in the desired range, which is exactly what we needed. Now, all columns will have the same weight in calculating<a id="_idIndexMarker589"/> distances <a id="_idIndexMarker590"/>based on the Pythagorean theorem. We can finally move on and introduce the critical node of the workflow, allowing us to cluster our customers: <strong class="keyword">k-Means</strong>.</p>
</li>
</ol>

<h2 id="_idParaDest-106" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor108"/><img src="image/image0851.png" alt="Diagram&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">k-Means</em></h2><p class="normal">This<a id="_idIndexMarker591"/> node (<strong class="screenText">Analytics &gt; Mining &gt; Clustering</strong>) clusters <a id="_idIndexMarker592"/>the rows of the input table using the k-means algorithm. The first parameter to be set as part of its configuration (<em class="italic">Figure 5.50</em>) is the <strong class="screenText">Number of clusters</strong>, which can be chosen by entering an integer in the textbox at the very top. You can then choose the method for the <strong class="screenText">Centroid initialization</strong>, which, by default, happens by random draw (you can still set a static random seed to make the process repeatable), and the maximum number of iterations used to force termination (it is preset to 99, which, in most cases, is good enough since k-means would naturally converge in fewer iterations). The last configuration step is to choose which numeric columns to consider when clustering, which can be done using the <strong class="screenText">Column selection</strong> panel at the bottom:</p><figure class="mediaobject"><img src="image/B17125_05_50.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.50: Configuration window of the k-Means node: select the columns to normalize and the method to apply</p><p class="normal">Let's apply <a id="_idIndexMarker593"/>our new node to the <a id="_idIndexMarker594"/>normalized data and see what happens.</p>

<ol>
				<li class="numbered" value="4">Implement the <strong class="keyword">k-Means</strong> node and connect it downstream to the first output of the <strong class="keyword">Normalizer</strong> node. We can keep its configuration simple, ensuring that the <strong class="screenText">Number of clusters</strong> is set to 3 and deselecting <em class="italic">Customer_ID</em> from the list since we don't want to consider the column in the clustering exercise. Click on <strong class="screenText">OK </strong>and then execute the node and open its main view (<em class="italic">Shift</em> + <em class="italic">F10</em>, or right-click and then select <strong class="screenText">Execute and Open Views...</strong>).<p class="normal">The main view of the <strong class="keyword">k-Means </strong>node (right-click on the node and then select <strong class="screenText">View: Cluster View</strong> to make it appear if needed) will look similar to what you find in <em class="italic">Figure 5.51</em>:</p><figure class="mediaobject"><img src="image/B17125_05_51.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.51: Summary view of the k-Means node: we can start seeing what the three clusters look like</p><p class="normal">This <a id="_idIndexMarker595"/>summary view is already<a id="_idIndexMarker596"/> telling us a lot: k-means segmented our customer base into three different groups of 830, 1,126, and 1,816 customers, respectively (see the <strong class="screenText">coverage</strong> labels in the figure). If you open the different clusters (click on the <strong class="screenText">+</strong> button on the left), you find a numeric description of the three centroids. According to what you see in <em class="italic">Figure 5.51</em>, for example, the first cluster (generically named <strong class="screenText">cluster_0</strong> by KNIME) shows the smallest <em class="italic">Basket Size</em> of the three and the highest <em class="italic">Unique Products</em>. If you open the first output port of the node (right-click on the node and then select <strong class="screenText">Labeled input</strong>), you will see that every row has been assigned to one of the three clusters, as indicated in the additional <em class="italic">Cluster</em> column (see <em class="italic">Figure 5.52</em>):</p><figure class="mediaobject"><img src="image/B17125_05_52.png" alt="Table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.52: Output of the k-Means node: every customer—whether they like it or not—gets assigned to a cluster</p>
</li>
</ol>

<p class="normal">As <a id="_idIndexMarker597"/>aligned with our <a id="_idIndexMarker598"/>business partner, the CRM manager, we need to go one step ahead and build a couple of visualizations to simplify the process of interpreting our clustering results.</p><p class="normal">Before doing that, we realize that our values are still normalized and forced to fall within the 0 to 1 range. To make our visuals easier to interpret, we would prefer to come back to the original scales instead. To do so, we can revert the normalization by leveraging the <strong class="keyword">Denormalizer </strong>node.</p>


<h2 id="_idParaDest-107" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor109"/><img src="image/image087.png" alt="Diagram, schematic&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Denormalizer</em></h2><p class="normal">This<a id="_idIndexMarker599"/> node (<strong class="screenText">Manipulation &gt; Column &gt; Transform</strong>) brings <a id="_idIndexMarker600"/>the values in a dataset back to their original range. It requires two input connections: the first one is the model generated by the previous <strong class="keyword">Normalizer</strong> node, which carries a description of the normalization method and parameters. The second input is the normalized table to be denormalized. The node does not require any configuration.</p>
<ol>
				<li class="numbered" value="5">Implement the <strong class="keyword">Denormalizer</strong> node and set up the wiring. The cyan output of the <strong class="keyword">Normalizer</strong> node should be connected to the first input of the <strong class="keyword">Denormalizer node. </strong>The first output of the <strong class="keyword">k-Means</strong> node should be connected, instead, to the second input port of the <strong class="keyword">Denormalizer</strong> node. You can have a sneak view of the final workflow in <em class="italic">Figure 5.57</em> to see how to get the connections right. After executing the node, you can see how the values have been reverted to their original range.
</li>
</ol>

<p class="normal">To build the visuals, we will need three more nodes. The first one (<strong class="keyword">Color Manager</strong>) is required for assigning colors to the various rows of the dataset (according to the cluster), while the other two (<strong class="keyword">Scatter Matrix (local)</strong> and <strong class="keyword">Conditional Box Plot</strong>) will generate<a id="_idIndexMarker601"/> a couple of nice<a id="_idIndexMarker602"/> charts.</p><h2 id="_idParaDest-108" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor110"/><img src="image/Image73183.png" alt="A picture containing icon&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Color Manager</em></h2><p class="normal">This <a id="_idIndexMarker603"/>node (<strong class="screenText">Views &gt; Property</strong>) assigns<a id="_idIndexMarker604"/> colors to each row of a dataset. Its configuration window (<em class="italic">Figure 5.53</em>) asks you to select two things. First, you specify the nominal column used to evaluate what color to assign: every possible value associated with that column will correspond to a specific color. Second, you need to select the color set to adopt. On top of the three default color sets, you can also manually define which color to assign to each possible value of the nominal column. To do so, you will have to select <strong class="screenText">Custom </strong>in the <strong class="screenText">Palettes</strong> tab and then use one of the tabs on the right (such as <strong class="screenText">Swatches</strong>, <strong class="screenText">RGB</strong>, and <strong class="screenText">CMYK</strong>) to pick the right color for each nominal value manually:</p><figure class="mediaobject"><img src="image/B17125_05_53.png" alt="Graphical user interface, chart&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.53: Configuration of the Color Manager node: you can pick which color to assign to which value of the nominal column of your choice</p>
<ol>
				<li class="numbered" value="6">Add a<a id="_idIndexMarker605"/> <strong class="keyword">Color Manager</strong> <a id="_idIndexMarker606"/>node and connect it to the output of the <strong class="keyword">Denormalizer</strong>. Confirm the <em class="italic">Cluster</em> column in the drop-down menu at the top, and then select the color set of your choice. In the specific example of <em class="italic">Figure 5.53</em>, a custom palette has been manually created so that blue, orange, and green could be assigned to the three clusters.<p class="normal">Now that the colors are set, it's finally time to pull together the first chart with the help of a new node.</p>
</li>
</ol>

<h2 id="_idParaDest-109" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor111"/><img src="image/image084.png" alt="A close-up of a logo&#10;&#10;Description automatically generated with low confidence"/> </span><em class="bold-italic">Scatter Matrix (local)</em></h2><p class="normal">This<a id="_idIndexMarker607"/> node (<strong class="screenText">Views &gt; Local (Swing)</strong>) generates<a id="_idIndexMarker608"/> a matrix of scatter plots, displaying multiple combinations of variables in a single view. The node does not require any configuration, but you can optionally increase the maximum number of points that will be plotted.</p>
<ol>
				<li class="numbered" value="7">Implement the <strong class="keyword">Scatter Matrix (local)</strong> node after <strong class="keyword">Color Manager</strong>. Execute and open its main view (<em class="italic">F10</em> after selecting the node). From the <strong class="screenText">Column Selection </strong>tab at the bottom, you can choose which variables to display. In our case, let's make sure we have only <em class="italic">Average Price</em>, <em class="italic">Basket Size</em>, and <em class="italic">Unique Products</em> selected on the right: you will end up with a visual similar to <em class="italic">Figure 5.54</em>:<figure class="mediaobject"><img src="image/B17125_05_54.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.54: Output view of the Scatter Matrix (local) node: your customers have become colored points. By looking at how the cloud of dots is scattered, you can interpret what each cluster is all about</p>
</li>
</ol>

<p class="normal">The scatter matrix we just obtained renders the result of the clustering in a more human-friendly way. As we look at it together with the CRM manager, we notice some initial clear patterns. For example, look at the chart at the top-right corner of <em class="italic">Figure 5.54</em>, which shows <em class="italic">Average Price</em> on the vertical axis and <em class="italic">Unique Products</em> on the horizontal axis. The blue cluster (cluster_0) clearly dominates the right-hand side of the chart, confirming that this is the segment of consumers that tend to try a more diverse set of products (high values of <em class="italic">Unique Products</em>). At the same time, the orange cluster (cluster_1) has customers that seem to go for less unique products and lower prices. Instead, the green cluster (cluster_2) includes those willing to pay more premium prices when shopping <a id="_idIndexMarker609"/>at our<a id="_idIndexMarker610"/> website. This is all starting to make sense, and the visual is already a big help in understanding how our clustering worked.</p><p class="normal">Let's add one last visual to clarify even further the composition of our segments: meet the <strong class="keyword">Conditional Box Plot</strong> node.</p>


<h2 id="_idParaDest-110" class="title" lang="en-GB" xml:lang="en-GB"><span class="mediaobject"><a id="_idTextAnchor112"/><img src="image/image086.png" alt="A picture containing text&#10;&#10;Description automatically generated"/> </span><em class="bold-italic">Conditional Box Plot</em></h2><p class="normal">This <a id="_idIndexMarker611"/>node (<strong class="screenText">Views &gt; JavaScript</strong>) produces <a id="_idIndexMarker612"/>an interactive view with multiple box plots, one for each value in a given categorical column. Such a view enables the parallel comparison of distributions. Its configuration window (<em class="italic">Figure 5.55</em>) requires selecting the <strong class="screenText">Category Column </strong>to be used for differentiating parallel box plots and the choice of the numeric columns whose distribution will be visualized:</p><figure class="mediaobject"><img src="image/B17125_05_55.png" alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 5.55: Configuration dialog of the Conditional Box Plot node: which distributions are you interested in comparing between?</p>

<ol>
				<li class="numbered" value="8">Drag <a id="_idIndexMarker613"/>and<a id="_idIndexMarker614"/> drop the <strong class="keyword">Conditional Box Plot </strong>node onto the workflow editor and connect it to the output port of the <strong class="keyword">Denormalizer </strong>node. Select <em class="italic">Cluster </em>as <strong class="screenText">Category Column</strong> and ensure that only <strong class="screenText">Average Price</strong>, <strong class="screenText">Basket Size</strong>, and <strong class="screenText">Unique Products </strong>are on the right of the column selector placed at the center of the dialog. Click on <strong class="screenText">OK</strong> and then press <em class="italic">Shift</em> + <em class="italic">F10</em> to execute it and open its main view. In the interactive window that appears, you can swap which distribution to visualize by operating on the <strong class="screenText">Selected Column </strong>drop-down menu: you can find this selector by clicking on the icon at the far top-right of the interactive window.</li>
			</ol>
			<p class="normal">The output views of the <strong class="keyword">Conditional Box Plot</strong> node (<em class="italic">Figure 5.56</em>) clarify even better the essential features of each cluster. The k-means algorithm was able to produce three homogeneous clusters with peculiar and differentiating characteristics. The box plots are great at showing such differences. As an example, take the third plot in the figure, which refers to <em class="italic">Unique Products</em>. The blue cluster dominates when it comes to this measure: the median number of unique products purchased by customers belonging to this segment is 32, while for the others it is near 10. The lack of visual overlap in height between the blue box and the other two means this difference is meaningful. On the other hand, the orange and the green clusters seem to be quite similar in terms of unique<a id="_idIndexMarker615"/> products, as<a id="_idIndexMarker616"/> the boxes are almost coinciding:</p>
			<figure class="mediaobject"><img src="image/B17125_05_56.png" alt="Chart, box and whisker chart&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.56: Outputs of the Conditional Box Plot node: you can readily appreciate the differences in the distributions across clusters</p>
			<p class="normal">We can now sit together with the CRM manager and, having the scatter matrix and the conditional box plots at hand, we can finally describe each customer segment and give a business-oriented interpretation of their meaning:</p>
			<ul>
				<li class="bullet">The blue cluster includes those <strong class="keyword">curious customers</strong> who are willing to try different products. In our communication with this segment, we can give disproportionate space to the "new arrivals" and intrigue them with an ample selection of products they haven't tried yet.</li>
				<li class="bullet">The orange cluster possibly comprises <strong class="keyword">small retailers</strong> who buy "in bulk" from our website to resell their shops. They tend to buy relatively few products but in large quantities. We can offer them quantity discounts and regularly communicate the list of best-selling products, hopefully leading them to add our best-selling articles to their assortment for mutual business growth.</li>
				<li class="bullet">The green cluster is made up of our <strong class="keyword">high-value customers</strong>, who systematically put <a id="_idIndexMarker617"/>quality<a id="_idIndexMarker618"/> ahead of price in their shopping choices. Therefore, when communicating with them, we should advertise the premium end of the products portfolio and focus on topics such as the quality and the safety of our assortment, deprioritizing price-cut offers, and other types of promotional levers.</li>
			</ul>
			<p class="normal">By using only 8 KNIME nodes (see the full workflow in <em class="italic">Figure 5.57</em>), we came up with a simple segmentation of customers and a first proposition of how to drive the most value when personalizing their experience. By uniting the business expertise of our partners (the CRM managers in this case) with the power of data and algorithms, such as k-means, we can make the magic happen!</p>
			<figure class="mediaobject"><img src="image/B17125_05_57.png" alt="Diagram&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 5.57: The full workflow for segmenting consumers using clustering</p>
			<h1 id="_idParaDest-111" class="title"><a id="_idTextAnchor113"/>Summary</h1>
			<p class="normal">In this chapter, you touched on the serious potential behind data analytics and machine learning with your own hands. You have solved three real-world problems by putting data and algorithms at work. </p>
			<p class="normal">In the first tutorial, you managed to predict with a decent level of accuracy the rental price of properties, collecting, in the process, a few interesting insights into real estate price formation. You have now acquired a proven methodology, based on the linear regression model, that you can replicate on many business cases where you have to predict numeric quantities.</p>
			<p class="normal">In the second tutorial, you entered the fascinating world of classification and propensity modeling, experiencing firsthand the game-changing role of data analytics in marketing. You were able to put together a couple of classification models through which you met multiple business needs. First, you were able to reveal the "unwritten rules" that make a product generally attractive to customers by building and interpreting a decision tree model. Then, you built a random forest model that proved effective in anticipating the level of propensity of individual bank customers. Lastly, you managed to estimate the possible ROI of further marketing campaigns, unlocking serious value creation opportunities for a business. Also, in this case, you gained a series of general-purpose techniques that you can easily reapply in your own work every time you need to predict anything of business relevancy. By going through the tutorial, you also experienced the "back and forth" iterations needed to fine-tune machine learning models to fit your business needs.</p>
			<p class="normal">In our third tutorial, you experienced the power of unsupervised learning. You were able to put together a meaningful and straightforward customer segmentation that can be used to design personalized communication strategies and maximize the overall customer value. With this new algorithm, k-means, in your backpack, you can potentially cluster anything: stores, products, contracts, defects, events, virtually any business entity that can benefit from the algorithmic tidying that comes with clustering. Think about the value you can create by applying this new concept to the work items you deal with on a daily basis. In the process of learning k-means, we also got acquainted with the fundamental statistical concept of outliers and saw how to spot and manage them systematically.</p>
			<p class="normal">Let's now move on and learn how we can make our data accessible to our business partners through self-service dashboards. It's time to meet our next travel companion in the data analytics journey: Power BI.</p>
		</div>
	</body></html>