- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Machine Learning at Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ve heard a lot about creating business value with intelligent algorithms:
    it''s finally time to roll up our sleeves and make it happen. In this chapter,
    we are going to experience what it means to apply machine learning to tangible
    cases by going through a few step-by-step tutorials. Our companion KNIME is back
    on stage: we will learn how to build workflows for implementing machine learning
    models using real-world data. We are going to meet a few specific algorithms and
    learn the intuitive mechanisms behind how they operate. We''ll glimpse into their
    underlying mathematical models, focusing on the basics to comprehend their results
    and leverage them in our work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This practical chapter will answer several questions, including:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I make predictions using supervised machine learning algorithms in KNIME?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I check whether a model is performing well?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we avoid the risk of overfitting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What techniques can I use to improve the performance of a model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I group similar elements together using clustering algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tutorials included in this chapter cover three of the most recurrent cases
    when you can rely on machine learning as part of your work: predicting numbers,
    classifying entities, and grouping elements. Think of them as "templates" that
    you can widely reapply after you reach the last page of the chapter and that you
    are likely to keep using as a reference. The steps of the tutorials are also organized
    in the same order they would unfold in everyday practice, including the "back
    and forth" iterations required for improving the performance of your model. This
    will prepare you to face the actual use of real-life machine learning, which often
    follows a circuitous route made of trial and error attempts.'
  prefs: []
  type: TYPE_NORMAL
- en: Within each tutorial, you will encounter one or two machine learning algorithms
    (specifically, **linear regression** in the first, **decision tree** and **random
    forest** in the second, and **k-means** in the third) that will be introduced
    and explained before being seen in action. Let's get started with some first predictions!
  prefs: []
  type: TYPE_NORMAL
- en: Predicting numbers through regressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this tutorial, you will assume the—somewhat—enviable role of a real estate
    agent based in Rome, Italy. The company you work for owns multiple agencies specialized
    in rentals of properties located in the broader metropolitan area of the Eternal
    City. Your passion for data analytics got you noticed by the CEO: she asked you
    to figure out a way to support agents in objectively evaluating the fair monthly
    rent of a property based on its features. She noticed that the business greatly
    suffers when the rent set for a property is not aligned with the market. In fact,
    if the rent is too low, the agency fee (which is a fixed percentage of the agreed
    rent) will end up being lower than what it could have been, leaving profit on
    the table. On the other hand, if the ask is too high, revenues for the agency
    will take longer to materialize, causing a substantial impact on the cash flow.
    The traditional approach to set the monthly rent for new properties is a "negotiation"
    between owners and agents, who will use their market understanding (and sometimes
    the benchmark of similar properties) to convince the owners about the right rent
    to ask for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are sure that machine learning has the potential to make a difference,
    and you are resolute in finding an ML way to improve this business process. The
    idea that comes to mind is to use the database of the monthly rent of previously
    rented properties (for which we have available their full description) to predict
    the right monthly rent of future properties based on their objective characteristics.
    Such a data-driven approach, if well communicated, can ease the price-setting
    process and result in a mutual advantage for all the parties involved: the landlord
    and the agency will get a quick and profitable transaction, and the tenant will
    obtain a fair rent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prospect of building a machine able to predict rental prices is exhilarating
    and makes you impatient to start. You manage to obtain an extraction of the last
    4,000 rental agreements signed at the agency (`RomeHousing-History.xlsx`). The
    table contains, for each property:'
  prefs: []
  type: TYPE_NORMAL
- en: '*House_ID*: a unique identifier of the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neighborhood*: the name of the area where the property lies, ranging from
    the fancy surroundings of `Piazza` `Navona` to the tranquil, lakeside towns of
    `Castelli` `Romani`. *Figure 5.1* shows a map of the Rome area with some of these
    neighborhoods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Property_type*: a string clarifying if the property is a `flat`, a `house`,
    a `villa`, or a `penthouse`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rooms*: the number of available rooms in the property, including bathrooms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Surface*: the usable floor area of the property in square meters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elevator*: a binary category indicating if an elevator is available (`1`)
    or not (`0`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Floor_type*: a category showing if the property is on a `Mezzanine`, a `Ground`
    floor, or an `Upper` level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Floor_number*: the floor number on which the property is situated, based on
    the European convention (`0` is for the ground floor, `0.5` is the mezzanine,
    `1` is for the first level above the ground, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rent*: the all-inclusive, monthly rent in euros on the final rental agreement.![Map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_01.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.1: The Rome neighborhoods covered by our real estate. Have you visited
    any of these places already?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before building the model, you wisely stop for a second and think through the
    ways you are going to practically leverage it once ready. You realize that the
    potential business value for completing this endeavor is two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, by interpreting how the model works, you can find out some insightful
    evidence on the market price formation mechanisms. You might be able to find answers
    to the questions: *what features really do make a difference in the pricing?*,
    *does the floor number impact the value greatly?*, and *which neighborhoods prove
    to be most expensive ones, at parity of all other characteristics of the property?*.
    Some of the answers will reinforce the market understanding that your agency already
    has, adding the benefit of making this knowledge explicit and formally described.
    More interestingly, other findings might be truly unexpected and unveil original
    dynamics you did not know about.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, your model can be used to generate data-based recommendations on the
    rent to be set for new properties as they go on the market and enter the portfolio
    of the agency. To make things more interesting on this front, the owner shares
    with you a list (`RomeHousing-NewProperties.xlsx`) of 10 incoming properties for
    which the rental price has not been fixed yet, using the same features (such as
    *Neighborhood*, *Property_type*, and so on) available in the historical database.
    Once ready, you will apply your model to these sample properties as an illustration
    of how it works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You are now clear on what the business requires, and you can finally translate
    it into definite machine learning terms, building on what we have learned in the
    previous chapter. You need to build a machine that predicts "unknown" rental prices
    by learning from some "known" examples: the database of previously rented properties
    is your *labeled* dataset, as it has examples of your target variable, in this
    case, the *Rent*. Going through the catalog of machine learning algorithms (*Figure
    4.5*), you realize we are clearly in the category of *supervised* machine learning.
    More specifically, you need to predict numbers (rent in euros), so you definitely
    need to leverage an algorithm for doing a *regression*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ML way to solve this business opportunity is now clear in front of your
    eyes: you can finally get KNIME started and create a new workflow (**File** |
    **New…** | **New KNIME Workflow**):'
  prefs: []
  type: TYPE_NORMAL
- en: As a very first step, you load your labeled dataset by dragging and dropping
    the file(`RomeHousing-History.xlsx`) into your blank workflow or by implementing
    the**Excel Reader**node. In either case, KNIME will have recognized the structure
    of the file, and you just need to accept its default configuration. After running
    the node, you obtain the dataset shown in *Figure 5.2*, where you find the nine
    columns you expected:![](img/B17125_05_02.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.2: Historical rental data loaded into KNIME: 4,000 properties to learn
    from'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you build a machine learning model, you will interact in various ways with
    the columns of your data table. It is sensible to get an understanding of what
    you are going to deal with by exploring the columns right at the beginning. Fortunately,
    the **Statistics** node helps as it displays at once the most important things
    you need to know about your columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/NEW_statistics_node.png) *Statistics*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Statistics**) calculates summary statistics for each
    column available in the input table. The checkbox appearing at the top of its
    configuration dialog (*Figure 5.3*) lets you decide whether to **Calculate median
    values** of the numeric columns: this calculation might be computationally expensive
    for large datasets, so you will tick it only if necessary. The column selector
    in the middle lets you decide which columns should be treated as **Nominal**.
    For these columns, the node will count the number of instances of each unique
    value: this is useful for categorical columns when you want to quickly assess
    the relative footprint of every category in a table. The main summary metrics
    calculated by the node are minimum (**Min**), average (**Mean**), **Median**,
    maximum (**Max**), standard deviations (**Std. Dev.**), **Skewness**, **Kurtosis**,
    count of non-numeric values such as missing values (**No. Missing**), and plus
    or minus infinite (**No. +∞**, **No. –∞**). The node will also output the histograms
    showing the distributions of the values and, for nominal columns, the list of
    the most and least numerous categories identified in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Skewness** and **Kurtosis** are certainly the least known summary statistics
    among the ones mentioned above. However, they are useful in telling you quickly
    how much the shape of a distribution differs from the iconic bell-shaped curve
    of a pure Gaussian distribution. Skewness tells you about the symmetry of the
    distribution: if it has a positive value, it is skewed on the left while if it
    has a negative value, it is skewed on the right. Kurtosis tells you about the
    flatness of the distribution: if negative it is flatter than a bell curve, while
    if positive it shows a sharper peak.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Configuration of Statistics: explore the data with its summary
    statistics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the **Statistics** node and connect it with the previous one. When
    configuring it, check the first box so we can have a look at the median values
    of the numeric columns. In the selector of the nominal values, keep only the string-typed
    columns (*Neighborhood*, *Property_type*, and *Floor_type*) plus *Elevator*. Although
    formally numeric, this latter column splits our samples into two categories, the
    properties equipped with the elevator and the ones missing it: it will be interesting
    to read a count of how many properties fall into each category, so we shall treat
    this column as nominal. If you run the node and display its main output (just
    press *Shift* + *10* or, after you execute the node, right-click on it and select
    **View: Statistics View**) you will obtain a window with three useful tabs. The
    first one (*Figure 5.4*) gives you all the highlights on the numeric columns:
    we learn that the average rent of the properties in our database is slightly above
    €1,000 and that the median floor surface is around 70 square meters. We also learn
    that there are no missing values: this is good news as we don''t need to engage
    in clean up chores:![A picture containing table'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_04.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.4: Numeric panel within the Statistics output: how are my numeric
    features distributed?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second and third (*Figure 5.5*) tabs tell you about the nominal columns:
    we learn that some neighborhoods (such as `Magliana` and `Portuense`) are much
    less represented in our dataset than others. By looking at the values in the *Property_type*
    column, we also learn that the vast majority of our rented properties have been
    flats:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_05.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.5: Top/bottom panel within the Statistics output: check the values
    of your categorical columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have explored the dataset and have become acquainted with the main
    characteristics of its columns, we can proceed with the fun part and design our
    model. To build a robust supervised machine learning model, we need to rely on
    the typical flow that we encountered in the previous chapter. Let''s refresh our
    memory on this critical point: in order to stay away from the trap of overfitting,
    we need to partition our labeled data into training and test sets, learn on the
    training set, predict on the test set, and—finally—assess the expected performance
    of the model by scoring the predicted values. You can go back to *Chapter 4*,
    *What is Machine Learning?*, and check *Figure 4.13* out to see once again the
    full process: we are always required to follow this approach when implementing
    a machine that can predict something useful. So, the very first step is to randomly
    partition all our labeled data rows into two separate subsets. This is exactly
    the "specialty" of our next node: **Partitioning**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image007.png) *Partitioning*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This node (**Manipulation > Row > Transform**) performs a row-wise split of
    the input table into two tables corresponding to the upper (first partition) and
    lower (second partition) output ports. The selector at the top of its configuration
    window (*Figure 5.6*) lets you set the size of the first partition (upper output
    port). You can either specify the number of rows to be included (**Absolute**)
    or the relative size of the partition in percentage points (**Relative[%]**).
    The second selector specifies the method used for splitting the rows into the
    two partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Take from top**: if you select this option, the split will happen according
    to the current sorting order. The top rows of the input table will end up in the
    first partition while all others, after a certain threshold, will go to the second.
    The position of the threshold depends on the size of the partition that you have
    already decided above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear sampling**: also, in this case, the order of the input table rows
    is preserved: every *n*^(th) row will go to an output port, alternating regularly
    across the two partitions. If, for instance, you run a linear sampling for creating
    two equally sized partitions (each having half of the original rows), you will
    end up with all the odd rows in a partition and all the even ones in the other.
    If, instead, the split is one-third and two-thirds, you will have every third
    row in the first partition and all others in the second one. This is particularly
    useful when your dataset is a time series, with records sorted chronologically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Draw randomly**: if you go for this option, you obtain a random sampling.
    The only thing you can be sure of is that the number of rows in the first partition
    will be exactly what you have set in the first selector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stratified sampling**: in this case, you also run a random sampling but,
    you force the distribution of a nominal column to be preserved in both output
    partitions. For example: if you have an input table describing 1,000 patients,
    out of which 90% are labeled as `negative` and 10% as `positive`, you can use
    stratified sampling to retain the ratio between positive and negative patients
    in each partition. In this case, if you want to have 700 rows to go to the first
    partition, you will end up with exactly 630 negative patients and 70 positive
    ones: the proportion is kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you have selected a splitting method based on a random selection (the last
    two options in the list above), you can protect the reproducibility of your workflow
    by ticking the **Use random seed** optional box. When you specify a constant number
    for initializing the random sampling, you are "fixing" the random behavior: as
    a result, you will always obtain the same partitions every time you execute the
    node. This is handy when you want to keep the partitioning constant as you go
    back and forth in the construction of your workflow or when you want other people
    to get the same partitioning on their machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_06.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.6: Configuration dialog of Partitioning: how do you want to split
    your dataset?'
  prefs: []
  type: TYPE_NORMAL
- en: One thing that computers really struggle with is behaving randomly and doing
    anything "unexpected" as they are built and programmed to follow a deterministic
    set of steps. For this reason, computers leverage special algorithms for generating
    sequences of **pseudo-random numbers** that "look" as if they are truly random.
    Notably, the starting point of these sequences (the **random seed**) can determine
    the full progression of numbers. When needed, a computer can still generate a
    random seed by looking at a quickly changing state (like the number of clock cycles
    of the CPU from the last boot) or by measuring some microscopic physical quantities
    (like a voltage on a port) that are affected by uncontrollable phenomena, such
    as thermal noise and other quantic effects. It's interesting how computers struggle
    with what would take us just a flip of a coin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our supervised learning typical flow and split our full housing
    dataset into training and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement the **Partitioning** node and connect it with the output of
    the **Excel Reader** output (you can keep the **Statistics** node unhooked as
    we don't need to use its outputs). In the configuration dialog, let's make sure
    that we select the **Relative[%]** option with the value `70`. This means that,
    out of the 4,000 properties available at the inputs, 70% of them will be used
    for training (which is a fair thing to do since, as anticipated in *Chapter 4*,
    *What is Machine Learning?*, the training set should normally cover between 70%
    and 80% of the total full dataset). We want the partitioning to happen randomly.
    In the previous step, we noticed that some nominal columns (like *Neighborhood*)
    display an unbalanced distribution across their values. This means that we have
    the risk of having the very few properties in a smaller neighborhood (like the
    26 rows referring to `Magliana`) ending up solely in a partition. Although this
    is not strictly required, we better avoid any unbalance that can affect our learning
    and select **Stratified sampling** on *Neighborhood* in the dialog. You can also
    click on the bottom tick box and, on the right, type in a random seed, like `12345`,
    so that you can count on the same partitioning over and over. When you run the
    node, you find that in the upper output port (right-click on the node and select
    **First partition**) you find 2,800 rows that are exactly 70% of the original
    dataset. This is a good sign and we can move ahead with the learning step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we need to add the nodes (both learner and predictor) that implement
    the specific machine learning algorithm we want to use. The simplest algorithm
    for predicting numbers is **linear regression,** which is what we are going to
    use in this tutorial. It's worth introducing first the underlying mathematical
    model so that we can get ready to interpret its results.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The linear regression model is a generalization of the simple regression we
    have used to predict second-hand car prices in *Chapter 4*, *What is Machine Learning?*.
    In that case, we modeled the price as a straight line, following the simple equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *y* was the dependent variable, so the target of our prediction (the
    price of the car), *x*, was the only independent variable (in that case, the age
    of the car in years) and ![](img/B17125_05_002.png) and ![](img/B17125_05_003.png)
    were the parameters of the model, defining the *height* of the line (also known
    as the *offset* or *intercept*) and its *slope*, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Linear regression of car prices: the line shows the prediction
    as the age varies'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, as you can see in *Figure 5.7*, we have ![](img/B17125_05_004.png)
    (it''s where the model line encounters the vertical axis) and ![](img/B17125_05_005.png)
    so the price of the car is predicted through the simple model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The price of a 2-year-old car will be estimated to be $12,600, since:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_007.png)'
  prefs: []
  type: TYPE_IMG
- en: The purpose of the *learner* algorithm of a simple linear regression is to find
    the right parameters (![](img/B17125_05_008.png) and ![](img/B17125_05_009.png))
    that minimize the error of a prediction, while the *predictor* algorithm will
    just apply the model on new numbers, like we did when we came up with the estimated
    price of a 2-year-old car.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression is a generalization of the simple model that we have just
    seen in action. Its underlying mathematical description is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *y* is still the (single) target variable that we are trying to predict,
    the various *x*[i] values represent the (many) independent variables that correspond
    to the features we have available, and the ![](img/B17125_05_011.png) values are
    the parameters of the model that define its "shape." Since we have several independent
    variables this time (for this reason, we call it a **multivariate model**), we
    cannot "visualize" it any longer with a simple line on a 2D chart. Still, its
    underlying mathematical model is quite simple because it assumes that every feature
    is "linearly" connected with the target variable. Here you go: you have just met
    the multivariate linear regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply this model to the prediction of the rental prices, our target variable
    is represented by the column *Rent* while the features (independent variables)
    are all the other columns, like *Rooms*, *Surface*, and so on. The multivariate
    linear regression model will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_012.png)'
  prefs: []
  type: TYPE_IMG
- en: and the aim of the learner algorithm implementing this model will be to find
    the "best" values of ![](img/B17125_05_013.png), ![](img/B17125_05_014.png), ![](img/B17125_05_015.png),
    and so on that minimize the error produced on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are ways to find analytically (meaning through a set of given formulas,
    nothing overly complex) the set of parameters ![](img/B17125_05_016.png) that
    minimize the error of a linear regression model. The simplest one is called **Ordinary
    Least Squares (OLS)**: it minimizes the sum of the squared errors of a linear
    regression. Do you remember the **Root Mean Squared Error** (**RMSE**) metric
    introduced in *Chapter 4*? By using the ordinary least squares procedure, we are
    going to minimize the RMSE, which is exactly what we need to do here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model above expects every independent variable to be a number. So, how
    do we deal with the nominal features we have in our dataset like *Floor_type*?
    We can solve this apparent limitation with a common trick used in machine learning:
    creating the so-called **dummy variables**. The idea is very simple: we transform
    every nominal variable into multiple numerical variables. Let''s take the example
    of *Floor_type*: this is a categorical variable whose value can be either `Upper`,
    `Mezzanine`, or `Ground floor`. In this case we would replace this categorical
    variable by creating three numeric dummy variables: *Floor_type=Upper*, *Floor_type=Mezzanine*,
    and *Floor_type=Ground*. The dummy variables will take as values either `1` or
    `0`, depending on the category: for a given row, only one dummy variable will
    take `1` and all others will take `0`. For example, if a row refers to an `Upper`
    floor property, the dummy variable *Floor_type=Upper* will be `1` and the other
    two will be `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to this trick, we can apply a linear regression model on any categorical
    variables as well; we just need to "convert" them into multiple additional dummy
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: We have all we need to give the linear regression model a try by introducing
    the KNIME node that implements its learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing logo'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image024.png) *Linear Regression Learner*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) trains a
    multivariate linear regression model for predicting a numeric quantity. For its
    configuration (see *Figure 5.8*) you will have to specify the numeric column to
    be predicted by picking it in the **Target** drop-down menu at the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_08.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.8: Configuration dialog of Linear Regression Learner: choose what
    to predict and the features to use'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the central box, you can select which columns should be used as features:
    only the columns that appear on the green box on the right will be considered
    as independent variables in the model. The nominal columns, such as strings, will
    be automatically converted by the node into dummy variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a nominal column (like *Type*) admits *N* unique values (like `A`, `B`,
    and `C`), this node will actually create not *N*, but *N-1* dummy variables (*Type=A*
    and *Type=B*). In fact, one of the nominal values can be covered by the combination
    of all zeros: in our case, if *Type* is `C`, both *Type=A* and *Type=B* will be
    zero, implying that the only possible value for that row is `C`. In this way,
    we make the model simpler and avoid the so-called dummy variable trap, which might
    make our model parameters impossible to calculate. The node takes care of this
    automatically, so you don''t have to worry about it: just keep this in mind when
    reading the model parameters related to dummy variables.'
  prefs: []
  type: TYPE_NORMAL
- en: By clicking on the **Predefined Offset Value** tick box, you can "force" the
    offset value of the linear regression model (we also called it ![](img/B17125_05_017.png)
    or intercept earlier) to a certain value or remove it, by setting it to zero.
    This reduces the "flexibility" of the model to minimize the error so it will reduce
    its accuracy. However, this trick might be helpful when you are trying to reduce
    the complexity of the model and improve its explain ability, as we have one less
    parameter to interpret. By default, this node will fail if there are some missing
    values in the input data. To manage this, you can either manage them earlier in
    the workflow, using the **Missing Value** node, or select the **Ignore rows with
    missing value** option at the bottom-left corner of the configuration dialog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once executed, the node will return at its first output port the regression
    model, which can then be used by a predictor node for making predictions. The
    second output is a table (*Figure 5.9*) that contains a summary view of the regression
    model parameters, where for each variable (including the dummy ones) you can find:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coeff.**: this is the **parameter** (also called coefficient) of the variable.
    This is the ![](img/B17125_05_018.png) parameter we have seen in the regression
    model formula.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Std. Err.**: this is **the standard deviation of the error** expected for
    this parameter. If you compare it with the value of the parameter, you get a rough
    idea of how "precise" the estimation of that parameter can be. You can use it
    also to get a rough confidence interval for the given parameter as we did in *Chapter
    4*, *What is Machine Learning?*, when talking about RMSE. In the case of the car
    price regression, if the parameter for the variable *Age* is -1.7 and the standard
    error is 0.1, you can say that 95% of the time, the price of a car declines by
    $M 1.7 ± 0.2 (2 times the standard error) every year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-value** and **P>|t|**: these are two summary statistics (**t-value** and
    **p-value**) generated by the application of the Student test, which clarifies
    how significant a variable is for the model. The smaller the p-value, the more
    confident you can be in rejecting the possibility that that parameter looks significant
    just "by chance" (it''s called **null hypothesis**). As a general rule of thumb,
    when the p-value (the last column in this table) is above 0.05, you should remove
    that variable from the model, as it is likely insignificant:![Table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_09.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.9: The summary output of the Linear Regression Learner node: find
    out what the parameters of the regression are and if they turn out significant
    or not'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you right-click on the node after it is executed, you can open an additional
    graphical view (select **View: Linear Regression Scatterplot View**) where you
    can visually compare the individual features against the target to look for steep
    slopes and other patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now put this node to work with our Rome properties and see what it''s
    got:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **Linear Regression Learner** node and connect it with the upper
    output of the **Partitioning** node, which is the training set (a 70% random sample
    of the historical database of rents). In the configuration window, double-check
    that *Rent* is set as the **Target** variable on top. Feature-wise, at this point,
    we can keep all of them to see if they are significant or not. However, we can
    already remove one, *House_ID*, as we already know we don't want it to be used.
    We don't want to make use of the unique identifier of the property to infer the
    rental price. That number has been assigned artificially when the property was
    added to the database, and it is not connected with features of the property itself,
    so we don't want to consider it in a predictive model. Run the model and open
    the second output port to obtain the summary view of the model parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This summary view will look similar to what is displayed in *Figure 5.9*, although
    the numbers could differ given that the random partitioning might have generated
    in your case different partitions: welcome to the world of probabilistic models!
    However, we can already notice that some parameters display a p-value (the last
    column of the table, **P>|t|**) higher than 0.05\. This means we can come back
    to this step later and do some cleaning and improve the performance of the model.
    For now, let''s proceed further so that we can make some predictions and score
    the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Icon'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image028.png) *Regression Predictor*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) applies a
    regression model (given as an input in the first blue port on the left) to a dataset
    (second port) and returns the result of the prediction for each input row. The
    node does not require any configuration and can be used in conjunction with either
    the **Linear Regression Learner** node, introduced above, or the **Polynomial
    Regression** **Learner** node: you can check this one out by yourself if you want
    to build linear regressions on different polynomial degrees as we did in *Chapter
    4*, *What is Machine Learning?* (have a look at *Figure 4.9*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the **Regression Predictor** node to the workflow and make the connections:
    link the blue square output of the **Linear Regression Learner** to the upper
    input port of the predictor and connect the bottom output port of the **Partitioning**
    (the test set) to the second input port. No configuration is needed so you can
    execute the node and look at the output, which is similar to what you find in
    *Figure 5.10*:![Table'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_10.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.10: Output of the Regression Predictor node: we finally have a prediction
    of the rental price.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can look with pride at the last column on the right, called *Prediction
    (Rent)*: for each row in the test set (which has not been "seen" by the learner
    node) the node has generated a prediction of the rent. This prediction was obtained
    by just "applying" the parameters of the regression model to the values of the
    rows in the test set. Let''s see how this works with an example: consider the
    parameters in *Figure 5.9*. In this case the intercept (last row) is 569.9, the
    parameter of *Rooms* is around 25.8, the one for *Surface* is 9.6, the parameter
    of the dummy variable associated with the `Collatino` neighborhood (*Neighborhood=Collatino*)
    is -561.9, and so on. When the predictor had to come up with a prediction for
    the first row in the test set (see the first line in *Figure 5.10*), it had to
    just apply the formula of the regression model, with the parameters found by the
    learner, to this property (with 3 rooms, 80 square meters, based in `Collatino`,
    and so on). Hence, the resulting calculation for the **Regression Predictor**
    node is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this specific case, if you add all the other features that were not reported
    in the preceding formula, we come up with a final prediction of €896.4, making
    around €50 of error versus the actual rental price, which we know is €950: not
    bad for our first prediction! To have a complete view of the performance of the
    current model, we would need to check the difference between predicted and real
    rents for all rows in the test set, using the **Numeric Scorer** node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image032.png) *Numeric Scorer*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    metrics of a regression by comparing two numeric columns. Its only required configuration
    (*Figure 5.11*) is the selection of the two columns to be compared: you can select
    the target column of the regression, containing the actual values, in the **Reference
    column** dropdown, and the predictions in the next one, labeled as **Predicted
    column**. If you want to output the performance scores as variables as well (this
    is useful when doing hyperparameter optimization), you need to tick the **Output
    scores as flow variables** box at the bottom. The node outputs the most popular
    scoring metrics of a regression, including the **Coefficient of Determination**,
    **R**², and the **RMSE**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.11: Configuration dialog of the Numeric Scorer node: select the columns
    to compare.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the **Numeric Scorer** node (watch out: don''t get confused with
    the **Scorer** node, which is used for classifications) and connect the output
    of the **Regression Predictor** with its input port. For its configuration, just
    double-check that you have *Rent* and *Prediction (Rent)* in the drop-down menus
    at the top and run the node. Its output (*Figure 5.12*) is very encouraging (of
    course, you can get slightly different results from what you find in these figures
    and that''s normal):![Table'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_12.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.12: Performance metrics as returned by the Numeric Scorer node:not
    bad for your first regression'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We obtained an R² of 0.92, which means that our current model accounts for
    around 92% of the full variability of rental prices in Rome. Considering the limited
    sample and the few features available, this looks quite good already. Also, the
    RMSE is €110, which means that 68% of the time (one standard deviation) we will
    make a prediction error that is, in absolute terms, below €110, and 95% of the
    time our error will be below €220 (two times the RMSE). The last performance metric,
    **Mean Absolute Percentage Error** (**MAPE**) tells us that, on average, our predicted
    rent will differ from the actual rent by around 10%: again, not bad at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, we strive for the best and question ourselves if we can do anything
    to improve the model. The simplest thing to do will be to consider whether we
    can improve the selection of features. Let''s go back and have a look at the parameters
    obtained by the regression (*Figure 5.9*) and if we can remove some unneeded (or
    damaging) features. When we remove excess features from a model, we obtain at
    least two advantages: first, we make the model simpler and more explanatory to
    other human beings, as we have fewer parameters to explain. Secondly, we reduce
    the possibility for the model to overfit on the training set and, so, we increase
    its general robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason for removing features is to avoid the risk of **multicollinearity**,
    which happens when features are correlated with each other. Correlated features
    are redundant: they can produce degradation of the predictive performance of your
    model and should be removed. The **Linear Correlation** node can help you calculate
    the correlation across all pairs of numeric columns in a table. As an alternative,
    you can use the **Variance Inflation Filter** (**VIF**) component, available in
    the KNIME Hub: as a rule of thumb, all variables showing a VIF higher than 5 should
    be removed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the p-values (last column of the table) and see if we
    can unveil some opportunities. Remember, the higher they are, the less meaningful
    their associated features proved to be. For sure we notice that the feature *Elevator*
    should be removed: its p-value is way above the thumb-rule threshold of 0.05 so
    we can go ahead and remove it. Also, the variable *Property_type* shall be removed:
    the p-values of their dummy variables are high, with the exception of *Property_type=Penthouse*
    (indicating that `Penthouse` is the only type that seems to be significant in
    affecting the value of the rent). Still, considering how few penthouses we have
    in the dataset, it''s worth removing this feature and further simplifying the
    model. Let''s give this simplification a try and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the configuration dialog of the **Linear Regression Learner** node and
    move *Elevator* and *Property_type* to the left box of the column selector, so
    as to remove them as features of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let''s run the full model and see if something changed. To do so, it will
    be enough to execute the **Numeric Scorer** node: all previous nodes will be forced
    to run as well.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By removing these two features (see the updated results in *Figure 5.13*),
    we managed to keep the same performance levels, proving that they were unneeded.
    Actually, the performance has marginally increased (notice the lower RMSE), probably
    showing that we were slightly overfitting because of these uninformative variables.
    Additionally, we simplified the model, making it simpler to explain. Now we can
    predict the rental price of a property in Rome by knowing only the neighborhood,
    the number of rooms, the surface, and its floor (number and type):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_13.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.13: Updated parameters and performance scores after the removal of
    two features: every little helps'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These last two steps have shown us the value of selecting features wisely. As
    anticipated in *Chapter 4*, *What is Machine Learning?*, feature selection is
    an important practice in machine learning, indeed.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we applied feature selection "by hand," checking the parameters
    manually and selecting the least meaningful ones. There are more systemic and
    semi-automated techniques to find out the best subset of features to use in a
    machine learning model. If you are curious, check the KNIME nodes for **Feature
    Selection** loops and have a look at the sample workflow available on the KNIME
    Hub called **Performing a Forward Feature Selection**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before concluding, we need to do one last thing: it''s time to apply our model
    to the 10 incoming properties for which the rental price is not available yet.
    This will be a way to illustrate our findings to the owner of the company. It
    will also be an opportunity for us to understand how predictive models are used
    in real life after they are built. In fact, once models are constructed (and validated
    against overfitting, as we did through the partitioning in training and test sets,
    and so on) they are **operationalized** in a way that they can be applied to future
    samples (in this case, the 10 new properties) whenever a prediction is needed.
    Let''s see this in action with our properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the Excel file with the new properties *(*`RomeHousing-NewProperties.xlsx`*)
    by dragging and dropping it into your workflow or implementing an* **Excel Reader**
    *node*. Once executed, you will find a short table that has exactly the same columns
    as the historical database, but—of course—lacks the *Rent* value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a new **Regression Predictor** node (or copy/paste the existing one)
    and connect it as displayed in *Figure 5.14*. You should link the output of the
    **Linear Regression Learner** (yes—we are going to reuse the model we learned
    earlier) to the first input of the predictor. Then connect the **Excel Reader**
    output (the 10 new properties) to the second input of the predictor. You can now
    execute the node and have a look at the output:![](img/B17125_05_14.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.14: Full workflow for the Rome rent prediction'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you have all you need to go back to the owner of the real estate
    with your output table (which will have a similar format to what you find in *Figure
    5.15*) and wait impatiently for her reaction, which turns out to be very positive!
    She loves it, as she finds that the estimates make, at least at a first glance,
    a lot of sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_15.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.15: The predicted rental prices on the new properties: do you fancy
    a 145 square meter flat near Piazza Navona at this price?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The understandable initial stress turns quickly to a broad sense of enthusiasm.
    The model you created responds to the initial business objectives. In fact:'
  prefs: []
  type: TYPE_NORMAL
- en: The interpretation of the parameters of the model tells us something quite useful
    about the price formation mechanisms. For instance, you have found that the presence
    of the elevators and the type of flat doesn't count as much as the surface, the
    number of rooms, the floors, and, very importantly, the neighborhood to which
    the property belongs. By looking at the parameters of the neighborhood dummy variables
    (*Figure 5.13*), you find out what additional value each neighborhood brings (of
    course to be added to the rest of the components of your regression). For instance,
    Piazza Navona is by far the most expensive area while Castelli Romani seems to
    offer (at parity of characteristics) the most accessible rent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of this, you now have a simple approach to quickly generate a recommendation
    of what fair rent looks like, which could be the basis for the discussion with
    the prospective landlord when fixing the rental price. By having a data-based
    number to start from, the agents can aim at a smoother negotiation session, which
    will more likely end up with a quicker and more profitable matching of demand
    and offer in the housing market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Congratulations on completing your first regression model! It''s now time to
    move on and challenge ourselves with a different undertaking: anticipating consumers''
    behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Anticipating preferences with classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this tutorial, you will step into the role of a marketing analyst working
    for a mid-sized national consumer bank, offering services such as accounts, personal
    loans, and mortgages to around 300,000 customers in the country. The bank is currently
    trying to launch a new type of low-cost savings account, providing essential services
    and a pre-paid card that can be fully managed online. The product manager of this
    new account is not very pleased with how things are going and invites you to join
    a review meeting. You can see he is tense as he presents the outcome of a pilot
    telemarketing campaign run to support the launch. As part of this pilot, 10,000
    people were randomly selected among the full bank customer base and were phoned
    by an outbound call center. The outcome was apparently not so bad: 1,870 of the
    contacted customers (19% of the total) signed up for a new account. However, the
    calculation of the **Return On Investment** (**ROI**) pulled the entire audience
    back to the unsettling reality. The average cost of attempting to contact a customer
    through a call center is $15 per person while the incremental revenue resulting
    from a confirmed sale is estimated to be, on average, $60\. The math is simple:
    the pilot telemarketing campaign cost $150,000 and generated revenues amounting
    only to $112,200, implying a net loss of $37,800\. Now it is clear why the product
    manager looked disappointed: repeating the same campaign on more customers would
    be financially devastating.'
  prefs: []
  type: TYPE_NORMAL
- en: You timidly raise your hand and ask whether the outcomes of the pilot calls
    could be used to rethink the campaign target and improve the ROI of the marketing
    efforts. You explain that some machine learning algorithms might be able to predict
    whether a customer is willing or not to buy a product by learning from previous
    examples. As it normally happens in these cases, you instantly earn the opportunity
    to try what you suggested, and your manager asks you to put together a proposal
    on an ML way to support the launch of the new savings account.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have mixed feelings about what just happened: on one hand, you are wondering
    whether you were a bit too quick in sharing the idea. On the other hand, you are
    very excited as you get to try leveraging algorithms to impact the business on
    such an important case. You are impatient to start and ask for all the available
    information related to the customers that were involved in the pilot. The file
    you receive (`BankTelemarketing.csv`) contains the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Age*: the age of the customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Job*: a string describing the job family of the customer, like `blue-collar`,
    `management`, `student`, `unemployed`, and `retired`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Marital*: the marital status, which could be `married`, `single`, `divorced`,
    or `unknown`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Education*: the highest education level reached to date by the customer, ranging
    from `illiterate` and `basic.4y` (4 years of basic education in total) to `university.degree`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Default*: this tells us whether we know that the customer has defaulted due
    to extended payment delinquency or not. Only a few customers end up being marked
    as defaulted (`yes`): most of them either show a good rating history (`no`) or
    do not have enough history to be assigned in a category (`unknown`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mortgage* and *Loan*: tells us whether the user has ever requested a housing
    mortgage or a personal loan, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contact*: indicates if the telephone number provided as a preferred contact
    method is a `landline` or a `mobile` `phone`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outcome*: a string recording the result of the call center contact during
    the pilot campaign. It can be `yes` or `no`, depending on whether the customer
    opened the new savings account or decided to decline the offer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you get cracking, you have a chat with the product manager to get clear
    on what would be the most valuable outputs for the business given the situation:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, it would be very useful to understand and document what characteristics
    make a customer most likely to buy the new banking product. Given its novelty,
    it is not clear yet who will find its proposition particularly appealing. Having
    some more clues on this aspect can help to build more tailored campaigns, personalize
    their content, and—by doing so—transfer the learnings from the call center pilot
    to other types of media touchpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given that the pilot covered only a relatively small subset of customers—around
    3% of the total—it would be useful to identify "who else" to call within the other
    97% to maximize the ROI of the marketing initiative. In fact, we can assume that
    the same features we found in our pilot dataset—such as age, job, marital status,
    and so on—are available for the entire customer database. If we were able to *score*
    the remaining customers in terms of their *propensity* to buy the product, we
    would be focusing our efforts on the most inclined ones and greatly improving
    the campaign''s effectiveness. In other words, we should create a **propensity
    model** that will score current (and future) customers to enable a better marketing
    targeting. We will use the propensity scores to "limit" the next marketing efforts
    to a selected subset of the total customer base where the percentage of people
    in the new product is higher than 19% (as it was in our pilot): by doing so, we
    would increase the ROI of our marketing efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From a machine learning standpoint, you need to create a machine able to predict
    whether a consumer will buy or will not open a savings account before you make
    the call. This is still a clear case of supervised learning, since you aim at
    predicting something based on previous examples (the pilot calls). In contrast
    with the Rome real estate case, where we had to predict a number (the rental price)
    using *regression* algorithms, here we need to predict the value of the categorical
    column *Outcome*. We will then need to implement *classification* algorithms,
    such as decision trees and random forest, which we are going to meet shortly.
    We are clear on the business need, the available data, and the type of machine
    learning route we want to take: we have all we need to start getting serious about
    this challenge. After creating a new workflow in KNIME, we load the data into
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the file `BankTelemarketing.csv` onto the blank workflow. After
    the **CSV Reader** node dialog appears, we can quickly check that all is in order
    and close the window by clicking on **OK**. Once executed, the output of the node
    (*Figure 5.16*) confirms that our dataset is ready to go:![Table
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_16.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.16: The pilot campaign data: 10,000 customers through 8 features and
    for which we know the outcome of their call center contact'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As usual, we implement the node **Statistics**, to explore the characteristics
    of our dataset. After confirming its default configuration, we check the **Top/bottom**
    tab of its main view (press *F10* or right-click and select **View: Statistics
    View** to open it). It seems that there are no missing values and that all seems
    to be in line with what we knew about the pilot campaign: the *Outcome* column
    shows 1,870 rows with `yes`, which is what the product manager managed in his
    presentation. We also notice that the *Default* column has only one row referring
    to a defaulted customer. This column might still be useful as it differentiates
    between customers who never defaulted and ones we don''t have any certainty about,
    so we decide to keep it and move on:![Text'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_17.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.17: The Top/bottom output of the Statistics node: only one person
    in this sample defaulted—good for everyone!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since we are in the supervised learning scenario, we need to implement the
    usual partitioning/learn/predict/score structure in order to validate against
    the risk of overfitting. We start by adding the **Partitioning** node and connecting
    it downstream to the **CSV Reader** node. In its configuration dialog, we leave
    the **Relative** 70% size for the training partition and we decide to protect
    the distribution of the target variable *Outcome* in both partitions, selecting
    the **Stratified sampling** option. Additionally, we put a static number in the
    random seed box (you can put `12345` as you see in *Figure 5.18*) and tick the
    adjacent checkbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a general rule, always perform a stratified sampling on the target variable
    of a classification. This will reduce the impact of imbalanced classes when learning
    and validating your model. There are other ways to restore a balance in the distribution
    of classes, such as under-sampling the majority class or over-sampling the minority
    one. One interesting approach is the creation of synthetic (and realistic) additional
    samples using algorithms like the **Synthetic Minority Over-sampling Technique**:
    check out the **SMOTE** node to learn more.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_18.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.18: Performing a stratified sampling using the Partitioning node:
    this way, we ensure a fair presence of yes and no customers in each partition'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have a training and test dataset readily available, we can proceed
    with implementing our first classification algorithm: **decision trees**. Let''s
    get a hint of how it works.'
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Decision trees are simple models that describe a decision-making process. Have
    a look at the tree shown in *Figure 5.19* to get an idea of how they work. Their
    hierarchical structure resembles an upside-down tree. The root on top corresponds
    to the first question: according to the possible answers, there is a split between
    two or more subsequent *branches*. Every branch can either lead to additional
    questions (and respective splits into more branches) or terminate in *leaves*,
    indicating the outcome of the decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19: How will you go to work tomorrow? A decision tree can help you
    make up your mind'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees can be used to describe the process that assigns an entity to
    a class: in this case, we call it a **classification tree**. Think about a table
    where each entity (corresponding to a row) is described by multiple features (columns)
    and is assigned to one specific class, among different alternatives. For example,
    a classification tree that assigns consumers to multiple classes will answer the
    question *to which class does the consumer belong?*: every branching will correspond
    to different outcomes of a test on the features (like *is the age of the consumer
    higher than 35?* or *is the person married?*) while each terminal leaf will be
    one of the possible classes. Once you have defined the decision tree, you can
    apply it to all consumers (current and future). For every consumer in the table,
    you follow the decision tree: the features of the consumer will dictate which
    specific path to follow and result in a single leaf to be assigned as the class
    of the consumer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many tree-based learning algorithms available for classification.
    They are able to "draw" trees by learning from labeled examples. These algorithms
    can find out the right splits and paths that end up with a decision model able
    to predict classes of new, unlabeled entities. The simplest version of a decision
    tree learning algorithm will proceed by iteration, starting from the root of the
    tree and checking what the "best possible" next split to make is so as to differentiate
    classes in the least ambiguous way. This concept will become clear by means of
    a practical example. Let''s imagine that we want to build a decision tree in order
    to predict which drink fast-food customers are going to order (among soda, wine,
    or beer), based on the food menu they had (the delicious alternatives are pizza,
    burger, or salad) and the composition of the table (whether it is among kids,
    couples, or groups of adults). The dataset to learn from will look like the one
    shown in *Figure 5.20*: we have 36 rows, each referring to a previous customer,
    and three columns, one for each feature (*Menu* and *Type*) and the target class
    (the *Favorite drink*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, electronics'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.20: Drink preferences for 36 fast-food customers: can you predict
    their preferences based on their food menu and type?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have only two features, the resulting decision tree can only have
    two levels, resulting in two alternative shapes: either the first split is by
    *Menu* and the second, at the level below, by *Type*, or the other way around.
    The learning algorithm will pick the split that makes the most sense by looking
    at the count of the items falling into each branch and checking which splits make
    the "clearest cut" among classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific case, the alternative choices for the first split are the
    ones drawn in *Figure 5.21*: you can find the number of customers falling into
    each branch, separated by alternative class (beer, soda, or wine). Have a look
    at the number and ask yourself: between the *Menu* split on the left and the *Type*
    split on the right, which one is differentiating in the "purest" way among the
    three classes?'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.21: Which of these two alternative splits gives you the most help
    in anticipating the choice of drinks?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, it seems that the *Type* split on the right is a no-brainer:
    kids are consistently going for sodas (with the exception of 2 customers who—hopefully—got
    served with alcohol-free beer), groups prefer beers, while couples go mainly with
    wine. The other alternative (split by *Menu*) is messier: for those having salad
    and, to some extent, burger, there is no such clear cut drinks choice. Our preference
    for the option on the right is guided by human intuition: for an algorithm, we
    need to have a more deterministic way to make a decision. Tree learning algorithms
    use, in fact, metrics to decide which splits are best to pick. One of these metrics
    is called the **Gini index**, or **Impurity index**. Its formula is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *f*[i] is the relative frequency of *i-n*^(th) class (it''s in the *%*
    column in *Figure 5.21*), among the *M* possible classes. The algorithm will calculate
    the *I*[G] for each possible branching of a split and average the results out.
    The option with the lowest Gini index (meaning, with the least "impure" cut) will
    win among the others. In our fast-food case, the overall Gini index for the option
    on the left will be the average of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_021.png)![](img/B17125_05_022.png)![](img/B17125_05_023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By averaging them out, we find that the Gini index for the left option is 0.60
    while the one for the right option is 0.38\. These metrics are confirming our
    intuition: the option on the right (the split by *Type*) is "purer" as demonstrated
    by the lower Gini index. Now you have all the elements to see how the decision
    tree learning algorithm works: it will iteratively calculate the average *I*[G]
    for all possible splits (at least one for each available feature), pick the one
    with the lowest index, and repeat the same at the levels below, for all possible
    branches, until it is not possible to split further. In the end, the leaves are
    assigned by just looking at where the majority of the known examples fall. For
    instance, take the branching on the right in *Figure 5.21*: if this was the last
    level of a tree, kids will be classified with soda, couples with wine, and groups
    with beer. You can see in *Figure 5.22* the resulting full decision tree you would
    obtain by using the fast-food data we presented above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Decision tree for classifying fast-food customers according to
    their favorite drink.In which path would you normally be?'
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at the obtained decision tree, you will notice that not all branches
    at the top level incur further splits at the level below. Take the example of
    the *Type*=`Kids` branch on the top left: the vast majority of kids (10 out of
    12) go for `Soda`. There are not enough remaining examples to make a meaningful
    further split by *Menu*, so the tree just stops there. On top of this basic stopping
    criterion, you can implement additional (and more stringent) conditions that limit
    the growth of the tree by removing less meaningful branches: these are called—quite
    appropriately, I must say—**pruning mechanisms**. By pruning a decision tree,
    you end up with a less complex model: this is very handy to use when you want
    to avoid model overfitting. Think about this: if you have many features and examples,
    your tree can grow massively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every combination of values might, in theory, produce a very specific path.
    Chances are that these small branches cover an insignificant case that just happened
    to be in the training set but has no general value: this is a typical case of
    overfitting that we want to avoid as much as possible. That is why, as you will
    soon see in KNIME, you might need to activate some of the pruning mechanisms to
    avoid overfitting when growing trees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make another consideration related to numeric features in decision trees.
    In the fast-food example, we only had nominal features, which make every split
    quite simple to imagine: every underlying branch covered a possible value of the
    categorical column. If you have a numeric column to be considered, the algorithm
    will check what the Gini index would be if you split your samples using a numeric
    threshold. The algorithm will try multiple thresholds and pick the best split
    that minimizes impurity. Let''s imagine that in our example we had an additional
    feature, called *Size*, that counts the number of people sitting at each table.
    The algorithm will test multiple thresholds and will check what the Gini index
    would be if you divided your samples according to these conditions, which are
    questions like "is *Size* > 3?", "is *Size* > 5?", and "is *Size* > 7?". If one
    of these conditions is meaningful, the split will be made according to the numeric
    variable: all samples having *Size* lower than the threshold will go to the left
    branch, and all others to the right branch. The Gini indices resulting from all
    the thresholds on the numeric features will be compared across all other indices
    coming from the categorical variables as we saw earlier: at each step, the purest
    split will win, irrespectively of its type. This is how decision trees can cleverly
    mix all types of features when classifying samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree models can be extended to predict numbers and, so, become **regression
    trees**. In these trees, each leaf is labeled with a different value of the target
    variable. Normally, the value of the leaf is just the average of all the samples
    that ended up in such a leaf node, after going through a construction mechanism
    similar to the ones for classification trees (using Gini indices and all that).
    You can build regression trees in KNIME as well: have a look at the **simple regression
    tree** nodes in the repository.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what decision trees are, let''s grow one to classify our bank
    customers according to the outcome of the telemarketing campaign. We''ll use a
    new node for that: the **Decision Tree Learner**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logo'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image0491.png) *Decision Tree Learner*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Decision tree**) trains a decision tree model
    for predicting nominal variables (classification). The most important fields to
    be set in its configuration dialog (see *Figure 5.23*) are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class column**: you need to specify your nominal target variable to be predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality measure**: this is the metric used to decide how to make the splits.
    The default value is the **Gini index** we have encountered above. You can also
    select the information for **Gain ratio**, which would tend to create more numerous
    and smaller branches. There is not a good and bad choice, and in most cases both
    measures generate very similar trees: you can try them both and see which one
    produces the best results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning method**: you can use this selector to activate a robust pruning
    technique called **MDL** (**Minimum Description Length**) that removes the less
    meaningful branches and generates a balanced tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Min number records per node**: you can control the tree growth-stopping criterion
    by setting a minimum number of samples for allowing a further split. By default,
    this hyperparameter is set to `2`: this means that no branch will be generated
    with less than 2 samples. As you increase this number, you will prune more branches
    and obtain smaller and smaller trees: this is an effective way for tuning the
    complexity of the trees and obtaining an optimal, well-fitted model. By activating
    the MDL technique in the earlier selector, you go the "easy way" as it will automatically
    guess the right level of pruning.![](img/B17125_05_23.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 5.23: Configuration window of the Decision Tree Learner node: are you
    up for some pruning today?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the node is the definition of the tree model, which can be explored
    by opening its main view (right-click on the node and select **View: Decision
    Tree View**). In *Figure 5.24*, you will find the KNIME output of the fast-food
    classification tree we obtained earlier (see, for comparison, *Figures 5.22* and
    *5.21*): at each node of the tree, you find the number of training samples falling
    into each value of the class. You can expand and collapse the branches by clicking
    on the circled **+** and **–** signs appearing at each split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.24: The fast-food classification tree, as outputted by the Decision
    Tree Learner node in KNIME.The gray rows correspond to the majority class'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag and drop the **Decision Tree Learner** node from the repository and connect
    the upper output of the **Partitioning node** (the training set) with it. Let''s
    leave all the default values for now in its configuration (we will have the opportunity
    for some pruning later): the only selector to double-check is the one setting
    the **Class column** that in our case is *Outcome*. If you run the node and open
    its decision tree view (select the node and press *F10*), you will meet the tree
    you have just grown:![](img/B17125_05_25.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.25: A first tree classifying bank customers by Outcome: this is just
    a partial view of the many levels and branches available'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you expand some of the branches, you realize that the tree is very wide
    and deep: *Figure 5.25* shows an excerpt of what the tree might look like (depending
    on your random partitioning, you might end up with a different tree, which is
    fine). In this case, we noticed that the top split divided customers into mobile
    and landline users. This is what happened: the Gini index was calculated across
    all features and scored the lowest for *Contact*, making this the single most
    important variable to differentiate customers according to their *Outcome*. Let''s
    see whether this tree is good enough and predict the outcomes in the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Icon'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image0531.png) *Decision Tree Predictor*
  prefs: []
  type: TYPE_NORMAL
- en: This (**Analytics > Mining > Decision tree**) applies a decision tree model
    (provided as an input in the first port) to a dataset (second port) and returns
    the prediction for each input row. This node will not require any configuration
    and will produce a similar table to the one provided in the input with an additional
    column that includes the result of the classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement the **Decision Tree Predictor** node and wire it in such a way
    it gets as inputs the tree model outputted by the **Decision Tree Learner** node
    and the second outport of the **Partitioning** node, which is our test set. As
    you execute the node, you will find an output that the precious additional column
    called *Prediction (Outcome)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, we can finally assess the performance of the model by calculating
    the metrics used for classification. Do you remember the accuracy, precision,
    sensitivity measures, and confusion matrix we obtained in the cute dog versus
    muffin example? It''s time to calculate these metrics by using the right node:
    **Scorer**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image054.png) *Scorer*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    scores of classification by comparing two nominal columns. The only step required
    for its configuration (*Figure 5.26*) is the selection of the columns to be compared:
    you should select the column carrying the observed (actual) values in the **First
    Column** dropdown, while predictions go in the **Second Column** selector. The
    node outputs the most important metrics for assessing a classification performance,
    namely: the Confusion Matrix, provided as a table in the first output (columns
    will refer to the predictions, while actual values will go as rows) and summary
    metrics such as **Accuracy**, **Precision**, and **Sensitivity**, which you can
    find in the second output of the node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the performance metrics for a classification will depend on which class
    you decide to be considered as `Positive`: have a look at *Figure 4.8* in the
    previous chapter to get a refresher. In the second output of the Scorer node,
    you will find one row for every possible class: each row contains the metrics
    calculated under the assumption that one specific class is labeled as `Positive`
    and all the other classes are `Negative`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_26.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.26: The configuration window of the Scorer node: just select the columns
    to compare across'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add the **Scorer** node (make sure you don''t get confused and pick
    the **Numeric Scorer** node, which can only be used for regressions) to the workflow
    and connect it downstream to the **Decision Tree Predictor**. In the configuration
    window, we can leave everything as it is, just checking that we have *Outcome*
    as **First Column** and *Prediction (Outcome)* as **Second Column**. Execute the
    node and open its main view (*F10* or right-click and select **View: Confusion
    Matrix**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the **Scorer** node (*Figure 5.27*) tells us that we get an accuracy
    level of 78.3%: out of 100 predictions, 78 of them turn out to be correct. The
    confusion matrix helps us understand whether the model can bring value to our
    business case:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_27.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.27: The output of the node Scorer after our first classification:
    78% accuracy is not bad as a starting point'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the case shown in *Figure 5.27*, we have 450 customers (180 + 270) in the
    test set that were predicted as interested in the account (*Prediction (Outcome)*
    = `yes`). Out of this, only 180 (40%, which corresponds to the precision of our
    model) were predicted correctly, meaning that these customers ended up buying
    the product. The number seems to be low, but it is already encouraging: the algorithm
    can help to find a subset of customers that are more likely to buy the product.
    If we indiscriminately called every customer—as we know from the pilot—we would
    have achieved a success rate of 19% while, by focusing on the (fewer) customers
    that the algorithm identified as potential (*Prediction (Outcome)* = `yes`), the
    success rate would double and reach 40%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now think about what we can do to improve the results of the modeling.
    We remember that our decision tree was deep and wide: some of the branches were
    leading to very "specific" cases, which interested only a handful of examples
    in the training set. This doesn''t look right: a decision tree that adapted so
    closely to the training set might produce high errors in future cases as it is
    not able to comprehend the essential patterns of general validity. We might be
    overfitting! Let''s equip ourselves with a good pair of pruning shears: we can
    try to fix the overfitting by reducing the complexity of the tree, making some
    smart cuts here and there:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, the Decision Tree Predictor node generates null predictions (red
    `?` in KNIME tables, which caused the warning message you see at the top of *Figure
    5.27*). This is a sign that the tree might be overfitted: its paths are too "specific"
    and do not encompass the set of values that require a prediction (this "pathology"
    is called **No True Child**). Besides taking care of the overfitting, one trick
    you can apply to solve the missing values is to open the **PMMLSettings** panel
    (second tab in the **Decision Tree Learner** configuration) and set **No true
    child strategy** to **returnLastPrediction**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the configuration dialog of the **Decision Tree Learner** and select **MDL**
    as the **Pruning method**. This is the simplest and quickest way to prune our
    tree: we could have also iterated through higher values of **Min number records
    per node** (give it a try to check how it works), but MDL is a safe approach to
    get quick improvements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see if it worked. We don't need to change anything else, so let's just
    execute the **Scorer** node and open its main view to see what happened.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you look at the results (*Figure 5.28*) you feel a thrill of excitement:
    things got better. The accuracy raised to 83% and, most importantly, the precision
    of the model greatly increased. Out of the 175 customers in the test set who are
    now predicted as *Outcome*=`yes`, 117 would have ended up actually buying the
    product. If we followed the recommendation of the model (which we can assume will
    keep a similar predictive performance on customers we didn''t call yet—so the
    remaining 97% of our customer base), the success rate of our marketing campaign
    will move to 67%, which is more than 3 times better than our initial baseline
    of 19%!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_28.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.28: The output of the node Scorer after our tree pruning: the precision'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The model was previously overfitting and some pruning clearly helped. If you
    now open the tree view of the **Decision Tree Learner** node, you will find a
    much simpler model that can be explored and, finally, interpreted. You can expand
    all branches at once by selecting the root node (just left-click on it) and then
    clicking on **Tree** | **Expand Selected Branch** from the top menu. By looking
    at the tree, which might be similar to the one shown in *Figure 5.29*, we can
    finally attempt some interpretation of the model. Look at the different percentages
    of the `yes` category within each node: we found some buckets of customers that
    are disproportionally interested in our product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.29: An excerpt of the decision tree classifying bank customers by
    Outcome: students, retired, and 60+ customers using landlines are showing the
    most interest in our new savings account'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we find out that customers falling into these three segments:'
  prefs: []
  type: TYPE_NORMAL
- en: Mobile users who are students
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobile users who are retired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landline users who are 60+ years old
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'responded much more to our pilot campaign than all others, having more than
    50% of the samples ending up with opening a new savings account. We have a quick
    chat with the product manager and show these results to him. He is very excited
    about the findings and, after some thinking, he confirms that what the algorithm
    spotted makes perfect sense from a business standpoint. The new type of account
    has less fixed costs than the others, so this explains while its proposition proves
    more compelling to lower-income customers, such as students and the retired. Additionally,
    this account includes a free prepaid card, which is a great tool for students,
    who can get their balance topped up progressively, but also for older customers,
    who do not fully trust yet the usage of traditional credit cards and prefer keeping
    the risk of fraud under control. The account manager is very pleased with what
    you shared with him and does not stop thanking you: by having data-based evidence
    of the characteristics that make a customer more likely to buy his new product,
    he can now finetune the marketing concept, highlighting benefits and reinforcing
    the message to share with prospective customers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The positive feedback you just received was invigorating and you want to quickly
    move to the second part of the challenge: building a propensity model able to
    "score" the 97% of the customers that have not been contacted yet. To do so, we
    will first need to introduce another classification algorithm particularly well
    suited for anticipating propensities: **random forest**.'
  prefs: []
  type: TYPE_NORMAL
- en: Random forest algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One approach used in machine learning to obtain better performance is **ensemble
    learning**. The idea behind it is very simple: instead of building a single model,
    you combine multiple *base* models together and obtain an *ensemble* model that,
    collectively, produces stronger results than any of the underlying models. If
    we apply this concept to decision trees, we will grow multiple models in parallel
    and obtain… a forest. However, if we run the decision tree algorithm we''ve seen
    in the previous pages to the same data set multiple times, we will just obtain
    "copies" of identical trees. Think about it: the procedure we described earlier
    (with the calculation of the Gini index and the building of subsequent branches)
    is completely deterministic and will always produce the same outputs when using
    the same inputs. To encourage "diversity" across the base models, we need to force
    some variance in the inputs: one way to do so is to randomly sample subsets of
    rows and columns of our input dataset, and offer them as different training sets
    to independently growing base models. Then, we will just need to aggregate the
    results of the several base models into a single ensemble model. This is called
    **Bagging**, short for **Bootstrap Aggregation**, which is the secret ingredient
    that we are going to use to move from decision trees to random forests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how it works, let''s visualize it in a practical example: *Figure
    5.30* shows both a simple decision tree and a random forest (made of four trees)
    built on our bank telemarketing example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: A decision tree and random forest compared: with the forest you
    get a propensity score and higher accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to a random sampling of rows and columns, we managed to grow four different
    trees, starting from the same initial dataset. Look at the tree on the bottom
    left (marked as *#1* in the figure): it only had the *Mortgage* and the *Contact*
    columns available to learn from, as they were the ones randomly sampled in its
    case. Given the subset of rows that were offered to it (that were also randomly
    drawn as part of the bootstrap process), the model applies the decision tree algorithm
    and produces a tree that differs from all other base models (you can check the
    four trees at the bottom—they are all different). Given the four trees that make
    our forest, let''s imagine that we want to predict the outcome for a 63-year-old
    retired customer, who has a mortgage and gets contacted by landline. The *same*
    customer will follow four *different* paths (one for each tree), which will lead
    to different outcomes. In this case, 3 trees out of 4 agree that the prediction
    should be `yes`. The resulting ensemble prediction will be made in a very democratic
    manner, by voting. Since the majority believes that this customer is a `yes`,
    the final outcome will be `yes` with a **Propensity score** of 0.75 (3 divided
    by 4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The assumption we make is that the more trees that are in agreement with a
    customer being classified as `yes`, the "closer" the customer is to buying our
    product. Of course, we normally build many more trees than just four: the diversity
    of the different branching each tree displays will make our ensemble model more
    "sensitive" to the smaller nuances of feature combinations that can tell us something
    useful about the propensity of a customer. Every tree offers a slightly "different"
    point of view on how to classify a customer: by bringing all these contributions
    together—in a sort of decisions crowdsourcing—we obtain more robust collective
    predictions: this is yet another proof of the universal value of diversity in
    life!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the propensity score is related to the probability that a classification
    is correct, they are not the same thing. We are still in the uncertain world of
    probabilistic models: even if 100% of the trees agree on a specific classification,
    you cannot be 100% sure that the classification is right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get acquainted with the KNIME node that can grow forests: meet the **Random
    Forest Learner** node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image060.png) *Random Forest Learner*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) trains a random forest model for classification. At the top
    of its configuration window (*Figure 5.31*) you can select the nominal column
    to use as the target of the classification (**Target Column**). Then, in the column
    selector in the middle, you can choose which columns to use as features (the ones
    appearing on the **Include** box on the right): all others will be ignored by
    the learning algorithm. The option **Save target distribution…** will record the
    number of samples that fell into each leaf of the underlying tree models: although
    it is memory expensive, it can help to generate more accurate propensity scores,
    by means of the **soft voting** technique, which we will talk about later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Toward the bottom of the window, you will find also a box that lets you choose
    how many trees you want to grow (**Number of models**). Lastly, you can decide
    to check a tick box (labeled as **Use static random seed**) that, similarly to
    what you found in the **Partitioning** node, lets you "fix" the initialization
    seed of the pseudo-random number generator used for the random sampling of rows
    and columns: in this case, you will obtain, at parity of input and configuration
    parameters, always the same forest generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_31.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.31: Configuration window of the Random Forest Learner node: how many
    trees you want to see in the forest?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the **Random Forest Learner** node and connect the training
    set (the first output port of the **Partitioning** node) with its input: there
    is no harm in reusing the same training and test sets used for the decision tree
    learner. If we execute the node and open its main view (*F10* or right-click and
    then select **View: Tree Views**), we will find a tree-like output, as in the
    case of the decision trees: however, this time, we have a little selector at the
    top that lets us scroll across all 100 trees of the forest.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Random forests are **black box** models as they are hard to interpret: going
    through 100 different trees would not offer us a hint for explaining how the predictions
    are made. However, there is a simple way to check which features proved to be
    most meaningful. Open the second outport of the **Random Forest Learner** node
    (right-click and click on **Attribute statistics**). The first column—called *#splits
    (level 0)*—tells you how many times that feature was selected as the top split
    of a tree. The higher that number, the more useful that feature has been in the
    learning process of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image062.png) *Random Forest Predictor*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) applies a random forest model (which needs to be provided in
    the first gray input port) to a dataset (second port) and returns the ensemble
    prediction for each input row. As part of its configuration, you can decide whether
    you want to output the propensity scores for each individual class (**Append individual
    class probabilities**). If you tick the **Use soft voting** box, you enable a
    more accurate estimation of propensity: in this case, the vote of each tree will
    be weighted by a factor that depends on how many samples fell in each leaf during
    the learning process. The more samples a leaf has "seen," the more confident we
    can be about its estimation. To use this feature, you will have to select the
    option **Save target distribution…** in the **Random Forest Learning** node, which
    is upstream.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.32: The configuration dialog of Random Forest Learner node. You can
    decide whether you want to see propensity scores or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the **Random Forest Predictor** node onto the workflow and connect
    its inputs with the forest model, outputted by the **Random Forest Learner** and
    the training set, meaning the bottom outport of the **Partitioning** node. Configure
    the node by unticking the **Append overall prediction confidence** box, and ticking
    both the **Append individual class probabilities** (we need the propensity score)
    and the **Use soft voting** boxes. After you execute it, you will find at its
    output the test set enriched with the prediction, *Prediction (Outcome)*, and
    the propensity scores by class. Specifically, the propensity of a customer being
    interested in our product is *P (Outcome=Yes)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a new **Scorer** node (for simplicity, you can copy/paste the one
    you used for the decision tree) and connect it downstream to the **Random Forest
    Predictor**. For its configuration, just make sure you select *Outcome* and *Prediction
    (Outcome)* in the first two drop-down menus. Execute it and open its main output
    view (*F10*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results of **Scorer** (*Figure 5.33*) confirm that, at least in this case,
    the ensemble model comes with better performance metrics. Accuracy has increased
    by a few decimal points and, most importantly (as it directly affects the ROI
    of our marketing campaigns), precision has reached 72% (open the **Accuracy statistics**
    outport to check it or compute it easily from the confusion matrix):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_33.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.33: The Scorer node output for our random forest. Both accuracy and
    precision increased versus the decision tree: diversity helps'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have confirmation that we have built a robust model at hand, let's
    concentrate on the propensity score we calculated and see what we can do with
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the output of the **Random Forest Predictor** node and sort the rows by
    decreasing level of propensity (click on the header of column *P (Outcome=yes)*
    and then on **Sort Descending**): you will obtain a view similar to the one shown
    in *Figure 5.34*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: The predictions generated by the random forest in descending order
    of propensity, P (Outcome=yes): the more we go down the list, the less interested
    customers (column Outcome) we find'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the list, we have the customers in the test set that most decision
    trees identified as interested. In fact, if you look at the column *Outcome*,
    we find that most rows show a `yes`, proving that, indeed, these customers were
    very interested in the product (when called, they agreed to open the savings account).
    If you scroll down the list, the propensity will go down and you will start finding
    increasingly more `no` values in column *Outcome*. Now, let''s think about the
    business case once again: now that we have a model able to predict the level of
    propensity, we could run it on the other 97% of customers that were not contacted
    as part of the pilot. If we then sorted our customer list by decreasing level
    of propensity (as we just did on the test set), we will obtain a prioritized list
    of the next people to call about our product. We will expect that the first calls
    (the ones directed to the most inclined people) will end up with a very high success
    rate (like we noticed in the test set).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, little by little, the success rate will decay: more and more people will
    start saying `no` and, at some point, it will start to become counterproductive
    to make a call. So, the key question becomes: at what point should we "stop" to
    get the maximum possible ROI from the initiative? How many calls should we make?
    What is the minimum level of propensity, below which we should avoid attempting
    to make a sale? The exciting part of propensity modeling is that you can find
    an answer to these questions before making *any* call!'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, if we assume that the customers that were part of the pilot were a
    fair sample of the total population, then we can use our test set (which has not
    been "seen" by the training algorithm, so there is no risk of overfitting) as
    a base for simulating the ROI of a marketing campaign where we call customers
    by following a decreasing level of propensity. This is exactly what we are going
    to do right now: we will need to first sort the test set by decreasing level of
    propensity (the temporary sorting we did earlier did not impact the permanent
    order of the rows in the underlying table); then, we calculate the cumulative
    profit we would make by "going through the list," using the cost and revenue estimates
    shared by the product manager. We check at which level of propensity we maximized
    our profit, so that we have a good estimate of the number of calls that we will
    need to make in total to optimize the ROI. Let''s get cracking!'
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Sorter** node and connect it at the output of the **Random Forest
    Predictor** node. We want to sort the customers in the test set by decreasing
    level of propensity, so select column *P (Outcome=yes)* and go for the **Descending**
    option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a **Rule Engine** node to calculate the marginal profit we make on
    each individual customer. We know that every call we make costs us $15, irrespective
    of its outcome. We also know that every account opening brings an incremental
    revenue of $60\. Hence, every customer that ends up buying the product (*Outcome*=`Yes`)
    brings $45 of profit while all others hit us by $–15\. Let's create a column (we
    can call it *Profit*) that implements this simple logic, as shown in *Figure 5.35*:![Graphical
    user interface, text, application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_35.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.35: The Rule Engine node for calculating the marginal profit for each
    individual customer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To calculate the cumulative profit we will need to use a new node, called **Moving
    Aggregation**.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image067.png) *Moving Aggregation*
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, this node (**Other Data Types > Time Series > Smoothing**)
    aggregates values on moving windows and calculates cumulative summarizations.
    To use a moving window, you will have to declare the **Window length** in terms
    of the number of rows to be considered and the **Window type** (meaning the direction
    of movement of the window in the table). For example, if you select **3** as the
    length and **Backward** as the type, the previous 3 rows will be aggregated together.
    If you want to aggregate by cumulating values from the first row to the last,
    you need to check the **Cumulative computation** box. Similarly to a Group By
    node, the **Aggregation settings** tab will let you select which columns should
    be aggregated and using which method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_36.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.36: Configuration dialog of the Moving Aggregation node: you can aggregate
    through moving windows or by progressively cumulating'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **Moving Aggregation** node and connect it downstream from the
    **Rule Engine**. Check the **Cumulative Computation** box, double-click on the
    *Profit* column on the left, and select **Sum** as the aggregation method. Execute
    the node and open its outport view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Moving Aggregation** node has cumulated the marginal profit generated
    by each customer. If we scroll the list (similar to the one displayed in *Figure
    5.37*) and keep an eye on the last column, *Sum(Profit)*, we noticed that the
    profit peaks when we are slightly below the first third of the full list. When
    the *P (Outcome=yes)* propensity is near 0.23, we obtain a profit of around $8,200\.
    This means that by calling only people above this level of propensity (called
    the **Cutoff** point), we maximize the ROI of our campaign.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_37.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.37: The output of the Moving Aggregation node: it seems that we reach
    maximum profit when we call people having a propensity of around 0.23.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To make this concept clearer, let's visualize the changing profit by employing
    a line chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image070.png) *Line Plot (local)*
  prefs: []
  type: TYPE_NORMAL
- en: This node (**View > Local (Swing)**) generates a line plot. The only configuration
    that might be needed is the box labeled **No. of rows to display**, which you
    can use to extend the limit of rows considered for creating the plot.
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Line Plot (local)** node, extend the number of rows to display
    to at least 3,000 (the size of the test set), execute it, and open its view at
    once (*Shift* + *F10*). In the **Column Selection** tab, keep only *Sum(Profit)*
    on the right and remove all other columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the chart (shown in *Figure 5.38*) confirms what we noticed in
    the table and makes it more evident: if we use the propensity score to decide
    the calling order of customers, our profit will follow the shape of the curve
    in the figure. We will start with a steep increase of profit (see the first segment
    on the left), as most of the first people we call (which are top prospects, given
    their high propensity score) will actually buy the product. Then, at around one-third
    of the list (when we know that the propensity score is near 0.23), we reach the
    maximum possible profit. After that, it will drop fast as we will encounter fewer
    and fewer interested customers. If we called all the people on the list, we will
    end up with a significant loss, as we have painfully learned as part of the pilot
    campaign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_38.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.38: The cumulative profit curve for our machine learning-assisted
    telemarketing campaign: we maximize the ROI at around one-third of the list sorted
    by propensity'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this simulation, we have discovered that if we limit our campaign
    to customers with a propensity score higher than 0.23 (which will be around one-third
    of the total population), we will maximize our profit. By doing the required proportions
    (our simulation covered *only* the test set, so 3,000 customers in total), we
    can estimate how much profit we would make if we applied our propensity model
    to the *entire* bank database. In this case, we would use the scores to decide
    who to call within the remaining 97% of the customer base. The overall "size of
    the prize" of conducting a mass telemarketing campaign will bring around $800,000
    of profit, if we were to call one-third of the bank''s customers. Considering
    that it might not be viable to make so many calls, we might stop earlier in the
    list: in any case, we will make some considerable profit by following the list
    that our random forest can now generate. The simulation that we just did can be
    used as a tool for planning the marketing spend and sizing the right level of
    investment. The product manager and your boss are pleased with the great work
    you pulled together. You definitely proved that spotting (and following) the ML
    way can bring sizeable value to the business: in this case, you completely reversed
    the potential outcome of a marketing campaign. The heavy losses in the pilot can
    now be transformed into a meaningful value, thanks to data, algorithms, and—most
    importantly—your expertise in leveraging them. It was a terrific result, and it
    took only 12 KNIME nodes (*Figure 5.39*) to put all of this together!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: Full workflow for the bank telemarketing optimization'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting consumers with clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, you will re-enter the shoes of the business analyst working
    for the online retailer we encountered in *Chapter 3*, *Transforming Data*. This
    time, instead of automating the creation of a set of financial reports, you are
    after a seemingly sexier objective. The **Customer Relationship Management** (**CRM**)
    team is looking for a smarter way to communicate with those customers who opted-in
    to receive regular newsletters. Instead of sending a weekly email equal for all,
    the CRM manager asked you to find a data-based approach for creating a few meaningful
    consumer segments. Once segments are defined, the CRM team can build multiple
    messages, one for each segment. By doing so, they will offer a more personalized
    (and engaging) experience for the entire customer base, which will ultimately
    affect customer loyalty and drive sustainable revenue growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning offers a proven methodology that can meet this business
    need: by using a clustering algorithm, we can create several groups of customers
    that *look similar* in terms of their characteristics (such as age, family composition,
    and income level) and the consumption patterns they displayed through previous
    purchases (like the average price of the products they selected, the overall amount
    of money they spent, or the frequency of their orders). This is the ML way of
    helping the business: use clustering to segment consumers appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CRM manager has already initiated the gathering of some basic consumer-level
    data and obtained a CSV file (`eCommerce-CRM.csv`), which has 4,157 rows—one for
    each customer—and four columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Customer_ID*: a unique identifier of the customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Average Price*: the average unit price for all purchases made by each customer.
    It gives us a directional view of the "premiumness" of the former shopping choices
    displayed by the customer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basket Size*: the average number of units purchased within any single order
    created by the customers. This measure indicates whether they prefer to go for
    "bulk" shopping with fat baskets or smaller, occasion-driven purchase acts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unique Products*: the average number of different articles that the customer
    buys on each occasion. This metric indicates the breadth of the assortment "tried"
    by each customer. It gives us an idea of the customer''s willingness to explore
    new products versus their preference of "keep buying" the same articles all the
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we exchange thoughts with the CRM manager about this dataset, she confirms
    what we had already noticed: the three consumption metrics included in the data
    (the last three columns) are far from giving us a comprehensive picture of each
    customer''s preferences. If we wanted, we could have generated many more columns
    by aggregating the transactions history: think about the absolute number of purchases
    by customer, the total generated value, the "mix" of purchased categories and
    subcategories, the premiumness of the purchased products within each category
    and subcategory, and also the customer characteristics, like their age, the average
    income of the neighborhood they live in, and so on. Still, we decide to go ahead
    and leverage the power of machine learning on this first dataset: we can always
    increase the level of the model sophistication later if we want. Now, the important
    thing is to "start rocking" and pragmatically prove some first business value
    from this new way of operating. In terms of deliverables, you align with your
    business partner the need to assign each customer to a small number of clusters
    and put together some visualizations to interpret what differentiates clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time to power KNIME on, create a new workflow, and load our CRM extract
    into it:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the file `eCommerce-CRM.csv` onto the workflow editor. As the **CSV Reader**
    node dialog pops up, we can check that all four columns are showing in the preview
    and click **OK** to confirm the default setting. After executing the node, we
    can look at its output view (*Figure 5.40*) and move to the next step:![Table
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_40.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.40: The CRM extract once loaded: for every customer, we have three
    metrics, each one giving us a hint of their shopping habits'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Creating homogenous groups of elements, such as customers in our case, requires
    the use of a clustering algorithm. Let's make acquaintance with possibly the most
    popular clustering algorithm available today – **k-means**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means algorithm is perhaps the easiest (and yet probably the most used)
    approach used for clustering. The big idea is elementary and can be summarized
    in two lines: each element in a dataset is assigned to the closest cluster. At
    each step of the process, the position of the clusters gets updated, so they become
    more and more compact.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine we want to cluster a set of points displayed on a bi-dimensional
    scatter plot. Each point is described employing two numbers that represent the
    horizontal and the vertical coordinates, respectively. The distance between any
    two points can be easily calculated through the Pythagorean theorem (yes, the
    same used for calculating the sides of a right triangle—see *Figure 5.41* for
    a refresher):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_41.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.41: Calculating the distance between two points using the Pythagorean
    theorem: you make the square root of the sum of the squared differences for each
    coordinate'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the k-means algorithm is to create a given number (*k*) of homogenous
    groups formed by points that are relatively close to one another. Like many other
    machine learning algorithms, k-means has an iterative approach: at each iteration,
    it groups the points based on their proximity to some special points called the
    **centroids** of each cluster. Every point is associated with its closest centroid.
    The algorithm then updates the position of the centroids iteratively: at each
    iteration, the groups will tend to be more and more homogenous, meaning that the
    points forming these clusters will be gradually closer and closer to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see in detail the sequence of steps that make the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: the first step of the algorithm is making the initial choice
    of the centroids, one per cluster. There are different ways to make this choice.
    The simplest way is to randomly select *k* points in our dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Grouping**: the algorithm now calculates the distance of each point from
    each centroid (using the Pythagorean theorem), and each point is matched with
    its closest centroid (the one lying at the smallest distance). In this way, all
    the points near a centroid are grouped together as they belong to the same cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update**: the algorithm now calculates the centroid of each cluster again
    by making an average of the coordinates of all the points that belong to the cluster.
    Basically, the centroid is updated so that it matches the center of mass of the
    newly formed group.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point of the process, we return to step *b* to start a new iteration
    and repeat steps *b* and *c* as long as it is possible to improve the centroids.
    At every iteration, the clusters will converge, meaning that they will become
    increasingly more meaningful. We will stop when the update step produces no change
    in the way in which points are assigned to clusters. When this happens, the algorithm
    terminates: a stable solution has been found, and the current definition of clusters
    is returned as the resulting output. Should this convergence not take place, the
    algorithm will stop in any case once a preset number of maximum iterations is
    reached.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This process might still look complicated but let me stress how simple the
    underlying mathematics is: random draws, averages, squares, and the square roots
    in the Pythagorean theorem are all the math we need to implement the k-means algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand how the algorithm works, let''s go through a concrete
    example and use some charts to display the evolution of the various iterations
    graphically. For the sake of simplicity, we will use a simple dataset formed only
    by two columns: by having only two columns, we can visualize the distances between
    the various points on 2-dimensional scatter plots (Cartesian diagrams).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we work with datasets with more than two columns (as is usually the case),
    the concept of distance becomes more difficult to visualize in our human mind.
    While, with three columns, we can still imagine the algorithm working on a 3-dimensional
    space, with 4, 5, or 10 columns, we will necessarily need to delegate the task
    to machines. Luckily, they are much more at ease than humans when navigating multidimensional
    spaces. The good news is that the basic formula for calculating distances (the
    Pythagorean theorem you found in *Figure 5.41*) stays the same: you will have
    to calculate the squares of the distances across *all* dimensions—no matter how
    many they are—and sum them across.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the real estate sector for a second, let''s imagine that we have
    a dataset describing 16 properties utilizing their price per square meter and
    their age in years (see *Figure 5.42* on the left). We want to cluster these properties
    in three homogeneous clusters. The business reason we want to create such a cluster
    is immediate: should a client show interest in any of these properties, we want
    to immediately recommend considering all other properties in the same cluster
    since they should exhibit *similar* features. This example looks naïve with 16
    properties: we wouldn''t need k-means to identify similarities with so little
    data involved. However, the beauty of k-means is that it could easily scale to
    many dimensions and properties, while our human brain would start struggling with
    a few more data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_42.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.42: Kicking k-means off: out of the 16 properties with different prices
    and ages (left), three are randomly picked and elected as initial centroids (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to run is the initialization: the algorithm will draw at random
    three properties, as three is the number of requested clusters (*k=3*). The algorithm
    has randomly extracted properties **C**, **G**, and **I**, as you can see on the
    right side of *Figure 5.42*. As part of the first iteration, the algorithm will
    proceed with the grouping step: first, it will use the Pythagorean theorem to
    calculate the distances between each property and each centroid and will associate
    every property to its closest centroid out of the three. Let''s follow how k-means
    proceeds at each iteration with the help of the figures. As you can see in the
    left handside of *Figure 5.43*, the grouping step has created three first cluster
    compositions, each one represented by a different color. The blue-colored properties
    (**C**, **A**, **B**, and **D**) are the closest ones to the blue centroid that
    overlaps with property **C**. The ones belonging to the red cluster (**G**, **E**,
    **F**, and **H**) are, instead, closest to the red centroid, **G**. Finally, the
    green cluster is made of the points (**I**, **L**, **M**, **N**, **O**, **P**,
    **Q**, and **R**) whose closest centroid is **I**. The next step for the algorithm
    is to update the centroids: considering the points falling into each cluster,
    it will be enough to calculate the actual center of mass of the cluster by averaging
    out the prices and the ages of the properties belonging to it. For example, let''s
    look at the green cluster: the properties forming this cluster tend to be older,
    leading the new centroid to be placed on the right side of the scatter plot. The
    centroid in the red cluster has instead moved toward the top: indeed, the properties
    associated with this cluster all have in common a higher price compared to point
    **C** (the old centroid):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_43.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.43: The first full iteration of k-means: with the update step, the
    centroids make a move'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can finally start the second iteration (*Figure 5.44*). Once again,
    we begin by grouping the points using the centroid we have just recalculated.
    As a consequence of this shift in the centroids, the clusters have changed, and
    some properties switched color: for instance, property **E** used to be red and
    is now blue as its closest centroid is now the blue one, and no longer the red
    one. The same applies to points **I** and **L**, which used to be green and are
    now red. It could appear that our algorithm has taken the right road as it is
    converging to a solution that makes sense: after this iteration, the clusters
    have changed in a way that makes their elements closer to each other. In the second
    step of the iteration, the algorithm will again update the centroids, taking into
    account the new compositions of the clusters. The most remarkable change is now
    in the red cluster, whose centroid has moved toward the bottom (where prices are
    lower), given the addition of properties **I** and **L** to the group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_44.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.44: The second iteration of k-means: the groups make more and more
    sense'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third iteration (*Figure 5.45*), the algorithm repeats the grouping
    step, and other properties change color (for instance, **M** moves from green
    to red, and **F** becomes blue). However, something new happens: despite having
    updated the centroids, the composition of the cluster does not change at all.
    This is the sign that our algorithm has found a stable solution and can be terminated,
    returning our final cluster composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_45.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.45: The third and last iteration of k-means: no more updates are possible
    and the algorithm converges'
  prefs: []
  type: TYPE_NORMAL
- en: 'This final cluster composition seems to be making a lot of sense. By looking
    at the scatter plots, we can also attempt a business interpretation of each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Blue properties (**A**, **B**, **C**, **D**, **E**, and **F**) are in the top-left
    corner of our diagram. They were all recently built and, as new properties, they
    tend to display a higher price than the rest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Red properties (**G**, **H**, **I**, **L**, and **M**) are in the bottom central
    part of the diagram and refer to buildings built in the seventies with lower quality
    materials; hence, their price is more accessible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the green points (**N**, **O**, **P**, **Q**, and **R**) are associated
    with older buildings, which tend to be more prestigious and come with a higher
    price tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters we obtained after only a handful of iterations of the k-means algorithm
    can certainly help real estate agents present convincing alternatives to potential
    buyers. Not bad for an algorithm repeating a set of simple mathematical steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural question that comes to mind when using k-means is: what is the right
    value of *k* or, in other words, how many clusters should I create? Even though
    there are some numerical techniques (check out the **Elbow method**, for instance)
    to infer an optimal value for *k*, the business practice of machine learning demands
    taking another, less mathematically rigorous approach. When choosing the number
    of clusters, the advice is to take a step back and think of the actual business
    utilization of the cluster definitions. The right question to ask becomes: how
    many clusters shall I create so that the result can be used in practice in my
    business case? In the example of segmenting consumers for personalizing communication,
    is it reasonable to create—let''s say—100 clusters of consumers if I can only
    afford to produce three versions of a newsletter at most? We will often use the
    business constraints for deciding a range of reasonable values of *k* and then
    pick the one that looks most interpretable. The moral of this story is that data
    analytics is a mix of art and science, and human judgment is often needed to guide
    algorithms to the right path.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving back to our tutorial flow, let''s go through a couple of considerations
    regarding "what can go wrong" when using a distance-based approach like k-means
    and how to avoid it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers can spoil the game**. If some points in your dataset exhibit extreme
    values, they will naturally "stay apart" from the rest, making the clustering
    exercise less meaningful. For example, imagine that in our real estate case, we
    have a single property with a price ten times higher than every other property:
    this exceptional property will probably make a cluster by itself. Most times,
    we don''t want this to happen, so we remove outliers upfront. The **Numeric Outliers**
    node will do the job for us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extreme range differences can make distance calculations unbalanced**. This
    one is easy to see through an example. Think again about the formula in *Figure
    5.41* for calculating distances: in the real estate example, it would leverage
    the differences in house prices (which are in the thousands of dollars) and the
    age differences (which, instead, vary in the area of dozens of years). The massive
    gap between the two orders of magnitude becomes even wider when you square them,
    as the formula provides. This means that the house prices will count disproportionally
    more than the age, making the latter almost meaningless. To fix this numeric disadvantage,
    we need to normalize all the measures used in k-means and reduce their scale to
    a common range (generally from zero to one) while keeping the differences across
    data points. This is what the **Normalizer** node (and its inverse companion,
    the **Denormalizer** node) will do for us in KNIME.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid these issues, remember this general advice: always remove outliers
    and normalize your data before applying k-means. With more practice and expertise,
    you might be able to "bend" these rules to meet your specific business needs at
    best, but in most cases, these two steps can only improve your clustering results,
    so they are no-brainers. Let''s now see how to apply them in KNIME.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing icon'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image079.png) *Numeric Outliers*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Statistics**) identifies outliers in a data table
    and manages them according to the needs. At the top of its configuration window
    (*Figure 5.46*), you can select which columns to consider in the outliers detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_46.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.46: Configuration window of Numeric Outliers node: what do you want
    to do with your extreme values?'
  prefs: []
  type: TYPE_NORMAL
- en: In the **Outlier Treatment** panel on the bottom right, you can decide how to
    manage outliers once detected. In particular, the **Treatment option** drop-down
    menu lets you choose whether you want to **Remove outlier rows** (so as to ignore
    them in the rest of the workflow), **Remove non-outlier rows** (so you keep *only*
    the outliers and study them further), or **Replace outliers values** (by either
    assigning them a missing value status or substituting them with the closest value
    within the permitted range—you can specify your preference in the **Replacement
    strategy** menu).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key parameter for setting the sensitivity to use in detecting outliers
    is the **Interquartile range multiplier (k)**, which you can set on the bottom-left
    area of the configuration window. To understand how it works, have a look at the
    **box plot** shown in *Figure 5.47*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_47.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.47: How to interpret a box-and-whisker plot: the box in the middle
    covers the central 50% of points in a distribution. Beyond the whiskers, you find
    outliers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Box plots show us at a glance the key features of a numeric distribution: quartile
    values (see in the picture **Q1**, **Q2**, which is the **Median**, and **Q3**)
    tell us where we could "cut" a population of sorted numbers so as to get 25% of
    the values in each slice. Now, look at the central box, whose length is called
    **Interquartile range** (**IQR**): within this range, we will find nearly 50%
    of the values of the population—this is the *core* of our distribution. Keeping
    this in mind, outliers can be defined as the values that lie *far* from this core.
    Typically, the values that are further than 1.5 times the interquartile range
    above the third quartile or below the first quartile are considered **mild outliers**.'
  prefs: []
  type: TYPE_NORMAL
- en: They are represented as circles in *Figure 5.47*, while the limit of mild outliers
    is represented by the dashed "whiskers" you see above and below the central box
    (this is why box plots are also known as box-and-whisker plots). If you increase
    the multiplier of the interquartile range to 3.0, you find the **extreme outliers**,
    which are shown as crosses in the figure. By editing the interquartile range multiplier
    parameter in the configuration dialog, you can tell the node how "aggressive"
    it should be in detecting outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s leverage our new node straight away on the CRM dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Numeric Outliers** node and connect it with the output port of
    the CSV reader. In its configuration window, deselect the column *Customer_ID*
    since we don't want to use it in our clustering. Since we are after extreme outliers,
    set `3.0` as the **Interquartile range multiplier (k)**, and select **Remove outlier
    rows** as the **Treatment option**. Finally, execute the node and have a look
    at its output ports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first output (**Treated table**) is the cleaned-up version of the table,
    showing only 3,772 rows: this means that we removed 10% of rows as they were considered
    outliers according to some columns. We could have played with the IQR multiplier
    value and increased it to 5.0 or more, so as to focus on more extreme values and
    remove fewer rows, but for the sake of this exercise, we can carry on with this.
    The second output of the node (**Summary**, shown in *Figure 5.48*) tells us the
    number of rows regarded as outliers according to each individual column (*Basket
    Size* seems to be the one displaying more extreme values):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_48.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.48: Summary output view of the Numeric Outliers node: which columns
    are causing most of the outliers?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s proceed with the second preparation step before applying k-means: normalize
    the data to a set range through the **Normalizer** node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image083.png) *Normalizer*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Manipulation > Column > Transform** in the node repository) normalizes
    all values in selected numerical columns of a dataset. In its configuration window
    (*Figure 5.49*), you first choose which columns to normalize and, then, pick a
    normalization method. The most useful one (especially indicated in conjunction
    with distance-based procedures like k-means clustering) is the **Min-Max Normalization**,
    which linearly projects the original range onto a predefined range (usually 0
    to 1, but you can manually edit the boundaries using the text boxes provided).
    With this normalization approach, the original minimum value is transformed to
    0, the maximum to 1, and everything in the middle is proportionally assigned to
    a value within the 0 to 1 range. Another popular normalization method is the **Z-Score
    Normalization (Gaussian)**, also known as **Standardization**. Using this method,
    each value is transformed into the number of standard deviations by which it is
    above or below the population''s mean. For instance, a Z-score of –3 means that
    the value is three standard deviations below the population''s average. This is
    useful when you want to assess how much your points deviate from their mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_49.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.49: Configuration window of the Normalizer node:select the columns
    to normalize and the method to apply'
  prefs: []
  type: TYPE_NORMAL
- en: 'The node has two outputs: the upper output port returns the table with normalized
    values, and the bottom (the cyan square) holds the normalization model. Such a
    model can restate the original values using the **Denormalizer** node, which we
    will encounter in a few pages.'
  prefs: []
  type: TYPE_NORMAL
- en: We now have all we need to proceed and normalize our outliers-less CRM data
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick the **Normalizer** node from the repository and connect its input to the
    first output of **Numeric Outliers**. The node configuration is straightforward:
    exclude the *Customer­­_ID* column from the normalization process by double-clicking
    on it and making sure it appears on the red box on the right. The default settings
    of the normalization method work well for us: indeed, the **Min-Max Normalization**
    with a range between 0 and 1 is great for calculating distances with algorithms
    such as k-means. Finally, click on **OK** and execute the node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you look at the first output of the **Normalizer** node, you will notice
    how the values of the affected columns are now falling in the desired range, which
    is exactly what we needed. Now, all columns will have the same weight in calculating
    distances based on the Pythagorean theorem. We can finally move on and introduce
    the critical node of the workflow, allowing us to cluster our customers: **k-Means**.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Diagram'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image0851.png) *k-Means*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Clustering**) clusters the rows of the input
    table using the k-means algorithm. The first parameter to be set as part of its
    configuration (*Figure 5.50*) is the **Number of clusters**, which can be chosen
    by entering an integer in the textbox at the very top. You can then choose the
    method for the **Centroid initialization**, which, by default, happens by random
    draw (you can still set a static random seed to make the process repeatable),
    and the maximum number of iterations used to force termination (it is preset to
    99, which, in most cases, is good enough since k-means would naturally converge
    in fewer iterations). The last configuration step is to choose which numeric columns
    to consider when clustering, which can be done using the **Column selection**
    panel at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_50.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.50: Configuration window of the k-Means node: select the columns to
    normalize and the method to apply'
  prefs: []
  type: TYPE_NORMAL
- en: Let's apply our new node to the normalized data and see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **k-Means** node and connect it downstream to the first output
    of the **Normalizer** node. We can keep its configuration simple, ensuring that
    the **Number of clusters** is set to 3 and deselecting *Customer_ID* from the
    list since we don't want to consider the column in the clustering exercise. Click
    on **OK** and then execute the node and open its main view (*Shift* + *F10*, or
    right-click and then select **Execute and Open Views...**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main view of the **k-Means** node (right-click on the node and then select
    **View: Cluster View** to make it appear if needed) will look similar to what
    you find in *Figure 5.51*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_51.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.51: Summary view of the k-Means node: we can start seeing what the
    three clusters look like'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This summary view is already telling us a lot: k-means segmented our customer
    base into three different groups of 830, 1,126, and 1,816 customers, respectively
    (see the **coverage** labels in the figure). If you open the different clusters
    (click on the **+** button on the left), you find a numeric description of the
    three centroids. According to what you see in *Figure 5.51*, for example, the
    first cluster (generically named **cluster_0** by KNIME) shows the smallest *Basket
    Size* of the three and the highest *Unique Products*. If you open the first output
    port of the node (right-click on the node and then select **Labeled input**),
    you will see that every row has been assigned to one of the three clusters, as
    indicated in the additional *Cluster* column (see *Figure 5.52*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_52.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.52: Output of the k-Means node: every customer—whether they like it
    or not—gets assigned to a cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As aligned with our business partner, the CRM manager, we need to go one step
    ahead and build a couple of visualizations to simplify the process of interpreting
    our clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: Before doing that, we realize that our values are still normalized and forced
    to fall within the 0 to 1 range. To make our visuals easier to interpret, we would
    prefer to come back to the original scales instead. To do so, we can revert the
    normalization by leveraging the **Denormalizer** node.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image087.png) *Denormalizer*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Manipulation > Column > Transform**) brings the values in a dataset
    back to their original range. It requires two input connections: the first one
    is the model generated by the previous **Normalizer** node, which carries a description
    of the normalization method and parameters. The second input is the normalized
    table to be denormalized. The node does not require any configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **Denormalizer** node and set up the wiring. The cyan output of
    the **Normalizer** node should be connected to the first input of the **Denormalizer
    node.** The first output of the **k-Means** node should be connected, instead,
    to the second input port of the **Denormalizer** node. You can have a sneak view
    of the final workflow in *Figure 5.57* to see how to get the connections right.
    After executing the node, you can see how the values have been reverted to their
    original range.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To build the visuals, we will need three more nodes. The first one (**Color
    Manager**) is required for assigning colors to the various rows of the dataset
    (according to the cluster), while the other two (**Scatter Matrix (local)** and
    **Conditional Box Plot**) will generate a couple of nice charts.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing icon'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/Image73183.png) *Color Manager*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Views > Property**) assigns colors to each row of a dataset. Its
    configuration window (*Figure 5.53*) asks you to select two things. First, you
    specify the nominal column used to evaluate what color to assign: every possible
    value associated with that column will correspond to a specific color. Second,
    you need to select the color set to adopt. On top of the three default color sets,
    you can also manually define which color to assign to each possible value of the
    nominal column. To do so, you will have to select **Custom** in the **Palettes**
    tab and then use one of the tabs on the right (such as **Swatches**, **RGB**,
    and **CMYK**) to pick the right color for each nominal value manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_53.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.53: Configuration of the Color Manager node: you can pick which color
    to assign to which value of the nominal column of your choice'
  prefs: []
  type: TYPE_NORMAL
- en: Add a **Color Manager** node and connect it to the output of the **Denormalizer**.
    Confirm the *Cluster* column in the drop-down menu at the top, and then select
    the color set of your choice. In the specific example of *Figure 5.53*, a custom
    palette has been manually created so that blue, orange, and green could be assigned
    to the three clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the colors are set, it's finally time to pull together the first chart
    with the help of a new node.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A close-up of a logo'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated with low confidence](img/image084.png) *Scatter
    Matrix (local)*
  prefs: []
  type: TYPE_NORMAL
- en: This node (**Views > Local (Swing)**) generates a matrix of scatter plots, displaying
    multiple combinations of variables in a single view. The node does not require
    any configuration, but you can optionally increase the maximum number of points
    that will be plotted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the **Scatter Matrix (local)** node after **Color Manager**. Execute
    and open its main view (*F10* after selecting the node). From the **Column Selection**
    tab at the bottom, you can choose which variables to display. In our case, let''s
    make sure we have only *Average Price*, *Basket Size*, and *Unique Products* selected
    on the right: you will end up with a visual similar to *Figure 5.54*:![Graphical
    user interface, application'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_54.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.54: Output view of the Scatter Matrix (local) node: your customers
    have become colored points. By looking at how the cloud of dots is scattered,
    you can interpret what each cluster is all about'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The scatter matrix we just obtained renders the result of the clustering in
    a more human-friendly way. As we look at it together with the CRM manager, we
    notice some initial clear patterns. For example, look at the chart at the top-right
    corner of *Figure 5.54*, which shows *Average Price* on the vertical axis and
    *Unique Products* on the horizontal axis. The blue cluster (cluster_0) clearly
    dominates the right-hand side of the chart, confirming that this is the segment
    of consumers that tend to try a more diverse set of products (high values of *Unique
    Products*). At the same time, the orange cluster (cluster_1) has customers that
    seem to go for less unique products and lower prices. Instead, the green cluster
    (cluster_2) includes those willing to pay more premium prices when shopping at
    our website. This is all starting to make sense, and the visual is already a big
    help in understanding how our clustering worked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add one last visual to clarify even further the composition of our segments:
    meet the **Conditional Box Plot** node.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image086.png) *Conditional Box Plot*
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Views > JavaScript**) produces an interactive view with multiple
    box plots, one for each value in a given categorical column. Such a view enables
    the parallel comparison of distributions. Its configuration window (*Figure 5.55*)
    requires selecting the **Category Column** to be used for differentiating parallel
    box plots and the choice of the numeric columns whose distribution will be visualized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_55.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.55: Configuration dialog of the Conditional Box Plot node: which distributions
    are you interested in comparing between?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag and drop the **Conditional Box Plot** node onto the workflow editor and
    connect it to the output port of the **Denormalizer** node. Select *Cluster* as
    **Category Column** and ensure that only **Average Price**, **Basket Size**, and
    **Unique Products** are on the right of the column selector placed at the center
    of the dialog. Click on **OK** and then press *Shift* + *F10* to execute it and
    open its main view. In the interactive window that appears, you can swap which
    distribution to visualize by operating on the **Selected Column** drop-down menu:
    you can find this selector by clicking on the icon at the far top-right of the
    interactive window.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output views of the **Conditional Box Plot** node (*Figure 5.56*) clarify
    even better the essential features of each cluster. The k-means algorithm was
    able to produce three homogeneous clusters with peculiar and differentiating characteristics.
    The box plots are great at showing such differences. As an example, take the third
    plot in the figure, which refers to *Unique Products*. The blue cluster dominates
    when it comes to this measure: the median number of unique products purchased
    by customers belonging to this segment is 32, while for the others it is near
    10\. The lack of visual overlap in height between the blue box and the other two
    means this difference is meaningful. On the other hand, the orange and the green
    clusters seem to be quite similar in terms of unique products, as the boxes are
    almost coinciding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, box and whisker chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_56.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.56: Outputs of the Conditional Box Plot node: you can readily appreciate
    the differences in the distributions across clusters'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now sit together with the CRM manager and, having the scatter matrix
    and the conditional box plots at hand, we can finally describe each customer segment
    and give a business-oriented interpretation of their meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: The blue cluster includes those **curious customers** who are willing to try
    different products. In our communication with this segment, we can give disproportionate
    space to the "new arrivals" and intrigue them with an ample selection of products
    they haven't tried yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orange cluster possibly comprises **small retailers** who buy "in bulk"
    from our website to resell their shops. They tend to buy relatively few products
    but in large quantities. We can offer them quantity discounts and regularly communicate
    the list of best-selling products, hopefully leading them to add our best-selling
    articles to their assortment for mutual business growth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The green cluster is made up of our **high-value customers**, who systematically
    put quality ahead of price in their shopping choices. Therefore, when communicating
    with them, we should advertise the premium end of the products portfolio and focus
    on topics such as the quality and the safety of our assortment, deprioritizing
    price-cut offers, and other types of promotional levers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using only 8 KNIME nodes (see the full workflow in *Figure 5.57*), we came
    up with a simple segmentation of customers and a first proposition of how to drive
    the most value when personalizing their experience. By uniting the business expertise
    of our partners (the CRM managers in this case) with the power of data and algorithms,
    such as k-means, we can make the magic happen!
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_57.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.57: The full workflow for segmenting consumers using clustering'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you touched on the serious potential behind data analytics
    and machine learning with your own hands. You have solved three real-world problems
    by putting data and algorithms at work.
  prefs: []
  type: TYPE_NORMAL
- en: In the first tutorial, you managed to predict with a decent level of accuracy
    the rental price of properties, collecting, in the process, a few interesting
    insights into real estate price formation. You have now acquired a proven methodology,
    based on the linear regression model, that you can replicate on many business
    cases where you have to predict numeric quantities.
  prefs: []
  type: TYPE_NORMAL
- en: In the second tutorial, you entered the fascinating world of classification
    and propensity modeling, experiencing firsthand the game-changing role of data
    analytics in marketing. You were able to put together a couple of classification
    models through which you met multiple business needs. First, you were able to
    reveal the "unwritten rules" that make a product generally attractive to customers
    by building and interpreting a decision tree model. Then, you built a random forest
    model that proved effective in anticipating the level of propensity of individual
    bank customers. Lastly, you managed to estimate the possible ROI of further marketing
    campaigns, unlocking serious value creation opportunities for a business. Also,
    in this case, you gained a series of general-purpose techniques that you can easily
    reapply in your own work every time you need to predict anything of business relevancy.
    By going through the tutorial, you also experienced the "back and forth" iterations
    needed to fine-tune machine learning models to fit your business needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our third tutorial, you experienced the power of unsupervised learning.
    You were able to put together a meaningful and straightforward customer segmentation
    that can be used to design personalized communication strategies and maximize
    the overall customer value. With this new algorithm, k-means, in your backpack,
    you can potentially cluster anything: stores, products, contracts, defects, events,
    virtually any business entity that can benefit from the algorithmic tidying that
    comes with clustering. Think about the value you can create by applying this new
    concept to the work items you deal with on a daily basis. In the process of learning
    k-means, we also got acquainted with the fundamental statistical concept of outliers
    and saw how to spot and manage them systematically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on and learn how we can make our data accessible to our business
    partners through self-service dashboards. It''s time to meet our next travel companion
    in the data analytics journey: Power BI.'
  prefs: []
  type: TYPE_NORMAL
