- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Applying Machine Learning at Work
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在工作中应用机器学习
- en: 'You''ve heard a lot about creating business value with intelligent algorithms:
    it''s finally time to roll up our sleeves and make it happen. In this chapter,
    we are going to experience what it means to apply machine learning to tangible
    cases by going through a few step-by-step tutorials. Our companion KNIME is back
    on stage: we will learn how to build workflows for implementing machine learning
    models using real-world data. We are going to meet a few specific algorithms and
    learn the intuitive mechanisms behind how they operate. We''ll glimpse into their
    underlying mathematical models, focusing on the basics to comprehend their results
    and leverage them in our work.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经听说过如何通过智能算法创造商业价值：现在是时候大显身手了。在本章中，我们将通过几个逐步教程，亲身体验将机器学习应用于具体案例的过程。我们的伙伴 KNIME
    回到了舞台上：我们将学习如何构建工作流来使用实际数据实施机器学习模型。我们将接触几个特定的算法，了解它们背后直观的工作原理。我们将窥视它们的数学模型，重点理解基本原理，以便理解它们的结果并在工作中加以利用。
- en: 'This practical chapter will answer several questions, including:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本实用章节将回答几个问题，包括：
- en: How do I make predictions using supervised machine learning algorithms in KNIME?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使用 KNIME 中的监督式机器学习算法进行预测？
- en: How can I check whether a model is performing well?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何检查模型是否表现良好？
- en: How do we avoid the risk of overfitting?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何避免过拟合的风险？
- en: What techniques can I use to improve the performance of a model?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以使用什么技巧来提高模型的性能？
- en: How can I group similar elements together using clustering algorithms?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使用聚类算法将相似的元素分组？
- en: 'The tutorials included in this chapter cover three of the most recurrent cases
    when you can rely on machine learning as part of your work: predicting numbers,
    classifying entities, and grouping elements. Think of them as "templates" that
    you can widely reapply after you reach the last page of the chapter and that you
    are likely to keep using as a reference. The steps of the tutorials are also organized
    in the same order they would unfold in everyday practice, including the "back
    and forth" iterations required for improving the performance of your model. This
    will prepare you to face the actual use of real-life machine learning, which often
    follows a circuitous route made of trial and error attempts.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的教程涵盖了你可以在工作中依赖机器学习的三种最常见情况：预测数字、分类实体和分组元素。你可以把它们看作是“模板”，在你翻到本章最后一页后，可以广泛地重新应用，并且你很可能会继续将其作为参考。教程的步骤也按它们在日常实践中展开的顺序进行组织，包括为了提高模型性能所需的“反复迭代”。这将帮助你准备面对实际的机器学习应用，现实中机器学习往往需要经历一条曲折的试错路线。
- en: Within each tutorial, you will encounter one or two machine learning algorithms
    (specifically, **linear regression** in the first, **decision tree** and **random
    forest** in the second, and **k-means** in the third) that will be introduced
    and explained before being seen in action. Let's get started with some first predictions!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个教程中，你将遇到一两个机器学习算法（具体来说，第一个是**线性回归**，第二个是**决策树**和**随机森林**，第三个是**K-means**），这些算法将在实际应用之前进行介绍和解释。让我们从一些初步的预测开始吧！
- en: Predicting numbers through regressions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过回归预测数字
- en: 'For this tutorial, you will assume the—somewhat—enviable role of a real estate
    agent based in Rome, Italy. The company you work for owns multiple agencies specialized
    in rentals of properties located in the broader metropolitan area of the Eternal
    City. Your passion for data analytics got you noticed by the CEO: she asked you
    to figure out a way to support agents in objectively evaluating the fair monthly
    rent of a property based on its features. She noticed that the business greatly
    suffers when the rent set for a property is not aligned with the market. In fact,
    if the rent is too low, the agency fee (which is a fixed percentage of the agreed
    rent) will end up being lower than what it could have been, leaving profit on
    the table. On the other hand, if the ask is too high, revenues for the agency
    will take longer to materialize, causing a substantial impact on the cash flow.
    The traditional approach to set the monthly rent for new properties is a "negotiation"
    between owners and agents, who will use their market understanding (and sometimes
    the benchmark of similar properties) to convince the owners about the right rent
    to ask for.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将假设自己扮演的是一名位于意大利罗马的房地产经纪人的—有点—令人羡慕的角色。你所在的公司拥有多家专注于出租位于永恒之城更广泛都市区的物业的代理机构。你对数据分析的热情引起了首席执行官的注意：她要求你找到一种方法，帮助代理商根据物业的特点客观评估合理的月租。她注意到，当物业的租金与市场不符时，业务将受到严重影响。事实上，如果租金过低，代理费（这是约定租金的固定百分比）最终会低于本应有的水平，导致利润流失。另一方面，如果租金过高，代理机构的收入将需要更长时间才能显现，影响现金流。为新物业设定月租的传统方法是房东与代理商之间的“谈判”，代理商会利用他们对市场的了解（有时还会参考类似物业的基准）来说服房东设定合适的租金。
- en: 'You are sure that machine learning has the potential to make a difference,
    and you are resolute in finding an ML way to improve this business process. The
    idea that comes to mind is to use the database of the monthly rent of previously
    rented properties (for which we have available their full description) to predict
    the right monthly rent of future properties based on their objective characteristics.
    Such a data-driven approach, if well communicated, can ease the price-setting
    process and result in a mutual advantage for all the parties involved: the landlord
    and the agency will get a quick and profitable transaction, and the tenant will
    obtain a fair rent.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你确信机器学习有潜力带来改变，并决心找到一种机器学习的方法来改善这一业务流程。你想到的一个方法是使用以前租赁物业的月租数据库（这些物业有完整的描述）来预测未来物业的合理月租，基于其客观特征。这样一种数据驱动的方法，如果传达得当，可以简化定价过程，带来各方的共同利益：房东和代理机构能够迅速达成有利可图的交易，租客则能够获得公正的租金。
- en: 'The prospect of building a machine able to predict rental prices is exhilarating
    and makes you impatient to start. You manage to obtain an extraction of the last
    4,000 rental agreements signed at the agency (`RomeHousing-History.xlsx`). The
    table contains, for each property:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 能够预测租金价格的机器的前景令人兴奋，让人迫不及待地想开始。你设法获取了最近签署的 4,000 份租赁协议的提取数据（`RomeHousing-History.xlsx`）。表格包含了每个物业的以下信息：
- en: '*House_ID*: a unique identifier of the property.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*House_ID*: 物业的唯一标识符。'
- en: '*Neighborhood*: the name of the area where the property lies, ranging from
    the fancy surroundings of `Piazza` `Navona` to the tranquil, lakeside towns of
    `Castelli` `Romani`. *Figure 5.1* shows a map of the Rome area with some of these
    neighborhoods.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Neighborhood*: 物业所在区域的名称，从`Piazza` `Navona`周围的豪华区域到宁静的湖畔小镇`Castelli` `Romani`。*图5.1*展示了罗马地区的地图，标出了这些社区的一部分。'
- en: '*Property_type*: a string clarifying if the property is a `flat`, a `house`,
    a `villa`, or a `penthouse`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Property_type*: 一个字符串，说明物业是`flat`（公寓）、`house`（房屋）、`villa`（别墅）还是`penthouse`（顶层公寓）。'
- en: '*Rooms*: the number of available rooms in the property, including bathrooms.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rooms*: 物业中可用房间的数量，包括浴室。'
- en: '*Surface*: the usable floor area of the property in square meters.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Surface*: 物业的可用楼面面积，单位为平方米。'
- en: '*Elevator*: a binary category indicating if an elevator is available (`1`)
    or not (`0`).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elevator*: 一个二元类别，表示物业是否有电梯（`1`表示有，`0`表示没有）。'
- en: '*Floor_type*: a category showing if the property is on a `Mezzanine`, a `Ground`
    floor, or an `Upper` level.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Floor_type*: 一个类别，显示物业所在的楼层是`Mezzanine`（夹层）、`Ground`（底层）还是`Upper`（上层）。'
- en: '*Floor_number*: the floor number on which the property is situated, based on
    the European convention (`0` is for the ground floor, `0.5` is the mezzanine,
    `1` is for the first level above the ground, and so on).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Floor_number*：物业所在的楼层，按照欧洲惯例（`0`表示底层，`0.5`表示夹层，`1`表示地面以上的第一层，依此类推）。'
- en: '*Rent*: the all-inclusive, monthly rent in euros on the final rental agreement.![Map'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rent*：最终租赁协议中的所有包括项，即月租金，以欧元计算。[地图图像]'
- en: Description automatically generated](img/B17125_05_01.png)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/B17125_05_01.png)'
- en: 'Figure 5.1: The Rome neighborhoods covered by our real estate. Have you visited
    any of these places already?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：我们的房地产覆盖的罗马邻里。你去过这些地方吗？
- en: 'Before building the model, you wisely stop for a second and think through the
    ways you are going to practically leverage it once ready. You realize that the
    potential business value for completing this endeavor is two-fold:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型之前，你明智地停下来，认真思考一旦模型完成后，如何实际运用它。你意识到，完成这一工作潜在的商业价值有两个方面：
- en: 'First, by interpreting how the model works, you can find out some insightful
    evidence on the market price formation mechanisms. You might be able to find answers
    to the questions: *what features really do make a difference in the pricing?*,
    *does the floor number impact the value greatly?*, and *which neighborhoods prove
    to be most expensive ones, at parity of all other characteristics of the property?*.
    Some of the answers will reinforce the market understanding that your agency already
    has, adding the benefit of making this knowledge explicit and formally described.
    More interestingly, other findings might be truly unexpected and unveil original
    dynamics you did not know about.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过解读模型如何运作，你可以发现一些关于市场价格形成机制的深刻证据。你可能能够找到以下问题的答案：*哪些特征在定价中真正起到了决定性作用？*、*楼层数是否对价值有很大影响？*以及*在其他特征相同的情况下，哪些邻里最为昂贵？*其中一些答案将强化你所在机构已有的市场理解，同时增加将这些知识明确化并正式描述的好处。更有趣的是，其他发现可能是完全意外的，并揭示出你之前未曾了解的原始动态。
- en: Second, your model can be used to generate data-based recommendations on the
    rent to be set for new properties as they go on the market and enter the portfolio
    of the agency. To make things more interesting on this front, the owner shares
    with you a list (`RomeHousing-NewProperties.xlsx`) of 10 incoming properties for
    which the rental price has not been fixed yet, using the same features (such as
    *Neighborhood*, *Property_type*, and so on) available in the historical database.
    Once ready, you will apply your model to these sample properties as an illustration
    of how it works.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，你的模型可以用来生成基于数据的租金建议，当新物业进入市场并加入机构的投资组合时。为了让这方面的事情更加有趣，业主向你分享了一份名为(`RomeHousing-NewProperties.xlsx`)的列表，里面列出了10个尚未确定租金的即将上市的物业，这些物业使用与历史数据库中相同的特征（如*Neighborhood*、*Property_type*等）。一旦模型完成，你将运用它来对这些样本物业进行示范，展示它的工作原理。
- en: 'You are now clear on what the business requires, and you can finally translate
    it into definite machine learning terms, building on what we have learned in the
    previous chapter. You need to build a machine that predicts "unknown" rental prices
    by learning from some "known" examples: the database of previously rented properties
    is your *labeled* dataset, as it has examples of your target variable, in this
    case, the *Rent*. Going through the catalog of machine learning algorithms (*Figure
    4.5*), you realize we are clearly in the category of *supervised* machine learning.
    More specifically, you need to predict numbers (rent in euros), so you definitely
    need to leverage an algorithm for doing a *regression*.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经清楚了业务需求，最终可以将其转化为具体的机器学习术语，基于我们在上一章学到的内容。你需要建立一个通过学习一些“已知”示例来预测“未知”租金价格的机器：以前租赁过的物业数据库就是你的*标签化*数据集，因为它包含了目标变量的示例，在本案例中，即*Rent*。浏览机器学习算法的目录（*图4.5*），你意识到我们显然处于*监督式*机器学习的范畴。更具体地说，你需要预测数值（欧元租金），因此你确实需要利用一个进行*回归*的算法。
- en: 'The ML way to solve this business opportunity is now clear in front of your
    eyes: you can finally get KNIME started and create a new workflow (**File** |
    **New…** | **New KNIME Workflow**):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，解决这个商业机会的机器学习方法已经清晰地出现在你面前：你终于可以启动KNIME并创建一个新的工作流（**文件** | **新建...** | **新建KNIME工作流**）：
- en: As a very first step, you load your labeled dataset by dragging and dropping
    the file(`RomeHousing-History.xlsx`) into your blank workflow or by implementing
    the**Excel Reader**node. In either case, KNIME will have recognized the structure
    of the file, and you just need to accept its default configuration. After running
    the node, you obtain the dataset shown in *Figure 5.2*, where you find the nine
    columns you expected:![](img/B17125_05_02.png)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，你通过将文件（`RomeHousing-History.xlsx`）拖放到空白工作流中，或者通过使用**Excel Reader**节点来加载带标签的数据集。在这两种情况下，KNIME都会识别文件的结构，你只需接受其默认配置。运行节点后，你将获得如*图
    5.2*所示的数据集，其中包含你预期的九列：![](img/B17125_05_02.png)
- en: 'Figure 5.2: Historical rental data loaded into KNIME: 4,000 properties to learn
    from'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.2：加载到KNIME中的历史租赁数据：4,000个房产供学习使用
- en: When you build a machine learning model, you will interact in various ways with
    the columns of your data table. It is sensible to get an understanding of what
    you are going to deal with by exploring the columns right at the beginning. Fortunately,
    the **Statistics** node helps as it displays at once the most important things
    you need to know about your columns.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建机器学习模型时，你将以各种方式与数据表中的列进行交互。通过在一开始就探索这些列，了解你将要处理的数据是非常明智的。幸运的是，**统计**节点可以帮助你，它会立即显示你需要了解的列的最重要信息。
- en: '![](img/NEW_statistics_node.png) *Statistics*'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/NEW_statistics_node.png) *统计*'
- en: 'This node (**Analytics > Statistics**) calculates summary statistics for each
    column available in the input table. The checkbox appearing at the top of its
    configuration dialog (*Figure 5.3*) lets you decide whether to **Calculate median
    values** of the numeric columns: this calculation might be computationally expensive
    for large datasets, so you will tick it only if necessary. The column selector
    in the middle lets you decide which columns should be treated as **Nominal**.
    For these columns, the node will count the number of instances of each unique
    value: this is useful for categorical columns when you want to quickly assess
    the relative footprint of every category in a table. The main summary metrics
    calculated by the node are minimum (**Min**), average (**Mean**), **Median**,
    maximum (**Max**), standard deviations (**Std. Dev.**), **Skewness**, **Kurtosis**,
    count of non-numeric values such as missing values (**No. Missing**), and plus
    or minus infinite (**No. +∞**, **No. –∞**). The node will also output the histograms
    showing the distributions of the values and, for nominal columns, the list of
    the most and least numerous categories identified in the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 统计**）计算输入表中每一列的摘要统计信息。其配置对话框顶部出现的复选框（*图 5.3*）让你决定是否**计算数值列的中位数值**：对于大数据集来说，这个计算可能会很耗费计算资源，因此只有在必要时才勾选。中间的列选择器让你决定哪些列应该被视为**名义型**。对于这些列，节点会计算每个唯一值的实例数：这对于分类列非常有用，当你想快速评估表格中每个类别的相对分布时。节点计算的主要摘要指标包括最小值（**Min**）、平均值（**Mean**）、**中位数**、最大值（**Max**）、标准差（**Std.
    Dev.**）、**偏度**、**峰度**、非数值数据（如缺失值）的计数（**No. Missing**），以及正负无穷大（**No. +∞**、**No.
    –∞**）。节点还会输出显示值分布的直方图，并且对于名义型列，会列出数据集中最常见和最少见的类别：
- en: '**Skewness** and **Kurtosis** are certainly the least known summary statistics
    among the ones mentioned above. However, they are useful in telling you quickly
    how much the shape of a distribution differs from the iconic bell-shaped curve
    of a pure Gaussian distribution. Skewness tells you about the symmetry of the
    distribution: if it has a positive value, it is skewed on the left while if it
    has a negative value, it is skewed on the right. Kurtosis tells you about the
    flatness of the distribution: if negative it is flatter than a bell curve, while
    if positive it shows a sharper peak.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏度**和**峰度**无疑是上述摘要统计中最不为人知的。然而，它们对于迅速告诉你分布的形状与纯高斯分布的典型钟形曲线有多大不同非常有用。偏度告诉你分布的对称性：如果它的值为正，则表示分布左偏；如果值为负，则表示分布右偏。峰度告诉你分布的平坦程度：如果为负值，则分布比钟形曲线更平坦；如果为正值，则分布的峰值更尖锐。'
- en: '![](img/B17125_05_03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_03.png)'
- en: 'Figure 5.3: Configuration of Statistics: explore the data with its summary
    statistics'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：统计配置：通过其摘要统计探索数据
- en: 'Implement the **Statistics** node and connect it with the previous one. When
    configuring it, check the first box so we can have a look at the median values
    of the numeric columns. In the selector of the nominal values, keep only the string-typed
    columns (*Neighborhood*, *Property_type*, and *Floor_type*) plus *Elevator*. Although
    formally numeric, this latter column splits our samples into two categories, the
    properties equipped with the elevator and the ones missing it: it will be interesting
    to read a count of how many properties fall into each category, so we shall treat
    this column as nominal. If you run the node and display its main output (just
    press *Shift* + *10* or, after you execute the node, right-click on it and select
    **View: Statistics View**) you will obtain a window with three useful tabs. The
    first one (*Figure 5.4*) gives you all the highlights on the numeric columns:
    we learn that the average rent of the properties in our database is slightly above
    €1,000 and that the median floor surface is around 70 square meters. We also learn
    that there are no missing values: this is good news as we don''t need to engage
    in clean up chores:![A picture containing table'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**Statistics**节点并将其与之前的节点连接。当配置时，勾选第一个框，以便我们可以查看数值列的中位数值。在名义值的选择器中，仅保留字符串类型的列（*Neighborhood*，*Property_type*，和*Floor_type*）以及*Elevator*。尽管这个列在形式上是数值型的，但它将我们的样本分为两类：配备电梯的房产和没有电梯的房产：读取每个类别中有多少房产将会很有意思，因此我们将把这个列视为名义列。如果你运行节点并显示其主要输出（只需按*Shift*
    + *10*，或者在执行节点后，右击它并选择**查看：统计视图**），你将获得一个包含三个有用标签的窗口。第一个标签（*图 5.4*）展示了数值列的所有重点：我们了解到，数据库中房产的平均租金略高于€1,000，中位数的楼层面积约为70平方米。我们还了解到没有缺失值：这对我们来说是好消息，因为我们无需进行清理工作：![A
    picture containing table
- en: Description automatically generated](img/B17125_05_04.png)
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_04.png)
- en: 'Figure 5.4: Numeric panel within the Statistics output: how are my numeric
    features distributed?'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.4：统计输出中的数值面板：我的数值特征如何分布？
- en: 'The second and third (*Figure 5.5*) tabs tell you about the nominal columns:
    we learn that some neighborhoods (such as `Magliana` and `Portuense`) are much
    less represented in our dataset than others. By looking at the values in the *Property_type*
    column, we also learn that the vast majority of our rented properties have been
    flats:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二和第三个（*图 5.5*）标签向你展示了名义列：我们了解到，某些社区（如`Magliana`和`Portuense`）在我们的数据集中比其他社区的表示要少得多。通过查看*Property_type*列的值，我们还了解到，我们租赁的绝大多数房产都是公寓：
- en: '![](img/B17125_05_05.png)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_05.png)'
- en: 'Figure 5.5: Top/bottom panel within the Statistics output: check the values
    of your categorical columns'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.5：统计输出中的顶部/底部面板：检查你的类别列的值
- en: 'Now that we have explored the dataset and have become acquainted with the main
    characteristics of its columns, we can proceed with the fun part and design our
    model. To build a robust supervised machine learning model, we need to rely on
    the typical flow that we encountered in the previous chapter. Let''s refresh our
    memory on this critical point: in order to stay away from the trap of overfitting,
    we need to partition our labeled data into training and test sets, learn on the
    training set, predict on the test set, and—finally—assess the expected performance
    of the model by scoring the predicted values. You can go back to *Chapter 4*,
    *What is Machine Learning?*, and check *Figure 4.13* out to see once again the
    full process: we are always required to follow this approach when implementing
    a machine that can predict something useful. So, the very first step is to randomly
    partition all our labeled data rows into two separate subsets. This is exactly
    the "specialty" of our next node: **Partitioning**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了数据集，并了解了各列的主要特征，我们可以继续进行有趣的部分，设计我们的模型。为了构建一个强健的监督学习模型，我们需要依赖上一章中遇到的典型流程。让我们在这一关键点上刷新记忆：为了避免过拟合的陷阱，我们需要将标记数据随机划分为训练集和测试集，在训练集上学习，在测试集上预测，并最终通过评分预测值来评估模型的预期性能。你可以回到*第4章*，*什么是机器学习？*，查看*图
    4.13*，再次了解完整的过程：在实现一个能够预测有用内容的机器时，我们总是需要遵循这一方法。因此，第一步是将所有标记数据行随机划分为两个独立的子集。这正是我们下一个节点的“专长”：**Partitioning**。
- en: '![](img/image007.png) *Partitioning*'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/image007.png) *划分*'
- en: 'This node (**Manipulation > Row > Transform**) performs a row-wise split of
    the input table into two tables corresponding to the upper (first partition) and
    lower (second partition) output ports. The selector at the top of its configuration
    window (*Figure 5.6*) lets you set the size of the first partition (upper output
    port). You can either specify the number of rows to be included (**Absolute**)
    or the relative size of the partition in percentage points (**Relative[%]**).
    The second selector specifies the method used for splitting the rows into the
    two partitions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**Manipulation > Row > Transform**）执行输入表格的按行拆分，将其拆分为两个表格，分别对应上（第一部分）和下（第二部分）输出端口。其配置窗口顶部的选择器（*图
    5.6*）允许你设置第一部分（上输出端口）的大小。你可以指定要包含的行数（**绝对值**），或者指定该部分的相对大小（以百分比表示，**相对[%]**）。第二个选择器指定了将行拆分成两个部分时所使用的方法：
- en: '**Take from top**: if you select this option, the split will happen according
    to the current sorting order. The top rows of the input table will end up in the
    first partition while all others, after a certain threshold, will go to the second.
    The position of the threshold depends on the size of the partition that you have
    already decided above.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从顶部取**：如果选择此选项，拆分将按照当前排序顺序进行。输入表格的顶部行将进入第一部分，而所有其他行在超过某个阈值后将进入第二部分。阈值的位置取决于你在上面已决定的部分大小。'
- en: '**Linear sampling**: also, in this case, the order of the input table rows
    is preserved: every *n*^(th) row will go to an output port, alternating regularly
    across the two partitions. If, for instance, you run a linear sampling for creating
    two equally sized partitions (each having half of the original rows), you will
    end up with all the odd rows in a partition and all the even ones in the other.
    If, instead, the split is one-third and two-thirds, you will have every third
    row in the first partition and all others in the second one. This is particularly
    useful when your dataset is a time series, with records sorted chronologically.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性抽样**：在这种情况下，输入表格行的顺序得以保持：每隔 *n*^(行)将会进入一个输出端口，两个部分交替分配。如果，举例来说，你进行线性抽样，目的是创建两个大小相等的部分（每部分包含原始行数的一半），那么你将会得到所有奇数行在一个部分，所有偶数行在另一个部分。如果拆分是三分之一和三分之二，那么第一部分会有每三行中的一行，第二部分则包含所有其他行。如果你的数据集是时间序列，并且记录按时间顺序排列，这种方法特别有用。'
- en: '**Draw randomly**: if you go for this option, you obtain a random sampling.
    The only thing you can be sure of is that the number of rows in the first partition
    will be exactly what you have set in the first selector.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机抽取**：如果选择此选项，你将获得一个随机抽样。唯一可以确保的是，第一部分中的行数将恰好是你在第一个选择器中设置的行数。'
- en: '**Stratified sampling**: in this case, you also run a random sampling but,
    you force the distribution of a nominal column to be preserved in both output
    partitions. For example: if you have an input table describing 1,000 patients,
    out of which 90% are labeled as `negative` and 10% as `positive`, you can use
    stratified sampling to retain the ratio between positive and negative patients
    in each partition. In this case, if you want to have 700 rows to go to the first
    partition, you will end up with exactly 630 negative patients and 70 positive
    ones: the proportion is kept.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层抽样**：在这种情况下，你同样执行随机抽样，但你强制保持一个名义列的分布在两个输出部分中。例如：如果你有一个描述1,000名患者的输入表格，其中90%标记为`negative`（阴性），10%标记为`positive`（阳性），你可以使用分层抽样来保持每个拆分部分中阳性与阴性患者的比例不变。在这种情况下，如果你希望将700行分配到第一部分，你将得到恰好630个阴性患者和70个阳性患者：比例保持不变。'
- en: 'If you have selected a splitting method based on a random selection (the last
    two options in the list above), you can protect the reproducibility of your workflow
    by ticking the **Use random seed** optional box. When you specify a constant number
    for initializing the random sampling, you are "fixing" the random behavior: as
    a result, you will always obtain the same partitions every time you execute the
    node. This is handy when you want to keep the partitioning constant as you go
    back and forth in the construction of your workflow or when you want other people
    to get the same partitioning on their machines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择了基于随机选择的拆分方法（上面列表中的最后两个选项），你可以通过勾选**使用随机种子**选框来保护工作流的可重现性。当你指定一个常数值来初始化随机抽样时，你是在“固定”随机行为：结果是每次执行节点时，你都会得到相同的拆分。这在你希望在构建工作流时保持拆分一致，或者希望其他人在他们的机器上得到相同拆分时非常方便：
- en: '![Graphical user interface, text, application'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序'
- en: Description automatically generated](img/B17125_05_06.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_06.png)
- en: 'Figure 5.6: Configuration dialog of Partitioning: how do you want to split
    your dataset?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：分割配置对话框：你希望如何分割数据集？
- en: One thing that computers really struggle with is behaving randomly and doing
    anything "unexpected" as they are built and programmed to follow a deterministic
    set of steps. For this reason, computers leverage special algorithms for generating
    sequences of **pseudo-random numbers** that "look" as if they are truly random.
    Notably, the starting point of these sequences (the **random seed**) can determine
    the full progression of numbers. When needed, a computer can still generate a
    random seed by looking at a quickly changing state (like the number of clock cycles
    of the CPU from the last boot) or by measuring some microscopic physical quantities
    (like a voltage on a port) that are affected by uncontrollable phenomena, such
    as thermal noise and other quantic effects. It's interesting how computers struggle
    with what would take us just a flip of a coin!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机真正难以应对的一件事是表现得随机并执行任何“意外”的操作，因为它们是根据一套确定性的步骤来构建和编程的。因此，计算机利用特殊算法生成**伪随机数**序列，这些序列“看起来”就像是真正的随机数。值得注意的是，这些序列的起始点（**随机种子**）可以决定数字的完整进程。必要时，计算机仍然可以通过查看快速变化的状态（如从上次启动以来CPU时钟周期的数量）或通过测量一些微观物理量（如端口上的电压），这些量受到不可控现象的影响，如热噪声和其他量子效应，从而生成随机种子。很有趣的是，计算机在处理这些任务时会遇到我们只需抛个硬币就能完成的事情！
- en: 'Let''s start our supervised learning typical flow and split our full housing
    dataset into training and test subsets:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始典型的监督学习流程，并将完整的房屋数据集分割为训练集和测试集：
- en: Let's implement the **Partitioning** node and connect it with the output of
    the **Excel Reader** output (you can keep the **Statistics** node unhooked as
    we don't need to use its outputs). In the configuration dialog, let's make sure
    that we select the **Relative[%]** option with the value `70`. This means that,
    out of the 4,000 properties available at the inputs, 70% of them will be used
    for training (which is a fair thing to do since, as anticipated in *Chapter 4*,
    *What is Machine Learning?*, the training set should normally cover between 70%
    and 80% of the total full dataset). We want the partitioning to happen randomly.
    In the previous step, we noticed that some nominal columns (like *Neighborhood*)
    display an unbalanced distribution across their values. This means that we have
    the risk of having the very few properties in a smaller neighborhood (like the
    26 rows referring to `Magliana`) ending up solely in a partition. Although this
    is not strictly required, we better avoid any unbalance that can affect our learning
    and select **Stratified sampling** on *Neighborhood* in the dialog. You can also
    click on the bottom tick box and, on the right, type in a random seed, like `12345`,
    so that you can count on the same partitioning over and over. When you run the
    node, you find that in the upper output port (right-click on the node and select
    **First partition**) you find 2,800 rows that are exactly 70% of the original
    dataset. This is a good sign and we can move ahead with the learning step.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现**分割**节点，并将其与**Excel 阅读器**的输出连接（你可以保持**统计**节点未连接，因为我们不需要使用它的输出）。在配置对话框中，我们确保选择**相对[%]**选项，值为`70`。这意味着，在输入的4,000个属性中，将有70%的数据用于训练（这是合理的做法，因为如*第4章*中所述，*什么是机器学习？*，训练集通常应涵盖总数据集的70%到80%）。我们希望分割是随机进行的。在上一步中，我们注意到一些名义列（如*Neighborhood*）在其值的分布上存在不平衡。这意味着，我们有可能在一个较小的邻里（如与`Magliana`相关的26行数据）中仅仅获得少量的属性，进而仅将这些数据分配到一个分区。虽然这并非严格要求，但我们最好避免任何可能影响学习的不平衡，因此在对话框中选择**分层抽样**，并对*Neighborhood*进行处理。你也可以勾选下方的复选框，并在右侧输入一个随机种子，如`12345`，这样你就可以确保每次都得到相同的分区。当你运行节点时，你会发现，在上方的输出端口（右键点击节点并选择**第一个分区**）中，你将找到2,800行数据，正好是原始数据集的70%。这是一个好兆头，我们可以继续进行学习步骤。
- en: At this point, we need to add the nodes (both learner and predictor) that implement
    the specific machine learning algorithm we want to use. The simplest algorithm
    for predicting numbers is **linear regression,** which is what we are going to
    use in this tutorial. It's worth introducing first the underlying mathematical
    model so that we can get ready to interpret its results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要添加实现特定机器学习算法的节点（包括学习器和预测器）。预测数字的最简单算法是**线性回归**，这也是我们在本教程中使用的算法。值得先介绍一下其基础数学模型，以便我们为理解其结果做好准备。
- en: Linear regression algorithm
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归算法
- en: 'The linear regression model is a generalization of the simple regression we
    have used to predict second-hand car prices in *Chapter 4*, *What is Machine Learning?*.
    In that case, we modeled the price as a straight line, following the simple equation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型是我们在*第4章*《什么是机器学习？》中用来预测二手车价格的简单回归的推广。在那种情况下，我们将价格建模为一条直线，遵循简单的方程：
- en: '![](img/B17125_05_001.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_001.png)'
- en: 'where *y* was the dependent variable, so the target of our prediction (the
    price of the car), *x*, was the only independent variable (in that case, the age
    of the car in years) and ![](img/B17125_05_002.png) and ![](img/B17125_05_003.png)
    were the parameters of the model, defining the *height* of the line (also known
    as the *offset* or *intercept*) and its *slope*, respectively:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*y* 是因变量，因此是我们预测的目标（汽车的价格），*x* 是唯一的自变量（在那个例子中是汽车的年龄，单位为年），而 ![](img/B17125_05_002.png)
    和 ![](img/B17125_05_003.png) 是模型的参数，分别定义了直线的*高度*（也称为*偏移量*或*截距*）和*斜率*：
- en: '![](img/B17125_05_07.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_07.png)'
- en: 'Figure 5.7: Linear regression of car prices: the line shows the prediction
    as the age varies'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：汽车价格的线性回归：随着年龄变化，直线显示了预测值
- en: 'Specifically, as you can see in *Figure 5.7*, we have ![](img/B17125_05_004.png)
    (it''s where the model line encounters the vertical axis) and ![](img/B17125_05_005.png)
    so the price of the car is predicted through the simple model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，正如你在*图 5.7*中看到的那样，我们有 ![](img/B17125_05_004.png)（它是模型线与纵轴相交的地方）和 ![](img/B17125_05_005.png)，因此汽车的价格可以通过这个简单模型预测：
- en: '![](img/B17125_05_006.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_006.png)'
- en: 'The price of a 2-year-old car will be estimated to be $12,600, since:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆2年车龄的汽车的价格将被估计为 $12,600，因为：
- en: '![](img/B17125_05_007.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_007.png)'
- en: The purpose of the *learner* algorithm of a simple linear regression is to find
    the right parameters (![](img/B17125_05_008.png) and ![](img/B17125_05_009.png))
    that minimize the error of a prediction, while the *predictor* algorithm will
    just apply the model on new numbers, like we did when we came up with the estimated
    price of a 2-year-old car.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归的*学习器*算法的目的是找到正确的参数（![](img/B17125_05_008.png) 和 ![](img/B17125_05_009.png)），以最小化预测误差，而*预测器*算法则会将模型应用于新的数据，就像我们在得出2年车龄汽车估计价格时所做的那样。
- en: 'Linear regression is a generalization of the simple model that we have just
    seen in action. Its underlying mathematical description is:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是我们刚刚看到的简单模型的推广。其基础数学描述为：
- en: '![](img/B17125_05_010.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_010.png)'
- en: 'where *y* is still the (single) target variable that we are trying to predict,
    the various *x*[i] values represent the (many) independent variables that correspond
    to the features we have available, and the ![](img/B17125_05_011.png) values are
    the parameters of the model that define its "shape." Since we have several independent
    variables this time (for this reason, we call it a **multivariate model**), we
    cannot "visualize" it any longer with a simple line on a 2D chart. Still, its
    underlying mathematical model is quite simple because it assumes that every feature
    is "linearly" connected with the target variable. Here you go: you have just met
    the multivariate linear regression model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*y* 仍然是我们试图预测的（单一的）目标变量，各种 *x*[i] 值代表对应我们所拥有的特征的（多个）自变量，![](img/B17125_05_011.png)
    值是定义模型“形状”的参数。由于这次我们有多个自变量（因此我们称之为**多变量模型**），我们无法再通过简单的二维图表中的直线进行“可视化”。尽管如此，它的基础数学模型仍然相当简单，因为它假设每个特征与目标变量之间是“线性”关联的。好了，你刚刚遇到了多变量线性回归模型。
- en: 'If we apply this model to the prediction of the rental prices, our target variable
    is represented by the column *Rent* while the features (independent variables)
    are all the other columns, like *Rooms*, *Surface*, and so on. The multivariate
    linear regression model will look like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个模型应用于租金预测，我们的目标变量是*Rent*列，而特征（自变量）则是所有其他列，如*Rooms*、*Surface*等。多变量线性回归模型将会是：
- en: '![](img/B17125_05_012.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_012.png)'
- en: and the aim of the learner algorithm implementing this model will be to find
    the "best" values of ![](img/B17125_05_013.png), ![](img/B17125_05_014.png), ![](img/B17125_05_015.png),
    and so on that minimize the error produced on the training set.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 并且实现此模型的学习算法的目标是找到“最佳”值，如![](img/B17125_05_013.png)、![](img/B17125_05_014.png)、![](img/B17125_05_015.png)等，这些值能够最小化在训练集上产生的误差。
- en: 'There are ways to find analytically (meaning through a set of given formulas,
    nothing overly complex) the set of parameters ![](img/B17125_05_016.png) that
    minimize the error of a linear regression model. The simplest one is called **Ordinary
    Least Squares (OLS)**: it minimizes the sum of the squared errors of a linear
    regression. Do you remember the **Root Mean Squared Error** (**RMSE**) metric
    introduced in *Chapter 4*? By using the ordinary least squares procedure, we are
    going to minimize the RMSE, which is exactly what we need to do here.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一组给定的公式（并不复杂），我们可以分析性地找到能够最小化线性回归模型误差的参数集![](img/B17125_05_016.png)。最简单的方法称为**普通最小二乘法（OLS）**：它最小化线性回归的平方误差和。你还记得在*第4章*中介绍的**均方根误差**（**RMSE**）指标吗？通过使用普通最小二乘法程序，我们将最小化RMSE，这正是我们在这里需要做的。
- en: 'The model above expects every independent variable to be a number. So, how
    do we deal with the nominal features we have in our dataset like *Floor_type*?
    We can solve this apparent limitation with a common trick used in machine learning:
    creating the so-called **dummy variables**. The idea is very simple: we transform
    every nominal variable into multiple numerical variables. Let''s take the example
    of *Floor_type*: this is a categorical variable whose value can be either `Upper`,
    `Mezzanine`, or `Ground floor`. In this case we would replace this categorical
    variable by creating three numeric dummy variables: *Floor_type=Upper*, *Floor_type=Mezzanine*,
    and *Floor_type=Ground*. The dummy variables will take as values either `1` or
    `0`, depending on the category: for a given row, only one dummy variable will
    take `1` and all others will take `0`. For example, if a row refers to an `Upper`
    floor property, the dummy variable *Floor_type=Upper* will be `1` and the other
    two will be `0`.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的模型期望每个自变量都是一个数值。那么，我们如何处理数据集中像*Floor_type*这样的名义特征呢？我们可以通过机器学习中的一个常用技巧来解决这个显而易见的限制：创建所谓的**虚拟变量**。这个想法非常简单：我们将每个名义变量转换为多个数值变量。以*Floor_type*为例：这是一个类别变量，其值可以是`Upper`、`Mezzanine`或`Ground
    floor`。在这种情况下，我们将通过创建三个数值虚拟变量来替换这个类别变量：*Floor_type=Upper*、*Floor_type=Mezzanine*和*Floor_type=Ground*。虚拟变量的值可以是`1`或`0`，取决于类别：对于某一行数据，只有一个虚拟变量的值为`1`，其余的都为`0`。例如，如果某一行数据指的是`Upper`楼层的房产，那么虚拟变量*Floor_type=Upper*的值将为`1`，其他两个将为`0`。
- en: Thanks to this trick, we can apply a linear regression model on any categorical
    variables as well; we just need to "convert" them into multiple additional dummy
    variables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于这个技巧，我们可以将线性回归模型应用于任何类别变量；我们只需要将它们“转换”成多个额外的虚拟变量。
- en: We have all we need to give the linear regression model a try by introducing
    the KNIME node that implements its learning algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好通过引入实现其学习算法的KNIME节点，来尝试线性回归模型。
- en: '![A picture containing logo'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![一张包含logo的图片'
- en: Description automatically generated](img/image024.png) *Linear Regression Learner*
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image024.png) *线性回归学习器*
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) trains a
    multivariate linear regression model for predicting a numeric quantity. For its
    configuration (see *Figure 5.8*) you will have to specify the numeric column to
    be predicted by picking it in the **Target** drop-down menu at the top:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 线性/多项式回归**）训练一个多变量线性回归模型，用于预测一个数值量。对于其配置（见*图5.8*），你需要在顶部的**目标**下拉菜单中选择要预测的数值列：
- en: '![Graphical user interface, application'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序'
- en: Description automatically generated](img/B17125_05_08.png)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_08.png)
- en: 'Figure 5.8: Configuration dialog of Linear Regression Learner: choose what
    to predict and the features to use'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：线性回归学习器的配置对话框：选择要预测的内容和要使用的特征
- en: 'Then, in the central box, you can select which columns should be used as features:
    only the columns that appear on the green box on the right will be considered
    as independent variables in the model. The nominal columns, such as strings, will
    be automatically converted by the node into dummy variables.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在中央框中，您可以选择哪些列应作为特征使用：只有右侧绿色框中出现的列才会被视为模型中的自变量。名义列，如字符串，将自动由节点转换为虚拟变量。
- en: 'If a nominal column (like *Type*) admits *N* unique values (like `A`, `B`,
    and `C`), this node will actually create not *N*, but *N-1* dummy variables (*Type=A*
    and *Type=B*). In fact, one of the nominal values can be covered by the combination
    of all zeros: in our case, if *Type* is `C`, both *Type=A* and *Type=B* will be
    zero, implying that the only possible value for that row is `C`. In this way,
    we make the model simpler and avoid the so-called dummy variable trap, which might
    make our model parameters impossible to calculate. The node takes care of this
    automatically, so you don''t have to worry about it: just keep this in mind when
    reading the model parameters related to dummy variables.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个名义列（如*类型*）具有*N*个唯一值（如`A`、`B`和`C`），那么该节点实际上会创建 *N-1* 个虚拟变量（*Type=A* 和 *Type=B*）。实际上，名义值中的一个可以通过所有零的组合来表示：在我们的例子中，如果*类型*是`C`，那么*Type=A*和*Type=B*都会是零，意味着该行唯一可能的值是`C`。这样，我们使模型变得更简单，并避免了所谓的虚拟变量陷阱，这可能导致我们的模型参数无法计算。该节点会自动处理这一点，因此您不必担心：只需要在阅读与虚拟变量相关的模型参数时记住这一点。
- en: By clicking on the **Predefined Offset Value** tick box, you can "force" the
    offset value of the linear regression model (we also called it ![](img/B17125_05_017.png)
    or intercept earlier) to a certain value or remove it, by setting it to zero.
    This reduces the "flexibility" of the model to minimize the error so it will reduce
    its accuracy. However, this trick might be helpful when you are trying to reduce
    the complexity of the model and improve its explain ability, as we have one less
    parameter to interpret. By default, this node will fail if there are some missing
    values in the input data. To manage this, you can either manage them earlier in
    the workflow, using the **Missing Value** node, or select the **Ignore rows with
    missing value** option at the bottom-left corner of the configuration dialog.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击**预定义偏移值**复选框，您可以“强制”将线性回归模型的偏移值（我们之前也叫它 ![](img/B17125_05_017.png) 或截距）设置为某个值，或者通过将其设置为零来移除它。这会减少模型最小化误差的“灵活性”，因此会降低其准确性。然而，当您试图降低模型复杂度并提高模型的可解释性时，这个技巧可能会有所帮助，因为我们少了一个参数需要解释。默认情况下，如果输入数据中存在缺失值，此节点将无法执行。为了处理这个问题，您可以提前在工作流中管理它们，使用**缺失值**节点，或者选择配置对话框左下角的**忽略缺失值的行**选项。
- en: 'Once executed, the node will return at its first output port the regression
    model, which can then be used by a predictor node for making predictions. The
    second output is a table (*Figure 5.9*) that contains a summary view of the regression
    model parameters, where for each variable (including the dummy ones) you can find:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，节点将在其第一个输出端口返回回归模型，然后可以通过预测节点用于预测。第二个输出是一个表格（*图 5.9*），其中包含回归模型参数的摘要视图，对于每个变量（包括虚拟变量），您可以找到：
- en: '**Coeff.**: this is the **parameter** (also called coefficient) of the variable.
    This is the ![](img/B17125_05_018.png) parameter we have seen in the regression
    model formula.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Coeff.**：这是变量的**参数**（也叫系数）。这是我们在回归模型公式中看到的 ![](img/B17125_05_018.png) 参数。'
- en: '**Std. Err.**: this is **the standard deviation of the error** expected for
    this parameter. If you compare it with the value of the parameter, you get a rough
    idea of how "precise" the estimation of that parameter can be. You can use it
    also to get a rough confidence interval for the given parameter as we did in *Chapter
    4*, *What is Machine Learning?*, when talking about RMSE. In the case of the car
    price regression, if the parameter for the variable *Age* is -1.7 and the standard
    error is 0.1, you can say that 95% of the time, the price of a car declines by
    $M 1.7 ± 0.2 (2 times the standard error) every year.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准误差**：这是该参数的**误差标准差**。如果将其与参数值进行比较，你可以大致了解该参数的估计值有多“精确”。你还可以用它来大致获取该参数的置信区间，正如我们在*第4章*《什么是机器学习？》中讨论RMSE时所做的那样。在汽车价格回归的例子中，如果*年龄*变量的参数是-1.7，标准误差是0.1，你可以说，95%的情况下，汽车的价格每年下降$M
    1.7 ± 0.2（2倍标准误差）。'
- en: '**t-value** and **P>|t|**: these are two summary statistics (**t-value** and
    **p-value**) generated by the application of the Student test, which clarifies
    how significant a variable is for the model. The smaller the p-value, the more
    confident you can be in rejecting the possibility that that parameter looks significant
    just "by chance" (it''s called **null hypothesis**). As a general rule of thumb,
    when the p-value (the last column in this table) is above 0.05, you should remove
    that variable from the model, as it is likely insignificant:![Table'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t值**和**P>|t|**：这两个统计值（**t值**和**p值**）是通过应用学生t检验生成的，用于说明某个变量对于模型的重要性。p值越小，你越能有信心拒绝该参数仅仅是“偶然”显著的可能性（这被称为**零假设**）。作为一个通用规则，当p值（该表格的最后一列）大于0.05时，你应该从模型中删除该变量，因为它可能是不显著的：![表格'
- en: Description automatically generated](img/B17125_05_09.png)
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/B17125_05_09.png)'
- en: 'Figure 5.9: The summary output of the Linear Regression Learner node: find
    out what the parameters of the regression are and if they turn out significant
    or not'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.9：线性回归学习节点的摘要输出：找出回归的参数，并确定它们是否显著
- en: 'If you right-click on the node after it is executed, you can open an additional
    graphical view (select **View: Linear Regression Scatterplot View**) where you
    can visually compare the individual features against the target to look for steep
    slopes and other patterns.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在节点执行后右键点击，可以打开一个额外的图形视图（选择**视图：线性回归散点图视图**），在这里你可以直观地比较各个特征与目标之间的关系，寻找陡峭的斜率和其他模式。
- en: 'Let''s now put this node to work with our Rome properties and see what it''s
    got:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用我们的罗马房产数据来实际操作这个节点，看看它能带来什么：
- en: Implement the **Linear Regression Learner** node and connect it with the upper
    output of the **Partitioning** node, which is the training set (a 70% random sample
    of the historical database of rents). In the configuration window, double-check
    that *Rent* is set as the **Target** variable on top. Feature-wise, at this point,
    we can keep all of them to see if they are significant or not. However, we can
    already remove one, *House_ID*, as we already know we don't want it to be used.
    We don't want to make use of the unique identifier of the property to infer the
    rental price. That number has been assigned artificially when the property was
    added to the database, and it is not connected with features of the property itself,
    so we don't want to consider it in a predictive model. Run the model and open
    the second output port to obtain the summary view of the model parameters.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**线性回归学习者**节点，并将其与**分区**节点的上输出端口连接，该输出端口是训练集（来自历史租金数据库的70%随机样本）。在配置窗口中，再次检查*租金*是否被设置为顶部的**目标**变量。就特征而言，此时我们可以保留所有特征，看看哪些是显著的。然而，我们可以删除其中一个，*房屋ID*，因为我们已经知道不希望使用它。我们不希望用房产的唯一标识符来推断租金。该编号是在房产加入数据库时人为分配的，并且与房产本身的特征无关，因此我们不希望在预测模型中考虑它。运行模型并打开第二个输出端口以获取模型参数的摘要视图。
- en: 'This summary view will look similar to what is displayed in *Figure 5.9*, although
    the numbers could differ given that the random partitioning might have generated
    in your case different partitions: welcome to the world of probabilistic models!
    However, we can already notice that some parameters display a p-value (the last
    column of the table, **P>|t|**) higher than 0.05\. This means we can come back
    to this step later and do some cleaning and improve the performance of the model.
    For now, let''s proceed further so that we can make some predictions and score
    the model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个总结视图将类似于*图5.9*中显示的内容，尽管数字可能不同，因为随机划分可能在你的案例中生成了不同的划分：欢迎来到概率模型的世界！然而，我们已经注意到，一些参数的p值（表格的最后一列，**P>|t|**）高于0.05。这意味着我们可以稍后返回此步骤，进行清理并提高模型的性能。目前，让我们继续进行预测并评估模型。
- en: '![Icon'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图标'
- en: Description automatically generated](img/image028.png) *Regression Predictor*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/image028.png) *回归预测器*
- en: 'This node (**Analytics > Mining > Linear/Polynomial Regression**) applies a
    regression model (given as an input in the first blue port on the left) to a dataset
    (second port) and returns the result of the prediction for each input row. The
    node does not require any configuration and can be used in conjunction with either
    the **Linear Regression Learner** node, introduced above, or the **Polynomial
    Regression** **Learner** node: you can check this one out by yourself if you want
    to build linear regressions on different polynomial degrees as we did in *Chapter
    4*, *What is Machine Learning?* (have a look at *Figure 4.9*).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 线性/多项式回归**）应用回归模型（作为输入给出的第一个蓝色端口）到数据集（第二个端口），并返回每个输入行的预测结果。该节点不需要任何配置，可以与**线性回归学习器**节点或**多项式回归**
    **学习器**节点一起使用：如果你想像我们在*第4章*《什么是机器学习？》中那样在不同多项式度数下构建线性回归，可以自行查看该节点（参见*图4.9*）。
- en: 'Let''s add the **Regression Predictor** node to the workflow and make the connections:
    link the blue square output of the **Linear Regression Learner** to the upper
    input port of the predictor and connect the bottom output port of the **Partitioning**
    (the test set) to the second input port. No configuration is needed so you can
    execute the node and look at the output, which is similar to what you find in
    *Figure 5.10*:![Table'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将**回归预测器**节点添加到工作流中并建立连接：将**线性回归学习器**的蓝色方形输出连接到预测器的上输入端口，并将**划分**（测试集）的底部输出端口连接到第二个输入端口。无需配置，因此你可以执行节点并查看输出，输出结果与*图5.10*中的类似：![表格
- en: Description automatically generated](img/B17125_05_10.png)
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_10.png)
- en: 'Figure 5.10: Output of the Regression Predictor node: we finally have a prediction
    of the rental price.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.10：回归预测器节点的输出：我们最终得到了租金价格的预测。
- en: 'You can look with pride at the last column on the right, called *Prediction
    (Rent)*: for each row in the test set (which has not been "seen" by the learner
    node) the node has generated a prediction of the rent. This prediction was obtained
    by just "applying" the parameters of the regression model to the values of the
    rows in the test set. Let''s see how this works with an example: consider the
    parameters in *Figure 5.9*. In this case the intercept (last row) is 569.9, the
    parameter of *Rooms* is around 25.8, the one for *Surface* is 9.6, the parameter
    of the dummy variable associated with the `Collatino` neighborhood (*Neighborhood=Collatino*)
    is -561.9, and so on. When the predictor had to come up with a prediction for
    the first row in the test set (see the first line in *Figure 5.10*), it had to
    just apply the formula of the regression model, with the parameters found by the
    learner, to this property (with 3 rooms, 80 square meters, based in `Collatino`,
    and so on). Hence, the resulting calculation for the **Regression Predictor**
    node is:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自豪地查看右侧的最后一列，称为*预测（租金）*：对于测试集中的每一行（学习器节点未“见过”的行），该节点生成了租金的预测。这个预测是通过将回归模型的参数直接“应用”到测试集中的每一行的值上获得的。让我们通过一个例子来看这个是如何工作的：考虑*图5.9*中的参数。在这种情况下，截距（最后一行）为569.9，*房间数*的参数约为25.8，*面积*的参数为9.6，与`Collatino`社区相关的虚拟变量（*Neighborhood=Collatino*）的参数为-561.9，依此类推。当预测器需要为测试集中的第一行（见*图5.10*中的第一行）生成预测时，它只需将学习器找到的回归模型公式应用到该属性（具有3个房间，80平方米，位于`Collatino`等）中。因此，**回归预测器**节点的结果计算如下：
- en: '![](img/B17125_05_019.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_019.png)'
- en: 'In this specific case, if you add all the other features that were not reported
    in the preceding formula, we come up with a final prediction of €896.4, making
    around €50 of error versus the actual rental price, which we know is €950: not
    bad for our first prediction! To have a complete view of the performance of the
    current model, we would need to check the difference between predicted and real
    rents for all rows in the test set, using the **Numeric Scorer** node.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，如果加上所有未在前述公式中报告的其他特征，我们得出的最终预测为€896.4，与实际租金价格€950相比，误差约为€50：对我们的第一个预测来说，这并不差！要完整了解当前模型的性能，我们需要检查测试集中所有行的预测和实际租金之间的差异，使用**数值评分器**节点。
- en: '![A picture containing text'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![包含文本的图片'
- en: Description automatically generated](img/image032.png) *Numeric Scorer*
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image032.png) *数值评分器*
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    metrics of a regression by comparing two numeric columns. Its only required configuration
    (*Figure 5.11*) is the selection of the two columns to be compared: you can select
    the target column of the regression, containing the actual values, in the **Reference
    column** dropdown, and the predictions in the next one, labeled as **Predicted
    column**. If you want to output the performance scores as variables as well (this
    is useful when doing hyperparameter optimization), you need to tick the **Output
    scores as flow variables** box at the bottom. The node outputs the most popular
    scoring metrics of a regression, including the **Coefficient of Determination**,
    **R**², and the **RMSE**:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此节点（**分析 > 挖掘 > 评分**）通过比较两个数值列来计算回归的摘要性能指标。它唯一需要的配置（*图 5.11*）是选择要比较的两列：您可以在**参考列**下拉菜单中选择回归的目标列，其中包含实际值，然后在下一个标记为**预测列**的列中选择预测值。如果您还想将性能评分作为流变量输出（在进行超参数优化时很有用），则需要勾选底部的**将分数作为流变量输出**框。该节点输出回归的最流行评分指标，包括**确定系数**、**R**²
    和 **RMSE**：
- en: '![Graphical user interface, text, application, email'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，文本，应用程序，电子邮件'
- en: Description automatically generated](img/B17125_05_11.png)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_11.png)
- en: 'Figure 5.11: Configuration dialog of the Numeric Scorer node: select the columns
    to compare.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：数值评分器节点的配置对话框：选择要比较的列。
- en: 'Implement the **Numeric Scorer** node (watch out: don''t get confused with
    the **Scorer** node, which is used for classifications) and connect the output
    of the **Regression Predictor** with its input port. For its configuration, just
    double-check that you have *Rent* and *Prediction (Rent)* in the drop-down menus
    at the top and run the node. Its output (*Figure 5.12*) is very encouraging (of
    course, you can get slightly different results from what you find in these figures
    and that''s normal):![Table'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现**数值评分器**节点（注意：不要与用于分类的**评分器**节点混淆），并将**回归预测器**的输出连接到其输入端口。在其配置中，请仔细检查顶部下拉菜单中是否包含*租金*和*预测（租金）*，然后运行节点。其输出（*图
    5.12*）非常令人鼓舞（当然，您可能会得到与这些图中稍有不同的结果，这是正常的）：![表
- en: Description automatically generated](img/B17125_05_12.png)
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17125_05_12.png)
- en: 'Figure 5.12: Performance metrics as returned by the Numeric Scorer node:not
    bad for your first regression'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.12：作为数值评分器节点返回的性能指标：作为您的第一个回归不错
- en: 'We obtained an R² of 0.92, which means that our current model accounts for
    around 92% of the full variability of rental prices in Rome. Considering the limited
    sample and the few features available, this looks quite good already. Also, the
    RMSE is €110, which means that 68% of the time (one standard deviation) we will
    make a prediction error that is, in absolute terms, below €110, and 95% of the
    time our error will be below €220 (two times the RMSE). The last performance metric,
    **Mean Absolute Percentage Error** (**MAPE**) tells us that, on average, our predicted
    rent will differ from the actual rent by around 10%: again, not bad at all.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个R²值为0.92，这意味着我们当前的模型解释了罗马租金价格的大约92%的完全变异性。考虑到样本有限和可用特征较少，这看起来已经相当不错了。此外，RMSE为€110，这意味着我们将在68%的时间内（一个标准偏差）做出预测误差，绝对值低于€110，而在95%的时间内，我们的误差将低于€220（RMSE的两倍）。最后一个性能指标，**平均绝对百分比误差**（**MAPE**），告诉我们，我们的预测租金与实际租金的平均差异约为10%：再次非常不错。
- en: 'Still, we strive for the best and question ourselves if we can do anything
    to improve the model. The simplest thing to do will be to consider whether we
    can improve the selection of features. Let''s go back and have a look at the parameters
    obtained by the regression (*Figure 5.9*) and if we can remove some unneeded (or
    damaging) features. When we remove excess features from a model, we obtain at
    least two advantages: first, we make the model simpler and more explanatory to
    other human beings, as we have fewer parameters to explain. Secondly, we reduce
    the possibility for the model to overfit on the training set and, so, we increase
    its general robustness.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们依然追求最优，并不断质疑自己是否能做些什么来改进模型。最简单的方法是考虑是否可以改善特征的选择。让我们回顾一下回归分析得到的参数（*图5.9*），看看是否能移除一些不必要（或有害）的特征。当我们从模型中移除多余的特征时，至少会带来两个好处：首先，我们让模型更简洁，便于其他人理解，因为我们需要解释的参数变少了。其次，我们减少了模型在训练集上的过拟合可能性，从而增强了模型的泛化能力。
- en: 'Another reason for removing features is to avoid the risk of **multicollinearity**,
    which happens when features are correlated with each other. Correlated features
    are redundant: they can produce degradation of the predictive performance of your
    model and should be removed. The **Linear Correlation** node can help you calculate
    the correlation across all pairs of numeric columns in a table. As an alternative,
    you can use the **Variance Inflation Filter** (**VIF**) component, available in
    the KNIME Hub: as a rule of thumb, all variables showing a VIF higher than 5 should
    be removed.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 移除特征的另一个原因是避免**多重共线性**的风险，这种情况发生在特征之间相互关联时。相关的特征是冗余的：它们会降低模型的预测性能，应当被移除。**线性相关性**节点可以帮助你计算表格中所有数字列对之间的相关性。作为替代方案，你可以使用KNIME
    Hub中的**方差膨胀因子滤波器**（**VIF**）组件：根据经验法则，所有VIF值大于5的变量应该被移除。
- en: 'Let''s have a look at the p-values (last column of the table) and see if we
    can unveil some opportunities. Remember, the higher they are, the less meaningful
    their associated features proved to be. For sure we notice that the feature *Elevator*
    should be removed: its p-value is way above the thumb-rule threshold of 0.05 so
    we can go ahead and remove it. Also, the variable *Property_type* shall be removed:
    the p-values of their dummy variables are high, with the exception of *Property_type=Penthouse*
    (indicating that `Penthouse` is the only type that seems to be significant in
    affecting the value of the rent). Still, considering how few penthouses we have
    in the dataset, it''s worth removing this feature and further simplifying the
    model. Let''s give this simplification a try and see what happens:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看p值（表格的最后一列），看看是否能揭示一些机会。记住，p值越高，相关特征的意义就越小。我们可以明显看到特征*Elevator*应该被移除：它的p值远高于0.05的规则阈值，因此我们可以直接将其移除。同时，变量*Property_type*也应该被移除：它的虚拟变量的p值较高，唯一例外是*Property_type=Penthouse*（表明`Penthouse`是唯一一个似乎对租金有显著影响的类型）。不过，考虑到数据集中顶层公寓的数量非常少，还是值得移除这个特征，进一步简化模型。让我们尝试一下这种简化，看看会发生什么：
- en: Open the configuration dialog of the **Linear Regression Learner** node and
    move *Elevator* and *Property_type* to the left box of the column selector, so
    as to remove them as features of the model.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**线性回归学习器**节点的配置对话框，将*Elevator*和*Property_type*移到列选择器的左侧框中，以便将它们移除作为模型的特征。
- en: 'Now let''s run the full model and see if something changed. To do so, it will
    be enough to execute the **Numeric Scorer** node: all previous nodes will be forced
    to run as well.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来运行完整模型，看看是否有变化。为此，只需执行**Numeric Scorer**节点：之前的所有节点将被强制一起运行。
- en: 'By removing these two features (see the updated results in *Figure 5.13*),
    we managed to keep the same performance levels, proving that they were unneeded.
    Actually, the performance has marginally increased (notice the lower RMSE), probably
    showing that we were slightly overfitting because of these uninformative variables.
    Additionally, we simplified the model, making it simpler to explain. Now we can
    predict the rental price of a property in Rome by knowing only the neighborhood,
    the number of rooms, the surface, and its floor (number and type):'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过移除这两个特征（见*图5.13*中的更新结果），我们成功保持了相同的性能水平，证明它们是不必要的。实际上，性能略微提高了（注意到RMSE更低），这可能表明由于这些无信息的变量，我们稍微出现了过拟合现象。此外，我们简化了模型，使得模型更容易解释。现在，只需要知道一个房产在罗马的所在区域、房间数、面积和楼层（编号及类型），就能预测它的租金。
- en: '![Graphical user interface, table'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面，表格'
- en: Description automatically generated](img/B17125_05_13.png)
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_13.png)
- en: 'Figure 5.13: Updated parameters and performance scores after the removal of
    two features: every little helps'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.13：移除两个特征后的更新参数和性能评分：每一点改进都很有帮助
- en: These last two steps have shown us the value of selecting features wisely. As
    anticipated in *Chapter 4*, *What is Machine Learning?*, feature selection is
    an important practice in machine learning, indeed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后的两步展示了我们明智选择特征的价值。正如*第4章*《什么是机器学习？》中所预测的那样，特征选择在机器学习中是一个重要的实践，的确如此。
- en: In this case, we applied feature selection "by hand," checking the parameters
    manually and selecting the least meaningful ones. There are more systemic and
    semi-automated techniques to find out the best subset of features to use in a
    machine learning model. If you are curious, check the KNIME nodes for **Feature
    Selection** loops and have a look at the sample workflow available on the KNIME
    Hub called **Performing a Forward Feature Selection**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们是“手动”进行特征选择的，通过手动检查参数并选择最不重要的那些。实际上，还有一些系统化和半自动化的技术来找出最适合用于机器学习模型的特征子集。如果你感兴趣，可以查看KNIME中的**特征选择**节点，并查看KNIME
    Hub上名为**执行前向特征选择**的示例工作流。
- en: 'Before concluding, we need to do one last thing: it''s time to apply our model
    to the 10 incoming properties for which the rental price is not available yet.
    This will be a way to illustrate our findings to the owner of the company. It
    will also be an opportunity for us to understand how predictive models are used
    in real life after they are built. In fact, once models are constructed (and validated
    against overfitting, as we did through the partitioning in training and test sets,
    and so on) they are **operationalized** in a way that they can be applied to future
    samples (in this case, the 10 new properties) whenever a prediction is needed.
    Let''s see this in action with our properties:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我们需要做最后一件事：是时候将我们的模型应用于10个尚未提供租金价格的待租房产了。这将是向公司所有者展示我们发现的一个方式，也为我们提供了一个机会来理解在构建完模型后，如何在实际生活中使用预测模型。事实上，一旦模型构建完成（并且经过过拟合验证，如我们通过在训练集和测试集上进行划分所做的那样），它们会被**投入使用**，以便在需要进行预测时，可以将其应用于未来的样本（在本例中，即10个新房产）。让我们通过这些房产来看一下实际操作：
- en: Load the Excel file with the new properties *(*`RomeHousing-NewProperties.xlsx`*)
    by dragging and dropping it into your workflow or implementing an* **Excel Reader**
    *node*. Once executed, you will find a short table that has exactly the same columns
    as the historical database, but—of course—lacks the *Rent* value.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拖拽或实现一个**Excel读取器**节点，加载包含新房产的Excel文件*（`RomeHousing-NewProperties.xlsx`）*。执行后，你会看到一个简短的表格，其列与历史数据库完全相同，但——当然——缺少*租金*值。
- en: Implement a new **Regression Predictor** node (or copy/paste the existing one)
    and connect it as displayed in *Figure 5.14*. You should link the output of the
    **Linear Regression Learner** (yes—we are going to reuse the model we learned
    earlier) to the first input of the predictor. Then connect the **Excel Reader**
    output (the 10 new properties) to the second input of the predictor. You can now
    execute the node and have a look at the output:![](img/B17125_05_14.png)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个新的**回归预测器**节点（或者复制/粘贴现有的节点），并按照*图 5.14*中的显示方式连接它。你应该将**线性回归学习器**的输出（是的，我们将重用之前学到的模型）连接到预测器的第一个输入。然后，将**Excel读取器**的输出（10个新房产）连接到预测器的第二个输入。现在，你可以执行该节点并查看输出：![](img/B17125_05_14.png)
- en: 'Figure 5.14: Full workflow for the Rome rent prediction'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：罗马租金预测的完整工作流
- en: 'At this point, you have all you need to go back to the owner of the real estate
    with your output table (which will have a similar format to what you find in *Figure
    5.15*) and wait impatiently for her reaction, which turns out to be very positive!
    She loves it, as she finds that the estimates make, at least at a first glance,
    a lot of sense:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经具备了所有必要的条件，可以将输出表格（格式与*图 5.15*中类似）带回给房地产所有者，并焦急地等待她的反应，结果反应非常积极！她很喜欢，因为她发现这些估算结果至少从表面看起来非常有道理：
- en: '![Table'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![表格'
- en: Description automatically generated](img/B17125_05_15.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_15.png)
- en: 'Figure 5.15: The predicted rental prices on the new properties: do you fancy
    a 145 square meter flat near Piazza Navona at this price?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：新房产的预测租金：你喜欢在这个价格下租一套位于纳沃纳广场附近的145平方米公寓吗？
- en: 'The understandable initial stress turns quickly to a broad sense of enthusiasm.
    The model you created responds to the initial business objectives. In fact:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初步的紧张感很快转变为一种广泛的热情。你创建的模型回应了最初的商业目标。事实上：
- en: The interpretation of the parameters of the model tells us something quite useful
    about the price formation mechanisms. For instance, you have found that the presence
    of the elevators and the type of flat doesn't count as much as the surface, the
    number of rooms, the floors, and, very importantly, the neighborhood to which
    the property belongs. By looking at the parameters of the neighborhood dummy variables
    (*Figure 5.13*), you find out what additional value each neighborhood brings (of
    course to be added to the rest of the components of your regression). For instance,
    Piazza Navona is by far the most expensive area while Castelli Romani seems to
    offer (at parity of characteristics) the most accessible rent.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数的解释告诉我们一些关于价格形成机制的非常有用的信息。例如，你发现电梯的存在和公寓的类型并不像面积、房间数量、楼层以及非常重要的，物业所属的邻里那样重要。通过查看邻里虚拟变量的参数（*图
    5.13*），你可以发现每个邻里所带来的额外价值（当然，这些价值需要加到回归模型的其他组成部分上）。例如，纳沃纳广场是迄今为止最贵的地区，而卡斯特尔利·罗马尼则似乎提供（在相同特征下）最具可及性的租金。
- en: On top of this, you now have a simple approach to quickly generate a recommendation
    of what fair rent looks like, which could be the basis for the discussion with
    the prospective landlord when fixing the rental price. By having a data-based
    number to start from, the agents can aim at a smoother negotiation session, which
    will more likely end up with a quicker and more profitable matching of demand
    and offer in the housing market.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，你现在有了一个简单的方法，可以快速生成一个公平租金的推荐，这可能成为与潜在房东讨论租金时的基础。通过拥有一个基于数据的起始数字，代理商可以更顺利地进行谈判，最终更有可能实现需求和供应在住房市场上更快、更有利的匹配。
- en: 'Congratulations on completing your first regression model! It''s now time to
    move on and challenge ourselves with a different undertaking: anticipating consumers''
    behavior.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了第一个回归模型！现在是时候向前迈进，挑战自己去做一个不同的任务：预测消费者行为。
- en: Anticipating preferences with classification
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类预测偏好
- en: 'In this tutorial, you will step into the role of a marketing analyst working
    for a mid-sized national consumer bank, offering services such as accounts, personal
    loans, and mortgages to around 300,000 customers in the country. The bank is currently
    trying to launch a new type of low-cost savings account, providing essential services
    and a pre-paid card that can be fully managed online. The product manager of this
    new account is not very pleased with how things are going and invites you to join
    a review meeting. You can see he is tense as he presents the outcome of a pilot
    telemarketing campaign run to support the launch. As part of this pilot, 10,000
    people were randomly selected among the full bank customer base and were phoned
    by an outbound call center. The outcome was apparently not so bad: 1,870 of the
    contacted customers (19% of the total) signed up for a new account. However, the
    calculation of the **Return On Investment** (**ROI**) pulled the entire audience
    back to the unsettling reality. The average cost of attempting to contact a customer
    through a call center is $15 per person while the incremental revenue resulting
    from a confirmed sale is estimated to be, on average, $60\. The math is simple:
    the pilot telemarketing campaign cost $150,000 and generated revenues amounting
    only to $112,200, implying a net loss of $37,800\. Now it is clear why the product
    manager looked disappointed: repeating the same campaign on more customers would
    be financially devastating.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将扮演一位中型全国性消费者银行的市场分析师，负责为大约30万名客户提供账户、个人贷款和抵押贷款等服务。该银行目前正在尝试推出一种新的低成本储蓄账户，提供基本服务并配备可以完全在线管理的预付卡。这个新账户的产品经理对目前的情况并不满意，并邀请你参加一次评审会议。从他紧张的表现来看，显然他对试点电话营销活动的结果并不满意。作为这个试点的一部分，从整个银行客户群中随机选取了1万名客户，并通过外呼中心进行了电话联系。结果似乎还不错：1,870名被联系的客户（占总数的19%）签约开设了新账户。然而，**投资回报率**（**ROI**）的计算让全场观众回到了令人不安的现实。通过呼叫中心联系客户的平均成本为每人15美元，而确认销售所带来的增量收入平均为60美元。算式很简单：试点电话营销活动花费了15万美元，产生的收入仅为112,200美元，意味着净损失37,800美元。现在大家都明白为什么产品经理看起来失望：如果再对更多客户重复这个活动，结果将是财务上的灾难。
- en: You timidly raise your hand and ask whether the outcomes of the pilot calls
    could be used to rethink the campaign target and improve the ROI of the marketing
    efforts. You explain that some machine learning algorithms might be able to predict
    whether a customer is willing or not to buy a product by learning from previous
    examples. As it normally happens in these cases, you instantly earn the opportunity
    to try what you suggested, and your manager asks you to put together a proposal
    on an ML way to support the launch of the new savings account.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你腼腆地举手问是否可以使用试点电话营销的结果来重新思考活动目标，并提高市场营销工作的投资回报率（ROI）。你解释说，一些机器学习算法可能能够通过学习之前的案例预测客户是否愿意购买产品。像往常一样，你立即获得了尝试你所建议的机会，经理要求你提出一个支持新储蓄账户推出的机器学习方案。
- en: 'You have mixed feelings about what just happened: on one hand, you are wondering
    whether you were a bit too quick in sharing the idea. On the other hand, you are
    very excited as you get to try leveraging algorithms to impact the business on
    such an important case. You are impatient to start and ask for all the available
    information related to the customers that were involved in the pilot. The file
    you receive (`BankTelemarketing.csv`) contains the following columns:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你对刚才发生的事情感到复杂的心情：一方面，你在想自己是不是分享想法有点太急了；另一方面，你也很兴奋，因为你将有机会尝试利用算法来影响这样一个重要的商业案例。你迫不及待想要开始，并请求所有与参与试点的客户相关的信息。你收到的文件（`BankTelemarketing.csv`）包含以下几列：
- en: '*Age*: the age of the customer.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Age*：客户的年龄。'
- en: '*Job*: a string describing the job family of the customer, like `blue-collar`,
    `management`, `student`, `unemployed`, and `retired`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Job*：描述客户工作类别的字符串，例如 `蓝领`、`管理`、`学生`、`失业` 和 `退休`。'
- en: '*Marital*: the marital status, which could be `married`, `single`, `divorced`,
    or `unknown`.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Marital*：婚姻状况，可以是 `已婚`、`单身`、`离婚` 或 `未知`。'
- en: '*Education*: the highest education level reached to date by the customer, ranging
    from `illiterate` and `basic.4y` (4 years of basic education in total) to `university.degree`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Education*：客户目前达到的最高教育水平，范围从 `文盲` 和 `基础教育4年`（总共4年基础教育）到 `大学学位`。'
- en: '*Default*: this tells us whether we know that the customer has defaulted due
    to extended payment delinquency or not. Only a few customers end up being marked
    as defaulted (`yes`): most of them either show a good rating history (`no`) or
    do not have enough history to be assigned in a category (`unknown`).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*违约*：这告诉我们客户是否因为长期未支付款项而违约。只有少数客户最终会被标记为违约（`yes`）：大多数客户要么有良好的信用记录（`no`），要么没有足够的历史记录，无法归类为某个类别（`unknown`）。'
- en: '*Mortgage* and *Loan*: tells us whether the user has ever requested a housing
    mortgage or a personal loan, respectively.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*抵押贷款* 和 *贷款*：分别告诉我们用户是否曾申请过住房抵押贷款或个人贷款。'
- en: '*Contact*: indicates if the telephone number provided as a preferred contact
    method is a `landline` or a `mobile` `phone`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联系方式*：指示提供的首选联系电话是`固定电话`还是`手机`。'
- en: '*Outcome*: a string recording the result of the call center contact during
    the pilot campaign. It can be `yes` or `no`, depending on whether the customer
    opened the new savings account or decided to decline the offer.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结果*：记录试点活动中呼叫中心联系结果的字符串。它可以是`yes`或`no`，具体取决于客户是否开设了新储蓄账户，或者是否决定拒绝此项优惠。'
- en: 'Before you get cracking, you have a chat with the product manager to get clear
    on what would be the most valuable outputs for the business given the situation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，你需要和产品经理进行一次沟通，以明确在当前情况下，什么样的输出对业务最有价值：
- en: First of all, it would be very useful to understand and document what characteristics
    make a customer most likely to buy the new banking product. Given its novelty,
    it is not clear yet who will find its proposition particularly appealing. Having
    some more clues on this aspect can help to build more tailored campaigns, personalize
    their content, and—by doing so—transfer the learnings from the call center pilot
    to other types of media touchpoints.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，了解并记录哪些特征使得客户最有可能购买新银行产品是非常有用的。鉴于该产品的创新性，目前还不清楚哪些人群会特别看中它的提议。了解更多关于这一点的线索有助于构建更具针对性的营销活动，个性化其内容，并且——通过这样做——将呼叫中心试点的经验教训转移到其他媒体接触点。
- en: 'Given that the pilot covered only a relatively small subset of customers—around
    3% of the total—it would be useful to identify "who else" to call within the other
    97% to maximize the ROI of the marketing initiative. In fact, we can assume that
    the same features we found in our pilot dataset—such as age, job, marital status,
    and so on—are available for the entire customer database. If we were able to *score*
    the remaining customers in terms of their *propensity* to buy the product, we
    would be focusing our efforts on the most inclined ones and greatly improving
    the campaign''s effectiveness. In other words, we should create a **propensity
    model** that will score current (and future) customers to enable a better marketing
    targeting. We will use the propensity scores to "limit" the next marketing efforts
    to a selected subset of the total customer base where the percentage of people
    in the new product is higher than 19% (as it was in our pilot): by doing so, we
    would increase the ROI of our marketing efforts.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到试点只覆盖了一个相对较小的客户子集——约占总数的3%——识别“其他”97%的客户进行联系，以最大化营销投资回报率是非常有用的。事实上，我们可以假设，在我们的试点数据集中找到的相同特征——例如年龄、职业、婚姻状况等——也适用于整个客户数据库。如果我们能够根据客户购买产品的*倾向*为剩余客户进行`评分`，那么我们就能将精力集中在最有可能的客户身上，从而大大提高营销活动的效果。换句话说，我们应该建立一个**倾向模型**，对当前（以及未来）客户进行评分，以便更好地进行市场营销定位。我们将使用倾向得分来“限制”下一步营销活动的范围，选择出新产品的客户比例高于19%的客户子集（就像我们的试点一样）：通过这样做，我们可以提高营销工作的投资回报率。
- en: 'From a machine learning standpoint, you need to create a machine able to predict
    whether a consumer will buy or will not open a savings account before you make
    the call. This is still a clear case of supervised learning, since you aim at
    predicting something based on previous examples (the pilot calls). In contrast
    with the Rome real estate case, where we had to predict a number (the rental price)
    using *regression* algorithms, here we need to predict the value of the categorical
    column *Outcome*. We will then need to implement *classification* algorithms,
    such as decision trees and random forest, which we are going to meet shortly.
    We are clear on the business need, the available data, and the type of machine
    learning route we want to take: we have all we need to start getting serious about
    this challenge. After creating a new workflow in KNIME, we load the data into
    it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，在拨打电话之前，你需要创建一个能预测消费者是否会购买或是否会开设储蓄账户的机器。这仍然是一个典型的监督学习案例，因为你是基于先前的示例（试点电话）来预测某些内容。与罗马房地产案例不同，后者是用*回归*算法预测一个数字（租金价格），在这里，我们需要预测分类列*Outcome*的值。因此，我们需要实现*分类*算法，如决策树和随机森林，我们将很快遇到这些方法。我们已经清楚了业务需求、可用数据以及我们希望采取的机器学习路线：我们具备了开始认真对待这个挑战的所有条件。在
    KNIME 中创建一个新的工作流后，我们将数据加载进去：
- en: Drag and drop the file `BankTelemarketing.csv` onto the blank workflow. After
    the **CSV Reader** node dialog appears, we can quickly check that all is in order
    and close the window by clicking on **OK**. Once executed, the output of the node
    (*Figure 5.16*) confirms that our dataset is ready to go:![Table
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件`BankTelemarketing.csv`拖放到空白工作流中。弹出**CSV Reader**节点对话框后，我们可以快速检查一切正常，然后通过点击**OK**按钮关闭窗口。执行后，节点的输出（*图
    5.16*）确认我们的数据集已经准备好：![表格
- en: Description automatically generated](img/B17125_05_16.png)
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_16.png)
- en: 'Figure 5.16: The pilot campaign data: 10,000 customers through 8 features and
    for which we know the outcome of their call center contact'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.16：试点活动数据：10,000 个客户通过 8 个特征，且我们知道他们与呼叫中心联系的结果
- en: 'As usual, we implement the node **Statistics**, to explore the characteristics
    of our dataset. After confirming its default configuration, we check the **Top/bottom**
    tab of its main view (press *F10* or right-click and select **View: Statistics
    View** to open it). It seems that there are no missing values and that all seems
    to be in line with what we knew about the pilot campaign: the *Outcome* column
    shows 1,870 rows with `yes`, which is what the product manager managed in his
    presentation. We also notice that the *Default* column has only one row referring
    to a defaulted customer. This column might still be useful as it differentiates
    between customers who never defaulted and ones we don''t have any certainty about,
    so we decide to keep it and move on:![Text'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按惯例，我们实现了**统计**节点，来探索数据集的特征。确认其默认配置后，我们检查其主视图的**Top/bottom**选项卡（按*F10*或右键点击选择**视图：统计视图**以打开）。似乎没有缺失值，所有数据都符合我们对试点活动的预期：*Outcome*列显示
    1,870 行标注为`yes`，这正是产品经理在其演示中展示的内容。我们还注意到，*Default*列仅有一行表示一个违约客户。这个列仍然可能有用，因为它区分了从未违约的客户和我们不确定的客户，所以我们决定保留它并继续：![文本
- en: Description automatically generated](img/B17125_05_17.png)
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_17.png)
- en: 'Figure 5.17: The Top/bottom output of the Statistics node: only one person
    in this sample defaulted—good for everyone!'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.17：统计节点的Top/bottom输出：该样本中仅有一个人违约——对大家有利！
- en: 'Since we are in the supervised learning scenario, we need to implement the
    usual partitioning/learn/predict/score structure in order to validate against
    the risk of overfitting. We start by adding the **Partitioning** node and connecting
    it downstream to the **CSV Reader** node. In its configuration dialog, we leave
    the **Relative** 70% size for the training partition and we decide to protect
    the distribution of the target variable *Outcome* in both partitions, selecting
    the **Stratified sampling** option. Additionally, we put a static number in the
    random seed box (you can put `12345` as you see in *Figure 5.18*) and tick the
    adjacent checkbox:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们处于监督学习场景中，我们需要实现通常的分割/学习/预测/评分结构，以便验证是否存在过拟合的风险。我们首先添加**Partitioning**节点，并将其下游连接到**CSV
    Reader**节点。在其配置对话框中，我们将训练集的**相对**大小设置为 70%，并选择保护目标变量*Outcome*在两个分区中的分布，勾选**分层抽样**选项。此外，我们在随机种子框中填写一个静态数字（如*图
    5.18*所示，你可以填写`12345`），并勾选旁边的复选框：
- en: 'As a general rule, always perform a stratified sampling on the target variable
    of a classification. This will reduce the impact of imbalanced classes when learning
    and validating your model. There are other ways to restore a balance in the distribution
    of classes, such as under-sampling the majority class or over-sampling the minority
    one. One interesting approach is the creation of synthetic (and realistic) additional
    samples using algorithms like the **Synthetic Minority Over-sampling Technique**:
    check out the **SMOTE** node to learn more.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为一般规则，在分类的目标变量上始终执行分层抽样。这将减少学习和验证模型时类不平衡的影响。在恢复类分布平衡的其他方法中，可以通过对多数类进行欠采样或对少数类进行过采样来实现。一个有趣的方法是使用诸如**合成少数过采样技术**（SMOTE）这样的算法创建合成（和真实的）额外样本：了解更多信息，请查看**SMOTE**节点。
- en: '![Graphical user interface, text, application'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图形用户界面、文本、应用'
- en: Description automatically generated](img/B17125_05_18.png)
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成描述](img/B17125_05_18.png)
- en: 'Figure 5.18: Performing a stratified sampling using the Partitioning node:
    this way, we ensure a fair presence of yes and no customers in each partition'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.18：使用分区节点执行分层抽样：这样可以确保每个分区中都有足够的是和否客户
- en: 'Now that we have a training and test dataset readily available, we can proceed
    with implementing our first classification algorithm: **decision trees**. Let''s
    get a hint of how it works.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个训练和测试数据集，可以继续实施我们的第一个分类算法：**决策树**。让我们先了解一下它是如何工作的。
- en: Decision tree algorithm
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树算法
- en: 'Decision trees are simple models that describe a decision-making process. Have
    a look at the tree shown in *Figure 5.19* to get an idea of how they work. Their
    hierarchical structure resembles an upside-down tree. The root on top corresponds
    to the first question: according to the possible answers, there is a split between
    two or more subsequent *branches*. Every branch can either lead to additional
    questions (and respective splits into more branches) or terminate in *leaves*,
    indicating the outcome of the decision:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是描述决策过程的简单模型。看一下*图5.19*中显示的树，以了解它们的工作原理。它们的分层结构类似于倒置的树。顶部的根对应于第一个问题：根据可能的答案，会在两个或更多的*分支*之间进行分割。每个分支可以通向额外的问题（及相应的更多分支），也可以终止在*叶子*上，指示决策的结果：
- en: '![](img/B17125_05_19.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_19.png)'
- en: 'Figure 5.19: How will you go to work tomorrow? A decision tree can help you
    make up your mind'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19：明天你会如何去上班？决策树可以帮助你做出决定。
- en: 'Decision trees can be used to describe the process that assigns an entity to
    a class: in this case, we call it a **classification tree**. Think about a table
    where each entity (corresponding to a row) is described by multiple features (columns)
    and is assigned to one specific class, among different alternatives. For example,
    a classification tree that assigns consumers to multiple classes will answer the
    question *to which class does the consumer belong?*: every branching will correspond
    to different outcomes of a test on the features (like *is the age of the consumer
    higher than 35?* or *is the person married?*) while each terminal leaf will be
    one of the possible classes. Once you have defined the decision tree, you can
    apply it to all consumers (current and future). For every consumer in the table,
    you follow the decision tree: the features of the consumer will dictate which
    specific path to follow and result in a single leaf to be assigned as the class
    of the consumer.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可用于描述将实体分配给类别的过程：在这种情况下，我们称之为**分类树**。想象一张表，其中每个实体（对应一行）由多个特征（列）描述，并被分配到一种特定的类别中，其中有多种替代方案。例如，将消费者分配到多个类别的分类树将回答消费者属于哪个类别的问题：每个分支对应于对特征进行测试的不同结果（例如*消费者的年龄是否大于35岁？*或*这个人是否已婚？*），而每个终端叶子将是可能的类别之一。一旦定义了决策树，就可以将其应用于所有消费者（当前和未来）。对于表中的每个消费者，都要按照决策树进行操作：消费者的特征将决定跟随哪条具体路径，并导致一个单一叶子被分配为消费者的类别。
- en: 'There are many tree-based learning algorithms available for classification.
    They are able to "draw" trees by learning from labeled examples. These algorithms
    can find out the right splits and paths that end up with a decision model able
    to predict classes of new, unlabeled entities. The simplest version of a decision
    tree learning algorithm will proceed by iteration, starting from the root of the
    tree and checking what the "best possible" next split to make is so as to differentiate
    classes in the least ambiguous way. This concept will become clear by means of
    a practical example. Let''s imagine that we want to build a decision tree in order
    to predict which drink fast-food customers are going to order (among soda, wine,
    or beer), based on the food menu they had (the delicious alternatives are pizza,
    burger, or salad) and the composition of the table (whether it is among kids,
    couples, or groups of adults). The dataset to learn from will look like the one
    shown in *Figure 5.20*: we have 36 rows, each referring to a previous customer,
    and three columns, one for each feature (*Menu* and *Type*) and the target class
    (the *Favorite drink*).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, electronics'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_20.png)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.20: Drink preferences for 36 fast-food customers: can you predict
    their preferences based on their food menu and type?'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have only two features, the resulting decision tree can only have
    two levels, resulting in two alternative shapes: either the first split is by
    *Menu* and the second, at the level below, by *Type*, or the other way around.
    The learning algorithm will pick the split that makes the most sense by looking
    at the count of the items falling into each branch and checking which splits make
    the "clearest cut" among classes.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific case, the alternative choices for the first split are the
    ones drawn in *Figure 5.21*: you can find the number of customers falling into
    each branch, separated by alternative class (beer, soda, or wine). Have a look
    at the number and ask yourself: between the *Menu* split on the left and the *Type*
    split on the right, which one is differentiating in the "purest" way among the
    three classes?'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_21.png)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.21: Which of these two alternative splits gives you the most help
    in anticipating the choice of drinks?'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, it seems that the *Type* split on the right is a no-brainer:
    kids are consistently going for sodas (with the exception of 2 customers who—hopefully—got
    served with alcohol-free beer), groups prefer beers, while couples go mainly with
    wine. The other alternative (split by *Menu*) is messier: for those having salad
    and, to some extent, burger, there is no such clear cut drinks choice. Our preference
    for the option on the right is guided by human intuition: for an algorithm, we
    need to have a more deterministic way to make a decision. Tree learning algorithms
    use, in fact, metrics to decide which splits are best to pick. One of these metrics
    is called the **Gini index**, or **Impurity index**. Its formula is quite simple:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，右侧按*类型*划分似乎是显而易见的：孩子们一致选择苏打水（除了2个顾客——希望——他们得到的是无酒精啤酒），团体更喜欢啤酒，而情侣则主要选择葡萄酒。另一种选择（按*菜单*划分）则复杂一些：对于吃沙拉和在某种程度上吃汉堡的人来说，饮料的选择没有这么明确。我们选择右侧选项的偏好是基于人类的直觉：对于算法来说，我们需要一个更具确定性的方法来做出决策。实际上，决策树学习算法使用度量标准来决定哪些划分是最合适的。这些度量标准之一叫做**基尼指数**，或者**不纯度指数**。它的公式非常简单：
- en: '![](img/B17125_05_020.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_020.png)'
- en: 'where *f*[i] is the relative frequency of *i-n*^(th) class (it''s in the *%*
    column in *Figure 5.21*), among the *M* possible classes. The algorithm will calculate
    the *I*[G] for each possible branching of a split and average the results out.
    The option with the lowest Gini index (meaning, with the least "impure" cut) will
    win among the others. In our fast-food case, the overall Gini index for the option
    on the left will be the average of:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*f*[i]是第*i-n*^(th)类的相对频率（它位于*图 5.21*中的*%*列），在*M*个可能的类别中。算法会计算每个可能划分的*I*[G]，并对结果进行平均。基尼指数最低的选项（意味着“最纯”的划分）将胜出。在我们的快餐案例中，左侧选项的总体基尼指数将是以下内容的平均值：
- en: '![](img/B17125_05_021.png)![](img/B17125_05_022.png)![](img/B17125_05_023.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_021.png)![](img/B17125_05_022.png)![](img/B17125_05_023.png)'
- en: 'By averaging them out, we find that the Gini index for the left option is 0.60
    while the one for the right option is 0.38\. These metrics are confirming our
    intuition: the option on the right (the split by *Type*) is "purer" as demonstrated
    by the lower Gini index. Now you have all the elements to see how the decision
    tree learning algorithm works: it will iteratively calculate the average *I*[G]
    for all possible splits (at least one for each available feature), pick the one
    with the lowest index, and repeat the same at the levels below, for all possible
    branches, until it is not possible to split further. In the end, the leaves are
    assigned by just looking at where the majority of the known examples fall. For
    instance, take the branching on the right in *Figure 5.21*: if this was the last
    level of a tree, kids will be classified with soda, couples with wine, and groups
    with beer. You can see in *Figure 5.22* the resulting full decision tree you would
    obtain by using the fast-food data we presented above:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对它们进行平均，我们发现左侧选项的基尼指数为0.60，而右侧选项的基尼指数为0.38。这些度量标准验证了我们的直觉：右侧选项（按*类型*划分）更“纯净”，因为其基尼指数较低。现在你已经具备了所有元素，能够理解决策树学习算法是如何工作的：它将迭代计算所有可能划分的平均*I*[G]（至少对每个可用特征有一个），选择基尼指数最低的一个，并在下一级的所有可能分支中重复同样的过程，直到无法进一步划分。最后，叶子节点将根据已知示例的大多数归属情况进行分配。例如，看看*图
    5.21*中的右侧分支：如果这是树的最后一层，孩子们将被分类为选择苏打水，情侣选择葡萄酒，团体选择啤酒。在*图 5.22*中，你可以看到使用我们上面介绍的快餐数据得到的完整决策树：
- en: '![](img/B17125_05_22.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_22.png)'
- en: 'Figure 5.22: Decision tree for classifying fast-food customers according to
    their favorite drink.In which path would you normally be?'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22：根据顾客最喜欢的饮料分类的快餐顾客决策树。你通常会在哪条路径上？
- en: 'By looking at the obtained decision tree, you will notice that not all branches
    at the top level incur further splits at the level below. Take the example of
    the *Type*=`Kids` branch on the top left: the vast majority of kids (10 out of
    12) go for `Soda`. There are not enough remaining examples to make a meaningful
    further split by *Menu*, so the tree just stops there. On top of this basic stopping
    criterion, you can implement additional (and more stringent) conditions that limit
    the growth of the tree by removing less meaningful branches: these are called—quite
    appropriately, I must say—**pruning mechanisms**. By pruning a decision tree,
    you end up with a less complex model: this is very handy to use when you want
    to avoid model overfitting. Think about this: if you have many features and examples,
    your tree can grow massively.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看得到的决策树，你会注意到并非所有顶层的分支都会在下面的层级继续分裂。以左上方的*Type*=`Kids`分支为例：绝大多数孩子（12个中有10个）选择了`Soda`。剩余的样本不足以通过*Menu*做出有意义的进一步分裂，因此树就停在了这里。除了这个基本的停止标准之外，你还可以实现其他（更严格的）条件，通过移除不太有意义的分支来限制树的生长：这些被称为——我必须说，这个名字非常恰当——**修剪机制**。通过修剪决策树，你最终得到的是一个更简单的模型：当你想避免模型过拟合时，这非常有用。想一想：如果你有很多特征和样本，树可能会极其庞大。
- en: 'Every combination of values might, in theory, produce a very specific path.
    Chances are that these small branches cover an insignificant case that just happened
    to be in the training set but has no general value: this is a typical case of
    overfitting that we want to avoid as much as possible. That is why, as you will
    soon see in KNIME, you might need to activate some of the pruning mechanisms to
    avoid overfitting when growing trees.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每一种值的组合理论上都可能产生一个非常特定的路径。很有可能这些小分支覆盖的只是训练集中偶然出现的一个无关紧要的案例，而这个案例没有普遍价值：这是典型的过拟合问题，我们希望尽可能避免。这就是为什么，正如你很快在KNIME中看到的那样，你可能需要激活一些修剪机制，以避免在生长树时过拟合。
- en: 'Let''s make another consideration related to numeric features in decision trees.
    In the fast-food example, we only had nominal features, which make every split
    quite simple to imagine: every underlying branch covered a possible value of the
    categorical column. If you have a numeric column to be considered, the algorithm
    will check what the Gini index would be if you split your samples using a numeric
    threshold. The algorithm will try multiple thresholds and pick the best split
    that minimizes impurity. Let''s imagine that in our example we had an additional
    feature, called *Size*, that counts the number of people sitting at each table.
    The algorithm will test multiple thresholds and will check what the Gini index
    would be if you divided your samples according to these conditions, which are
    questions like "is *Size* > 3?", "is *Size* > 5?", and "is *Size* > 7?". If one
    of these conditions is meaningful, the split will be made according to the numeric
    variable: all samples having *Size* lower than the threshold will go to the left
    branch, and all others to the right branch. The Gini indices resulting from all
    the thresholds on the numeric features will be compared across all other indices
    coming from the categorical variables as we saw earlier: at each step, the purest
    split will win, irrespectively of its type. This is how decision trees can cleverly
    mix all types of features when classifying samples.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一下与决策树中数值特征相关的问题。在快餐示例中，我们只有名义特征，这使得每一次分裂都很容易想象：每个底层分支涵盖了类别列的一个可能值。如果你有一个数值列需要考虑，算法会检查如果使用数值阈值来划分样本时，基尼指数会是什么。算法会尝试多个阈值，并选择最佳的分裂方式来最小化杂质。假设在我们的示例中，我们有一个额外的特征，叫做*Size*，它统计了每张桌子上坐着的人数。算法会测试多个阈值，并检查如果根据这些条件划分样本时基尼指数会是什么，这些条件包括像“*Size*
    > 3?”、“*Size* > 5?”和“*Size* > 7?”。如果这些条件中的一个有意义，那么分裂会根据数值变量来进行：所有*Size*低于阈值的样本将进入左分支，其他样本进入右分支。来自所有数值特征的基尼指数将与我们之前看到的所有来自类别变量的指数进行比较：在每一步中，最纯净的分裂会胜出，不管它是什么类型。这就是决策树在分类样本时如何巧妙地混合各种特征类型的方式。
- en: 'Decision tree models can be extended to predict numbers and, so, become **regression
    trees**. In these trees, each leaf is labeled with a different value of the target
    variable. Normally, the value of the leaf is just the average of all the samples
    that ended up in such a leaf node, after going through a construction mechanism
    similar to the ones for classification trees (using Gini indices and all that).
    You can build regression trees in KNIME as well: have a look at the **simple regression
    tree** nodes in the repository.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树模型可以扩展为预测数字，从而成为**回归树**。在这些树中，每个叶子节点标注了目标变量的不同值。通常，叶子的值只是所有进入该叶子节点的样本的平均值，这些样本经过类似分类树构建机制的过程（使用基尼指数等）。你也可以在KNIME中构建回归树：请查看存储库中的**简单回归树**节点。
- en: 'Now that we know what decision trees are, let''s grow one to classify our bank
    customers according to the outcome of the telemarketing campaign. We''ll use a
    new node for that: the **Decision Tree Learner**.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了决策树是什么，让我们生长一棵树，用它来根据电话营销活动的结果对我们的银行客户进行分类。我们将使用一个新的节点：**决策树学习器**。
- en: '![Logo'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![Logo'
- en: Description automatically generated](img/image0491.png) *Decision Tree Learner*
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image0491.png) *决策树学习器*
- en: 'This node (**Analytics > Mining > Decision tree**) trains a decision tree model
    for predicting nominal variables (classification). The most important fields to
    be set in its configuration dialog (see *Figure 5.23*) are:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该节点（**分析 > 挖掘 > 决策树**）训练一个决策树模型，用于预测名义变量（分类）。在其配置对话框中（见*图 5.23*）需要设置的最重要字段是：
- en: '**Class column**: you need to specify your nominal target variable to be predicted.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别列**：你需要指定你要预测的名义目标变量。'
- en: '**Quality measure**: this is the metric used to decide how to make the splits.
    The default value is the **Gini index** we have encountered above. You can also
    select the information for **Gain ratio**, which would tend to create more numerous
    and smaller branches. There is not a good and bad choice, and in most cases both
    measures generate very similar trees: you can try them both and see which one
    produces the best results.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量度量**：这是用来决定如何进行分裂的度量标准。默认值是我们上面遇到的**基尼指数**。你也可以选择**增益比**的信息，这通常会创建更多且更小的分支。没有“好”与“不好”的选择，在大多数情况下，两种度量都会生成非常相似的树：你可以尝试它们两者，看看哪个能产生更好的结果。'
- en: '**Pruning method**: you can use this selector to activate a robust pruning
    technique called **MDL** (**Minimum Description Length**) that removes the less
    meaningful branches and generates a balanced tree.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝方法**：你可以使用此选择器来激活一种强大的剪枝技术，称为**MDL**（**最小描述长度**），它去除不太重要的分支并生成一个平衡的树。'
- en: '**Min number records per node**: you can control the tree growth-stopping criterion
    by setting a minimum number of samples for allowing a further split. By default,
    this hyperparameter is set to `2`: this means that no branch will be generated
    with less than 2 samples. As you increase this number, you will prune more branches
    and obtain smaller and smaller trees: this is an effective way for tuning the
    complexity of the trees and obtaining an optimal, well-fitted model. By activating
    the MDL technique in the earlier selector, you go the "easy way" as it will automatically
    guess the right level of pruning.![](img/B17125_05_23.png)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个节点的最小记录数**：你可以通过设置允许进一步分裂的最小样本数来控制树的生长停止标准。默认情况下，此超参数设置为`2`：这意味着没有分支会生成少于
    2 个样本的节点。随着这个数字的增加，你将剪去更多的分支，并得到越来越小的树：这是调整树复杂度并获得最佳拟合模型的有效方法。通过在之前的选择器中激活MDL技术，你走的是“捷径”，因为它会自动推测正确的剪枝水平。![](img/B17125_05_23.png)'
- en: 'Figure 5.23: Configuration window of the Decision Tree Learner node: are you
    up for some pruning today?'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23：决策树学习器节点的配置窗口：今天要剪枝吗？
- en: 'The output of the node is the definition of the tree model, which can be explored
    by opening its main view (right-click on the node and select **View: Decision
    Tree View**). In *Figure 5.24*, you will find the KNIME output of the fast-food
    classification tree we obtained earlier (see, for comparison, *Figures 5.22* and
    *5.21*): at each node of the tree, you find the number of training samples falling
    into each value of the class. You can expand and collapse the branches by clicking
    on the circled **+** and **–** signs appearing at each split:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '节点的输出是树模型的定义，可以通过打开其主视图来探索（右键单击节点，选择**View: Decision Tree View**）。在*图 5.24*中，您将找到我们之前获取的快餐分类树的
    KNIME 输出（可参见比较*图 5.22*和*5.21*）：在树的每个节点处，您会发现落入每个类值的训练样本数。您可以通过单击每个分割点处显示的带有圆形
    **+** 和 **–** 标志来展开和折叠分支：'
- en: '![](img/B17125_05_24.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_24.png)'
- en: 'Figure 5.24: The fast-food classification tree, as outputted by the Decision
    Tree Learner node in KNIME.The gray rows correspond to the majority class'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：快餐分类树，由 KNIME 中的 Decision Tree Learner 节点输出。灰色行对应于主要类别
- en: 'Drag and drop the **Decision Tree Learner** node from the repository and connect
    the upper output of the **Partitioning node** (the training set) with it. Let''s
    leave all the default values for now in its configuration (we will have the opportunity
    for some pruning later): the only selector to double-check is the one setting
    the **Class column** that in our case is *Outcome*. If you run the node and open
    its decision tree view (select the node and press *F10*), you will meet the tree
    you have just grown:![](img/B17125_05_25.png)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从存储库中拖放**Decision Tree Learner**节点，并将其连接到**Partitioning node**的上输出（训练集）。暂时保留其配置的所有默认值（稍后我们将有机会进行一些修剪）：唯一需要双重检查的选择器是设置**Class
    column**的选择器，在我们的案例中为*Outcome*。如果您运行节点并打开其决策树视图（选择节点并按 *F10*），您将会看到刚刚生成的树：![](img/B17125_05_25.png)
- en: 'Figure 5.25: A first tree classifying bank customers by Outcome: this is just
    a partial view of the many levels and branches available'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.25：首个根据 Outcome 对银行客户进行分类的树：这只是可用的许多级别和分支的部分视图
- en: 'As you expand some of the branches, you realize that the tree is very wide
    and deep: *Figure 5.25* shows an excerpt of what the tree might look like (depending
    on your random partitioning, you might end up with a different tree, which is
    fine). In this case, we noticed that the top split divided customers into mobile
    and landline users. This is what happened: the Gini index was calculated across
    all features and scored the lowest for *Contact*, making this the single most
    important variable to differentiate customers according to their *Outcome*. Let''s
    see whether this tree is good enough and predict the outcomes in the test set.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当您展开某些分支时，您会意识到树非常宽和深：*图 5.25*展示了树可能的部分外观（根据随机分区，您可能会得到不同的树，这是正常的）。在这种情况下，我们注意到顶部分割将客户分为移动用户和固定电话用户。事情是这样的：对所有特征计算了基尼指数，并且在*Contact*方面得分最低，使其成为区分客户*Outcome*最重要的变量。让我们看看这棵树是否足够好并预测测试集中的结果。
- en: '![Icon'
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![图标'
- en: Description automatically generated](img/image0531.png) *Decision Tree Predictor*
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/image0531.png) *Decision Tree Predictor*
- en: This (**Analytics > Mining > Decision tree**) applies a decision tree model
    (provided as an input in the first port) to a dataset (second port) and returns
    the prediction for each input row. This node will not require any configuration
    and will produce a similar table to the one provided in the input with an additional
    column that includes the result of the classification.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个（**Analytics > Mining > Decision tree**）应用决策树模型（作为第一个端口的输入）到数据集（第二个端口），并返回每个输入行的预测结果。这个节点不需要任何配置，并且将生成与输入中提供的表格类似的表格，其中包含一个额外的列，包含分类结果。
- en: Let's implement the **Decision Tree Predictor** node and wire it in such a way
    it gets as inputs the tree model outputted by the **Decision Tree Learner** node
    and the second outport of the **Partitioning** node, which is our test set. As
    you execute the node, you will find an output that the precious additional column
    called *Prediction (Outcome)*.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实施**Decision Tree Predictor**节点，并将其连接到如下所示的方式：该节点获取由**Decision Tree Learner**节点输出的树模型以及**Partitioning**节点的第二输出端口，即我们的测试集。当您执行该节点时，您将找到一个输出，其中包含名为*Prediction
    (Outcome)*的宝贵附加列。
- en: 'At this point, we can finally assess the performance of the model by calculating
    the metrics used for classification. Do you remember the accuracy, precision,
    sensitivity measures, and confusion matrix we obtained in the cute dog versus
    muffin example? It''s time to calculate these metrics by using the right node:
    **Scorer**.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image054.png) *Scorer*
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Scoring**) calculates the summary performance
    scores of classification by comparing two nominal columns. The only step required
    for its configuration (*Figure 5.26*) is the selection of the columns to be compared:
    you should select the column carrying the observed (actual) values in the **First
    Column** dropdown, while predictions go in the **Second Column** selector. The
    node outputs the most important metrics for assessing a classification performance,
    namely: the Confusion Matrix, provided as a table in the first output (columns
    will refer to the predictions, while actual values will go as rows) and summary
    metrics such as **Accuracy**, **Precision**, and **Sensitivity**, which you can
    find in the second output of the node.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the performance metrics for a classification will depend on which class
    you decide to be considered as `Positive`: have a look at *Figure 4.8* in the
    previous chapter to get a refresher. In the second output of the Scorer node,
    you will find one row for every possible class: each row contains the metrics
    calculated under the assumption that one specific class is labeled as `Positive`
    and all the other classes are `Negative`.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_26.png)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.26: The configuration window of the Scorer node: just select the columns
    to compare across'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add the **Scorer** node (make sure you don''t get confused and pick
    the **Numeric Scorer** node, which can only be used for regressions) to the workflow
    and connect it downstream to the **Decision Tree Predictor**. In the configuration
    window, we can leave everything as it is, just checking that we have *Outcome*
    as **First Column** and *Prediction (Outcome)* as **Second Column**. Execute the
    node and open its main view (*F10* or right-click and select **View: Confusion
    Matrix**).'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the **Scorer** node (*Figure 5.27*) tells us that we get an accuracy
    level of 78.3%: out of 100 predictions, 78 of them turn out to be correct. The
    confusion matrix helps us understand whether the model can bring value to our
    business case:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_27.png)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.27: The output of the node Scorer after our first classification:
    78% accuracy is not bad as a starting point'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the case shown in *Figure 5.27*, we have 450 customers (180 + 270) in the
    test set that were predicted as interested in the account (*Prediction (Outcome)*
    = `yes`). Out of this, only 180 (40%, which corresponds to the precision of our
    model) were predicted correctly, meaning that these customers ended up buying
    the product. The number seems to be low, but it is already encouraging: the algorithm
    can help to find a subset of customers that are more likely to buy the product.
    If we indiscriminately called every customer—as we know from the pilot—we would
    have achieved a success rate of 19% while, by focusing on the (fewer) customers
    that the algorithm identified as potential (*Prediction (Outcome)* = `yes`), the
    success rate would double and reach 40%.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now think about what we can do to improve the results of the modeling.
    We remember that our decision tree was deep and wide: some of the branches were
    leading to very "specific" cases, which interested only a handful of examples
    in the training set. This doesn''t look right: a decision tree that adapted so
    closely to the training set might produce high errors in future cases as it is
    not able to comprehend the essential patterns of general validity. We might be
    overfitting! Let''s equip ourselves with a good pair of pruning shears: we can
    try to fix the overfitting by reducing the complexity of the tree, making some
    smart cuts here and there:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, the Decision Tree Predictor node generates null predictions (red
    `?` in KNIME tables, which caused the warning message you see at the top of *Figure
    5.27*). This is a sign that the tree might be overfitted: its paths are too "specific"
    and do not encompass the set of values that require a prediction (this "pathology"
    is called **No True Child**). Besides taking care of the overfitting, one trick
    you can apply to solve the missing values is to open the **PMMLSettings** panel
    (second tab in the **Decision Tree Learner** configuration) and set **No true
    child strategy** to **returnLastPrediction**.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the configuration dialog of the **Decision Tree Learner** and select **MDL**
    as the **Pruning method**. This is the simplest and quickest way to prune our
    tree: we could have also iterated through higher values of **Min number records
    per node** (give it a try to check how it works), but MDL is a safe approach to
    get quick improvements.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see if it worked. We don't need to change anything else, so let's just
    execute the **Scorer** node and open its main view to see what happened.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you look at the results (*Figure 5.28*) you feel a thrill of excitement:
    things got better. The accuracy raised to 83% and, most importantly, the precision
    of the model greatly increased. Out of the 175 customers in the test set who are
    now predicted as *Outcome*=`yes`, 117 would have ended up actually buying the
    product. If we followed the recommendation of the model (which we can assume will
    keep a similar predictive performance on customers we didn''t call yet—so the
    remaining 97% of our customer base), the success rate of our marketing campaign
    will move to 67%, which is more than 3 times better than our initial baseline
    of 19%!'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你看到结果时（*图 5.28*），你会感到一阵兴奋：情况变得更好。准确率提高到了 83%，最重要的是，模型的精度大大提升。在测试集中175名现在被预测为*Outcome*=`yes`的客户中，117人最终会真正购买产品。如果我们遵循模型的推荐（我们可以假设它在我们尚未联系的客户群体中也会保持类似的预测效果——即剩余的97%客户群体），那么我们的营销活动的成功率将提升至67%，这比最初的19%基准提高了三倍多！
- en: '![](img/B17125_05_28.png)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17125_05_28.png)'
- en: 'Figure 5.28: The output of the node Scorer after our tree pruning: the precision'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.28：树修剪后节点评分器的输出：精度
- en: 'The model was previously overfitting and some pruning clearly helped. If you
    now open the tree view of the **Decision Tree Learner** node, you will find a
    much simpler model that can be explored and, finally, interpreted. You can expand
    all branches at once by selecting the root node (just left-click on it) and then
    clicking on **Tree** | **Expand Selected Branch** from the top menu. By looking
    at the tree, which might be similar to the one shown in *Figure 5.29*, we can
    finally attempt some interpretation of the model. Look at the different percentages
    of the `yes` category within each node: we found some buckets of customers that
    are disproportionally interested in our product:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 模型之前存在过拟合问题，一些修剪明显有所帮助。如果你现在打开**决策树学习器**节点的树状视图，你会发现一个更简单的模型，它可以被探索并最终进行解释。你可以通过选择根节点（只需左键点击它），然后点击顶部菜单中的**树**
    | **展开选定分支**来一次性展开所有分支。通过查看这棵树（可能类似于*图 5.29*所示），我们最终可以尝试对模型进行一些解释。观察每个节点中`yes`类别的不同百分比：我们发现一些客户群体对我们的产品表现出不成比例的兴趣：
- en: '![](img/B17125_05_29.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17125_05_29.png)'
- en: 'Figure 5.29: An excerpt of the decision tree classifying bank customers by
    Outcome: students, retired, and 60+ customers using landlines are showing the
    most interest in our new savings account'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29：决策树摘录，按结果分类的银行客户：学生、退休人员和使用固定电话的60岁以上客户表现出对我们新储蓄账户的最大兴趣
- en: 'For example, we find out that customers falling into these three segments:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们发现这三个细分市场的客户群体：
- en: Mobile users who are students
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动设备用户中的学生
- en: Mobile users who are retired
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动设备用户中退休的人群
- en: Landline users who are 60+ years old
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定电话的60岁以上用户
- en: 'responded much more to our pilot campaign than all others, having more than
    50% of the samples ending up with opening a new savings account. We have a quick
    chat with the product manager and show these results to him. He is very excited
    about the findings and, after some thinking, he confirms that what the algorithm
    spotted makes perfect sense from a business standpoint. The new type of account
    has less fixed costs than the others, so this explains while its proposition proves
    more compelling to lower-income customers, such as students and the retired. Additionally,
    this account includes a free prepaid card, which is a great tool for students,
    who can get their balance topped up progressively, but also for older customers,
    who do not fully trust yet the usage of traditional credit cards and prefer keeping
    the risk of fraud under control. The account manager is very pleased with what
    you shared with him and does not stop thanking you: by having data-based evidence
    of the characteristics that make a customer more likely to buy his new product,
    he can now finetune the marketing concept, highlighting benefits and reinforcing
    the message to share with prospective customers.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'The positive feedback you just received was invigorating and you want to quickly
    move to the second part of the challenge: building a propensity model able to
    "score" the 97% of the customers that have not been contacted yet. To do so, we
    will first need to introduce another classification algorithm particularly well
    suited for anticipating propensities: **random forest**.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Random forest algorithm
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One approach used in machine learning to obtain better performance is **ensemble
    learning**. The idea behind it is very simple: instead of building a single model,
    you combine multiple *base* models together and obtain an *ensemble* model that,
    collectively, produces stronger results than any of the underlying models. If
    we apply this concept to decision trees, we will grow multiple models in parallel
    and obtain… a forest. However, if we run the decision tree algorithm we''ve seen
    in the previous pages to the same data set multiple times, we will just obtain
    "copies" of identical trees. Think about it: the procedure we described earlier
    (with the calculation of the Gini index and the building of subsequent branches)
    is completely deterministic and will always produce the same outputs when using
    the same inputs. To encourage "diversity" across the base models, we need to force
    some variance in the inputs: one way to do so is to randomly sample subsets of
    rows and columns of our input dataset, and offer them as different training sets
    to independently growing base models. Then, we will just need to aggregate the
    results of the several base models into a single ensemble model. This is called
    **Bagging**, short for **Bootstrap Aggregation**, which is the secret ingredient
    that we are going to use to move from decision trees to random forests.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how it works, let''s visualize it in a practical example: *Figure
    5.30* shows both a simple decision tree and a random forest (made of four trees)
    built on our bank telemarketing example:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_30.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: A decision tree and random forest compared: with the forest you
    get a propensity score and higher accuracy'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to a random sampling of rows and columns, we managed to grow four different
    trees, starting from the same initial dataset. Look at the tree on the bottom
    left (marked as *#1* in the figure): it only had the *Mortgage* and the *Contact*
    columns available to learn from, as they were the ones randomly sampled in its
    case. Given the subset of rows that were offered to it (that were also randomly
    drawn as part of the bootstrap process), the model applies the decision tree algorithm
    and produces a tree that differs from all other base models (you can check the
    four trees at the bottom—they are all different). Given the four trees that make
    our forest, let''s imagine that we want to predict the outcome for a 63-year-old
    retired customer, who has a mortgage and gets contacted by landline. The *same*
    customer will follow four *different* paths (one for each tree), which will lead
    to different outcomes. In this case, 3 trees out of 4 agree that the prediction
    should be `yes`. The resulting ensemble prediction will be made in a very democratic
    manner, by voting. Since the majority believes that this customer is a `yes`,
    the final outcome will be `yes` with a **Propensity score** of 0.75 (3 divided
    by 4).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'The assumption we make is that the more trees that are in agreement with a
    customer being classified as `yes`, the "closer" the customer is to buying our
    product. Of course, we normally build many more trees than just four: the diversity
    of the different branching each tree displays will make our ensemble model more
    "sensitive" to the smaller nuances of feature combinations that can tell us something
    useful about the propensity of a customer. Every tree offers a slightly "different"
    point of view on how to classify a customer: by bringing all these contributions
    together—in a sort of decisions crowdsourcing—we obtain more robust collective
    predictions: this is yet another proof of the universal value of diversity in
    life!'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the propensity score is related to the probability that a classification
    is correct, they are not the same thing. We are still in the uncertain world of
    probabilistic models: even if 100% of the trees agree on a specific classification,
    you cannot be 100% sure that the classification is right.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get acquainted with the KNIME node that can grow forests: meet the **Random
    Forest Learner** node.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image060.png) *Random Forest Learner*'
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) trains a random forest model for classification. At the top
    of its configuration window (*Figure 5.31*) you can select the nominal column
    to use as the target of the classification (**Target Column**). Then, in the column
    selector in the middle, you can choose which columns to use as features (the ones
    appearing on the **Include** box on the right): all others will be ignored by
    the learning algorithm. The option **Save target distribution…** will record the
    number of samples that fell into each leaf of the underlying tree models: although
    it is memory expensive, it can help to generate more accurate propensity scores,
    by means of the **soft voting** technique, which we will talk about later.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个节点（**分析 > 挖掘 > 决策树集成 > 随机森林 > 分类**）训练一个用于分类的随机森林模型。在其配置窗口的顶部（*图5.31*），你可以选择用作分类目标的名义列（**目标列**）。然后，在中间的列选择器中，你可以选择哪些列作为特征（在右侧**包括**框中显示的列）：所有其他列将被学习算法忽略。选项**保存目标分布…**将记录每个叶节点中样本的数量：尽管这会消耗大量内存，但它可以帮助通过**软投票**技术生成更准确的倾向得分，关于这一点我们稍后会讨论。
- en: 'Toward the bottom of the window, you will find also a box that lets you choose
    how many trees you want to grow (**Number of models**). Lastly, you can decide
    to check a tick box (labeled as **Use static random seed**) that, similarly to
    what you found in the **Partitioning** node, lets you "fix" the initialization
    seed of the pseudo-random number generator used for the random sampling of rows
    and columns: in this case, you will obtain, at parity of input and configuration
    parameters, always the same forest generated:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在窗口底部，你还会看到一个框，让你选择想要生成的树木数量（**模型数量**）。最后，你可以决定勾选一个复选框（标记为**使用静态随机种子**），它类似于你在**分区**节点中找到的内容，让你“固定”用于随机抽取行和列的伪随机数生成器的初始化种子：在这种情况下，输入和配置参数相同的情况下，你将始终获得相同的森林：
- en: '![Graphical user interface, application'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序'
- en: Description automatically generated](img/B17125_05_31.png)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17125_05_31.png)
- en: 'Figure 5.31: Configuration window of the Random Forest Learner node: how many
    trees you want to see in the forest?'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31：随机森林学习器节点的配置窗口：你想在森林中看到多少棵树？
- en: 'Let''s implement the **Random Forest Learner** node and connect the training
    set (the first output port of the **Partitioning** node) with its input: there
    is no harm in reusing the same training and test sets used for the decision tree
    learner. If we execute the node and open its main view (*F10* or right-click and
    then select **View: Tree Views**), we will find a tree-like output, as in the
    case of the decision trees: however, this time, we have a little selector at the
    top that lets us scroll across all 100 trees of the forest.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实现**随机森林学习器**节点，并将训练集（**分区**节点的第一个输出端口）连接到它的输入：重新使用决策树学习器使用的相同训练和测试集是没有问题的。如果我们执行该节点并打开其主视图（*F10*或右键点击然后选择**视图：树视图**），我们将看到类似树形结构的输出，就像决策树的情况一样：然而，这次，我们在顶部有一个小的选择器，允许我们滚动查看森林中的100棵树。
- en: 'Random forests are **black box** models as they are hard to interpret: going
    through 100 different trees would not offer us a hint for explaining how the predictions
    are made. However, there is a simple way to check which features proved to be
    most meaningful. Open the second outport of the **Random Forest Learner** node
    (right-click and click on **Attribute statistics**). The first column—called *#splits
    (level 0)*—tells you how many times that feature was selected as the top split
    of a tree. The higher that number, the more useful that feature has been in the
    learning process of the model.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是**黑箱**模型，因为它们难以解释：浏览100棵不同的树并不能提供关于如何做出预测的线索。然而，有一种简单的方法可以检查哪些特征最为重要。打开**随机森林学习器**节点的第二个输出端口（右键点击并选择**属性统计**）。第一列——称为*#划分（级别0）*——告诉你该特征作为树的顶部划分被选择了多少次。这个数字越高，说明该特征在模型的学习过程中越有用。
- en: '![](img/image062.png) *Random Forest Predictor*'
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '![](img/image062.png) *随机森林预测器*'
- en: 'This node (**Analytics > Mining > Decision Tree Ensemble > Random Forest >
    Classification**) applies a random forest model (which needs to be provided in
    the first gray input port) to a dataset (second port) and returns the ensemble
    prediction for each input row. As part of its configuration, you can decide whether
    you want to output the propensity scores for each individual class (**Append individual
    class probabilities**). If you tick the **Use soft voting** box, you enable a
    more accurate estimation of propensity: in this case, the vote of each tree will
    be weighted by a factor that depends on how many samples fell in each leaf during
    the learning process. The more samples a leaf has "seen," the more confident we
    can be about its estimation. To use this feature, you will have to select the
    option **Save target distribution…** in the **Random Forest Learning** node, which
    is upstream.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_32.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.32: The configuration dialog of Random Forest Learner node. You can
    decide whether you want to see propensity scores or not.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the **Random Forest Predictor** node onto the workflow and connect
    its inputs with the forest model, outputted by the **Random Forest Learner** and
    the training set, meaning the bottom outport of the **Partitioning** node. Configure
    the node by unticking the **Append overall prediction confidence** box, and ticking
    both the **Append individual class probabilities** (we need the propensity score)
    and the **Use soft voting** boxes. After you execute it, you will find at its
    output the test set enriched with the prediction, *Prediction (Outcome)*, and
    the propensity scores by class. Specifically, the propensity of a customer being
    interested in our product is *P (Outcome=Yes)*.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a new **Scorer** node (for simplicity, you can copy/paste the one
    you used for the decision tree) and connect it downstream to the **Random Forest
    Predictor**. For its configuration, just make sure you select *Outcome* and *Prediction
    (Outcome)* in the first two drop-down menus. Execute it and open its main output
    view (*F10*).
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results of **Scorer** (*Figure 5.33*) confirm that, at least in this case,
    the ensemble model comes with better performance metrics. Accuracy has increased
    by a few decimal points and, most importantly (as it directly affects the ROI
    of our marketing campaigns), precision has reached 72% (open the **Accuracy statistics**
    outport to check it or compute it easily from the confusion matrix):'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_33.png)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.33: The Scorer node output for our random forest. Both accuracy and
    precision increased versus the decision tree: diversity helps'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have confirmation that we have built a robust model at hand, let's
    concentrate on the propensity score we calculated and see what we can do with
    it.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the output of the **Random Forest Predictor** node and sort the rows by
    decreasing level of propensity (click on the header of column *P (Outcome=yes)*
    and then on **Sort Descending**): you will obtain a view similar to the one shown
    in *Figure 5.34*:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_34.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: The predictions generated by the random forest in descending order
    of propensity, P (Outcome=yes): the more we go down the list, the less interested
    customers (column Outcome) we find'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the list, we have the customers in the test set that most decision
    trees identified as interested. In fact, if you look at the column *Outcome*,
    we find that most rows show a `yes`, proving that, indeed, these customers were
    very interested in the product (when called, they agreed to open the savings account).
    If you scroll down the list, the propensity will go down and you will start finding
    increasingly more `no` values in column *Outcome*. Now, let''s think about the
    business case once again: now that we have a model able to predict the level of
    propensity, we could run it on the other 97% of customers that were not contacted
    as part of the pilot. If we then sorted our customer list by decreasing level
    of propensity (as we just did on the test set), we will obtain a prioritized list
    of the next people to call about our product. We will expect that the first calls
    (the ones directed to the most inclined people) will end up with a very high success
    rate (like we noticed in the test set).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, little by little, the success rate will decay: more and more people will
    start saying `no` and, at some point, it will start to become counterproductive
    to make a call. So, the key question becomes: at what point should we "stop" to
    get the maximum possible ROI from the initiative? How many calls should we make?
    What is the minimum level of propensity, below which we should avoid attempting
    to make a sale? The exciting part of propensity modeling is that you can find
    an answer to these questions before making *any* call!'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, if we assume that the customers that were part of the pilot were a
    fair sample of the total population, then we can use our test set (which has not
    been "seen" by the training algorithm, so there is no risk of overfitting) as
    a base for simulating the ROI of a marketing campaign where we call customers
    by following a decreasing level of propensity. This is exactly what we are going
    to do right now: we will need to first sort the test set by decreasing level of
    propensity (the temporary sorting we did earlier did not impact the permanent
    order of the rows in the underlying table); then, we calculate the cumulative
    profit we would make by "going through the list," using the cost and revenue estimates
    shared by the product manager. We check at which level of propensity we maximized
    our profit, so that we have a good estimate of the number of calls that we will
    need to make in total to optimize the ROI. Let''s get cracking!'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Sorter** node and connect it at the output of the **Random Forest
    Predictor** node. We want to sort the customers in the test set by decreasing
    level of propensity, so select column *P (Outcome=yes)* and go for the **Descending**
    option.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement a **Rule Engine** node to calculate the marginal profit we make on
    each individual customer. We know that every call we make costs us $15, irrespective
    of its outcome. We also know that every account opening brings an incremental
    revenue of $60\. Hence, every customer that ends up buying the product (*Outcome*=`Yes`)
    brings $45 of profit while all others hit us by $–15\. Let's create a column (we
    can call it *Profit*) that implements this simple logic, as shown in *Figure 5.35*:![Graphical
    user interface, text, application
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_35.png)
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.35: The Rule Engine node for calculating the marginal profit for each
    individual customer'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To calculate the cumulative profit we will need to use a new node, called **Moving
    Aggregation**.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clipart'
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image067.png) *Moving Aggregation*
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, this node (**Other Data Types > Time Series > Smoothing**)
    aggregates values on moving windows and calculates cumulative summarizations.
    To use a moving window, you will have to declare the **Window length** in terms
    of the number of rows to be considered and the **Window type** (meaning the direction
    of movement of the window in the table). For example, if you select **3** as the
    length and **Backward** as the type, the previous 3 rows will be aggregated together.
    If you want to aggregate by cumulating values from the first row to the last,
    you need to check the **Cumulative computation** box. Similarly to a Group By
    node, the **Aggregation settings** tab will let you select which columns should
    be aggregated and using which method:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_36.png)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.36: Configuration dialog of the Moving Aggregation node: you can aggregate
    through moving windows or by progressively cumulating'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **Moving Aggregation** node and connect it downstream from the
    **Rule Engine**. Check the **Cumulative Computation** box, double-click on the
    *Profit* column on the left, and select **Sum** as the aggregation method. Execute
    the node and open its outport view.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Moving Aggregation** node has cumulated the marginal profit generated
    by each customer. If we scroll the list (similar to the one displayed in *Figure
    5.37*) and keep an eye on the last column, *Sum(Profit)*, we noticed that the
    profit peaks when we are slightly below the first third of the full list. When
    the *P (Outcome=yes)* propensity is near 0.23, we obtain a profit of around $8,200\.
    This means that by calling only people above this level of propensity (called
    the **Cutoff** point), we maximize the ROI of our campaign.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_05_37.png)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.37: The output of the Moving Aggregation node: it seems that we reach
    maximum profit when we call people having a propensity of around 0.23.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To make this concept clearer, let's visualize the changing profit by employing
    a line chart.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text'
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image070.png) *Line Plot (local)*
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: This node (**View > Local (Swing)**) generates a line plot. The only configuration
    that might be needed is the box labeled **No. of rows to display**, which you
    can use to extend the limit of rows considered for creating the plot.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Line Plot (local)** node, extend the number of rows to display
    to at least 3,000 (the size of the test set), execute it, and open its view at
    once (*Shift* + *F10*). In the **Column Selection** tab, keep only *Sum(Profit)*
    on the right and remove all other columns.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output of the chart (shown in *Figure 5.38*) confirms what we noticed in
    the table and makes it more evident: if we use the propensity score to decide
    the calling order of customers, our profit will follow the shape of the curve
    in the figure. We will start with a steep increase of profit (see the first segment
    on the left), as most of the first people we call (which are top prospects, given
    their high propensity score) will actually buy the product. Then, at around one-third
    of the list (when we know that the propensity score is near 0.23), we reach the
    maximum possible profit. After that, it will drop fast as we will encounter fewer
    and fewer interested customers. If we called all the people on the list, we will
    end up with a significant loss, as we have painfully learned as part of the pilot
    campaign:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_38.png)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.38: The cumulative profit curve for our machine learning-assisted
    telemarketing campaign: we maximize the ROI at around one-third of the list sorted
    by propensity'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to this simulation, we have discovered that if we limit our campaign
    to customers with a propensity score higher than 0.23 (which will be around one-third
    of the total population), we will maximize our profit. By doing the required proportions
    (our simulation covered *only* the test set, so 3,000 customers in total), we
    can estimate how much profit we would make if we applied our propensity model
    to the *entire* bank database. In this case, we would use the scores to decide
    who to call within the remaining 97% of the customer base. The overall "size of
    the prize" of conducting a mass telemarketing campaign will bring around $800,000
    of profit, if we were to call one-third of the bank''s customers. Considering
    that it might not be viable to make so many calls, we might stop earlier in the
    list: in any case, we will make some considerable profit by following the list
    that our random forest can now generate. The simulation that we just did can be
    used as a tool for planning the marketing spend and sizing the right level of
    investment. The product manager and your boss are pleased with the great work
    you pulled together. You definitely proved that spotting (and following) the ML
    way can bring sizeable value to the business: in this case, you completely reversed
    the potential outcome of a marketing campaign. The heavy losses in the pilot can
    now be transformed into a meaningful value, thanks to data, algorithms, and—most
    importantly—your expertise in leveraging them. It was a terrific result, and it
    took only 12 KNIME nodes (*Figure 5.39*) to put all of this together!'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_05_39.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: Full workflow for the bank telemarketing optimization'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting consumers with clustering
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, you will re-enter the shoes of the business analyst working
    for the online retailer we encountered in *Chapter 3*, *Transforming Data*. This
    time, instead of automating the creation of a set of financial reports, you are
    after a seemingly sexier objective. The **Customer Relationship Management** (**CRM**)
    team is looking for a smarter way to communicate with those customers who opted-in
    to receive regular newsletters. Instead of sending a weekly email equal for all,
    the CRM manager asked you to find a data-based approach for creating a few meaningful
    consumer segments. Once segments are defined, the CRM team can build multiple
    messages, one for each segment. By doing so, they will offer a more personalized
    (and engaging) experience for the entire customer base, which will ultimately
    affect customer loyalty and drive sustainable revenue growth.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning offers a proven methodology that can meet this business
    need: by using a clustering algorithm, we can create several groups of customers
    that *look similar* in terms of their characteristics (such as age, family composition,
    and income level) and the consumption patterns they displayed through previous
    purchases (like the average price of the products they selected, the overall amount
    of money they spent, or the frequency of their orders). This is the ML way of
    helping the business: use clustering to segment consumers appropriately.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'The CRM manager has already initiated the gathering of some basic consumer-level
    data and obtained a CSV file (`eCommerce-CRM.csv`), which has 4,157 rows—one for
    each customer—and four columns:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '*Customer_ID*: a unique identifier of the customer.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Average Price*: the average unit price for all purchases made by each customer.
    It gives us a directional view of the "premiumness" of the former shopping choices
    displayed by the customer.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Basket Size*: the average number of units purchased within any single order
    created by the customers. This measure indicates whether they prefer to go for
    "bulk" shopping with fat baskets or smaller, occasion-driven purchase acts.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unique Products*: the average number of different articles that the customer
    buys on each occasion. This metric indicates the breadth of the assortment "tried"
    by each customer. It gives us an idea of the customer''s willingness to explore
    new products versus their preference of "keep buying" the same articles all the
    time.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we exchange thoughts with the CRM manager about this dataset, she confirms
    what we had already noticed: the three consumption metrics included in the data
    (the last three columns) are far from giving us a comprehensive picture of each
    customer''s preferences. If we wanted, we could have generated many more columns
    by aggregating the transactions history: think about the absolute number of purchases
    by customer, the total generated value, the "mix" of purchased categories and
    subcategories, the premiumness of the purchased products within each category
    and subcategory, and also the customer characteristics, like their age, the average
    income of the neighborhood they live in, and so on. Still, we decide to go ahead
    and leverage the power of machine learning on this first dataset: we can always
    increase the level of the model sophistication later if we want. Now, the important
    thing is to "start rocking" and pragmatically prove some first business value
    from this new way of operating. In terms of deliverables, you align with your
    business partner the need to assign each customer to a small number of clusters
    and put together some visualizations to interpret what differentiates clusters.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time to power KNIME on, create a new workflow, and load our CRM extract
    into it:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Load the file `eCommerce-CRM.csv` onto the workflow editor. As the **CSV Reader**
    node dialog pops up, we can check that all four columns are showing in the preview
    and click **OK** to confirm the default setting. After executing the node, we
    can look at its output view (*Figure 5.40*) and move to the next step:![Table
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_40.png)
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.40: The CRM extract once loaded: for every customer, we have three
    metrics, each one giving us a hint of their shopping habits'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Creating homogenous groups of elements, such as customers in our case, requires
    the use of a clustering algorithm. Let's make acquaintance with possibly the most
    popular clustering algorithm available today – **k-means**.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: K-means algorithm
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means algorithm is perhaps the easiest (and yet probably the most used)
    approach used for clustering. The big idea is elementary and can be summarized
    in two lines: each element in a dataset is assigned to the closest cluster. At
    each step of the process, the position of the clusters gets updated, so they become
    more and more compact.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine we want to cluster a set of points displayed on a bi-dimensional
    scatter plot. Each point is described employing two numbers that represent the
    horizontal and the vertical coordinates, respectively. The distance between any
    two points can be easily calculated through the Pythagorean theorem (yes, the
    same used for calculating the sides of a right triangle—see *Figure 5.41* for
    a refresher):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_41.png)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.41: Calculating the distance between two points using the Pythagorean
    theorem: you make the square root of the sum of the squared differences for each
    coordinate'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the k-means algorithm is to create a given number (*k*) of homogenous
    groups formed by points that are relatively close to one another. Like many other
    machine learning algorithms, k-means has an iterative approach: at each iteration,
    it groups the points based on their proximity to some special points called the
    **centroids** of each cluster. Every point is associated with its closest centroid.
    The algorithm then updates the position of the centroids iteratively: at each
    iteration, the groups will tend to be more and more homogenous, meaning that the
    points forming these clusters will be gradually closer and closer to each other.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see in detail the sequence of steps that make the k-means algorithm:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: the first step of the algorithm is making the initial choice
    of the centroids, one per cluster. There are different ways to make this choice.
    The simplest way is to randomly select *k* points in our dataset.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Grouping**: the algorithm now calculates the distance of each point from
    each centroid (using the Pythagorean theorem), and each point is matched with
    its closest centroid (the one lying at the smallest distance). In this way, all
    the points near a centroid are grouped together as they belong to the same cluster.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update**: the algorithm now calculates the centroid of each cluster again
    by making an average of the coordinates of all the points that belong to the cluster.
    Basically, the centroid is updated so that it matches the center of mass of the
    newly formed group.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point of the process, we return to step *b* to start a new iteration
    and repeat steps *b* and *c* as long as it is possible to improve the centroids.
    At every iteration, the clusters will converge, meaning that they will become
    increasingly more meaningful. We will stop when the update step produces no change
    in the way in which points are assigned to clusters. When this happens, the algorithm
    terminates: a stable solution has been found, and the current definition of clusters
    is returned as the resulting output. Should this convergence not take place, the
    algorithm will stop in any case once a preset number of maximum iterations is
    reached.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'This process might still look complicated but let me stress how simple the
    underlying mathematics is: random draws, averages, squares, and the square roots
    in the Pythagorean theorem are all the math we need to implement the k-means algorithm.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand how the algorithm works, let''s go through a concrete
    example and use some charts to display the evolution of the various iterations
    graphically. For the sake of simplicity, we will use a simple dataset formed only
    by two columns: by having only two columns, we can visualize the distances between
    the various points on 2-dimensional scatter plots (Cartesian diagrams).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'When we work with datasets with more than two columns (as is usually the case),
    the concept of distance becomes more difficult to visualize in our human mind.
    While, with three columns, we can still imagine the algorithm working on a 3-dimensional
    space, with 4, 5, or 10 columns, we will necessarily need to delegate the task
    to machines. Luckily, they are much more at ease than humans when navigating multidimensional
    spaces. The good news is that the basic formula for calculating distances (the
    Pythagorean theorem you found in *Figure 5.41*) stays the same: you will have
    to calculate the squares of the distances across *all* dimensions—no matter how
    many they are—and sum them across.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the real estate sector for a second, let''s imagine that we have
    a dataset describing 16 properties utilizing their price per square meter and
    their age in years (see *Figure 5.42* on the left). We want to cluster these properties
    in three homogeneous clusters. The business reason we want to create such a cluster
    is immediate: should a client show interest in any of these properties, we want
    to immediately recommend considering all other properties in the same cluster
    since they should exhibit *similar* features. This example looks naïve with 16
    properties: we wouldn''t need k-means to identify similarities with so little
    data involved. However, the beauty of k-means is that it could easily scale to
    many dimensions and properties, while our human brain would start struggling with
    a few more data points:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_42.png)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.42: Kicking k-means off: out of the 16 properties with different prices
    and ages (left), three are randomly picked and elected as initial centroids (right)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to run is the initialization: the algorithm will draw at random
    three properties, as three is the number of requested clusters (*k=3*). The algorithm
    has randomly extracted properties **C**, **G**, and **I**, as you can see on the
    right side of *Figure 5.42*. As part of the first iteration, the algorithm will
    proceed with the grouping step: first, it will use the Pythagorean theorem to
    calculate the distances between each property and each centroid and will associate
    every property to its closest centroid out of the three. Let''s follow how k-means
    proceeds at each iteration with the help of the figures. As you can see in the
    left handside of *Figure 5.43*, the grouping step has created three first cluster
    compositions, each one represented by a different color. The blue-colored properties
    (**C**, **A**, **B**, and **D**) are the closest ones to the blue centroid that
    overlaps with property **C**. The ones belonging to the red cluster (**G**, **E**,
    **F**, and **H**) are, instead, closest to the red centroid, **G**. Finally, the
    green cluster is made of the points (**I**, **L**, **M**, **N**, **O**, **P**,
    **Q**, and **R**) whose closest centroid is **I**. The next step for the algorithm
    is to update the centroids: considering the points falling into each cluster,
    it will be enough to calculate the actual center of mass of the cluster by averaging
    out the prices and the ages of the properties belonging to it. For example, let''s
    look at the green cluster: the properties forming this cluster tend to be older,
    leading the new centroid to be placed on the right side of the scatter plot. The
    centroid in the red cluster has instead moved toward the top: indeed, the properties
    associated with this cluster all have in common a higher price compared to point
    **C** (the old centroid):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_43.png)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.43: The first full iteration of k-means: with the update step, the
    centroids make a move'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can finally start the second iteration (*Figure 5.44*). Once again,
    we begin by grouping the points using the centroid we have just recalculated.
    As a consequence of this shift in the centroids, the clusters have changed, and
    some properties switched color: for instance, property **E** used to be red and
    is now blue as its closest centroid is now the blue one, and no longer the red
    one. The same applies to points **I** and **L**, which used to be green and are
    now red. It could appear that our algorithm has taken the right road as it is
    converging to a solution that makes sense: after this iteration, the clusters
    have changed in a way that makes their elements closer to each other. In the second
    step of the iteration, the algorithm will again update the centroids, taking into
    account the new compositions of the clusters. The most remarkable change is now
    in the red cluster, whose centroid has moved toward the bottom (where prices are
    lower), given the addition of properties **I** and **L** to the group:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_44.png)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.44: The second iteration of k-means: the groups make more and more
    sense'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third iteration (*Figure 5.45*), the algorithm repeats the grouping
    step, and other properties change color (for instance, **M** moves from green
    to red, and **F** becomes blue). However, something new happens: despite having
    updated the centroids, the composition of the cluster does not change at all.
    This is the sign that our algorithm has found a stable solution and can be terminated,
    returning our final cluster composition:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_45.png)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.45: The third and last iteration of k-means: no more updates are possible
    and the algorithm converges'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'This final cluster composition seems to be making a lot of sense. By looking
    at the scatter plots, we can also attempt a business interpretation of each cluster:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Blue properties (**A**, **B**, **C**, **D**, **E**, and **F**) are in the top-left
    corner of our diagram. They were all recently built and, as new properties, they
    tend to display a higher price than the rest.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Red properties (**G**, **H**, **I**, **L**, and **M**) are in the bottom central
    part of the diagram and refer to buildings built in the seventies with lower quality
    materials; hence, their price is more accessible.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the green points (**N**, **O**, **P**, **Q**, and **R**) are associated
    with older buildings, which tend to be more prestigious and come with a higher
    price tag.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters we obtained after only a handful of iterations of the k-means algorithm
    can certainly help real estate agents present convincing alternatives to potential
    buyers. Not bad for an algorithm repeating a set of simple mathematical steps.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural question that comes to mind when using k-means is: what is the right
    value of *k* or, in other words, how many clusters should I create? Even though
    there are some numerical techniques (check out the **Elbow method**, for instance)
    to infer an optimal value for *k*, the business practice of machine learning demands
    taking another, less mathematically rigorous approach. When choosing the number
    of clusters, the advice is to take a step back and think of the actual business
    utilization of the cluster definitions. The right question to ask becomes: how
    many clusters shall I create so that the result can be used in practice in my
    business case? In the example of segmenting consumers for personalizing communication,
    is it reasonable to create—let''s say—100 clusters of consumers if I can only
    afford to produce three versions of a newsletter at most? We will often use the
    business constraints for deciding a range of reasonable values of *k* and then
    pick the one that looks most interpretable. The moral of this story is that data
    analytics is a mix of art and science, and human judgment is often needed to guide
    algorithms to the right path.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving back to our tutorial flow, let''s go through a couple of considerations
    regarding "what can go wrong" when using a distance-based approach like k-means
    and how to avoid it:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers can spoil the game**. If some points in your dataset exhibit extreme
    values, they will naturally "stay apart" from the rest, making the clustering
    exercise less meaningful. For example, imagine that in our real estate case, we
    have a single property with a price ten times higher than every other property:
    this exceptional property will probably make a cluster by itself. Most times,
    we don''t want this to happen, so we remove outliers upfront. The **Numeric Outliers**
    node will do the job for us.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extreme range differences can make distance calculations unbalanced**. This
    one is easy to see through an example. Think again about the formula in *Figure
    5.41* for calculating distances: in the real estate example, it would leverage
    the differences in house prices (which are in the thousands of dollars) and the
    age differences (which, instead, vary in the area of dozens of years). The massive
    gap between the two orders of magnitude becomes even wider when you square them,
    as the formula provides. This means that the house prices will count disproportionally
    more than the age, making the latter almost meaningless. To fix this numeric disadvantage,
    we need to normalize all the measures used in k-means and reduce their scale to
    a common range (generally from zero to one) while keeping the differences across
    data points. This is what the **Normalizer** node (and its inverse companion,
    the **Denormalizer** node) will do for us in KNIME.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid these issues, remember this general advice: always remove outliers
    and normalize your data before applying k-means. With more practice and expertise,
    you might be able to "bend" these rules to meet your specific business needs at
    best, but in most cases, these two steps can only improve your clustering results,
    so they are no-brainers. Let''s now see how to apply them in KNIME.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing icon'
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image079.png) *Numeric Outliers*
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Statistics**) identifies outliers in a data table
    and manages them according to the needs. At the top of its configuration window
    (*Figure 5.46*), you can select which columns to consider in the outliers detection:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_46.png)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.46: Configuration window of Numeric Outliers node: what do you want
    to do with your extreme values?'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: In the **Outlier Treatment** panel on the bottom right, you can decide how to
    manage outliers once detected. In particular, the **Treatment option** drop-down
    menu lets you choose whether you want to **Remove outlier rows** (so as to ignore
    them in the rest of the workflow), **Remove non-outlier rows** (so you keep *only*
    the outliers and study them further), or **Replace outliers values** (by either
    assigning them a missing value status or substituting them with the closest value
    within the permitted range—you can specify your preference in the **Replacement
    strategy** menu).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'The key parameter for setting the sensitivity to use in detecting outliers
    is the **Interquartile range multiplier (k)**, which you can set on the bottom-left
    area of the configuration window. To understand how it works, have a look at the
    **box plot** shown in *Figure 5.47*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_47.png)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.47: How to interpret a box-and-whisker plot: the box in the middle
    covers the central 50% of points in a distribution. Beyond the whiskers, you find
    outliers'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'Box plots show us at a glance the key features of a numeric distribution: quartile
    values (see in the picture **Q1**, **Q2**, which is the **Median**, and **Q3**)
    tell us where we could "cut" a population of sorted numbers so as to get 25% of
    the values in each slice. Now, look at the central box, whose length is called
    **Interquartile range** (**IQR**): within this range, we will find nearly 50%
    of the values of the population—this is the *core* of our distribution. Keeping
    this in mind, outliers can be defined as the values that lie *far* from this core.
    Typically, the values that are further than 1.5 times the interquartile range
    above the third quartile or below the first quartile are considered **mild outliers**.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: They are represented as circles in *Figure 5.47*, while the limit of mild outliers
    is represented by the dashed "whiskers" you see above and below the central box
    (this is why box plots are also known as box-and-whisker plots). If you increase
    the multiplier of the interquartile range to 3.0, you find the **extreme outliers**,
    which are shown as crosses in the figure. By editing the interquartile range multiplier
    parameter in the configuration dialog, you can tell the node how "aggressive"
    it should be in detecting outliers.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s leverage our new node straight away on the CRM dataset:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Implement a **Numeric Outliers** node and connect it with the output port of
    the CSV reader. In its configuration window, deselect the column *Customer_ID*
    since we don't want to use it in our clustering. Since we are after extreme outliers,
    set `3.0` as the **Interquartile range multiplier (k)**, and select **Remove outlier
    rows** as the **Treatment option**. Finally, execute the node and have a look
    at its output ports.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first output (**Treated table**) is the cleaned-up version of the table,
    showing only 3,772 rows: this means that we removed 10% of rows as they were considered
    outliers according to some columns. We could have played with the IQR multiplier
    value and increased it to 5.0 or more, so as to focus on more extreme values and
    remove fewer rows, but for the sake of this exercise, we can carry on with this.
    The second output of the node (**Summary**, shown in *Figure 5.48*) tells us the
    number of rows regarded as outliers according to each individual column (*Basket
    Size* seems to be the one displaying more extreme values):'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, table'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_48.png)
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.48: Summary output view of the Numeric Outliers node: which columns
    are causing most of the outliers?'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s proceed with the second preparation step before applying k-means: normalize
    the data to a set range through the **Normalizer** node.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic'
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image083.png) *Normalizer*
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Manipulation > Column > Transform** in the node repository) normalizes
    all values in selected numerical columns of a dataset. In its configuration window
    (*Figure 5.49*), you first choose which columns to normalize and, then, pick a
    normalization method. The most useful one (especially indicated in conjunction
    with distance-based procedures like k-means clustering) is the **Min-Max Normalization**,
    which linearly projects the original range onto a predefined range (usually 0
    to 1, but you can manually edit the boundaries using the text boxes provided).
    With this normalization approach, the original minimum value is transformed to
    0, the maximum to 1, and everything in the middle is proportionally assigned to
    a value within the 0 to 1 range. Another popular normalization method is the **Z-Score
    Normalization (Gaussian)**, also known as **Standardization**. Using this method,
    each value is transformed into the number of standard deviations by which it is
    above or below the population''s mean. For instance, a Z-score of –3 means that
    the value is three standard deviations below the population''s average. This is
    useful when you want to assess how much your points deviate from their mean:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_49.png)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.49: Configuration window of the Normalizer node:select the columns
    to normalize and the method to apply'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'The node has two outputs: the upper output port returns the table with normalized
    values, and the bottom (the cyan square) holds the normalization model. Such a
    model can restate the original values using the **Denormalizer** node, which we
    will encounter in a few pages.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: We now have all we need to proceed and normalize our outliers-less CRM data
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick the **Normalizer** node from the repository and connect its input to the
    first output of **Numeric Outliers**. The node configuration is straightforward:
    exclude the *Customer­­_ID* column from the normalization process by double-clicking
    on it and making sure it appears on the red box on the right. The default settings
    of the normalization method work well for us: indeed, the **Min-Max Normalization**
    with a range between 0 and 1 is great for calculating distances with algorithms
    such as k-means. Finally, click on **OK** and execute the node.'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you look at the first output of the **Normalizer** node, you will notice
    how the values of the affected columns are now falling in the desired range, which
    is exactly what we needed. Now, all columns will have the same weight in calculating
    distances based on the Pythagorean theorem. We can finally move on and introduce
    the critical node of the workflow, allowing us to cluster our customers: **k-Means**.'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Diagram'
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image0851.png) *k-Means*
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Analytics > Mining > Clustering**) clusters the rows of the input
    table using the k-means algorithm. The first parameter to be set as part of its
    configuration (*Figure 5.50*) is the **Number of clusters**, which can be chosen
    by entering an integer in the textbox at the very top. You can then choose the
    method for the **Centroid initialization**, which, by default, happens by random
    draw (you can still set a static random seed to make the process repeatable),
    and the maximum number of iterations used to force termination (it is preset to
    99, which, in most cases, is good enough since k-means would naturally converge
    in fewer iterations). The last configuration step is to choose which numeric columns
    to consider when clustering, which can be done using the **Column selection**
    panel at the bottom:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_50.png)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.50: Configuration window of the k-Means node: select the columns to
    normalize and the method to apply'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Let's apply our new node to the normalized data and see what happens.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **k-Means** node and connect it downstream to the first output
    of the **Normalizer** node. We can keep its configuration simple, ensuring that
    the **Number of clusters** is set to 3 and deselecting *Customer_ID* from the
    list since we don't want to consider the column in the clustering exercise. Click
    on **OK** and then execute the node and open its main view (*Shift* + *F10*, or
    right-click and then select **Execute and Open Views...**).
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main view of the **k-Means** node (right-click on the node and then select
    **View: Cluster View** to make it appear if needed) will look similar to what
    you find in *Figure 5.51*:'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_51.png)
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.51: Summary view of the k-Means node: we can start seeing what the
    three clusters look like'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This summary view is already telling us a lot: k-means segmented our customer
    base into three different groups of 830, 1,126, and 1,816 customers, respectively
    (see the **coverage** labels in the figure). If you open the different clusters
    (click on the **+** button on the left), you find a numeric description of the
    three centroids. According to what you see in *Figure 5.51*, for example, the
    first cluster (generically named **cluster_0** by KNIME) shows the smallest *Basket
    Size* of the three and the highest *Unique Products*. If you open the first output
    port of the node (right-click on the node and then select **Labeled input**),
    you will see that every row has been assigned to one of the three clusters, as
    indicated in the additional *Cluster* column (see *Figure 5.52*):'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Table'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_52.png)
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.52: Output of the k-Means node: every customer—whether they like it
    or not—gets assigned to a cluster'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As aligned with our business partner, the CRM manager, we need to go one step
    ahead and build a couple of visualizations to simplify the process of interpreting
    our clustering results.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Before doing that, we realize that our values are still normalized and forced
    to fall within the 0 to 1 range. To make our visuals easier to interpret, we would
    prefer to come back to the original scales instead. To do so, we can revert the
    normalization by leveraging the **Denormalizer** node.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic'
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image087.png) *Denormalizer*
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Manipulation > Column > Transform**) brings the values in a dataset
    back to their original range. It requires two input connections: the first one
    is the model generated by the previous **Normalizer** node, which carries a description
    of the normalization method and parameters. The second input is the normalized
    table to be denormalized. The node does not require any configuration.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Implement the **Denormalizer** node and set up the wiring. The cyan output of
    the **Normalizer** node should be connected to the first input of the **Denormalizer
    node.** The first output of the **k-Means** node should be connected, instead,
    to the second input port of the **Denormalizer** node. You can have a sneak view
    of the final workflow in *Figure 5.57* to see how to get the connections right.
    After executing the node, you can see how the values have been reverted to their
    original range.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To build the visuals, we will need three more nodes. The first one (**Color
    Manager**) is required for assigning colors to the various rows of the dataset
    (according to the cluster), while the other two (**Scatter Matrix (local)** and
    **Conditional Box Plot**) will generate a couple of nice charts.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing icon'
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/Image73183.png) *Color Manager*
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Views > Property**) assigns colors to each row of a dataset. Its
    configuration window (*Figure 5.53*) asks you to select two things. First, you
    specify the nominal column used to evaluate what color to assign: every possible
    value associated with that column will correspond to a specific color. Second,
    you need to select the color set to adopt. On top of the three default color sets,
    you can also manually define which color to assign to each possible value of the
    nominal column. To do so, you will have to select **Custom** in the **Palettes**
    tab and then use one of the tabs on the right (such as **Swatches**, **RGB**,
    and **CMYK**) to pick the right color for each nominal value manually:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_53.png)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.53: Configuration of the Color Manager node: you can pick which color
    to assign to which value of the nominal column of your choice'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Add a **Color Manager** node and connect it to the output of the **Denormalizer**.
    Confirm the *Cluster* column in the drop-down menu at the top, and then select
    the color set of your choice. In the specific example of *Figure 5.53*, a custom
    palette has been manually created so that blue, orange, and green could be assigned
    to the three clusters.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the colors are set, it's finally time to pull together the first chart
    with the help of a new node.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A close-up of a logo'
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated with low confidence](img/image084.png) *Scatter
    Matrix (local)*
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: This node (**Views > Local (Swing)**) generates a matrix of scatter plots, displaying
    multiple combinations of variables in a single view. The node does not require
    any configuration, but you can optionally increase the maximum number of points
    that will be plotted.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the **Scatter Matrix (local)** node after **Color Manager**. Execute
    and open its main view (*F10* after selecting the node). From the **Column Selection**
    tab at the bottom, you can choose which variables to display. In our case, let''s
    make sure we have only *Average Price*, *Basket Size*, and *Unique Products* selected
    on the right: you will end up with a visual similar to *Figure 5.54*:![Graphical
    user interface, application'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_54.png)
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.54: Output view of the Scatter Matrix (local) node: your customers
    have become colored points. By looking at how the cloud of dots is scattered,
    you can interpret what each cluster is all about'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The scatter matrix we just obtained renders the result of the clustering in
    a more human-friendly way. As we look at it together with the CRM manager, we
    notice some initial clear patterns. For example, look at the chart at the top-right
    corner of *Figure 5.54*, which shows *Average Price* on the vertical axis and
    *Unique Products* on the horizontal axis. The blue cluster (cluster_0) clearly
    dominates the right-hand side of the chart, confirming that this is the segment
    of consumers that tend to try a more diverse set of products (high values of *Unique
    Products*). At the same time, the orange cluster (cluster_1) has customers that
    seem to go for less unique products and lower prices. Instead, the green cluster
    (cluster_2) includes those willing to pay more premium prices when shopping at
    our website. This is all starting to make sense, and the visual is already a big
    help in understanding how our clustering worked.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add one last visual to clarify even further the composition of our segments:
    meet the **Conditional Box Plot** node.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text'
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Description automatically generated](img/image086.png) *Conditional Box Plot*
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'This node (**Views > JavaScript**) produces an interactive view with multiple
    box plots, one for each value in a given categorical column. Such a view enables
    the parallel comparison of distributions. Its configuration window (*Figure 5.55*)
    requires selecting the **Category Column** to be used for differentiating parallel
    box plots and the choice of the numeric columns whose distribution will be visualized:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_55.png)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.55: Configuration dialog of the Conditional Box Plot node: which distributions
    are you interested in comparing between?'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag and drop the **Conditional Box Plot** node onto the workflow editor and
    connect it to the output port of the **Denormalizer** node. Select *Cluster* as
    **Category Column** and ensure that only **Average Price**, **Basket Size**, and
    **Unique Products** are on the right of the column selector placed at the center
    of the dialog. Click on **OK** and then press *Shift* + *F10* to execute it and
    open its main view. In the interactive window that appears, you can swap which
    distribution to visualize by operating on the **Selected Column** drop-down menu:
    you can find this selector by clicking on the icon at the far top-right of the
    interactive window.'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output views of the **Conditional Box Plot** node (*Figure 5.56*) clarify
    even better the essential features of each cluster. The k-means algorithm was
    able to produce three homogeneous clusters with peculiar and differentiating characteristics.
    The box plots are great at showing such differences. As an example, take the third
    plot in the figure, which refers to *Unique Products*. The blue cluster dominates
    when it comes to this measure: the median number of unique products purchased
    by customers belonging to this segment is 32, while for the others it is near
    10\. The lack of visual overlap in height between the blue box and the other two
    means this difference is meaningful. On the other hand, the orange and the green
    clusters seem to be quite similar in terms of unique products, as the boxes are
    almost coinciding:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, box and whisker chart'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_56.png)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.56: Outputs of the Conditional Box Plot node: you can readily appreciate
    the differences in the distributions across clusters'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now sit together with the CRM manager and, having the scatter matrix
    and the conditional box plots at hand, we can finally describe each customer segment
    and give a business-oriented interpretation of their meaning:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: The blue cluster includes those **curious customers** who are willing to try
    different products. In our communication with this segment, we can give disproportionate
    space to the "new arrivals" and intrigue them with an ample selection of products
    they haven't tried yet.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The orange cluster possibly comprises **small retailers** who buy "in bulk"
    from our website to resell their shops. They tend to buy relatively few products
    but in large quantities. We can offer them quantity discounts and regularly communicate
    the list of best-selling products, hopefully leading them to add our best-selling
    articles to their assortment for mutual business growth.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The green cluster is made up of our **high-value customers**, who systematically
    put quality ahead of price in their shopping choices. Therefore, when communicating
    with them, we should advertise the premium end of the products portfolio and focus
    on topics such as the quality and the safety of our assortment, deprioritizing
    price-cut offers, and other types of promotional levers.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using only 8 KNIME nodes (see the full workflow in *Figure 5.57*), we came
    up with a simple segmentation of customers and a first proposition of how to drive
    the most value when personalizing their experience. By uniting the business expertise
    of our partners (the CRM managers in this case) with the power of data and algorithms,
    such as k-means, we can make the magic happen!
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_05_57.png)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.57: The full workflow for segmenting consumers using clustering'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you touched on the serious potential behind data analytics
    and machine learning with your own hands. You have solved three real-world problems
    by putting data and algorithms at work.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: In the first tutorial, you managed to predict with a decent level of accuracy
    the rental price of properties, collecting, in the process, a few interesting
    insights into real estate price formation. You have now acquired a proven methodology,
    based on the linear regression model, that you can replicate on many business
    cases where you have to predict numeric quantities.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: In the second tutorial, you entered the fascinating world of classification
    and propensity modeling, experiencing firsthand the game-changing role of data
    analytics in marketing. You were able to put together a couple of classification
    models through which you met multiple business needs. First, you were able to
    reveal the "unwritten rules" that make a product generally attractive to customers
    by building and interpreting a decision tree model. Then, you built a random forest
    model that proved effective in anticipating the level of propensity of individual
    bank customers. Lastly, you managed to estimate the possible ROI of further marketing
    campaigns, unlocking serious value creation opportunities for a business. Also,
    in this case, you gained a series of general-purpose techniques that you can easily
    reapply in your own work every time you need to predict anything of business relevancy.
    By going through the tutorial, you also experienced the "back and forth" iterations
    needed to fine-tune machine learning models to fit your business needs.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'In our third tutorial, you experienced the power of unsupervised learning.
    You were able to put together a meaningful and straightforward customer segmentation
    that can be used to design personalized communication strategies and maximize
    the overall customer value. With this new algorithm, k-means, in your backpack,
    you can potentially cluster anything: stores, products, contracts, defects, events,
    virtually any business entity that can benefit from the algorithmic tidying that
    comes with clustering. Think about the value you can create by applying this new
    concept to the work items you deal with on a daily basis. In the process of learning
    k-means, we also got acquainted with the fundamental statistical concept of outliers
    and saw how to spot and manage them systematically.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on and learn how we can make our data accessible to our business
    partners through self-service dashboards. It''s time to meet our next travel companion
    in the data analytics journey: Power BI.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
