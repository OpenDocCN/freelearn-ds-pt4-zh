<html><head></head><body>
		<div id="_idContainer387">
			<h1 class="chapterNumber">9</h1>
			<h1 id="_idParaDest-151" class="chapterTitle" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor153"/>Extending Your Toolbox</h1>
			<p class="normal">Getting up to speed with the latest tools and novel techniques in data analytics is surely a never-ending process. In this field, you need to be ready to update and expand your knowledge continuously. So far in this book, we have acquired a number of vital, application-agnostic data techniques such as data cleaning and modeling, machine learning, data visualization, and storytelling. We have also learned how to apply them through a solid application toolbox made of KNIME and Power BI. As we approach the end of our journey, we should see what else is available and how to integrate all the applications together to make the best out of all of them.</p>
			<p class="normal">In particular, we will cover the following questions:</p>
			<ul>
				<li class="bullet">What is Tableau, and how can I use it for visualization and storytelling?</li>
				<li class="bullet">What is Python, and how do I get started with it?</li>
				<li class="bullet">How can I boost my workflows by integrating Python or any other code?</li>
				<li class="bullet">How can I use KNIME extension packages to add functionalities?</li>
				<li class="bullet">What is automated machine learning, and what should I expect from its future business applications?</li>
			</ul>
			<p class="normal">This chapter is not meant to make you an autonomous user of Python, Tableau, and other tools. There is no need for it at this stage. Your initial toolbox (KNIME and Power BI) covers your essential analytical needs well. The point of this chapter is to show you what <em class="italic">else</em> is available and make you curious and excited about the many directions you can take to expand your abilities in data analytics from now on. </p>
			<p class="normal">We will first look at Tableau, another data visualization tool: by means of a simple example, we will see how what we learned for Power BI can be easily applied in Tableau as well. Then, we will learn about Python and get a friendly introduction to how it is used in analytics. We will see how to integrate Python code into KNIME through extension packages. Lastly, we will learn about automated machine learning and see the concept in action with the help of the H2O.ai platform. All the tools in this chapter are either open source or provide free trial options so that you have the opportunity to put your hands on them and evaluate by yourself how they can help you and your business.</p>
			<h1 id="_idParaDest-152" class="title"><a id="_idTextAnchor154"/>Getting started with Tableau</h1>
			<p class="normal">Founded as a Stanford University<a id="_idIndexMarker801"/> spin-off, Tableau has pioneered in the data visualization arena for nearly two decades and is now regarded as one of the leading business intelligence platforms. Its straightforward drag-and-drop user interface, the integration with many data platforms, and its highly customizable, high-quality chart types have made Tableau very popular among business professionals, analysts, and data journalists.</p>
			<p class="normal">Similar to Power BI, Tableau comes in different versions. In this chapter, we will use <strong class="keyword">Tableau Public</strong>: this is a free desktop<a id="_idIndexMarker802"/> application (you can download it from <a href="http://public.tableau.com"><span class="url">public.tableau.com</span></a>) that has nearly all the functionalities<a id="_idIndexMarker803"/> included in the full version (called <strong class="keyword">Desktop Professional</strong>) but also a couple of important<a id="_idIndexMarker804"/> limitations. First, it relies on local data, so you cannot connect to remote data sources. Additionally, the public version lets you save your result solely on the public Tableau server, which is open to everyone: this means you cannot save your work on your computer. Given its lack of privacy protection, Tableau Public is not viable for day-to-day business needs, but we can still use it for exploring Tableau functionalities and comparing them with Power BI's.</p>
			<div>
				<div id="_idContainer354" class="note">
					<p class="Information-Box--PACKT-">You can publish dashboards<a id="_idIndexMarker805"/> online using the cloud-based service called <strong class="keyword">Tableau Server</strong>. You can also design dashboards just using your browser and avoid the installation of new software. To do so, you will need to register with the Tableau Public website mentioned above, go to your profile, and click on <strong class="screenText">Create a Viz</strong>. The user interface on the web app is very similar to the one you could find in the desktop application, which we will use in this chapter.</p>
				</div>
			</div>
			<p class="normal">Following the spirit we kept throughout the book, let's explore Tableau through practice by getting our hands on it. In this short tutorial, we will create a couple of visualizations based on the sales database we leveraged in <em class="chapterRef">Chapter 6</em>, <em class="italic">Getting Started with Power BI</em>: a treemap to display the relative weight of categories and a line chart that shows the evolution of sales over time. This time, the three tables (Transactions, ProductMD, and CustomerMD) are saved as separate sheets in one single Excel file (<code class="Code-In-Text--PACKT-">SalesDashboardTableau.xlsx</code>):</p>
			<ol>
				<li class="numbered">Open Tableau Public. In the first screen (which looks similar to what you see in <em class="italic">Figure 9.1</em>), click on <strong class="screenText">Microsoft Excel</strong> on the left and open the file containing our data:<figure class="mediaobject"><img src="image/B17125_09_01.png" alt=""/></figure><p class="packt_figref">Figure 9.1: The initial screen of Tableau Public: select the type of files you want to use on the left</p></li>
				<li class="numbered">In the next<a id="_idIndexMarker806"/> window, called the <strong class="keyword">Data Source</strong> screen (<em class="italic">Figure 9.2</em>), you will find on the left the three<a id="_idIndexMarker807"/> sheets included in the Excel file we just opened. By dragging them on the blank area on the top left, you can build an entity-relationship diagram that defines the dashboard's underlying data model:<figure class="mediaobject"><img src="image/B17125_09_02.png" alt="Graphical user interface, table&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.2: The Data Source screen in Tableau: drag and drop your source tables and build the data model</p><p class="bullet-para">To do so, we need to follow the right order. First, drag the <strong class="keyword">Transactions</strong> table and wait a few seconds<a id="_idIndexMarker808"/> for the data to load and the preview to appear on the bottom. Then, drag the Customer Master Data (<strong class="screenText">CustomerMD</strong>) table and drop it on the right of the <strong class="screenText">Transactions</strong> box: the line between the two indicates that Tableau will join the two tables. As you release the mouse button, the <strong class="screenText">Edit Relationship</strong> window will appear (<em class="italic">Figure 9.3</em>): Tableau has successfully identified <em class="italic">Customer ID </em>as the column to be used for matching rows. We can confirm the relationship by closing the window, without making any changes to the default settings. Once this is done, it's time to drag also the Product Master Data (<strong class="screenText">ProductMD</strong>) table to the right of <strong class="screenText">Transactions</strong>, making sure that the resulting connections look similar to what you had in <em class="italic">Figure 9.2</em>. Finally, confirm <em class="italic">StockCode</em> as the matching column by closing the window that pops up: your data model is good to go:</p><figure class="mediaobject"><img src="image/B17125_09_03.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.3: Edit Relationship window: select one or more matching conditions for your joins</p><p class="normal">To proceed further, click on the <strong class="screenText">Sheet 1</strong> tab on the bottom left: you will land<a id="_idIndexMarker809"/> in the main interface of Tableau, called <strong class="keyword">Workspace</strong> (<em class="italic">Figure 9.4</em>). Let's explore the four fundamental sections<a id="_idIndexMarker810"/> in the workspace:</p><p class="alphabetic-l2">A.	The <strong class="keyword">Data Panel</strong> is where you find<a id="_idIndexMarker811"/> all your data columns, organized by table. From here, you will drag the quantities you wish to use<a id="_idIndexMarker812"/> in a visualization or create calculated fields. This is similar to the <strong class="keyword">Fields </strong>section in Power BI.</p><p class="alphabetic-l2">B.	The <strong class="keyword">Visualization View</strong> is where you can build<a id="_idIndexMarker813"/> your visuals. From here, you can connect the fields available in the data panel with the visual attributes<a id="_idIndexMarker814"/> in a visual, which in Tableau are called <strong class="keyword">Shelves</strong>. For example, the height of bars, the position of points, their color, size, and the text appearing on the labels are all controlled through the <strong class="screenText">Rows</strong>, <strong class="screenText">Columns</strong> and <strong class="screenText">Marks</strong> shelves that you find in the visualization view (their usage will become clearer as we go through the tutorial). From the same view, you can also implement pagination and split one visual into multiple pages, each one showing different values for a given column (that you have to drop in the <strong class="screenText">Pages </strong>shelf). Additionally, from the visualization view, you can decide which fields to use for limiting the data to be visualized (<strong class="screenText">Filters</strong> shelf).</p><p class="alphabetic-l2">C.	The <strong class="keyword">Show Me Panel</strong> is where you can select<a id="_idIndexMarker815"/> the type of chart to use, such as line charts, treemaps, histograms, or geographic maps.</p><p class="alphabetic-l2">D.	The bar at the bottom<a id="_idIndexMarker816"/> lets you add <strong class="keyword">Sheet tabs </strong>and navigate through<a id="_idIndexMarker817"/> them. In Tableau, every sheet can<a id="_idIndexMarker818"/> be one among three different types: a Worksheet (a single chart), a Dashboard (a composition of multiple<a id="_idIndexMarker819"/> charts), or a Story (a controlled sequence of worksheets<a id="_idIndexMarker820"/> or dashboards that progress to convey a data story, as you learned to do in <em class="chapterRef">Chapter 8</em>, <em class="italic">Telling Stories with Data</em>):</p><figure class="mediaobject"><img src="image/B17125_09_04.png" alt=""/></figure><p class="packt_figref">Figure 9.4: The Workspace screen in Tableau: drag and drop columns to the visualization features, pick chart types, and move across visuals, dashboards, and stories</p><p class="normal">Now that we are acquainted with the workspace interface, we can build our first chart to show the relative size of each category and subcategory in terms of sales. However, we do not yet have a column carrying the revenues generated by each transaction and, so, we need to first add a calculated field that does the math for us.</p></li>
				<li class="numbered">Right click on the <em class="italic">Price</em> column<a id="_idIndexMarker821"/> available in the <strong class="keyword">Data Panel</strong> and then select <strong class="screenText">Create</strong> | <strong class="screenText">Calculated Field…</strong> as shown in <em class="italic">Figure 9.5</em>:<figure class="mediaobject"><img src="image/B17125_09_05.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.5: Creating a calculated field in Tableau: we can add math formulas and generate new quantities to be visualized</p><p class="bullet-para">You will be prompted<a id="_idIndexMarker822"/> with a dialog (<em class="italic">Figure 9.6</em>) that lets you enter the name of the new column (in the text box on the top left—we are going for <code class="Code-In-Text--PACKT-">Sales</code> in this case) and the mathematical expression to be used, where fields are indicated through square brackets. Click on the little arrow showing on the far right of the window. You will open an extra panel that, similarly<a id="_idIndexMarker823"/> to what we had in the <strong class="keyword">Math Formula</strong> node in KNIME, provides many logical and mathematical functions to be used, together with a textual description on the right. In our case, the expression <code class="Code-In-Text--PACKT-">[Price]*[Quantity]</code> will do: write it in the box on the left (Tableau will help by attempting to autocomplete the names of the columns as you type them) and then click on <strong class="screenText">OK</strong> to move on. The new<a id="_idIndexMarker824"/> calculated field will appear in the <strong class="keyword">Data Panel </strong>and can now be used as we wish:</p><figure class="mediaobject"><img src="image/B17125_09_06.png" alt=""/></figure><p class="packt_figref">Figure 9.6: Defining a calculated field: add the math expression that combines columns as you need</p></li>
				<li class="numbered">We now have all the ingredients<a id="_idIndexMarker825"/> to bake our first visual: having learned in <em class="chapterRef">Chapter 7, </em><em class="italic">Visualizing Data Effectively</em><em class="chapterRef"> </em><em class="italic">BI</em>, how to resist the sweet temptation of using pie charts, we want to build a nice treemap that shows the relative magnitude of sales by category and subcategory. Building a visual in Tableau requires dragging the data fields of interest (the columns listed in the data panel on the left) and dropping them to some visual attributes (the shelves appearing in the visualization view). In the case of the treemap, you can follow the arrows in <em class="italic">Figure 9.7</em> as a guide. Start by dropping <em class="italic">Category</em> (first) and <em class="italic">Subcategory</em> (second) in the box called <strong class="screenText">Text</strong> (it will be automatically renamed to <strong class="screenText">Label</strong> later, when the chart type is established). Then, take the newly created <em class="italic">Sales</em> field and drop it in the <strong class="screenText">Size</strong> box. Lastly, get the <em class="italic">Category</em> field to also control the color of the areas by dropping it to the <strong class="screenText">Color</strong> box:<figure class="mediaobject"><img src="image/B17125_09_07.png" alt=""/></figure><p class="packt_figref">Figure 9.7: Building a treemap in Tableau: take the fields on the left and drop them to the right box in the Marks shelf</p><div id="_idContainer362" class="packt_tip"><p class="Tip--PACKT-">After you build a visual in Tableau, you can easily explore alternative versions where different chart types are applied to the same data. To try this, click on the various boxes you find in the <strong class="screenText">Show Me</strong> panel on the right. The chart types that cannot be rendered given the current data are grayed out, and you cannot select them. Tableau might also recommend one specific chart type and highlight its border in orange. I recommend you choose the chart type by always keeping the business question in mind, as you learned in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Data Effectively</em>.</p></div><p class="normal">Well done! With just four drag<a id="_idIndexMarker826"/> and drops, you have built your first visual in Tableau. We can easily see how the "Home" <em class="italic">Category</em> (and—within it—the "Kitchen" <em class="italic">Subcategory</em>) generates the biggest revenue bucket in our business.</p><p class="normal">Let's move to the second business question: this time, we want to focus our message on the trend of <em class="italic">Sales</em> by <em class="italic">Category</em>. We decide to put together a line chart as it is our natural chart type for communicating insights related to the evolution of quantities over time.</p></li>
				<li class="numbered">To create a new visual, click on the first small <strong class="screenText">+</strong> icon on the right of the <strong class="screenText">Sheet 1</strong> tab at the bottom (alternatively, you can click on <strong class="screenText">Worksheet | New Worksheet </strong>from the top menu, or just press <em class="italic">CTRL</em> + <em class="italic">M</em>). By doing this, a blank <strong class="screenText">Sheet 2 </strong>appears: this is the space to draw our line chart on.</li>
				<li class="numbered">The first field<a id="_idIndexMarker827"/> to drag is <em class="italic">Invoice time</em>: drop it to the <strong class="screenText">Columns</strong> shelf. Given its type (carrying the date and time of each transaction), we need to tell Tableau at which level of granularity (years, quarters, months, weeks, and so on) we want to aggregate. In this case, we want to visualize one data point for every month of transactions: right-click on the field as it appears on the shelf, and then select the second <strong class="screenText">Month</strong> entry in the pop-up menu (use <em class="italic">Figure 9.8 </em>as a guide):<figure class="mediaobject"><img src="image/B17125_09_08.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.8: Using a date field for a line chart: right-click on the field and select the time granularity you need. In this case, we go for months</p></li>
				<li class="numbered">Let's move on to implementing the other fields we want to use, following the drag and drops you see in <em class="italic">Figure 9.9</em>. Drop <em class="italic">Sales</em> to the <strong class="screenText">Rows</strong> shelf (by default, the aggregation by sum is applied, but you can change it easily by right-clicking on the field and picking the proper function in the <strong class="screenText">Measure</strong> submenu). The following field to move is <em class="italic">Category</em>. Drop it twice: first to the <strong class="screenText">Color</strong> box (so we differentiate lines by <em class="italic">Category</em>) and then to the <strong class="screenText">Label</strong> box (so we show a direct label for each line):<figure class="mediaobject"><img src="image/B17125_09_09.png" alt=""/></figure><p class="packt_figref">Figure 9.9: Building a line chart in Tableau: the columns and rows shelves control the x- and the y-axis, respectively</p></li>
				<li class="numbered">Before composing<a id="_idIndexMarker828"/> our dashboard, we can edit the sheet names at the bottom by right-clicking on each tab and selecting <strong class="screenText">Rename</strong>. We can go for something more meaningful like <code class="Code-In-Text--PACKT-">Business by category</code> and <code class="Code-In-Text--PACKT-">Sales trend</code> for the first and the second visualizations, respectively.</li>
				<li class="numbered">It's time to build the dashboard by combining the two visuals. Click on the second <strong class="screenText">+</strong> icon in the <strong class="screenText">Sheets</strong> tab at the bottom or select <strong class="screenText">Dashboard | New Dashboard</strong> from the top menu.</li>
				<li class="numbered">On the left of the new dashboard view, you will find a list of the two worksheets we built, one for each chart we created. To create a dashboard, you will just need to drag and drop the worksheets in the blank area on the right, as you can see in <em class="italic">Figure 9.10</em>:<figure class="mediaobject"><img src="image/B17125_09_10.png" alt="Chart&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.10: Building a dashboard in Tableau: drag and drop the visualizations to their positions</p><div id="_idContainer366" class="note"><p class="Information-Box--PACKT-">On the bottom left of the dashboard view, you find several icons that you can drag and drop to your dashboard to add additional objects, such as text labels, images, web pages, or extensions. Check it out.</p></div></li>
				<li class="numbered">Before publishing<a id="_idIndexMarker829"/> the dashboard, let's configure the interactions across visuals. If you click on any empty space of the first visual, you will select it, and its borders get highlighted (letting you adjust its shape, if you wish). Also, a few icons will appear on the top right of the selected visual, as you can see in <em class="italic">Figure 9.11</em>. If you click on the filter icon, as the arrow in the picture indicates, you will set that visual as a filter for every other visual in the dashboard. You can quickly test that this works properly: if you click on any subcategory in the treemap (you can select more than one at once by keeping the <em class="italic">CTRL</em> key pressed), you will notice that the line chart updates accordingly, showing only the trend of the selection portion of business. This is exactly what we were after:<figure class="mediaobject"><img src="image/B17125_09_11.png" alt="Chart, treemap chart&#10;&#10;Description automatically generated"/></figure><p class="packt_figref">Figure 9.11: Use a visual for filtering subsequent charts in a Tableau dashboard.Click on the filter icon in the top right</p></li>
				<li class="numbered">We can now publish<a id="_idIndexMarker830"/> our work on the server: just open the <strong class="screenText">File</strong> menu at the top and then click on <strong class="screenText">Save to Tableau Public…</strong>. Next, pick a name (I went for <code class="Code-In-Text--PACKT-">Ecom Sales Dashboard</code>), click on <strong class="screenText">OK</strong>, and wait for a few seconds for the data to upload. Your browser will open up and show your published dashboard<a id="_idIndexMarker831"/> in all its grace (you can check my version out at <a href="http://tiny.cc/ecomdashboard"><span class="url">tiny.cc/ecomdashboard</span></a> and in <em class="italic">Figure 9.12</em>):<figure class="mediaobject"><img src="image/B17125_09_12.png" alt="Chart, treemap chart&#10;&#10;Description automatically generated"/></figure></li>
			</ol>
			<p class="packt_figref">Figure 9.12: The dashboard published on the Tableau Public server: let others access your work and interact with it</p>
			<p class="normal">In the last few pages, we ran through<a id="_idIndexMarker832"/> the fundamental functionalities of Tableau: we have learned how to load data, combine it in a simple data model, created calculated fields, and built visuals and combined them in an interactive dashboard. I am sure you noticed the extensive similarities between Tableau and what we have learned on Power BI. We could carry on in the exploration of other business intelligence platforms such as Qlik, MicroStrategy, and TIBCO Spotfire, to mention a few. The (exciting) reality is that the <em class="italic">bulk</em> of how they work is very similar, and the last few chapters have equipped you with all you need to get started and create value for your business, irrespective of the tool you used.</p>
			<p class="normal">Let's now move to the next "expansion" phase of our data analytics toolbox with Python.</p>
			<h1 id="_idParaDest-153" class="title"><a id="_idTextAnchor155"/>Python for data analytics</h1>
			<p class="normal">Python is an increasingly popular high-level<a id="_idIndexMarker833"/> programming language that is particularly well suited for data analytics<a id="_idIndexMarker834"/> and machine learning applications. The ample availability of analytics-related libraries and its easy-to-learn syntax make it the preferred choice for many data science practitioners.</p>
			<div>
				<div id="_idContainer369" class="note">
					<p class="Information-Box--PACKT-">The story behind Python's name<a id="_idIndexMarker835"/> has nothing to do with snakes. Its creator, Dutch programmer Guido van Rossum, was a big fan of the 1970s BBC comedy series "Monty Python's Flying Circus." So he picked Python as the name of the project to honor the irreverent genius of the British comedy troupe running that show.</p>
				</div>
			</div>
			<p class="normal">As this book focuses on visual programming, we will not go through any thorough explanation of coding principles. Instead, the purpose of this section is to let you see Python in action on a familiar problem and get some perspective on how it can be used in our everyday work. We will first go through a script that repeats the exact same regression tutorial we saw in <em class="chapterRef">Chapter 5</em>, <em class="italic">Applying Machine Learning at Work</em>. Then, we will see how Python can smoothly integrate with KNIME to make the best out of the two complementary approaches to programming for analytics.</p>
			<h2 id="_idParaDest-154" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor156"/>A gentle introduction to the Python language</h2>
			<p class="normal">To use Python, you can either install a development<a id="_idIndexMarker836"/> platform like Anaconda (we will do this later) or leverage a web-based interface such as Colab. Google Colab (short for Colaboratory) is a free cloud service that lets you write and run Python code without any setup<a id="_idIndexMarker837"/> being needed: you can access it at <a href="http://colab.research.google.com"><span class="url">colab.research.google.com</span></a>.</p>
			<p class="normal">As you can see in <em class="italic">Figure 9.13</em>, the user interface of Colab is an interactive web page where you can add text and code and then run it, line by line:</p>
			<figure class="mediaobject"><img src="image/B17125_09_13.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.13: The welcome screen of Google Colab: get some Python going without installing any software</p>
			<p class="normal">To simplify the comparison with KNIME, let's use Colab on the same Rome housing business<a id="_idIndexMarker838"/> case we encountered in <em class="chapterRef">Chapter 5</em>, <em class="italic">Applying Machine Learning at Work</em>. As a reminder, the objective is to predict rental prices by applying the linear regression learning algorithm to the database of historical rental agreements. You can follow step by step the full Colab script by connecting to <a id="_idTextAnchor157"/><a href="https://colab.research.google.com/github/laibniz/AnalyticsMadeEasy/blob/main/Rome_housing.ipynb"><span class="url">tiny.cc/romecolab</span></a>.</p>
			<p class="normal">Let's go through the code and, for each portion, understand what is going on:</p>
			<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> statsmodels.formula.api <span class="hljs-keyword">as</span> smf
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> r2_score, mean_squared_error
</code></pre>
			<p class="normal">The first step is to import some useful libraries into the Python environment. This will make a few extra functionalities (like loading Excel files and calculating a linear regression) available for us to leverage in our code. In particular, in the preceding code, we use a few <code class="Code-In-Text--PACKT-">import</code> statements to include some of the most popular Python libraries used for data<a id="_idIndexMarker839"/> analytics, namely: <strong class="keyword">Pandas </strong>for data manipulation, <strong class="keyword">NumPy</strong> for numerical routines<a id="_idIndexMarker840"/> and array calculations, <strong class="keyword">Statsmodels</strong> for hardcore<a id="_idIndexMarker841"/> statistics, and <strong class="keyword">Scikit-learn</strong> (<code class="Code-In-Text--PACKT-">sklearn</code> in the code) for machine<a id="_idIndexMarker842"/> learning:</p>
			<pre class="programlisting code"><code class="hljs-code">full_data = pd.read_excel(<span class="hljs-string">"RomeHousing-History.xlsx"</span>) 
full_data.head()
</code></pre>
			<p class="normal">As a next step, we read the data<a id="_idIndexMarker843"/> stored in an Excel file by using the <code class="Code-In-Text--PACKT-">pd.read_excel()</code> function and assign its content to the <code class="Code-In-Text--PACKT-">full_data</code> variable. We can then check the imported data by visualizing its top five rows, using the function <code class="Code-In-Text--PACKT-">head()</code>, producing the output shown in <em class="italic">Figure 9.14</em>:</p>
			<figure class="mediaobject"><img src="image/B17125_09_14.png" alt="Table&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.14: The output of the head() function as displayed in Colab:a useful peek into the top five rows in our dataset</p>
			<p class="normal">The data we obtain is exactly what we encountered at the beginning of the first tutorial in <em class="chapterRef">Chapter 5</em>, <em class="italic">Applying Machine Learning at Work</em> (see the first rows in <em class="italic">Figure 5.2</em>). We can move on and proceed with the first step of every supervised machine learning procedure, that is partitioning:</p>
			<pre class="programlisting code"><code class="hljs-code">train_set, test_set = train_test_split(full_data,test_size=<span class="hljs-number">0.3</span>)
</code></pre>
			<p class="normal">With the help of the <code class="Code-In-Text--PACKT-">train_test_split()</code> function, we apply a random sampling to our full data and obtain the training and the test set (which we set to be 30% of the total), which are stored in the <code class="Code-In-Text--PACKT-">train_set</code> and <code class="Code-In-Text--PACKT-">test_set</code> variables. This line<a id="_idIndexMarker844"/> of Python code implements what the <strong class="keyword">Partitioning </strong>node did for us in KNIME. We now have all we need to learn the model using the training set:</p>
			<pre class="programlisting code"><code class="hljs-code">model = smf.ols(formula=<span class="hljs-string">'Rent ~ Rooms + Surface + \</span>
<span class="hljs-string">                         Elevator + Floor_number + \</span>
<span class="hljs-string">                         C(Neighborhood) + C(Property_type) + \</span>
<span class="hljs-string">                         C(Floor_type)'</span>,data=train_set).fit()
print(model.summary())
</code></pre>
			<p class="normal">Leveraging the function <code class="Code-In-Text--PACKT-">smf.ols()</code>, this code portion trains an Ordinary Least Square regression model (OLS, which we encountered<a id="_idIndexMarker845"/> in <em class="chapterRef">Chapter 5</em>, <em class="italic">Applying Machine Learning at Work</em>) using the <code class="Code-In-Text--PACKT-">train_set</code> variable<a id="_idIndexMarker846"/> as an input. The output model is stored in an object called <code class="Code-In-Text--PACKT-">model</code>. As we train the model, we can edit the formula string you see in the code (<code class="Code-In-Text--PACKT-">Rent ~ Rooms + Surface + ...</code>) to select which column is the target (in our case <code class="Code-In-Text--PACKT-">Rent</code>, which appears before the <code class="Code-In-Text--PACKT-">~</code> sign) and which other columns should be used as predictors (the ones that go after the <code class="Code-In-Text--PACKT-">~</code> symbol, separated by a <code class="Code-In-Text--PACKT-">+</code> sign) in the linear regression. Categorical columns need to be encapsulated by the <code class="Code-In-Text--PACKT-">C()</code> function (like in <code class="Code-In-Text--PACKT-">C(Neighborhood)</code>): by doing so, Python converts them into multiple numerical columns (dummy variables) that are compatible with a linear regression model. The definition of the linear regression formula and the conversion of the nominal variable<a id="_idIndexMarker847"/> were done "under the hood" by the <strong class="keyword">Linear Regression Learner</strong> node in KNIME, while in Python, they need to be specified in the code. Finally, the <code class="Code-In-Text--PACKT-">summary()</code> function summarizes the regression results, including coefficients and p-values for each feature. If you compare the output obtained in Python (<em class="italic">Figure 9.15</em>) with the one obtained as an output of the <strong class="keyword">Linear Regression Learner</strong> node in KNIME (<em class="italic">Figure 5.9</em>), you will find<a id="_idIndexMarker848"/> different numbers (of course, the random sampling will always produce slightly different results), but they are consistent. For instance, we notice that Piazza Navona is a pricey neighborhood (since its coefficient, displayed in the <code class="Code-In-Text--PACKT-">coef</code> column, is higher than all others) and that the presence of elevators can be ignored (high p-value, as you can see in the <code class="Code-In-Text--PACKT-">P&gt;|t|</code> column): </p>
			<figure class="mediaobject"><img src="image/B17125_09_15.png" alt="Table&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.15: The summary output of the OLS regression in Colab: you will get different numbers as the randomized portioning makes each specific model unique</p>
			<p class="normal">We can now move the final<a id="_idIndexMarker849"/> bit of our machine learning procedures: predicting on the test set and scoring the results:</p>
			<pre class="programlisting code"><code class="hljs-code">predictions = model.predict(test_set) 
print(<span class="hljs-string">'R2 score is'</span>,r2_score(test_set.Rent,predictions)) 
print(<span class="hljs-string">'Root Mean Squared Error is'</span>, \
       np.sqrt(mean_squared_error(test_set.Rent,predictions)))
</code></pre>
			<p class="normal">Similar to what we would<a id="_idIndexMarker850"/> do with the <strong class="keyword">Regression Predictor </strong>node in KNIME, we need to apply the regression model to <code class="Code-In-Text--PACKT-">test_set</code> and obtain some <code class="Code-In-Text--PACKT-">predictions</code>: as you can see in the first line<a id="_idIndexMarker851"/> of the code, we use the function <code class="Code-In-Text--PACKT-">predict()</code> to do exactly that. Afterward, we need to calculate two metrics for scoring our regression by comparing the real rent values in the test set (<code class="Code-In-Text--PACKT-">test_set.Rent</code>) with our predictions, similar<a id="_idIndexMarker852"/> to what we did with the <strong class="keyword">Numeric Scorer</strong> node KNIME tutorial. Specifically, we calculate the two main summary metrics for assessing regression accuracy, which we introduced in <em class="chapterRef">Chapter 4</em>, <em class="italic">What is Machine Learning?</em>:</p>
			<ul>
				<li class="bullet">The <strong class="keyword">Coefficient of Determination</strong>, R<sup class="Superscript--PACKT-">2</sup>, using the function <code class="Code-In-Text--PACKT-">r2_score()</code>, which takes<a id="_idIndexMarker853"/> as parameters the two columns to compare.</li>
				<li class="bullet">The <strong class="keyword">Root Mean Squared Error</strong> (<strong class="keyword">RMSE</strong>), which gives us an idea<a id="_idIndexMarker854"/> of the level of error to expect in the predictions. To calculate this metric, we need to combine the functions <code class="Code-In-Text--PACKT-">mean_squared_error()</code> to get the average of the squared residuals and <code class="Code-In-Text--PACKT-">np.sqrt()</code> to obtain its square root.</li>
			</ul>
			<p class="normal">When we run this last portion of code, the output we obtain confirms that we have built quite a robust model as R<sup class="Superscript--PACKT-">2</sup> nears 0.91 and the RMSE is around €118 (of course, you will obtain slightly different values):</p>
			 <pre class="programlisting con"><code class="hljs-con">R2 score is 0.9144775136955545
Root Mean Squared Error is 117.92107041510327
</code></pre>
			<p class="normal">New tools, same story: by writing around a dozen lines of Python code, we replicated the bulk of what we did in KNIME. Have a look at <em class="italic">Figure 9.16</em>: the gray boxes contain the key Python functions that do the same job as the KNIME nodes we met earlier in our journey:</p>
			<figure class="mediaobject"><img src="image/B17125_09_16.png" alt=""/></figure>
			<p class="packt_figref">Figure 9.16: A comparison view showing Python's key functions together with the corresponding KNIME nodes required for linear regression: the fundamental steps are exactly the same</p>
			<p class="normal">This exercise<a id="_idIndexMarker855"/> has clarified the differences between visual programming (what you can do in KNIME) and traditional programming by coding (which you can do using Python or any other language). There are pros and cons to each approach, and it is natural to have personal preferences toward any of the two routes. The good news is…you don't have to make a definitive choice among the alternatives. In fact, visual programming and coding can also be mixed together, making a powerful potion for your data analytics magic to shine. In the following few pages, you will learn how to embed pieces of Python code into a KNIME workflow. This is a valuable trick to know, as it allows you to "make the best" out of the joint power of KNIME accessibility and "Python's" breadth of functionalities. Even if you are not interested at this stage in the integration of KNIME and Python, I would suggest you go through the next few pages anyway. They will give you the opportunity to acquaint yourself with two powerful features you should know: KNIME extensions and KNIME Hub.</p>
			<h2 id="_idParaDest-155" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor158"/>Integrating Python with KNIME</h2>
			<p class="normal">First, you need to make sure<a id="_idIndexMarker856"/> you have a local installation of Python<a id="_idIndexMarker857"/> up and running on your computer. The easiest<a id="_idIndexMarker858"/> way to procure one is to install <strong class="keyword">Anaconda Individual Edition</strong>, one of the most popular Python distribution platforms for data analytics. Download and install the latest version of the software, available for free at <a href="http://anaconda.com/download"><span class="url">anaconda.com/download</span></a>. Anaconda comes<a id="_idIndexMarker859"/> packed with several applications for coding both in Python and in R. An example is <strong class="keyword">Jupyter Notebook, </strong>which lets<a id="_idIndexMarker860"/> you create Python<a id="_idIndexMarker861"/> scripts using a web browser—similarly to what we did in Colab but without<a id="_idIndexMarker862"/> any restrictions. From the welcome page of <strong class="keyword">Anaconda Navigator </strong>(<em class="italic">Figure 9.17</em>), you can launch<a id="_idIndexMarker863"/> Jupyter Notebook or install additional applications, like RStudio for developing in R:</p>
			<figure class="mediaobject"><img src="image/B17125_09_17.png" alt=""/></figure>
			<p class="packt_figref">Figure 9.17: The welcome screen of Anaconda Navigator: from here, you can launch Jupyter notebooks for coding in Python or install additional free packages</p>
			<p class="normal">As anticipated in <em class="chapterRef">Chapter 2</em>, <em class="italic">Getting Started with KINME</em>, you can expand KNIME functionalities by installing additional extensions. To embed Python<a id="_idIndexMarker864"/> in our workflows, we need to install the <strong class="keyword">KNIME Python Integration</strong> extension. To do so, open KNIME, go to <strong class="screenText">File</strong> | <strong class="screenText">Install KNIME Extensions...</strong> in the top bar and search for the right extension by typing <code class="Code-In-Text--PACKT-">python</code> in the text box at the top (<em class="italic">Figure 9.18</em>). Check the box for the <strong class="screenText">KNIME Python integration</strong> option, click on <strong class="screenText">Next</strong>, and follow the installation process:</p>
			<figure class="mediaobject"><img src="image/B17125_09_18.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.18: The dialog for installing extensions in KNIME. Look at the list of available packages: you can easily extend your analytical toolkit with thousands of new KNIME nodes</p>
			<p class="normal">Once finished, you<a id="_idIndexMarker865"/> will be prompted with a message asking<a id="_idIndexMarker866"/> to restart KNIME to apply the software update. After the restart, go to the node repository and open the <strong class="screenText">Scripting &gt; Python</strong> folder. As you can see in <em class="italic">Figure 9.19</em>, you have gained several new nodes to be used in your workflows by installing the extension:</p>
			<figure class="mediaobject"><img src="image/B17125_09_19.png" alt="Graphical user interface, application&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.19: The Node Repository after installing the KNIME Python Integration extension:several new nodes have materialized</p>
			<p class="normal">Before getting there, let's perform<a id="_idIndexMarker867"/> the last step needed to set KNIME<a id="_idIndexMarker868"/> up so it can connect with the Python environment that came with Anaconda. To do so, go to <strong class="screenText">File | Preferences</strong> in KNIME. Then, in the menu appearing on the left, go to <strong class="screenText">KNIME &gt; Python</strong> or use the text box on the left to look up the Python preferences window, which you can see in <em class="italic">Figure 9.20</em>. You should find the path to your Anaconda installation directory prepopulated (if that's not the case, you will have to set it up by clicking on the <strong class="screenText">Browse...</strong> button). Once done, click on the second <strong class="screenText">New environment...</strong> button in the <strong class="screenText">Python 3 (Default)</strong> section, as you can see in the following figure: this will create a new Python environment with all the packages needed for integration with KNIME:</p>
			<figure class="mediaobject"><img src="image/B17125_09_20.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.20: The Python window within the KNIME preferences: you need to tell KNIME where the local Python environment lies</p>
			<p class="normal">In the next window (<em class="italic">Figure 9.21</em>), click on <strong class="screenText">Create new environment</strong> and wait patiently for the environment<a id="_idIndexMarker869"/> to be generated. After this, you<a id="_idIndexMarker870"/> are done and all set up for enriching your KNIME workflows with all the Python you need:</p>
			<figure class="mediaobject"><img src="image/B17125_09_21.png" alt="Graphical user interface, text, application, email&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.21: New environment dialog: the final step for getting up and running with Python in KNIME</p>
			<div>
				<div id="_idContainer379" class="note">
					<p class="Information-Box--PACKT-">If, instead of Python, you want<a id="_idIndexMarker871"/> to use R in your KNIME workflows, you will have to install the <strong class="keyword">KNIME Interactive R Statistics Integration </strong>and set up the R environment from the preferences menu, similar to what we did for Python. KNIME also allows you to run some<a id="_idIndexMarker872"/> Java code for every row of a table: check the <strong class="keyword">Java Snippet</strong> node to find out more. </p>
				</div>
			</div>
			<p class="normal">Among the new nodes<a id="_idIndexMarker873"/> you acquired by installing the Python<a id="_idIndexMarker874"/> extension (<em class="italic">Figure 9.19</em>), <strong class="keyword">Python Script</strong> is certainly the most versatile<a id="_idIndexMarker875"/> one: the node lets you embed a sequence of Python code that gets applied to the data stored in the input table (generically called <code class="Code-In-Text--PACKT-">input_table_1</code>) to generate one or more output tables (called <code class="Code-In-Text--PACKT-">output_table_1</code>). You can refer to these tables in your script and freely utilize them as you would do with any data frame in Python. For instance, if you wanted to apply a simple multiplication across two columns (<em class="italic">Quantity</em> and <em class="italic">Price</em>) and output an additional column (<em class="italic">Sales</em>) with the result, the Python script to be used with this node will be:</p>
			<pre class="programlisting code"><code class="hljs-code">output_table_1 = input_table_1
output_table_1[<span class="hljs-string">'Sales'</span>] = output_table_1[<span class="hljs-string">'Quantity'</span>] * output_table_1[<span class="hljs-string">'Price'</span>]
</code></pre>
			<p class="normal">The first line of code is just copying the input table to the output one, leaving it unchanged. The second line is applying the multiplication across the two columns—that's it. We could have imported any library (provided that they are installed in the Python environment within Anaconda) and leveraged it to perform any operation we need.</p>
			<p class="normal">Let's look at a simple workflow<a id="_idIndexMarker876"/> that illustrates the power of integrating Python<a id="_idIndexMarker877"/> in our analytical workflows. Instead of building<a id="_idIndexMarker878"/> this workflow from scratch, we can find it already available in <strong class="keyword">KNIME Hub</strong>, the online repository of workflows, extensions, components, and nodes. As depicted in <em class="italic">Figure 9.22</em>, open the <strong class="screenText">KNIME Hub</strong> panel (you'll find it beside the <strong class="screenText">Node Description</strong> tab in the top right) and type <code class="Code-In-Text--PACKT-">Python Gaussian Fit</code> in the search box. Among the many alternatives, you should find a workflow with my name and picture on it: this is the workflow I have prepared for you. To import it into your KNIME installation, you can just drag the box (highlighted in the figure) and drop it onto your workflow editor. An alternative approach would be to import the KNIME workflow (<strong class="screenText">File</strong> | <strong class="screenText">Import KNIME Workflow...</strong>) that you will find in the GitHub repository:</p>
			<figure class="mediaobject"><img src="image/B17125_09_22.png" alt="Graphical user interface&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.22: The KNIME Hub panel: search for the workflows, nodes, or components you need and drop them onto your workspace</p>
			<p class="normal">If you open the configuration<a id="_idIndexMarker879"/> dialog of the <strong class="keyword">Python View </strong>node, you will find the code window shown in <em class="italic">Figure 9.23</em>. The large text box in the middle is where you can write your Python<a id="_idIndexMarker880"/> code. On the left, you have<a id="_idIndexMarker881"/> the list of columns available in the input table: by double-clicking on them, the corresponding Python data frames are added to the code. You can also test your code by clicking on the <strong class="screenText">Execute Script</strong> or <strong class="screenText">Execute selected lines</strong> buttons and checking whether it works fine. If you receive any warnings or errors during the execution of the script, they will be conveniently displayed in the console box at the bottom of the window:</p>
			<figure class="mediaobject"><img src="image/B17125_09_23.png" alt="Graphical user interface, text, application&#10;&#10;Description automatically generated"/></figure>
			<p class="packt_figref">Figure 9.23: Configuration window of the Python View node: use Python graphic  libraries to generate any chart you like</p>
			<p class="normal">In this specific case, we leverage the <strong class="keyword">Python View</strong> node to fit a Gaussian<a id="_idIndexMarker882"/> function (the famous bell curve) to the distribution of Rome rental prices and return the histogram with the fitting curve. Going through the details of the code is not needed at this stage. However, you will notice that the <em class="italic">Rent</em> column has been referred to in the code with the name <code class="Code-In-Text--PACKT-">input_table['Rent']</code> while the generated chart has been saved to the variable called <code class="Code-In-Text--PACKT-">output_image</code>: you find the final result in <em class="italic">Figure 9.24</em>:</p>
			<figure class="mediaobject"><img src="image/B17125_09_24.png" alt=""/></figure>
			<p class="packt_figref">Figure 9.24: The output of the Python View node: rent prices in Rome are centered around €1,000</p>
			<p class="normal">This gives you an illustration of how Python nodes work: the data at the input port is translated into input variables, and, at the end of the script, whatever is assigned to the output variables gets returned at the output port of the node. In the same workflow you downloaded<a id="_idIndexMarker883"/> from KNIME Hub, you will also see an example<a id="_idIndexMarker884"/> of a <strong class="keyword">Python Script</strong> node: essentially, both nodes<a id="_idIndexMarker885"/> run Python code on the input<a id="_idIndexMarker886"/> data, but the <strong class="keyword">Python View</strong> node is "specialized" in outputting images.</p>
			<p class="normal">Interlacing code within a workflow has massive potential. If you want to apply some complex logic or reuse specialized code that has been developed outside of KNIME for solving your specific business need, you can now seamlessly integrate it all and significantly expand the power of your toolbox.</p>
			<p class="normal">After seeing Python in action, let's go back to the world of codeless analytics and meet one of the promising directions of advanced analytics: automated machine learning.</p>
			<h1 id="_idParaDest-156" class="title"><a id="_idTextAnchor159"/>Automated machine learning</h1>
			<p class="normal">"Brute-force patterns finding": this is how<a id="_idIndexMarker887"/> we can briefly (and colorfully) summarize what <strong class="keyword">Automated Machine Learning</strong> or, for short, <strong class="keyword">AutoML</strong>, is all about. As you saw in <em class="chapterRef">Chapters 4</em> and <em class="chapterRef">5</em>, building a machine learning model is far from being a linear, single-attempt endeavor. The usual procedure for obtaining high-performing supervised models is to go through a series of "back and forth" attempts: each time, we apply some "tuning" to the model or its features and check whether the predictive performance increases or not. We have seen already some of these mechanisms in action:</p>
			<ul>
				<li class="bullet"><strong class="keyword">Hyperparameters optimization</strong>: this is when you apply changes<a id="_idIndexMarker888"/> to the way the learning algorithm operates, like when we activated pruning in decision trees or changed the degree of a polynomial regression. In more complex models (like in the case of deep neural networks), changing parameters (for instance, the number of neurons in the network) can make a significant difference to performance.</li>
				<li class="bullet"><strong class="keyword">Feature selection</strong>: by selecting a subset of features (and removing the redundant ones), you make your model learning<a id="_idIndexMarker889"/> focus on what matters most, increasing its ability to predict. We did this when we decided to remove some high p-value features from regression models. Additionally, making a model run on fewer features means saving time and computing resources.</li>
				<li class="bullet"><strong class="keyword">Feature engineering</strong>: you can generate new features<a id="_idIndexMarker890"/> by combining or transforming the original ones to make them more informative for the model. For instance, this is what we did when we created dummy variables in regression.</li>
				<li class="bullet"><strong class="keyword">Stacking</strong>: we said that sometimes we could combine different algorithms together in a single<a id="_idIndexMarker891"/> learning procedure. Think of predicting rental prices using five different intermediate regression models and then adopting the average of the five intermediate predictions as the overall prediction: by collating alternative models together, you might obtain a more robust one.</li>
			</ul>
			<p class="normal">Instead of manually checking the effect of each tuning step one by one, we can build a procedure that leverages all the available computing power to find the way to the best possible model. This is what the AutoML approach promises to do: automating the "trial and error" process of identifying parameters, features, and model combinations that maximize the overall performance (and—hopefully—the business impact) of our machine learning procedure.</p>
			<p class="normal">AutoML<a id="_idIndexMarker892"/> is currently a trending topic in business-applied AI, and there is a growing number of products and open-source libraries available out there for applying AutoML to real-world tasks, including H2O.ai, DataRobot, auto-sklearn, Google Cloud AutoML, IBM AutoAI, Amazon AutoGluon, and Azure AutomatedML. As we explore ways to expand our analytics toolbox, let's see one of these products in action: this will give you an idea of what is already available and what's to come from our companies<a id="_idIndexMarker893"/> in the next few years.</p>
			<p class="normal">We will explore <strong class="keyword">H2O Driverless AI</strong>, a cloud-based service that lets you use a web interface to upload data, run AutoML to make predictions, and interpret results. If you want to test it yourself, go to <a href="http://h2o.ai/products/h2o-driverless-ai"><span class="url">h2o.ai/products/h2o-driverless-ai</span></a>, register for a free account, and create an instance<a id="_idIndexMarker894"/> of Driverless AI.</p>
			<h2 id="_idParaDest-157" class="title" lang="en-GB" xml:lang="en-GB"><a id="_idTextAnchor160"/>AutoML in action: an example with H2O.ai</h2>
			<p class="normal">In this example, we will reuse the Rome housing business case once again: this time, we will<a id="_idIndexMarker895"/> upload the Excel dataset<a id="_idIndexMarker896"/> and create a new experiment. <em class="italic">Figure 9.25</em> shows what the interface looks like: you can select the <strong class="screenText">Target Column</strong> (<em class="italic">Rent</em>, in our case), pick a metric for the <strong class="screenText">Scorer</strong> (in the figure, you can see we picked RMSE), and then turn the three knobs at the bottom to set the expected level of prediction <strong class="screenText">Accuracy</strong>, the <strong class="screenText">Time</strong> required for training the model, and the level of human <strong class="screenText">Interpretability</strong>. This bit is fascinating: as you operate the knobs, the system updates its "trial and error" strategy (you can see a dynamic summary on the left) to be performed during the AutoML search routine. If you go for high accuracy and low interpretability, you will end up with high-performing black-box models, while if you set interpretability to a high level, you will obtain simpler models with fewer features so that you can explain to your business partners how it works:</p>
			<figure class="mediaobject"><img src="image/B17125_09_25.png" alt="A screenshot of a computer&#10;&#10;Description automatically generated with medium confidence"/></figure>
			<p class="packt_figref">Figure 9.25: The experiment setup page in H2O Driverless AI: play with the knobs to determine how you would like your generated model to be cooked</p>
			<p class="normal">After clicking on <strong class="screenText">Launch Experiment</strong>, the remote<a id="_idIndexMarker897"/> computing power will do<a id="_idIndexMarker898"/> the hard work for you while you grab something to drink. The following view shows you the live evolution of the score metrics as more and more models are iteratively tried and, when completed, will display the best results (<em class="italic">Figure 9.26</em>):</p>
			<figure class="mediaobject"><img src="image/B17125_09_26.png" alt="A screenshot of a computer&#10;&#10;Description automatically generated with medium confidence"/></figure>
			<p class="packt_figref">Figure 9.26: The results of an AutoML routine:at the bottom left, you can see how the scoring metric changes as the search iteration progresses</p>
			<p class="normal">As part of the AuotML<a id="_idIndexMarker899"/> procedure, we also get<a id="_idIndexMarker900"/> some useful views that equip us for understanding how the model works. Have a look at the interpretation dashboard generated for our rental price predictions (<em class="italic">Figure 9.27</em>):</p>
			<ul>
				<li class="bullet">At the top right, we see a bar<a id="_idIndexMarker901"/> chart displaying <strong class="keyword">Features importance</strong>: unsurprisingly, <em class="italic">Neighborhood</em> is the single most useful column when predicting the rent, followed by the <em class="italic">Surface</em> of the property.</li>
				<li class="bullet">At the bottom left, we have<a id="_idIndexMarker902"/> a tree-based <strong class="keyword">Surrogate model</strong>: this is the "minimalist" version of the actual, full-on prediction model generated by the AutoML routine. By looking at the first three levels of this tree, we get a high-level, easy-to-explain view of the patterns that link the most important features to the rental price.</li>
				<li class="bullet">At the bottom<a id="_idIndexMarker903"/> right, we find the <strong class="keyword">Partial dependence </strong>plot: this shows us the marginal effect of a specific feature (<em class="italic">Surface</em>, in the case of <em class="italic">Figure 9.27</em>) on the predicted outcome (<em class="italic">Rent</em>). This chart provides us with an additional interpretation key, revealing "how" the rent increases as the surface grows:<figure class="mediaobject"><img src="image/B17125_09_27.png" alt="A screenshot of a computer&#10;&#10;Description automatically generated with medium confidence"/></figure></li>
			</ul>
			<p class="packt_figref">Figure 9.27: The model interpretation dashboard: get some hints on how the prediction model works</p>
			<p class="normal">With this example, we have admired the AutoML<a id="_idIndexMarker904"/> approach in all its potential. With only<a id="_idIndexMarker905"/> a few clicks, we obtained a robust predictive model (that can be exported and deployed for further use) and a simple framework for explaining its results. It's important to make a further consideration: although it looks like the "holy grail" of machine learning, leveraging AutoML in a business context will still require its users to always "know what they are doing." This means that building machine learning expertise and, in general, data analytics fluency (like you did in this book) is and still will be crucial for making the best of this technology, however automated and easy to use it looks.</p>
			<p class="normal">AutoML can be another valuable tool to keep in our data analytics kit. The good news is that you find this approach nicely implemented in KNIME as well, so you can connect it with everything else you have learned in the book. If you open the example workflow called H2O AutoML for Regression (you will find it in the Examples server in <strong class="screenText">KNIME Explorer </strong>or by searching in <strong class="screenText">KNIME Hub</strong>), you will be asked<a id="_idIndexMarker906"/> to install a new extension: <strong class="keyword">KNIME H2O Machine Learning Integration</strong>. By installing this extension, you make many of the AutoML functionalities we have seen in H2O Driverless AI available to you in KNIME. Look at the sample workflow mentioned earlier (<em class="italic">Figure 9.28</em>): by employing a few H2O nodes—organized as per the usual supervised machine learning structure with partitioning, learner, predictor, and scorer—you get the full power<a id="_idIndexMarker907"/> of AutoML directly<a id="_idIndexMarker908"/> in KNIME: </p>
			<figure class="mediaobject"><img src="image/B17125_09_28.png" alt=""/></figure>
			<p class="packt_figref">Figure 9.28: The H2O AutoML for Regression KNIME workflow: use AutoML to find the best model for you</p>
			<h1 id="_idParaDest-158" class="title"><a id="_idTextAnchor161"/>Summary</h1>
			<p class="normal">I hope this final chapter got you excited about all the directions you can take to further expand your data analytics toolbox. We took our first steps in Tableau and realized how similar it is, in its fundamental features, to Power BI. We have also gone through a friendly introduction to Python, the ubiquitous programming language in data science. As we integrated Python in KNIME, we have seen how to take the best from both the visual and coding programming worlds. As we did so, we took the opportunity to learn how to expand KNIME further by using its vast extensions base and leveraging the public KNIME Hub environment. Lastly, we got a quick tour through the attractive land of AutoML, being exposed to its promising ability to simplify the process of building high-performing machine learning models considerably.</p>
			<p class="normal">In this chapter, we extended our toolbox by exploring new tools and approaches to run better data analytics in our everyday work. My advice is to make this a habit. One limitation I have seen in many data practitioners is to think that the few tools they feel comfortable with will <em class="italic">always</em> be the best for them, falling in the limiting bias of self-sufficiency. So instead, don't feel satisfied with the toolbox we have just built—be ready to explore continuously: stay curious, as the expanding world of data analytics will have a lot to offer!</p>
		</div>
	</body></html>