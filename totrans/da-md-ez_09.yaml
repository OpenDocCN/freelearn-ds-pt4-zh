- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extending Your Toolbox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting up to speed with the latest tools and novel techniques in data analytics
    is surely a never-ending process. In this field, you need to be ready to update
    and expand your knowledge continuously. So far in this book, we have acquired
    a number of vital, application-agnostic data techniques such as data cleaning
    and modeling, machine learning, data visualization, and storytelling. We have
    also learned how to apply them through a solid application toolbox made of KNIME
    and Power BI. As we approach the end of our journey, we should see what else is
    available and how to integrate all the applications together to make the best
    out of all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Tableau, and how can I use it for visualization and storytelling?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Python, and how do I get started with it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I boost my workflows by integrating Python or any other code?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I use KNIME extension packages to add functionalities?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is automated machine learning, and what should I expect from its future
    business applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter is not meant to make you an autonomous user of Python, Tableau,
    and other tools. There is no need for it at this stage. Your initial toolbox (KNIME
    and Power BI) covers your essential analytical needs well. The point of this chapter
    is to show you what *else* is available and make you curious and excited about
    the many directions you can take to expand your abilities in data analytics from
    now on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first look at Tableau, another data visualization tool: by means of
    a simple example, we will see how what we learned for Power BI can be easily applied
    in Tableau as well. Then, we will learn about Python and get a friendly introduction
    to how it is used in analytics. We will see how to integrate Python code into
    KNIME through extension packages. Lastly, we will learn about automated machine
    learning and see the concept in action with the help of the H2O.ai platform. All
    the tools in this chapter are either open source or provide free trial options
    so that you have the opportunity to put your hands on them and evaluate by yourself
    how they can help you and your business.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Tableau
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Founded as a Stanford University spin-off, Tableau has pioneered in the data
    visualization arena for nearly two decades and is now regarded as one of the leading
    business intelligence platforms. Its straightforward drag-and-drop user interface,
    the integration with many data platforms, and its highly customizable, high-quality
    chart types have made Tableau very popular among business professionals, analysts,
    and data journalists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Power BI, Tableau comes in different versions. In this chapter,
    we will use **Tableau Public**: this is a free desktop application (you can download
    it from [public.tableau.com](http://public.tableau.com)) that has nearly all the
    functionalities included in the full version (called **Desktop Professional**)
    but also a couple of important limitations. First, it relies on local data, so
    you cannot connect to remote data sources. Additionally, the public version lets
    you save your result solely on the public Tableau server, which is open to everyone:
    this means you cannot save your work on your computer. Given its lack of privacy
    protection, Tableau Public is not viable for day-to-day business needs, but we
    can still use it for exploring Tableau functionalities and comparing them with
    Power BI''s.'
  prefs: []
  type: TYPE_NORMAL
- en: You can publish dashboards online using the cloud-based service called **Tableau
    Server**. You can also design dashboards just using your browser and avoid the
    installation of new software. To do so, you will need to register with the Tableau
    Public website mentioned above, go to your profile, and click on **Create a Viz**.
    The user interface on the web app is very similar to the one you could find in
    the desktop application, which we will use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the spirit we kept throughout the book, let''s explore Tableau through
    practice by getting our hands on it. In this short tutorial, we will create a
    couple of visualizations based on the sales database we leveraged in *Chapter
    6*, *Getting Started with Power BI*: a treemap to display the relative weight
    of categories and a line chart that shows the evolution of sales over time. This
    time, the three tables (Transactions, ProductMD, and CustomerMD) are saved as
    separate sheets in one single Excel file (`SalesDashboardTableau.xlsx`):'
  prefs: []
  type: TYPE_NORMAL
- en: Open Tableau Public. In the first screen (which looks similar to what you see
    in *Figure 9.1*), click on **Microsoft Excel** on the left and open the file containing
    our data:![](img/B17125_09_01.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 9.1: The initial screen of Tableau Public: select the type of files
    you want to use on the left'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next window, called the **Data Source** screen (*Figure 9.2*), you will
    find on the left the three sheets included in the Excel file we just opened. By
    dragging them on the blank area on the top left, you can build an entity-relationship
    diagram that defines the dashboard's underlying data model:![Graphical user interface,
    table
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_02.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.2: The Data Source screen in Tableau: drag and drop your source tables
    and build the data model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To do so, we need to follow the right order. First, drag the **Transactions**
    table and wait a few seconds for the data to load and the preview to appear on
    the bottom. Then, drag the Customer Master Data (**CustomerMD**) table and drop
    it on the right of the **Transactions** box: the line between the two indicates
    that Tableau will join the two tables. As you release the mouse button, the **Edit
    Relationship** window will appear (*Figure 9.3*): Tableau has successfully identified
    *Customer ID* as the column to be used for matching rows. We can confirm the relationship
    by closing the window, without making any changes to the default settings. Once
    this is done, it''s time to drag also the Product Master Data (**ProductMD**)
    table to the right of **Transactions**, making sure that the resulting connections
    look similar to what you had in *Figure 9.2*. Finally, confirm *StockCode* as
    the matching column by closing the window that pops up: your data model is good
    to go:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_03.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.3: Edit Relationship window: select one or more matching conditions
    for your joins'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To proceed further, click on the **Sheet 1** tab on the bottom left: you will
    land in the main interface of Tableau, called **Workspace** (*Figure 9.4*). Let''s
    explore the four fundamental sections in the workspace:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A. The **Data Panel** is where you find all your data columns, organized by
    table. From here, you will drag the quantities you wish to use in a visualization
    or create calculated fields. This is similar to the **Fields** section in Power
    BI.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. The **Visualization View** is where you can build your visuals. From here,
    you can connect the fields available in the data panel with the visual attributes
    in a visual, which in Tableau are called **Shelves**. For example, the height
    of bars, the position of points, their color, size, and the text appearing on
    the labels are all controlled through the **Rows**, **Columns** and **Marks**
    shelves that you find in the visualization view (their usage will become clearer
    as we go through the tutorial). From the same view, you can also implement pagination
    and split one visual into multiple pages, each one showing different values for
    a given column (that you have to drop in the **Pages** shelf). Additionally, from
    the visualization view, you can decide which fields to use for limiting the data
    to be visualized (**Filters** shelf).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. The **Show Me Panel** is where you can select the type of chart to use, such
    as line charts, treemaps, histograms, or geographic maps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'D. The bar at the bottom lets you add **Sheet tabs** and navigate through them.
    In Tableau, every sheet can be one among three different types: a Worksheet (a
    single chart), a Dashboard (a composition of multiple charts), or a Story (a controlled
    sequence of worksheets or dashboards that progress to convey a data story, as
    you learned to do in *Chapter 8*, *Telling Stories with Data*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_09_04.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9.4: The Workspace screen in Tableau: drag and drop columns to the visualization
    features, pick chart types, and move across visuals, dashboards, and stories'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we are acquainted with the workspace interface, we can build our first
    chart to show the relative size of each category and subcategory in terms of sales.
    However, we do not yet have a column carrying the revenues generated by each transaction
    and, so, we need to first add a calculated field that does the math for us.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Right click on the *Price* column available in the **Data Panel** and then select
    **Create** | **Calculated Field…** as shown in *Figure 9.5*:![Graphical user interface,
    application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_05.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.5: Creating a calculated field in Tableau: we can add math formulas
    and generate new quantities to be visualized'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will be prompted with a dialog (*Figure 9.6*) that lets you enter the name
    of the new column (in the text box on the top left—we are going for `Sales` in
    this case) and the mathematical expression to be used, where fields are indicated
    through square brackets. Click on the little arrow showing on the far right of
    the window. You will open an extra panel that, similarly to what we had in the
    **Math Formula** node in KNIME, provides many logical and mathematical functions
    to be used, together with a textual description on the right. In our case, the
    expression `[Price]*[Quantity]` will do: write it in the box on the left (Tableau
    will help by attempting to autocomplete the names of the columns as you type them)
    and then click on **OK** to move on. The new calculated field will appear in the
    **Data Panel** and can now be used as we wish:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17125_09_06.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9.6: Defining a calculated field: add the math expression that combines
    columns as you need'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now have all the ingredients to bake our first visual: having learned in
    *Chapter 7,* *Visualizing Data Effectively**BI*, how to resist the sweet temptation
    of using pie charts, we want to build a nice treemap that shows the relative magnitude
    of sales by category and subcategory. Building a visual in Tableau requires dragging
    the data fields of interest (the columns listed in the data panel on the left)
    and dropping them to some visual attributes (the shelves appearing in the visualization
    view). In the case of the treemap, you can follow the arrows in *Figure 9.7* as
    a guide. Start by dropping *Category* (first) and *Subcategory* (second) in the
    box called **Text** (it will be automatically renamed to **Label** later, when
    the chart type is established). Then, take the newly created *Sales* field and
    drop it in the **Size** box. Lastly, get the *Category* field to also control
    the color of the areas by dropping it to the **Color** box:![](img/B17125_09_07.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 9.7: Building a treemap in Tableau: take the fields on the left and
    drop them to the right box in the Marks shelf'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After you build a visual in Tableau, you can easily explore alternative versions
    where different chart types are applied to the same data. To try this, click on
    the various boxes you find in the **Show Me** panel on the right. The chart types
    that cannot be rendered given the current data are grayed out, and you cannot
    select them. Tableau might also recommend one specific chart type and highlight
    its border in orange. I recommend you choose the chart type by always keeping
    the business question in mind, as you learned in *Chapter 7*, *Visualizing Data
    Effectively*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Well done! With just four drag and drops, you have built your first visual in
    Tableau. We can easily see how the "Home" *Category* (and—within it—the "Kitchen"
    *Subcategory*) generates the biggest revenue bucket in our business.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s move to the second business question: this time, we want to focus our
    message on the trend of *Sales* by *Category*. We decide to put together a line
    chart as it is our natural chart type for communicating insights related to the
    evolution of quantities over time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To create a new visual, click on the first small **+** icon on the right of
    the **Sheet 1** tab at the bottom (alternatively, you can click on **Worksheet
    | New Worksheet** from the top menu, or just press *CTRL* + *M*). By doing this,
    a blank **Sheet 2** appears: this is the space to draw our line chart on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first field to drag is *Invoice time*: drop it to the **Columns** shelf.
    Given its type (carrying the date and time of each transaction), we need to tell
    Tableau at which level of granularity (years, quarters, months, weeks, and so
    on) we want to aggregate. In this case, we want to visualize one data point for
    every month of transactions: right-click on the field as it appears on the shelf,
    and then select the second **Month** entry in the pop-up menu (use *Figure 9.8*
    as a guide):![Graphical user interface, application'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_08.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.8: Using a date field for a line chart: right-click on the field and
    select the time granularity you need. In this case, we go for months'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s move on to implementing the other fields we want to use, following the
    drag and drops you see in *Figure 9.9*. Drop *Sales* to the **Rows** shelf (by
    default, the aggregation by sum is applied, but you can change it easily by right-clicking
    on the field and picking the proper function in the **Measure** submenu). The
    following field to move is *Category*. Drop it twice: first to the **Color** box
    (so we differentiate lines by *Category*) and then to the **Label** box (so we
    show a direct label for each line):![](img/B17125_09_09.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 9.9: Building a line chart in Tableau: the columns and rows shelves
    control the x- and the y-axis, respectively'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before composing our dashboard, we can edit the sheet names at the bottom by
    right-clicking on each tab and selecting **Rename**. We can go for something more
    meaningful like `Business by category` and `Sales trend` for the first and the
    second visualizations, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's time to build the dashboard by combining the two visuals. Click on the
    second **+** icon in the **Sheets** tab at the bottom or select **Dashboard |
    New Dashboard** from the top menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left of the new dashboard view, you will find a list of the two worksheets
    we built, one for each chart we created. To create a dashboard, you will just
    need to drag and drop the worksheets in the blank area on the right, as you can
    see in *Figure 9.10*:![Chart
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_10.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.10: Building a dashboard in Tableau: drag and drop the visualizations
    to their positions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the bottom left of the dashboard view, you find several icons that you can
    drag and drop to your dashboard to add additional objects, such as text labels,
    images, web pages, or extensions. Check it out.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before publishing the dashboard, let''s configure the interactions across visuals.
    If you click on any empty space of the first visual, you will select it, and its
    borders get highlighted (letting you adjust its shape, if you wish). Also, a few
    icons will appear on the top right of the selected visual, as you can see in *Figure
    9.11*. If you click on the filter icon, as the arrow in the picture indicates,
    you will set that visual as a filter for every other visual in the dashboard.
    You can quickly test that this works properly: if you click on any subcategory
    in the treemap (you can select more than one at once by keeping the *CTRL* key
    pressed), you will notice that the line chart updates accordingly, showing only
    the trend of the selection portion of business. This is exactly what we were after:![Chart,
    treemap chart'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_11.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.11: Use a visual for filtering subsequent charts in a Tableau dashboard.Click
    on the filter icon in the top right'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now publish our work on the server: just open the **File** menu at the
    top and then click on **Save to Tableau Public…**. Next, pick a name (I went for
    `Ecom Sales Dashboard`), click on **OK**, and wait for a few seconds for the data
    to upload. Your browser will open up and show your published dashboard in all
    its grace (you can check my version out at [tiny.cc/ecomdashboard](http://tiny.cc/ecomdashboard)
    and in *Figure 9.12*):![Chart, treemap chart'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_12.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.12: The dashboard published on the Tableau Public server: let others
    access your work and interact with it'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last few pages, we ran through the fundamental functionalities of Tableau:
    we have learned how to load data, combine it in a simple data model, created calculated
    fields, and built visuals and combined them in an interactive dashboard. I am
    sure you noticed the extensive similarities between Tableau and what we have learned
    on Power BI. We could carry on in the exploration of other business intelligence
    platforms such as Qlik, MicroStrategy, and TIBCO Spotfire, to mention a few. The
    (exciting) reality is that the *bulk* of how they work is very similar, and the
    last few chapters have equipped you with all you need to get started and create
    value for your business, irrespective of the tool you used.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move to the next "expansion" phase of our data analytics toolbox with
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python for data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is an increasingly popular high-level programming language that is particularly
    well suited for data analytics and machine learning applications. The ample availability
    of analytics-related libraries and its easy-to-learn syntax make it the preferred
    choice for many data science practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: The story behind Python's name has nothing to do with snakes. Its creator, Dutch
    programmer Guido van Rossum, was a big fan of the 1970s BBC comedy series "Monty
    Python's Flying Circus." So he picked Python as the name of the project to honor
    the irreverent genius of the British comedy troupe running that show.
  prefs: []
  type: TYPE_NORMAL
- en: As this book focuses on visual programming, we will not go through any thorough
    explanation of coding principles. Instead, the purpose of this section is to let
    you see Python in action on a familiar problem and get some perspective on how
    it can be used in our everyday work. We will first go through a script that repeats
    the exact same regression tutorial we saw in *Chapter 5*, *Applying Machine Learning
    at Work*. Then, we will see how Python can smoothly integrate with KNIME to make
    the best out of the two complementary approaches to programming for analytics.
  prefs: []
  type: TYPE_NORMAL
- en: A gentle introduction to the Python language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use Python, you can either install a development platform like Anaconda
    (we will do this later) or leverage a web-based interface such as Colab. Google
    Colab (short for Colaboratory) is a free cloud service that lets you write and
    run Python code without any setup being needed: you can access it at [colab.research.google.com](http://colab.research.google.com).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 9.13*, the user interface of Colab is an interactive
    web page where you can add text and code and then run it, line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.13: The welcome screen of Google Colab: get some Python going without
    installing any software'
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the comparison with KNIME, let's use Colab on the same Rome housing
    business case we encountered in *Chapter 5*, *Applying Machine Learning at Work*.
    As a reminder, the objective is to predict rental prices by applying the linear
    regression learning algorithm to the database of historical rental agreements.
    You can follow step by step the full Colab script by connecting to [tiny.cc/romecolab](https://colab.research.google.com/github/laibniz/AnalyticsMadeEasy/blob/main/Rome_housing.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the code and, for each portion, understand what is going
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to import some useful libraries into the Python environment.
    This will make a few extra functionalities (like loading Excel files and calculating
    a linear regression) available for us to leverage in our code. In particular,
    in the preceding code, we use a few `import` statements to include some of the
    most popular Python libraries used for data analytics, namely: **Pandas** for
    data manipulation, **NumPy** for numerical routines and array calculations, **Statsmodels**
    for hardcore statistics, and **Scikit-learn** (`sklearn` in the code) for machine
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As a next step, we read the data stored in an Excel file by using the `pd.read_excel()`
    function and assign its content to the `full_data` variable. We can then check
    the imported data by visualizing its top five rows, using the function `head()`,
    producing the output shown in *Figure 9.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_14.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.14: The output of the head() function as displayed in Colab:a useful
    peek into the top five rows in our dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data we obtain is exactly what we encountered at the beginning of the first
    tutorial in *Chapter 5*, *Applying Machine Learning at Work* (see the first rows
    in *Figure 5.2*). We can move on and proceed with the first step of every supervised
    machine learning procedure, that is partitioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of the `train_test_split()` function, we apply a random sampling
    to our full data and obtain the training and the test set (which we set to be
    30% of the total), which are stored in the `train_set` and `test_set` variables.
    This line of Python code implements what the **Partitioning** node did for us
    in KNIME. We now have all we need to learn the model using the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Leveraging the function `smf.ols()`, this code portion trains an Ordinary Least
    Square regression model (OLS, which we encountered in *Chapter 5*, *Applying Machine
    Learning at Work*) using the `train_set` variable as an input. The output model
    is stored in an object called `model`. As we train the model, we can edit the
    formula string you see in the code (`Rent ~ Rooms + Surface + ...`) to select
    which column is the target (in our case `Rent`, which appears before the `~` sign)
    and which other columns should be used as predictors (the ones that go after the
    `~` symbol, separated by a `+` sign) in the linear regression. Categorical columns
    need to be encapsulated by the `C()` function (like in `C(Neighborhood)`): by
    doing so, Python converts them into multiple numerical columns (dummy variables)
    that are compatible with a linear regression model. The definition of the linear
    regression formula and the conversion of the nominal variable were done "under
    the hood" by the **Linear Regression Learner** node in KNIME, while in Python,
    they need to be specified in the code. Finally, the `summary()` function summarizes
    the regression results, including coefficients and p-values for each feature.
    If you compare the output obtained in Python (*Figure 9.15*) with the one obtained
    as an output of the **Linear Regression Learner** node in KNIME (*Figure 5.9*),
    you will find different numbers (of course, the random sampling will always produce
    slightly different results), but they are consistent. For instance, we notice
    that Piazza Navona is a pricey neighborhood (since its coefficient, displayed
    in the `coef` column, is higher than all others) and that the presence of elevators
    can be ignored (high p-value, as you can see in the `P>|t|` column):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_15.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.15: The summary output of the OLS regression in Colab: you will get
    different numbers as the randomized portioning makes each specific model unique'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now move the final bit of our machine learning procedures: predicting
    on the test set and scoring the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to what we would do with the **Regression Predictor** node in KNIME,
    we need to apply the regression model to `test_set` and obtain some `predictions`:
    as you can see in the first line of the code, we use the function `predict()`
    to do exactly that. Afterward, we need to calculate two metrics for scoring our
    regression by comparing the real rent values in the test set (`test_set.Rent`)
    with our predictions, similar to what we did with the **Numeric Scorer** node
    KNIME tutorial. Specifically, we calculate the two main summary metrics for assessing
    regression accuracy, which we introduced in *Chapter 4*, *What is Machine Learning?*:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Coefficient of Determination**, R², using the function `r2_score()`, which
    takes as parameters the two columns to compare.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Root Mean Squared Error** (**RMSE**), which gives us an idea of the level
    of error to expect in the predictions. To calculate this metric, we need to combine
    the functions `mean_squared_error()` to get the average of the squared residuals
    and `np.sqrt()` to obtain its square root.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we run this last portion of code, the output we obtain confirms that we
    have built quite a robust model as R² nears 0.91 and the RMSE is around €118 (of
    course, you will obtain slightly different values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'New tools, same story: by writing around a dozen lines of Python code, we replicated
    the bulk of what we did in KNIME. Have a look at *Figure 9.16*: the gray boxes
    contain the key Python functions that do the same job as the KNIME nodes we met
    earlier in our journey:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_09_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: A comparison view showing Python''s key functions together with
    the corresponding KNIME nodes required for linear regression: the fundamental
    steps are exactly the same'
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise has clarified the differences between visual programming (what
    you can do in KNIME) and traditional programming by coding (which you can do using
    Python or any other language). There are pros and cons to each approach, and it
    is natural to have personal preferences toward any of the two routes. The good
    news is…you don''t have to make a definitive choice among the alternatives. In
    fact, visual programming and coding can also be mixed together, making a powerful
    potion for your data analytics magic to shine. In the following few pages, you
    will learn how to embed pieces of Python code into a KNIME workflow. This is a
    valuable trick to know, as it allows you to "make the best" out of the joint power
    of KNIME accessibility and "Python''s" breadth of functionalities. Even if you
    are not interested at this stage in the integration of KNIME and Python, I would
    suggest you go through the next few pages anyway. They will give you the opportunity
    to acquaint yourself with two powerful features you should know: KNIME extensions
    and KNIME Hub.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Python with KNIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, you need to make sure you have a local installation of Python up and
    running on your computer. The easiest way to procure one is to install **Anaconda
    Individual Edition**, one of the most popular Python distribution platforms for
    data analytics. Download and install the latest version of the software, available
    for free at [anaconda.com/download](http://anaconda.com/download). Anaconda comes
    packed with several applications for coding both in Python and in R. An example
    is **Jupyter Notebook,** which lets you create Python scripts using a web browser—similarly
    to what we did in Colab but without any restrictions. From the welcome page of
    **Anaconda Navigator** (*Figure 9.17*), you can launch Jupyter Notebook or install
    additional applications, like RStudio for developing in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_09_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: The welcome screen of Anaconda Navigator: from here, you can launch
    Jupyter notebooks for coding in Python or install additional free packages'
  prefs: []
  type: TYPE_NORMAL
- en: 'As anticipated in *Chapter 2*, *Getting Started with KINME*, you can expand
    KNIME functionalities by installing additional extensions. To embed Python in
    our workflows, we need to install the **KNIME Python Integration** extension.
    To do so, open KNIME, go to **File** | **Install KNIME Extensions...** in the
    top bar and search for the right extension by typing `python` in the text box
    at the top (*Figure 9.18*). Check the box for the **KNIME Python integration**
    option, click on **Next**, and follow the installation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.18: The dialog for installing extensions in KNIME. Look at the list
    of available packages: you can easily extend your analytical toolkit with thousands
    of new KNIME nodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once finished, you will be prompted with a message asking to restart KNIME
    to apply the software update. After the restart, go to the node repository and
    open the **Scripting > Python** folder. As you can see in *Figure 9.19*, you have
    gained several new nodes to be used in your workflows by installing the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.19: The Node Repository after installing the KNIME Python Integration
    extension:several new nodes have materialized'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before getting there, let''s perform the last step needed to set KNIME up so
    it can connect with the Python environment that came with Anaconda. To do so,
    go to **File | Preferences** in KNIME. Then, in the menu appearing on the left,
    go to **KNIME > Python** or use the text box on the left to look up the Python
    preferences window, which you can see in *Figure 9.20*. You should find the path
    to your Anaconda installation directory prepopulated (if that''s not the case,
    you will have to set it up by clicking on the **Browse...** button). Once done,
    click on the second **New environment...** button in the **Python 3 (Default)**
    section, as you can see in the following figure: this will create a new Python
    environment with all the packages needed for integration with KNIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.20: The Python window within the KNIME preferences: you need to tell
    KNIME where the local Python environment lies'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next window (*Figure 9.21*), click on **Create new environment** and
    wait patiently for the environment to be generated. After this, you are done and
    all set up for enriching your KNIME workflows with all the Python you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.21: New environment dialog: the final step for getting up and running
    with Python in KNIME'
  prefs: []
  type: TYPE_NORMAL
- en: 'If, instead of Python, you want to use R in your KNIME workflows, you will
    have to install the **KNIME Interactive R Statistics Integration** and set up
    the R environment from the preferences menu, similar to what we did for Python.
    KNIME also allows you to run some Java code for every row of a table: check the
    **Java Snippet** node to find out more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the new nodes you acquired by installing the Python extension (*Figure
    9.19*), **Python Script** is certainly the most versatile one: the node lets you
    embed a sequence of Python code that gets applied to the data stored in the input
    table (generically called `input_table_1`) to generate one or more output tables
    (called `output_table_1`). You can refer to these tables in your script and freely
    utilize them as you would do with any data frame in Python. For instance, if you
    wanted to apply a simple multiplication across two columns (*Quantity* and *Price*)
    and output an additional column (*Sales*) with the result, the Python script to
    be used with this node will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first line of code is just copying the input table to the output one, leaving
    it unchanged. The second line is applying the multiplication across the two columns—that's
    it. We could have imported any library (provided that they are installed in the
    Python environment within Anaconda) and leveraged it to perform any operation
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a simple workflow that illustrates the power of integrating
    Python in our analytical workflows. Instead of building this workflow from scratch,
    we can find it already available in **KNIME Hub**, the online repository of workflows,
    extensions, components, and nodes. As depicted in *Figure 9.22*, open the **KNIME
    Hub** panel (you''ll find it beside the **Node Description** tab in the top right)
    and type `Python Gaussian Fit` in the search box. Among the many alternatives,
    you should find a workflow with my name and picture on it: this is the workflow
    I have prepared for you. To import it into your KNIME installation, you can just
    drag the box (highlighted in the figure) and drop it onto your workflow editor.
    An alternative approach would be to import the KNIME workflow (**File** | **Import
    KNIME Workflow...**) that you will find in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.22: The KNIME Hub panel: search for the workflows, nodes, or components
    you need and drop them onto your workspace'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open the configuration dialog of the **Python View** node, you will
    find the code window shown in *Figure 9.23*. The large text box in the middle
    is where you can write your Python code. On the left, you have the list of columns
    available in the input table: by double-clicking on them, the corresponding Python
    data frames are added to the code. You can also test your code by clicking on
    the **Execute Script** or **Execute selected lines** buttons and checking whether
    it works fine. If you receive any warnings or errors during the execution of the
    script, they will be conveniently displayed in the console box at the bottom of
    the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17125_09_23.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.23: Configuration window of the Python View node: use Python graphic
    libraries to generate any chart you like'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this specific case, we leverage the **Python View** node to fit a Gaussian
    function (the famous bell curve) to the distribution of Rome rental prices and
    return the histogram with the fitting curve. Going through the details of the
    code is not needed at this stage. However, you will notice that the *Rent* column
    has been referred to in the code with the name `input_table[''Rent'']` while the
    generated chart has been saved to the variable called `output_image`: you find
    the final result in *Figure 9.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_09_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.24: The output of the Python View node: rent prices in Rome are centered
    around €1,000'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives you an illustration of how Python nodes work: the data at the input
    port is translated into input variables, and, at the end of the script, whatever
    is assigned to the output variables gets returned at the output port of the node.
    In the same workflow you downloaded from KNIME Hub, you will also see an example
    of a **Python Script** node: essentially, both nodes run Python code on the input
    data, but the **Python View** node is "specialized" in outputting images.'
  prefs: []
  type: TYPE_NORMAL
- en: Interlacing code within a workflow has massive potential. If you want to apply
    some complex logic or reuse specialized code that has been developed outside of
    KNIME for solving your specific business need, you can now seamlessly integrate
    it all and significantly expand the power of your toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'After seeing Python in action, let''s go back to the world of codeless analytics
    and meet one of the promising directions of advanced analytics: automated machine
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Automated machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Brute-force patterns finding": this is how we can briefly (and colorfully)
    summarize what **Automated Machine Learning** or, for short, **AutoML**, is all
    about. As you saw in *Chapters 4* and *5*, building a machine learning model is
    far from being a linear, single-attempt endeavor. The usual procedure for obtaining
    high-performing supervised models is to go through a series of "back and forth"
    attempts: each time, we apply some "tuning" to the model or its features and check
    whether the predictive performance increases or not. We have seen already some
    of these mechanisms in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameters optimization**: this is when you apply changes to the way
    the learning algorithm operates, like when we activated pruning in decision trees
    or changed the degree of a polynomial regression. In more complex models (like
    in the case of deep neural networks), changing parameters (for instance, the number
    of neurons in the network) can make a significant difference to performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: by selecting a subset of features (and removing the
    redundant ones), you make your model learning focus on what matters most, increasing
    its ability to predict. We did this when we decided to remove some high p-value
    features from regression models. Additionally, making a model run on fewer features
    means saving time and computing resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: you can generate new features by combining or transforming
    the original ones to make them more informative for the model. For instance, this
    is what we did when we created dummy variables in regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking**: we said that sometimes we could combine different algorithms
    together in a single learning procedure. Think of predicting rental prices using
    five different intermediate regression models and then adopting the average of
    the five intermediate predictions as the overall prediction: by collating alternative
    models together, you might obtain a more robust one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instead of manually checking the effect of each tuning step one by one, we
    can build a procedure that leverages all the available computing power to find
    the way to the best possible model. This is what the AutoML approach promises
    to do: automating the "trial and error" process of identifying parameters, features,
    and model combinations that maximize the overall performance (and—hopefully—the
    business impact) of our machine learning procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML is currently a trending topic in business-applied AI, and there is a
    growing number of products and open-source libraries available out there for applying
    AutoML to real-world tasks, including H2O.ai, DataRobot, auto-sklearn, Google
    Cloud AutoML, IBM AutoAI, Amazon AutoGluon, and Azure AutomatedML. As we explore
    ways to expand our analytics toolbox, let''s see one of these products in action:
    this will give you an idea of what is already available and what''s to come from
    our companies in the next few years.'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore **H2O Driverless AI**, a cloud-based service that lets you use
    a web interface to upload data, run AutoML to make predictions, and interpret
    results. If you want to test it yourself, go to [h2o.ai/products/h2o-driverless-ai](http://h2o.ai/products/h2o-driverless-ai),
    register for a free account, and create an instance of Driverless AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML in action: an example with H2O.ai'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will reuse the Rome housing business case once again: this
    time, we will upload the Excel dataset and create a new experiment. *Figure 9.25*
    shows what the interface looks like: you can select the **Target Column** (*Rent*,
    in our case), pick a metric for the **Scorer** (in the figure, you can see we
    picked RMSE), and then turn the three knobs at the bottom to set the expected
    level of prediction **Accuracy**, the **Time** required for training the model,
    and the level of human **Interpretability**. This bit is fascinating: as you operate
    the knobs, the system updates its "trial and error" strategy (you can see a dynamic
    summary on the left) to be performed during the AutoML search routine. If you
    go for high accuracy and low interpretability, you will end up with high-performing
    black-box models, while if you set interpretability to a high level, you will
    obtain simpler models with fewer features so that you can explain to your business
    partners how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated with medium confidence](img/B17125_09_25.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.25: The experiment setup page in H2O Driverless AI: play with the
    knobs to determine how you would like your generated model to be cooked'
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on **Launch Experiment**, the remote computing power will do
    the hard work for you while you grab something to drink. The following view shows
    you the live evolution of the score metrics as more and more models are iteratively
    tried and, when completed, will display the best results (*Figure 9.26*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated with medium confidence](img/B17125_09_26.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.26: The results of an AutoML routine:at the bottom left, you can see
    how the scoring metric changes as the search iteration progresses'
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of the AuotML procedure, we also get some useful views that equip us
    for understanding how the model works. Have a look at the interpretation dashboard
    generated for our rental price predictions (*Figure 9.27*):'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top right, we see a bar chart displaying **Features importance**: unsurprisingly,
    *Neighborhood* is the single most useful column when predicting the rent, followed
    by the *Surface* of the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the bottom left, we have a tree-based **Surrogate model**: this is the "minimalist"
    version of the actual, full-on prediction model generated by the AutoML routine.
    By looking at the first three levels of this tree, we get a high-level, easy-to-explain
    view of the patterns that link the most important features to the rental price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the bottom right, we find the **Partial dependence** plot: this shows us
    the marginal effect of a specific feature (*Surface*, in the case of *Figure 9.27*)
    on the predicted outcome (*Rent*). This chart provides us with an additional interpretation
    key, revealing "how" the rent increases as the surface grows:![A screenshot of
    a computer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description automatically generated with medium confidence](img/B17125_09_27.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.27: The model interpretation dashboard: get some hints on how the
    prediction model works'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this example, we have admired the AutoML approach in all its potential.
    With only a few clicks, we obtained a robust predictive model (that can be exported
    and deployed for further use) and a simple framework for explaining its results.
    It''s important to make a further consideration: although it looks like the "holy
    grail" of machine learning, leveraging AutoML in a business context will still
    require its users to always "know what they are doing." This means that building
    machine learning expertise and, in general, data analytics fluency (like you did
    in this book) is and still will be crucial for making the best of this technology,
    however automated and easy to use it looks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML can be another valuable tool to keep in our data analytics kit. The
    good news is that you find this approach nicely implemented in KNIME as well,
    so you can connect it with everything else you have learned in the book. If you
    open the example workflow called H2O AutoML for Regression (you will find it in
    the Examples server in **KNIME Explorer** or by searching in **KNIME Hub**), you
    will be asked to install a new extension: **KNIME H2O Machine Learning Integration**.
    By installing this extension, you make many of the AutoML functionalities we have
    seen in H2O Driverless AI available to you in KNIME. Look at the sample workflow
    mentioned earlier (*Figure 9.28*): by employing a few H2O nodes—organized as per
    the usual supervised machine learning structure with partitioning, learner, predictor,
    and scorer—you get the full power of AutoML directly in KNIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17125_09_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.28: The H2O AutoML for Regression KNIME workflow: use AutoML to find
    the best model for you'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope this final chapter got you excited about all the directions you can take
    to further expand your data analytics toolbox. We took our first steps in Tableau
    and realized how similar it is, in its fundamental features, to Power BI. We have
    also gone through a friendly introduction to Python, the ubiquitous programming
    language in data science. As we integrated Python in KNIME, we have seen how to
    take the best from both the visual and coding programming worlds. As we did so,
    we took the opportunity to learn how to expand KNIME further by using its vast
    extensions base and leveraging the public KNIME Hub environment. Lastly, we got
    a quick tour through the attractive land of AutoML, being exposed to its promising
    ability to simplify the process of building high-performing machine learning models
    considerably.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we extended our toolbox by exploring new tools and approaches
    to run better data analytics in our everyday work. My advice is to make this a
    habit. One limitation I have seen in many data practitioners is to think that
    the few tools they feel comfortable with will *always* be the best for them, falling
    in the limiting bias of self-sufficiency. So instead, don''t feel satisfied with
    the toolbox we have just built—be ready to explore continuously: stay curious,
    as the expanding world of data analytics will have a lot to offer!'
  prefs: []
  type: TYPE_NORMAL
