- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Characterizing and Collecting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we focused on general concepts and ideas around probability
    and statistics, but how does this translate to the data within your organization
    or for your project?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover different types of data you might find within
    your organization, methods for collecting and processing that data to apply the
    statistical techniques covered in the previous chapter, and more advanced machine
    learning and deep learning techniques we will cover in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into topics such as the different categories of data and methods
    for collecting, storing, and processing data, we need to ask a fundamental question:'
  prefs: []
  type: TYPE_NORMAL
- en: “What data in my organization is valuable and useful?”
  prefs: []
  type: TYPE_NORMAL
- en: Initially, this might seem like a trivial and obvious question, but many data
    science projects start on the wrong foot by not properly evaluating the feasibility
    of achieving business results with the data available.
  prefs: []
  type: TYPE_NORMAL
- en: Often, decision-makers incorrectly assume that the data can be used for the
    identified business use case.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of data out there and not all data is created equally, so it
    is worth understanding whether it fits the criteria for your business use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about different types of data, as well as methods
    for collecting and processing data so that it can be prepared for data science
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you a clearer idea of how to unlock the valuable data within, or external
    to, your organization, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The key criteria to consider when evaluating data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between first-, second-, and third-party data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between structured, unstructured, and semi-structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technologies and methods for collecting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technologies and methods for storing and processing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can my team navigate the landscape of data-focused solutions, including
    cloud, on-premises, and hybrid solutions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To better understand how to unlock the valuable data within, or external to,
    your organization, let’s explore the key criteria to consider when evaluating
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: What are the key criteria to consider when evaluating datasets?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will understand what the key criteria are when it comes
    to evaluating datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data quantity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is there sufficient data to train an accurate model or to make inferences about
    a wider population if you’re working with a data sample? As mentioned in the previous
    chapter, in statistics, you must often work with a limited sample of data, and
    the ability of that sample to represent the wider population often depends on
    the size of the sample. Within machine learning, models trained on larger datasets
    perform much better than those trained on a small sample. There are more advanced
    techniques, such as data augmentation and transfer learning, that can help in
    this situation and will be covered later, but an initial consideration is whether
    there is enough data available to meet business requirements around accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for instance, a customer churn model designed to predict which customers
    are at risk of leaving. To effectively generalize to current and future customers,
    it’s important to assess whether there is sufficient historical data and an adequate
    number of examples of customers closing their accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Data velocity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another consideration is data velocity, which is how frequently data is generated
    or travels. Some data may not update very frequently, such as a company’s quarterly
    profit and loss, whereas other data may be very frequent, or even real time, such
    as stock prices. The solutions that data teams build need to be able to handle
    the frequency of the incoming data. For example, batch systems might need to process
    data daily or streaming systems (such as Kafka) might be required for real-time
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The velocity or frequency of the data required also depends on the business
    use case. For example, a public relations issue tracker that monitors news and
    social media sentiment may require data every minute to detect emerging issues
    quickly. Another example would be predictive maintenance, where data outputs from
    IoT sensors within manufacturing equipment would benefit from being streamed in
    real time to detect issues and defects so that they can be resolved quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Data variety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For some use cases, a single source of data will not be sufficient to make accurate
    inferences about a population or provide compelling insights to the end users.
    One industry where this is often the case is market research, where the data from
    a single survey may not provide the in-depth insights around consumer behavior
    the brand or company is looking for, and combining this data with additional data
    sources, such as product reviews, consumer social media data, or sales data, will
    provide the context to understand consumer behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, one of the key criteria to evaluate is the quality of the data. Data
    quality and governance is a whole domain within itself, but some of the things
    you must consider are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Data is accurate when it reflects reality. This could refer to
    the value of financial transactions accurately reflecting the exact amount spent,
    consumer survey responses reflecting the respondents’ true opinions, or customer
    details within a CRM being accurately filled for a given customer. Data should
    not always be assumed to be accurate, and high data accuracy can allow models
    to be trained and inferences to be made that can also be trusted. The expression
    “garbage in, garbage out” is often used to describe this fact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: Data is “complete” when all fields required for a particular
    use case are present. It doesn’t necessarily mean that all fields need to be complete.
    For example, if a company is looking to understand the average price of products
    from different vendors, then fields such as price, quantity, and pack size are
    important to complete, but other fields, such as ingredients, product descriptions,
    or product image URLs, are not as important for this use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniqueness**: Data is unique if it appears only once within a given dataset.
    Sometimes, duplicates are expected. For example, in a set of financial transactions,
    the same customer can appear many times across transactions, and this is nothing
    to worry about. However, in other situations, such as within a master list of
    customers within a CRM, each customer should be unique, and having duplicates
    can lead to inaccurate reporting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**: Consistency is an important factor when it comes to data as
    it ensures that the values within a single record and across multiple datasets
    do not conflict with each other. For example, a postcode should always begin with
    the same characters that represent the locality of the address, and the date of
    birth for any given individual should be the same across different datasets. By
    ensuring consistent data, it is possible to link information from multiple sources.
    This can add to your dataset and increase its value by providing additional insight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timeliness**: Relevant to the data velocity criteria, the timeliness of data
    is also a consideration for data quality. More recent data may be required to
    accurately represent the current-day situation. Take, for example, a loan default
    model a bank may have to predict the probability of customers defaulting on loans.
    If this model was trained on old historical data where the macroeconomic environment
    was different, and interest rates were lower, then the data that’s used for training
    may not be timely enough to make accurate inferences about current customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validity**: Validity is a measure of how well data conforms to the expected
    format, type, and range. For example, a valid US postcode must be within the range
    of 00001 to 99950, and a valid email address must consist of an email prefix and
    an email domain separated by an “@” symbol. Often, tools such as regular expressions
    (a way of checking that data matches a certain pattern) or cross-referencing against
    standard datasets (such as ISO standards) can be used to ensure data is valid.
    Having valid data means that it can be used in harmony with other sources, and
    it helps to ensure that automated data processes run efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discussed the key criteria for evaluating datasets, it’s important
    to understand that data can come from various sources, such as first parties,
    second parties, and third parties. Let’s look into these different types of data.
  prefs: []
  type: TYPE_NORMAL
- en: First-, second-, and third-party data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within any mid to large-sized organization, you can find a vast swathe of different
    types of data that serve unique purposes and can provide valuable insights when
    harnessed effectively. Even if your organization does not have easily accessible
    or useful internal data to suit the business use cases you have identified, it
    is possible to look outside your organization toward external data sources. This
    is where the concepts of first-, second-, and third-party data are useful to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents how your data (darker) interacts with data
    from external organizations (lighter) for the definitions of first-, second-,
    and third-party data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First-party data** is data that’s internal to your organization, such as
    customer data or employee data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second-party data** is data that’s shared as part of a data-sharing partnership
    or agreement with another organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third-party data** is data that’s collected from external sources, such as
    proprietary or open source APIs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.1: How first-, second-, and third-party data interacts with external
    organizations](img/B19633_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: How first-, second-, and third-party data interacts with external
    organizations'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at each.
  prefs: []
  type: TYPE_NORMAL
- en: First-party data – the treasure trove within
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will begin with first-party data – the data that your organization
    collects directly from its customers, users, or other sources. Have you ever considered
    the wealth of information that lies within your systems? Customer interactions,
    sales transactions, website analytics, and even employee records are all examples
    of first-party data. This data is invaluable as it provides organizations with
    a unique perspective on their customers, products, and services.
  prefs: []
  type: TYPE_NORMAL
- en: But why is first-party data so important? The answer lies in its accuracy, relevance,
    and control. Since it is collected directly from the source, it is often considered
    the most reliable and accurate type of data. Furthermore, this data is inherently
    relevant to your organization’s specific needs and goals. Lastly, your organization
    maintains full control over its first-party data, ensuring compliance with privacy
    regulations and minimizing potential data breaches.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, the insights gained from analyzing customer purchase
    history or website behavior. How can these insights inform marketing strategies,
    product development, or even customer support? By effectively harnessing first-party
    data, organizations can unlock a treasure trove of insights and opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: Second-party data – building bridges through collaboration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will cover second-party data, which is essentially another organization’s
    first-party data that is shared or purchased for mutual benefit. While this may
    initially seem counterintuitive, sharing data can lead to fruitful collaborations
    and partnerships, expanding your organization’s reach and knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: What makes second-party data valuable? The answer lies in its exclusivity and
    potential for collaboration. Second-party data can provide unique insights that
    are not readily available in the public domain, giving your organization a competitive
    edge. Furthermore, the process of sharing data can lead to collaborative opportunities
    and foster strategic partnerships.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, for example, a retail company collaborating with a payment processing
    platform to better understand customer spending habits and preferences. By sharing
    data, both organizations stand to benefit from enhanced insights and informed
    decision-making. However, it is crucial to ensure that all parties involved adhere
    to strict data privacy and security standards when sharing second-party data.
  prefs: []
  type: TYPE_NORMAL
- en: Third-party data – broadening horizons with external expertise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, let’s consider third-party data, which is collected by organizations
    that specialize in aggregating and selling data to other businesses. This data
    can offer insights into market trends, demographics, and other valuable information
    that may be difficult or time-consuming to collect internally.
  prefs: []
  type: TYPE_NORMAL
- en: Why should organizations consider third-party data? The answer lies in its breadth,
    specialization, and potential for uncovering new opportunities. Third-party data
    can provide access to a wide range of datasets, including industry-specific information,
    demographic data, and location data. These providers often possess specialized
    expertise in collecting and analyzing data, ensuring high-quality insights.
  prefs: []
  type: TYPE_NORMAL
- en: Picture, for example, a company in the fast-moving consumer goods industry looking
    to expand its product offerings. By acquiring third-party data on consumer preferences
    and market trends, the company can make informed decisions regarding product development
    and marketing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Another example might be a company looking to understand how companies are talking
    about their brand online, through social listening. This would involve collecting
    and analyzing third-party social media data from sources such as X (formerly Twitter)
    and Reddit.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note, however, that the quality of third-party data can vary,
    and organizations must carefully evaluate the reliability and accuracy of the
    data they purchase.
  prefs: []
  type: TYPE_NORMAL
- en: While categorizing data based on its source is essential, it’s also important
    to understand how data can be structured differently. In the next section, we’ll
    explore the differences between structured, unstructured, and semi-structured
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Structured, unstructured, and semi-structured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with data from data sources, how can you usefully categorize them?
    There are three broad categories of data: structured, unstructured, and semi-structured.'
  prefs: []
  type: TYPE_NORMAL
- en: As a decision-maker, it is useful to understand the nuances and applications
    of structured, unstructured, and semi-structured data to make informed decisions
    regarding data storage, management, and analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Structured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured data, which is organized in a specific format such as relational
    databases, is easily searchable and analyzable. This type of data can include
    a wide range of information, such as customer names, addresses, ages, and transaction
    amounts, to name a few. The advantage of structured data is that it is well-defined
    and easier to use by data scientists and engineers, often requiring less pre-processing
    than other forms of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: An example of structured data in a SQL table](img/B19633_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: An example of structured data in a SQL table'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, unstructured data, which lacks a predefined format or organization,
    encompasses a wide array of information, including documents, emails, social media
    posts, images, and videos. Value can be gained from unstructured data. For example,
    you can analyze e-commerce product reviews, emails, social media posts, or legal
    contracts to identify patterns or insights or analyze images or videos for applications
    such as quality control in manufacturing.
  prefs: []
  type: TYPE_NORMAL
- en: There has been a huge amount of progress in deep learning techniques and their
    application to natural language data via **natural language processing** (**NLP**),
    including **large language models** (**LLMs**) such as GPT-4\. There has also
    been great progress in applying deep learning models to image and video data (**computer
    vision**). This explosion in capability means that unstructured data, while sometimes
    neglected by large, slow-moving organizations due to its relative complexity compared
    to traditional structured data, is more valuable to organizations than ever before.
    Other formats of unstructured data, such as audio data and sensor data, can also
    be analyzed through the use of deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover some of the applications of NLP and computer vision in more detail
    later in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: An example of unstructured data in the form of a legal contract](img/B19633_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: An example of unstructured data in the form of a legal contract'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-structured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Semi-structured data bridges the gap between structured and unstructured data,
    offering a more flexible approach to data organization. While it does not adhere
    to the rigid structure of data models typically associated with relational databases,
    it contains tags or markers that help in organizing the data. Examples of semi-structured
    data include XML, JSON, or HTML files, all of which are common data standards.
    Web scraping can be employed to collect this type of data, such as product prices
    and descriptions, from multiple websites for competitive analysis. Additionally,
    data integration projects can benefit from the adaptability of semi-structured
    data when combining information from different sources or systems. IoT devices
    often generate semi-structured data, which can be used to monitor and optimize
    performance in various areas, such as energy usage, manufacturing, or transportation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: An example of semi-structured data in the form of an HTML web
    page file](img/B19633_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: An example of semi-structured data in the form of an HTML web page
    file'
  prefs: []
  type: TYPE_NORMAL
- en: To successfully navigate the world of data, decision-makers should consider
    some key takeaways.
  prefs: []
  type: TYPE_NORMAL
- en: First, understand the types of data your organization works with so that you
    can make informed decisions about data storage, management, and analytics. Second,
    make use of structured data for traditional business analytics while employing
    unstructured and semi-structured data for more complex analyses, such as NLP or
    computer vision. Finally, foster collaboration between data scientists, IT professionals,
    and business stakeholders to ensure the efficient understanding, storage, and
    use of the different types of data in your organization’s data-driven initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: By comprehending the distinctions and applications of structured, unstructured,
    and semi-structured data, you will be better prepared to lead your organization’s
    data science, machine learning, or AI initiatives toward success.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a clear understanding of the different types and structures
    of data, the next step is to explore the various methods for collecting data that
    will best serve your organization’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for collecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine standing at the edge of a vast field and you wish to discover hidden
    treasures buried beneath the surface. You know the treasures are there, waiting
    to be found, but how do you go about finding them? This is the challenge that
    organizations face when it comes to collecting data. Data is the lifeblood of
    data-driven decision-making, and the process of collecting it is as crucial as
    the insights it can ultimately provide. But how do you collect data that is accurate,
    relevant, and valuable? How do you ensure the data you gather will help you make
    informed decisions?
  prefs: []
  type: TYPE_NORMAL
- en: As a decision-maker, understanding the methods and best practices for collecting
    data is essential to maximizing the potential of data science within your organization.
    In this section, we will explore data collection, its challenges, and its opportunities
    to help you establish a solid foundation for your data-driven journey.
  prefs: []
  type: TYPE_NORMAL
- en: How do you know which type of data is right for your organization? How do you
    ensure that the data you collect is relevant, accurate, and reliable?
  prefs: []
  type: TYPE_NORMAL
- en: The key to answering these questions lies in understanding your organization’s
    unique needs and objectives. Begin by identifying the specific questions you want
    to answer, the problems you want to solve, or the decisions you want to make.
    For example, a retailer might want to understand customer preferences to optimize
    their product offerings, while a hospital might want to study patient outcomes
    to improve the quality of care. Once you have a clear understanding of your goals,
    you can determine the type of data that will best serve your needs and guide your
    data collection efforts.
  prefs: []
  type: TYPE_NORMAL
- en: When collecting data, it’s essential to choose the right data collection method
    for your specific objectives. Within market research, surveys, interviews, focus
    groups, and observations are some of the most used methods, each with its strengths
    and limitations. Surveys, for instance, can provide a wealth of quantitative data
    from a large sample of respondents but may lack the depth and nuance of qualitative
    data obtained from interviews or focus groups. As you select a data collection
    method, consider factors such as the scale of your research, the resources available,
    and the level of detail you require.
  prefs: []
  type: TYPE_NORMAL
- en: You may look at collecting data from other parts of your organization or externally
    via methods such as data transfer services, **application programming interfaces**
    (**APIs**) (a way for different applications to efficiently talk to each other),
    or more indirect methods of collecting data, such as web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment to ponder the challenges you might face when collecting data.
    Is there a large enough sample to be representative of the total population? What
    are the potential sources of bias or error in your data collection process? How
    can you ensure the data you collect is representative and reliable? One of the
    most critical aspects of data collection is ensuring that your sample is diverse
    and representative of the population you are studying.
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful of potential sampling biases that could skew your results, such as
    **nonresponse bias** or **self-selection bias**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Nonresponse bias** occurs when certain groups of people are less likely to
    respond to your survey or participate in your study, leading to an unrepresentative
    sample. **Self-selection bias** happens when individuals voluntarily choose to
    participate in a study, and these self-selected participants may differ from the
    general population in important ways.'
  prefs: []
  type: TYPE_NORMAL
- en: To minimize these biases, consider using probability sampling methods such as
    random sampling, which we discussed in the previous chapter. Follow up with non-respondents
    to encourage participation and analyze differences between respondents and non-respondents.
    Additionally, avoid relying solely on voluntary participation and actively recruit
    participants from diverse backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve collected this data using the appropriate methods, the next critical
    step is to store and process it so that you can extract meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: Storing and processing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you were to walk into a library, you would find hundreds of books lining
    the shelves, each containing valuable knowledge. Now, think of how difficult it
    would be to find the exact book you need if they were all scattered randomly on
    the floor. This is the challenge that businesses face when it comes to storing
    and processing data. Organizing, categorizing, processing, and labeling data are
    essential steps in turning raw information into valuable insights that can drive
    effective decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: As a decision-maker, understanding how data is stored and processed will empower
    you to unlock the full potential of data science in your organization. So, how
    do you ensure your company’s data is properly stored and processed to facilitate
    accurate and actionable insights? What are the best practices to make data easily
    accessible, interpretable, and actionable for your team? Let’s dive into the world
    of data storage and processing to answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Picture your company’s data as a vast ocean of information, constantly growing
    and changing. To navigate this ocean, you need a robust and reliable storage system
    that can handle the volume, variety, and velocity of the data. There are numerous
    data storage options available, from traditional databases such as SQL and NoSQL
    to cloud-based storage solutions such as Amazon S3 and Google Cloud Storage. How
    do you choose the right option for your organization? What factors should you
    consider when selecting a data storage system? The answers lie in understanding
    your data’s characteristics, your organization’s unique requirements, and the
    storage system’s capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Types of data storage and databases](img/B19633_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Types of data storage and databases'
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Relational databases have long been the bedrock of business data management
    systems thanks to their organized structure of tables, rows, and columns that
    facilitate efficient querying and data retrieval using SQL. They are a tried-and-true
    solution, with widespread adoption making it easier to find resources and talent.
    Their suitability for managing structured data and complex queries, coupled with
    their compliance with ACID principles, make them a solid choice for ensuring data
    integrity and consistency. However, relational databases can face challenges when
    it comes to scaling horizontally, particularly with large datasets, and they’re
    not designed to handle unstructured or semi-structured data. Some well-known examples
    of relational databases include MySQL, PostgreSQL, and Microsoft SQL Server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Relational databases](img/B19633_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Relational databases'
  prefs: []
  type: TYPE_NORMAL
- en: Object storage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In contrast, object storage offers a flexible solution for managing unstructured
    data, such as images, videos, and large documents. Object storage handles data
    as objects rather than files or blocks, allowing for easy scalability and cost-effective
    storage, particularly for long-term needs. Object storage shines in distributed
    systems and cloud-based environments, though it isn’t designed for structured
    data management or complex queries, and its performance might lag compared to
    block storage. Amazon S3, Google Cloud Storage, and Microsoft Azure Blob Storage
    are prime examples of object storage solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Object storage](img/B19633_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Object storage'
  prefs: []
  type: TYPE_NORMAL
- en: Document databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Document databases provide a versatile approach to storing data and handling
    semi-structured data and they offer a more flexible schema than relational databases.
    They store data as documents, supporting horizontal scaling and distributed systems.
    While document databases are powerful, they are less mature than their relational
    counterparts, and they may not be the best fit for complex relational queries.
    MongoDB, Couchbase, and Amazon DocumentDB are popular examples of document databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Document databases](img/B19633_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Document databases'
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For applications requiring intricate data relationships, graph databases excel
    in representing and querying complex connections between data entities. They employ
    graph structures comprising nodes, edges, and properties, making them an excellent
    choice for social networks, recommendation systems, and fraud detection applications.
    However, graph databases are less mature compared to relational databases and
    may not be the best option for use cases that don’t require complex relationships.
    Some well-known graph databases include Neo4j and Amazon Neptune:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: Graph databases](img/B19633_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Graph databases'
  prefs: []
  type: TYPE_NORMAL
- en: Key-value databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Key-value databases focus on simplicity and speed, storing data as key-value
    pairs. They are best suited for use cases that require data retrieval based on
    a single key and don’t necessitate complex querying. With fast and efficient low-latency
    performance, they easily scale and are ideal for caching and real-time applications.
    Their limitations lie in their querying capabilities and inability to handle complex
    relationships or data models. Redis, Amazon DynamoDB, and Riak are examples of
    key-value databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: Key-value databases](img/B19633_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Key-value databases'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data warehouses are centralized repositories that are designed to store and
    manage vast amounts of structured data from various sources, enabling businesses
    to perform complex queries, generate reports, and derive valuable insights. Unlike
    the **online transaction processing** (**OLTP**) relational databases mentioned
    earlier, which are optimized for real-time transactional processing and frequent
    updates, data warehouses are built for read-intensive operations and complex analytics
    workloads. They follow a schema-on-write approach, where data is transformed and
    structured before being loaded, ensuring data consistency and quality. Data warehouses
    are useful for supporting data-driven decision-making and are designed to handle
    large-scale data processing and analytics, making them an essential component
    of modern business intelligence and data science ecosystems. Some well-known examples
    of data warehouses include Google BigQuery, Amazon Redshift, Snowflake, and Azure
    Synapse Analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: Data warehouses](img/B19633_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Data warehouses'
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vector databases are a new type of database that stores data as high-dimensional
    vectors, which are numerical representations of data points in a multi-dimensional
    space. Unlike traditional databases, which handle structured data such as tables
    and rows, vector databases excel at managing unstructured data such as text, images,
    and audio. They enable fast and accurate similarity searches, making them ideal
    for powering advanced applications such as recommendation systems, semantic search,
    and question-answering. When combined with LLMs through techniques such as **retrieval-augmented
    generation** (**RAG**), vector databases help deliver highly relevant and contextual
    results by allowing the LLM to quickly access the most semantically similar information.
    This powerful combination is revolutionizing many NLP and information retrieval
    tasks. Key players in the vector database space include pgvector, Pinecone, Milvus,
    and Weaviate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: Vector databases](img/B19633_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Vector databases'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the various database types, their advantages and disadvantages,
    and their use cases will empower decision-makers to choose the most appropriate
    database technology for their organization’s unique needs.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud, on-premises, and hybrid solutions – navigating the data storage and analysis
    landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As executives and decision-makers, understanding the various options for data
    storage, analysis, and machine learning is critical to the success of your organization’s
    data-driven initiatives. In this chapter, we will explore the advantages and challenges
    of cloud, on-premises (on-prem), and hybrid approaches, delving into their unique
    applications and impact on business decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing – scalable services in the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine having access to virtually unlimited computing resources, scalable storage,
    and advanced analytics capabilities without having to invest in expensive infrastructure
    or manage complex hardware. This is the promise of cloud computing, a paradigm
    that enables organizations to store, process, and analyze data using remote servers
    hosted on the internet. The cloud has revolutionized the way businesses approach
    data science, machine learning, and artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: But why is cloud computing so appealing to organizations? The answer lies in
    its flexibility, cost-effectiveness, and ease of use. Cloud platforms offer the
    ability to scale resources up or down as needed, ensuring that organizations only
    pay for what they use. Additionally, cloud providers maintain, secure, and update
    their infrastructure, freeing up valuable time for your IT staff. Furthermore,
    the cloud enables seamless collaboration, allowing teams to access data and analytics
    tools from virtually anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, a company that wants to analyze vast amounts of customer
    data to improve its marketing strategies. By leveraging cloud-based machine learning
    tools and storage, the company can quickly and cost-effectively process and analyze
    the data, gaining valuable insights without incurring excessive costs or overwhelming
    its on-premises infrastructure. Cloud providers often have different storage and
    compute tiers, as well as the ability to scale up and scale down services as needed,
    allowing the customer to use only what they need and save cost. Many cloud providers
    also have out-of-the-box solutions for complex AI tasks, such as speech-to-text
    transcription (for example, Amazon Transcribe) or translation (for example, Amazon
    Translate), which can save customers time compared to building solutions in-house.
  prefs: []
  type: TYPE_NORMAL
- en: On-premises – maintaining control within your walls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the numerous advantages of cloud computing, some organizations still
    prefer to keep their data and analytics infrastructure on-premises. On-premises
    solutions involve housing data storage, processing, and analysis tools within
    the organization’s data centers or facilities.
  prefs: []
  type: TYPE_NORMAL
- en: Why would an organization choose on-premises over cloud computing? The answer
    lies in control, security, and customization. On-premises solutions allow organizations
    to maintain complete control over their infrastructure, data, and applications.
    This can be particularly important for companies with strict security or compliance
    requirements or those that handle sensitive data. Additionally, on-premises solutions
    offer the potential for greater customization, enabling organizations to tailor
    their infrastructure to their specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a financial institution that deals with sensitive customer data and
    must adhere to stringent regulations. In this case, an on-premises solution may
    be preferable as it allows the organization to maintain control over its data
    and ensure compliance with industry standards.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid – the best of both worlds?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For some organizations, the choice between cloud and on-premises is not black
    and white. Instead, they opt for a hybrid approach, which combines elements of
    both cloud and on-premises infrastructure. Hybrid solutions enable organizations
    to leverage the benefits of both paradigms, providing flexibility, scalability,
    and control.
  prefs: []
  type: TYPE_NORMAL
- en: Why should an organization consider a hybrid approach? The answer lies in its
    versatility and adaptability. Hybrid solutions allow organizations to maintain
    control over sensitive data on-premises while also taking advantage of the scalability
    and cost-effectiveness of cloud-based resources for less sensitive or resource-intensive
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Envision a healthcare organization that must securely store patient records
    while also processing large volumes of medical research data. A hybrid approach
    allows the organization to keep sensitive patient data on-premises while utilizing
    cloud-based resources for computationally intensive research tasks, effectively
    balancing security and performance.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the various types of databases, their applications, and the
    pros and cons of cloud, on-premises, and hybrid solutions, you and your team can
    make informed decisions about data storage and processing that best serve your
    business use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting and storing data on the right infrastructure isn’t the end of the
    story. Data is only useful once it has been processed, analyzed, modeled, and
    used for business purposes. In the next section, we will discuss data processing.
    Later in this book, we will look into data analytics and machine learning so that
    you can gain further value from your data.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your data has been stored securely, the next step is processing it to extract
    meaningful insights. Data processing involves cleaning, transforming, and analyzing
    data to make it suitable for data science and decision-making. But how do you
    ensure that your data is processed accurately and efficiently? What tools and
    techniques can you use to transform raw data into valuable information?
  prefs: []
  type: TYPE_NORMAL
- en: 'Data processing typically involves three main stages: data preparation, data
    transformation, and data analysis. During the data preparation phase, your data
    is cleansed, and any inconsistencies, errors, or missing values are addressed.
    This is an important step to ensure that your subsequent analyses are based on
    accurate and reliable data. There are many proprietary and open source solutions
    to help you and your team in this process. It is also often helpful to involve
    subject matter experts from the business who can identify and help rectify any
    issues in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, data transformation involves converting data into a suitable format for
    further analysis. This may involve aggregating data, normalizing variables, or
    encoding categorical variables, among other tasks. Consider a retail company that
    wants to analyze sales data to identify trends and make informed decisions. The
    raw sales data may include transaction-level information such as customer names,
    product IDs, and purchase amounts. To make sense of this data, it needs to be
    transformed into a format that can be easily interpreted and analyzed, such as
    aggregating sales by product category or calculating average purchase amounts
    per customer.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the data analysis and modeling stage involves using statistical and
    machine learning techniques to uncover patterns, relationships, and trends in
    the data. This will be the subject of subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the landscape of data collection, storage, processing, analysis,
    and machine learning is no small task. However, understanding the different sources
    and categories of data, various databases, and the pros and cons of cloud, on-premises,
    and hybrid solutions will equip you to make informed decisions and better understand
    the data landscape within your organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider these key questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the key criteria when evaluating data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the difference between first, second, and third-party data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the difference between structured, unstructured, and semi-structured
    data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the methods for collecting data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the methods for storing and processing data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you navigate the landscape of data solutions, and what are the pros
    and cons of each?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This knowledge will help you and your team make the right decisions about data
    collection and technology to best serve your business use cases and gain tangible
    value from your data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a solid understanding of the data landscape, the next step
    is to explore and understand the data you have gathered through **exploratory
    data analysis** (**EDA**). EDA allows you to summarize the main characteristics
    of your datasets, often using visual methods, and develop a deeper understanding
    of the patterns, trends, and potential issues within your data before proceeding
    with more advanced analysis or modeling.
  prefs: []
  type: TYPE_NORMAL
- en: EDA will help you uncover valuable insights, identify potential biases or anomalies,
    and communicate your findings effectively to stakeholders. This foundational knowledge
    will empower you to make data-driven decisions with confidence and lay the groundwork
    for successful machine learning and statistical modeling projects.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to the next chapter and see how EDA can help you unlock more of
    the potential of your data.
  prefs: []
  type: TYPE_NORMAL
