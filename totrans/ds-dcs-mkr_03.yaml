- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered methods for characterizing and collecting
    data. So, now that you have collected some data, what will you do with it? Well,
    that is the topic of this chapter. In this chapter, we will learn about the process
    of **exploratory data** **analysis** (**EDA**).
  prefs: []
  type: TYPE_NORMAL
- en: EDA is an approach to analyzing datasets so that you can summarize their main
    characteristics, often with visual methods. It is used to understand data, get
    a context of it, develop more hypotheses, and consequently build better models
    and business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will get a bit more hands-on, with code examples that you
    can try out.
  prefs: []
  type: TYPE_NORMAL
- en: If you would rather focus on reading the content, feel free to skip the code
    exercises. They are completely optional and they’ve been written to help reinforce
    some of the things we’ll be learning about in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Also, don’t worry if you have never used Python before; every exercise will
    be explained step by step and assumes no prior experience with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Google Colab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the data you have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA techniques and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive in by learning how to set up a code environment so that you can follow
    the exercises laid out in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Google Colab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help you gain a better understanding of the different data science techniques
    that will be covered in this chapter, there will be some hands-on exercises you
    can complete with Python.
  prefs: []
  type: TYPE_NORMAL
- en: To set everything up, we will be using **Google Colab** as it is an easy place
    to get started if you haven’t used Python before.
  prefs: []
  type: TYPE_NORMAL
- en: What is Google Colab?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Colaboratory, commonly known as Google Colab, is a free cloud service
    that provides an environment where you can run Python code. It’s like having a
    powerful computer right in your browser, which is particularly useful for data
    science tasks, including but not limited to statistics, machine learning, and
    **natural language processing** (**NLP**). You don’t need to install anything,
    and it’s available on any device with internet access.
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to setting up Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to set up Colab with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need a Google account. If you don’t already have one, you can create
    one here: [https://accounts.google.com/signup](https://accounts.google.com/signup).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, open your preferred web browser and go to the Google Colab website: [https://colab.research.google.com/](https://colab.research.google.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Sign In** button in the top-right corner and log in with your
    Google account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you’ve signed in, you’ll see a page with a menu at the top. Click **File**,
    then choose **New Notebook** from the dropdown. This will open a new tab that
    contains your fresh notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, your notebook will be named `Untitled0.ipynb`. You can change this
    by clicking on the name at the top of the page. A dialog box will appear where
    you can enter your desired name. A good idea would be to name each notebook by
    specifying the number and name of the chapter (for example, `Chapter 3` `– Exploratory`
    `Data Analysis.ipynb`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the notebook, you’ll see a cell with a play button (it looks like a right
    arrow). Click on the cell to activate it, type `print("Hello, world!")`, and then
    click the play button to run the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To save your notebook, you can click **File** in the menu and choose **Save**,
    or simply use the *Ctrl* + *S* (Windows/Linux) or *Cmd* + *S* (Mac) keyboard shortcut.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And that’s it! You’re now ready to start your journey into data science using
    Python on Google Colab. As you read through this book, you’ll find Google Colab
    notebooks that have been prepared for you to practice your skills while considering
    real-world examples concerning data science and machine learning. Happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s get back to the topic of this chapter: **EDA**.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data you have
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have gone through the process of collecting and storing data, it can
    be tempting to dive straight into the more interesting and exciting work of training
    machine learning models or building dashboards to present to your customers or
    stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: However, an important stage before model training or presenting results is to
    explore and understand the data you have, as well as its main characteristics,
    patterns and trends in the data, and potential anomalies or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: EDA is a fundamental step in the data analysis process that involves systematically
    examining datasets to understand their main characteristics, identify patterns
    and trends, and uncover potential anomalies or outliers. EDA typically precedes
    more formal statistical or machine learning modeling, and its primary goal is
    to provide insights and context that will inform further analysis and model development.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of EDA cannot be overstated. It not only helps decision-makers
    develop a better understanding of their data but also assists in identifying potential
    issues or biases in the data that could impact the accuracy and reliability of
    subsequent analyses. Furthermore, EDA enables the development of more meaningful
    visualizations and representations of data that can be easily communicated to
    stakeholders and team members.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the importance of EDA, let’s explore the various tools
    and techniques at our disposal for performing it.
  prefs: []
  type: TYPE_NORMAL
- en: EDA techniques and tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are numerous EDA techniques and tools available to data scientists, analysts,
    and decision-makers.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most used methods for EDA are mentioned in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest form of EDA involves calculating the summary statistics we covered
    in the previous chapter, such as the mean, median, mode, standard deviation, and
    range, to provide an initial understanding of the data’s central tendencies and
    dispersion.
  prefs: []
  type: TYPE_NORMAL
- en: Code example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will show you an example of how to calculate the mean, median, mode,
    standard deviation, and range for an example dataset showing monthly sales figures
    for a year.
  prefs: []
  type: TYPE_NORMAL
- en: For each code snippet, you can copy and paste it into Google Colab and press
    *Shift* + *Enter* to run them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your code editor and run the following code to calculate the mean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The expected output is `"The average monthly sales across the year is` `14667
    units."`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, calculate the median (middle value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The expected output is `"The median monthly sales, a typical sales month, is`
    `15000 units."`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, calculate the standard deviation (a measure of the amount of variation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The expected output is `"The standard deviation, showing the typical variation
    from the mean sales, is` `2015 units."`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, calculate the mode (the most common value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The expected output is `"The most common monthly sales volume is` `15000 units."`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, calculate the range (the difference between the maximum and the minimum):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The expected output is `"The range of monthly sales volumes is` `7000 units."`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s calculate the same statistics for another year’s data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What can you say about the 2 years’ monthly sales figures?
  prefs: []
  type: TYPE_NORMAL
- en: Before moving ahead, think about what you could conclude from these summary
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Did you observe any of the differences mentioned here?
  prefs: []
  type: TYPE_NORMAL
- en: The average sales seem to have increased from year 1 to year 2, which is good
    news for the business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The median sales also increased, suggesting a general shift in sales rather
    than them just being influenced by a few high sales months
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard deviation is higher in year 2, indicating that sales were more
    variable or dispersed that year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common monthly sales volume changed from year 1 to year 2, indicating
    a shift in sales performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The range of sales volumes has also increased, showing that the spread in sales
    volumes is larger in year 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another useful descriptive statistic that is useful when profiling data is the
    proportion of missing or null data. For example, if you had a dataset of customers
    from a **customer relationship management** (**CRM**) system, you might want to
    know the proportion of customers who had empty “Company Name” or “Job Title” fields.
    This can help you understand where there are gaps in data that can be filled with
    better data collection or annotation, or that should be treated with care when
    used as features when training machine learning or statistical models.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond descriptive statistics, a useful way to explore data is by visualizing
    it. The use of graphical representations such as histograms, boxplots, scatter
    plots, and heatmaps can help identify patterns, trends, and outliers in data.
  prefs: []
  type: TYPE_NORMAL
- en: These visualizations can easily be created using dedicated dashboard software
    such as Microsoft’s PowerBI and Tableau or Python data visualization packages
    such as `matplotlib` and `plotly`.
  prefs: []
  type: TYPE_NORMAL
- en: Code example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following our previous calculation of summary statistics, we can also visualize
    the sales data for better insights. The `matplotlib` library in Python provides
    an excellent platform for us to create different types of data visualizations.
    Here, we will use a bar plot to represent the sales for each month and line charts
    to indicate the mean and median sales.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your code editor and run the following code. This builds upon the code
    provided in the previous exercise, so make sure you complete that exercise first,
    in the same notebook, before running this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The charts should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Units sold for year 1 and year 2 by month](img/B19633_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Units sold for year 1 and year 2 by month'
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the bar plot, we can observe the sales figures for each month.
    The red line represents the mean, while the blue line represents the median. As
    you can see, year 2 had higher sales figures, which is consistent with the calculations
    from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: With visualizations, it becomes easier to observe trends, fluctuations, and
    other characteristics of your dataset that may not be apparent from the raw numbers
    alone. For instance, in year 2, you may have noticed a certain fluctuation in
    the sales figures, which is causing a higher standard deviation, or you might
    have observed that the mean and median are more spread apart.
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that visualizing your data is a key step in the data exploration
    process. It allows you to better understand the data before you apply more complex
    analyses or modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A histogram is a simple and easy-to-understand visual tool that helps us see
    how data is spread out and organized. Think of it as a bar chart where each bar
    represents a range or group of data, called a “bin.” The height of each bar shows
    how many data points fall into that specific range. The higher the bar, the more
    data points are within that range. A histogram can help us quickly grasp the overall
    shape and distribution of the data, making it easier to identify patterns or trends
    and spot any unusual values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we have data on the heights of a group of people, measured
    in inches. We can create a histogram to visualize this information by dividing
    the heights into bins or groups, such as 150-159 centimeters, 160-169 centimeters,
    170-179 centimeters, and so on. Then, we can count how many people fall into each
    height range and represent that count as a bar on the chart. So, if we have 12
    people with heights between 150-159 centimeters, the bar representing that range
    would be of a certain height, and if there are 20 people with heights between
    160-169 centimeters, the bar for that range would be taller.
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at the histogram, we can easily see where most people’s heights
    are concentrated (for example, if many people are between 160-169 centimeters
    tall, there will be a taller bar in that range) and if there are any outliers
    (for example, if only one person is over 190 centimeters tall, there would be
    a very short bar for that range). This visual representation allows even those
    who are non-technical to quickly grasp the distribution and patterns in the data,
    making histograms a valuable tool for understanding and communicating data insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: A histogram showing the counts from a group of people in different
    ranges of heights](img/B19633_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: A histogram showing the counts from a group of people in different
    ranges of heights'
  prefs: []
  type: TYPE_NORMAL
- en: Density curves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A density curve is a smooth, continuous line that represents the distribution
    of data in a way that is easy to understand for non-technical readers. It provides
    a clear picture of how data is spread out and helps us visualize the overall shape
    and pattern of the data. The curve is drawn in such a way that the total area
    under it equals one, which means that it shows the relative frequencies or proportions
    of data points in different ranges, rather than the actual counts. The height
    of the curve at any point indicates the density of the data at that value, meaning
    that higher points on the curve represent areas where more data points are concentrated.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that we have data on the test scores of a group of students.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using a histogram, which consists of bars representing the number
    of students in each score range, we can use a density curve to show the same information
    in a smoother, more visually appealing manner. To create the curve, we must estimate
    the frequency distribution of the test scores and then draw a smooth line that
    closely follows the shape of the data. The peaks and valleys of the curve will
    indicate where the scores are more or less dense, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at the density curve, we can easily see the overall distribution
    of test scores, such as whether the majority of students scored within a specific
    range (indicated by a peak on the curve) or if there are any unusual patterns,
    such as multiple peaks or a wide spread of scores. The curve also helps us identify
    the central tendency (for example, the mode, where the curve is at its highest
    point) and the dispersion or spread of the data (for example, a wider curve indicates
    a larger range of scores). This smooth, visually intuitive representation makes
    it simple for non-technical readers to grasp and interpret the underlying patterns
    and characteristics of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: A density curve showing the distribution of test scores among
    a group of students](img/B19633_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: A density curve showing the distribution of test scores among a
    group of students'
  prefs: []
  type: TYPE_NORMAL
- en: Boxplots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A boxplot, also known as a box-and-whisker plot, is a straightforward and easy-to-understand
    visualization that displays key information about the distribution of a dataset.
    It consists of a rectangular box and two lines (whiskers) extending from the box,
    representing different aspects of the data. Boxplots are particularly useful for
    identifying the central tendency, spread, and potential outliers in the data,
    making them excellent tools for non-technical readers.
  prefs: []
  type: TYPE_NORMAL
- en: To understand a boxplot, let’s break down its components using an example. Imagine
    that we have data on the ages of people attending a community event. We can use
    a boxplot to visualize the age distribution clearly and concisely. The box in
    the plot represents the middle 50% of the data, also known as the **interquartile
    range** (**IQR**). The lower edge of the box, called the **first quartile** (**Q1**),
    marks the age at which 25% of the attendees are younger, while the upper edge,
    the **third quartile** (**Q3**), marks the age at which 75% of the attendees are
    younger. The line inside the box is the median, representing the exact middle
    age of the dataset, where 50% of the attendees are younger and 50% are older.
  prefs: []
  type: TYPE_NORMAL
- en: The whiskers extending from the box help us understand the spread of the remaining
    data. Typically, the whiskers extend to the smallest and largest values within
    1.5 times the IQR. In other words, they show the range of “typical” ages, excluding
    any potential outliers. Any data points outside of the whiskers are considered
    outliers and are often plotted as individual points or circles. These can be seen
    as the “Xs” in the chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'By looking at a boxplot, non-technical readers can quickly grasp essential
    information about the data, such as the median age (the line inside the box),
    the overall age distribution (the size of the box and whiskers), and any unusual
    ages that stand out from the rest (outliers). This simple yet powerful visualization
    provides a clear snapshot of the data’s characteristics, making it an invaluable
    tool for understanding and communicating data insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: A boxplot (box-and-whisker chart), showing the median, Q1, Q3,
    and lower and upper ranges for the ages of people attending an event](img/B19633_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: A boxplot (box-and-whisker chart), showing the median, Q1, Q3,
    and lower and upper ranges for the ages of people attending an event'
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A heatmap is a visually intuitive and easy-to-understand representation of data
    that uses colors to display the values or frequencies of variables in a two-dimensional
    grid. Each cell in the grid corresponds to a specific combination of the two variables,
    and the color of the cell represents the value or frequency associated with that
    combination. The color scale typically ranges from one color representing low
    values to another color representing high values, with a gradient in between.
    Heatmaps are especially useful for examining patterns, trends, or relationships
    between variables in large datasets, making them a valuable tool for non-technical
    readers.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how a heatmap works, let’s consider an example. Imagine that we
    have data on the sales of different products in a store across various months.
    We can create a heatmap to visualize this information clearly and concisely. In
    the heatmap, one axis (that is, the rows) represents the products, and the other
    axis (that is, the columns) represents the months. Each cell in the grid corresponds
    to the sales of a particular product in a specific month. The color of the cell
    represents the sales amount, with a color scale ranging from light green for low
    sales to dark green for high sales.
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the heatmap, non-technical readers can quickly identify patterns
    and trends in the data, such as which products tend to have higher sales in certain
    months or whether there are seasonal variations in sales. The color-coded cells
    make it easy to spot high and low values at a glance, allowing users to focus
    on areas of interest or concern. For instance, a row of dark green cells for a
    specific product may indicate consistently high sales, while a column of light
    green cells may suggest that sales are generally low during a particular month.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, heatmaps offer a visually appealing and easily interpretable way
    to display complex data, allowing non-technical readers to quickly identify patterns,
    trends, and relationships between variables. This powerful visualization technique
    simplifies the process of understanding and communicating data insights, making
    heatmaps an essential tool in any data analysis toolkit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: A heatmap showing the monthly sales of different product categories
    for an example fashion store](img/B19633_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: A heatmap showing the monthly sales of different product categories
    for an example fashion store'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction is a technique that’s used to simplify complex data
    by reducing the number of variables or dimensions while still retaining as much
    of the original information as possible. This process makes it easier for non-technical
    readers to understand and analyze the data, and it also improves the performance
    of various machine learning algorithms. The idea behind dimensionality reduction
    is to find the most important features or patterns within the data and represent
    them using fewer dimensions, effectively condensing the data while preserving
    its essential structure.
  prefs: []
  type: TYPE_NORMAL
- en: A popular method for dimensionality reduction is **principal component** **analysis**
    (**PCA**).
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a mathematical technique that transforms the original data into a new
    set of variables, called principal components. These principal components are
    chosen in such a way that they capture the most important patterns and variations
    in the data. By selecting a few principal components, we can create a simplified
    representation of the data that still captures most of the essential information.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, let’s consider an example. Imagine that you have
    a dataset containing information about various cars, including their price, fuel
    efficiency, horsepower, and weight. Each of these attributes represents a dimension
    in the data. However, some of these dimensions may be related or redundant. For
    instance, a car’s weight and horsepower are often correlated – heavier cars tend
    to have more horsepower.
  prefs: []
  type: TYPE_NORMAL
- en: PCA can help identify these relationships and combine related dimensions into
    a single principal component. In this case, PCA might create a new component called
    **performance** that combines information from both horsepower and weight. By
    focusing on this single component, we can simplify the data while still capturing
    the essential information about the cars’ performance.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a complex mathematical technique, but as a decision maker, you don’t
    need to understand all the technical details. The key takeaway is that PCA helps
    simplify data by identifying the most important patterns and representing them
    using fewer dimensions, making the data easier to understand and analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, dimensionality reduction techniques such as PCA are valuable tools
    for simplifying complex data and making it more accessible to non-technical readers.
    By reducing the number of dimensions while preserving the essential structure
    of the data, PCA facilitates more effective communication of data insights and
    improves the performance of machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: An illustration of principle components calculated on customer
    preference data](img/B19633_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: An illustration of principle components calculated on customer
    preference data'
  prefs: []
  type: TYPE_NORMAL
- en: Correlation analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By calculating the correlation coefficients between variables, decision-makers
    can identify relationships and dependencies among the data features, which can
    help inform further analysis and modeling decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation analysis is a statistical technique that’s used to evaluate the
    strength and direction of the relationship between two variables. Simply put,
    it helps us understand whether changes in one variable are associated with changes
    in another variable, and if so, how strongly they are related. Correlation analysis
    is valuable for non-technical readers as it provides a clear and easy-to-understand
    measure of how variables are connected. This can then be used to identify patterns,
    make predictions, or inform decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: The result of correlation analysis is typically expressed as a correlation coefficient,
    a number between -1 and 1\. A positive correlation coefficient (between 0 and
    1) indicates that as one variable increases, the other variable also tends to
    increase. A negative correlation coefficient (between -1 and 0) indicates that
    as one variable increases, the other variable tends to decrease. The closer the
    correlation coefficient is to 1 or -1, the stronger the relationship between the
    variables. A correlation coefficient close to 0 indicates that there is little
    or no relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we want to understand the relationship between the amount
    of time students spend studying and their exam scores. We can use correlation
    analysis to calculate the correlation coefficient between the study time variable
    and the exam score variable. If we find a positive correlation coefficient of
    0.8, this suggests a strong positive relationship between the time spent studying
    and exam scores, meaning that students who study more tend to have higher scores.
    Conversely, a negative correlation coefficient, such as -0.3, would suggest a
    weak negative relationship, indicating that students who study more might have
    slightly lower scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: A chart illustrating the correlation between exam scores and
    the number of hours studied for a group of students](img/B19633_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: A chart illustrating the correlation between exam scores and the
    number of hours studied for a group of students'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that correlation does not imply causation. A strong correlation
    between two variables does not necessarily mean that one variable causes change
    in the other; it merely indicates that there is a relationship between them. Other
    factors or variables might be responsible for the observed relationship.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, correlation analysis is a powerful tool for non-technical readers
    to assess the relationship between variables, providing a straightforward measure
    of the strength and direction of the association. This information can be used
    to identify patterns, inform decision-making, and generate hypotheses for further
    investigation, making correlation analysis a valuable method for understanding
    and communicating data insights.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying and addressing outliers in data can help improve the accuracy and
    reliability of subsequent analyses. Techniques such as the Z-score method or the
    IQR method can be used to detect and handle outliers.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to select the most appropriate EDA techniques and tools based
    on the nature of the data and the specific goals of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier detection is the process of identifying data points that are significantly
    different from the majority of the data in a dataset. These unusual data points,
    called outliers, can sometimes be the result of errors, anomalies, or exceptional
    cases that warrant further investigation. Identifying outliers is important because
    they can have a significant impact on the analysis and interpretation of the data,
    potentially skewing results or leading to incorrect conclusions. Outlier detection
    is especially valuable for non-technical readers as it helps ensure the data is
    accurate and reliable, leading to better insights and decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods for detecting outliers, including the Z-score method,
    the IQR method, and other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Z-score measures how far a data point is from the mean (average) of the
    dataset, expressed in terms of the standard deviation (a measure of the spread
    of the data). A high Z-score indicates that the data point is far from the mean,
    potentially making it an outlier. Typically, a Z-score threshold (for example,
    2 or 3) is chosen, and data points with Z-scores greater than this threshold are
    considered outliers. This method is most effective when the data is normally distributed
    (that is, it follows a bell-shaped curve).
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that we have data on the heights of a group of people.
    If we find one person with a Z-score of 3.5, it means their height is 3.5 standard
    deviations away from the average height, indicating that this person is unusually
    tall and could be considered an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: IQR method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The IQR method is another way to detect outliers by looking at the spread of
    the data. The IQR is the difference between the Q1 and the Q3 values of the data,
    representing the range in which the middle 50% of the data lies. Outliers are
    typically defined as data points that fall below Q1 - 1.5IQR or above Q3 + 1.5IQR.
    This method is more robust than the Z-score method as it is less sensitive to
    extreme values and works well for data that is not normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with the height example, if we calculate the IQR of the heights and
    find that a few people’s heights fall below Q1 - 1.5IQR or above Q3 + 1.5IQR,
    we will consider them outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Other techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are numerous other outlier detection methods, including machine learning
    algorithms such as clustering or classification models, and statistical tests
    such as Grubbs’ test or the Tukey method. The choice of method depends on the
    nature of the data, the distribution of the data, and the specific goals of the
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, outlier detection is an essential step in the data analysis process,
    helping non-technical readers ensure the accuracy and reliability of the data.
    Methods such as the Z-score and IQR can be used to identify unusual data points
    that may impact the analysis or reveal interesting patterns. By detecting and
    understanding outliers, data science teams can make more informed decisions and
    gain deeper insights from their data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: A chart illustrating outliers in a dataset containing the weights
    (in kilograms) of patients](img/B19633_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: A chart illustrating outliers in a dataset containing the weights
    (in kilograms) of patients'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how EDA is an important stage in the data science
    project process and provides the means to understand the characteristics and limitations
    of data, as well as to find insightful patterns within the data before machine
    learning or statistical models can be developed based on it.
  prefs: []
  type: TYPE_NORMAL
- en: This initial analysis also allows teams to present results and train models
    with more confidence since they have a deeper understanding of the data they are
    working with and the issues it may present.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered a large range of methods that can be used for EDA.
    Not all of them are always necessary, but hopefully, these tools will allow you
    to analyze data for yourself and give you the knowledge to interpret these visualizations
    and analyses when they are presented to you.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn how to test business hypotheses with statistics.
    This technique, known as significance testing, is critical in validating the findings
    from your data, ensuring that your decisions are grounded in statistical rigor.
  prefs: []
  type: TYPE_NORMAL
