- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about *regression*, a powerful statistical tool
    that can enable decision-makers to identify and understand relationships between
    variables, uncover trends, and forecast future trends.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be asking yourself the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: How can I benefit from understanding regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression, in simple terms, is a statistical method that helps uncover patterns
    and relationships within data.
  prefs: []
  type: TYPE_NORMAL
- en: Within businesses, regression can allow decision-makers to better understand
    how different factors or variables impact their key performance indicators, such
    as sales, revenue, or customer satisfaction. By identifying these relationships,
    businesses can make more informed decisions, optimize their strategies, and improve
    their overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some use cases for regression in a business context:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forecasting**: Regression techniques help businesses forecast future trends,
    sales, and demands by analyzing historical data. Accurate forecasting is crucial
    for strategic planning, resource allocation, and budgeting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifying relationships**: Regression analysis can help businesses uncover
    relationships between variables, such as the impact of advertising spending on
    sales or the influence of pricing on customer demand. Understanding these relationships
    can help businesses optimize their strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient resource allocation**: By understanding how different factors impact
    business performance, companies can allocate resources more effectively and make
    better investment decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk management**: Regression analysis can help businesses identify potential
    risks and vulnerabilities in their operations by understanding the factors that
    contribute to variability in outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluating performance**: Businesses can use regression analysis to evaluate
    the performance of different departments, teams, or employees by examining the
    relationship between inputs (for example, resources and time) and outputs (for
    example, sales and productivity).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price optimization**: By understanding how price changes affect demand, businesses
    can use regression analysis to determine optimal pricing strategies to maximize
    revenue and profit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy evaluation**: Regression analysis can help businesses assess the effectiveness
    of various policies and initiatives by comparing their outcomes with expected
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive into the complexities of regression analysis, it’s essential
    to start with a foundational concept that you’ve likely come across, albeit perhaps
    not in a statistical context – trend lines. While trend lines might seem straightforward,
    understanding their nuances can offer a wealth of insights.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to trend lines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By the end of this section, you’ll have a firm grasp of trend lines, the foundation
    of regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with a practical example. Suppose you own an e-commerce store and
    have been recording your daily sales for the past few months. With a list of numbers
    at hand, you’re curious about any patterns in the data that could inform your
    business decisions. This is where trend lines come into play.
  prefs: []
  type: TYPE_NORMAL
- en: A trend line is a line that represents the general direction or pattern in a
    dataset. It allows us to visualize the relationship between data points and helps
    us make predictions about future values. In simple terms, it connects the dots
    in a way that best illustrates the overall trend.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our e-commerce store scenario, imagine plotting your daily sales
    on a graph, with days on the horizontal axis and sales on the vertical axis. Each
    day’s sales become a data point on the graph. Your goal is to draw a line that
    best fits these data points, which represent the general trend in your sales data.
    A positive slope indicates increasing sales over time, while a negative slope
    suggests decreasing sales.
  prefs: []
  type: TYPE_NORMAL
- en: How would you fit a line that best fits this data? Would it have a positive
    or negative slope?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: An e-commerce store’s daily sales](img/B19633_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: An e-commerce store’s daily sales'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world data, however, is rarely so straightforward. Fluctuations in the
    data may occur due to various factors, such as holidays, promotions, or even the
    weather. This is where regression shines. Regression helps us fit a trend line
    to the data while accounting for these fluctuations, resulting in a more accurate
    representation of the overall pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider another example to further understand trend lines. As a marketing
    manager, you might be interested in the relationship between your advertising
    budget and product sales. You could plot your monthly advertising budget on the
    horizontal axis and monthly product sales on the vertical axis, with each data
    point representing a specific advertising budget and its corresponding sales.
    Fitting a trend line to this data allows you to see if there’s a general trend
    suggesting that higher advertising budgets lead to higher sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Positive and negative trend lines when comparing Monthly Advertising
    Budget to Monthly Sales](img/B19633_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Positive and negative trend lines when comparing Monthly Advertising
    Budget to Monthly Sales'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what a trend is, how do we go about estimating it based
    on our data? This is the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a trend line to data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore the process of fitting a trend line to a dataset
    and the techniques that can be used to minimize error and maximize accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The art of fitting a line to data is all about finding the line that best represents
    the underlying pattern or trend. But how do we define “best?” The answer lies
    in minimizing the *error* between the predicted values generated by our trend
    line and the actual data points. The most commonly used method for achieving this
    is known as the **least** **squares** technique.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’ve drawn a line through your data points, and for each point, you
    measure the vertical distance between the actual data point and the corresponding
    point on the line. This distance is known as the “residual” or the “error.” The
    goal of the least squares technique is to find the line that minimizes the sum
    of the squared residuals. Squaring the residuals is crucial because it eliminates
    any negative values and emphasizes larger deviations, ensuring the line fits the
    data as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the following diagram, each data point is represented by black dots,
    and the line we are trying to fit is represented by a long red line. The “residuals”
    or “errors” are represented by the red lines perpendicular to the line we are
    trying to fit. It is the sum of the squares of all these distances that we minimize
    in the least squares method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Line of best fit through a set of data points and the residuals/errors
    between the line of best fit and the data points](img/B19633_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Line of best fit through a set of data points and the residuals/errors
    between the line of best fit and the data points'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this process, let’s return to our marketing manager example. Once
    you’ve plotted your advertising budget and corresponding product sales data points,
    you’ll need to find the line that best fits the data. By applying the least squares
    technique, you’ll minimize the overall error between the predicted sales values
    generated by the line and the actual sales data points. This line will then give
    you valuable insights into the relationship between your advertising budget and
    product sales, helping you make more informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: While fitting a line to data might seem intimidating at first, modern software
    tools and programming languages make this process easier than ever. Many tools,
    such as Excel, Python libraries, Tableau, and PowerBI, offer built-in functions
    to fit trend lines and perform regression analysis with just a few clicks or lines
    of code. As a decision-maker, you don’t need to be an expert in the mathematical
    details, but understanding the concept and its applications is crucial for effectively
    leading data-driven projects.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a trend line to a dataset is a powerful technique that helps us uncover
    the hidden patterns within our data, allowing for better decision-making. By minimizing
    the error between the actual data points and the predictions generated by the
    line, we can extract valuable insights to drive business decisions and achieve
    desired outcomes. In the next section, we’ll explore how to estimate the line
    of best fit.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the line of best fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll dive deeper into the least squares method, the gold standard
    for estimating the line of best fit. We’ll explore the intuition behind this technique
    and walk through various examples to illustrate its power in uncovering patterns
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the least squares method, let’s walk through a couple
    of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: A school principal wants to understand the relationship between
    students’ study hours and their test scores. The principal plots the data on a
    graph, with study hours on the horizontal axis and test scores on the vertical
    axis. Each data point represents a student’s study hours and the corresponding
    test score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the least squares method, the principal aims to find the line that
    minimizes the sum of the squared residuals – the squared vertical distances between
    the actual test scores and the predicted scores generated by the line. Once the
    line of best fit is determined, the principal can identify patterns in the data,
    such as whether longer study hours generally lead to higher test scores, and use
    this information to inform school policies and study programs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Example 2**: A fitness coach wants to evaluate the correlation between a
    person’s daily calorie intake and weight loss. The coach plots the data, with
    daily calorie intake on the horizontal axis and weight loss on the vertical axis.
    Each data point represents an individual’s daily calorie intake and the corresponding
    weight loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the line of best fit in place, the coach can analyze the relationship between
    calorie intake and weight loss, providing tailored recommendations to clients
    based on their goals and dietary preferences.
  prefs: []
  type: TYPE_NORMAL
- en: As a decision-maker, it’s essential to understand the core concepts of the least
    squares method and its applications in various scenarios. In the following sections,
    we’ll expand upon this foundation, exploring more advanced regression techniques
    and tools.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the equations of the lines of best fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll dive into the process of calculating the equation of
    the line of best fit for both simple and multiple linear regression. While equations
    may seem daunting, we’ll break them down step by step to ensure a clear understanding.
    By mastering the equations behind the lines of best fit, you’ll gain a deeper
    appreciation for the underlying mechanics of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simple linear regression, the equation of the line of best fit can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = a + bx
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *y* is the dependent variable, *x* is the independent variable, *a* is
    the y-intercept (the point where the line intersects the *Y*-axis), and *b* is
    the slope (which determines the steepness of the line):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Line of best fit and the equation of the line](img/B19633_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Line of best fit and the equation of the line'
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate *a* and *b*, we can use the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: b = ∑ i=1 n  (x i − x̄)(y i − ȳ) _ (x i − x̄) 2
  prefs: []
  type: TYPE_NORMAL
- en: a = ȳ − bx̄
  prefs: []
  type: TYPE_NORMAL
- en: In these formulas, xi specifies the individual data points, x ̄  and ȳ represent
    the mean (average) of the x and y values, and Σ denotes summation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate this process with an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: A small business owner wants to predict their monthly revenue
    based on the number of items sold. The owner has the following data for the past
    4 months:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Month 1**: Items sold = 10, Revenue = $1,060'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Month 2**: Items sold = 15, Revenue = $1,400'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Month 3**: Items sold = 18, Revenue = $1,580'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Month 4**: Items sold = 26, Revenue = $2,150'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Month 5**: Items sold = 31, Revenue = $2,320'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To find the equation of the line of best fit, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the mean of the *x* values (items sold) and the *y* values (revenue):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: x ̄  =  (10 + 15 + 18 + 26 + 31)  __________________ 5  = 20
  prefs: []
  type: TYPE_NORMAL
- en: ȳ =  (1,060 + 1,400 + 1,600 + 2,150 + 2,320 )   ____________________________  5 
    = $1,702
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the formulas to find the slope, *b*, and the y-intercept, *a*, shown
    previously. After making the calculations in the equations for *a* and *b*, we
    obtain the line of best fit equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: y = $467.03 + $61.75x
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: Line of best fit equation, estimated for a scatter plot of Items
    Sold against Revenue](img/B19633_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Line of best fit equation, estimated for a scatter plot of Items
    Sold against Revenue'
  prefs: []
  type: TYPE_NORMAL
- en: 'Armed with the line of best fit equation, y = $467.03 + $61.75, the small business
    owner can confidently make predictions about their monthly revenue. For example,
    if they plan to sell 22 items in the upcoming month, they can estimate the revenue
    by plugging the value of *x* (number of items sold) into the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: y = $467.03 + $61.75(22) = $467.03 + $1,358.50 = $1,825.53
  prefs: []
  type: TYPE_NORMAL
- en: According to the model, the business owner can expect a monthly revenue of approximately
    $1,825.53 when they sell 22 items. This estimate can be useful for budgeting,
    resource allocation, and setting sales targets. By continually updating the model
    with new data, the business owner can refine the predictions and make well-informed
    decisions that contribute to the growth and success of their small business.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multiple linear regression, the process is more complex as it involves
    multiple independent variables. The general equation is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = a + b 1 x 1 + b 2 x 2 + … + b n x n
  prefs: []
  type: TYPE_NORMAL
- en: Here, *a* is the constant, and b1, b2, …, bn are the coefficients for each independent
    variable, x1, x2, …, xn. Calculating the coefficients in multiple linear regression
    typically requires specialized software or programming languages such as Python
    or R.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the equations behind the lines of best fit is essential for comprehending
    the mechanics of linear regression. By calculating these equations, you’ll be
    better equipped to interpret and apply the results of linear regression models
    in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to estimate the equation for a line of best fit (regression
    line), let’s learn how to interpret the parameters of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important parameters to interpret: the **slope** of the regression
    line and the **intercept**.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll interpret the slope; then, we’ll interpret the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the slope of a regression line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll focus on the significance of the slope of a regression
    line and how it informs our understanding of the relationship between variables.
    By studying the slope, we can derive meaningful insights from our regression models
    and make well-informed decisions. We’ll illustrate this concept through various
    examples, highlighting the practical implications of interpreting the slope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the equation for a simple linear regression line is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = a + bx
  prefs: []
  type: TYPE_NORMAL
- en: The slope, *b*, represents the average change in the dependent variable, *y*,
    for each one-unit increase in the independent variable, *x*. In other words, it
    tells us how *y* is expected to change as *x* changes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore some examples to better understand the interpretation of the slope.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: A fitness coach has developed a simple linear regression model
    to predict weight loss based on the number of calories burned during exercise.
    The equation of the line of best fit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = 5 − 0.01x
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* is the weight loss in pounds and *x* is the number of calories burned.
    The slope, -0.01, indicates that for every additional calorie burned, the expected
    weight loss increases by 0.01 pounds, on average. In this case, a negative slope
    is expected since burning more calories should lead to weight loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2**: An e-commerce company has built a model to predict revenue as
    a function of marketing spend. The equation of the line of best fit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = $10,000 + 2x
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, *y* represents the monthly revenue and *x* denotes the number
    of website visitors. The intercept suggests that there is a baseline revenue of
    $10,000 even without marketing spend. The slope, *2*, suggests that for every
    additional dollar of marketing spend, the expected monthly revenue increases by
    $2, on average. Here, the positive slope indicates a positive relationship between
    marketing spend and revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the slope of the regression line is essential because it quantifies
    the relationship between the dependent and independent variables. A positive slope
    implies that as the independent variable increases, so does the dependent variable,
    while a negative slope suggests that as the independent variable increases, the
    dependent variable decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned how the slope of the regression line can be interpreted,
    let’s move on to the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the intercept of a regression line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore the importance of interpreting the intercept
    of a regression line, which provides crucial context for understanding the baseline
    level of the dependent variable when the independent variable is equal to zero.
    We’ll investigate various examples to demonstrate the practical relevance of interpreting
    the intercept in linear regression models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our equation for a simple linear regression line, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: y = a + bx
  prefs: []
  type: TYPE_NORMAL
- en: The intercept, *a*, represents the expected value of the dependent variable,
    *y*, when the independent variable, *x*, is equal to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine some examples to better understand the interpretation of the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: An energy provider has developed a simple linear regression
    model to predict monthly electricity bills based on the number of **kilowatt-hours**
    (**kWh**) consumed by a household. The equation of the line of best fit is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = $20 + $0.12x
  prefs: []
  type: TYPE_NORMAL
- en: In this case, *y* represents the monthly electricity bill and *x* denotes the
    number of kWh consumed. The intercept, *20*, suggests that when a household consumes
    zero kWh (that is, *x* = 0), the expected monthly electricity bill is $20\. This
    value can be interpreted as the base charge or fixed cost that households need
    to pay regardless of their electricity consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 2**: A marketing analyst has built a model to predict the number
    of sales leads generated based on the advertising budget. The equation of the
    line of best fit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = 30 + 5x
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* represents the number of sales leads and *x* is the advertising budget
    in thousands of dollars. The intercept, *30*, indicates that when the advertising
    budget is zero (that is, *x* = 0), the expected number of sales leads generated
    is 30\. This value could be understood as the baseline number of leads generated
    through organic, non-advertising methods, such as referrals or search engine traffic.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential to note that interpreting the intercept may not always be meaningful,
    especially if the value of the independent variable, *x*, cannot be zero or if
    the regression model is not valid in that range. For example, when making a prediction
    based on the age of a customer, such as predicting the number of insurance claims
    based on the customer’s age, the age cannot be zero, so the intercept doesn’t
    relate to a specific meaning. In such cases, the intercept serves mainly to fine-tune
    the position of the regression line, rather than providing direct insights.
  prefs: []
  type: TYPE_NORMAL
- en: As a decision-maker, interpreting the intercept of the regression line helps
    you understand the baseline level of the dependent variable when the independent
    variable is zero, offering valuable context for your data. This understanding
    enables you to make more informed decisions and capitalize on the relationships
    between variables.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we understand a lot about our line of best fit, but what about
    how well the real data matches our line of best fit? This will be the topic of
    the next section, where we’ll measure the difference between our line of best
    fit and the actual data via **residuals**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding residuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll explore the concept of residuals in depth, focusing on
    their role in linear regression and their importance in assessing the accuracy
    and quality of our models. We’ll illustrate the significance of residuals through
    various examples, ensuring you have a comprehensive understanding of this critical
    aspect of regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals are the differences between the actual observed values (data points)
    and the values predicted by the regression model (the line of best fit). In simple
    terms, residuals represent the errors in our model – how far off our predictions
    are from reality. By analyzing the residuals, we can evaluate the performance
    of our regression model and identify potential areas for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula to calculate the residual for a specific data point is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: residual = observed value − predicted value
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the concept of residuals through an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: A sales manager has built a simple linear regression model to
    predict monthly revenue based on the number of sales calls made. The equation
    of the line of best fit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = $1,000 + $50x
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* is the predicted monthly revenue and *x* is the number of sales calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a specific month, the team made 30 sales calls and generated $2,300 in
    revenue. To calculate the residual for this month, we must find the predicted
    revenue using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicted revenue = $1,000 + $50 * 30 = $2,500
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can calculate the residual:'
  prefs: []
  type: TYPE_NORMAL
- en: Residual = observed value − predicted value
  prefs: []
  type: TYPE_NORMAL
- en: Residual = $2,300 − $2,500 = − $200
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the residual is -$200, indicating that the actual revenue was
    $200 lower than the predicted revenue based on the model.
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing residuals, it’s essential to look for patterns or trends that
    may suggest issues with our regression model. Ideally, the residuals should be
    randomly distributed around zero, with no discernible patterns. If the residuals
    exhibit trends or systematic patterns, it may indicate that the model isn’t adequately
    capturing the underlying relationship between the variables, and adjustments may
    be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common residual patterns and their potential causes are provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A U-shaped or inverted U-shaped pattern**: This may suggest that a quadratic
    term (a variable squared) needs to be added to the model to better capture the
    relationship between the variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A pattern showing residuals increasing or decreasing as the predicted value
    increases**: This could indicate that the relationship between the variables is
    not strictly linear, and a transformation (for example, logarithmic) might be
    required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding and analyzing residuals, decision-makers can evaluate the accuracy
    and reliability of their regression models, leading to better predictions and
    more effective data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: How can we use residuals to evaluate how well our model fits our data? This
    is what we’ll explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the goodness of fit in least-squares regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss how to evaluate the goodness of fit in least-squares
    regression, a critical step in determining the accuracy and effectiveness of our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding how well our model fits the data, we can make more informed
    decisions and improve our predictions. We’ll investigate various examples and
    introduce key metrics for evaluating the goodness of fit in regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goodness of fit is a measure of how well the regression line represents
    the relationship between the dependent and independent variables. A model with
    a high goodness of fit accurately describes the underlying data, while a model
    with a low goodness of fit may not capture the true relationship between the variables.
    To evaluate the goodness of fit, we commonly use two key metrics: the coefficient
    of determination (R-squared) and the **root mean square** **error** (**RMSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coefficient of determination (R-squared)**: R-squared is a measure that ranges
    from 0 to 1 and represents the proportion of the total variation in the dependent
    variable, *y*, that is explained by the independent variable, *x*. An R-squared
    value close to 1 indicates that the model explains a high percentage of variation
    in the data, while a value close to 0 suggests that the model has little explanatory
    power. However, it’s important to note that a high R-squared value does not necessarily
    imply a good model as it could be the result of overfitting or the presence of
    irrelevant variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: A car rental company has built a regression model to predict daily
    revenue based on the number of vehicles rented. The R-squared value for this model
    is 0.85\. This indicates that 85% of the variation in daily revenue can be explained
    by the number of vehicles rented, suggesting a strong relationship between the
    two variables.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**RMSE**: RMSE is a measure of the average difference between the actual observed
    values and the values predicted by the regression model. Lower RMSE values indicate
    that the model’s predictions are closer to the true values, while higher RMSE
    values suggest larger discrepancies between the predictions and the actual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: A clothing retailer has developed a regression model to predict
    monthly sales based on the amount spent on advertising. The RMSE for this model
    is $500\. This means that, on average, the model’s predicted sales differ from
    the actual sales by $500\. The retailer can use this information to assess the
    accuracy of the model and determine whether adjustments are necessary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Evaluating the goodness of fit is crucial in determining the effectiveness of
    our regression models. By understanding key metrics such as R-squared and RMSE,
    decision-makers can assess the reliability of their models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of trend lines and their significance
    in visualizing patterns in datasets. We explored the least squares method for
    estimating the line of best fit, discussed the importance of understanding residuals,
    and explained how to interpret the slope and intercept of the regression line.
    Finally, we covered how to evaluate a model’s goodness of fit using R-squared
    and RMSE. This knowledge has equipped you to carry out (or interpret from your
    team) regression analysis and apply it to various business scenarios. These scenarios
    could include forecasting sales, optimizing advertising budgets, and assessing
    the impact of different factors on key performance indicators, leading to informed
    data-driven decisions and business growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we transition into *Part 2* of this guide, we’re about to open a new dimension
    of analytical capabilities: machine learning. You’ll learn how to move from understanding
    relationships between variables to predicting future outcomes and even making
    automated decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms have a plethora of applications across many industries,
    from optimizing supply chains to helping in customer segmentation and targeted
    marketing, and even automating the process of gathering insights from textual
    data. As the capabilities of machine learning models grow, the number of applications
    is ever-expanding, with too many to list here. However, by understanding the core
    concepts and some of the well-understood applications, you will be able to better
    identify which business problems can be properly framed as machine learning problems,
    and the pitfalls to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter kicks off with an introduction to machine learning, where we’ll
    lay the groundwork for more advanced techniques and applications. The focus will
    be on how machine learning can bring scalability and automation to your data analysis
    and business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Machine Learning – Concepts, Applications, and Pitfalls'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part focuses on machine learning, covering its importance, types of machine
    learning techniques, supervised and unsupervised learning, model evaluation and
    interpretation, and common pitfalls to avoid. This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19633_06.xhtml#_idTextAnchor119)*, Introducing Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19633_07.xhtml#_idTextAnchor163)*, Supervised Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19633_08.xhtml#_idTextAnchor187)*, Unsupervised Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19633_09.xhtml#_idTextAnchor216)*, Interpreting and Evaluating
    Machine Learning Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19633_10.xhtml#_idTextAnchor238)*, Common Pitfalls in Machine
    Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
