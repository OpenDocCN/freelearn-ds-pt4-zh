<html><head></head><body>
<div id="_idContainer067">
<h1 class="chapter-number" id="_idParaDest-151"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-152"><a id="_idTextAnchor164"/><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.2.1">Supervised Machine Learning</span></h1>
<p><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.3.1">Within machine learning, supervised learning is one of the most used and most useful subfields. </span><span class="koboSpan" id="kobo.3.2">It is often the first area students learn within machine learning and what people think of when first hearing about machine learning, as it involves learning on annotated or labeled data, similar to how we learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">correct examples.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">The applications of supervised machine learning are wide and varied. </span><span class="koboSpan" id="kobo.5.2">From the spam detection on your email inbox, through to recommendation systems used when recommending TV shows and movies on your favorite streaming service, through to the call you get from your bank when its systems believe they may have detected fraudulent transactions, these are all applications of supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will discuss in more detail the steps involved in training and deploying supervised machine learning models, some of the core supervised machine learning models, factors to consider when training and evaluating supervised machine learning models, and applications of supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Defining </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">supervised learning</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Steps within </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">supervised learning</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Characteristics of regression and </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">classification algorithms</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Applications of </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">supervised learning</span></span></li>
</ul>
<h1 id="_idParaDest-153"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.19.1">Defining supervised learning</span></h1>
<p><span class="koboSpan" id="kobo.20.1">Building upon the foundations </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.21.1">covered in the previous chapter, let’s dive deeper into supervised learning. </span><span class="koboSpan" id="kobo.21.2">As discussed earlier, supervised learning involves training a model using labeled data, where the correct answers are already known. </span><span class="koboSpan" id="kobo.21.3">This process is analogous to a student learning under the guidance of a </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">knowledgeable teacher.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">In the context of business, imagine you’re trying to predict future sales based on historical data. </span><span class="koboSpan" id="kobo.23.2">The historical sales data, along with the factors that influence sales (such as marketing spend, seasonality and more), are your labeled data. </span><span class="koboSpan" id="kobo.23.3">Your machine learning model learns from </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.24.1">this data to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">future sales.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">Before getting into the detail of the process of supervised machine learning and different supervised learning algorithms, let’s look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">common applications.</span></span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.28.1">Applications of supervised learning</span></h2>
<p><span class="koboSpan" id="kobo.29.1">Supervised learning has a broad range of </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.30.1">applications across various industries, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.32.1">Consumer goods </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.33.1">and retail</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.35.1">Demand forecasting</span></strong><span class="koboSpan" id="kobo.36.1">: Retailers can use supervised learning to forecast product demand. </span><span class="koboSpan" id="kobo.36.2">By training a model on historical sales data, including product features, store locations, promotions, and external factors such as weather and holidays, along with corresponding sales figures, the model can learn patterns that influence demand. </span><span class="koboSpan" id="kobo.36.3">This allows retailers to optimize inventory management, reduce stockouts, and improve supply </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">chain efficiency.</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.38.1">Financial services</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.40.1">Credit risk assessment</span></strong><span class="koboSpan" id="kobo.41.1">: Financial institutions can use supervised learning to assess the creditworthiness of loan applicants. </span><span class="koboSpan" id="kobo.41.2">By training a model on historical data of loan repayments, along with relevant features such as credit score, income, and employment status, the model can learn to predict the likelihood of an applicant defaulting on a loan. </span><span class="koboSpan" id="kobo.41.3">This helps institutions make informed lending decisions and </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">manage risk.</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.43.1">Utilities</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Customer churn prediction</span></strong><span class="koboSpan" id="kobo.46.1">: Utilities companies, such as electricity, gas, and water companies, as well as telecoms and broadband companies, can use supervised learning to predict which customers are likely to churn (i.e., switch to a competitor). </span><span class="koboSpan" id="kobo.46.2">By training a model on historical customer data, including usage patterns, customer service interactions, and demographic information, along with </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.47.1">churn labels, the model can identify customers at high risk of churning. </span><span class="koboSpan" id="kobo.47.2">This allows companies to proactively offer personalized incentives, such as discounts or loyalty rewards, or improve targeted aspects of their services based on the identified churn drivers, ultimately reducing </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">customer attrition.</span></span></li></ul></li>
</ul>
<h2 id="_idParaDest-155"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.49.1">The two types of supervised learning</span></h2>
<p><span class="koboSpan" id="kobo.50.1">Supervised learning can be further</span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.51.1"> divided into two main categories: </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">regression</span></strong><span class="koboSpan" id="kobo.53.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">classification</span></strong><span class="koboSpan" id="kobo.55.1">. </span><span class="koboSpan" id="kobo.55.2">The key difference between them lies in the type of output </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">they predict.</span></span></p>
<h3><span class="koboSpan" id="kobo.57.1">Regression</span></h3>
<p><span class="koboSpan" id="kobo.58.1">Regression is used when the </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.59.1">output variable is a continuous value. </span><span class="koboSpan" id="kobo.59.2">The </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.60.1">goal is to predict a numerical value based on input features. </span><span class="koboSpan" id="kobo.60.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">some examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Sales forecasting</span></strong><span class="koboSpan" id="kobo.63.1">: A company can </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.64.1">use regression to predict sales figures for the next quarter based on historical sales data, marketing expenditure, and economic indicators. </span><span class="koboSpan" id="kobo.64.2">The</span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.65.1"> model learns the relationship between these input features and the continuous output variable (</span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">sales figures).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.67.1">House price prediction</span></strong><span class="koboSpan" id="kobo.68.1">: A real estate company can use regression to predict the price of a house based on features such as square footage, number of bedrooms, location, and age of the property. </span><span class="koboSpan" id="kobo.68.2">The model learns from past housing data to estimate the continuous output variable (</span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">house price).</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.70.1">Classification</span></h3>
<p><span class="koboSpan" id="kobo.71.1">Classification is used when the output variable is a categorical value. </span><span class="koboSpan" id="kobo.71.2">The goal is to predict the class or category to</span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.72.1"> which an input belongs. </span><span class="koboSpan" id="kobo.72.2">Here are </span><a id="_idIndexMarker501"/><span class="No-Break"><span class="koboSpan" id="kobo.73.1">some examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Talent acquisition</span></strong><span class="koboSpan" id="kobo.75.1">: HR departments can use classification to screen and shortlist job applicants. </span><span class="koboSpan" id="kobo.75.2">The model is trained on a dataset of past applicants, including their resumes, qualifications, and interview performance, along with hiring decision labels. </span><span class="koboSpan" id="kobo.75.3">Given a new applicant’s information, the model predicts whether they are likely to be accepted for the role, streamlining the </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">recruitment process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.77.1">Survey fraud detection</span></strong><span class="koboSpan" id="kobo.78.1">: Within market research surveys, particularly those provided online, there is often a high proliferation of fraudulent responses, as fake respondents aim to get the reward for taking the survey without providing accurate answers. </span><span class="koboSpan" id="kobo.78.2">To prevent this, supervised machine learning can be used to classify and filter out fraudulent responses, looking at aspects such as the length of time taken to answer the survey, the answers selected, and even the location of the </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">IP address.</span></span></li>
</ul>
<h2 id="_idParaDest-156"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.80.1">Key factors in supervised learning</span></h2>
<p><span class="koboSpan" id="kobo.81.1">As we touched upon in the previous chapter, there are several factors and risks to consider when training a machine learning model. </span><span class="koboSpan" id="kobo.81.2">Factors such as the bias-variance trade-off, the amount of</span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.82.1"> training data, the dimensionality of the input space, and noise in the target values play a crucial role in supervised learning. </span><span class="koboSpan" id="kobo.82.2">Let’s investigate how these factors specifically impact supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">learning algorithms.</span></span></p>
<h3><span class="koboSpan" id="kobo.84.1">The bias-variance trade-off – balancing simplicity and complexity</span></h3>
<p><span class="koboSpan" id="kobo.85.1">In supervised learning, it’s important to </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.86.1">strike the right balance between a model that is too simple (high bias) and a model that is too complex (</span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">high variance):</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.88.1">A model that is too simple may miss important patterns in the data, while a model that is too complex may memorize noise and </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">irrelevant details</span></span></li>
<li><span class="koboSpan" id="kobo.90.1">The goal is to find a model that captures the underlying patterns without being overly influenced by</span><a id="_idIndexMarker504"/> <span class="No-Break"><span class="koboSpan" id="kobo.91.1">random fluctuations</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.92.1">Quantity of training data</span></h3>
<ul>
<li><span class="koboSpan" id="kobo.93.1">The amount of data used to</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.94.1"> train the model is </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">pivotal</span></strong><span class="koboSpan" id="kobo.96.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">its performance.</span></span></li>
<li><span class="koboSpan" id="kobo.98.1">Generally, having more training data allows the model to learn better and make more </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">accurate predictions.</span></span></li>
<li><span class="koboSpan" id="kobo.100.1">However, it’s also important to consider the relationship between the number of data points and the number of input variables (features). </span><span class="koboSpan" id="kobo.100.2">If there are too many features compared to the number of data points, the model may become overly complex and perform poorly on new, </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">unseen data.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.102.1">Number of input variables</span></h3>
<ul>
<li><span class="koboSpan" id="kobo.103.1">The number of input </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.104.1">variables, also known as features or attributes, can impact the </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">model’s performance</span></span></li>
<li><span class="koboSpan" id="kobo.106.1">When dealing with a large number of input variables (high-dimensional data), the model becomes more complex and may require more data to </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">learn effectively</span></span></li>
<li><span class="koboSpan" id="kobo.108.1">In such cases, techniques such as variable selection or dimensionality reduction can be used to identify the most important variables and simplify </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">the model</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.110.1">Quality of the target data</span></h3>
<ul>
<li><span class="koboSpan" id="kobo.111.1">The quality of the</span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.112.1"> labeled data used for training is crucial to the </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">model’s performance</span></span></li>
<li><span class="koboSpan" id="kobo.114.1">If the target data contains errors or inconsistencies (noise), it can mislead the model during the learning process, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">inaccurate predictions</span></span></li>
<li><span class="koboSpan" id="kobo.116.1">Techniques such as data cleaning and outlier detection can help improve the quality of the target data and enhance the </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">model’s performance</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.118.1">To summarize, when implementing supervised learning, it’s important to consider the trade-off between model simplicity and complexity, the quantity and quality of training data, the number of input variables, and the presence of noise in the target data. </span><span class="koboSpan" id="kobo.118.2">By carefully managing these</span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.119.1"> factors, businesses can develop effective supervised learning models that make accurate predictions and </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">support decision-making.</span></span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.121.1">Steps within supervised learning</span></h1>
<p><span class="koboSpan" id="kobo.122.1">In this section, we will explore in more detail all the steps involved in supervised learning. </span><span class="koboSpan" id="kobo.122.2">From data</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.123.1"> preparation to model deployment, we’ll walk through each stage, providing insights and examples along </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">the way.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.125.1">Data preparation – laying the foundation</span></h2>
<p><span class="koboSpan" id="kobo.126.1">The success of any supervised learning </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.127.1">project hinges on the quality of the data. </span><span class="koboSpan" id="kobo.127.2">Data preparation is an important first step that involves </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Data cleaning</span></strong><span class="koboSpan" id="kobo.130.1">: Identifying and correcting erroneous, incomplete, or inconsistent data points to ensure the integrity of </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">your dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Feature selection</span></strong><span class="koboSpan" id="kobo.133.1">: Choosing the most informative and relevant attributes that contribute to the predictive power of your model, while discarding irrelevant or </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">redundant features.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.135.1">Data transformation</span></strong><span class="koboSpan" id="kobo.136.1">: Converting raw data into a format that can be effectively processed by machine learning algorithms. </span><span class="koboSpan" id="kobo.136.2">This may involve scaling numerical features, encoding categorical variables, or handling </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">missing values.</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.138.1">Example</span></strong><span class="koboSpan" id="kobo.139.1">: A retail company preparing customer purchase data might clean up inconsistencies, select key features such as purchase history and demographics, and transform them into </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">numerical representations.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.141.1">Algorithm selection – choosing the right tool</span></h2>
<p><span class="koboSpan" id="kobo.142.1">With a wide array of supervised learning algorithms available, selecting the most appropriate one is essential. </span><span class="koboSpan" id="kobo.142.2">The choice depends on the nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">your problem:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.144.1">Regression algorithms</span></strong><span class="koboSpan" id="kobo.145.1">: Used</span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.146.1"> for predicting continuous target variables. </span><span class="koboSpan" id="kobo.146.2">Popular choices include linear regression, polynomial regression, ridge regression, and </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">lasso regression.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.148.1">Classification algorithms</span></strong><span class="koboSpan" id="kobo.149.1">: Used for predicting categorical target variables. </span><span class="koboSpan" id="kobo.149.2">Common options include </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.150.1">logistic regression, </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">Support vector machines</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">SVMs</span></strong><span class="koboSpan" id="kobo.154.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">k-nearest neighbors</span></strong><span class="koboSpan" id="kobo.156.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.157.1">k-NN</span></strong><span class="koboSpan" id="kobo.158.1">), decision trees, and </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">random </span></span><span class="No-Break"><a id="_idIndexMarker513"/></span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">forests.</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.161.1">Example</span></strong><span class="koboSpan" id="kobo.162.1">: A real estate company might use regression algorithms to predict house prices based on features such as location, size, and property age, while a marketing firm could employ classification algorithms to predict customer behavior based on demographic and </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">interaction data.</span></span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.164.1">Model training – learning from data</span></h2>
<p><span class="koboSpan" id="kobo.165.1">Once you’ve selected an algorithm, it’s time</span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.166.1"> to train your model using the prepared training data. </span><span class="koboSpan" id="kobo.166.2">This step involves feeding the algorithm with input features and corresponding target values, allowing it to learn the underlying patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">and relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">During training, the algorithm iteratively adjusts its internal parameters to minimize the difference between predicted and actual target values. </span><span class="koboSpan" id="kobo.168.2">This process enables the model to capture the complex mappings between inputs </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">and outputs.</span></span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.170.1">Model evaluation – assessing performance</span></h2>
<p><span class="koboSpan" id="kobo.171.1">Before deploying your trained</span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.172.1"> model, it’s crucial to assess its performance using evaluation metrics. </span><span class="koboSpan" id="kobo.172.2">This step helps you understand how well the model generalizes to unseen data and identifies potential areas </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">for improvement.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.174.1">Regression metrics</span></strong><span class="koboSpan" id="kobo.175.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.177.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.178.1">MAE</span></strong><span class="koboSpan" id="kobo.179.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.180.1">mean squared error</span></strong><span class="koboSpan" id="kobo.181.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.182.1">MSE</span></strong><span class="koboSpan" id="kobo.183.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.184.1">root mean squared error </span></strong><span class="koboSpan" id="kobo.185.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.186.1">RMSE</span></strong><span class="koboSpan" id="kobo.187.1">) are commonly used to measure the average difference</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.188.1"> between predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">actual values</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.190.1">Classification metrics</span></strong><span class="koboSpan" id="kobo.191.1">: Accuracy, precision, recall, and</span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.192.1"> F1 score provide insights into the model’s ability to correctly classify instances across </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">different categories</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.194.1">By evaluating your model on a</span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.195.1"> separate validation set, you can gauge its performance and make informed decisions about further refinements or </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">parameter tuning.</span></span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.197.1">Prediction and deployment – putting the model to work</span></h2>
<p><span class="koboSpan" id="kobo.198.1">Once you’re confident in your </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.199.1">model’s performance, it’s time to deploy it for real-world predictions. </span><span class="koboSpan" id="kobo.199.2">This step involves integrating the trained model into your application or system, allowing it to generate predictions based on new, unseen </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">input data.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.201.1">Example</span></strong><span class="koboSpan" id="kobo.202.1">: A consumer goods company can use a trained sales forecasting model to predict future demand based on factors such as marketing spend, seasonality, and competitor activity, enabling proactive inventory management and </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">resource allocation.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">By carefully following the key steps of data preparation, algorithm selection, model training, evaluation, and deployment, you can realize the full potential of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">supervised learning.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">In the next section, we’ll dive deeper into specific algorithms for supervised learning, for both regression and classification. </span><span class="koboSpan" id="kobo.206.2">We’ll explore their characteristics and how they can be used to predict continuous and categorical </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">data respectively.</span></span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.208.1">Characteristics of regression and classification algorithms</span></h1>
<p><span class="koboSpan" id="kobo.209.1">In this section, we’ll explore the characteristics of a range of different regression and classification algorithms. </span><span class="koboSpan" id="kobo.209.2">We will explore their practical applications and how they can be used to drive decision-making in </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">various industries.</span></span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.211.1">Regression algorithms</span></h2>
<p><span class="koboSpan" id="kobo.212.1">We have already covered</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.213.1"> regression, which is a form of supervised machine learning. </span><span class="koboSpan" id="kobo.213.2">Regression algorithms are used when the output or target variable is continuous or numerical. </span><span class="koboSpan" id="kobo.213.3">They are primarily used for forecasting, predicting trends, and determining relationships between variables. </span><span class="koboSpan" id="kobo.213.4">Beyond the ordinary least squares regression we have already covered, there are other, more advanced variations of regression. </span><span class="koboSpan" id="kobo.213.5">These variations can be used to account for different interactions between variables, or to mitigate </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.214.1">overfitting by applying what is known </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">as </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.216.1">regularization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.218.1">Polynomial regression</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.219.1">Polynomial regression</span></strong><span class="koboSpan" id="kobo.220.1"> extends linear</span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.221.1"> regression by adding</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.222.1"> extra predictors, obtained by raising each of the original predictors to a power – for example, x</span><span class="superscript"><span class="koboSpan" id="kobo.223.1">2</span></span><span class="koboSpan" id="kobo.224.1"> or x</span><span class="superscript"><span class="koboSpan" id="kobo.225.1">3</span></span><span class="koboSpan" id="kobo.226.1">. </span><span class="koboSpan" id="kobo.226.2">This provides a broader range of functions to fit </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">In market research, polynomial regression can capture non-linear relationships between input variables and sales. </span><span class="koboSpan" id="kobo.228.2">For example, it might reveal that sales increase with advertising spend up to a certain point, but then level off or decline past that threshold, helping inform optimal </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">budget allocation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.230.1"><img alt="Figure 7.1: Polynomial regression" src="image/B19633_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.231.1">Figure 7.1: Polynomial regression</span></p>
<p><span class="koboSpan" id="kobo.232.1">Take, for example, this chart of polynomial regression, which has been fit to a set of data points. </span><span class="koboSpan" id="kobo.232.2">Remember that with ordinary least squares regression, we could only fit a linear, straight-line relationship between the points. </span><span class="koboSpan" id="kobo.232.3">However, polynomial regression allows us to model a more complex relationship between the variables, which may not be a straightforward, linear </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.233.1">relationship, as in the</span><a id="_idIndexMarker525"/> <span class="No-Break"><span class="koboSpan" id="kobo.234.1">previous example.</span></span></p>
<h3><span class="koboSpan" id="kobo.235.1">Ridge regression</span></h3>
<p><span class="koboSpan" id="kobo.236.1">Often within regression, we have </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.237.1">many input variables to consider, which can lead to problems such as overfitting or </span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.238.1">what is known as </span><strong class="bold"><span class="koboSpan" id="kobo.239.1">multicollinearity</span></strong><span class="koboSpan" id="kobo.240.1">, where the input variables are correlated with one </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.241.1">another. </span><span class="koboSpan" id="kobo.241.2">This can lead to less reliable inferences from </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">our model.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">To deal with this, there are different forms of “regularized” regression that add terms to our regression equation to help mitigate the </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">aforementioned issues.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.245.1">Ridge regression</span></strong><span class="koboSpan" id="kobo.246.1"> is a regularization method used to analyze multiple regression data that suffer from multicollinearity – when </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.247.1">predictor variables are highly correlated. </span><span class="koboSpan" id="kobo.247.2">By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. </span><span class="koboSpan" id="kobo.247.3">This could be useful in retail, where one might want to understand the relationship between advertising spend and sales while considering multicollinearity between different </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">advertising channels.</span></span></p>
<h3><span class="koboSpan" id="kobo.249.1">Lasso regression</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.250.1">Lasso</span></strong><span class="koboSpan" id="kobo.251.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.252.1">least absolute shrinkage and selection operator</span></strong><span class="koboSpan" id="kobo.253.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.254.1">regression</span></strong><span class="koboSpan" id="kobo.255.1"> is another regularization</span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.256.1"> technique for linear regression that allows for the selection of variables in a regression model, such that</span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.257.1"> not all the input variables have an effect on the outcome variable. </span><span class="koboSpan" id="kobo.257.2">This can reduce the likelihood of overfitting and is particularly useful when dealing with high-dimensional data. </span><span class="koboSpan" id="kobo.257.3">In the consumer goods industry, lasso regression can be used to predict demand based on various factors while </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">avoiding overfitting.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.259.1"><img alt="Figure 7.2: Regularized regression (e.g., ridge regression/lasso regression)" src="image/B19633_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.260.1">Figure 7.2: Regularized regression (e.g., ridge regression/lasso regression)</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.261.1">Classification algorithms</span></h2>
<p><span class="koboSpan" id="kobo.262.1">Classification algorithms are used when the output or target variable is categorical or discrete. </span><span class="koboSpan" id="kobo.262.2">They are mainly used</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.263.1"> for categorizing data into </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">specific groups.</span></span></p>
<h3><span class="koboSpan" id="kobo.265.1">SVM</span></h3>
<p><span class="koboSpan" id="kobo.266.1">An SVM is a powerful classification</span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.267.1"> algorithm that seeks the best hyperplane separating </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.268.1">different classes. </span><span class="koboSpan" id="kobo.268.2">It’s particularly effective in high-dimensional spaces and situations where the number of dimensions exceeds the number of samples. </span><span class="koboSpan" id="kobo.268.3">In marketing, SVM can be used to segment customers into different groups for </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">targeted advertising.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.270.1"><img alt="Figure 7.3: SVM" src="image/B19633_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.271.1">Figure 7.3: SVM</span></p>
<h3><span class="koboSpan" id="kobo.272.1">k-NN</span></h3>
<p><span class="koboSpan" id="kobo.273.1">k-NN is a simple, easy-to-understand</span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.274.1"> algorithm that classifies a data point based</span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.275.1"> on how its neighbors are classified. </span><span class="koboSpan" id="kobo.275.2">It’s widely used in preliminary studies to gain insights from the data. </span><span class="koboSpan" id="kobo.275.3">For example, in retail, k-NN can be used to predict whether a customer will make a purchase based on the behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">similar customers.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.277.1"><img alt="Figure 7.4: k-NN" src="image/B19633_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.278.1">Figure 7.4: k-NN</span></p>
<h3><span class="koboSpan" id="kobo.279.1">Decision trees and random forests</span></h3>
<p><span class="koboSpan" id="kobo.280.1">Decision trees split the data into multiple sets based on certain conditional control statements. </span><span class="koboSpan" id="kobo.280.2">They are easy to </span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.281.1">understand and interpret, making them useful for exploratory research. </span><span class="koboSpan" id="kobo.281.2">Random forests, an ensemble of decision trees, can</span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.282.1"> improve prediction accuracy. </span><span class="koboSpan" id="kobo.282.2">In the consumer goods industry, these algorithms can be used to predict whether a new product will be successful based on features such as price, marketing spend, </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">and competition.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.284.1"><img alt="Figure 7.5: Decision tree" src="image/B19633_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.285.1">Figure 7.5: Decision tree</span></p>
<p><span class="koboSpan" id="kobo.286.1">Take, for example, a decision tree that has been fitted to data to predict whether a patient is at higher risk or </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.287.1">lower risk of type 2 diabetes. </span><span class="koboSpan" id="kobo.287.2">We can see that a decision tree can split the population on the variables it has been trained on, including variables such as age (are they older or younger</span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.288.1"> than 45?), as well as whether they are overweight or have a relative with type 2 diabetes. </span><span class="koboSpan" id="kobo.288.2">This is a simple example but illustrates how you can go down a decision tree to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">an outcome.</span></span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.290.1">Key considerations in supervised learning</span></h2>
<p><span class="koboSpan" id="kobo.291.1">While applying these algorithms, it’s important to consider factors such as the bias-variance trade-off, the amount </span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.292.1">of training data, the dimensionality of the input space, and noise in the target values. </span><span class="koboSpan" id="kobo.292.2">Striking a balance between bias and variance ensures that your model is neither too simple (underfitting) nor too complex (overfitting). </span><span class="koboSpan" id="kobo.292.3">Having sufficient training data, managing high-dimensional input spaces, and dealing with noise in target values are also crucial to building </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">robust models.</span></span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.294.1">Evaluation metrics</span></h2>
<p><span class="koboSpan" id="kobo.295.1">Evaluation metrics play a vital role in </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.296.1">assessing the performance of supervised learning models. </span><span class="koboSpan" id="kobo.296.2">While the previous chapter introduced some common metrics, let’s now explore their specific applications in </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">supervised learning:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.298.1">Regression metrics</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.300.1">MAE</span></strong><span class="koboSpan" id="kobo.301.1">: MAE measures the </span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.302.1">average absolute difference between the predicted and actual values. </span><span class="koboSpan" id="kobo.302.2">It provides a clear interpretation of the model’s average prediction error in the same units as the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">target variable.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.304.1">MSE</span></strong><span class="koboSpan" id="kobo.305.1">: MSE calculates the average squared difference between the predicted and actual values. </span><span class="koboSpan" id="kobo.305.2">It emphasizes larger errors and is sensitive to outliers. </span><span class="koboSpan" id="kobo.305.3">Taking the square root of MSE gives the RMSE, which is in the same units as the </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">target variable.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.307.1">R-squared</span></strong><span class="koboSpan" id="kobo.308.1">: R-squared represents the proportion of variance in the target variable that is predictable from the input features. </span><span class="koboSpan" id="kobo.308.2">It ranges from 0 to 1, with higher values indicating a better fit of the model to </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">the data.</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.310.1">Classification metrics</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.312.1">Accuracy</span></strong><span class="koboSpan" id="kobo.313.1">: Accuracy measures the proportion of correct predictions out of the total predictions made. </span><span class="koboSpan" id="kobo.313.2">It is a</span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.314.1"> simple and intuitive metric but can be misleading when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">imbalanced classes.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.316.1">Precision</span></strong><span class="koboSpan" id="kobo.317.1">: Precision represents the proportion of true positive predictions out of all positive predictions made by the model. </span><span class="koboSpan" id="kobo.317.2">It focuses on the model’s ability to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">false positives.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Recall (sensitivity)</span></strong><span class="koboSpan" id="kobo.320.1">: Recall measures the proportion of true positive predictions out of all actual positive instances. </span><span class="koboSpan" id="kobo.320.2">It emphasizes the model’s ability to identify positive </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">instances</span></span><span class="No-Break"><a id="_idIndexMarker545"/></span><span class="No-Break"><span class="koboSpan" id="kobo.322.1"> correctly.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.323.1">F1 score</span></strong><span class="koboSpan" id="kobo.324.1">: The F1 score is the harmonic mean of precision and recall. </span><span class="koboSpan" id="kobo.324.2">It provides a balanced measure of the model’s performance, especially when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">imbalanced classes.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.326.1">Area under the ROC</span></strong><span class="koboSpan" id="kobo.327.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.328.1">AUC-ROC</span></strong><span class="koboSpan" id="kobo.329.1">): AUC-ROC evaluates the model’s ability to discriminate between classes</span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.330.1"> across different probability thresholds. </span><span class="koboSpan" id="kobo.330.2">It is particularly useful when the decision threshold needs to be adjusted based on the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">problem requirements.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.332.1">By understanding and applying these evaluation metrics, businesses can gain valuable insights into the performance of their supervised learning models, identify areas for improvement, and make informed decisions based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">models’ predictions.</span></span></p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.334.1">Applications of supervised learning</span></h1>
<p><span class="koboSpan" id="kobo.335.1">Supervised learning has found its </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.336.1">place in numerous industries. </span><span class="koboSpan" id="kobo.336.2">It enables many businesses to predict future outcomes based on historical data. </span><span class="koboSpan" id="kobo.336.3">Let’s explore some more practical examples of how supervised learning algorithms are applied in </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">different industries.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.338.1">Consumer goods</span></h2>
<p><span class="koboSpan" id="kobo.339.1">In the consumer goods industry, supervised learning is</span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.340.1"> being leveraged for </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">various applications:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.342.1">Consumer trend identification</span></strong><span class="koboSpan" id="kobo.343.1">: By analyzing data from eCommerce platforms, social media, search engines, sales, and surveys, companies can identify emerging consumer trends – for example, trending product categories, ingredients, flavors, and claims that are predicted to see future growth. </span><span class="koboSpan" id="kobo.343.2">This helps in developing new products or making changes to existing ones that align with consumer preferences, potentially leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">increased revenue.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.345.1">Price optimization</span></strong><span class="koboSpan" id="kobo.346.1">: By considering factors such as historical sales data, competitor pricing, and </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.347.1">marketing initiatives, companies can determine the optimal price for their products to </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">maximize profitability.</span></span></li>
</ul>
<h2 id="_idParaDest-170"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.349.1">Retail</span></h2>
<p><span class="koboSpan" id="kobo.350.1">In the retail industry, supervised learning is transforming various aspects of </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">the business:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.352.1">Product recommendations</span></strong><span class="koboSpan" id="kobo.353.1">: By analyzing customers’ purchase histories and product similarities, retailers can </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.354.1">recommend products that are likely to interest specific customers, potentially increasing sales and </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">customer loyalty.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.356.1">Customer feedback classification</span></strong><span class="koboSpan" id="kobo.357.1">: By categorizing customer feedback from various sources such as call centers, social media, and website forms, retailers can identify common issues and concerns. </span><span class="koboSpan" id="kobo.357.2">This information can be used to prioritize customer service efforts and address areas that </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">need improvement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.359.1">Demand forecasting</span></strong><span class="koboSpan" id="kobo.360.1">: By analyzing historical sales data, retailers can predict future demand for products. </span><span class="koboSpan" id="kobo.360.2">This helps in optimizing inventory management, reducing stockouts or overstocking, and improving overall </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">operational efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.362.1">Workforce optimization</span></strong><span class="koboSpan" id="kobo.363.1">: By forecasting customer traffic and sales patterns, retailers can optimize staff scheduling to ensure adequate coverage during peak hours while minimizing labor costs during </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">slower periods.</span></span></li>
</ul>
<h2 id="_idParaDest-171"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.365.1">Manufacturing</span></h2>
<p><span class="koboSpan" id="kobo.366.1">Supervised learning is transforming</span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.367.1"> the manufacturing industry in </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">several ways:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.369.1">Predictive maintenance</span></strong><span class="koboSpan" id="kobo.370.1">: By analyzing sensor data from equipment, supervised learning algorithms can predict when a machine is likely to fail, allowing for proactive maintenance and </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">minimizing downtime</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.372.1">Quality control</span></strong><span class="koboSpan" id="kobo.373.1">: By analyzing product data and identifying patterns, supervised learning can help detect defects or anomalies in the manufacturing process, ensuring higher product quality and </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">reducing waste</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.375.1">These are just a few examples of how </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.376.1">supervised learning is being applied across a few major industries to drive </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">business value.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">Think about potential use cases for supervised machine learning within the industries you are interested in. </span><span class="koboSpan" id="kobo.378.2">Are there existing, common use cases for supervised machine learning that other companies in your industry have </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">successfully applied?</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">When evaluating potential use cases within your organization, it’s important to have a clear understanding of the expected benefits and costs associated with implementation. </span><span class="koboSpan" id="kobo.380.2">With a well-defined value proposition, supervised learning can help transform various aspects of these industries and contribute to </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">successful outcomes.</span></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.382.1">Source:</span></strong></span><span class="No-Break"> </span><a href="https://www.qualifai.co.uk/post/ai-use-cases-in-consumer-goods-retail"><span class="No-Break"><span class="koboSpan" id="kobo.383.1">https://www.qualifai.co.uk/post/ai-use-cases-in-consumer-goods-retail</span></span></a></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.384.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.385.1">In this chapter, we’ve explored the applications of supervised learning in various industries, walked through the steps involved in supervised learning, discussed important considerations, and touched on different algorithms and </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">evaluation metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.387.1">With this knowledge, you’re now equipped to harness the power of supervised learning in </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">your business.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">But remember, the journey doesn’t end here. </span><span class="koboSpan" id="kobo.389.2">In the next chapter, we’ll delve into unsupervised learning – another exciting domain of </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">machine learning.</span></span></p>
</div>
</body></html>