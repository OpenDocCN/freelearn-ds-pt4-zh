<html><head></head><body>
<div id="_idContainer071">
<h1 class="chapter-number" id="_idParaDest-173"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-174"><a id="_idTextAnchor188"/><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.2.1">Unsupervised Machine Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Unlike the more familiar terrain of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">supervised learning</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">SL</span></strong><span class="koboSpan" id="kobo.7.1">), where data comes neatly labeled and the learning path is predefined, </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">UL</span></strong><span class="koboSpan" id="kobo.11.1">) ventures into the territory of unlabeled data, offering an opportunity to uncover hidden patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">and insights.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">This chapter delves into the field of UL, where we will learn about some practical examples of UL, the key steps involved in UL, and techniques around clustering, anomaly detection, dimensionality reduction, and association </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">rule learning.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">This chapter covers the </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">following topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.17.1">Defining UL</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Steps </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">in UL</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Clustering – unveiling hidden patterns in </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">your data</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Association </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">rule learning</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Applications </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">of UL</span></span></li>
</ul>
<h1 id="_idParaDest-175"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.26.1">Defining UL</span></h1>
<p><span class="koboSpan" id="kobo.27.1">UL is a type of </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">machine learning</span></strong><span class="koboSpan" id="kobo.29.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.30.1">ML</span></strong><span class="koboSpan" id="kobo.31.1">) that finds patterns in data without any prior training. </span><span class="koboSpan" id="kobo.31.2">Distinct from its counterpart, SL, where</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.32.1"> the model is trained using labeled data, UL algorithms work with unlabeled data. </span><span class="koboSpan" id="kobo.32.2">The aim is to model the underlying structure or distribution in the data to learn more </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">about it.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">Think of it as a detective who walks into a crime scene with no initial clues or suspects. </span><span class="koboSpan" id="kobo.34.2">The detective’s job is to uncover patterns, find hidden groups, or establish relationships between different elements at </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">the scene.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.36.1">Practical examples of UL</span></h2>
<p><span class="koboSpan" id="kobo.37.1">To make this concept more tangible, let’s look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">practical examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.39.1">Market research</span></strong><span class="koboSpan" id="kobo.40.1">: A company wants to understand its customer base better and tailor their marketing to different consumer segments. </span><span class="koboSpan" id="kobo.40.2">They have a wealth of data (for example, customer </span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.41.1">data or consumer survey data) but no specific categories or labels. </span><span class="koboSpan" id="kobo.41.2">UL can help identify distinct groups or segments within their customers. </span><span class="koboSpan" id="kobo.41.3">The company can then better understand the demographics, behaviors, and opinions of these different segments, leading to more targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">marketing strategies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.43.1">Consumer goods and retail</span></strong><span class="koboSpan" id="kobo.44.1">: An e-commerce store wants to understand the buying behavior of its customers. </span><span class="koboSpan" id="kobo.44.2">Using UL, they can discover associations between different products. </span><span class="koboSpan" id="kobo.44.3">For instance, they might find that customers who buy a certain brand of remote control also buy a certain battery type and pack size, enabling the e-commerce store to automatically recommend items the consumer is likely to add to </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">their order.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Supplier performance analysis</span></strong><span class="koboSpan" id="kobo.47.1">: By clustering suppliers based on performance metrics such as delivery time, quality of goods, cost, customer support, and reliability, companies can gain insights into their supply chain’s strengths and weaknesses. </span><span class="koboSpan" id="kobo.47.2">This aids in making informed decisions about which suppliers to prioritize </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">or re-evaluate.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.49.1">UL is a powerful tool that can uncover hidden patterns and associations in your data. </span><span class="koboSpan" id="kobo.49.2">It’s like having a detective on your team who can make sense of seemingly unrelated information. </span><span class="koboSpan" id="kobo.49.3">Whether you’re looking to understand your customers better, optimize your marketing strategies, or discover new opportunities, UL can provide </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">valuable insights.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Now that we’ve established a solid understanding of what UL is, let’s dive deeper into the process. </span><span class="koboSpan" id="kobo.51.2">In the next section, we’ll explore the steps involved in UL, from data collection </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">to interpretation.</span></span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.53.1">Steps in UL</span></h1>
<p><span class="koboSpan" id="kobo.54.1">UL is a type of ML that allows us to draw</span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.55.1"> inferences from datasets consisting of input data without labeled responses. </span><span class="koboSpan" id="kobo.55.2">Unlike SL, where we have a clear target or outcome to predict, UL is more about discovering hidden patterns and structures within data. </span><span class="koboSpan" id="kobo.55.3">But how does this process work? </span><span class="koboSpan" id="kobo.55.4">Let’s break it down into </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">digestible steps:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.57.1"><img alt="Figure 8.1: Steps involved in unsupervised ML (UML)" src="image/B19633_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.58.1">Figure 8.1: Steps involved in unsupervised ML (UML)</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.59.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.60.1">While the diagram presents a linear flow, in practice, these steps may not always follow a strict linear sequence. </span><span class="koboSpan" id="kobo.60.2">Throughout the process, insights gained about the data, such as during evaluation, may inform iterations and refinements in data processing or </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">model selection.</span></span></p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.62.1">Step 1 – Data collection</span></h2>
<p><span class="koboSpan" id="kobo.63.1">Just as with any other ML project, UL</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.64.1"> begins with data collection. </span><span class="koboSpan" id="kobo.64.2">This could be customer data for a retail company, patient data for a healthcare organization, or user behavior data for a tech firm. </span><span class="koboSpan" id="kobo.64.3">The key here is to gather as much relevant data as possible to help the model learn and make </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">accurate predictions.</span></span></p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.66.1">Step 2 – Data preprocessing</span></h2>
<p><span class="koboSpan" id="kobo.67.1">Once the data is collected, it needs to be cleaned and preprocessed. </span><span class="koboSpan" id="kobo.67.2">This step involves handling missing values, removing </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.68.1">outliers, and normalizing the data. </span><span class="koboSpan" id="kobo.68.2">This step is important as the quality of data</span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.69.1"> impacts the ability of the model to </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">learn effectively.</span></span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.71.1">Step 3 – Choosing the right model</span></h2>
<p><span class="koboSpan" id="kobo.72.1">After preprocessing, the next step is to </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.73.1">choose the right model for your data. </span><span class="koboSpan" id="kobo.73.2">There are various UL algorithms, such as k-means clustering, hierarchical clustering, and </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">Density-Based Spatial Clustering of Applications with Noise</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">DBSCAN</span></strong><span class="koboSpan" id="kobo.77.1">). </span><span class="koboSpan" id="kobo.77.2">The choice of </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.78.1">model depends on the problem at hand and the nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">your data.</span></span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.80.1">Step 4 – Training the model</span></h2>
<p><span class="koboSpan" id="kobo.81.1">Now comes the</span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.82.1"> exciting part – training the model. </span><span class="koboSpan" id="kobo.82.2">Here, the model learns to identify patterns and structures within the data without any supervision. </span><span class="koboSpan" id="kobo.82.3">For instance, in a market research context, a UL model could identify distinct segments within your customer base based on </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">purchasing behavior.</span></span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.84.1">Step 5 – Interpretation and evaluation</span></h2>
<p><span class="koboSpan" id="kobo.85.1">The final step involves interpreting the </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.86.1">results and evaluating the model’s performance. </span><span class="koboSpan" id="kobo.86.2">As with SL, the </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.87.1">unsupervised model performance can be evaluated with evaluation metrics. </span><span class="koboSpan" id="kobo.87.2">In UL, evaluation metrics can be a bit tricky as we don’t have a clear target to</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.88.1"> compare our predictions with. </span><span class="koboSpan" id="kobo.88.2">However, metrics such as Silhouette Score or the </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">Davies-Bouldin Index</span></strong><span class="koboSpan" id="kobo.90.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.91.1">DBI</span></strong><span class="koboSpan" id="kobo.92.1">) can be used to evaluate the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">of clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">In a business context, interpretation is equally important. </span><span class="koboSpan" id="kobo.94.2">For example, in a retail setting, understanding the characteristics of different customer segments can help tailor marketing strategies to each segment, ultimately leading to increased sales and </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">customer satisfaction.</span></span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.96.1">In summary</span></h2>
<p><span class="koboSpan" id="kobo.97.1">By now, you should have an understanding of the steps involved in UL and how it can be applied in a business context. </span><span class="koboSpan" id="kobo.97.2">But we’re just scratching the surface here. </span><span class="koboSpan" id="kobo.97.3">Next, we’ll dive deeper into one of the </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.98.1">most common techniques in UL – clustering. </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">Stay tuned!</span></span></p>
<p><span class="koboSpan" id="kobo.100.1">In the next section, we will delve into the world of clustering algorithms, exploring how they work, their applications, and how they can be used to drive decision-making </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">in business.</span></span></p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.102.1">Clustering – unveiling hidden patterns in your data</span></h1>
<p><span class="koboSpan" id="kobo.103.1">Clustering is a powerful tool in the UL toolkit. </span><span class="koboSpan" id="kobo.103.2">But what is it, and how can it help decision-makers in business? </span><span class="koboSpan" id="kobo.103.3">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">dive in.</span></span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.105.1">What is clustering?</span></h2>
<p><span class="koboSpan" id="kobo.106.1">Clustering is a method of UL that involves</span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.107.1"> grouping data points together based on their similarity. </span><span class="koboSpan" id="kobo.107.2">Unlike SL, where we have a clear target or outcome variable, UL (and, by extension, clustering) is all about finding hidden structures and patterns in data without any </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">predefined labels.</span></span></p>
<p><span class="koboSpan" id="kobo.109.1">Think of clustering as a way to discover and explore unknown territories in your data. </span><span class="koboSpan" id="kobo.109.2">It’s like an explorer setting out on a journey without a map, using only their observations to make sense of </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">the landscape.</span></span></p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.111.1">How does clustering work?</span></h2>
<p><span class="koboSpan" id="kobo.112.1">The process of clustering involves </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">several steps:</span></span></p>
<ol>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.114.1">Feature selection</span></strong></span><p class="list-inset"><span class="koboSpan" id="kobo.115.1">In this step, you choose the</span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.116.1"> characteristics or attributes of your data that you believe can help differentiate between different groups. </span><span class="koboSpan" id="kobo.116.2">For example, if you’re clustering customers, you might select features such as age, income, and </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">purchase history.</span></span></p></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.118.1">Distance measurement</span></strong></span><ul><li><span class="koboSpan" id="kobo.119.1">To group similar data points together, you need to define what “similar” means. </span><span class="koboSpan" id="kobo.119.2">This is done by measuring the “distance” or “dissimilarity” between </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">data points.</span></span></li><li><span class="koboSpan" id="kobo.121.1">One common distance measure is Euclidean distance, which is the straight-line distance between</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.122.1"> two points. </span><span class="koboSpan" id="kobo.122.2">You can imagine this as the distance “as the crow flies,” whereas other distance measures, such as Manhattan distance or cosine similarity, consider different aspects of the data. </span><span class="koboSpan" id="kobo.122.3">The cosine distance is the cosine of the angle between </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">two points.</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.124.1">Clustering algorithm</span></strong></span><ul><li><span class="koboSpan" id="kobo.125.1">Once you have your features and distance measures, you apply a clustering algorithm to group similar data </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">points together.</span></span></li><li><span class="koboSpan" id="kobo.127.1">Different algorithms make different assumptions about the structure of the clusters. </span><span class="koboSpan" id="kobo.127.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">some examples:</span></span><ul><li><span class="koboSpan" id="kobo.129.1">k-means tries to partition n observations into k clusters where each observation belongs to the cluster with the </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">nearest mean.</span></span></li><li><span class="koboSpan" id="kobo.131.1">Hierarchical clustering builds a hierarchy of clusters, either from individual elements by merging clusters (agglomerative approach) or from the entire dataset by dividing the dataset into smaller clusters (</span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">divisive approach).</span></span></li><li><span class="koboSpan" id="kobo.133.1">DBSCAN groups together points that are closely packed together and marks points that are in low-density regions </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">as outliers.</span></span></li></ul></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.135.1">Evaluation</span></strong></span><ul><li><span class="koboSpan" id="kobo.136.1">After clustering, you need to evaluate the quality of your clusters. </span><span class="koboSpan" id="kobo.136.2">This helps determine if your clustering makes sense and is useful for </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">your problem.</span></span></li><li><span class="koboSpan" id="kobo.138.1">Metrics such as Silhouette Score measure how similar an object is to its own cluster compared to other clusters. </span><span class="koboSpan" id="kobo.138.2">A high silhouette score indicates that the object is well matched to its own cluster and poorly matched to </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">neighboring clusters.</span></span></li><li><span class="koboSpan" id="kobo.140.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.141.1">Dunn Index</span></strong><span class="koboSpan" id="kobo.142.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.143.1">DI</span></strong><span class="koboSpan" id="kobo.144.1">) is another </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.145.1">metric that measures the ratio between the minimal inter-cluster distance and the maximal intra-cluster distance. </span><span class="koboSpan" id="kobo.145.2">A higher DI indicates </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">better clustering.</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.147.1">Remember – clustering is an </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.148.1">exploratory technique. </span><span class="koboSpan" id="kobo.148.2">It can help uncover patterns and structures in your data that you might not have known about beforehand. </span><span class="koboSpan" id="kobo.148.3">Experiment with different features, distance measures, and algorithms to see what insights you can uncover in </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">your data.</span></span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.150.1">k-means clustering</span></h2>
<p><span class="koboSpan" id="kobo.151.1">Here is an example of one clustering </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.152.1">algorithm </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">called</span></span><span class="No-Break"><a id="_idIndexMarker572"/></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1"> k-means:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.155.1"><img alt="Figure 8.2: k-means clustering" src="image/B19633_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.156.1">Figure 8.2: k-means clustering</span></p>
<p><span class="koboSpan" id="kobo.157.1">In the preceding diagram, let’s look at each side before and after the </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">k-means process.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.159.1">Before k-means (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.160.1">left side)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">:</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">On this side, before we carry out k-means, the data points are scattered across the two-dimensional space defined by axes x</span><span class="subscript"><span class="koboSpan" id="kobo.163.1">1</span></span><span class="koboSpan" id="kobo.164.1"> and x</span><span class="subscript"><span class="koboSpan" id="kobo.165.1">2</span></span><span class="koboSpan" id="kobo.166.1">. </span><span class="koboSpan" id="kobo.166.2">These could be variables such as, say, the total spend (x</span><span class="subscript"><span class="koboSpan" id="kobo.167.1">1</span></span><span class="koboSpan" id="kobo.168.1">) and the number </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.169.1">of visits (x</span><span class="subscript"><span class="koboSpan" id="kobo.170.1">2</span></span><span class="koboSpan" id="kobo.171.1">) of customers at </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">a store.</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">At this stage, the data is unlabeled, meaning we don’t yet know which cluster each data point </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">belongs to.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.175.1">After k-means (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.176.1">right side)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">:</span></span></p>
<p><span class="koboSpan" id="kobo.178.1">After k-means, the data points have been grouped into clusters based on their proximity to one another. </span><span class="koboSpan" id="kobo.178.2">These clusters can be informative about underlying patterns within </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">the data.</span></span></p>
<h3><span class="koboSpan" id="kobo.180.1">The k-means clustering process</span></h3>
<p><span class="koboSpan" id="kobo.181.1">To carry out k-means, we need to choose the number of clusters we want to identify in our data. </span><span class="koboSpan" id="kobo.181.2">Let’s say we have visualized the data and decided that </span><em class="italic"><span class="koboSpan" id="kobo.182.1">k</span></em><span class="koboSpan" id="kobo.183.1">=3, meaning we want to find </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">three clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">The k-means algorithm follows </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">these steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.187.1">Initialization</span></strong><span class="koboSpan" id="kobo.188.1">: Randomly select </span><em class="italic"><span class="koboSpan" id="kobo.189.1">k</span></em> <a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.190.1">points from the data as the initial centroids (the center of </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">each cluster).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.192.1">Assignment step</span></strong><span class="koboSpan" id="kobo.193.1">: Assign each data point to the nearest centroid based on the distance between the point and </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">the centroid.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Update step</span></strong><span class="koboSpan" id="kobo.196.1">: Recalculate the centroid of each cluster by taking the mean of all points assigned to </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">that cluster.</span></span></li>
<li><span class="koboSpan" id="kobo.198.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.199.1">steps 2</span></em><span class="koboSpan" id="kobo.200.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.201.1">3</span></em><span class="koboSpan" id="kobo.202.1"> until the centroids no longer move significantly or a maximum number of iterations is reached. </span><span class="koboSpan" id="kobo.202.2">This indicates that the clusters </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">have stabilized.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.204.1">After applying the k-means algorithm, the data points are colored differently based on the cluster they belong to. </span><span class="koboSpan" id="kobo.204.2">In this case, three clusters have been identified: Cluster A, Cluster B, and </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">Cluster C.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">The k-means algorithm is widely used because it’s relatively simple and efficient. </span><span class="koboSpan" id="kobo.206.2">However, it assumes that clusters are spherical and evenly sized, which might not always be the case in real-world data. </span><span class="koboSpan" id="kobo.206.3">Additionally, the number of clusters k needs to be specified beforehand, which can be a drawback if the optimal number of clusters is not known. </span><span class="koboSpan" id="kobo.206.4">Despite these limitations, k-means</span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.207.1"> remains a powerful tool for </span><strong class="bold"><span class="koboSpan" id="kobo.208.1">exploratory data analysis</span></strong><span class="koboSpan" id="kobo.209.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.210.1">EDA</span></strong><span class="koboSpan" id="kobo.211.1">) and pattern recognition in various fields, which we will explore in this </span><a id="_idIndexMarker576"/><span class="No-Break"><span class="koboSpan" id="kobo.212.1">next section.</span></span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.213.1">Practical applications of clustering</span></h2>
<p><span class="koboSpan" id="kobo.214.1">Clustering has a wide range of applications across </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">various industries:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.216.1">Risk assessment in insurance</span></strong><span class="koboSpan" id="kobo.217.1">: In the insurance industry, clustering algorithms can be used to group</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.218.1"> policyholders based on various risk factors. </span><span class="koboSpan" id="kobo.218.2">For instance, clustering can identify groups of individuals with similar driving habits in auto insurance or health profiles in life insurance. </span><span class="koboSpan" id="kobo.218.3">This segmentation allows insurance companies to tailor their policies and pricing more accurately according to the risk levels, leading to more efficient risk management and </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">pricing strategies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.220.1">Energy consumption analysis in utilities</span></strong><span class="koboSpan" id="kobo.221.1">: Utility companies can use clustering to analyze energy usage patterns of their customers. </span><span class="koboSpan" id="kobo.221.2">By grouping customers into clusters based on their consumption patterns, peak usage times, and seasonal variations, utilities can better understand demand, plan energy distribution, and even design customized energy-saving programs. </span><span class="koboSpan" id="kobo.221.3">This can also help in identifying areas where infrastructure improvements are needed or where energy conservation measures can be </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">most effective.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Content personalization in digital media</span></strong><span class="koboSpan" id="kobo.224.1">: In the digital media and entertainment industry, clustering is used to analyze user preferences and viewing habits. </span><span class="koboSpan" id="kobo.224.2">By clustering users based on their interactions with different content types (such as genres of movies, music, or articles), media companies can provide personalized content recommendations. </span><span class="koboSpan" id="kobo.224.3">This not only enhances user experience but also increases engagement and, potentially, </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">subscription retention.</span></span></li>
</ul>
<h2 id="_idParaDest-189"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.226.1">Evaluation metrics for clustering</span></h2>
<p><span class="koboSpan" id="kobo.227.1">As decision-makers, it’s important to understand how well your clustering model is performing. </span><span class="koboSpan" id="kobo.227.2">Here are a few metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">to consider:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.229.1">Silhouette Score</span></strong><span class="koboSpan" id="kobo.230.1">: The Silhouette</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.231.1"> Score metric is a way to quantify how</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.232.1"> well data is grouped into clusters. </span><span class="koboSpan" id="kobo.232.2">It ranges from -1 to 1. </span><span class="koboSpan" id="kobo.232.3">A score close to 1 means that the data points are very similar to others in the same cluster but dissimilar to those in other clusters, which is ideal. </span><span class="koboSpan" id="kobo.232.4">Essentially, it’s a measure of how appropriately each data point belongs to its cluster: the higher the score, the better each data point fits within its own cluster as opposed to others. </span><span class="koboSpan" id="kobo.232.5">This score helps to validate consistency within clusters of data and can be used to determine the optimal number of clusters by comparing scores across different numbers </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">of clusters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.234.1">DI</span></strong><span class="koboSpan" id="kobo.235.1">: DI is a more nuanced</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.236.1"> gauge of clustering quality that considers both the compactness of clusters and the separation between them. </span><span class="koboSpan" id="kobo.236.2">It does this by examining the smallest distance between points in different clusters and the largest distance between points within the same cluster. </span><span class="koboSpan" id="kobo.236.3">A higher DI indicates that the clusters are compact (with data points closely bunched together) and well separated (with each cluster being a good distance away from the others). </span><span class="koboSpan" id="kobo.236.4">This index is especially useful when you want to ensure that clusters are distinct from each other while also being </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">internally coherent.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.238.1">Remember – the choice of metric should align with your business objectives. </span><span class="koboSpan" id="kobo.238.2">For instance, if your goal is to create highly distinct customer segments for targeted marketing, a high silhouette score would </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">be desirable.</span></span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.240.1">In summary</span></h2>
<p><span class="koboSpan" id="kobo.241.1">Clustering is a powerful UL technique that can reveal hidden patterns and structures in your data. </span><span class="koboSpan" id="kobo.241.2">By understanding its process</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.242.1"> and applications, you can harness its power to make more informed </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">business decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.244.1">In the next section, we’ll explore another key UL technique: association rule learning. </span><span class="koboSpan" id="kobo.244.2">This method can help you discover interesting relations between variables in large datasets – an important skill for any </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">data-savvy decision-maker.</span></span></p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.246.1">Association rule learning</span></h1>
<p><span class="koboSpan" id="kobo.247.1">Imagine you’re at a supermarket, and you notice that people who buy diapers often also buy beer. </span><span class="koboSpan" id="kobo.247.2">This is not a random observation but a result of a powerful UL technique called association rule learning. </span><span class="koboSpan" id="kobo.247.3">It</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.248.1"> uncovers hidden patterns in large datasets, enabling businesses to make </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">data-driven decisions.</span></span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.250.1">What is association rule learning?</span></h2>
<p><span class="koboSpan" id="kobo.251.1">Association rule learning is an ML method that identifies frequent if-then associations called “rules” among a set of items. </span><span class="koboSpan" id="kobo.251.2">It’s like finding relationships between products often grouped together. </span><span class="koboSpan" id="kobo.251.3">These rules can be leveraged to predict future behavior, enabling businesses to strategize their marketing </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">efforts effectively.</span></span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.253.1">The Apriori algorithm – a practical example</span></h2>
<p><span class="koboSpan" id="kobo.254.1">One of the most popular algorithms </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.255.1">used in association rule learning is the</span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.256.1"> Apriori algorithm. </span><span class="koboSpan" id="kobo.256.2">Let’s break down how it works with a </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">practical example.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">Suppose you’re a decision-maker at a retail store. </span><span class="koboSpan" id="kobo.258.2">You want to understand the buying patterns of your customers to optimize product placement and boost sales. </span><span class="koboSpan" id="kobo.258.3">Here’s how you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">Apriori algorithm:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Set a minimum support </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.261.1">and confidence</span></strong></span><p class="list-inset"><span class="koboSpan" id="kobo.262.1">These are two key metrics in the </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.263.1">Apriori algorithm. </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">Support</span></strong><span class="koboSpan" id="kobo.265.1"> measures the frequency of an item set in all transactions, while </span><strong class="bold"><span class="koboSpan" id="kobo.266.1">confidence</span></strong><span class="koboSpan" id="kobo.267.1"> measures the likelihood that item Y is purchased when item X </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">is purchased.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">Generate </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.270.1">item sets</span></strong></span><p class="list-inset"><span class="koboSpan" id="kobo.271.1">The algorithm will start by creating a list of all individual items (item sets) that meet the minimum </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">support threshold.</span></span></p></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.273.1">Create rules</span></strong></span><p class="list-inset"><span class="koboSpan" id="kobo.274.1">For each item set, the </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.275.1">algorithm will generate rules that meet the minimum </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">confidence threshold.</span></span></p></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.277.1">Rank rules</span></strong></span><p class="list-inset"><span class="koboSpan" id="kobo.278.1">The rules are then ranked by their lift, another metric that measures how much more likely item Y is purchased when item X is purchased, compared to purchasing item </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">Y alone.</span></span></p></li>
</ol>
<p><span class="koboSpan" id="kobo.280.1">By following these steps, you</span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.281.1"> might discover rules such as {Diapers} -&gt; {Beer}, indicating that customers who buy diapers are likely to buy beer as well. </span><span class="koboSpan" id="kobo.281.2">This insight can be used to strategically place products in your store to </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">increase sales.</span></span></p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.283.1">Evaluation metrics</span></h2>
<p><span class="koboSpan" id="kobo.284.1">In association rule learning, the key </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.285.1">evaluation metrics are support, confidence, and lift. </span><span class="koboSpan" id="kobo.285.2">These metrics help in identifying the most relevant rules. </span><span class="koboSpan" id="kobo.285.3">However, it’s important to strike a balance. </span><span class="koboSpan" id="kobo.285.4">High support may lead to obvious rules, while high confidence may lead to overly specific rules. </span><span class="koboSpan" id="kobo.285.5">Lift, on the other hand, provides a balance by measuring the strength of a rule over the random occurrence of </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">item sets.</span></span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.287.1">In summary</span></h2>
<p><span class="koboSpan" id="kobo.288.1">Association rule learning is a powerful tool in the UL toolkit. </span><span class="koboSpan" id="kobo.288.2">It uncovers hidden patterns in large datasets, enabling businesses to make strategic decisions. </span><span class="koboSpan" id="kobo.288.3">Whether you’re in retail, marketing, or any industry dealing with large datasets, association rule learning can provide </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">valuable insights.</span></span></p>
<p><span class="koboSpan" id="kobo.290.1">In the next section, </span><em class="italic"><span class="koboSpan" id="kobo.291.1">Applications of UL</span></em><span class="koboSpan" id="kobo.292.1">, we’ll explore more applications of UL, diving deeper into how these techniques can be leveraged across various </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">business scenarios.</span></span></p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.294.1">Applications of UL</span></h1>
<p><span class="koboSpan" id="kobo.295.1">UL, as we’ve discussed, is a type of ML that</span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.296.1"> identifies patterns in data without the need for explicit supervision. </span><span class="koboSpan" id="kobo.296.2">It’s like a detective who arrives at a crime scene with no witnesses but must still piece together the story from the available evidence. </span><span class="koboSpan" id="kobo.296.3">But where does this kind of “detective work” find its application in the business world? </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">Let’s explore.</span></span></p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.298.1">Market segmentation</span></h2>
<p><span class="koboSpan" id="kobo.299.1">One of the most common applications of UL is in market segmentation. </span><span class="koboSpan" id="kobo.299.2">Businesses with a diverse customer base use clustering </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.300.1">algorithms to group customers based on their behavior, demographics, and purchase history. </span><span class="koboSpan" id="kobo.300.2">This allows them to tailor their marketing strategies to each group, maximizing engagement and </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">conversion rates.</span></span></p>
<p><span class="koboSpan" id="kobo.302.1">Consider a global retail brand with millions of customers. </span><span class="koboSpan" id="kobo.302.2">They could use UL to segment their customers into groups, such as “young professionals,” “parents,” or “retirees,” each with distinct shopping habits and preferences. </span><span class="koboSpan" id="kobo.302.3">The company could then create personalized marketing campaigns for each segment, increasing customer satisfaction </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">and loyalty.</span></span></p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.304.1">Anomaly detection</span></h2>
<p><span class="koboSpan" id="kobo.305.1">UL is also excellent at detecting anomalies</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.306.1"> or outliers in data. </span><span class="koboSpan" id="kobo.306.2">This is particularly useful in industries such as finance and cybersecurity, where identifying unusual patterns can prevent fraud or </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">security breaches.</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">For instance, a bank could use UL to monitor transactions and flag any that deviate significantly from a customer’s usual behavior, as shown in the following simple diagram. </span><span class="koboSpan" id="kobo.308.2">UL algorithms can identify anomalies by measuring the distance of a data point from the centroid of its assigned cluster. </span><span class="koboSpan" id="kobo.308.3">Data points that are far away from their cluster centroid are considered anomalies. </span><span class="koboSpan" id="kobo.308.4">This could indicate fraudulent activity, prompting the bank to take </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">preventive measures:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.310.1"><img alt="Figure 8.3: Anomaly detection for financial transactions" src="image/B19633_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.311.1">Figure 8.3: Anomaly detection for financial transactions</span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.312.1">Feature extraction</span></h2>
<p><span class="koboSpan" id="kobo.313.1">UL can also be used for feature extraction, which simplifies complex datasets by reducing their dimensionality. </span><span class="koboSpan" id="kobo.313.2">This can </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.314.1">make other ML tasks more efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">and accurate.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">For example, a car manufacturer might have data on hundreds of features for each vehicle. </span><span class="koboSpan" id="kobo.316.2">UL could identify the most important features that affect a car’s performance or popularity, allowing the manufacturer to focus on these in their design and </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">marketing efforts.</span></span></p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.318.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.319.1">We have learned that UL is a versatile tool that can uncover hidden insights from data and applies to many business use cases, such as market segmentation, anomaly detection, and </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">feature extraction.</span></span></p>
<p><span class="koboSpan" id="kobo.321.1">By grasping the capabilities of UL, decision-makers can harness it to uncover valuable insights, streamline processes, and make data-driven decisions that impact the </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">bottom line.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">Throughout this chapter, we’ve covered the fundamental concepts of UL, outlined its key steps, and explored some of its most prevalent real-world applications. </span><span class="koboSpan" id="kobo.323.2">We’ve also discussed methods for evaluating the performance of UL models in a </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">business setting.</span></span></p>
<p><span class="koboSpan" id="kobo.325.1">Building on this foundational knowledge of ML, the next chapter will take a closer look at strategies for interpreting and assessing ML models, equipping you with the tools needed to effectively communicate insights and justify decisions based on your </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">ML projects.</span></span></p>
</div>
</body></html>