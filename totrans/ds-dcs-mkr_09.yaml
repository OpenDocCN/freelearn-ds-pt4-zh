- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpreting and Evaluating Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The promise and potential of machine learning systems to create systems that
    can make decisions without the need for hardcoded rules or heuristics is huge.
    However, this promise is often far from straightforward to fulfil, and in developing
    machine learning models or leading teams who develop machine learning models,
    great care needs to be taken to ensure their accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to interpret and evaluate different machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of, if not the most important skill you can have in your toolkit
    as a decision-maker working on data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: While it can be convenient to allow data scientists to evaluate their own models
    and “mark their own homework,” this is a risky decision to make and will, invariably,
    eventually lead to problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I know whether this model will be accurate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods for explaining machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I know whether this model will be accurate?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As decision-makers, you need to be confident that the machine learning models
    you’re using provide you with reliable, accurate predictions or insights. However,
    how can you be sure? What metrics should you use to evaluate your models? And
    what do these metrics really mean?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s attempt to understand how metrics are used to evaluate machine learning
    models and look at some common examples.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating on test (holdout) data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get into the specifics around the different types of evaluation metrics,
    first, you need to understand the importance of evaluating on test (a.k.a. holdout)
    data.
  prefs: []
  type: TYPE_NORMAL
- en: One very important aspect of model evaluation is the use of holdout (or test)
    data. This is a subset of your data that the model hasn’t seen during training
    or validation. Evaluating your model on holdout data gives you a more realistic
    estimate of its performance in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: This test data should follow the same distribution of data that the model would
    see in the real world once it is in production. It should not be used in the training
    process or even for tuning different model hyperparameters, and care should be
    taken such that data do not leak between the training and test sets or between
    the independent variables and dependent (outcome) variables.
  prefs: []
  type: TYPE_NORMAL
- en: It is only with a good set of test data that you can accurately evaluate a model
    and gain some confidence in its performance once it goes live in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, an evaluation metric is a measure used to quantify the
    quality of a model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: If understood and interpreted correctly, they can provide you with a measure
    with which to evaluate the quality of a model and, therefore, make more informed
    decisions about its use or whether more work is needed to train a more accurate
    model.
  prefs: []
  type: TYPE_NORMAL
- en: There is a wide range of evaluation metrics within machine learning, and different
    types of machine learning models require different evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering supervised machine learning, which we covered in [*Chapter
    7*](B19633_07.xhtml#_idTextAnchor163), there are two groups of models: regression
    models and classification models, each with its own set of evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s look at some of the more common metrics used for evaluating regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re a retail executive trying to forecast the next quarter’s sales.
    You’ve built a regression model for this purpose. However, how can you assess
    its accuracy so that you can have some confidence in the next quarter’s sales
    projections?
  prefs: []
  type: TYPE_NORMAL
- en: Three common metrics for evaluating regression models are R-squared, **root
    mean squared error** (**RMSE**), and **mean absolute error** (**MAE**). Let’s
    discuss each in turn.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The R-squared metric, also known as the coefficient of determination, is a statistical
    measure in regression analysis that represents the proportion of the variance
    in the dependent variable that is **explained** by the model. In simpler terms,
    it’s a measure of how well the regression predictions approximate the real data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared formula
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for R-squared can be seen in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: R 2 = 1−  SS res _ SS tot
  prefs: []
  type: TYPE_NORMAL
- en: 'where we see the following denotations:'
  prefs: []
  type: TYPE_NORMAL
- en: SS res is the sum of the squares of the residuals, also known as the residual
    sum of squares. It measures the variability of the model’s errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SS tot is the total sum of the squares. It measures the total variability of
    the dependent variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sums of the squares are calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: SS res = ∑ (y i−  ˆ y  i) 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SS tot = ∑ (y i− _ y ) 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these formulas, we denote the following:'
  prefs: []
  type: TYPE_NORMAL
- en: y i is the actual observed value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ˆ y  i is the predicted value by the regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: _ y  is the mean of the observed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding R-squared
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An R-squared of 1 indicates that the regression predictions perfectly fit the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: An R-squared of 0 means that the model does not explain any of the variability
    of the response data around its mean.
  prefs: []
  type: TYPE_NORMAL
- en: Example of an R-squared calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine you work for a retailer and that you have a dataset of monthly sales
    volumes for one of your top products, as well as the predictions of those sales
    volumes from a regression model covering the past 6-months:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observed Data (y): [725, 693, 654, 712, 722, 695]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predicted Data ( ˆ y ) : [720, 695, 660, 715, 724, 698]'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, calculate the mean of the observed data ( _ y ):'
  prefs: []
  type: TYPE_NORMAL
- en: _ y  =  725 + 693 + 654 + 712 + 722 + 695   ________________________  6  = 700.17
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, calculate SS res and SS tot:'
  prefs: []
  type: TYPE_NORMAL
- en: SS res = (725 − 720) 2 + (693 − 695) 2 + (654 − 660) 2 + (712 − 715) 2 + (722
    − 724) 2 + (695 − 698) 2 = 87
  prefs: []
  type: TYPE_NORMAL
- en: SS tot = (725 −  _ y ) 2 + (693 −  _ y ) 2 + (654 −  _ y ) 2 + (712 −  _ y ) 2
    + (722 −  _ y ) 2 + (695 −  _ y ) 2 = 3442.8
  prefs: []
  type: TYPE_NORMAL
- en: R 2 = 1−  87 _ 3442.8  = 0.97
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, our R-squared value is approximately 0.97, indicating that
    the regression model can explain 97% of the observed data. This is a high value,
    suggesting that the model provides a very good fit for the data (this could be
    due to overfitting, which we will cover in a later chapter).
  prefs: []
  type: TYPE_NORMAL
- en: When interpreting R-squared values, there is no universal benchmark for a “good”
    R-square value, but understanding that the closer the value is to 1, the better
    the model is at explaining the data can help when comparing different models.
  prefs: []
  type: TYPE_NORMAL
- en: Two evaluation metrics that can be clearer to interpret are **RMSE** and **RAE**,
    which we will look into now.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RMSE is a widely used metric in regression analysis that measures the average
    magnitude of the errors between the values predicted by a model and the values
    observed. It gives an estimate of the standard deviation of the prediction errors.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike R-squared, which is a relative measure of fit, RMSE provides an absolute
    scale of measurement, giving a direct interpretation of the model’s prediction
    accuracy in the units of the variable of interest. It’s particularly useful in
    evaluating the precision of prediction models and is sensitive to large errors,
    making it a useful tool for assessing model performance.
  prefs: []
  type: TYPE_NORMAL
- en: RMSE formula
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for calculating the RMSE is this:'
  prefs: []
  type: TYPE_NORMAL
- en: √ ____________   1 _ n  ∑ i=1 n ( y i −  ˆ y  i) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'where we see the following denotations:'
  prefs: []
  type: TYPE_NORMAL
- en: n is the number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y i is the actual observed value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ˆ y  i is the predicted value by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This formula effectively measures the square root of the average squared differences
    between the actual and predicted values, providing a clear measure of model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Example of an RMSE calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By continuing with the retailer’s dataset from the R-squared example, where
    the observed monthly sales volumes (y) are [725, 693, 654, 712, 722, 695] and
    the predicted sales volumes ( ˆ y ) are [720, 695, 660, 715, 724, 698], we can
    calculate the RMSE as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the squared differences between the actual and predicted values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the average of these squared differences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the square root of this average to find the RMSE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s calculate the RMSE for this dataset. We know from the R-squared calculation
    that the sum of squared differences between the actual and predicted values is
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: SS res = ∑ (y i−  ˆ y  i) 2 = 87
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we know the number of observations is n = 6 (i.e., 6 months).
  prefs: []
  type: TYPE_NORMAL
- en: 'By plugging these numbers into our RMSE equation, we find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE = √ ____________   1 _ n  ∑ i=1 n ( y i −  ˆ y  i) 2  = √ _  1 _ 6  * 87 
    = 3.8
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RMSE values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The RMSE value gives insight into the average error in the same units as the
    response variable, making it intuitively easier to understand. A lower RMSE value
    indicates a better fit of the model to the data.
  prefs: []
  type: TYPE_NORMAL
- en: However, like R-squared, there’s no absolute “good” or “bad” threshold for RMSE,
    as it depends on the context of the data and the specific domain of its application.
    It’s best used comparatively to assess improvements in model accuracy or to compare
    performance across different models or datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the RMSE for the given dataset is approximately 3.8\. This indicates
    that, on average, the model’s predictions are within 3.8 units of the actual sales
    figures. This appears to be a very accurate set of predictions, but the context
    of the business needs and how this may compare to other models helps us understand
    the evaluation accuracy in context.
  prefs: []
  type: TYPE_NORMAL
- en: Practical tips for interpreting RMSE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some practical tips for interpreting RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparative analysis**: Use RMSE to compare model performance, especially
    when tweaking models or choosing between different types of models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit sensitivity**: Remember that RMSE is sensitive to the scale of the data,
    so interpret it in the context of the magnitude of your dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complement with other metrics**: Combine RMSE with other metrics, such as
    R-squared, to get a more holistic view of model performance. While RMSE provides
    a measure of accuracy in the response variable’s units, R-squared offers insight
    into the variance explained by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s consider one other evaluation metric for regression models: MAE.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MAE is a measure used in regression analysis to quantify the average magnitude
    of the errors between predicted values and observed actual outcomes without considering
    their direction. It calculates the average of the absolute differences between
    the predicted values and actual values, making it a simple yet clear metric for
    assessing model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: MAE formula
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for calculating MAE is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: MAE =  1 _ n  ∑ i=1 n |y i −  ˆ y  i|
  prefs: []
  type: TYPE_NORMAL
- en: 'where, as we have seen before, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: n is the number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y i is the actual observed value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ˆ y  i is the predicted value from the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This formula emphasizes the absolute value of the errors, thereby treating all
    errors with equal weight, regardless of their direction.
  prefs: []
  type: TYPE_NORMAL
- en: Example of an MAE calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuing with the retailer example used for the R-squared and RMSE calculations,
    as an exercise, you can calculate the MAE using the observed sales volume data
    (y) and predicted data ( ˆ y ) to understand (using a practical illustration)
    how MAE is determined.
  prefs: []
  type: TYPE_NORMAL
- en: A reminder that the absolute value of a difference is the positive magnitude
    of the difference. For example, the absolute value of |5 − 10| = |− 5| = 5 and
    the absolute value of, for example, |7 − 4| = |3| = 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MAE for the example sales volume data and predictions should be this:'
  prefs: []
  type: TYPE_NORMAL
- en: MAE = 3.5
  prefs: []
  type: TYPE_NORMAL
- en: See if you can calculate this answer using the sales volume values and predictions
    provided previously.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MAE values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MAE provides an intuitive understanding of the average error magnitude. A lower
    MAE value indicates a model with better predictive accuracy. Unlike RMSE, MAE
    is not as sensitive to outliers, as it does not square the errors before averaging.
    This characteristic makes MAE particularly useful in scenarios where you want
    to avoid the disproportionate effect of large errors.
  prefs: []
  type: TYPE_NORMAL
- en: Practical tips for interpreting MAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some practical tips for interpreting MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error interpretation**: Use MAE to get a direct understanding of the average
    error in the same units as the data. This makes it particularly accessible for
    non-technical stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier sensitivity**: Consider the nature of your data and whether emphasizing
    or de-emphasizing outliers is important. MAE treats all errors equally, making
    it a robust measure against large individual errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complementary metrics**: Just as with RMSE, it’s advisable to use MAE alongside
    other metrics to get a fuller picture of model performance. MAE can be particularly
    informative when used in conjunction with RMSE, as the two metrics together can
    provide insights into the error distribution and the presence of outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When and how to use each metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen each of these regression metrics, it is worthwhile discussing
    when to use each and how they complement each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R-squared**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: It is ideal for assessing the explanatory power of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is useful for comparing the model’s performance against a baseline model
    or other models on the same dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: Higher values (closer to 1) indicate a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: Consider using it in conjunction with other metrics for a comprehensive model
    evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSE**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: It is best for models where large errors are particularly undesirable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is suitable for comparing across models or model versions to gauge improvement
    in predictive accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: Lower values indicate a more accurate model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use it as a primary metric for precision, but analyze a model alongside R-squared
    to understand both fit and accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAE**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: Use it when you require a straightforward metric that is easy to explain and
    understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use it in scenarios where outliers are present but should not disproportionately
    impact the model’s error metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How to** **use it**:'
  prefs: []
  type: TYPE_NORMAL
- en: Lower values are better, indicating tighter conformity of predictions to actual
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider using MAE to complement RMSE for a nuanced view of error distribution
    and to assess the impact of outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical evaluation strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some of the most popular strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Balanced approach**: Utilize a combination of these metrics to get a holistic
    view of model performance. R-squared offers insights into how well the model explains
    the data. RMSE helps identify how large, on average, the errors are, with a penalty
    for larger errors. MAE provides a simple average error magnitude, which is useful
    for understanding the typical error size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual interpretation**: Always contextualize the metrics within your
    specific business or research objectives. A good metric value in one context might
    not be acceptable in another, depending on the precision required or the cost
    of errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparative analysis**: Use these metrics not just in isolation but also
    comparatively across different models or iterations of the same model. This can
    help with selecting the best model or refining a model to better meet objectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error sensitivity**: Consider the nature of your prediction task and the
    consequences of errors. If large errors are more problematic, RMSE will be particularly
    informative. If consistent errors are of concern, regardless of their size, MAE
    will provide valuable insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing the evaluation of regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By leveraging R-squared, RMSE, and MAE thoughtfully, as a decision-maker, you
    can critically assess model performance beyond just a single dimension of accuracy
    or fit. This multi-metric approach enables a more nuanced understanding and evaluation
    of regression models, guiding the selection, development, and refinement of models
    to align with specific business goals.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at evaluation metrics for regression models, let’s now
    turn to classification models and how they can be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are running a business with a large portfolio of customers, and
    you are trying to predict which customers are likely to stop using your service
    within the next year. This is a common binary classification model known as a
    customer churn model; many companies, whether banks, telecoms providers, insurance
    companies, or streaming services, can benefit from knowing which of their customers
    are most likely to churn so that they can take action to retain these customers.
  prefs: []
  type: TYPE_NORMAL
- en: You may have evaluated your customer churn model’s predictions on a test (holdout)
    set, for example, for the previous year, where you know whether a customer did,
    indeed, leave or stay with the company.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For this example, let’s refer to a customer who has churned as a “positive”
    outcome, as this is the outcome we are trying to predict (in this context, “positive”
    or “negative” does not have anything to do with the sentiment or favorability
    of the outcome).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four different types of outcomes you would observe when evaluating
    your model’s predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive**: A true positive would be when our model predicts that the
    customer churned, and the customer did, indeed, churn (i.e., a correct prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive**: A false positive, also known as a Type I error, is when
    our model predicts that a customer churned; however, the actual outcome was that
    a customer did **not** churn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative**: A true negative is when the model predicted that the customer
    did not churn, and the actual outcome is that the customer did not churn (i.e.,
    another correct prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative**: A false negative, also known as a Type II error, is when
    our model predicts that the customer did **not** churn; however, the actual outcome
    was that the customer did churn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these four types of results can help inform us how accurate the model
    (on the test set) is at predicting the different outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different evaluation metrics that we can use to calculate the counts
    of each of these outcomes, which we will see. A useful way to visualize classification
    results is with a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Confusion matrix for binary classification](img/B19633_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Confusion matrix for binary classification'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solidify our understanding of a confusion matrix, let’s map the outcomes
    from our example onto a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Confusion matrix for the binary classification of the customer
    churn example](img/B19633_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Confusion matrix for the binary classification of the customer
    churn example'
  prefs: []
  type: TYPE_NORMAL
- en: From the counts of these different outcomes on a test (holdout) dataset, we
    can calculate useful evaluation metrics for a machine learning classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look into some of these metrics and how they can be interpreted to help
    us understand the predictive power of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Classification model evaluation metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s consider our example of the customer churn model. Let’s say that
    on our test data, we predicted the outcome for 1,000 customers. Of these, 150
    were predicted to churn, and they did, in fact, churn (true positives), and the
    50 who we predicted to churn did not churn and stayed as customers (false positives).
    We also predicted that 600 customers would not churn, and they did not churn (true
    negatives). However, 200 customers who we predicted would not churn did churn
    (false negatives):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: Confusion matrix results for the binary classification of the
    customer churn example](img/B19633_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Confusion matrix results for the binary classification of the customer
    churn example'
  prefs: []
  type: TYPE_NORMAL
- en: From these values, we can calculate several useful metrics to help evaluate
    our model. Let’s take a look at some of these evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Precision, recall, and F1-Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first metric we will look at is precision.
  prefs: []
  type: TYPE_NORMAL
- en: Precision allows us to evaluate how **precisely** our model makes positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, precision is the ratio of true positive predictions to the **total**
    number of positive predictions made by the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: It answers the question, Of all the instances labeled as positive by my model,
    how many are actually positive?
  prefs: []
  type: TYPE_NORMAL
- en: Precision calculation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for calculating precision is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision =  True Positives (TP)  ____________________________   True Positives
    (TP) + False Positives (FP)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the precision would be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision =  150 _ 150 + 50  = 0.75
  prefs: []
  type: TYPE_NORMAL
- en: Precision can vary from 0 to 1, where precision of 1 implies that the model
    is perfectly precise at predicting the positive outcome (that the customer churns).
    Here, the precision suggests that when the model predicts that a customer will
    churn, then three out of four times, they will, in fact, churn.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Precision focuses solely on the model’s performance in accurately predicting
    the positive class. A high precision score indicates that the model is reliable
    in its positive classifications, meaning that when it predicts a positive result,
    you can be quite confident in its accuracy. However, precision does not take into
    account the false negatives (instances that are positive but predicted as negative)
    that are covered by another metric, such as recall, which we will discuss.
  prefs: []
  type: TYPE_NORMAL
- en: When to use precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some examples of when best to use precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High cost of false positives**: Precision is particularly useful when the
    cost of a false positive is high. For example, in email spam detection, a high
    precision is required because classifying an important email as spam (a false
    positive) could mean missing critical information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalanced datasets**: In datasets where the positive class is rare (imbalanced
    datasets), precision becomes a crucial measure to ensure that the positive predictions
    made by the model are, indeed, correct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision is a key metric in classification that helps assess the reliability
    of the positive predictions made by a model, making it very useful in contexts
    where false positives have significant consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall, also known as sensitivity or the true positive rate, is a critical performance
    metric used in classification tasks to evaluate the ability of a model to correctly
    identify all relevant instances of a particular class. It is especially important
    in situations where the cost of missing a positive instance (false negative) is
    high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a detailed explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall measures the proportion of actual positive cases that were correctly
    identified by the model. It addresses the question, Of all the actual positives
    in the dataset, how many were correctly identified as positive by the model?
  prefs: []
  type: TYPE_NORMAL
- en: Formula for recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for calculating recall is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall =  True Positives (TP)  _____________________________   True Positives
    (TP) + False Negatives (FN)
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, recall would be calculated according to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall =  150 _ 150 + 200  ≈ 0.43
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that our model correctly identifies less than half of the customers
    who churned.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall focuses on the model’s capability to find all the relevant cases within
    a dataset. A high recall score indicates that the model is effective at capturing
    the majority of positive instances, minimizing the number of false negatives.
    However, it does not account for the correctness of negative predictions, which
    is covered by specificity.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of our example, it may be the case that recall is more important
    than precision, as we may want to correctly identify all of the customers who
    will churn so that we can take some remedial actions to try and retain them, even
    at the cost of some potential false positives.
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, it is sometimes possible to change the **threshold** that
    the model uses to predict true or false outcomes to favor increasing recall at
    the cost of precision or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand which is more important for your business case:
    having fewer false positives (i.e., higher precision) or fewer false negatives
    (i.e., higher recall).'
  prefs: []
  type: TYPE_NORMAL
- en: When to use recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some examples of when best to use recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High cost of false negatives**: Recall is crucial in contexts where missing
    a positive instance is more critical than falsely identifying a negative instance
    as positive. For instance, in medical screening tests for diseases, a high recall
    is necessary to ensure that as many positive cases as possible are identified
    for further testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalanced datasets**: In datasets where the positive class is rare, maximizing
    recall ensures that the model does not overlook the few positive instances present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comprehensive coverage**: It is useful when the goal is to ensure no positive
    instance is missed, even at the expense of higher false positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical implications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While recall is an essential metric for evaluating the comprehensiveness of
    a model in identifying positive cases, focusing solely on recall can lead to models
    that classify too many instances as positive (high false positives), reducing
    precision. This is why recall is often used alongside precision to understand
    the trade-offs between capturing all positives and the accuracy of positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The balance between recall and precision is quantified by F1-score, which provides
    a single metric to assess model performance when both recall and precision are
    considered important, which we will look at now.
  prefs: []
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: F1-score is a helpful metric used in the evaluation of binary classification
    models, especially in situations where the balance between precision and recall
    is important. It is particularly useful when dealing with datasets that have an
    uneven class distribution or when the cost of false positives and false negatives
    varies significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of F1-score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: F1-score is the harmonic mean of precision and recall, providing a single metric
    that balances both the model’s ability to correctly identify positive instances
    (recall) and the accuracy of these positive identifications (precision). Unlike
    the arithmetic mean, the harmonic mean gives a higher weight to lower numbers,
    meaning that F1-score will be more influenced by lower precision or recall. This
    makes F1-score a stringent measure of a model’s accuracy, which is especially
    useful when you seek a balance between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Formula for F1-score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The formula for calculating F1-score is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score = 2 ×  Precision × Recall  _____________  Precision + Recall
  prefs: []
  type: TYPE_NORMAL
- en: Understanding F1-score
  prefs: []
  type: TYPE_NORMAL
- en: F1-score ranges from 0 to 1, where a score of 1 indicates perfect precision
    and recall, and a score of 0 indicates the worst. A high F1-score suggests that
    the model has a robust balance between precision and recall, managing to accurately
    identify a high proportion of actual positives while minimizing the number of
    false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: When to use F1-score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some examples of when best to use F1-score:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imbalanced classes**: It is particularly useful in scenarios where there
    are significantly more instances of one class than another, and the cost of false
    positives and false negatives are both critical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-off analysis**: It is ideal when you need to evaluate models based
    on their balance between precision (the quality of the positive predictions) and
    recall (the completeness of the positive predictions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparative model evaluation**: When comparing models and a balance between
    precision and recall is desired, F1-score provides a single metric to assess performance,
    simplifying decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical implications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: F1-score is an essential tool in the model evaluation process, allowing for
    a more nuanced assessment than evaluating precision or recall independently. However,
    it’s important to consider the specific context of your application.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, precision might be more important than recall or vice versa.
    Adjusting the emphasis on one over the other might be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: F1-score assumes the equal importance of precision and recall, which might not
    always align with business objectives or cost considerations.
  prefs: []
  type: TYPE_NORMAL
- en: F1-score is a powerful metric for assessing the accuracy of binary classification
    models, particularly in complex scenarios where both the ability to correctly
    identify positive instances and the precision of these identifications are important.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside evaluating the accuracy of machine learning models, it is also important
    to understand how they make decisions. This can often be a difficult process,
    as many machine learning models can seem like a “black box” to the user. However,
    some machine learning models are more explainable than others, and even for those
    less explainable models, there exist a number of techniques in the field of “Explainable
    AI” that aims to shine a light on the decision-making process of what can be opaque
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for explaining machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incorporating methods for interpreting and explaining machine learning models
    into your analytical toolkit can enhance transparency and provide insight into
    the decision-making process used by a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: In some industries, explainability is an important aspect to consider; for example,
    in sensitive sectors, such as medicine and law, opaque “black-box” models are
    insufficient in scenarios where the reasoning behind how a machine learning model
    made a prediction is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first look at a simple example, using coefficients to understand regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Making sense of regression models – the power of coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re using a regression model to predict future sales based on various
    factors such as marketing spend, seasonality, and product price. In this context,
    interpreting coefficients becomes akin to decoding the direct influence each factor
    has on your sales.
  prefs: []
  type: TYPE_NORMAL
- en: A positive coefficient for marketing spend would suggest that increasing your
    marketing budget is likely to boost sales, whereas a negative coefficient for
    product price might indicate that higher prices could deter customers. Understanding
    these coefficients empowers you to prioritize investments and strategic initiatives
    effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding classification models – unveiling feature importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying classification models—say, to identify which customers are most
    likely to churn or to flag potentially fraudulent transactions—understanding feature
    importance is key. This method ranks the attributes (e.g., customer behavior patterns
    and transaction sizes) according to their impact on the model’s predictions. By
    focusing on the most influential factors, you can tailor interventions more precisely,
    whether that’s through personalized retention strategies or targeted fraud prevention
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you had a machine learning model that predicted the predicted the expected
    spending of a customer in the next year. This machine learning model has been
    trained on a range of different features to predict the value of a customer. Generating
    a feature importance plot (by following the training of a model) can explain which
    of the features are more important to the model in making predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: Feature importance plot](img/B19633_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Feature importance plot'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, you can see that the features related to the previous
    number of transactions and spend from the customer are more important to the model
    in predicting their next year spend, which is contrast to more superfluous information
    such as page views and whether their cookies are enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond specific models – universal insights using SHAP values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of whether you’re working with regression, classification, or any
    other predictive model, **SHAP** (**SHapley Additive exPlanations**) offers a
    powerful, model-agnostic approach to explanation. SHAP values dissect any prediction
    to reveal the contribution of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if a loan application is predicted to be high risk, SHAP can show
    you exactly how factors such as the applicant’s credit score, income, and loan
    amount contributed to this assessment. This level of insight is invaluable for
    refining risk models, addressing customer inquiries about decision outcomes, and
    ensuring compliance with regulatory requirements for explainability.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5: SHAP value waterfall plot](img/B19633_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: SHAP value waterfall plot'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the preceding chart, a SHAP plot for an individual prediction
    on a house pricing dataset is shown. In this case, you can see the contribution
    to the prediction from each feature to the final prediction. For example, you
    can see that the age of the house (HouseAge) had a negative effect on the predicted
    house price.
  prefs: []
  type: TYPE_NORMAL
- en: This is an incredibly useful tool for explaining individual predictions, particularly
    for models that are not inherently explainable. You can imagine situations where
    a model’s decisions may need to be audited or explained following, for example,
    a complaint or investigation, and without tools such as SHAP, this can be a difficult
    situation for companies to find themselves in.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter on *Interpreting and Evaluating Machine Learning Models* emphasizes
    the critical importance of understanding, interpreting, and evaluating **machine
    learning** (**ML**) models in the context of data science projects. It highlights
    that the potential of ML systems to make decisions without hardcoded rules presents
    significant opportunities, yet realizing this potential is complex and requires
    the careful evaluation of models to ensure accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaways from this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The necessity of evaluating ML models on test (holdout) data to get a realistic
    estimate of their performance in real-world scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of various evaluation metrics, such as R-squared, RMSE, and MAE,
    for regression models, and precision, recall, and F1-score for classification
    models. These metrics help decision-makers understand a model’s accuracy, how
    well it fits the data, and its predictive power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discussion on feature importance and methods such SHAP values for explaining
    predictions provides tools for understanding how different features influence
    a model’s outcomes. This is crucial for both interpreting complex models and making
    informed decisions based on their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter concluded by stressing that effectively evaluating and interpreting
    ML models is essential for making informed business decisions. By understanding
    evaluation metrics, using holdout data, and interpreting feature importance, stakeholders
    can gain confidence in their models’ accuracy and usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: As we transition into the next chapter, *Common Pitfalls in Machine Learning*,
    we build on the foundation laid in evaluating and interpreting models by exploring
    the common challenges encountered in ML projects. This includes issues such as
    overfitting, underfitting, data quality, the curse of dimensionality, model complexity,
    and the trade-offs between model accuracy and interpretability. Understanding
    these challenges is crucial for developing effective ML solutions that are robust,
    reliable, and aligned with business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will delve into these challenges, providing insights into navigating
    the complexities of ML projects and strategies for mitigating common pitfalls,
    thereby enhancing the success and impact of ML initiatives in real-world applications.
  prefs: []
  type: TYPE_NORMAL
