- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common Pitfalls in Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Picture this: a seasoned data science manager just launched a new recommendation
    engine to boost product sales. The model performed brilliantly in tests, but now,
    customer interest is lukewarm. The problem? The model had gotten too good at mirroring
    the training data – niche tastes of early adopters that didn’t reflect broader
    customer preferences.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) promises incredible things, but it’s dangerously
    easy to stumble. According to a survey of over 500 developers working with ML
    systems (https://www.civo.com/newsroom/ai-project-failure), more than half (53%)
    of respondents have abandoned between 1% and 25% of ML projects, with an additional
    24% having left between 26% and 50% of projects. Only 11% of developers said they
    have never abandoned a project. The first lesson is this: ML isn’t some magic
    algorithm that just needs data. It’s about understanding what kind of model is
    right for the job, ensuring your data actually teaches the right lessons, and
    knowing when your model might be getting tripped up.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the complexity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dirty data, damaged models – how data quantity and quality impacts ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overcoming overfitting and underfitting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastering overfitting and underfitting for optimal model performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training-serving skew and model drift
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and fairness
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the complexity
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, let’s acknowledge that ML is a complex field, and it’s not just about
    crunching numbers. It involves intricate algorithms, vast amounts of data, and
    the ability to interpret and apply the results in a meaningful way.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a marketing executive at a consumer goods company. You have access
    to a wealth of customer data and want to use ML to predict which customers are
    most likely to buy your new product.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Sounds straightforward, right? But there are many places where complexity can
    come in. We will briefly explain some of the key considerations, then go into
    each one in more detail:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality and quantity**: Is your data clean and representative of your
    target population? Do you have enough high-quality data?'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection and tuning**: Have you selected the appropriate model for
    your data? Have you correctly trained or fine-tuned your model?'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting and underfitting**: Is your model too complex and just memorizing
    the training data (overfitting)? Or is it too simple and missing important patterns
    (underfitting)?'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training-serving skew**: Will your model perform as well in the real world
    as it does on your training data?'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model drift**: How will your model perform over time as the underlying data
    changes?'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness**: Is your model biased against certain groups? Is it treating different
    sub-groups based on characteristics such as gender, age, and ethnicity fairly?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some of the key considerations to have in mind when training ML models,
    which can initially sound overwhelming. However, by looking at each of these in
    turn, with some concrete examples, by the end of this chapter, you should be better
    equipped to know what to look out for. You will also know the steps you, or your
    team, can take to mitigate the challenges associated with deploying ML models
    to production.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Dirty data, damaged models – how data quantity and quality impact ML
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training or using ML and artificial intelligence models, data is not only
    an asset but also the foundation of success. Without high-quality, representative
    data, even the most sophisticated ML model is useless. But what happens when you
    don’t have enough data, or when the data you have is biased or inaccurate?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: To consider one hypothetical example, many banks use ML to flag potentially
    fraudulent transactions and block accounts based on information about the transaction.
    Imagine the model was only trained on a subset of account types, such as current
    accounts that have more regular, lower-value transactions. Let’s say the bank
    decides to then also apply the model to savings accounts that may have larger,
    less frequent transactions. The model may now incorrectly flag most typical savings
    account transactions as false positives, leading to frustrated customers and stressed
    customer service teams.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: To look at another example, imagine a large language model-based customer service
    chatbot. Let’s say this chatbot was trained primarily on interactions where customers
    are expressing frustration or dissatisfaction. The chatbot learns to associate
    most customer inquiries with negativity. A consequence could be that the chatbot
    becomes overly apologetic or defensive, even in neutral conversations. It may
    misunderstand simple requests and misinterpret the customers’ intent, hindering
    effective customer support.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look into the common data considerations around quantity
    and quality that can affect your ML models and how to address them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The importance of adequate training data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re a coach training a team for a basketball tournament. If you only
    train it on shooting free throws, it will struggle when faced with other aspects
    of the game such as defense or three-point shooting. Similarly, an ML model trained
    on insufficient or unrepresentative data will struggle to make accurate predictions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In industries such as market research and consumer goods, for instance, if a
    model is trained only on data from urban consumers, it may not perform well when
    applied to rural consumers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: For many ML models, particularly deep learning models, the quantity of data
    is paramount.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the challenge
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To mitigate this challenge, we must do the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect a sufficient volume of data**: This may seem like a brute-force approach,
    but in many cases, the best way to improve the accuracy of ML models, and particularly
    deep learning models, is to increase the volume of data the model is being trained
    on. One way this may be achieved is via collecting data over a longer period.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集足够的数据量**：这可能看起来像是一种蛮力方法，但在许多情况下，提升机器学习模型，特别是深度学习模型准确度的最佳方式，是增加模型训练所用数据的数量。实现这一目标的一种方法是通过在更长时间内收集数据。'
- en: '**Collect diverse data**: Ensure your training data covers a wide range of
    scenarios your model is likely to encounter. This may be achieved by expanding
    the sources where data is acquired, either internal data sources (first-party
    data), or external data sources (second- and third-party data). It is important,
    however, to expand data coverage to include only relevant data that is representative
    of the data your model will see in production. For example, in the previous chatbot
    use case, expanding the data to cover all types of customer interactions could
    benefit the accuracy and reliability of the model. However, adding irrelevant
    chatbot data, say from a different company or industry, may have the opposite
    effect and lead to a less reliable model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集多样化的数据**：确保你的训练数据涵盖模型可能遇到的广泛场景。这可以通过扩展数据获取的来源来实现，可以是内部数据源（第一方数据）或外部数据源（第二方和第三方数据）。然而，重要的是要扩展数据覆盖面，只包括与你模型在生产环境中会接触到的相关数据。例如，在前面的聊天机器人案例中，扩展数据以涵盖所有类型的客户互动可能有助于提升模型的准确性和可靠性。然而，添加无关的聊天机器人数据，例如来自不同公司或行业的数据，可能会产生相反的效果，导致模型的可靠性下降。'
- en: '**Use data augmentation techniques**: Data augmentation is the process of adjusting
    or augmenting data examples you already have. These techniques can artificially
    expand your dataset by creating variations of existing data points. For example,
    within image recognition, one common data augmentation approach is to adjust existing
    images by rotating, zooming, blurring, and cropping them, increasing the volume
    of the training data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用数据增强技术**：数据增强是调整或增加你已有数据样本的过程。这些技术可以通过创建现有数据点的变体，人工扩大你的数据集。例如，在图像识别中，一种常见的数据增强方法是通过旋转、缩放、模糊和裁剪现有图像来增加训练数据的量。'
- en: '**Generate synthetic data**: Synthetic data refers to artificially created
    data that closely mirrors the characteristics and patterns of real-world data.
    This can be particularly beneficial when real-world data is scarce, sensitive,
    or difficult to obtain. In the case of **large language models** (**LLMs**), these
    can be used to generate realistic synthetic data that can be used for fine-tuning
    models for specific tasks. LLMs excel at creating text-based data and can be fine-tuned
    to produce diverse and targeted variations, filling gaps in your original datasets
    and ensuring your model is better prepared for various real-world scenarios.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成合成数据**：合成数据是指人工创建的数据，能够紧密模拟现实世界数据的特征和模式。当现实世界的数据稀缺、敏感或难以获取时，这种数据尤其有益。以**大型语言模型**（**LLM**）为例，这些模型可以用来生成逼真的合成数据，以便为特定任务对模型进行微调。LLM擅长创建基于文本的数据，并可以微调以产生多样化和有针对性的变体，从而填补原始数据集的空白，确保模型更好地为各种现实世界场景做好准备。'
- en: Dealing with poor data quality
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理不良数据质量
- en: Poor data quality, such as missing values, inconsistencies, and outright errors,
    significantly hinders the performance of ML models. Imagine trying to teach someone
    math with a textbook full of typos and incorrect formulas – they’ll struggle to
    learn the concepts correctly. Similarly, a model trained on flawed data will produce
    unreliable results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 较差的数据质量，如缺失值、不一致性和明显错误，会显著阻碍机器学习模型的表现。想象一下，如果你用一本充满错别字和不正确公式的数学教材教别人数学——他们将难以正确理解概念。同样，基于有缺陷数据训练的模型将产生不可靠的结果。
- en: Consider, for example, an image recognition model within the healthcare technology
    industry that is trained to detect tumors from MRI scans. If the images this model
    is trained on are poorly labeled, it could lead to potentially disastrous results,
    with tumors going undetected and false positives being flagged. For critical applications
    such as this, having very high data quality is one of the most important considerations,
    if not the most important.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Take another example. A natural language processing model may be fine-tuned
    for content moderation on a social media platform. If the training data is poorly
    labeled (e.g., sarcastic statements flagged as hate speech) or lacks diverse examples,
    the model will struggle. This could lead to false positives, where legitimate
    content might be wrongly removed, restricting freedom of expression. Additionally,
    the poor quality training data could lead to the model producing false negatives,
    where actual hate speech might slip through, making the platform unsafe for users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the challenge
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various methods to mitigate the challenge of poor data quality, which
    we will describe in this section. However, often, the best place data quality
    can be addressed is at the source.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an ML model trained to classify customers in a CRM as
    likely or unlikely to churn. Have the customers, and all the information about
    them, been accurately entered into the CRM? Is there any validation on the forms
    to make sure invalid data has not been entered, or is important data missing for
    some customers? Are there processes for the business teams to follow when entering
    data? If there is poor quality or missing data, can this be manually fixed by
    business teams, or the data science teams themselves?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: This is all mundane stuff, but if, by the time data is in the hands of data
    scientists and ML engineers, it is already of poor quality, there is only so much
    that can be done with the more automated processes we will explain here. As the
    well-known expression goes, garbage in, garbage out.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some techniques that data scientists, engineers, and analysts can
    use to mitigate poor data quality:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning**: There are several techniques that data scientists can apply
    to clean data before training ML models, including the following:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing values**: Decide whether to remove entries with missing data or replace
    missing values with estimates (e.g., mean or median)'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicates**: Remove redundant entries that can skew results'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistencies**: Correct formatting errors (e.g., date formats), and standardize
    entries for better model understanding (e.g., convert all addresses to lowercase)'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: Data scientists can apply techniques to validate data
    and exclude or fix invalid data before training ML models. It is also important
    that when the model is in production (i.e., after training and during inference),
    the same processes for data cleaning and validation are applied:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range checks**: Ensure values fall within an acceptable range (e.g., a person’s
    age can’t be negative)'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围检查**：确保数值在可接受的范围内（例如，某人的年龄不能是负数）'
- en: '**Format checks**: Verify that data adheres to specific formats (e.g., phone
    numbers, zip codes)'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**格式检查**：验证数据是否符合特定格式（例如，电话号码、邮政编码）'
- en: '**Cross-field checks**: Ensure consistency across related data fields (e.g.,
    if a country is “USA,” the state field should match the list of US states)'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨领域检查**：确保相关数据字段的一致性（例如，如果国家是“美国”，州字段应该与美国各州的列表匹配）'
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Bad data will sabotage your ML models, plain and simple. Addressing these issues
    is essential to success, and we have covered some of the techniques that you can
    leverage in your next data science project. These techniques include improving
    data collection by expanding the scope and coverage of the training data, augmenting
    data, and synthesizing data if appropriate, as well as improving the quality of
    data through data cleaning and validation. These are the hard yards that will
    set up your project for success. The importance of data cannot be emphasized enough,
    to the extent that there is a growing approach called Data-centric AI ([https://datacentricai.org/](https://datacentricai.org/)),
    which is the discipline of systematically engineering the data used to build an
    AI system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕的数据会破坏你的机器学习模型，这一点非常简单。解决这些问题对成功至关重要，我们已经介绍了一些你可以在下一个数据科学项目中利用的技术。这些技术包括通过扩展训练数据的范围和覆盖面来改善数据收集、增加数据量以及在适当情况下合成数据，还包括通过数据清理和验证来提高数据的质量。这些艰苦的努力将为你的项目奠定成功的基础。数据的重要性无法过分强调，甚至有一个日益增长的方法叫做数据中心人工智能（[https://datacentricai.org/](https://datacentricai.org/)），这是系统性地工程化构建人工智能系统所使用的数据的学科。
- en: 'Next, we will explore another key challenge: ensuring your model doesn’t just
    memorize your training data but can also learn to generalize to new situations.
    That means understanding and avoiding overfitting and underfitting.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨另一个关键挑战：确保你的模型不仅仅是记住训练数据，而是能够学习如何推广到新的情况。这意味着要理解和避免过拟合与欠拟合。
- en: As we move to the next section, we’ll explore this critical aspect of ML – over-
    and underfitting. How can we ensure our model performs well, not just on our current
    data, but also on new, unseen data?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入下一个部分时，我们将探讨机器学习中的一个关键方面——过拟合和欠拟合。我们如何确保模型不仅在当前数据上表现良好，而且在新的、未见过的数据上也能表现得很好？
- en: Overcoming overfitting and underfitting
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服过拟合和欠拟合
- en: Choosing the right complexity for your model is a delicate balancing act. If
    your model is too complex, it might overfit the training data, meaning it performs
    well on the training data but poorly on new, unseen data. On the other hand, if
    your model is too simple, it might underfit the data, missing important patterns
    and leading to inaccurate predictions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为模型选择合适的复杂度是一项微妙的平衡工作。如果模型过于复杂，可能会对训练数据过拟合，这意味着它在训练数据上表现很好，但在新的、未见过的数据上表现较差。另一方面，如果模型过于简单，可能会欠拟合数据，错过重要的模式，导致不准确的预测。
- en: Imagine you’re a market researcher trying to predict consumer trends. An overfitted
    model might capture every minor fluctuation in past trends but fail to generalize
    to future trends. An underfitted model might miss important trends altogether.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一个市场研究员，试图预测消费者趋势。一个过拟合的模型可能会捕捉到过去趋势中的每一个微小波动，但无法推广到未来的趋势。而一个欠拟合的模型可能会完全错过重要的趋势。
- en: Navigating training-serving skew and model drift
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导航训练-服务偏差和模型漂移
- en: In an ideal world, your model would perform just as well in the real world as
    it does on your training data. But this is rarely the case. This discrepancy is
    known as **training-serving skew**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，你的模型在现实世界中的表现应该和在训练数据上表现得一样好。但现实中很少发生这种情况。这种差异被称为**训练-服务偏差**。
- en: Furthermore, as the underlying data changes over time, your model’s performance
    can degrade. This is known as **model drift**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着基础数据随时间变化，你的模型性能可能会下降。这就是所谓的**模型漂移**。
- en: Imagine you’re developing an ML model to predict customer churn for a telecommunications
    company. During the model training phase, you use a dataset that includes customer
    information such as demographics, usage patterns, and customer service interactions.
    However, when the model is deployed in the production environment (the serving
    phase), you realize that the data pipeline feeding the model is missing some important
    features, such as the latest customer service interaction data. This discrepancy
    between the data used for training and the data available during serving is a
    classic example of training-serving skew.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the model’s performance in production may suffer because it
    was trained on a more comprehensive dataset than what is available in the serving
    environment. The missing features during serving can lead to inaccurate predictions
    and suboptimal decision-making.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: To address training-serving skew, it’s important to ensure consistency between
    the data used for training and the data available during serving. This can involve
    regularizing data pipelines, monitoring data quality, and implementing data validation
    checks to catch any discrepancies early in the process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring fairness
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, it’s important to ensure that your models are fair and don’t discriminate
    against certain groups. This can be a challenge, especially when the training
    data itself is biased.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you’re an HR manager using ML to screen job applicants.
    If your training data is biased against certain groups, your model might unfairly
    reject qualified candidates from these groups.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve explored why ML can be hard, touching on challenges such
    as data quality and quantity, overfitting and underfitting, training-serving skew,
    model drift, and fairness. But don’t be discouraged. In the following sections,
    we’ll dive deeper into these challenges and provide practical strategies to overcome
    them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Mastering overfitting and underfitting for optimal model performance
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ML, achieving reliable predictions is often the main goal. Overfitting and
    underfitting are two common obstacles to this goal. Let’s break down these concepts
    and outline concrete techniques to build better models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting – when your model is too specific
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your model as a student preparing for a test. Overfitting occurs when
    the student memorizes the practice questions perfectly but struggles to answer
    variations of the same questions on the actual exam. Similarly, an overfitted
    model gets too focused on the details of the training data, including random noise,
    and fails to grasp the bigger picture.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Real-world consequences
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Market research**: A model obsessively tuned to existing customers’ data
    won’t be able to predict the behavior of new prospects with different characteristics'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retail recommendations**: A system trained exclusively on a loyal customer’s
    purchase history may offer irrelevant suggestions when trying to attract new shoppers'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting – when your model is too simplistic
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欠拟合——当你的模型过于简单时
- en: Picture underfitting as a student who only grasps the most basic concepts of
    a subject. They’ll fail the exam regardless of whether the questions are from
    practice problems or new material. Similarly, an underfitted model misses important
    relationships within the data and performs poorly across the board.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将欠拟合想象为一个仅掌握最基础概念的学生。不管考试题目是来自练习题还是新内容，他们都将不及格。同样，一个欠拟合的模型会忽略数据中的重要关系，整体表现较差。
- en: Real-world consequences
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界的后果
- en: '**Sales forecasts**: A model that ignores factors such as seasonality or marketing
    promotions will consistently underestimate or overestimate potential sales'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**销售预测**：忽略季节性或营销推广等因素的模型会始终低估或高估潜在销售额'
- en: Spotting the problem
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现问题
- en: '**Red flag**: Excellent performance on training data but terrible results on
    new data is a classic sign of overfitting'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警示信号**：在训练数据上表现优秀，但在新数据上表现糟糕，是过拟合的典型迹象'
- en: '**Warning sign**: If your model struggles with both training data and new data,
    it’s likely due to underfitting'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警告信号**：如果你的模型在训练数据和新数据上都表现不佳，可能是由于欠拟合'
- en: Solutions for building better models
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建更好模型的解决方案
- en: 'The following are some solutions for building good models:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些构建优秀模型的解决方案：
- en: '**More data = stronger foundation**: Larger, more diverse datasets help the
    model identify real trends, not just random fluctuations in the training sample.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多数据 = 更强的基础**：更大、更具多样性的数据集帮助模型识别真实的趋势，而不是训练样本中的随机波动。'
- en: '**Feature selection = laser focus**: Carefully choose the most relevant data
    features. Get rid of those that only add confusion, not insight.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择 = 激光聚焦**：仔细选择最相关的数据特征。去除那些只会增加混乱而非提供洞察的特征。'
- en: '**Example – predicting customer churn**: Imagine you’re predicting customer
    churn for a telecom company. Your dataset includes relevant features such as monthly
    charges and customer service calls, but also an irrelevant feature: favorite ice
    cream flavor. Including “favorite ice cream flavor” adds noise and makes it harder
    for the algorithm to identify important patterns. By selecting only relevant features,
    you create a focused model that zeroes in on key factors driving churn. Remember,
    more data isn’t always better. Quality and relevance matter most.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 – 预测客户流失**：假设你正在预测电信公司客户的流失。你的数据集包括了相关特征，如每月费用和客户服务电话，但也有一个无关的特征：最喜欢的冰淇淋口味。包含“最喜欢的冰淇淋口味”会增加噪声，使算法更难识别重要的模式。通过仅选择相关特征，你可以创建一个专注的模型，专注于推动流失的关键因素。记住，更多数据不一定更好。质量和相关性才是最重要的。'
- en: '**Regularization = the guardrails**: Regularization is a technique that adds
    penalties to the model during training to prevent it from becoming too complex
    and too reliant on the peculiarities of the training data.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化 = 防护栏**：正则化是一种技术，在训练过程中对模型添加惩罚，以防止其变得过于复杂并过度依赖训练数据的特殊性。'
- en: '**Example – predicting house prices**: When building a model to predict house
    prices, regularization acts as a safeguard. It discourages the model from giving
    too much importance to a few unusual, expensive houses with unique features in
    the training data. By adding these penalties, regularization helps the model generalize
    better to new, unseen data, rather than getting stuck on the specifics of the
    training data.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 – 预测房价**：在构建房价预测模型时，正则化起到了保护作用。它避免模型过度关注训练数据中一些特殊且昂贵的房屋，这些房屋具有独特的特征。通过添加这些惩罚，正则化帮助模型更好地对新、未见过的数据进行泛化，而不是被训练数据的细节所困扰。'
- en: '**Cross-validation = reality check**: Cross-validation is a method that helps
    assess how well a model will perform on new, unseen data by simulating real-world
    conditions.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证 = 现实检查**：交叉验证是一种方法，帮助评估模型在新数据（即未见过的数据）上的表现，模拟现实世界的条件。'
- en: '**Example – sentiment analysis of movie reviews**: When building a model to
    predict the sentiment of movie reviews, cross-validation provides a reality check.
    Instead of training the model on all the data and assuming it’s performing well,
    you split the data into subsets. You train the model on some subsets and test
    it on others. By doing this multiple times, you get a more realistic estimate
    of how the model will perform on new data. This helps you catch whether the model
    is just memorizing the training data instead of learning to generalize to new
    reviews.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例子——电影评论的情感分析**：当构建一个预测电影评论情感的模型时，交叉验证提供了一个现实检查。你将数据拆分成多个子集，而不是在所有数据上训练模型并假设其表现良好。你在某些子集上训练模型，并在其他子集上进行测试。通过多次这样做，你可以更实际地估计模型在新数据上的表现。这有助于你发现模型是否仅仅是在记忆训练数据，而不是学习如何推广到新的评论。'
- en: 'One way to visualize the trade-off between underfitting and overfitting is
    by looking at a bias-variance trade-off chart:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 观察欠拟合与过拟合之间权衡的一个方法是查看偏差-方差权衡图：
- en: '![Figure 10.1: Bias-variance trade-off](img/B19633_10_1.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1：偏差-方差权衡](img/B19633_10_1.jpg)'
- en: 'Figure 10.1: Bias-variance trade-off'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：偏差-方差权衡
- en: The chart (*Figure 10**.1*) visualizes the relationship between a model’s **complexity**,
    **generalizability**, and **accuracy** on unseen data. This is a very important
    concept to business-focused decision-makers in data science, ML, and AI because
    it directly impacts the real-world performance of their models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图表（*图 10.1*）展示了模型的**复杂度**、**泛化能力**和**在未见数据上的准确性**之间的关系。对于专注于业务的决策者，尤其是在数据科学、机器学习和人工智能领域，这是一个非常重要的概念，因为它直接影响到他们模型的实际表现。
- en: Understanding the axes
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解坐标轴
- en: '**X-axis (model complexity)**: This represents how flexible or complex your
    model is. Simpler models are on the left, while more complex models are on the
    right.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X 轴（模型复杂度）**：这表示你的模型的灵活性或复杂性。较简单的模型位于左侧，而更复杂的模型位于右侧。'
- en: '**Y-axis (error)**: This represents the overall error of your model, which
    is a combination of two key factors: bias and variance.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y 轴（误差）**：这表示模型的整体误差，它是两个关键因素——偏差和方差——的结合。'
- en: Key parts of the chart
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图表的关键部分
- en: 'Let’s explore the chart with a real example. Imagine you’re building a model
    to predict which customers are likely to stop using your product or service (churn):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际例子来探索这个图表。假设你正在构建一个模型，用来预测哪些客户可能停止使用你的产品或服务（流失）：
- en: '**Bias**: This refers to the systematic error introduced by the model itself.
    It’s the consistent difference between the model’s predictions and the actual
    values. A high bias means the model consistently misses the mark, regardless of
    the specific data point. A very simple model might only look at one feature, such
    as a customer’s average purchase amount. This model is likely to have high bias
    because it ignores a whole host of complex factors that contribute to churn (support
    experience, competitor offerings).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：这指的是模型本身引入的系统性误差。它是模型预测与实际值之间的持续差异。高偏差意味着无论具体的数据点如何，模型总是会偏离正确答案。一个非常简单的模型可能只关注一个特征，比如客户的平均购买金额。这个模型很可能有高偏差，因为它忽略了许多复杂的因素，这些因素也在推动客户流失（例如，客户支持体验，竞争者产品）。'
- en: '**High Variance (Overfitting)**: A very complex model with a huge number of
    features might fit the training data perfectly. However, it might pick up on irrelevant
    patterns or random fluctuations in your historical data, leading to inconsistent
    predictions for new customers (like darts going all over the board). This model
    would perform well on the data it was trained on but fail to generalize and predict
    new churn reliably.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高方差（过拟合）**：一个非常复杂的模型，具有大量特征，可能会完美地拟合训练数据。然而，它可能会捕捉到与你的历史数据无关的模式或随机波动，从而导致对新客户的预测不稳定（就像飞镖四处乱飞）。这个模型在训练数据上表现良好，但无法推广并可靠地预测新的流失客户。'
- en: '**Optimal Balance**: An ideal model would be complex enough to capture the
    key factors driving churn without overfitting to the specifics of your training
    data. This balance would lead to the lowest overall error rate (**Total Error**
    on the chart), successfully identifying those customers genuinely at risk of churn.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳平衡**：理想的模型应该足够复杂，能够捕捉到驱动客户流失的关键因素，同时不会对训练数据的具体细节过拟合。这个平衡将导致最低的整体误差率（图表中的**总误差**），成功地识别出那些真正面临流失风险的客户。'
- en: The trade-off
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权衡
- en: 'The key takeaway from this chart is the trade-off between bias and variance:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表的关键要点是偏差和方差之间的权衡：
- en: '**Simpler models (left side)**: These tend to have high bias (systematically
    missing the mark) but low variance (consistent predictions). This is because they
    are not flexible enough to capture all the complexities in the data.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More complex models (right side)**: These tend to have low bias (better fitting
    the data) but high variance (predictions jumping around for similar data points).
    This is because they are more flexible and can fit the training data very well,
    but they also risk memorizing noise or irrelevant patterns in the data, which
    leads to poor performance on new, unseen data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the optimal model
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal is to find the optimal model complexity that balances bias and variance
    to achieve the lowest total error. This is often achieved through techniques such
    as regularization, which helps to constrain the model’s flexibility and reduce
    variance without introducing too much bias.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Relevance to business decisions
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For business-focused decision-makers, understanding the bias-variance trade-off
    is useful because it helps you do the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate the generalizability of your models**: How well will your model
    perform on real-world data that it hasn’t seen before?'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make informed choices about model complexity**: Balance the need for accurate
    predictions with the risk of overfitting and poor generalizability'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid common pitfalls**: Knowing the signs of underfitting (high bias) and
    overfitting (high variance) can help you diagnose and fix issues with your models'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding this trade-off, you can make better decisions about your data
    science projects and ensure that your models are generalizable and impactful for
    your business.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best ML models aren’t about perfectly mimicking the past. They’re about
    uncovering patterns that help you make accurate predictions for the future. By
    understanding and tackling overfitting and underfitting, you’ll equip your models
    to deliver the insights that drive better business decisions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'As we move forward, we’ll investigate another important aspect of ML: training-serving
    skew and model drift. These concepts will further equip you to deploy effective
    and reliable ML models in your business.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Training-serving skew and model drift
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As decision-makers, it’s important to understand the potential pitfalls in deploying
    ML models into production. Two of these challenges are training-serving skew and
    model drift. Let’s explore these concepts, understand their implications, and
    learn how to mitigate their effects.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Training-serving skew
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training-serving skew occurs when the data used to train a model differs from
    the data used in serving predictions. This can lead to a significant drop in model
    performance. Imagine you’re a retail giant, and you’ve trained a model to predict
    customer purchasing behavior based on historical data. If your model is trained
    on online sales data but used to predict in-store sales, the skew could lead to
    inaccurate predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating training-serving skew
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How can we address this? Here are some steps to take:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensure consistency**: Make sure that the data used for training and serving
    is consistent. This includes aspects such as data sources, feature extraction
    methods, and data distribution.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor performance**: Regularly monitor your model’s performance. If there’s
    a sudden drop in performance, it could be due to training-serving skew.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update model regularly**: Update your model with recent data to ensure it
    remains relevant and accurate.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model drift
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model drift refers to the change in model performance over time due to changes
    in the underlying data distribution. Consider a marketing firm that uses a model
    to predict consumer trends. If there’s a sudden shift in consumer behavior, the
    model’s predictions may become less accurate over time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating model drift
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Addressing model drift involves doing the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor model performance**: Keep a close eye on your model’s performance
    metrics. If there’s a gradual decline, it could be due to model drift.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain models**: Regularly retrain your models with fresh data to ensure
    they stay up-to-date with the latest trends.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use robust models**: Some models are more susceptible to drift than others.
    Using robust models that can handle changes in data distribution can help mitigate
    this issue.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key takeaways
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ve explored two common pitfalls in deploying ML models:
    training-serving skew and model drift. We’ve learned how to identify these issues
    and looked at steps to mitigate their effects. By ensuring consistency in training
    and serving data, monitoring model performance, and regularly updating our models,
    we can ensure they remain effective and relevant.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'As we move forward, we’ll explore another critical aspect of ML models: bias
    and fairness. This will help us understand how models can be biased against different
    sub-populations and how to ensure our models are fair.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Bias and fairness
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within ML, bias and fairness are not just ethical considerations but critical
    factors that can significantly impact the effectiveness of your ML models. We
    have already encountered bias in how it is related to underfitting and overfitting.
    We will now explore bias in the context of how the model fully and accurately
    represents all groups within the data – for example, different demographic groups
    within a dataset of customers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Understanding bias
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bias in ML refers to a model’s tendency to systematically make errors due to
    limitations in the training data or the model’s design. This could be due to various
    reasons, including the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Inadequate or unrepresentative training data**: If your dataset doesn’t fully
    capture the complexities and diversity of the real world, your model might make
    inaccurate assumptions'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inherent prejudices in the data collection process**: If there are historical
    biases embedded in the way data was gathered, your model may perpetuate those
    biases'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – bias in loan approval
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider a bank that uses an ML model to approve or reject loan applications.
    If the training data used to build this model includes fewer examples of successful
    loan repayments from a certain demographic group, the model might learn to reject
    applications from this group more often, regardless of the applicants’ individual
    creditworthiness. This is an example of bias.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Understanding fairness
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fairness is a broader concept that looks at the impact of a model’s decisions.
    A model is considered unfair if it systematically favors one group over another,
    even if the bias itself is unintentional.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Example – fairness in advertising
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine an online retailer that uses an ML model to decide which customers to
    target with a promotional offer. If the model systematically excludes certain
    demographic groups from receiving the offer, it could be considered unfair, leading
    to missed opportunities and potential customer alienation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating bias and ensuring fairness
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some key strategies to address these issues:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '**Representative data**: Ensure your training data is as representative of
    the real-world population as possible. This may involve collecting more data,
    using techniques such as oversampling for underrepresented groups, and carefully
    addressing inherent biases in existing datasets.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness-aware algorithms**: Explore using algorithms that are specifically
    designed to consider fairness during the training process.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and evaluation**: Use evaluation metrics such as disparate impact
    and equal opportunity difference to measure potential biases and disparities in
    your model’s predictions. Regularly monitor these metrics to identify areas where
    fairness might be compromised.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key takeaways
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By understanding bias and fairness, you can take steps to build ML models that
    are both accurate and equitable. This is important not only for ethical reasons
    but also for ensuring that your models make sound business decisions that benefit
    all stakeholders.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve explored some of the common pitfalls in training and
    deploying ML models, including inadequate training data, poor data quality, over-
    and underfitting, training-serving skew, and model drift. We’ve also explored
    the concepts of bias and fairness, their impact on business outcomes, and how
    to mitigate these issues.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, remember that data science is not just about building models,
    but also about ensuring that these models are reliable, fair, and beneficial to
    all stakeholders.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore the different types of data science projects
    you might encounter and how to approach each of them.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Leading Successful Data Science Projects and Teams'
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part explores the leadership aspects of data science, including project
    structure, team composition, management strategies, and the importance of continuous
    learning and staying current with emerging technologies. This part has the following
    chapters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19633_11.xhtml#_idTextAnchor265)*, The Structure of a Data
    Science Project*'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19633_12.xhtml#_idTextAnchor291)*, The Data Science Team*'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19633_13.xhtml#_idTextAnchor319)*, Managing the Data Science
    Team*'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B19633_14.xhtml#_idTextAnchor337)*, Continuing Your Journey
    as a Data Science Leader*'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
