- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common Pitfalls in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Picture this: a seasoned data science manager just launched a new recommendation
    engine to boost product sales. The model performed brilliantly in tests, but now,
    customer interest is lukewarm. The problem? The model had gotten too good at mirroring
    the training data – niche tastes of early adopters that didn’t reflect broader
    customer preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) promises incredible things, but it’s dangerously
    easy to stumble. According to a survey of over 500 developers working with ML
    systems (https://www.civo.com/newsroom/ai-project-failure), more than half (53%)
    of respondents have abandoned between 1% and 25% of ML projects, with an additional
    24% having left between 26% and 50% of projects. Only 11% of developers said they
    have never abandoned a project. The first lesson is this: ML isn’t some magic
    algorithm that just needs data. It’s about understanding what kind of model is
    right for the job, ensuring your data actually teaches the right lessons, and
    knowing when your model might be getting tripped up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dirty data, damaged models – how data quantity and quality impacts ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overcoming overfitting and underfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastering overfitting and underfitting for optimal model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training-serving skew and model drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, let’s acknowledge that ML is a complex field, and it’s not just about
    crunching numbers. It involves intricate algorithms, vast amounts of data, and
    the ability to interpret and apply the results in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a marketing executive at a consumer goods company. You have access
    to a wealth of customer data and want to use ML to predict which customers are
    most likely to buy your new product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sounds straightforward, right? But there are many places where complexity can
    come in. We will briefly explain some of the key considerations, then go into
    each one in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality and quantity**: Is your data clean and representative of your
    target population? Do you have enough high-quality data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection and tuning**: Have you selected the appropriate model for
    your data? Have you correctly trained or fine-tuned your model?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting and underfitting**: Is your model too complex and just memorizing
    the training data (overfitting)? Or is it too simple and missing important patterns
    (underfitting)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training-serving skew**: Will your model perform as well in the real world
    as it does on your training data?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model drift**: How will your model perform over time as the underlying data
    changes?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness**: Is your model biased against certain groups? Is it treating different
    sub-groups based on characteristics such as gender, age, and ethnicity fairly?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some of the key considerations to have in mind when training ML models,
    which can initially sound overwhelming. However, by looking at each of these in
    turn, with some concrete examples, by the end of this chapter, you should be better
    equipped to know what to look out for. You will also know the steps you, or your
    team, can take to mitigate the challenges associated with deploying ML models
    to production.
  prefs: []
  type: TYPE_NORMAL
- en: Dirty data, damaged models – how data quantity and quality impact ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training or using ML and artificial intelligence models, data is not only
    an asset but also the foundation of success. Without high-quality, representative
    data, even the most sophisticated ML model is useless. But what happens when you
    don’t have enough data, or when the data you have is biased or inaccurate?
  prefs: []
  type: TYPE_NORMAL
- en: To consider one hypothetical example, many banks use ML to flag potentially
    fraudulent transactions and block accounts based on information about the transaction.
    Imagine the model was only trained on a subset of account types, such as current
    accounts that have more regular, lower-value transactions. Let’s say the bank
    decides to then also apply the model to savings accounts that may have larger,
    less frequent transactions. The model may now incorrectly flag most typical savings
    account transactions as false positives, leading to frustrated customers and stressed
    customer service teams.
  prefs: []
  type: TYPE_NORMAL
- en: To look at another example, imagine a large language model-based customer service
    chatbot. Let’s say this chatbot was trained primarily on interactions where customers
    are expressing frustration or dissatisfaction. The chatbot learns to associate
    most customer inquiries with negativity. A consequence could be that the chatbot
    becomes overly apologetic or defensive, even in neutral conversations. It may
    misunderstand simple requests and misinterpret the customers’ intent, hindering
    effective customer support.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look into the common data considerations around quantity
    and quality that can affect your ML models and how to address them.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of adequate training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re a coach training a team for a basketball tournament. If you only
    train it on shooting free throws, it will struggle when faced with other aspects
    of the game such as defense or three-point shooting. Similarly, an ML model trained
    on insufficient or unrepresentative data will struggle to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In industries such as market research and consumer goods, for instance, if a
    model is trained only on data from urban consumers, it may not perform well when
    applied to rural consumers.
  prefs: []
  type: TYPE_NORMAL
- en: For many ML models, particularly deep learning models, the quantity of data
    is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the challenge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To mitigate this challenge, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect a sufficient volume of data**: This may seem like a brute-force approach,
    but in many cases, the best way to improve the accuracy of ML models, and particularly
    deep learning models, is to increase the volume of data the model is being trained
    on. One way this may be achieved is via collecting data over a longer period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collect diverse data**: Ensure your training data covers a wide range of
    scenarios your model is likely to encounter. This may be achieved by expanding
    the sources where data is acquired, either internal data sources (first-party
    data), or external data sources (second- and third-party data). It is important,
    however, to expand data coverage to include only relevant data that is representative
    of the data your model will see in production. For example, in the previous chatbot
    use case, expanding the data to cover all types of customer interactions could
    benefit the accuracy and reliability of the model. However, adding irrelevant
    chatbot data, say from a different company or industry, may have the opposite
    effect and lead to a less reliable model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use data augmentation techniques**: Data augmentation is the process of adjusting
    or augmenting data examples you already have. These techniques can artificially
    expand your dataset by creating variations of existing data points. For example,
    within image recognition, one common data augmentation approach is to adjust existing
    images by rotating, zooming, blurring, and cropping them, increasing the volume
    of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate synthetic data**: Synthetic data refers to artificially created
    data that closely mirrors the characteristics and patterns of real-world data.
    This can be particularly beneficial when real-world data is scarce, sensitive,
    or difficult to obtain. In the case of **large language models** (**LLMs**), these
    can be used to generate realistic synthetic data that can be used for fine-tuning
    models for specific tasks. LLMs excel at creating text-based data and can be fine-tuned
    to produce diverse and targeted variations, filling gaps in your original datasets
    and ensuring your model is better prepared for various real-world scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with poor data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Poor data quality, such as missing values, inconsistencies, and outright errors,
    significantly hinders the performance of ML models. Imagine trying to teach someone
    math with a textbook full of typos and incorrect formulas – they’ll struggle to
    learn the concepts correctly. Similarly, a model trained on flawed data will produce
    unreliable results.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, an image recognition model within the healthcare technology
    industry that is trained to detect tumors from MRI scans. If the images this model
    is trained on are poorly labeled, it could lead to potentially disastrous results,
    with tumors going undetected and false positives being flagged. For critical applications
    such as this, having very high data quality is one of the most important considerations,
    if not the most important.
  prefs: []
  type: TYPE_NORMAL
- en: Take another example. A natural language processing model may be fine-tuned
    for content moderation on a social media platform. If the training data is poorly
    labeled (e.g., sarcastic statements flagged as hate speech) or lacks diverse examples,
    the model will struggle. This could lead to false positives, where legitimate
    content might be wrongly removed, restricting freedom of expression. Additionally,
    the poor quality training data could lead to the model producing false negatives,
    where actual hate speech might slip through, making the platform unsafe for users.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating the challenge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various methods to mitigate the challenge of poor data quality, which
    we will describe in this section. However, often, the best place data quality
    can be addressed is at the source.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an ML model trained to classify customers in a CRM as
    likely or unlikely to churn. Have the customers, and all the information about
    them, been accurately entered into the CRM? Is there any validation on the forms
    to make sure invalid data has not been entered, or is important data missing for
    some customers? Are there processes for the business teams to follow when entering
    data? If there is poor quality or missing data, can this be manually fixed by
    business teams, or the data science teams themselves?
  prefs: []
  type: TYPE_NORMAL
- en: This is all mundane stuff, but if, by the time data is in the hands of data
    scientists and ML engineers, it is already of poor quality, there is only so much
    that can be done with the more automated processes we will explain here. As the
    well-known expression goes, garbage in, garbage out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some techniques that data scientists, engineers, and analysts can
    use to mitigate poor data quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning**: There are several techniques that data scientists can apply
    to clean data before training ML models, including the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing values**: Decide whether to remove entries with missing data or replace
    missing values with estimates (e.g., mean or median)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duplicates**: Remove redundant entries that can skew results'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistencies**: Correct formatting errors (e.g., date formats), and standardize
    entries for better model understanding (e.g., convert all addresses to lowercase)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: Data scientists can apply techniques to validate data
    and exclude or fix invalid data before training ML models. It is also important
    that when the model is in production (i.e., after training and during inference),
    the same processes for data cleaning and validation are applied:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range checks**: Ensure values fall within an acceptable range (e.g., a person’s
    age can’t be negative)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Format checks**: Verify that data adheres to specific formats (e.g., phone
    numbers, zip codes)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-field checks**: Ensure consistency across related data fields (e.g.,
    if a country is “USA,” the state field should match the list of US states)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bad data will sabotage your ML models, plain and simple. Addressing these issues
    is essential to success, and we have covered some of the techniques that you can
    leverage in your next data science project. These techniques include improving
    data collection by expanding the scope and coverage of the training data, augmenting
    data, and synthesizing data if appropriate, as well as improving the quality of
    data through data cleaning and validation. These are the hard yards that will
    set up your project for success. The importance of data cannot be emphasized enough,
    to the extent that there is a growing approach called Data-centric AI ([https://datacentricai.org/](https://datacentricai.org/)),
    which is the discipline of systematically engineering the data used to build an
    AI system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will explore another key challenge: ensuring your model doesn’t just
    memorize your training data but can also learn to generalize to new situations.
    That means understanding and avoiding overfitting and underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: As we move to the next section, we’ll explore this critical aspect of ML – over-
    and underfitting. How can we ensure our model performs well, not just on our current
    data, but also on new, unseen data?
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the right complexity for your model is a delicate balancing act. If
    your model is too complex, it might overfit the training data, meaning it performs
    well on the training data but poorly on new, unseen data. On the other hand, if
    your model is too simple, it might underfit the data, missing important patterns
    and leading to inaccurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re a market researcher trying to predict consumer trends. An overfitted
    model might capture every minor fluctuation in past trends but fail to generalize
    to future trends. An underfitted model might miss important trends altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating training-serving skew and model drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an ideal world, your model would perform just as well in the real world as
    it does on your training data. But this is rarely the case. This discrepancy is
    known as **training-serving skew**.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as the underlying data changes over time, your model’s performance
    can degrade. This is known as **model drift**.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you’re developing an ML model to predict customer churn for a telecommunications
    company. During the model training phase, you use a dataset that includes customer
    information such as demographics, usage patterns, and customer service interactions.
    However, when the model is deployed in the production environment (the serving
    phase), you realize that the data pipeline feeding the model is missing some important
    features, such as the latest customer service interaction data. This discrepancy
    between the data used for training and the data available during serving is a
    classic example of training-serving skew.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the model’s performance in production may suffer because it
    was trained on a more comprehensive dataset than what is available in the serving
    environment. The missing features during serving can lead to inaccurate predictions
    and suboptimal decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: To address training-serving skew, it’s important to ensure consistency between
    the data used for training and the data available during serving. This can involve
    regularizing data pipelines, monitoring data quality, and implementing data validation
    checks to catch any discrepancies early in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, it’s important to ensure that your models are fair and don’t discriminate
    against certain groups. This can be a challenge, especially when the training
    data itself is biased.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you’re an HR manager using ML to screen job applicants.
    If your training data is biased against certain groups, your model might unfairly
    reject qualified candidates from these groups.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve explored why ML can be hard, touching on challenges such
    as data quality and quantity, overfitting and underfitting, training-serving skew,
    model drift, and fairness. But don’t be discouraged. In the following sections,
    we’ll dive deeper into these challenges and provide practical strategies to overcome
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering overfitting and underfitting for optimal model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ML, achieving reliable predictions is often the main goal. Overfitting and
    underfitting are two common obstacles to this goal. Let’s break down these concepts
    and outline concrete techniques to build better models.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting – when your model is too specific
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine your model as a student preparing for a test. Overfitting occurs when
    the student memorizes the practice questions perfectly but struggles to answer
    variations of the same questions on the actual exam. Similarly, an overfitted
    model gets too focused on the details of the training data, including random noise,
    and fails to grasp the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world consequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Market research**: A model obsessively tuned to existing customers’ data
    won’t be able to predict the behavior of new prospects with different characteristics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retail recommendations**: A system trained exclusively on a loyal customer’s
    purchase history may offer irrelevant suggestions when trying to attract new shoppers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting – when your model is too simplistic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Picture underfitting as a student who only grasps the most basic concepts of
    a subject. They’ll fail the exam regardless of whether the questions are from
    practice problems or new material. Similarly, an underfitted model misses important
    relationships within the data and performs poorly across the board.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world consequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Sales forecasts**: A model that ignores factors such as seasonality or marketing
    promotions will consistently underestimate or overestimate potential sales'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotting the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Red flag**: Excellent performance on training data but terrible results on
    new data is a classic sign of overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warning sign**: If your model struggles with both training data and new data,
    it’s likely due to underfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions for building better models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some solutions for building good models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**More data = stronger foundation**: Larger, more diverse datasets help the
    model identify real trends, not just random fluctuations in the training sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection = laser focus**: Carefully choose the most relevant data
    features. Get rid of those that only add confusion, not insight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example – predicting customer churn**: Imagine you’re predicting customer
    churn for a telecom company. Your dataset includes relevant features such as monthly
    charges and customer service calls, but also an irrelevant feature: favorite ice
    cream flavor. Including “favorite ice cream flavor” adds noise and makes it harder
    for the algorithm to identify important patterns. By selecting only relevant features,
    you create a focused model that zeroes in on key factors driving churn. Remember,
    more data isn’t always better. Quality and relevance matter most.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization = the guardrails**: Regularization is a technique that adds
    penalties to the model during training to prevent it from becoming too complex
    and too reliant on the peculiarities of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example – predicting house prices**: When building a model to predict house
    prices, regularization acts as a safeguard. It discourages the model from giving
    too much importance to a few unusual, expensive houses with unique features in
    the training data. By adding these penalties, regularization helps the model generalize
    better to new, unseen data, rather than getting stuck on the specifics of the
    training data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation = reality check**: Cross-validation is a method that helps
    assess how well a model will perform on new, unseen data by simulating real-world
    conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example – sentiment analysis of movie reviews**: When building a model to
    predict the sentiment of movie reviews, cross-validation provides a reality check.
    Instead of training the model on all the data and assuming it’s performing well,
    you split the data into subsets. You train the model on some subsets and test
    it on others. By doing this multiple times, you get a more realistic estimate
    of how the model will perform on new data. This helps you catch whether the model
    is just memorizing the training data instead of learning to generalize to new
    reviews.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to visualize the trade-off between underfitting and overfitting is
    by looking at a bias-variance trade-off chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Bias-variance trade-off](img/B19633_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Bias-variance trade-off'
  prefs: []
  type: TYPE_NORMAL
- en: The chart (*Figure 10**.1*) visualizes the relationship between a model’s **complexity**,
    **generalizability**, and **accuracy** on unseen data. This is a very important
    concept to business-focused decision-makers in data science, ML, and AI because
    it directly impacts the real-world performance of their models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the axes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**X-axis (model complexity)**: This represents how flexible or complex your
    model is. Simpler models are on the left, while more complex models are on the
    right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Y-axis (error)**: This represents the overall error of your model, which
    is a combination of two key factors: bias and variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key parts of the chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore the chart with a real example. Imagine you’re building a model
    to predict which customers are likely to stop using your product or service (churn):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias**: This refers to the systematic error introduced by the model itself.
    It’s the consistent difference between the model’s predictions and the actual
    values. A high bias means the model consistently misses the mark, regardless of
    the specific data point. A very simple model might only look at one feature, such
    as a customer’s average purchase amount. This model is likely to have high bias
    because it ignores a whole host of complex factors that contribute to churn (support
    experience, competitor offerings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Variance (Overfitting)**: A very complex model with a huge number of
    features might fit the training data perfectly. However, it might pick up on irrelevant
    patterns or random fluctuations in your historical data, leading to inconsistent
    predictions for new customers (like darts going all over the board). This model
    would perform well on the data it was trained on but fail to generalize and predict
    new churn reliably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal Balance**: An ideal model would be complex enough to capture the
    key factors driving churn without overfitting to the specifics of your training
    data. This balance would lead to the lowest overall error rate (**Total Error**
    on the chart), successfully identifying those customers genuinely at risk of churn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trade-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The key takeaway from this chart is the trade-off between bias and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simpler models (left side)**: These tend to have high bias (systematically
    missing the mark) but low variance (consistent predictions). This is because they
    are not flexible enough to capture all the complexities in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More complex models (right side)**: These tend to have low bias (better fitting
    the data) but high variance (predictions jumping around for similar data points).
    This is because they are more flexible and can fit the training data very well,
    but they also risk memorizing noise or irrelevant patterns in the data, which
    leads to poor performance on new, unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the optimal model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal is to find the optimal model complexity that balances bias and variance
    to achieve the lowest total error. This is often achieved through techniques such
    as regularization, which helps to constrain the model’s flexibility and reduce
    variance without introducing too much bias.
  prefs: []
  type: TYPE_NORMAL
- en: Relevance to business decisions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For business-focused decision-makers, understanding the bias-variance trade-off
    is useful because it helps you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluate the generalizability of your models**: How well will your model
    perform on real-world data that it hasn’t seen before?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make informed choices about model complexity**: Balance the need for accurate
    predictions with the risk of overfitting and poor generalizability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid common pitfalls**: Knowing the signs of underfitting (high bias) and
    overfitting (high variance) can help you diagnose and fix issues with your models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding this trade-off, you can make better decisions about your data
    science projects and ensure that your models are generalizable and impactful for
    your business.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best ML models aren’t about perfectly mimicking the past. They’re about
    uncovering patterns that help you make accurate predictions for the future. By
    understanding and tackling overfitting and underfitting, you’ll equip your models
    to deliver the insights that drive better business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we move forward, we’ll investigate another important aspect of ML: training-serving
    skew and model drift. These concepts will further equip you to deploy effective
    and reliable ML models in your business.'
  prefs: []
  type: TYPE_NORMAL
- en: Training-serving skew and model drift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As decision-makers, it’s important to understand the potential pitfalls in deploying
    ML models into production. Two of these challenges are training-serving skew and
    model drift. Let’s explore these concepts, understand their implications, and
    learn how to mitigate their effects.
  prefs: []
  type: TYPE_NORMAL
- en: Training-serving skew
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training-serving skew occurs when the data used to train a model differs from
    the data used in serving predictions. This can lead to a significant drop in model
    performance. Imagine you’re a retail giant, and you’ve trained a model to predict
    customer purchasing behavior based on historical data. If your model is trained
    on online sales data but used to predict in-store sales, the skew could lead to
    inaccurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating training-serving skew
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How can we address this? Here are some steps to take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensure consistency**: Make sure that the data used for training and serving
    is consistent. This includes aspects such as data sources, feature extraction
    methods, and data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor performance**: Regularly monitor your model’s performance. If there’s
    a sudden drop in performance, it could be due to training-serving skew.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update model regularly**: Update your model with recent data to ensure it
    remains relevant and accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model drift refers to the change in model performance over time due to changes
    in the underlying data distribution. Consider a marketing firm that uses a model
    to predict consumer trends. If there’s a sudden shift in consumer behavior, the
    model’s predictions may become less accurate over time.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating model drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Addressing model drift involves doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitor model performance**: Keep a close eye on your model’s performance
    metrics. If there’s a gradual decline, it could be due to model drift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrain models**: Regularly retrain your models with fresh data to ensure
    they stay up-to-date with the latest trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use robust models**: Some models are more susceptible to drift than others.
    Using robust models that can handle changes in data distribution can help mitigate
    this issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ve explored two common pitfalls in deploying ML models:
    training-serving skew and model drift. We’ve learned how to identify these issues
    and looked at steps to mitigate their effects. By ensuring consistency in training
    and serving data, monitoring model performance, and regularly updating our models,
    we can ensure they remain effective and relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we move forward, we’ll explore another critical aspect of ML models: bias
    and fairness. This will help us understand how models can be biased against different
    sub-populations and how to ensure our models are fair.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias and fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within ML, bias and fairness are not just ethical considerations but critical
    factors that can significantly impact the effectiveness of your ML models. We
    have already encountered bias in how it is related to underfitting and overfitting.
    We will now explore bias in the context of how the model fully and accurately
    represents all groups within the data – for example, different demographic groups
    within a dataset of customers.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bias in ML refers to a model’s tendency to systematically make errors due to
    limitations in the training data or the model’s design. This could be due to various
    reasons, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inadequate or unrepresentative training data**: If your dataset doesn’t fully
    capture the complexities and diversity of the real world, your model might make
    inaccurate assumptions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inherent prejudices in the data collection process**: If there are historical
    biases embedded in the way data was gathered, your model may perpetuate those
    biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – bias in loan approval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consider a bank that uses an ML model to approve or reject loan applications.
    If the training data used to build this model includes fewer examples of successful
    loan repayments from a certain demographic group, the model might learn to reject
    applications from this group more often, regardless of the applicants’ individual
    creditworthiness. This is an example of bias.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fairness is a broader concept that looks at the impact of a model’s decisions.
    A model is considered unfair if it systematically favors one group over another,
    even if the bias itself is unintentional.
  prefs: []
  type: TYPE_NORMAL
- en: Example – fairness in advertising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine an online retailer that uses an ML model to decide which customers to
    target with a promotional offer. If the model systematically excludes certain
    demographic groups from receiving the offer, it could be considered unfair, leading
    to missed opportunities and potential customer alienation.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating bias and ensuring fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some key strategies to address these issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representative data**: Ensure your training data is as representative of
    the real-world population as possible. This may involve collecting more data,
    using techniques such as oversampling for underrepresented groups, and carefully
    addressing inherent biases in existing datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fairness-aware algorithms**: Explore using algorithms that are specifically
    designed to consider fairness during the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and evaluation**: Use evaluation metrics such as disparate impact
    and equal opportunity difference to measure potential biases and disparities in
    your model’s predictions. Regularly monitor these metrics to identify areas where
    fairness might be compromised.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By understanding bias and fairness, you can take steps to build ML models that
    are both accurate and equitable. This is important not only for ethical reasons
    but also for ensuring that your models make sound business decisions that benefit
    all stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve explored some of the common pitfalls in training and
    deploying ML models, including inadequate training data, poor data quality, over-
    and underfitting, training-serving skew, and model drift. We’ve also explored
    the concepts of bias and fairness, their impact on business outcomes, and how
    to mitigate these issues.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, remember that data science is not just about building models,
    but also about ensuring that these models are reliable, fair, and beneficial to
    all stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore the different types of data science projects
    you might encounter and how to approach each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Leading Successful Data Science Projects and Teams'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part explores the leadership aspects of data science, including project
    structure, team composition, management strategies, and the importance of continuous
    learning and staying current with emerging technologies. This part has the following
    chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19633_11.xhtml#_idTextAnchor265)*, The Structure of a Data
    Science Project*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19633_12.xhtml#_idTextAnchor291)*, The Data Science Team*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19633_13.xhtml#_idTextAnchor319)*, Managing the Data Science
    Team*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B19633_14.xhtml#_idTextAnchor337)*, Continuing Your Journey
    as a Data Science Leader*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
