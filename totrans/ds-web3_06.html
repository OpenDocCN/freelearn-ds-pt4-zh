<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer099">
<h1 class="chapter-num er" id="_idParaDest-120"><a id="_idTextAnchor210"/>6</h1>
<h1 id="_idParaDest-121"><a id="_idTextAnchor211"/>Preparing and Exploring Our Data</h1>
<p>Data preparation is a common theme in data science, extending beyond its association with the machine learning pipeline. It takes on various monikers such as data wrangling, data cleaning, and data preprocessing for <span class="No-Break">feature engineering.</span></p>
<p>Here, we emphasize that significant time will be invested in data cleaning, feature engineering, and exploratory analysis, and we recognize the positive impact of robust preprocessing on outcomes, whether for a presentation for business stakeholders or its integration to a machine <span class="No-Break">learning model.</span></p>
<p><strong class="old">Data cleaning</strong> encompasses<a id="_idIndexMarker349"/> tasks focused on identifying and rectifying data issues, particularly errors and artifacts. Errors result from data loss in the acquisition pipeline, while artifacts arise from the system that generates the data. Cleaning involves addressing missing data, handling outliers, removing duplicates, and performing necessary translations for data readability <span class="No-Break">and conversion.</span></p>
<p><strong class="old">Data preparation</strong> spans<a id="_idIndexMarker350"/> tasks such as understanding and transforming received data to align with subsequent pipeline steps. This chapter delves into common scenarios that arise when working to understand, preprocess, and extract information from on-chain data. Specific topics include decimal treatment, approaches to smart contract evolution, and checksum validation. Additionally, the chapter introduces the <strong class="old">Exploratory Data Analysis</strong> (<strong class="old">EDA</strong>) concept <a id="_idIndexMarker351"/>and employs techniques for summary statistics and outlier detection to illustrate its advantages <span class="No-Break">and insights.</span></p>
<p>This chapter explores the intricacies of preparing on-chain data and introduces the concept of EDA, facilitating the transition from analytics to machine learning. It does not aim to provide an exhaustive overview of all tools and methods, due to the extensive nature of this field of <span class="No-Break">data science.</span></p>
<p>In summary, this chapter will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>On-chain <span class="No-Break">data preparation</span></li>
<li>Introduction to Exploratory <span class="No-Break">Data Analysis</span></li>
</ul>
<h1 id="_idParaDest-122"><a id="_idTextAnchor212"/>Technical requirements</h1>
<p>We extensively use the Pandas library, a popular and useful Python library for working with DataFrames and series. Pandas offers numerous functions to analyze, summarize, explore, normalize, and manipulate them. Series are one-dimensional array-like objects, and DataFrames are two-dimensional table structures with rows and columns. We use Pandas throughout this book’s exercises to perform the <span class="No-Break">aforementioned activities.</span></p>
<p>If you haven’t installed Pandas yet, you can do so with the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
pip install pandas.</pre> <p>The documentation for Pandas is available <span class="No-Break">at </span><a href="https://pandas.pydata.org/docs/"><span class="No-Break">https://pandas.pydata.org/docs/</span></a><span class="No-Break">.</span></p>
<p>For data visualization, we use the Matplotlib and Seaborn libraries. Matplotlib provides a wide range of tools and control over the images we build. Seaborn is built on top of Matplotlib and is more user-friendly but has <span class="No-Break">less flexibility.</span></p>
<p>The documentation for both libraries can be found at <a href="https://seaborn.pydata.org/">https://seaborn.pydata.org/</a> and <a href="https://matplotlib.org/"><span class="No-Break">https://matplotlib.org/</span></a><span class="No-Break">, respectively.</span></p>
<p>You can find all the data and code files for this chapter in the book’s GitHub repository at <a href="https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter06">https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter06</a>. We recommend that you read through the code files in the <strong class="source-inline">Chapter06</strong> folder to <span class="No-Break">follow along.</span></p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor213"/>Data preparation</h1>
<p>When dealing <a id="_idIndexMarker352"/>with information collected from diverse data sources, it is crucial to ensure consistency and uniformity across all records and fields before extracting insights or feeding the data into a machine learning model. In this section, we will explore various data preparation tasks that are particularly relevant to <span class="No-Break">on-chain da<a id="_idTextAnchor214"/>ta.</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor215"/>Hex values</h2>
<p>Hexadecimal notation<a id="_idIndexMarker353"/> is a base 16 system, utilizing symbols to represent numerical values from 0 to 9 and letters from A to F. In contrast, our everyday decimal notation employs 10 symbols to represent numerical values (0–9). Hexadecimal notation extends the range by including A to F, representing values from 10 to 15. This notation is often used for data storage purposes due to its efficiency in representing binary numbers with each hex digit representing <span class="No-Break">4 bits.</span></p>
<p>In the example presented in <strong class="source-inline">Chapter06/Preparation</strong>, we retrieve the latest block number from the Rootstock public node by following the documentation available <span class="No-Break">at </span><a href="https://developers.rsk.co/rsk/public-nodes/"><span class="No-Break">https://developers.rsk.co/rsk/public-nodes/</span></a><span class="No-Break">.</span></p>
<p>The resulting value is presented in the form of a hex number, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">0x4e07d0</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 6.1 – Hexadecimal block number" height="200" src="image/B19446_06_01.jpg" width="1173"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Hexadecimal block number</p>
<p>This hex number can be decoded into a decimal number providing the base (<strong class="source-inline">16</strong>) using the following <span class="No-Break">code snippet:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 6.2 – Hexadecimal block number decoded" height="207" src="image/B19446_06_02.jpg" width="1280"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Hexadecimal block number decoded</p>
<p>Following those steps, we are able to translate the hex response from the RSK node into our decimal system. To verify the accuracy of the translated information, we can compare our findings with the chain explorer available at <a href="https://explorer.rsk.co/blocks">https://explorer.rsk.co/blocks</a> or <a href="https://rootstock.blockscout.com/">https://rootstock.blockscout.com/</a>. We will see that the block was added to the chain just a <span class="No-Break">moment ago:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 6.3 – Block explorer" height="608" src="image/B19446_06_03.jpg" width="1151"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Block explorer</p>
<p>Certain <a id="_idIndexMarker354"/>SQL database engines have the capability to convert hex values into a human-readable format directly within the query. For example, the ClickHouse system used by Covalent provides the <strong class="source-inline">unhex</strong> method. You can find more details in the documentation <span class="No-Break">at </span><a href="https://clickhouse.com/docs/en/sql-reference/functions/encoding-functions#unhex"><span class="No-Break">https://clickhouse.com/docs/en/sql-reference/functions/encoding-functions#unhex</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor216"/>Checksum</h2>
<p>Checksum <a id="_idIndexMarker355"/>is an algorithm that hashes the address, enabling Ethereum to verify whether it is a valid address. In Ethereum, a checksummed address contains both uppercase and lowercase letters in a <span class="No-Break">specific pattern.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-6">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="old">Checksum address</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="old">Non-checksum address</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline">0x95222290DD7278Aa3Ddd389Cc1E1d 165CC4BAfe5</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline">0x95222290dd7278aa3ddd389cc1e1 d165cc4bafe5</strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 – Difference between addresses</p>
<p>Ethereum treats both lowercase and checksummed addresses as valid, and funds sent to either version will be directed to the same recipient. However, using a checksummed address provides an additional layer of security by preventing the accidental sending of funds to <span class="No-Break">non-existent addresses.</span></p>
<p>This<a id="_idIndexMarker356"/> section holds significance on two fronts. Firstly, Python, like many SQL engines, is case-sensitive. So, it becomes imperative to manage the differentiation between lowercase and checksummed addresses when comparing or merging data from diverse sources. This guarantees compatibility and precision in data analysis. The second dimension pertains to the differentiation between valid and invalid addresses, a crucial aspect in maintaining data integrity and making our queries faster <span class="No-Break">to run.</span></p>
<p>In <strong class="source-inline">Chapter06/Preparation</strong>, we test whether an address is a valid checksummed Ethereum address. For this purpose, we<a id="_idIndexMarker357"/> utilize the <strong class="old">eth-utils</strong> library, which we explain more comprehensively in <a href="B19446_10.xhtml#_idTextAnchor294"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. The <strong class="source-inline">test()</strong> function allows us to convert a lowercase address into its <span class="No-Break">checksummed version.</span></p>
<p>For another example, please refer to <strong class="source-inline">Chapter10/EDA</strong>, where we demonstrate the application of checksum addresses within a filter to remove invalid <span class="No-Break">Ethereum addresses.</span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor217"/>Decimal treatment</h2>
<p>Solidity<a id="_idIndexMarker358"/> is the most commonly used smart contract programming language for EVM-based blockchains, but it does not support floats. To express decimals in Solidity, we use integers with the <strong class="old">fixed-point</strong> methodology. The<a id="_idIndexMarker359"/> fixed-point methodology can be understood as the division of a variable numerator and a fixed denominator. If we review <strong class="source-inline">Chapter04/Art</strong> in Jupyter Notebook, in the <em class="italic">Chainlink</em> section, we can see the response from the oracle expressed in the <span class="No-Break">following way:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 6.4 – Chainlink floor price response" height="427" src="image/B19446_06_04.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Chainlink floor price response</p>
<p>Such a large number is not meaningful in an economic context and cannot be directly included in a dashboard or report. Therefore, it is necessary to translate it into our decimal system to make it <span class="No-Break">more useful.</span></p>
<p>Smart contracts<a id="_idIndexMarker360"/> provide the number of decimals through a specific function. In the case of Chainlink’s data-feed smart contract, the <strong class="source-inline">decimals()</strong> function informs us about the fixed point, or, in practical terms, how many decimal places we have to shift the comma to the left to convert the response into our decimal system. The steps to query the smart contract are explained in the <a href="B19446_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> section <em class="italic">Exploring state data</em>, and as shown in the Jupyter Notebook, the result <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">18</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 6.5 – The data-feed decimal" height="441" src="image/B19446_06_05.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – The data-feed decimal</p>
<p>The following figure showcases the transformed number that we can use in subsequent parts of a <span class="No-Break">data pipeline:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 6.6 – The result" height="336" src="image/B19446_06_06.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – The result</p>
<p>The same result can be achieved by applying the <strong class="source-inline">fromWei()</strong> function as per the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
Web3.fromWei(latestData), 'ether')</pre> <p>The decimal treatment we just explored is also relevant for tokens. <strong class="old">Ethereum Request for Comment 20</strong> (<strong class="old">ERC-20</strong>) is a <a id="_idIndexMarker361"/>technical standard for fungible tokens created on Ethereum. This standard defines a set of functions and events that developers must implement to ensure compliance with it. One of the optional functions in this standard is <strong class="source-inline">decimal()</strong>, which <em class="italic">returns the number of decimal places the token uses, for example 8, which means to divide the token amount by 100000000 (10 to the power 8) to get its </em><span class="No-Break"><em class="italic">user representation</em></span><span class="No-Break">.</span></p>
<p>More of this standard was analyzed in <a href="B19446_05.xhtml#_idTextAnchor168"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
<p class="callout-heading">Note on decimals</p>
<p class="callout">The most common decimal denominator for Ethereum smart contracts is 18, while Bitcoin uses 8 and USDT utilizes <span class="No-Break">6 decimals.</span></p>
<p>In this section, we<a id="_idIndexMarker362"/> have learned that consuming on-chain data often requires extensive transformations. If our dataset contains excessively large strings that lack economic meaning, we may need to search for the smart contract’s decimal value to properly position the decimal point. Additionally, if our dataset includes hex values, we need to decode them into our decimal system. Lastly, we have discovered how to transform lowercase addresses into checksum addresses to ensure compatibility with case-sensitive <span class="No-Break">programming language<a id="_idTextAnchor218"/>s.</span></p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor219"/>From Unix timestamps to datetime formats</h2>
<p>Unix timestamps <a id="_idIndexMarker363"/>are commonly used in data analysis, but for visualization purposes in dashboards and reports, it is necessary to convert them into a human-readable format. Unix time represents the number of seconds that have passed since January 1, 1970, providing a system to track time using a single <span class="No-Break">integer value.</span></p>
<p>In most SQL engines, the <strong class="source-inline">truncate</strong> function can be utilized to extract the relevant date part from <span class="No-Break">the timestamp.</span></p>
<p>In Python, we can use the <strong class="source-inline">datetime</strong> module’s <strong class="source-inline">fromtimestamp()</strong> function, which converts Unix timestamps to local datetime, and the <strong class="source-inline">utcfromtimestamp()</strong> function, which converts them to <span class="No-Break">UTC datetimes.</span></p>
<p>In the Jupyter Notebook’s <strong class="source-inline">Chapter06/Preparation</strong> section, we translate a Unix timestamp with the <span class="No-Break">following code:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 6.7 – Datetime translation" height="193" src="image/B19446_06_07.jpg" width="1041"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Datetime translation</p>
<p>To <a id="_idIndexMarker364"/>validate our results, we can compare them with those obtained from <a href="https://www.unixtimestamp.com/">https://www.unixtimestamp.com/</a>, a popular tool that shows the <span class="No-Break">same information:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 6.8 – Unix timestamp translator" height="335" src="image/B19446_06_08.jpg" width="906"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Unix timestamp translator</p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor220"/>Evolution of smart contracts</h2>
<p>Smart contracts, like <a id="_idIndexMarker365"/>any software product, may undergo changes and require upgrades for various reasons such as business necessity, security incidents, or to reduce gas costs. However, by design, everything deployed on the blockchain is immutable. The following information, sourced from <a href="http://Ethereum.org">Ethereum.org</a>, outlines multiple approaches to enable an upgrade. The content is quoted under the <strong class="old">Creative Commons Attribution 4.0 International</strong> (<strong class="old">CC BY 4.0</strong>) license, in<a id="_idIndexMarker366"/> compliance with the terms of use. The original document can be found in the <em class="italic">Further reading</em> section of <span class="No-Break">this chapter.</span></p>
<p>Smart contract upgrades can be achieved via the <span class="No-Break">following methods:</span></p>
<ul>
<li>Creating multiple versions of a smart contract and migrating state (i.e., data) from the old contract to a new instance of <span class="No-Break">the contract</span></li>
<li>Creating separate contracts to store business logic <span class="No-Break">and state</span></li>
<li>Using proxy patterns to delegate function calls from an immutable proxy contract to a modifiable <span class="No-Break">logic contract</span></li>
<li>Creating an immutable main contract that interfaces with and relies on flexible satellite contracts to execute <span class="No-Break">specific functions</span></li>
<li>Using the diamond pattern to delegate function calls from a proxy contract to <span class="No-Break">logic contracts</span></li>
</ul>
<p>In conclusion, the <a id="_idIndexMarker367"/>ways to upgrade a smart contract do not involve modifying the deployed code. Rather, it entails substituting one contract for another. Currently, the most popular method for upgrading smart contracts is <a id="_idIndexMarker368"/>the <strong class="old">proxy pattern</strong>. This pattern involves a separation between the proxy contract and the execution contract that holds the logic. The proxy acts on behalf of the logic smart contract redirecting transactions from the frontend to the correct smart contract in the backend. It is possible to swap the logic smart contract in the backend and update the proxy to start redirecting transactions to the newly deployed smart contract with the <span class="No-Break">latest logic.</span></p>
<p>The fact that contracts may change implies that our queries need to adapt to reflect those changes. For example, if a smart contract starts emitting an event at a certain block, we need to be aware of it to capture the new information. As we saw in <a href="B19446_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, we need <a id="_idIndexMarker369"/>the <strong class="old">Application Binary Interface</strong> (<strong class="old">ABI</strong>) to decode smart contracts that need to be duly updated to decode transactions after an upgrade. Not being aware that the contract we are parsing has changed may cause us to miss certain events and may negatively impact <span class="No-Break">our analysis.</span></p>
<p>When analyzing a specific project, it’s important to follow press releases, project representatives, and official information channels for news of any development launched to direct our queries to the smart contract that is less prone <span class="No-Break">to changes.</span></p>
<p>In conclusion, while smart contracts are designed to be immutable, there are circumstances where upgrades and changes become necessary, and we need to be prepared to adapt our <a id="_idIndexMarker370"/>queries or code to these changes. For example, if we were analyzing Ethereum, we would need to be aware that the entire chain changed with the Merge, which occurred in September 2022 and has an impact at the <span class="No-Break">data level.</span></p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor221"/>Exploratory Data Analysis</h1>
<p>Between the<a id="_idIndexMarker371"/> data cleaning phase and the modeling or formal statistical analysis, there exists an intermediate step known as EDA, which is a fundamental aspect of data science. EDA serves as the primary approach to understanding and making sense of a dataset, providing insights into the “population out of the sample” and transforming raw data into actionable information for businesses. EDA can<a id="_idIndexMarker372"/> include various techniques <span class="No-Break">and methods:</span></p>
<ul>
<li><strong class="old">Data summary or descriptive statistics</strong>: Used to summarize central tendencies within <span class="No-Break">the dataset.</span></li>
<li><strong class="old">Data visualization</strong>: Graphical techniques such as histograms, box plots, scatter plots, and line plots are employed to visualize the data, aiding in pattern identification, outlier detection, and understanding the relationship between variables. Furthermore, data visualization is particularly effective when presenting conclusions to a <span class="No-Break">non-technical audience.</span></li>
<li><strong class="old">Data exploration</strong>: Helps us understand the distribution of variables, assess their shapes and skewness, and identify the presence <span class="No-Break">of anomalies.</span></li>
<li><strong class="old">Handling missing data</strong>: This allows us to identify missing rows and assess their impact on the results. It helps to determine patterns in missing values and to develop strategies for handling <span class="No-Break">them effectively.</span></li>
<li><strong class="old">Outlier detection</strong>: Identifying values that significantly deviate from the rest of the dataset and evaluating their influence on the analysis. These outliers can result from various causes, which we will discuss in the <span class="No-Break">subsequent section.</span></li>
<li><strong class="old">Correlations and patterns</strong>: Explores the relationships between variables using techniques such as correlation analysis and scatter plots. Additionally, they help identify<a id="_idIndexMarker373"/> trends or seasonality in the data <span class="No-Break">over time.</span></li>
</ul>
<p>A concise description of EDA can be found at <a href="https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15">https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15</a>: "<em class="italic">Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and </em><span class="No-Break"><em class="italic">graphical representations."</em></span></p>
<p>In this chapter, we will provide a brief introduction to summary statistics and outlier detection with the aid of graphical representations. We chose those two topics as they are probably transversals to all the datasets we will encounter in our journey. For further exploration of EDA topics, feel free to refer to the books in the <em class="italic">Further reading</em> section that are <span class="No-Break">very useful.</span></p>
<p>To exemplify the concepts learned in this section, we will <a id="_idIndexMarker374"/>use the <strong class="old">Witches</strong> dataset available on Kaggle (<a href="https://www.kaggle.com/datasets/harrywang/crypto-coven?select=witches.csv">https://www.kaggle.com/datasets/harrywang/crypto-coven?select=witches.csv</a>). It contains information about the Crypto Coven NFT project, where each row represents one witch NFT. The Witches project’s main page can be found <span class="No-Break">at </span><a href="https://www.cryptocoven.xyz/"><span class="No-Break">https://www.cryptocoven.xyz/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor222"/>Summarizing data</h2>
<p>Our datasets <a id="_idIndexMarker375"/>will contain categorical or quantitative variables. Categorical variables are those that can be divided into groups, such as colors or brands. On the other hand, quantitative variables represent numerical amounts, such as prices or the number of sales. The <strong class="source-inline">df.describe()</strong> code snippet returns the quantitative variables and <span class="No-Break">their distribution.</span></p>
<p>Calculating the distribution of categorical data involves determining the frequency of each category. This analysis can provide meaningful insights. For example, in marketing, understanding the distribution of categorical variables such as age groups, income levels, or consumer preferences can help businesses segment their target audience effectively and create more targeted and successful campaigns. Another example is in fraud detection, where categorical variables such as transaction types or user behavior patterns can be crucial in detecting fraudulent activities. By studying the distribution of these variables, anomalies or unusual patterns can be identified, enabling organizations to develop effective fraud-detection models <span class="No-Break">and strategies.</span></p>
<p>An <a id="_idIndexMarker376"/>application in the NFT space is to help determine whether a collection is owned by the retail public or whether it is held by a small group of collectors (centralized). This knowledge about an art collection’s ownership characteristics can help an investor assess whether a project’s price aligns with the market value or whether it is susceptible to manipulation. A more decentralized ownership structure implies a price closer to the <span class="No-Break">market value.</span></p>
<p>In <strong class="source-inline">Chapter06/EDA.ipynb</strong>, we investigate the distribution of NFTs by creating two categories of holders: those with more than three NFTs of the same collection, which we name <em class="italic">collectors</em>, and those with fewer than three NFTs, that is, the <em class="italic">general public</em>. We follow <span class="No-Break">three steps:</span></p>
<ol>
<li>We count how many NFTs each address has, equivalent to a <strong class="source-inline">GROUP BY</strong> operation in a SQL query. This allows us to understand the holdings of <span class="No-Break">each address.</span></li>
<li>We create a list of addresses that hold more than three NFTs from the same collection. The number of NFTs per address criteria is part of our analysis and a decision we make during the EDA. These micro-decisions impact the final result, and it is good practice to <span class="No-Break">document them.</span></li>
<li>We build two separate <strong class="source-inline">collectors_df</strong> and <strong class="source-inline">distributed_df</strong> datasets depending on whether the address is in the list from <span class="No-Break">Step 2.</span></li>
</ol>
<p>With these straightforward steps, we can calculate the percentage of collectors and distributed owners for the project. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em> reveals that the collector percentage is only 32%, while the remaining 68% is held by <span class="No-Break">the public.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 6.9 – Collector and distributed percentages" height="625" src="image/B19446_06_09.jpg" width="1111"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Collector and distributed percentages</p>
<p>To <a id="_idIndexMarker377"/>summarize quantitative variables, we introduce concepts such as the mean, average, deviations, outliers, and others. Let’s start with measures of central tendency or summary statistics, which are used to describe a set of values with a single value. These include the mean, median, <span class="No-Break">and mode.</span></p>
<p>The <strong class="old">mean</strong>, or average, is the<a id="_idIndexMarker378"/> most commonly used measure. It is calculated by summing all the values in a dataset and dividing it by the number <span class="No-Break">of values:</span></p>
<pre class="console">
Formula: Mean = (Sum of all values) / (Total number of values)</pre> <p>For example: if we have <strong class="source-inline">5</strong>, <strong class="source-inline">7</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">10</strong>, and <strong class="source-inline">6</strong> as values, the mean would be (5 + 7 + 2 + 10 + 6) / 5 = <span class="No-Break">6.</span></p>
<p>Pandas provides the <strong class="source-inline">mean()</strong> function, which returns the mean value for a column passed. If we calculate the mean of prices in the Witch dataset, we will add all prices and divide the result by the number of rows. Please see the following code snippet in <strong class="source-inline">Chapter06/EDA.ipynb</strong>, where we calculated the mean for the <span class="No-Break"><strong class="source-inline">price</strong></span><span class="No-Break"> column:</span></p>
<pre class="console">
df['price'].mean()</pre> <p>To calculate the mean, all values are considered, but the resulting number may not be in the analyzed sample. Another important aspect is that the mean value is heavily influenced by outliers. Therefore, it is necessary to clean the dataset and remove outliers before <span class="No-Break">calculating it.</span></p>
<p>The mean value is not a perfect measure of central tendency when the data is skewed. A better alternative is <span class="No-Break">the median.</span></p>
<p>The <strong class="old">median</strong> is <a id="_idIndexMarker379"/>defined as the middle value of a column when arranged in order of magnitude, from <a id="_idIndexMarker380"/>smallest to largest. To manually calculate the median, we would follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Take all values from the <strong class="source-inline">price</strong> column and order them from smallest <span class="No-Break">to largest.</span></li>
<li>Find the number situated in the center that divides the dataset into two <span class="No-Break">equal parts:</span><p class="list-inset"><strong class="source-inline">Formula (odd number of values): Median = </strong><span class="No-Break"><strong class="source-inline">Middle value</strong></span></p><p class="list-inset"><strong class="source-inline">Formula (even number of values): Median = (Sum of two middle values) / 2</strong></p><p class="list-inset">For example: for the <strong class="source-inline">5</strong>, <strong class="source-inline">7</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">10</strong>, and <strong class="source-inline">6</strong> dataset, when arranged in ascending order, the median would <span class="No-Break">be 6.</span></p></li>
</ol>
<p>Pandas provides the <strong class="source-inline">median()</strong> function to perform this calculation. For instance, to calculate the median for the <strong class="source-inline">price</strong> column, we can use the following code snippet, which is also shown <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter06/EDA.ipynb</strong></span><span class="No-Break">:</span></p>
<pre class="console">
df['price'].median()</pre> <p>The median is often preferred over the mean when dealing with skewed data or data <span class="No-Break">with outliers.</span></p>
<p>In <strong class="source-inline">Chapter04/Art.ipynb</strong>, when summarizing the data to find the floor price from multiple marketplaces over a set period of time, we chose to show the median instead of the average and the result is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 6.10 – Median floor price by marketplace" height="277" src="image/B19446_06_10.jpg" width="1168"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Median floor price by marketplace</p>
<p>If we had used the average, the graphic would have shown significant variations between marketplaces, as depicted in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em>. Analyzing the floor price based on the average would <a id="_idIndexMarker381"/>not have been accurate, especially when referring to the <span class="No-Break">OpenSea offer.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 6.11 – Average floor price by marketplace" height="283" src="image/B19446_06_11.jpg" width="1174"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Average floor price by marketplace</p>
<p>The <strong class="old">mode</strong> is <a id="_idIndexMarker382"/>another measure of central tendency that represents the most frequently occurring value in the dataset. Graphically speaking, it is represented by the highest bar in the histogram. It can also be used with categorical variables. To calculate the mode manually, we would identify the most repeated price in <span class="No-Break">our dataset:</span></p>
<pre class="console">
Formula: No specific formula</pre> <p>For example, in the <strong class="source-inline">15</strong>, <strong class="source-inline">20</strong>, <strong class="source-inline">18</strong>, <strong class="source-inline">22</strong>, <strong class="source-inline">15</strong>, <strong class="source-inline">20</strong>, <strong class="source-inline">18</strong>, <strong class="source-inline">20</strong>, <strong class="source-inline">22</strong>, and <strong class="source-inline">25</strong> dataset, the value that appears most frequently is 20. It appears <span class="No-Break">three times.</span></p>
<p>Pandas provides the <strong class="source-inline">mode()</strong> function to calculate the mode for a column, as shown in the following code snippet <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">Chapter06/EDA.ipynb</strong></span><span class="No-Break">:</span></p>
<pre class="console">
df['price'].mode()[0]</pre> <p class="callout-heading">A note on missing prices</p>
<p class="callout">Measures of central tendency not only assist us in summarizing our dataset but also prove helpful in addressing missing values within <span class="No-Break">the dataset.</span></p>
<p class="callout">For instance, during periods of extreme volatility, certain trading houses may suspend commercialization. This holds true for both traditional markets and centralized crypto exchanges. If our database happens to source prices from such an exchange, it is possible that there will be missing rows of data. In such scenarios, pandas functions such as <strong class="source-inline">fillna()</strong> or <strong class="source-inline">interpolate()</strong> can be employed to impute missing values. With the <strong class="source-inline">fillna()</strong> function, we can specify whether to complete the NaN values with the mean or <span class="No-Break">the median.</span></p>
<p>In conclusion, in our EDA, we have explored measures of central tendency such as the mean, median, and<a id="_idIndexMarker383"/> mode, which provide insights into the distribution and characteristics of <span class="No-Break">our data.</span></p>
<p>Building upon our exploration of central tendency, we now turn our attention to outlier detection. Outliers are data points that deviate significantly from the overall pattern of the dataset, and they can have a substantial impact on our analysis and interpretation. In the next section, we will delve into various techniques and approaches for <span class="No-Break">ide<a id="_idTextAnchor223"/>ntifying outliers.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor224"/>Outlier detection</h2>
<p>According<a id="_idIndexMarker384"/> to the book <em class="italic">Introduction to Data Science</em> by Rafael A. Irizarry, outliers are defined as <em class="italic">“data samples with a value that is far from the central tendency</em>." While outliers are not inherently good or bad, they can significantly impact our analysis and lead to incorrect conclusions. This is <a id="_idIndexMarker385"/>particularly true when working with prices, where market volatility can distort the true value of assets transacted. Prices are used as proxies for value, but it’s important to recognize that some prices deviate significantly from the actual value, making <span class="No-Break">them outliers.</span></p>
<p>In certain instances, the primary goal is to identify and analyze outliers, which is typically the focus in anomaly detection techniques such as those discussed in <a href="B19446_04.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, specifically for uncovering fraud or <span class="No-Break">money laundering.</span></p>
<p>There are several reasons why outliers <span class="No-Break">may occur:</span></p>
<ul>
<li>An instrument measurement error such as a disconnected API or an <span class="No-Break">unbalanced scale</span></li>
<li>Data <span class="No-Break">entry errors</span></li>
<li>Working with samples or populations that are less homogeneous than <span class="No-Break">initially assumed</span></li>
</ul>
<p>Let’s explore some techniques to identify outliers in <span class="No-Break">our dataset:</span></p>
<ul>
<li><strong class="old">Box plot</strong> (<strong class="old">whisker plot</strong>): This <a id="_idIndexMarker386"/>graphical representation summarizes the data using five important numbers: minimum, first quartile, median, third quartile, and maximum. We can identify outliers as those that are far from <span class="No-Break">the box:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 6.12 – Parts of the box plot" height="568" src="image/B19446_06_12.jpg" width="1395"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Parts of the box plot</p>
<p class="list-inset">This image <a id="_idIndexMarker387"/>can be automatically generated by Pandas with the following <span class="No-Break">code snippet:</span></p>
<pre class="source-code">
<strong class="old">df.boxplot(column=['ETH_price'])</strong></pre> <p class="list-inset">Alternatively, we can use the Seaborn library, as shown in our Jupyter <span class="No-Break">notebook (</span><span class="No-Break"><strong class="source-inline">Chapter06/Outliers</strong></span><span class="No-Break">).</span></p>
<ul>
<li><strong class="old">IQR method</strong> (<strong class="old">Interquartile range</strong>): This <a id="_idIndexMarker388"/>method is also used by box plots. Quartiles (Q) are the four points that divide data into same-sized groups. In IQR, we use them as limits against which we will define whether a value is too far from the set and has to be considered an outlier. If a data point is more than 1.5 times the lower quartile (Q1) or more than 1.5 times the upper quartile (Q3), it is considered <span class="No-Break">an outlier.</span><p class="list-inset">Pandas helps to <a id="_idIndexMarker389"/>calculate the quartiles with the <strong class="source-inline">df.quantile()</strong> function. Once we have them, we can calculate other parts of <span class="No-Break">this formula.</span></p><p class="list-inset">In <strong class="source-inline">Chapter05/Outliers</strong>, we calculate the IQR, bottom, and upper limits in this part of <span class="No-Break">the code:</span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 6.13 – Calculation of IQR and limits" height="218" src="image/B19446_06_13.jpg" width="703"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Calculation of IQR and limits</p>
<p class="list-inset">The 1.5 multiplier is common practice but can be adjusted<a id="_idTextAnchor225"/> to adapt to <span class="No-Break">our case.</span></p>
<ul>
<li><strong class="old">Three-sigma rule</strong> (<strong class="old">Z-score</strong>): The <a id="_idIndexMarker390"/>three-sigma rule states that 99.7% of data falls within three standard deviations (three-sigma) of the mean in a normal distribution. This rule is used for outlier detection because data points outside the three sigmas can be <span class="No-Break">considered outliers.</span><p class="list-inset">This <a id="_idIndexMarker391"/>method uses the mean as a starting point, which can be influenced by outliers. A more robust modified Z-score method can be employed, which incorporates the median and median absolute deviation in its formula. The metric is a statistical measure that helps identify outliers in a dataset by comparing each data point to the median and median absolute deviation. It provides a robust way to detect extreme values, especially in datasets with <span class="No-Break">skewed distributions.</span></p></li>
</ul>
<h1 id="_idParaDest-132"><a id="_idTextAnchor226"/>Summary</h1>
<p>This chapter has addressed various preparation methods applicable to on-chain data scenarios. We explored techniques such as unhexing data, decimal treatment, handling checksum addresses, and converting Unix timestamps to datetime formats. These methods have proven foundational in preparing the on-chain data for <span class="No-Break">subsequent analysis.</span></p>
<p>Moreover, we introduced the concept of EDA as a crucial step in understanding and summarizing datasets, with a specific focus on central tendency metrics. Additionally, we delved into outlier detection techniques, such as box plots and the IQR method, aiding in the identification of extreme observations deviating significantly from the majority of <span class="No-Break">the data.</span></p>
<p>By applying these cleaning and EDA techniques, we have equipped ourselves with essential tools for the effective analysis and interpretation of on-chain data. These foundational concepts serve as building blocks for more advanced techniques and methodologies as we continue this journey. For more insights into these methodologies, please refer to the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
<h1 id="_idParaDest-133"><a id="_idTextAnchor227"/>Further reading</h1>
<p>The following links may help to complement <span class="No-Break">this chapter:</span></p>
<ul>
<li><span class="No-Break">Technical requirements:</span><ul><li><em class="italic">10 minutes to pandas, pandas 1.5.3 documentation, pandas - Python Data Analysis </em><span class="No-Break"><em class="italic">Library:</em></span><span class="No-Break"> </span><span class="No-Break">https://pandas.pydata.org/docs/user_guide/10min.xhtml#min</span></li></ul></li>
<li>Evolution of <span class="No-Break">smart contracts:</span><ul><li><em class="italic">Upgrading Smart Contracts, </em><span class="No-Break"><em class="italic">ethereum.org:</em></span><span class="No-Break"> </span><a href="https://ethereum.org/en/developers/docs/smart-contracts/upgrading/"><span class="No-Break">https://ethereum.org/en/developers/docs/smart-contracts/upgrading/</span></a></li></ul></li>
<li>Exploratory <span class="No-Break">Data Analysis:</span><ul><li><em class="italic">Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications, </em>Laura Igual, Santi <span class="No-Break">Seguí, Springer</span></li><li> <em class="italic">Three Ways to Detect Outliers, Colin Gorrie, Colin Gorrie’s Data </em><span class="No-Break"><em class="italic">Story:</em></span><span class="No-Break"> </span><a href="https://colingorrie.github.io/outlier-detection.xhtml#modified-z-score-method"><span class="No-Break">https://colingorrie.github.io/outlier-detection.xhtml#modified-z-score-method</span></a></li><li><em class="italic">Python for Data Analysis: Data Wrangling With Pandas, NumPy, and Jupyter, </em>Wes McKinney, <span class="No-Break">O’Reilly Media</span></li><li><em class="italic">Box Plot Review, Khan </em><span class="No-Break"><em class="italic">Academy:</em></span><span class="No-Break"> </span><a href="https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review"><span class="No-Break">https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review</span></a></li><li><em class="italic">Hands-On Exploratory Data Analysis With Python: Perform EDA Techniques to Understand, Summarize, and Investigate Your Data, </em>Usman Ahmed, Suresh Kumar Mukhiya, O’Reilly <span class="No-Break">Media</span><span class="No-Break"><em class="italic">:</em></span><span class="No-Break"> </span><a href="https://www.packtpub.com/product/hands-on-exploratory-data-analysis-with-python/9781789537253"><span class="No-Break">https://www.packtpub.com/product/hands-on-exploratory-data-analysis-with-python/9781789537253</span></a></li><li><em class="italic">The Data Science Design Manual, </em>Steven <span class="No-Break">Skiena, Springer</span></li></ul></li>
<li><span class="No-Break">Cleaning:</span><ul><li><em class="italic">RSK </em><span class="No-Break"><em class="italic">Explorer</em></span><span class="No-Break">: </span><a href="https://explorer.rsk.co/blocks"><span class="No-Break">https://explorer.rsk.co/blocks</span></a></li><li><em class="italic">More on Time on the Blockchain, Nick Furneaux, Wiley</em>, <a href="B19446_09.xhtml#_idTextAnchor269"><em class="italic">Chapter 9</em></a>, Investigating Cryptocurrencies, Understanding, Extracting, and Analyzing Blockchain Evidence, Page 156 to <span class="No-Break">Page 161</span></li></ul></li>
</ul>
</div>
</div></body></html>