- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing and Exploring Our Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preparation is a common theme in data science, extending beyond its association
    with the machine learning pipeline. It takes on various monikers such as data
    wrangling, data cleaning, and data preprocessing for feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we emphasize that significant time will be invested in data cleaning,
    feature engineering, and exploratory analysis, and we recognize the positive impact
    of robust preprocessing on outcomes, whether for a presentation for business stakeholders
    or its integration to a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning** encompasses tasks focused on identifying and rectifying data
    issues, particularly errors and artifacts. Errors result from data loss in the
    acquisition pipeline, while artifacts arise from the system that generates the
    data. Cleaning involves addressing missing data, handling outliers, removing duplicates,
    and performing necessary translations for data readability and conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation** spans tasks such as understanding and transforming received
    data to align with subsequent pipeline steps. This chapter delves into common
    scenarios that arise when working to understand, preprocess, and extract information
    from on-chain data. Specific topics include decimal treatment, approaches to smart
    contract evolution, and checksum validation. Additionally, the chapter introduces
    the **Exploratory Data Analysis** (**EDA**) concept and employs techniques for
    summary statistics and outlier detection to illustrate its advantages and insights.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores the intricacies of preparing on-chain data and introduces
    the concept of EDA, facilitating the transition from analytics to machine learning.
    It does not aim to provide an exhaustive overview of all tools and methods, due
    to the extensive nature of this field of data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: On-chain data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Exploratory Data Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We extensively use the Pandas library, a popular and useful Python library for
    working with DataFrames and series. Pandas offers numerous functions to analyze,
    summarize, explore, normalize, and manipulate them. Series are one-dimensional
    array-like objects, and DataFrames are two-dimensional table structures with rows
    and columns. We use Pandas throughout this book’s exercises to perform the aforementioned
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven’t installed Pandas yet, you can do so with the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The documentation for Pandas is available at [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: For data visualization, we use the Matplotlib and Seaborn libraries. Matplotlib
    provides a wide range of tools and control over the images we build. Seaborn is
    built on top of Matplotlib and is more user-friendly but has less flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for both libraries can be found at [https://seaborn.pydata.org/](https://seaborn.pydata.org/)
    and [https://matplotlib.org/](https://matplotlib.org/), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the data and code files for this chapter in the book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter06](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter06).
    We recommend that you read through the code files in the `Chapter06` folder to
    follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with information collected from diverse data sources, it is crucial
    to ensure consistency and uniformity across all records and fields before extracting
    insights or feeding the data into a machine learning model. In this section, we
    will explore various data preparation tasks that are particularly relevant to
    on-chain data.
  prefs: []
  type: TYPE_NORMAL
- en: Hex values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hexadecimal notation is a base 16 system, utilizing symbols to represent numerical
    values from 0 to 9 and letters from A to F. In contrast, our everyday decimal
    notation employs 10 symbols to represent numerical values (0–9). Hexadecimal notation
    extends the range by including A to F, representing values from 10 to 15\. This
    notation is often used for data storage purposes due to its efficiency in representing
    binary numbers with each hex digit representing 4 bits.
  prefs: []
  type: TYPE_NORMAL
- en: In the example presented in `Chapter06/Preparation`, we retrieve the latest
    block number from the Rootstock public node by following the documentation available
    at [https://developers.rsk.co/rsk/public-nodes/](https://developers.rsk.co/rsk/public-nodes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting value is presented in the form of a hex number, such as `0x4e07d0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Hexadecimal block number](img/B19446_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Hexadecimal block number
  prefs: []
  type: TYPE_NORMAL
- en: 'This hex number can be decoded into a decimal number providing the base (`16`)
    using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Hexadecimal block number decoded](img/B19446_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Hexadecimal block number decoded
  prefs: []
  type: TYPE_NORMAL
- en: 'Following those steps, we are able to translate the hex response from the RSK
    node into our decimal system. To verify the accuracy of the translated information,
    we can compare our findings with the chain explorer available at [https://explorer.rsk.co/blocks](https://explorer.rsk.co/blocks)
    or [https://rootstock.blockscout.com/](https://rootstock.blockscout.com/). We
    will see that the block was added to the chain just a moment ago:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Block explorer](img/B19446_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Block explorer
  prefs: []
  type: TYPE_NORMAL
- en: Certain SQL database engines have the capability to convert hex values into
    a human-readable format directly within the query. For example, the ClickHouse
    system used by Covalent provides the `unhex` method. You can find more details
    in the documentation at [https://clickhouse.com/docs/en/sql-reference/functions/encoding-functions#unhex](https://clickhouse.com/docs/en/sql-reference/functions/encoding-functions#unhex).
  prefs: []
  type: TYPE_NORMAL
- en: Checksum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Checksum is an algorithm that hashes the address, enabling Ethereum to verify
    whether it is a valid address. In Ethereum, a checksummed address contains both
    uppercase and lowercase letters in a specific pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Checksum address** | **Non-checksum address** |'
  prefs: []
  type: TYPE_TB
- en: '| `0x95222290DD7278Aa3Ddd389Cc1E1d 165CC4BAfe5` | `0x95222290dd7278aa3ddd389cc1e1
    d165cc4bafe5` |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Difference between addresses
  prefs: []
  type: TYPE_NORMAL
- en: Ethereum treats both lowercase and checksummed addresses as valid, and funds
    sent to either version will be directed to the same recipient. However, using
    a checksummed address provides an additional layer of security by preventing the
    accidental sending of funds to non-existent addresses.
  prefs: []
  type: TYPE_NORMAL
- en: This section holds significance on two fronts. Firstly, Python, like many SQL
    engines, is case-sensitive. So, it becomes imperative to manage the differentiation
    between lowercase and checksummed addresses when comparing or merging data from
    diverse sources. This guarantees compatibility and precision in data analysis.
    The second dimension pertains to the differentiation between valid and invalid
    addresses, a crucial aspect in maintaining data integrity and making our queries
    faster to run.
  prefs: []
  type: TYPE_NORMAL
- en: In `Chapter06/Preparation`, we test whether an address is a valid checksummed
    Ethereum address. For this purpose, we utilize the `test()` function allows us
    to convert a lowercase address into its checksummed version.
  prefs: []
  type: TYPE_NORMAL
- en: For another example, please refer to `Chapter10/EDA`, where we demonstrate the
    application of checksum addresses within a filter to remove invalid Ethereum addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Decimal treatment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solidity is the most commonly used smart contract programming language for
    EVM-based blockchains, but it does not support floats. To express decimals in
    Solidity, we use integers with the `Chapter04/Art` in Jupyter Notebook, in the
    *Chainlink* section, we can see the response from the oracle expressed in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Chainlink floor price response](img/B19446_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Chainlink floor price response
  prefs: []
  type: TYPE_NORMAL
- en: Such a large number is not meaningful in an economic context and cannot be directly
    included in a dashboard or report. Therefore, it is necessary to translate it
    into our decimal system to make it more useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart contracts provide the number of decimals through a specific function.
    In the case of Chainlink’s data-feed smart contract, the `decimals()` function
    informs us about the fixed point, or, in practical terms, how many decimal places
    we have to shift the comma to the left to convert the response into our decimal
    system. The steps to query the smart contract are explained in the [*Chapter 2*](B19446_02.xhtml#_idTextAnchor073)
    section *Exploring state data*, and as shown in the Jupyter Notebook, the result
    is `18`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The data-feed decimal](img/B19446_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – The data-feed decimal
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure showcases the transformed number that we can use in subsequent
    parts of a data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The result](img/B19446_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The result
  prefs: []
  type: TYPE_NORMAL
- en: 'The same result can be achieved by applying the `fromWei()` function as per
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The decimal treatment we just explored is also relevant for tokens. `decimal()`,
    which *returns the number of decimal places the token uses, for example 8, which
    means to divide the token amount by 100000000 (10 to the power 8) to get its*
    *user representation*.
  prefs: []
  type: TYPE_NORMAL
- en: More of this standard was analyzed in [*Chapter 5*](B19446_05.xhtml#_idTextAnchor168).
  prefs: []
  type: TYPE_NORMAL
- en: Note on decimals
  prefs: []
  type: TYPE_NORMAL
- en: The most common decimal denominator for Ethereum smart contracts is 18, while
    Bitcoin uses 8 and USDT utilizes 6 decimals.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned that consuming on-chain data often requires
    extensive transformations. If our dataset contains excessively large strings that
    lack economic meaning, we may need to search for the smart contract’s decimal
    value to properly position the decimal point. Additionally, if our dataset includes
    hex values, we need to decode them into our decimal system. Lastly, we have discovered
    how to transform lowercase addresses into checksum addresses to ensure compatibility
    with case-sensitive programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: From Unix timestamps to datetime formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unix timestamps are commonly used in data analysis, but for visualization purposes
    in dashboards and reports, it is necessary to convert them into a human-readable
    format. Unix time represents the number of seconds that have passed since January
    1, 1970, providing a system to track time using a single integer value.
  prefs: []
  type: TYPE_NORMAL
- en: In most SQL engines, the `truncate` function can be utilized to extract the
    relevant date part from the timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, we can use the `datetime` module’s `fromtimestamp()` function, which
    converts Unix timestamps to local datetime, and the `utcfromtimestamp()` function,
    which converts them to UTC datetimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Jupyter Notebook’s `Chapter06/Preparation` section, we translate a Unix
    timestamp with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Datetime translation](img/B19446_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Datetime translation
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate our results, we can compare them with those obtained from [https://www.unixtimestamp.com/](https://www.unixtimestamp.com/),
    a popular tool that shows the same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Unix timestamp translator](img/B19446_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Unix timestamp translator
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of smart contracts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Smart contracts, like any software product, may undergo changes and require
    upgrades for various reasons such as business necessity, security incidents, or
    to reduce gas costs. However, by design, everything deployed on the blockchain
    is immutable. The following information, sourced from [Ethereum.org](http://Ethereum.org),
    outlines multiple approaches to enable an upgrade. The content is quoted under
    the **Creative Commons Attribution 4.0 International** (**CC BY 4.0**) license,
    in compliance with the terms of use. The original document can be found in the
    *Further reading* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart contract upgrades can be achieved via the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple versions of a smart contract and migrating state (i.e., data)
    from the old contract to a new instance of the contract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating separate contracts to store business logic and state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using proxy patterns to delegate function calls from an immutable proxy contract
    to a modifiable logic contract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an immutable main contract that interfaces with and relies on flexible
    satellite contracts to execute specific functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the diamond pattern to delegate function calls from a proxy contract to
    logic contracts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, the ways to upgrade a smart contract do not involve modifying
    the deployed code. Rather, it entails substituting one contract for another. Currently,
    the most popular method for upgrading smart contracts is the **proxy pattern**.
    This pattern involves a separation between the proxy contract and the execution
    contract that holds the logic. The proxy acts on behalf of the logic smart contract
    redirecting transactions from the frontend to the correct smart contract in the
    backend. It is possible to swap the logic smart contract in the backend and update
    the proxy to start redirecting transactions to the newly deployed smart contract
    with the latest logic.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that contracts may change implies that our queries need to adapt to
    reflect those changes. For example, if a smart contract starts emitting an event
    at a certain block, we need to be aware of it to capture the new information.
    As we saw in [*Chapter 2*](B19446_02.xhtml#_idTextAnchor073), we need the **Application
    Binary Interface** (**ABI**) to decode smart contracts that need to be duly updated
    to decode transactions after an upgrade. Not being aware that the contract we
    are parsing has changed may cause us to miss certain events and may negatively
    impact our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing a specific project, it’s important to follow press releases,
    project representatives, and official information channels for news of any development
    launched to direct our queries to the smart contract that is less prone to changes.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while smart contracts are designed to be immutable, there are
    circumstances where upgrades and changes become necessary, and we need to be prepared
    to adapt our queries or code to these changes. For example, if we were analyzing
    Ethereum, we would need to be aware that the entire chain changed with the Merge,
    which occurred in September 2022 and has an impact at the data level.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Between the data cleaning phase and the modeling or formal statistical analysis,
    there exists an intermediate step known as EDA, which is a fundamental aspect
    of data science. EDA serves as the primary approach to understanding and making
    sense of a dataset, providing insights into the “population out of the sample”
    and transforming raw data into actionable information for businesses. EDA can
    include various techniques and methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data summary or descriptive statistics**: Used to summarize central tendencies
    within the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data visualization**: Graphical techniques such as histograms, box plots,
    scatter plots, and line plots are employed to visualize the data, aiding in pattern
    identification, outlier detection, and understanding the relationship between
    variables. Furthermore, data visualization is particularly effective when presenting
    conclusions to a non-technical audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data exploration**: Helps us understand the distribution of variables, assess
    their shapes and skewness, and identify the presence of anomalies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling missing data**: This allows us to identify missing rows and assess
    their impact on the results. It helps to determine patterns in missing values
    and to develop strategies for handling them effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection**: Identifying values that significantly deviate from the
    rest of the dataset and evaluating their influence on the analysis. These outliers
    can result from various causes, which we will discuss in the subsequent section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlations and patterns**: Explores the relationships between variables
    using techniques such as correlation analysis and scatter plots. Additionally,
    they help identify trends or seasonality in the data over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A concise description of EDA can be found at [https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15):
    "*Exploratory Data Analysis refers to the critical process of performing initial
    investigations on data so as to discover patterns, to spot anomalies, to test
    hypothesis and to check assumptions with the help of summary statistics and* *graphical
    representations."*'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will provide a brief introduction to summary statistics
    and outlier detection with the aid of graphical representations. We chose those
    two topics as they are probably transversals to all the datasets we will encounter
    in our journey. For further exploration of EDA topics, feel free to refer to the
    books in the *Further reading* section that are very useful.
  prefs: []
  type: TYPE_NORMAL
- en: To exemplify the concepts learned in this section, we will use the **Witches**
    dataset available on Kaggle ([https://www.kaggle.com/datasets/harrywang/crypto-coven?select=witches.csv](https://www.kaggle.com/datasets/harrywang/crypto-coven?select=witches.csv)).
    It contains information about the Crypto Coven NFT project, where each row represents
    one witch NFT. The Witches project’s main page can be found at [https://www.cryptocoven.xyz/](https://www.cryptocoven.xyz/).
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our datasets will contain categorical or quantitative variables. Categorical
    variables are those that can be divided into groups, such as colors or brands.
    On the other hand, quantitative variables represent numerical amounts, such as
    prices or the number of sales. The `df.describe()` code snippet returns the quantitative
    variables and their distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the distribution of categorical data involves determining the frequency
    of each category. This analysis can provide meaningful insights. For example,
    in marketing, understanding the distribution of categorical variables such as
    age groups, income levels, or consumer preferences can help businesses segment
    their target audience effectively and create more targeted and successful campaigns.
    Another example is in fraud detection, where categorical variables such as transaction
    types or user behavior patterns can be crucial in detecting fraudulent activities.
    By studying the distribution of these variables, anomalies or unusual patterns
    can be identified, enabling organizations to develop effective fraud-detection
    models and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: An application in the NFT space is to help determine whether a collection is
    owned by the retail public or whether it is held by a small group of collectors
    (centralized). This knowledge about an art collection’s ownership characteristics
    can help an investor assess whether a project’s price aligns with the market value
    or whether it is susceptible to manipulation. A more decentralized ownership structure
    implies a price closer to the market value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Chapter06/EDA.ipynb`, we investigate the distribution of NFTs by creating
    two categories of holders: those with more than three NFTs of the same collection,
    which we name *collectors*, and those with fewer than three NFTs, that is, the
    *general public*. We follow three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We count how many NFTs each address has, equivalent to a `GROUP BY` operation
    in a SQL query. This allows us to understand the holdings of each address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a list of addresses that hold more than three NFTs from the same collection.
    The number of NFTs per address criteria is part of our analysis and a decision
    we make during the EDA. These micro-decisions impact the final result, and it
    is good practice to document them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We build two separate `collectors_df` and `distributed_df` datasets depending
    on whether the address is in the list from Step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these straightforward steps, we can calculate the percentage of collectors
    and distributed owners for the project. *Figure 6**.9* reveals that the collector
    percentage is only 32%, while the remaining 68% is held by the public.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Collector and distributed percentages](img/B19446_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Collector and distributed percentages
  prefs: []
  type: TYPE_NORMAL
- en: To summarize quantitative variables, we introduce concepts such as the mean,
    average, deviations, outliers, and others. Let’s start with measures of central
    tendency or summary statistics, which are used to describe a set of values with
    a single value. These include the mean, median, and mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **mean**, or average, is the most commonly used measure. It is calculated
    by summing all the values in a dataset and dividing it by the number of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For example: if we have `5`, `7`, `2`, `10`, and `6` as values, the mean would
    be (5 + 7 + 2 + 10 + 6) / 5 = 6.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas provides the `mean()` function, which returns the mean value for a column
    passed. If we calculate the mean of prices in the Witch dataset, we will add all
    prices and divide the result by the number of rows. Please see the following code
    snippet in `Chapter06/EDA.ipynb`, where we calculated the mean for the `price`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To calculate the mean, all values are considered, but the resulting number may
    not be in the analyzed sample. Another important aspect is that the mean value
    is heavily influenced by outliers. Therefore, it is necessary to clean the dataset
    and remove outliers before calculating it.
  prefs: []
  type: TYPE_NORMAL
- en: The mean value is not a perfect measure of central tendency when the data is
    skewed. A better alternative is the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **median** is defined as the middle value of a column when arranged in
    order of magnitude, from smallest to largest. To manually calculate the median,
    we would follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Take all values from the `price` column and order them from smallest to largest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the number situated in the center that divides the dataset into two equal
    parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Formula (odd number of values): Median =` `Middle value`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Formula (even number of values): Median = (Sum of two middle values) / 2`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example: for the `5`, `7`, `2`, `10`, and `6` dataset, when arranged in
    ascending order, the median would be 6.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pandas provides the `median()` function to perform this calculation. For instance,
    to calculate the median for the `price` column, we can use the following code
    snippet, which is also shown in `Chapter06/EDA.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The median is often preferred over the mean when dealing with skewed data or
    data with outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Chapter04/Art.ipynb`, when summarizing the data to find the floor price
    from multiple marketplaces over a set period of time, we chose to show the median
    instead of the average and the result is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Median floor price by marketplace](img/B19446_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Median floor price by marketplace
  prefs: []
  type: TYPE_NORMAL
- en: If we had used the average, the graphic would have shown significant variations
    between marketplaces, as depicted in *Figure 6**.11*. Analyzing the floor price
    based on the average would not have been accurate, especially when referring to
    the OpenSea offer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Average floor price by marketplace](img/B19446_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Average floor price by marketplace
  prefs: []
  type: TYPE_NORMAL
- en: 'The **mode** is another measure of central tendency that represents the most
    frequently occurring value in the dataset. Graphically speaking, it is represented
    by the highest bar in the histogram. It can also be used with categorical variables.
    To calculate the mode manually, we would identify the most repeated price in our
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For example, in the `15`, `20`, `18`, `22`, `15`, `20`, `18`, `20`, `22`, and
    `25` dataset, the value that appears most frequently is 20\. It appears three
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas provides the `mode()` function to calculate the mode for a column, as
    shown in the following code snippet from `Chapter06/EDA.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A note on missing prices
  prefs: []
  type: TYPE_NORMAL
- en: Measures of central tendency not only assist us in summarizing our dataset but
    also prove helpful in addressing missing values within the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, during periods of extreme volatility, certain trading houses may
    suspend commercialization. This holds true for both traditional markets and centralized
    crypto exchanges. If our database happens to source prices from such an exchange,
    it is possible that there will be missing rows of data. In such scenarios, pandas
    functions such as `fillna()` or `interpolate()` can be employed to impute missing
    values. With the `fillna()` function, we can specify whether to complete the NaN
    values with the mean or the median.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, in our EDA, we have explored measures of central tendency such
    as the mean, median, and mode, which provide insights into the distribution and
    characteristics of our data.
  prefs: []
  type: TYPE_NORMAL
- en: Building upon our exploration of central tendency, we now turn our attention
    to outlier detection. Outliers are data points that deviate significantly from
    the overall pattern of the dataset, and they can have a substantial impact on
    our analysis and interpretation. In the next section, we will delve into various
    techniques and approaches for identifying outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to the book *Introduction to Data Science* by Rafael A. Irizarry,
    outliers are defined as *“data samples with a value that is far from the central
    tendency*." While outliers are not inherently good or bad, they can significantly
    impact our analysis and lead to incorrect conclusions. This is particularly true
    when working with prices, where market volatility can distort the true value of
    assets transacted. Prices are used as proxies for value, but it’s important to
    recognize that some prices deviate significantly from the actual value, making
    them outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In certain instances, the primary goal is to identify and analyze outliers,
    which is typically the focus in anomaly detection techniques such as those discussed
    in [*Chapter 4*](B19446_04.xhtml#_idTextAnchor145), specifically for uncovering
    fraud or money laundering.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why outliers may occur:'
  prefs: []
  type: TYPE_NORMAL
- en: An instrument measurement error such as a disconnected API or an unbalanced
    scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data entry errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with samples or populations that are less homogeneous than initially
    assumed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s explore some techniques to identify outliers in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Box plot** (**whisker plot**): This graphical representation summarizes the
    data using five important numbers: minimum, first quartile, median, third quartile,
    and maximum. We can identify outliers as those that are far from the box:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Parts of the box plot](img/B19446_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Parts of the box plot
  prefs: []
  type: TYPE_NORMAL
- en: 'This image can be automatically generated by Pandas with the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
