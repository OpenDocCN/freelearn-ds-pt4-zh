- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Primer on Machine Learning and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before applying any machine learning algorithm, having a comprehensive understanding
    of the dataset and its key features is essential. This understanding is typically
    derived through **exploratory data analysis** (**EDA**). Once acquainted with
    the data, we must invest time in feature engineering, which involves selecting,
    transforming, and creating new features (if necessary) to enable the use of the
    chosen model or enhance its performance. Feature engineering may include tasks
    such as converting classes into numerical values, scaling or normalizing features,
    creating new features from existing ones, and more. This process is tailored for
    each specific model and dataset under analysis. Once this process is completed,
    we can proceed to modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to review introductory concepts of machine learning
    and deep learning, laying the foundation for *Part 2* of this book. In *Part 2*,
    we will delve into various use cases where artificial intelligence is applied
    to Web3 data. While not covering every possible model in detail, we will provide
    brief descriptions of project motivations, the models themselves, and the tools
    used, and include useful references for further reading.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the main concepts of machine learning and deep learning, discussing
    two typical machine learning pipelines – one using scikit-learn and the other
    using Keras. Additionally, we have compiled an extensive *Further reading* section
    for each theme covered in this chapter to encourage continued learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics will be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of machine learning and deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning pipeline with scikit-learn and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using **scikit-learn**, a popular Python library specially designed
    for machine learning tasks. It offers algorithms and tools for data preprocessing,
    feature selection, model selection, and model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not worked with scikit-learn before, it can be installed by using
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The documentation for scikit-learn can be found at https://scikit-learn.org/stable/.
  prefs: []
  type: TYPE_NORMAL
- en: For deep learning, we have the option to use **TensorFlow** or **Keras**. TensorFlow
    is a powerful open source library for numerical computation that provides solutions
    to train, test, and deploy a variety of deep learning neural networks. It serves
    as the infrastructure layer, which enables low-level tensor operations on the
    CPU, TPU, and GPU. On the other hand, Keras is a high-level Python API built on
    top of TensorFlow. It is specially prepared to enable fast experimentation and
    provides informative feedback when an error is discovered. According to the 2022
    survey *State of Data Science and Machine Learning*, by Kaggle, Keras reached
    a 61% adoption rate among machine learning developers and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not worked with TensorFlow or Keras before, they can be installed
    with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For deep learning, a large amount of computational power is required; our normal
    CPU may not be fully prepared for the task, resulting in slow training and inference.
    The alternative is to run a GPU locally or in the cloud – hosted using Kaggle
    Kernel or Google Colab. They have a similar UI that resembles the structure of
    a Jupyter notebook, making it easy to run the code from the repository on any
    of these platforms.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the data and code files for this chapter in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter07).
    We recommend that you read through the code files in the `Chapter07` folder to
    follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The definition of machine learning, as provided by Computer Science Wiki, is
    “*a field of inquiry devoted to understanding and building methods that “learn”
    – that is, methods that leverage data to improve performance on some set of tasks.
    It is seen as a part of artificial intelligence. Machine learning algorithms build
    a model based on sample data, known as training data, in order to make predictions
    or decisions without being explicitly programmed to* *do so*.”
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: [https://computersciencewiki.org/index.php/Machine_learning](https://computersciencewiki.org/index.php/Machine_learning))'
  prefs: []
  type: TYPE_NORMAL
- en: Professor Jason Brownlee defines deep learning as “*a subfield of machine learning
    concerned with algorithms inspired by the structure and function of the brain
    called artificial neural networks*.” Deep learning is distinguishable from other
    machine learning methods because it uses artificial neural networks as a basis
    for its methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between these two fields is generally represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Venn diagram of artificial intelligence](img/B19446_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Venn diagram of artificial intelligence
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze the definition of machine learning further:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models create their own rules based on the data we provide,
    as stated by the phrases “*understanding and building methods that learn*” and
    “*make predictions or decisions without being explicitly programmed to do so*.”
    Previously, we used filters in our queries or *if statements* in our programs.
    With machine learning, particularly supervised learning, we feed data and let
    the model infer the rules. In the book *Python Data Science Handbook*, the author
    challenges the idea that the model learns by itself, instead suggesting that it
    tunes the parameters we provide by adapting to the observed data. Once it fits
    those parameters to the seen data, it can infer results as needed from unseen
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“*Machine learning algorithms build a model based on sample data, known as
    training data*.” Data passed to machine learning algorithms needs to be split
    at least into two: training and test data. The training dataset is used to build
    the model. The test dataset is used to evaluate the model’s capacity to make predictions
    with data it has not seen before. The model’s predictions are then compared to
    the ground-truth data and the evaluation metrics are calculated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning techniques can be classified as supervised learning, unsupervised
    learning, and reinforcement learning. Common tasks that are solved by machine
    learning techniques are shown in *Figure 7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Machine learning applications](img/B19446_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Machine learning applications
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** consists of creating a function that can map inputs
    to outputs, allowing the model to infer outputs from unseen or similar inputs.
    In this process, we use features to describe the characteristics of a variable
    and labels or tags to identify the predicted variable. Through this, our model
    can learn the relationship between the features and the labels or tags.'
  prefs: []
  type: TYPE_NORMAL
- en: In Web3 analysis, **tagging** plays a crucial role as it allows us to attribute
    an identity to addresses that are a combination of numbers and letters and have
    no direct connection to the outside world. However, creating a library of tagged
    addresses can be a challenging task and just recently, it has become the business
    of a company named Arkham that incentivizes the “*de-anonymizing of the blockchain*”
    with public data. Tagged addresses are one of the main leverages for companies
    such as Nansen, which have made significant progress in tagging hundreds of addresses
    on Ethereum and other chains, enabling machine learning techniques and data analysis
    reports.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging can also be found in Etherscan, where important projects tag their addresses
    to enable public audits. Also, Dune and Flipside have tables with labels where
    their research teams add relevant information that can help with queries. If you
    want to learn more about identity attribution, Nick Fourneaux, in the book *Investigating
    Cryptocurrencies*, teaches how to extract addresses from websites such as forums
    or software-sharing sites, download HTML as raw text, and execute a regex analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning can be further divided into regression and classification
    techniques. In classification techniques, we have a discrete set of categories
    as labels (such as fraudulent transactions or non-fraudulent transactions). In
    regression, we have quantitative labels, such as the price of NFT art or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning** consists of trying to identify the structure or patterns
    of a dataset that may not be explicit. The tasks that fall under unsupervised
    learning typically include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering – that is, identifying groups within a given dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction – that is, attempting to represent the dataset with
    a smaller amount of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Novelty detection – that is, trying to identify when a change has occurred in
    the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning** teaches a model to find the optimal solution for
    a problem by leveraging what the model already knows and what it can learn via
    a cumulative reward after interacting with its environment. The model receives
    feedback from the environment in the form of rewards or penalties, and its goal
    is to maximize its total reward. The idea behind reinforcement learning is to
    mimic the way humans learn by trial and error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Agent-environment loop (adapted from https://gymnasium.farama.org/content/basic_usage/)](img/B19446_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Agent-environment loop (adapted from [https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/))
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a project come to life, there are some initial business/data steps
    that must be undertaken:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Defining a Web3 data science problem* means stating what we want to solve
    with the data we have with precision. In such a definition, we have to be able
    to describe the problem we want to solve, why we want to solve it, and what assumptions
    are considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting the data* means getting our hands on the dataset we will work with.
    It is possible that the dataset has already been built with all the rows and columns
    of interest, or that we have to build it by combining multiple sources of data.
    An initial list of data sources is listed in *Chapters 2* and *3*. More data sources
    may be needed, depending on the problem we will tackle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*EDA* is used to make sense of the dataset using summary statistics and data
    visualization techniques. *Data preparation* is a preprocessing step where we
    transform the dataset to improve its quality or make it digestible to the model.
    On-chain data may need a lot of transformations. We analyzed some of those methods
    in [*Chapter 6*](B19446_06.xhtml#_idTextAnchor210).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s analyze the steps to select, train, and evaluate a model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After cleaning the data and selecting the most important features, the machine
    learning flow can be summarized into steps, as shown in *Figure 7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Machine learning pipeline](img/B19446_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Machine learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'To carry out this process, we must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a model and its initial parameters based on the problem and available
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train: First, we must split the data into a training set and a test set. The
    process of training consists of making the model learn from the data. Each model’s
    training process can vary in time and computational consumption. To improve the
    model’s performance, we must employ hyperparameter tuning through techniques such
    as grid search or random grid search.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Predict and evaluate: The trained model is then used to predict over the test
    set, which contains rows of data that have not been seen by the algorithm. If
    we evaluate the model with the data that we used to train it, the model will always
    predict well, and we will not be able to improve it. Model performance is assessed
    using task-specific evaluation metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we achieve a good model, we must save it so that we can use it when we
    receive unseen data. We can use tools such as *Pickle* and *Keras Tokenizer* to
    accomplish this. Pickle serializes the trained model and converts it into a file,
    allowing it to be used in another environment. To produce a result, we must pass
    data with the same structure that it is ready to receive so that the model can
    make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply this pipeline with a hands-on example. In `Chapter07/ML_warmup`,
    we aim to identify fraudulent transactions on the Ethereum network using a Kaggle
    dataset named *Ethereum Fraud Detection Dataset*, where only 17% of its rows are
    fraudulent. This is a typical supervised classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the problem at hand, we must select a model or a couple of models
    to test which one performs better on our data. If we are unsure about the model
    to select, we can examine similar structured problems solved on Kaggle. In the
    Jupyter notebook, we selected a random forest classifier with the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Many algorithms are available for training and the choice can be difficult.
    One way to choose among many models is to reduce the reducible error. Literature
    usually refers to this matter as the bias-variance trade-off. Before addressing
    that trade-off, we need to understand the different types of errors that exist.
    The prediction error for any machine learning algorithm can be classified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise** or **irreducible error**: This type of error cannot be deleted, no
    matter how well we implement the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias error**: This can be reduced. Wikipedia defines it as “*an error from
    erroneous assumptions in the learning algorithm*.” A model with high bias oversimplifies
    reality and leads to a high error between the prediction and the ground-truth
    value. High-bias models oversimplify, which means that they do not have enough
    parameters to capture the complexity of the data they learn from, resulting in
    underfitting. More on this concept will be covered in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance error**: This can also be reduced. Wikipedia defines it as an error
    derived from “*sensitivity to small fluctuations in the training set*.” This means
    that the model is learning the particularities of the training dataset so well
    that it will not generalize enough to predict on unseen data. These models are
    highly dependent on the exact training data and are unable to generalize. We encounter
    this error when the model performs well on training data but poorly on test/validation
    data, indicating an overfitting problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low variance with high bias algorithms train less complex models with a rather
    simple or rigid underlying structure – for example, linear regression. On the
    other hand, high variance with low bias algorithms train complex, flexible models
    that can be exact on training data but inconsistent in prediction – for example,
    KNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we understand bias and variance and recognize that both are derived from
    the choice of the model we make, to make an optimal decision we will have to choose
    the model that reduces the total error with a trade-off between both:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The bias-variance trade-off](img/B19446_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The bias-variance trade-off
  prefs: []
  type: TYPE_NORMAL
- en: Another criterion for selecting a model is its performance, which is measured
    by the evaluation metric of choice. We can run multiple models and evaluate them
    all with the same metric, and the model that performs better is the one we continue
    tuning. We will discuss evaluation metrics in subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: In `Chapter07/ML_warmup`, we selected a random forest classifier. This algorithm
    looks to reduce the variance of the model without compromising bias and performs
    well in the evaluation metric known as recall. More about the random forest algorithm
    can be found in the *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin the training process, we split the data into a training dataset and
    a test dataset. This allows us to keep part of the data unseen by the model during
    training, and we can evaluate its performance afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The training process consists of passing the features and labels to the algorithm
    so that it learns from them. The learning algorithm will try to find patterns
    in the training data that map the attributes of the input data to the target.
    The trained model captures these patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Chapter07/ML_warmup`, we instruct the model to learn with the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Underfitting and overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider three scenarios, where the model is represented by a black line.
    Which scenario performs classification better?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Three classification scenarios](img/B19446_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Three classification scenarios
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario A**: The model is very simple and unable to capture the boundary
    between the two classes. This is called **underfitting**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario B**: The model was able to find an acceptable boundary between both
    classes, although it may misclassify some of the border samples. In general, it
    captures the complexity of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario C**: The model adapted too much to the training dataset and learned
    all the details, not just the relevant characteristics that differentiate one
    class from another. It was unable to generalize. This is called **overfitting**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.7 – What a model does when overfitting (source: https://twitter.com/MaartenvSmeden/status/1522230905468862464)](img/B19446_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7 – What a model does when overfitting (source: https://twitter.com/MaartenvSmeden/status/1522230905468862464)'
  prefs: []
  type: TYPE_NORMAL
- en: We aim for scenario B, where the model is complex enough to capture the important
    features but does not adapt too much to the training data so that it performs
    well on unseen samples.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we pass unseen data to our trained model and evaluate how accurate its
    predictions are compared to the ground truth. If the result is acceptable, we
    keep the model; otherwise, we tune hyperparameters and train again. *A hyperparameter
    is a variable that is set before the training process and cannot be changed during
    learning. Parameters are those that are fine-tuned* *during training.*
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Jupyter notebook, we use the following code snippet to predict and evaluate
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate whether the result is acceptable and we can keep the model, we
    can use the confusion matrix for a binary classification task. The resulting confusion
    matrix for the dataset we analyzed in the Jupyter notebook is shown in *Figure
    7**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Confusion matrix](img/B19446_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the components of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True negative** (**TN**): The model predicted negative, and it is true. These
    transactions are not fraudulent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True positive** (**TP**): The model predicted positive, and it is true. These
    transactions are fraudulent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative** (**FN**): The model failed to predict and they were fraudulent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive** (**FP**): The model flagged these transactions as fraudulent,
    but they were not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these numbers, we can calculate precision and recall. **Precision**
    answers the question, of all the classes we predicted as positive, how many were
    actually positive?
  prefs: []
  type: TYPE_NORMAL
- en: TP _ TP + FP
  prefs: []
  type: TYPE_NORMAL
- en: Our precision is `0.91`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** answers the question, of all the fraudulent classes, how many did
    our model predict correctly? The formula for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TP _ TP + FN
  prefs: []
  type: TYPE_NORMAL
- en: Our recall is `0.98`.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation of the results depends on the problem at hand. Selecting the
    metrics correctly is very important as it will impact the subsequent decisions
    we make. In `Chapter07/ML_warmup`, we are working to find fraudulent transactions,
    so we value models that result in higher recall than precision. We prefer recall
    because the cost of missing a fraudulent transaction is much higher than flagging
    a potentially harmless transaction. However, the number of FP flags cannot be
    enormous because of the cost of the transaction and its impact on the client.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world datasets are mostly imbalanced, which means that the classes are
    not equally represented. It is our job to apply techniques that enable the model
    to learn about the existence and characteristics of both classes, particularly
    when the less frequent class is the one that we are trying to detect.
  prefs: []
  type: TYPE_NORMAL
- en: A note on balanced and imbalanced datasets
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**, as the percentage of correct predictions, is another commonly
    used evaluation metric. However, it will not yield good results if the dataset
    is not balanced. If we take accuracy as an evaluation metric in an imbalanced
    dataset, the model only needs to identify the majority class to return a good
    result, and that does not guarantee that this is a good model.'
  prefs: []
  type: TYPE_NORMAL
- en: In our EDA, we will examine the proportion of each class and determine whether
    we are dealing with a balanced or imbalanced dataset. For example, in `Chapter07/ML_warmup`,
    we know that the proportion of fraudulent stances is 17%.
  prefs: []
  type: TYPE_NORMAL
- en: We can solve this by using oversampling or undersampling techniques in the feature
    engineering preprocessing step. This must be done with caution as it may alter
    the underlying relationships in our data or remove some critical information.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use algorithms that have already been optimized for imbalanced datasets
    and allow the user to add that information to the training process – for example,
    by using the `class_weight` parameter in the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can optimize the split by considering the unequal representation
    of the classes by using `stratify` in `train_test_split`.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Part 2* of this book, we will also use deep learning methodologies when
    solving the use cases. Deep learning models employ multiple layers of interconnected
    nodes called neurons, which process input data and produce outputs based on learned
    weights and activation functions. The connections between neurons facilitate information
    flow, and the architecture of the network determines how information is processed
    and transformed.
  prefs: []
  type: TYPE_NORMAL
- en: We will study three types of neural network architectures in detail in their
    corresponding chapters. For now, let’s introduce the framework and terminology
    that we will use in them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neuron serves as the fundamental building block of the system and can be
    defined as a node with one or more input values, weights, and output values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – A neuron’s structure](img/B19446_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – A neuron’s structure
  prefs: []
  type: TYPE_NORMAL
- en: 'When we stack multiple layers with this structure, it becomes a neural network.
    This architecture typically consists of an input layer, hidden layers, and an
    output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Neural network structure](img/B19446_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Neural network structure
  prefs: []
  type: TYPE_NORMAL
- en: The input layer initiates the network and passes data to the hidden layers,
    which perform calculations on the features and patterns. The more hidden layers
    there are, the more complex calculations are executed.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer receives the processed information from the hidden layers and
    provides a result or output summarizing the information that’s been processed
    within the network.
  prefs: []
  type: TYPE_NORMAL
- en: The connections between nodes contain weights that carry information on how
    to solve a specific problem. During model training, we calibrate these weights
    to adapt the model to our dataset. The weights represent the learnable parameters
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This flexible structure allows users to tune numerous hyperparameters to enhance
    the model’s performance. The fundamentals are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate**: This hyperparameter controls how much a model changes in
    response to weight updates. Finding the correct value is crucial as a very small
    learning rate may result in a lengthy training process, while a higher one can
    lead to sub-optimal weight sets and altered results. The learning rate is closely
    related to the optimizers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation functions**: These functions determine whether a neuron should
    be activated or not, meaning they decide whether the neuron’s input to the network
    is important for the prediction process using simple mathematical operations.
    The activation function derives output from a set of input values fed into each
    layer. A list of activation functions in Keras can be found at [https://keras.io/api/layers/activations/](https://keras.io/api/layers/activations/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost functions**: These functions quantify the error between the predicted
    and expected values, summarizing the model’s performance in a single value to
    be minimized during training. The choice of the cost function depends on the problem
    being solved, with common examples being mean squared error for regression tasks
    and categorical cross-entropy for classification tasks. Keras lists the various
    losses at [https://keras.io/api/losses/](https://keras.io/api/losses/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizers**: These algorithms help improve model performance by adjusting
    the attributes of the neural network. Its responsibility in the architecture is
    to change the learning rate and weights of the neurons to reach the minimum of
    the loss function. The supported optimizers in Keras are listed here: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: This denotes the number of times the algorithm runs through the
    entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: This refers to the number of samples considered to update the
    model parameters. A batch size of *N* means that *N* samples from the training
    dataset will be used to update the model parameters. Keep in mind that these samples
    are held in memory, so a higher batch size requires more memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the models we use, will be analyzed within the Keras framework, which has
    excellent documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Model preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `Chapter07/DL_warmup`, we will work with the same dataset as in the previous
    section – *Ethereum Fraud Detection Dataset*. This time, we will select fewer
    columns and standardize the numbers using `RobustScaler()` from sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: As in all prediction problems, we want to separate the test and training datasets
    with `train test` `split ()`.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll create a sequential model with Keras. The structure of sequential models
    consists of a stack of the same or different layers, where the output of one layer
    goes into the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet sets the input layer to expect rows of data with
    the number of columns of the dataset. In this case, we are only working with 11
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We add three hidden layers, each with a decreasing number of nodes. All of
    them use `relu` as the activation function. The `Dense` layer is a fully connected
    layer and is one of the many types of layers, such as `Convolutional` or `LSTM`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this is a binary classification task, in the last layer, we will use
    the `sigmoid` activation function and `1` in the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before training the model, it needs to be compiled with the optimizer, the
    loss functions, and the metrics. The compiler configures the learning process.
    It’s worth mentioning that as this is an imbalanced dataset, we are interested
    in precision and recall, so we must build the metric by leveraging the `keras`
    library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we must add it to the compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Training and evaluating a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model has been built, we have to feed it our dataset, which means we
    have to train it. This is done with `fit(),` and in this case, we decided to do
    it for 90 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Once training has been performed, it is necessary to evaluate the model by predicting
    data that has not been part of the training. We can do this with `X_test` and
    `y_test`.
  prefs: []
  type: TYPE_NORMAL
- en: The classification report shows that recall for the minority class is 95%, which
    is very good. With more data preprocessing and by applying techniques for imbalanced
    datasets and hyperparameter tuning, we could further improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular exercise, one of the Zen of Python principles applies perfectly.
    *Simple is better than complex* – a simpler machine learning model performed better
    than a complex neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have explored both methodologies, we will highlight additional
    characteristics of each field:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Machine learning** | **Deep learning** |'
  prefs: []
  type: TYPE_TB
- en: '| Can train and make inferences from smaller datasets | Requires large amounts
    of data |'
  prefs: []
  type: TYPE_TB
- en: '| Shorter training and can be done with a CPU | Longer training and needs a
    GPU to train effectively |'
  prefs: []
  type: TYPE_TB
- en: '| Makes simple correlations | Makes non-linear complex correlations |'
  prefs: []
  type: TYPE_TB
- en: '| Mostly explainable | Opaque model, complex to explain |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Differences between machine learning and deep learning
  prefs: []
  type: TYPE_NORMAL
- en: A note on the ethical and social impact of artificial intelligence
  prefs: []
  type: TYPE_NORMAL
- en: 'Discussions regarding ethics and social impact may appear distant from our
    daily work, but given that our projects typically unfold within a business environment,
    it is advisable to consider their broader implications. The ethical and social
    ramifications of machine learning and deep learning encompass diverse dimensions,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias**: Similar to the bias error, which oversimplifies its learning outcome,
    machine learning models can inherit biases present in training data, potentially
    leading to discriminatory outcomes. Bias can be introduced at various stages of
    the machine learning life cycle, from data collection to model deployment. It
    is important to obtain unbiased data to train our models and regularly audit them
    to detect and rectify bias.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparency**: The opacity of complex machine learning models poses challenges
    for regulators and may undermine user trust. Many DeFi ventures are actively seeking
    regulatory approval to facilitate the flow of funds from traditional banking systems
    into the DeFi world. Given the highly regulated nature of the finance sector,
    data scientists working in this domain must make efforts to enhance model interpretability
    and provide explanations for their decisions to regulatory authorities.'
  prefs: []
  type: TYPE_NORMAL
- en: Addressing these ethical considerations necessitates a multidisciplinary approach
    that involves technology developers, policymakers, ethicists, and more. As professionals
    working with models, we need to keep these challenges in mind, especially when
    selecting datasets, preprocessing them, or evaluating their results in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the fundamental concepts of artificial intelligence,
    which will serve as the foundation for our journey in *Part 2* of this book. We
    explored various types of tasks, including supervised learning, unsupervised learning,
    and reinforcement learning. Through a hands-on example, we gained insights into
    the typical machine learning process, which encompasses model selection, training,
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we acquired essential knowledge related to common challenges
    in machine learning, such as striking the right balance between underfitting and
    overfitting models, the existence of imbalanced datasets, and which metrics are
    relevant to evaluate models that are trained with them. Understanding these concepts
    is vital for any successful machine learning project.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we progressed into the basics of deep learning, where we explored
    the key components of a neural network using Keras. Additionally, we implemented
    a pipeline to tackle a supervised problem to see all the concepts duly applied.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss an important topic, sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definitions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Igual, L. and Seguí, S. (2017). *Introduction to data science*: *A python approach
    to concepts, techniques and* *applications*. Springer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ertel, W. (2018). *Introduction to artificial* *intelligence*. Springer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skansi, S. (2018). *Introduction to deep learning: From logical calculus to
    artificial* *intelligence*. Springer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ian Goodfellow, Yoshua Bengio, and Aaron Courville. (2016). *Deep Learning*.
    Available at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet, F. (2017). *Deep Learning with Python*. Manning Publications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Müller, A. C. and Guido, S. (2016). *Introduction to Machine Learning with
    Python: A guide for data scientists*. O’Reilly Media.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: VanderPlas, J. (n.d.). *What Is Machine Learning?* Pythonic Perambulations.
    Available at [https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml](https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.xhtml).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is Deep* *Learning?*: [https://machinelearningmastery.com/what-is-deep-learning/](https://machinelearningmastery.com/what-is-deep-learning/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mining addresses from websites: Furneaux, Nick. *Investigating Cryptocurrencies,*
    [*Chapter 9*](B19446_09.xhtml#_idTextAnchor269). Understanding, Extracting, and
    Analyzing Blockchain Evidence, Wiley, 2018\. Page 125.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'James, G., Witten, D., Hastie, T., and Tibshirani, R. (2022). *An Introduction
    to Statistical Learning: With Applications*. In R. Springer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gymnasium documentation: [https://gymnasium.farama.org/](https://gymnasium.farama.org/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction – Spinning up documentation*. (n.d.). Welcome to Spinning Up
    in Deep RL! Spinning Up documentation. Available at [https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is](https://spinningup.openai.com/en/latest/user/introduction.xhtml#what-this-is).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nansen Wallet Labels and Emojis: What Do They Mean?* (2023, March 14). Nansen
    – Crypto, DeFi and NFT Analytics. Available at [https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels](https://www.nansen.ai/guides/wallet-labels-emojis-what-do-they-mean#alpha-labels).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipelines:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EliteDataScience. (2022, July 8). *WTF is the Bias-Variance Tradeoff?* (Infographic).
    Available at [https://elitedatascience.com/bias-variance-tradeoff](https://elitedatascience.com/bias-variance-tradeoff).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sklearn.ensemble.RandomForestClassifier*. (n.d.). scikit-learn. Retrieved
    March 14, 2023, from [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SMOTE oversampling*. (n.d.). Machine Learning Mastery. Available at [https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nyuytiymbiy, K. (2022, March 28). *Parameters and Hyperparameters in Machine
    Learning and Deep Learning*. Medium. Available at [https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heatmap on `Chapter07/ML_warmup`: T, D. (2019, July 25). *Confusion matrix
    visualization*. Medium. Available at [https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea](mailto:https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutorial to use a Kaggle dataset on Colaboratory. Useful to follow along with
    `Chapter07/ML_warmup`: Gupta, K. (2022, August 24). *How to Load Kaggle Datasets
    into Google Colab?* Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pramoditha, R. (2022, January 26). *How to Choose the Right Activation Function
    for Neural Networks*. Medium. Available at [https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c](https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta, A. (2022, May 24). *A Comprehensive Guide on Deep Learning Optimizers*.
    Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PEP 20 – The Zen of Python*. (2022, March 15). PEP 0 – Index of Python Enhancement
    Proposals (PEPs) | peps.python.org. Available at [https://peps.python.org/pep-0020/](https://peps.python.org/pep-0020/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras Team. (2020, April 17). *Keras documentation: Imbalanced classification:
    credit card fraud detection*. Keras: Deep Learning for Humans. Available at [https://keras.io/examples/structured_data/imbalanced_classification/](https://keras.io/examples/structured_data/imbalanced_classification/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramchandani, P. (2021, April 10). *Random Forests and the Bias-Variance Tradeoff*.
    Medium. Available at [https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4](https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning with Bayesian optimization: Rendyk. (2023, August 17). *Tuning the Hyperparameters
    and layers of neural network deep learning*. Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/](https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How to Grid Search Hyperparameters for Deep Learning Models in Python with
    Keras*. (2022, August). Machine Learning Mastery. Available at [https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
