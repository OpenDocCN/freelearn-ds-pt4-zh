<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer119">
<h1 class="chapter-num er" id="_idParaDest-149"><a id="_idTextAnchor250"/>8</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor251"/>Sentiment Analysis – NLP and Crypto News</h1>
<p><strong class="old">Natural language processing</strong> (<strong class="old">NLP</strong>) falls <a id="_idIndexMarker457"/>under the umbrella of artificial intelligence and is concerned with the comprehension of text by computers. Recent advancements in this field, exemplified by the emergence of tools such as ChatGPT, have become an integral part of our daily lives. Yet, the financial sector has been leveraging NLP for quite some time, particularly for <span class="No-Break">fundamental analysis.</span></p>
<p>Fundamental analysis seeks to ascertain the intrinsic value of assets such as stocks, tokens, or NFT art based on publicly accessible information. In traditional finance, textual data is sourced from periodic submissions to the SEC (such as Form 10K or 10Q), including financial statements, specialized media news, social media platforms such as X (formerly Twitter), and other avenues. Web3 has fostered a similar environment where market activities are continuous, and X and news platforms serve as predominant textual resources. It’s worth noting that while most Web3 companies may not yet be obligated to file regulatory reports, it’s probable that such data sources will eventually become available for <span class="No-Break">most companies.</span></p>
<p>NLP in the financial sector encompasses various applications, including <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="old">Sentiment analysis</strong>: Determining<a id="_idIndexMarker458"/> the positivity, negativity, or neutrality of text, which could be news articles, social media posts (tweets, Reddit, and so on), and more. These algorithms can also provide insights into polarity and subjectivity, aiding in assessing sentiments toward companies, industries, markets, government decisions, crypto developments, <span class="No-Break">and more.</span></li>
<li><strong class="old">Topic modeling</strong>: This <a id="_idIndexMarker459"/>helps classify and organize large volumes of financial documents based on the underlying topics they cover. This aids in efficiently managing and accessing <span class="No-Break">relevant information.</span></li>
<li><strong class="old">Summarization</strong>: In <a id="_idIndexMarker460"/>a world where content is being created at a non-stop rate, there simply is not enough time and/or resources to analyze and give hierarchy to all of it. NLP techniques are being applied to collect and create short summaries of documents that are easy to process <span class="No-Break">by analysts.</span></li>
<li><strong class="old">Fraud detection</strong>: Scrutinizing emails, chats, financial documents, transcribed conversations, and<a id="_idIndexMarker461"/> more using NLP techniques to uncover patterns that are potentially indicative of <span class="No-Break">fraudulent activities.</span></li>
<li><strong class="old">Trading</strong>: Incorporating <a id="_idIndexMarker462"/>NLP tools into trading strategies to signal or predict market trends, or enhance the decision-making process of fundamental <span class="No-Break">analysis traders.</span></li>
</ul>
<p>The data source for NLP techniques is text that is categorized as unstructured. We are surrounded by it, with more being produced every second. We explored some text sources in <a href="B19446_03.xhtml#_idTextAnchor114"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>; we will explore some others in <span class="No-Break">this chap<a id="_idTextAnchor252"/>ter.</span></p>
<p>In this chapter, we will analyze the sentiment of the news sourced from Crypto Panic. To do so, we will build a <strong class="old">neural network</strong> (<strong class="old">NN</strong>) and <a id="_idIndexMarker463"/>explain the concept and use of pre-trained embeddings. A crypto news dataset and a traditional finance news dataset will be employed to train our model. Additionally, we’ll learn the essentials of how to pre-process text for NN utilization and how to evaluate the results of such <span class="No-Break">a model.</span></p>
<p>At the time of writing, ChatGPT is a tangible reality. Publicly accessible information reveals its training on an extensive multilingual corpus, utilizing reinforcement learning to progressively enhance its performance. We’ll learn how to incorporate ChatGPT for sentiment analysis on the Crypto Panic dataset. This can be useful to implement as a ready-to-use tool while we build a specialized corpus to train <span class="No-Break">our models.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>A deep learning pipeline for sentiment analysis, encompassing preparation, model construction, training, and <span class="No-Break">evaluation phases.</span></li>
<li>Integrating ChatGPT for <span class="No-Break">sentiment analysis</span></li>
</ul>
<h1 id="_idParaDest-151">Technical requir<a id="_idTextAnchor253"/>ements</h1>
<p>In this chapter,<a id="_idTextAnchor254"/> we’ll utilize tools from the libraries that were introduced in <a href="B19446_07.xhtml#_idTextAnchor228"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> – that is, scikit-learn and Keras. Additionally, we will <a id="_idIndexMarker464"/>employ <strong class="old">NLTK</strong>, a Python library that proves valuable for working with human language data. NLTK includes a range of modules and functions that empower us to execute tasks such as tokenization, stemming, and part-of-speech tagging on our selected databases. This library streamlines the process of processing extensive text datasets so that they’re ready to be integrated with machine learning or deep <span class="No-Break">learning models.</span></p>
<p>If you have not worked with NLTK before, it can be installed with the <span class="No-Break">following code:</span></p>
<pre class="console">
pip install nltk</pre> <p>The documentation for <strong class="source-inline">nltk</strong> can be <a id="_idIndexMarker465"/>found at https://www.nltk.org. Another essential library when handling text manipulation and cleaning is <strong class="old">re</strong>, short for <strong class="old">Regular Expression</strong>. A regular expression<a id="_idIndexMarker466"/> is a sequence of characters that defines a search pattern. Here’s <span class="No-Break">an example:</span></p>
<table class="T---Table _idGenTablePara-1" id="table001-8">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="old">Pattern</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="old">Search Criteria</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>[<span class="No-Break">a-z]</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>Any single character in the <span class="No-Break">range a-z</span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>[<span class="No-Break">0-9A-Fa-f]</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>Match any <span class="No-Break">hexadecimal digit</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Example of a “re” pattern</p>
<p><a id="_idTextAnchor255"/>The <strong class="source-inline">re</strong> library provides functions and methods to employ the aforementioned patterns. For instance, <strong class="source-inline">re.sub</strong> replaces all characters that match the pattern with a specified string. A comprehensive list of functions is available <span class="No-Break">at </span><span class="No-Break">https://docs.python.org/3/library/re.xhtml#module-re</span><span class="No-Break">.</span></p>
<p>Throughout our work, we will utilize Google’s Colaboratory platform, which already includes the core library imports. However, for specific tasks, additional imports will <span class="No-Break">be required.</span></p>
<p>You can find all the data and code files for this chapter in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08">https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08</a>. We recommend that you read through the code files in the <strong class="source-inline">Chapter08</strong> folder to <span class="No-Break">follow along.</span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor256"/>Example datasets</h2>
<p>We will merge two headline datasets in this chapter; you can find links to the relevant sources in the <em class="italic">Further reading</em> section and the code. The datasets are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="old">Financial Phrase Bank</strong>: This <a id="_idIndexMarker467"/>dataset is also available on Kaggle and contains a headline accompanied by sentiment analysis labels from the viewpoint of a retail investor. It comprises multiple datasets, each of which categorizes sentences based on the level of consensus regarding the sentiment of the phrase. For this exercise, we will utilize the <span class="No-Break"><em class="italic">Sentence_AllAgree</em></span><span class="No-Break"> dataset.</span></li>
<li><strong class="old">CryptoGDELT2022</strong>: Presented<a id="_idIndexMarker468"/> in the paper <em class="italic">Cryptocurrency Curated News Event Database From GDELT</em>, this dataset comprises news events extracted from the <strong class="old">Global Database of Events, Language, and Tone</strong> (<strong class="old">GDELT</strong>). It <a id="_idIndexMarker469"/>covers news events between March 31, 2021, and April 30, 2022. The dataset includes various sentiment scores and manual labeling methods. In this exercise, we will exclusively employ <span class="No-Break">manual labeling.</span></li>
</ul>
<p>Let’s get started with building <span class="No-Break">our pipeline.</span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor257"/>Building our pipeline</h1>
<p>In an NLP pipeline, preparation <a id="_idIndexMarker470"/>generally encompasses a pre-processing step where we clean and normalize the data. Following that, a feature representation step translates the language into input that can be consumed by our chosen models. Once this is completed, we are ready to build, train, and evaluate the model. This strategic plan will be implemented throughout the <span class="No-Break">subsequent sections.</span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor258"/>Preparation</h2>
<p>Language <a id="_idIndexMarker471"/>manifests in numerous variations. There are formatting nuances, such as capitalization or punctuation; words that serve as linguistic aids without true semantic meaning, such as prepositions; and special characters, including emojis, further enrich the landscape. To work with this data, we must transform raw text into a dataset while following a similar criterion as numeric datasets. This cleaning process enables us to eliminate outliers, reduce noise, manage vocabulary size, and optimize data for ingestion by <span class="No-Break">NLP models.</span></p>
<p>A <a id="_idIndexMarker472"/>basic flow diagram of the data cleaning pipeline can be seen in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 8.1 – Cleaning diagram" height="402" src="image/B19446_08_01.jpg" width="1599"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Cleaning diagram</p>
<p>Let’s delve into some of <span class="No-Break">these steps.</span></p>
<h3>Normalization</h3>
<p>Normalization <a id="_idIndexMarker473"/>encompasses a series of tasks, including lowercasing and removing HTML traces, links, and emojis. Our objective is to clean our database so that only <span class="No-Break">words remain.</span></p>
<p>Furthermore, this process involves eliminating words that lack informativeness for our model or that could potentially introduce bias, leading to suboptimal predictions. Depending on the task, we will select words that we may have to delete. This step also addresses the process of removing words that may have escaped the <em class="italic">stop words</em> <span class="No-Break">cleaning process.</span></p>
<p>We can also convert Unidecode text into ASCII text. Unidecode accommodates characters <a id="_idIndexMarker474"/>from diverse languages, allowing them to be translated into their nearest ASCII counterpart. For instance, the Spanish character “ñ” becomes “n” in ASCII. We implement this transformation in <strong class="source-inline">Database_and_Preprocessing.ipynb</strong> using the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
text = unidecode.unidecode(text)</pre> <p>Normalization fosters uniformity by rendering all text in a consistent format, directing the model’s focus toward content rather than <span class="No-Break">superficial differ<a id="_idTextAnchor259"/>ences.</span></p>
<h3>Stop words</h3>
<p>Our objective<a id="_idIndexMarker475"/> here is to exclude words that contribute minimal semantic value or meaning to our model. Frequently employed words such as articles (“a,” “the,” “an”), prepositions (“on,” “at,” “from,” “to”), conjunctions (“and,” “so,” “although”), and pronouns (“she,” “he,” “it”) play functional roles in language but lack substantial semantic content that the model <span class="No-Break">can leverage.</span></p>
<p>Consequently, this collection of words is generally filtered out during preprocessing. The stop words collection can be downloaded by language and directly applied to our cleaning process using NLTK. In <strong class="source-inline">Database_and_Preprocessing.ipynb</strong>, we downloaded the English stop words with the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))</pre> <p>This process reduces the noise in the data and helps improve the efficiency of <span class="No-Break">the model.</span></p>
<h3>Tokenization</h3>
<p>Tokenization<a id="_idIndexMarker476"/> involves splitting the text within our database into smaller units of meaning referred to as tokens. These units can take the form of sentences or words. For instance, let’s take a look at the <span class="No-Break">following headline:</span></p>
<p><em class="italic">“SEC investigating Coinbase for its Earn product, wallet service, and </em><span class="No-Break"><em class="italic">exchange activity”</em></span></p>
<p>When tokenized into words, this yields the <span class="No-Break">following output:</span></p>
<p><strong class="source-inline">['SEC', 'investigating', 'Coinbase', 'Earn', 'product', ',', 'wallet', 'service', '</strong><span class="No-Break"><strong class="source-inline">exchange', 'activity']</strong></span></p>
<p>Tokenization <a id="_idIndexMarker477"/>results in a structured input that the NLP model can process effectively, facilitating data analysis. Such analysis guides decisions on vocabulary size for dimensionality reduction, a demonstrated example of which can be found in <strong class="source-inline">Database_and_Preprocessing.ipynb</strong>. This showcases the top recurring words in the analyzed dataset. Such analysis gives the <span class="No-Break">following result:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="Figure 8.2 – Top recurring words in analyzed headlines" height="524" src="image/B19446_08_02.jpg" width="902"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Top recurring words in analyzed h<a id="_idTextAnchor260"/>eadlines</p>
<h3>Lemmatization and part-of-speech (POS) tagging</h3>
<p>These<a id="_idIndexMarker478"/> techniques reduce words to their base or root form. It allows us to reduce the diversity of words we have to process by selecting those words that are conjugated and replacing them with their root word. In our example sentence (“SEC investigating Coinbase for its Earn product, wallet service, and exchange activity”), the word <em class="italic">investigating</em> will change <span class="No-Break">to </span><span class="No-Break"><em class="italic">investigate</em></span><span class="No-Break">.</span></p>
<p>The accuracy of lemmatization hinges on the library’s comprehension of the word’s context or function within the sentence. POS tagging contributes this contextual information to our analysis. Here’s <span class="No-Break">an example:</span></p>
<pre class="console">
nltk.pos_tag</pre> <p>POS tagging<a id="_idIndexMarker479"/> helps us programmatically assign a context to each word, depending on its position in the sentence or document. Here’s <span class="No-Break">an example:</span></p>
<pre class="console">
word= ['SEC', 'investigating', 'Coinbase', 'Earn', \
    'product', ',', 'wallet', 'service', 'exchange',<strong class="source-inline"> </strong>'activity']
[nltk.pos_tag([w]) for w in word]
[[('SEC', 'NNP')], [('investigating', 'VBG')], [('Coinbase', 'NN')], [('Earn', 'NN')], [('product', 'NN')], [(',', ',')], [('wallet', 'NN')], [('service', 'NN')], [('exchange', 'NN')], [('activity', 'NN')]]</pre> <p>The result of cleaning the sentence looks <span class="No-Break">like this:</span></p>
<pre class="console">
investigate coinbase earn product wallet service exchange activity</pre> <p>Let’s look at some additional <span class="No-Break">preprocessing techniques:</span></p>
<ul>
<li><strong class="old">Stemming</strong>: This<a id="_idIndexMarker480"/> involves removing prefixes and suffixes from <a id="_idIndexMarker481"/>words to derive a common base form, known as the “stem.” The resulting stem may not always form a valid word, but it aims to capture the <span class="No-Break">core meaning.</span></li>
<li><strong class="old">Named entity recognition</strong> (<strong class="old">NER</strong>): This technique <a id="_idIndexMarker482"/>automatically identifies and <a id="_idIndexMarker483"/>classifies named entities (for example, names of people, places, organizations, and dates) in text. NER extracts structured information from unstructured text, categorizing entities into predefined classes. An example of this approach is offered by the X (formerly Twitter) dataset, which we covered in <a href="B19446_03.xhtml#_idTextAnchor114"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></li>
<li><strong class="old">Dependency parsing</strong>: This<a id="_idIndexMarker484"/> technique <a id="_idIndexMarker485"/>analyzes a sentence’s grammatical structure to establish relationships between words. It creates a hierarchical structure where each word is linked to its governing word (the “head”) and assigned a <a id="_idIndexMarker486"/>grammatical role (the “<span class="No-Break">dependency label”).</span></li>
</ul>
<p class="callout-heading">Checkpoint</p>
<p class="callout">A step-by-step version of this pipeline is detailed in <strong class="source-inline">Database_and_Preprocessing.ipynb</strong>. If you want to skip this section, the resulting <strong class="source-inline">.csv</strong> file has been uploaded to this book's GitHub and is accessible <span class="No-Break">at <a id="_idTextAnchor261"/></span><span class="No-Break"><strong class="source-inline">preprocessed.csv</strong></span><span class="No-Break">.</span></p>
<h3>Feature representation</h3>
<p>Following <a id="_idIndexMarker487"/>the preprocessing phase, the next step involves transforming the resultant raw text data into features that the model can utilize for statistical inference. The goal is to extract relevant information from the text and encode it in a way that algorithms can understand. There are multiple ways to achieve this, but they generally involve representing words as vectors and measuring the frequency of words in a document. Some<a id="_idIndexMarker488"/> common techniques are bag of words, <strong class="old">term frequency-inverse document frequency</strong> (<strong class="old">TF-IDF</strong>), and word embeddings. Let’s briefly <span class="No-Break">describe them.</span></p>
<h3>Bag of words</h3>
<p>This<a id="_idIndexMarker489"/> technique builds a vector that represents all the words in the document by the number of times that word appears. It ignores the order of the sentence or the context of the word. Its implementation can be accomplished using <strong class="source-inline">sklearn.feature_extraction.text.CountVectorizer</strong> from the scikit-learn library. This approach is basic and loses important contextual information, and it may also create a very sparse matrix because the vocabulary <span class="No-Break">is vast:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/crypto-credit-card/" height="605" src="image/B19446_08_03.jpg" width="1210"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/<a id="_idTextAnchor262"/>crypto-credit-card/</p>
<p>Moreover, this approach can be enhanced by incorporating n-grams, which involve concatenating two or <a id="_idIndexMarker490"/>more words that hold contextual meaning together. For example, “natural language”, “machine learning,” and “press release” encapsulate specific concepts when combined, but in isolation, they do not retain the concept. Incorporating n-grams into bag of words can expand the <span class="No-Break">vocabulary further.</span></p>
<h3>TF-IDF</h3>
<p>This is an<a id="_idIndexMarker491"/> alternate technique where <strong class="old">TF</strong> signifies <strong class="old">term frequency,</strong> meaning that this method also counts the number of occurrences of a word in a text. <strong class="old">IDF,</strong> or <strong class="old">inverse document frequency </strong>implies that it also downscales those words that appear with high frequency. This way, we can score words that are distinct in a given document and therefore are considered informative. This approach can be achieved <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">sklearn.feature_extraction.text.TfidfVectorizer</strong></span><span class="No-Break">.</span></p>
<h3>Word embeddings</h3>
<p>Word embeddings <a id="_idIndexMarker492"/>represent words as dense vectors within a continuous space. This approach retains information about context and semantic meaning by capturing relationships between words. <strong class="old">Word2Vec</strong> and <strong class="old">GloVe</strong> (<a href="https://www.tensorflow.org/tutorials/text/word2vec">https://www.tensorflow.org/tutorials/text/word2vec</a>) are popular algorithms that generate word embeddings. These embeddings can either be pre-trained on large text corpora or fine-tun<a id="_idTextAnchor263"/>ed for specific tasks. In our model, we employ Glove (<a href="https://nlp.stanford.edu/projects/glove"><span class="No-Break">https://nlp.stanford.edu/projects/glove</span></a><span class="No-Break">) vectors.</span></p>
<p>Glove, in<a id="_idIndexMarker493"/> particular, is a <a id="_idIndexMarker494"/>pre-trained vector that’s developed via an unsupervised learning algorithm. This method leverages linear substructures prevalent in texts and gauges semantic similarity by measuring vector distances. The GloVe website provides a classic example illustrating the relationships discerned by <span class="No-Break">the model:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="" height="338" role="presentation" src="image/B19446_08_04.jpg" width="627"/>
</div>
</div>
<p class="IMG---Figure"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Example of structures found in the words by GloVe (source: <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>)</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor264"/>Model building</h2>
<p>We are <a id="_idIndexMarker495"/>working on a supervised task to classify Crypto Panic headlines into positive, negative, and neutral. For this purpose, we will employ the <strong class="old">LSTM</strong> model, an <a id="_idIndexMarker496"/>abbreviation for <strong class="old">long short-term memory</strong>. The entire process outlined in this section can be followed through the <span class="No-Break"><strong class="source-inline">Modeling.ipynb</strong></span><span class="No-Break"> file.</span></p>
<p>LSTM is a type<a id="_idIndexMarker497"/> of <strong class="old">recurrent neural network</strong> (<strong class="old">RNN</strong>) that’s capable of learning long-term dependencies that significantly outperform regular RNNs with text tasks. This structure is commonly used in NLP tasks as it can model sequences of input data well and retain dependencies between words in a sentence or document. Consequently, it can predict not only based on the current input but also consider long-distance information – that is, the context – and not just specific words. It is important to note that while LSTMs can be more effective in capturing long-term dependencies, their performance can also depend on factors such as the specific task, the size of the dataset, and the model’s architecture. In some cases, other advanced architectures such as transformer-based models (such as BERT and GPT) have also demonstrated superior performance in certain <span class="No-Break">NLP tasks.</span></p>
<p>Christopher Olah provides a great introduction to the model in his blog, describing it <span class="No-Break">as follows:</span></p>
<p>“<em class="italic">Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts </em><span class="No-Break"><em class="italic">have persistence.</em></span><span class="No-Break">”</span></p>
<p>LSTM <a id="_idIndexMarker498"/>serves as a specialized form of RNN that’s designed to detect patterns in data sequences, whether they arise from sensor data, asset prices, or natural language. Its distinct feature lies in its capacity to preserve information over extended periods compared to conventional RNNs. RNNs have a short-term memory that retains information in the current neuron, resulting in a limited ability to predict with longer sequences. When the memory is exceeded, the model simply discards the oldest data and replaces it with new data, without considering whether the discarded data was important or not. LSTM overcomes this short-term memory problem by selectively retaining relevant information in <a id="_idIndexMarker499"/>the <strong class="old">cell state</strong> in addition to the traditional short-term memory <a id="_idIndexMarker500"/>stored in the <span class="No-Break"><strong class="old">hidden state</strong></span><span class="No-Break">.</span></p>
<p>At the beginning of each computational step, we have the current input, <em class="italic">x(t)</em>, the previous state of the long-term memory, <em class="italic">c(t-1)</em>, and the previous state of the short-term memory stored in the hidden state, <em class="italic">h(t-1)</em>. At the end of the process, we obtain an updated cell state and a new hidden state. The cell state carries information along with the timestamps of our dataset, allowing it to extract meaning from the order of the <span class="No-Break">input data.</span></p>
<p>These three inputs navigate through three gates, each serving a <span class="No-Break">specific function:</span></p>
<ul>
<li><strong class="old">Forget gate</strong>: This<a id="_idIndexMarker501"/> gate determines which current and previous information is retained and which is discarded. It integrates the previous status of the hidden state and the current input, passing them through a sigmoid function. The <a id="_idIndexMarker502"/>sigmoid function outputs values between 0 and 1, with 0 indicating that the previous information is considered irrelevant and can be forgotten, and 1 indicating that it should be preserved. The result is multiplied by the current <span class="No-Break">cell state.</span></li>
<li><strong class="old">Input gate</strong>: This <a id="_idIndexMarker503"/>gate determines the importance of the current input in solving the task. It quantifies the relevance of the new information carried by the input, <em class="italic">x(t)</em>. The current input is multiplied by the hidden state from the previous run. All information deemed important by the input gate is added to the cell state, forming the new cell state, <em class="italic">c(t)</em>. This new cell state becomes the current state of the long-term memory and will be used in the <span class="No-Break">next run.</span></li>
<li><strong class="old">Output gate</strong>: The <a id="_idIndexMarker504"/>output of the LSTM model is computed in the <span class="No-Break">hidden state:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 8.5 – Cell states" height="806" src="image/B19446_08_05.jpg" width="1209"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Cell states</p>
<p>To interact with the <a id="_idIndexMarker505"/>LSTM, we need to input vectors with uniform lengths. To <a id="_idIndexMarker506"/>fulfill this requirement, we must encode the preprocessed input text sequentially, <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="source-inline">tokenizer.texts_to_sequences(X_train)</strong>: This step transforms each text into a sequence of integers using the tokenizer’s most frequently encountered words. If the tokenizer lacks certain words in its vocabulary, a predefined <strong class="source-inline">&lt;OOV&gt;</strong> (out-of-vocabulary) token <span class="No-Break">is employed.</span></li>
<li><strong class="source-inline">pad_sequences</strong>: This function transforms the previously converted sequences into a 2D array of shape: (number of sequences, length of the sequence desired). The length or <strong class="source-inline">maxlen</strong> argument can be user-defined or defaulted to the longest sequence in the list. Additionally, the user can select whether padding occurs at the sequence’s beginning <span class="No-Break">or end.</span></li>
</ul>
<p>The model we have constructed features the embedding layer as its foremost element, with <strong class="source-inline">trainable = False</strong> to retain insights from Glove500. Should we opt for training from scratch, we have to set that parameter <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">:</span></p>
<pre class="console">
model = Sequential()
model.add(Embedding(input_dim = len(word_index) + 1 ,\
    output_dim = embedding_dim ,\
    weights = [embedding_vectors],\
    input_length = max_length ,\
    trainable = False))</pre> <p>Furthermore, our <a id="_idIndexMarker507"/>design incorporates an LSTM layer, a dense layer, and a dropout layer. The dropout layer, a regularization technique that’s frequently employed to prevent overfitting, operates by randomly deactivating (that is, setting to zero) a fraction of neurons during each forward pass in training. This helps prevent the network from relying too heavily on specific neurons and encourages the network to learn more robust and <span class="No-Break">generalized features:</span></p>
<pre class="console">
model.add(Bidirectional(LSTM(embedding_dim, activation = 'relu',\
    dropout = 0.0 ,\
    recurrent_dropout = 0.0)))
model.add(Dense(embedding_dim, activation='relu'))
model.add(Dropout(0.3))</pre> <p>For the last dense layer, we use <strong class="source-inline">'softmax'</strong> activation, which assigns decimal probabilities to each <span class="No-Break">trained class:</span></p>
<pre class="console">
model.add(Dense(label_distinct, activation='softmax',\
    bias_initializer =  'zeros'))</pre> <p>We compile by utilizing the loss function of <strong class="source-inline">'categorical_crossentropy'</strong>, a standard choice for multiclass classification tasks encompassing more than two classes, as is the <span class="No-Break">scenario here:</span></p>
<pre class="console">
model.compile(loss = 'categorical_crossentropy',\
    optimizer = Adam(1e-3), \
    metrics = ['accuracy'])</pre> <p class="callout-heading">Checkpoint</p>
<p class="callout">A step-by-step version of this part of the pipeline is shown in <strong class="source-inline">Modeling.ipynb</strong>. If you want to skip this section, the resulting model and tokenizer have been uploaded to this book’s GitHub repository and are accessible via <strong class="source-inline">chapter8_model.h5</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">text_tokenizer.json</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor265"/>Training and evaluation</h2>
<p>We can train the model with the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
model.fit(X_train_pad, y_train, batch_size = batch_size, \
    epochs = 10, validation_data = (X_test_pad, y_test), \
    verbose = 0, callbacks=[tensorboard_callback])</pre> <p>Upon completing the training, we<a id="_idIndexMarker508"/> assess its performance and outcomes through three <span class="No-Break">distinct methods:</span></p>
<p><strong class="old">Mini test</strong>: We <a id="_idIndexMarker509"/>look for <a id="_idIndexMarker510"/>test samples, apply the model, and conduct evaluations. We must always remember to preprocess our samples and pass them to our model in the same shape as it is ready <span class="No-Break">to consume.</span></p>
<p><strong class="old">TensorBoard</strong>: This <a id="_idIndexMarker511"/>visualization tool, enabled by TensorFlow, illustrates the model’s performance during the learning process and the evolution of the loss function across epochs. The provided code establishes a <strong class="source-inline">log</strong> folder for storing the resulting training and <span class="No-Break">validation outcomes:</span></p>
<pre class="console">
tensorboard_callback = \
    tf.keras.callbacks.TensorBoard(log_dir="./logs")</pre> <p>This folder is referred to in the training instructions provided with <strong class="source-inline">callbacks=[tensorboard_callback])</strong>. TensorBoard then accesses this folder to display <span class="No-Break">the results.</span></p>
<p><strong class="old">ROC AUC curve</strong>: According<a id="_idIndexMarker512"/> to Jason Brownlee’s blog, “<em class="italic">A ROC curve is a diagnostic plot for summarizing the behavior of a model by calculating the false positive rate and true positive rate for a set of predictions by the model under different thresholds</em>.” The ROC curve is an evaluation metric for binary tasks. To apply it to our multiclass case, we must transform the multiclass problem into a binary one using either<a id="_idIndexMarker513"/> the <strong class="old">one-versus-one</strong> (<strong class="old">OvO</strong>) or <strong class="old">one-versus-all </strong>(<strong class="old">OvA</strong>)/<strong class="old">one-versus-rest</strong> (<strong class="old">OvR</strong>) approach. In OvR, we<a id="_idIndexMarker514"/> assess each class against the<a id="_idIndexMarker515"/> others. In OvO, we evaluate each class against every other class in pairs. Choosing between these techniques depends on specific problem nuances, class count, computational resources, and dataset characteristics. Certain machine learning libraries, such as scikit-learn, offer the choice between OvA and OvO strategies for multi-class <span class="No-Break">classification algorithms.</span></p>
<p>In this case, we use the OvA approach, where we measure how well our model predicts each label, considering one as true and all the others as false. That way, we can plot the ROC AUC. The closer to 1, the better the model; the further it approaches 0.5, the less skillful <span class="No-Break">it is:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 8.6 – ROC AUC curve applied" height="386" src="image/B19446_08_06.jpg" width="547"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – ROC AUC curve applied</p>
<p>Both accuracy and the ROC AUC curve can be overly optimistic when dealing with <span class="No-Break">imbalanced datasets.</span></p>
<p><strong class="old">F1 score</strong>: When<a id="_idIndexMarker516"/> addressing a multiclass classification problem through the OvA perspective, we acquire a binary set of values that allow us to calculate precision, recall, and the F1 score. The F1 score proves more suitable when the aim is to minimize both false positives and false negatives. This metric amalgamates information from both precision and recall and is their harmonic mean. The F1 score formula is <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">F</span><em class="italic">1</em><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">____________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable"> </span></p>
<p>This is succinctly summarized by Joos Korstanje in <span class="No-Break">his blog:</span></p>
<ul>
<li><em class="italic">A model will obtain a high F1 score if both precision and recall </em><span class="No-Break"><em class="italic">are high</em></span></li>
<li><em class="italic">A model will obtain a low F1 score if both precision and recall </em><span class="No-Break"><em class="italic">are low</em></span></li>
<li><em class="italic">A model will obtain a medium F1 score if precision or recall is low and the other </em><span class="No-Break"><em class="italic">is high</em></span></li>
</ul>
<p>The aforementioned metric can be generated with the subsequent <span class="No-Break">code snippet:</span></p>
<pre class="console">
classification_report(y_test, y_pred_onehot, target_names=target_names</pre> <p>The weighted average F1 score for the model is <strong class="source-inline">0.72</strong>.</p>
<p>We can save the trained model for future use using the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
text_tokenizer_json = tokenizer.to_json()
with io.open('text_tokenizer.json','w',encoding='utf-8') as f:
    f.write(json.dumps(text_tokenizer_json, \
        ensure_ascii=False))
model.save('chapter8_model.h5')</pre> <p>In the <strong class="source-inline">Chapter08/app.py</strong> file in this book’s GitHub repository, we’ve developed an app that retrieves titles from the Cryptopanic API, applies the trained sentiment model, and displays the outcome in <span class="No-Break">the console.</span></p>
<p class="callout-heading">A note on NLP challenges</p>
<p class="callout">Due to the inherent complexities of human language, NLP faces several challenges that may significantly impact the performance and accuracy of its models. However, potential mitigation strategies exist to address <span class="No-Break">these challenges:</span></p>
<p class="callout"><strong class="old">Ambiguity</strong>: Words <a id="_idIndexMarker517"/>and phrases often carry multiple meanings, with the correct interpretation depending on the context. This complexity poses challenges, even for native and non-native speakers of a language, as seen in metaphors. Similarly, models encounter difficulties in interpreting user intent. To tackle this, models can be designed to incorporate broader contextual information, leveraging surrounding words and phrases for more accurate <span class="No-Break">meaning inference.</span></p>
<p class="callout"><strong class="old">Language diversity</strong>: Languages<a id="_idIndexMarker518"/> exhibit wide variations in grammar, syntax, and semantics. Additionally, slang, regional dialects, and cultural nuances further contribute to linguistic diversity. NLP models, trained on a specific type of data, may struggle to generalize to diverse linguistic contexts. To address this limitation, models can be trained on more extensive and diverse datasets encompassing various <span class="No-Break">linguistic patterns.</span></p>
<p class="callout"><strong class="old">Data sparsity</strong>: NLP models <a id="_idIndexMarker519"/>heavily rely on vast amounts of labeled data for training. However, acquiring labeled data for all conceivable language variations and applications proves challenging. Transfer learning techniques, such as pre-training on a large corpus and fine-tuning for specific tasks, offer a viable solution by mitigating the need for extensive <span class="No-Break">labeled data.</span></p>
<p class="callout"><strong class="old">Ethical considerations and bias</strong>: NLP<a id="_idIndexMarker520"/> models may inadvertently learn bias present in training data, resulting in biased outputs. Addressing this issue requires the curation of diverse and representative training datasets, regular audits of models for bias, and the implementation of fairness-aware <span class="No-Break">training techniques.</span></p>
<p class="callout">While NLP faces numerous challenges, ongoing research and advancements in machine learning techniques contribute to overcoming <span class="No-Break">these hurdles.</span></p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor266"/>ChatGPT integration</h2>
<p>When<a id="_idIndexMarker521"/> pressed for time and unable to gather the necessary data for training a new model to build a classifier, we can explore the option of <a id="_idIndexMarker522"/>utilizing pre-trained models, such as those offered by TextBlob, spaCy, or the Hugging Face libraries. Additionally, we can seamlessly integrate readily available models such <span class="No-Break">as ChatGPT.</span></p>
<p>I asked ChatGPT to present itself and this is what <span class="No-Break">it said:</span></p>
<p>“<em class="italic">I’m ChatGPT, a large language model developed by OpenAI, based on the GPT-3 architecture. I’m trained on a vast amount of text data and can generate human-like responses to a wide variety of questions and prompts. My main purpose is to assist users in generating text, whether it’s writing an article, answering questions, or completing creative tasks. I have a broad range of knowledge on various topics, including science, technology, history, and literature, among others. I’m designed to understand natural language and can respond to text-based input in a </em><span class="No-Break"><em class="italic">conversational manner</em></span><span class="No-Break">.”</span></p>
<p>Of particular relevance to this chapter is the final statement. This tool is well-versed in diverse vocabulary and is primed to process <span class="No-Break">textual input.</span></p>
<p>ChatGPT has a different architecture compared to LSTM. It uses the <strong class="old">transformer</strong> architecture, which <a id="_idIndexMarker523"/>allows it to understand and generate natural language text. Transformers use self-attention mechanisms to capture relationships between words in a sentence, allowing for parallel processing of words rather than sequential processing like in LSTMs. Transformers are used to translate languages, summarize long articles, answer questions, complete sentences, and even create stories. BERT and GPT are popular <span class="No-Break">transformer models.</span></p>
<p>In the <strong class="source-inline">Chapter07/chat_gpt integration</strong> file, we’ve replicated the same use case as the previous segment, where we interacted with the Cryptopanic API to extract titles, apply the ChatGPT model, and display the output in the console, yielding excellent results. To facilitate this, an API key is required, which can be generated by following these steps on the <span class="No-Break">OpenAI website:</span></p>
<ol>
<li>Visit <a href="https://platform.openai.com/docs/api-reference">https://platform.openai.com/docs/api-reference</a>, go to the sign-up section, and proceed to sign up on <span class="No-Break">their website.</span></li>
<li>On the left-hand side, you will see a dropdown menu that says <strong class="old">View API keys</strong>. Click on this to access the page for generating a new <span class="No-Break">API key:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 8.7 – ChatGPT – API keys landing page" height="440" src="image/B19446_08_07.jpg" width="1205"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – ChatGPT – API keys landing page</p>
<ol>
<li value="3">It is <a id="_idIndexMarker524"/>essential to generate and securely store the <a id="_idIndexMarker525"/>generated API keys as they can’t be retrieved once they’ve <span class="No-Break">been generated:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="Figure 8.8 – Chat GPT – API key generated" height="395" src="image/B19446_08_08.jpg" width="742"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Chat GPT – API key generated</p>
<p>The idea of this section is to recognize that ChatGPT exists and can do great work as well as solve the sentiment analysis problem by connecting the API, which may be a temporary solution if there is not enough data to train a <span class="No-Break">specialized model.</span></p>
<p>It is possible to fine-tune ChatGPT for a specific task or domain using task-specific data. This process enables the model to adapt to the nuances and requirements of the target application. For instance, we can customize the model to generate shorter answers, reduce the amount of context required in a prompt for improved responses, and define how it handles edge cases. Let’s imagine we would like to integrate a specialized bot into our company’s internal communications system that delivers concise summaries of cryptocurrency news with a specific tone or format. This could be done with this training process. Detailed documentation for this process is available at <a href="https://platform.openai.com/docs/guides/fine-tuning">https://platform.openai.com/docs/guides/fine-tuning</a>, and a <a id="_idIndexMarker526"/>step-by-step tutorial can be found in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
<h1 id="_idParaDest-158"><a id="_idTextAnchor267"/>Summary</h1>
<p>The field of NLP is rapidly evolving, providing an effective means to extract insights from unstructured data such as text. Throughout this chapter, we introduced the field, illustrated a typical task within it, delineated the workflow, discussed pertinent data, and executed model training using embeddings. Additionally, we demonstrated the model evaluation process and showcased its integration into a program that sources headlines from the <span class="No-Break">CryptoPanic API.</span></p>
<p>It’s worth emphasizing that amassing a substantial volume of data is pivotal for high model accuracy. Nevertheless, in cases where constructing such a model isn’t feasible, alternative solutions are available. We explored one such solution involving the ChatGPT API, which provides access to a text bot trained on a comprehensive corpus <span class="No-Break">of data.</span></p>
<p>In the subsequent chapter, we will delve into the support that data teams can extend to artistic groups who are seeking to transform their artworks into unique products through the utilization <span class="No-Break">of NFTs.</span></p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor268"/>Further reading</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li><span class="No-Break">Introduction:</span><ul><li>Bird, Steven, Edward Loper and Ewan Klein (2009), <em class="italic">Natural Language Processing with Python</em>. O’Reilly Media Inc. Available <span class="No-Break">at</span><span class="No-Break"><span class="hidden"> </span></span><a href="https://www.nltk.org/book/"><span class="No-Break">https://www.nltk.org/book/</span></a><span class="No-Break">.</span></li><li>Yordanov, V. (2019, August 13). <em class="italic">Introduction to natural language processing for text</em>. Medium. Available <span class="No-Break">at </span><span class="No-Break">https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63</span><span class="No-Break">.</span></li><li>Gabriel Doyle, and Charles Elkan. (n.d.). <em class="italic">Financial Topic Models</em>. Available <span class="No-Break">at </span><a href="https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf"><span class="No-Break">https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf</span></a><span class="No-Break">.</span></li><li>Sigmoider. (2018, May 3). <em class="italic">Get started with NLP (Part I)</em>. Medium. Available <span class="No-Break">at </span><span class="No-Break">https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828</span><span class="No-Break">.</span></li><li>Suhyeon Kim, Haecheong Park, and Junghye Lee. (n.d.). <em class="italic">Word2vec-based latent semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology trend analysis</em>. Available <span class="No-Break">at </span><a href="https://www.sciencedirect.com/science/article/pii/S0957417420302256"><span class="No-Break">https://www.sciencedirect.com/science/article/pii/S0957417420302256</span></a><span class="No-Break">.</span></li><li>State of data science and machine learning (2022). <em class="italic">Kaggle: Your Machine Learning and Data Science Community</em>. Available <span class="No-Break">at </span><a href="https://www.kaggle.com/kaggle-survey-2022"><span class="No-Break">https://www.kaggle.com/kaggle-survey-2022</span></a><span class="No-Break">.</span></li><li><em class="italic">Very good ChatGPT fine tuning tutorial: Tech-At-Work</em>. (2023, September 11). Easily Fine Tune ChatGPT 3.5 to Outperform GPT-4! [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=8Ieu2v0v4oc"><span class="No-Break">https://www.youtube.com/watch?v=8Ieu2v0v4oc</span></a><span class="No-Break">.</span></li></ul></li>
<li><span class="No-Break">Example database:</span><ul><li>Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P. (2014). <em class="italic">Good debt or bad debt: Detecting semantic orientations in economic texts</em>. Journal of the Association for Information Science and Technology, 65(4), 782-796. Available <span class="No-Break">at </span><a href="https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news"><span class="No-Break">https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news</span></a><span class="No-Break">.</span></li><li>Manoel Fernando Alonso Gadi, and Miguel Ángel Sicilia. (2022, October 10). <em class="italic">Cryptocurrency Curated News Event Database From GDELT</em> [pdf]. Research Square. Available <span class="No-Break">at </span><span class="No-Break">https://assets.researchsquare.com/files/rs-2145757/v1_covered.pdf?c=1665769708</span><span class="No-Break">.</span></li></ul></li>
<li><span class="No-Break">Preprocessing:</span><ul><li>Bird, S., Klein, E., and Loper, E. (2009). <em class="italic">Natural language processing with Python</em>. <span class="No-Break">O’Reilly Media.</span></li><li><em class="italic">Sklearn.feature_extraction.text.CountVectorizer</em>. (n.d.). scikit-learn. Retrieved March 24, 2023. Available <span class="No-Break">at </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml</span></a><span class="No-Break">.</span></li><li><em class="italic">Sklearn.feature_extraction.text.TfidfVectorizer</em>. (n.d.). scikit-learn. Retrieved March 24, 2023. Available <span class="No-Break">at </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml</span></a><span class="No-Break">.</span></li></ul></li>
<li><span class="No-Break">Model:</span><ul><li>Dudeperf3ct. (2019, January 28). <em class="italic">Force of LSTM and GRU</em>. <span class="No-Break">Blog. </span><a href="https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model"><span class="No-Break">https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model</span></a><span class="No-Break">.</span></li><li>Brandon Rohrer. (n.d.). <em class="italic">Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</em> [Video]. YouTube. Available <span class="No-Break">at </span><span class="No-Break">https://www.youtube.com/watch?v=WCUNPb-5EYI&amp;list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp</span><span class="No-Break">.</span></li><li>Pennington, J. (n.d.). <em class="italic">GloVe: Global vectors for word representation</em>. The Stanford Natural Language Processing Group. Available <span class="No-Break">at </span><a href="https://nlp.stanford.edu/projects/glove/"><span class="No-Break">https://nlp.stanford.edu/projects/glove/</span></a><span class="No-Break">.</span></li><li>Jason Brownlee. (2020). <em class="italic">Deep Convolutional Neural Network for Sentiment Analysis (Text Classification)</em>. Machine Learning Mastery. Available <span class="No-Break">at </span><a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"><span class="No-Break">https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/</span></a><span class="No-Break">.</span></li></ul></li>
<li><span class="No-Break">Evaluation:</span><ul><li>T., B. (2022, December 9). <em class="italic">Comprehensive guide on Multiclass classification metrics</em>. Medium. Available <span class="No-Break">at </span><a href="https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd"><span class="No-Break">https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd</span></a><span class="No-Break">.</span></li><li>Jason Brownlee (2021). <em class="italic">Tour of Evaluation Metrics for Imbalanced Classification</em>. Machine Learning Mastery. Available <span class="No-Break">at </span><span class="No-Break">https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/</span><span class="No-Break">.</span></li><li>Korstanje, J. (2021, August 31). <em class="italic">The F1 score</em>. Medium. Available <span class="No-Break">at </span><a href="https://towardsdatascience.com/the-f1-score-bec2bbc38aa6"><span class="No-Break">https://towardsdatascience.com/the-f1-score-bec2bbc38aa6</span></a><span class="No-Break">.</span></li></ul></li>
</ul>
</div>
</div></body></html>