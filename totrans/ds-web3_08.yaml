- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment Analysis – NLP and Crypto News
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) falls under the umbrella of artificial
    intelligence and is concerned with the comprehension of text by computers. Recent
    advancements in this field, exemplified by the emergence of tools such as ChatGPT,
    have become an integral part of our daily lives. Yet, the financial sector has
    been leveraging NLP for quite some time, particularly for fundamental analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental analysis seeks to ascertain the intrinsic value of assets such as
    stocks, tokens, or NFT art based on publicly accessible information. In traditional
    finance, textual data is sourced from periodic submissions to the SEC (such as
    Form 10K or 10Q), including financial statements, specialized media news, social
    media platforms such as X (formerly Twitter), and other avenues. Web3 has fostered
    a similar environment where market activities are continuous, and X and news platforms
    serve as predominant textual resources. It’s worth noting that while most Web3
    companies may not yet be obligated to file regulatory reports, it’s probable that
    such data sources will eventually become available for most companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP in the financial sector encompasses various applications, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Determining the positivity, negativity, or neutrality
    of text, which could be news articles, social media posts (tweets, Reddit, and
    so on), and more. These algorithms can also provide insights into polarity and
    subjectivity, aiding in assessing sentiments toward companies, industries, markets,
    government decisions, crypto developments, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling**: This helps classify and organize large volumes of financial
    documents based on the underlying topics they cover. This aids in efficiently
    managing and accessing relevant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarization**: In a world where content is being created at a non-stop
    rate, there simply is not enough time and/or resources to analyze and give hierarchy
    to all of it. NLP techniques are being applied to collect and create short summaries
    of documents that are easy to process by analysts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: Scrutinizing emails, chats, financial documents, transcribed
    conversations, and more using NLP techniques to uncover patterns that are potentially
    indicative of fraudulent activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trading**: Incorporating NLP tools into trading strategies to signal or predict
    market trends, or enhance the decision-making process of fundamental analysis
    traders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data source for NLP techniques is text that is categorized as unstructured.
    We are surrounded by it, with more being produced every second. We explored some
    text sources in [*Chapter 3*](B19446_03.xhtml#_idTextAnchor114); we will explore
    some others in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will analyze the sentiment of the news sourced from Crypto
    Panic. To do so, we will build a **neural network** (**NN**) and explain the concept
    and use of pre-trained embeddings. A crypto news dataset and a traditional finance
    news dataset will be employed to train our model. Additionally, we’ll learn the
    essentials of how to pre-process text for NN utilization and how to evaluate the
    results of such a model.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, ChatGPT is a tangible reality. Publicly accessible information
    reveals its training on an extensive multilingual corpus, utilizing reinforcement
    learning to progressively enhance its performance. We’ll learn how to incorporate
    ChatGPT for sentiment analysis on the Crypto Panic dataset. This can be useful
    to implement as a ready-to-use tool while we build a specialized corpus to train
    our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A deep learning pipeline for sentiment analysis, encompassing preparation, model
    construction, training, and evaluation phases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating ChatGPT for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll utilize tools from the libraries that were introduced
    in [*Chapter 7*](B19446_07.xhtml#_idTextAnchor228) – that is, scikit-learn and
    Keras. Additionally, we will employ **NLTK**, a Python library that proves valuable
    for working with human language data. NLTK includes a range of modules and functions
    that empower us to execute tasks such as tokenization, stemming, and part-of-speech
    tagging on our selected databases. This library streamlines the process of processing
    extensive text datasets so that they’re ready to be integrated with machine learning
    or deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not worked with NLTK before, it can be installed with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The documentation for `nltk` can be found at https://www.nltk.org. Another
    essential library when handling text manipulation and cleaning is **re**, short
    for **Regular Expression**. A regular expression is a sequence of characters that
    defines a search pattern. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pattern** | **Search Criteria** |'
  prefs: []
  type: TYPE_TB
- en: '| [a-z] | Any single character in the range a-z |'
  prefs: []
  type: TYPE_TB
- en: '| [0-9A-Fa-f] | Match any hexadecimal digit |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Example of a “re” pattern
  prefs: []
  type: TYPE_NORMAL
- en: The `re` library provides functions and methods to employ the aforementioned
    patterns. For instance, `re.sub` replaces all characters that match the pattern
    with a specified string. A comprehensive list of functions is available at https://docs.python.org/3/library/re.xhtml#module-re.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout our work, we will utilize Google’s Colaboratory platform, which already
    includes the core library imports. However, for specific tasks, additional imports
    will be required.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the data and code files for this chapter in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter08).
    We recommend that you read through the code files in the `Chapter08` folder to
    follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Example datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will merge two headline datasets in this chapter; you can find links to
    the relevant sources in the *Further reading* section and the code. The datasets
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Financial Phrase Bank**: This dataset is also available on Kaggle and contains
    a headline accompanied by sentiment analysis labels from the viewpoint of a retail
    investor. It comprises multiple datasets, each of which categorizes sentences
    based on the level of consensus regarding the sentiment of the phrase. For this
    exercise, we will utilize the *Sentence_AllAgree* dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CryptoGDELT2022**: Presented in the paper *Cryptocurrency Curated News Event
    Database From GDELT*, this dataset comprises news events extracted from the **Global
    Database of Events, Language, and Tone** (**GDELT**). It covers news events between
    March 31, 2021, and April 30, 2022\. The dataset includes various sentiment scores
    and manual labeling methods. In this exercise, we will exclusively employ manual
    labeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started with building our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building our pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an NLP pipeline, preparation generally encompasses a pre-processing step
    where we clean and normalize the data. Following that, a feature representation
    step translates the language into input that can be consumed by our chosen models.
    Once this is completed, we are ready to build, train, and evaluate the model.
    This strategic plan will be implemented throughout the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language manifests in numerous variations. There are formatting nuances, such
    as capitalization or punctuation; words that serve as linguistic aids without
    true semantic meaning, such as prepositions; and special characters, including
    emojis, further enrich the landscape. To work with this data, we must transform
    raw text into a dataset while following a similar criterion as numeric datasets.
    This cleaning process enables us to eliminate outliers, reduce noise, manage vocabulary
    size, and optimize data for ingestion by NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic flow diagram of the data cleaning pipeline can be seen in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Cleaning diagram](img/B19446_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Cleaning diagram
  prefs: []
  type: TYPE_NORMAL
- en: Let’s delve into some of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalization encompasses a series of tasks, including lowercasing and removing
    HTML traces, links, and emojis. Our objective is to clean our database so that
    only words remain.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this process involves eliminating words that lack informativeness
    for our model or that could potentially introduce bias, leading to suboptimal
    predictions. Depending on the task, we will select words that we may have to delete.
    This step also addresses the process of removing words that may have escaped the
    *stop words* cleaning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also convert Unidecode text into ASCII text. Unidecode accommodates
    characters from diverse languages, allowing them to be translated into their nearest
    ASCII counterpart. For instance, the Spanish character “ñ” becomes “n” in ASCII.
    We implement this transformation in `Database_and_Preprocessing.ipynb` using the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Normalization fosters uniformity by rendering all text in a consistent format,
    directing the model’s focus toward content rather than superficial differences.
  prefs: []
  type: TYPE_NORMAL
- en: Stop words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our objective here is to exclude words that contribute minimal semantic value
    or meaning to our model. Frequently employed words such as articles (“a,” “the,”
    “an”), prepositions (“on,” “at,” “from,” “to”), conjunctions (“and,” “so,” “although”),
    and pronouns (“she,” “he,” “it”) play functional roles in language but lack substantial
    semantic content that the model can leverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, this collection of words is generally filtered out during preprocessing.
    The stop words collection can be downloaded by language and directly applied to
    our cleaning process using NLTK. In `Database_and_Preprocessing.ipynb`, we downloaded
    the English stop words with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This process reduces the noise in the data and helps improve the efficiency
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tokenization involves splitting the text within our database into smaller units
    of meaning referred to as tokens. These units can take the form of sentences or
    words. For instance, let’s take a look at the following headline:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“SEC investigating Coinbase for its Earn product, wallet service, and* *exchange
    activity”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'When tokenized into words, this yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[''SEC'', ''investigating'', ''Coinbase'', ''Earn'', ''product'', '','', ''wallet'',
    ''service'', ''``exchange'', ''activity'']`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenization results in a structured input that the NLP model can process effectively,
    facilitating data analysis. Such analysis guides decisions on vocabulary size
    for dimensionality reduction, a demonstrated example of which can be found in
    `Database_and_Preprocessing.ipynb`. This showcases the top recurring words in
    the analyzed dataset. Such analysis gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Top recurring words in analyzed headlines](img/B19446_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Top recurring words in analyzed headlines
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization and part-of-speech (POS) tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These techniques reduce words to their base or root form. It allows us to reduce
    the diversity of words we have to process by selecting those words that are conjugated
    and replacing them with their root word. In our example sentence (“SEC investigating
    Coinbase for its Earn product, wallet service, and exchange activity”), the word
    *investigating* will change to *investigate*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy of lemmatization hinges on the library’s comprehension of the
    word’s context or function within the sentence. POS tagging contributes this contextual
    information to our analysis. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'POS tagging helps us programmatically assign a context to each word, depending
    on its position in the sentence or document. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of cleaning the sentence looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at some additional preprocessing techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming**: This involves removing prefixes and suffixes from words to derive
    a common base form, known as the “stem.” The resulting stem may not always form
    a valid word, but it aims to capture the core meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named entity recognition** (**NER**): This technique automatically identifies
    and classifies named entities (for example, names of people, places, organizations,
    and dates) in text. NER extracts structured information from unstructured text,
    categorizing entities into predefined classes. An example of this approach is
    offered by the X (formerly Twitter) dataset, which we covered in [*Chapter 3*](B19446_03.xhtml#_idTextAnchor114).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency parsing**: This technique analyzes a sentence’s grammatical structure
    to establish relationships between words. It creates a hierarchical structure
    where each word is linked to its governing word (the “head”) and assigned a grammatical
    role (the “dependency label”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step version of this pipeline is detailed in `Database_and_Preprocessing.ipynb`.
    If you want to skip this section, the resulting `.csv` file has been uploaded
    to this book's GitHub and is accessible at `preprocessed.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the preprocessing phase, the next step involves transforming the resultant
    raw text data into features that the model can utilize for statistical inference.
    The goal is to extract relevant information from the text and encode it in a way
    that algorithms can understand. There are multiple ways to achieve this, but they
    generally involve representing words as vectors and measuring the frequency of
    words in a document. Some common techniques are bag of words, **term frequency-inverse
    document frequency** (**TF-IDF**), and word embeddings. Let’s briefly describe
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This technique builds a vector that represents all the words in the document
    by the number of times that word appears. It ignores the order of the sentence
    or the context of the word. Its implementation can be accomplished using `sklearn.feature_extraction.text.CountVectorizer`
    from the scikit-learn library. This approach is basic and loses important contextual
    information, and it may also create a very sparse matrix because the vocabulary
    is vast:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/crypto-credit-card/](img/B19446_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Bag of words. Text extracted from https://rif.technology/content-hub/crypto-credit-card/
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, this approach can be enhanced by incorporating n-grams, which involve
    concatenating two or more words that hold contextual meaning together. For example,
    “natural language”, “machine learning,” and “press release” encapsulate specific
    concepts when combined, but in isolation, they do not retain the concept. Incorporating
    n-grams into bag of words can expand the vocabulary further.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an alternate technique where `sklearn.feature_extraction.text.TfidfVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word embeddings represent words as dense vectors within a continuous space.
    This approach retains information about context and semantic meaning by capturing
    relationships between words. **Word2Vec** and **GloVe** ([https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec))
    are popular algorithms that generate word embeddings. These embeddings can either
    be pre-trained on large text corpora or fine-tuned for specific tasks. In our
    model, we employ Glove ([https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove))
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Glove, in particular, is a pre-trained vector that’s developed via an unsupervised
    learning algorithm. This method leverages linear substructures prevalent in texts
    and gauges semantic similarity by measuring vector distances. The GloVe website
    provides a classic example illustrating the relationships discerned by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19446_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4 – Example of structures found in the words by GloVe (source: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))'
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are working on a supervised task to classify Crypto Panic headlines into
    positive, negative, and neutral. For this purpose, we will employ the `Modeling.ipynb`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM is a type of **recurrent neural network** (**RNN**) that’s capable of learning
    long-term dependencies that significantly outperform regular RNNs with text tasks.
    This structure is commonly used in NLP tasks as it can model sequences of input
    data well and retain dependencies between words in a sentence or document. Consequently,
    it can predict not only based on the current input but also consider long-distance
    information – that is, the context – and not just specific words. It is important
    to note that while LSTMs can be more effective in capturing long-term dependencies,
    their performance can also depend on factors such as the specific task, the size
    of the dataset, and the model’s architecture. In some cases, other advanced architectures
    such as transformer-based models (such as BERT and GPT) have also demonstrated
    superior performance in certain NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Christopher Olah provides a great introduction to the model in his blog, describing
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “*Humans don’t start their thinking from scratch every second. As you read this
    essay, you understand each word based on your understanding of previous words.
    You don’t throw everything away and start thinking from scratch again. Your thoughts*
    *have persistence.*”
  prefs: []
  type: TYPE_NORMAL
- en: LSTM serves as a specialized form of RNN that’s designed to detect patterns
    in data sequences, whether they arise from sensor data, asset prices, or natural
    language. Its distinct feature lies in its capacity to preserve information over
    extended periods compared to conventional RNNs. RNNs have a short-term memory
    that retains information in the current neuron, resulting in a limited ability
    to predict with longer sequences. When the memory is exceeded, the model simply
    discards the oldest data and replaces it with new data, without considering whether
    the discarded data was important or not. LSTM overcomes this short-term memory
    problem by selectively retaining relevant information in the **cell state** in
    addition to the traditional short-term memory stored in the **hidden state**.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of each computational step, we have the current input, *x(t)*,
    the previous state of the long-term memory, *c(t-1)*, and the previous state of
    the short-term memory stored in the hidden state, *h(t-1)*. At the end of the
    process, we obtain an updated cell state and a new hidden state. The cell state
    carries information along with the timestamps of our dataset, allowing it to extract
    meaning from the order of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'These three inputs navigate through three gates, each serving a specific function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget gate**: This gate determines which current and previous information
    is retained and which is discarded. It integrates the previous status of the hidden
    state and the current input, passing them through a sigmoid function. The sigmoid
    function outputs values between 0 and 1, with 0 indicating that the previous information
    is considered irrelevant and can be forgotten, and 1 indicating that it should
    be preserved. The result is multiplied by the current cell state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input gate**: This gate determines the importance of the current input in
    solving the task. It quantifies the relevance of the new information carried by
    the input, *x(t)*. The current input is multiplied by the hidden state from the
    previous run. All information deemed important by the input gate is added to the
    cell state, forming the new cell state, *c(t)*. This new cell state becomes the
    current state of the long-term memory and will be used in the next run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gate**: The output of the LSTM model is computed in the hidden state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Cell states](img/B19446_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Cell states
  prefs: []
  type: TYPE_NORMAL
- en: 'To interact with the LSTM, we need to input vectors with uniform lengths. To
    fulfill this requirement, we must encode the preprocessed input text sequentially,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer.texts_to_sequences(X_train)`: This step transforms each text into
    a sequence of integers using the tokenizer’s most frequently encountered words.
    If the tokenizer lacks certain words in its vocabulary, a predefined `<OOV>` (out-of-vocabulary)
    token is employed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_sequences`: This function transforms the previously converted sequences
    into a 2D array of shape: (number of sequences, length of the sequence desired).
    The length or `maxlen` argument can be user-defined or defaulted to the longest
    sequence in the list. Additionally, the user can select whether padding occurs
    at the sequence’s beginning or end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model we have constructed features the embedding layer as its foremost
    element, with `trainable = False` to retain insights from Glove500\. Should we
    opt for training from scratch, we have to set that parameter to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, our design incorporates an LSTM layer, a dense layer, and a dropout
    layer. The dropout layer, a regularization technique that’s frequently employed
    to prevent overfitting, operates by randomly deactivating (that is, setting to
    zero) a fraction of neurons during each forward pass in training. This helps prevent
    the network from relying too heavily on specific neurons and encourages the network
    to learn more robust and generalized features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For the last dense layer, we use `''softmax''` activation, which assigns decimal
    probabilities to each trained class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We compile by utilizing the loss function of `''categorical_crossentropy''`,
    a standard choice for multiclass classification tasks encompassing more than two
    classes, as is the scenario here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step version of this part of the pipeline is shown in `Modeling.ipynb`.
    If you want to skip this section, the resulting model and tokenizer have been
    uploaded to this book’s GitHub repository and are accessible via `chapter8_model.h5`
    and `text_tokenizer.json`.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can train the model with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon completing the training, we assess its performance and outcomes through
    three distinct methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mini test**: We look for test samples, apply the model, and conduct evaluations.
    We must always remember to preprocess our samples and pass them to our model in
    the same shape as it is ready to consume.'
  prefs: []
  type: TYPE_NORMAL
- en: '`log` folder for storing the resulting training and validation outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This folder is referred to in the training instructions provided with `callbacks=[tensorboard_callback])`.
    TensorBoard then accesses this folder to display the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**ROC AUC curve**: According to Jason Brownlee’s blog, “*A ROC curve is a diagnostic
    plot for summarizing the behavior of a model by calculating the false positive
    rate and true positive rate for a set of predictions by the model under different
    thresholds*.” The ROC curve is an evaluation metric for binary tasks. To apply
    it to our multiclass case, we must transform the multiclass problem into a binary
    one using either the **one-versus-one** (**OvO**) or **one-versus-all** (**OvA**)/**one-versus-rest**
    (**OvR**) approach. In OvR, we assess each class against the others. In OvO, we
    evaluate each class against every other class in pairs. Choosing between these
    techniques depends on specific problem nuances, class count, computational resources,
    and dataset characteristics. Certain machine learning libraries, such as scikit-learn,
    offer the choice between OvA and OvO strategies for multi-class classification
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we use the OvA approach, where we measure how well our model
    predicts each label, considering one as true and all the others as false. That
    way, we can plot the ROC AUC. The closer to 1, the better the model; the further
    it approaches 0.5, the less skillful it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – ROC AUC curve applied](img/B19446_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – ROC AUC curve applied
  prefs: []
  type: TYPE_NORMAL
- en: Both accuracy and the ROC AUC curve can be overly optimistic when dealing with
    imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 score**: When addressing a multiclass classification problem through the
    OvA perspective, we acquire a binary set of values that allow us to calculate
    precision, recall, and the F1 score. The F1 score proves more suitable when the
    aim is to minimize both false positives and false negatives. This metric amalgamates
    information from both precision and recall and is their harmonic mean. The F1
    score formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: F*1* Score = 2 *  Precision * Recall  _____________  Precision + Recall
  prefs: []
  type: TYPE_NORMAL
- en: 'This is succinctly summarized by Joos Korstanje in his blog:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A model will obtain a high F1 score if both precision and recall* *are high*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A model will obtain a low F1 score if both precision and recall* *are low*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A model will obtain a medium F1 score if precision or recall is low and the
    other* *is high*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The aforementioned metric can be generated with the subsequent code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The weighted average F1 score for the model is `0.72`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can save the trained model for future use using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the `Chapter08/app.py` file in this book’s GitHub repository, we’ve developed
    an app that retrieves titles from the Cryptopanic API, applies the trained sentiment
    model, and displays the outcome in the console.
  prefs: []
  type: TYPE_NORMAL
- en: A note on NLP challenges
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the inherent complexities of human language, NLP faces several challenges
    that may significantly impact the performance and accuracy of its models. However,
    potential mitigation strategies exist to address these challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ambiguity**: Words and phrases often carry multiple meanings, with the correct
    interpretation depending on the context. This complexity poses challenges, even
    for native and non-native speakers of a language, as seen in metaphors. Similarly,
    models encounter difficulties in interpreting user intent. To tackle this, models
    can be designed to incorporate broader contextual information, leveraging surrounding
    words and phrases for more accurate meaning inference.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Language diversity**: Languages exhibit wide variations in grammar, syntax,
    and semantics. Additionally, slang, regional dialects, and cultural nuances further
    contribute to linguistic diversity. NLP models, trained on a specific type of
    data, may struggle to generalize to diverse linguistic contexts. To address this
    limitation, models can be trained on more extensive and diverse datasets encompassing
    various linguistic patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sparsity**: NLP models heavily rely on vast amounts of labeled data
    for training. However, acquiring labeled data for all conceivable language variations
    and applications proves challenging. Transfer learning techniques, such as pre-training
    on a large corpus and fine-tuning for specific tasks, offer a viable solution
    by mitigating the need for extensive labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ethical considerations and bias**: NLP models may inadvertently learn bias
    present in training data, resulting in biased outputs. Addressing this issue requires
    the curation of diverse and representative training datasets, regular audits of
    models for bias, and the implementation of fairness-aware training techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: While NLP faces numerous challenges, ongoing research and advancements in machine
    learning techniques contribute to overcoming these hurdles.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When pressed for time and unable to gather the necessary data for training a
    new model to build a classifier, we can explore the option of utilizing pre-trained
    models, such as those offered by TextBlob, spaCy, or the Hugging Face libraries.
    Additionally, we can seamlessly integrate readily available models such as ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'I asked ChatGPT to present itself and this is what it said:'
  prefs: []
  type: TYPE_NORMAL
- en: “*I’m ChatGPT, a large language model developed by OpenAI, based on the GPT-3
    architecture. I’m trained on a vast amount of text data and can generate human-like
    responses to a wide variety of questions and prompts. My main purpose is to assist
    users in generating text, whether it’s writing an article, answering questions,
    or completing creative tasks. I have a broad range of knowledge on various topics,
    including science, technology, history, and literature, among others. I’m designed
    to understand natural language and can respond to text-based input in a* *conversational
    manner*.”
  prefs: []
  type: TYPE_NORMAL
- en: Of particular relevance to this chapter is the final statement. This tool is
    well-versed in diverse vocabulary and is primed to process textual input.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT has a different architecture compared to LSTM. It uses the **transformer**
    architecture, which allows it to understand and generate natural language text.
    Transformers use self-attention mechanisms to capture relationships between words
    in a sentence, allowing for parallel processing of words rather than sequential
    processing like in LSTMs. Transformers are used to translate languages, summarize
    long articles, answer questions, complete sentences, and even create stories.
    BERT and GPT are popular transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Chapter07/chat_gpt integration` file, we’ve replicated the same use
    case as the previous segment, where we interacted with the Cryptopanic API to
    extract titles, apply the ChatGPT model, and display the output in the console,
    yielding excellent results. To facilitate this, an API key is required, which
    can be generated by following these steps on the OpenAI website:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference),
    go to the sign-up section, and proceed to sign up on their website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the left-hand side, you will see a dropdown menu that says **View API keys**.
    Click on this to access the page for generating a new API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – ChatGPT – API keys landing page](img/B19446_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – ChatGPT – API keys landing page
  prefs: []
  type: TYPE_NORMAL
- en: 'It is essential to generate and securely store the generated API keys as they
    can’t be retrieved once they’ve been generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Chat GPT – API key generated](img/B19446_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Chat GPT – API key generated
  prefs: []
  type: TYPE_NORMAL
- en: The idea of this section is to recognize that ChatGPT exists and can do great
    work as well as solve the sentiment analysis problem by connecting the API, which
    may be a temporary solution if there is not enough data to train a specialized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to fine-tune ChatGPT for a specific task or domain using task-specific
    data. This process enables the model to adapt to the nuances and requirements
    of the target application. For instance, we can customize the model to generate
    shorter answers, reduce the amount of context required in a prompt for improved
    responses, and define how it handles edge cases. Let’s imagine we would like to
    integrate a specialized bot into our company’s internal communications system
    that delivers concise summaries of cryptocurrency news with a specific tone or
    format. This could be done with this training process. Detailed documentation
    for this process is available at [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning),
    and a step-by-step tutorial can be found in the *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of NLP is rapidly evolving, providing an effective means to extract
    insights from unstructured data such as text. Throughout this chapter, we introduced
    the field, illustrated a typical task within it, delineated the workflow, discussed
    pertinent data, and executed model training using embeddings. Additionally, we
    demonstrated the model evaluation process and showcased its integration into a
    program that sources headlines from the CryptoPanic API.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth emphasizing that amassing a substantial volume of data is pivotal
    for high model accuracy. Nevertheless, in cases where constructing such a model
    isn’t feasible, alternative solutions are available. We explored one such solution
    involving the ChatGPT API, which provides access to a text bot trained on a comprehensive
    corpus of data.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent chapter, we will delve into the support that data teams can
    extend to artistic groups who are seeking to transform their artworks into unique
    products through the utilization of NFTs.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language Processing
    with Python*. O’Reilly Media Inc. Available at [https://www.nltk.org/book/](https://www.nltk.org/book/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Yordanov, V. (2019, August 13). *Introduction to natural language processing
    for text*. Medium. Available at https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gabriel Doyle, and Charles Elkan. (n.d.). *Financial Topic Models*. Available
    at [https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf](https://pages.ucsd.edu/~gdoyle/papers/doyle-elkan-2009-nips-paper.pdf).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoider. (2018, May 3). *Get started with NLP (Part I)*. Medium. Available
    at https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suhyeon Kim, Haecheong Park, and Junghye Lee. (n.d.). *Word2vec-based latent
    semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology
    trend analysis*. Available at [https://www.sciencedirect.com/science/article/pii/S0957417420302256](https://www.sciencedirect.com/science/article/pii/S0957417420302256).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State of data science and machine learning (2022). *Kaggle: Your Machine Learning
    and Data Science Community*. Available at [https://www.kaggle.com/kaggle-survey-2022](https://www.kaggle.com/kaggle-survey-2022).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Very good ChatGPT fine tuning tutorial: Tech-At-Work*. (2023, September 11).
    Easily Fine Tune ChatGPT 3.5 to Outperform GPT-4! [Video]. YouTube. Available
    at [https://www.youtube.com/watch?v=8Ieu2v0v4oc](https://www.youtube.com/watch?v=8Ieu2v0v4oc).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example database:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala, P. (2014). *Good
    debt or bad debt: Detecting semantic orientations in economic texts*. Journal
    of the Association for Information Science and Technology, 65(4), 782-796\. Available
    at [https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Manoel Fernando Alonso Gadi, and Miguel Ángel Sicilia. (2022, October 10). *Cryptocurrency
    Curated News Event Database From GDELT* [pdf]. Research Square. Available at https://assets.researchsquare.com/files/rs-2145757/v1_covered.pdf?c=1665769708.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preprocessing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bird, S., Klein, E., and Loper, E. (2009). *Natural language processing with
    Python*. O’Reilly Media.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sklearn.feature_extraction.text.CountVectorizer*. (n.d.). scikit-learn. Retrieved
    March 24, 2023\. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.xhtml).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sklearn.feature_extraction.text.TfidfVectorizer*. (n.d.). scikit-learn. Retrieved
    March 24, 2023\. Available at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.xhtml).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dudeperf3ct. (2019, January 28). *Force of LSTM and GRU*. Blog. [https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model](https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/#bag-of-words-model).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Brandon Rohrer. (n.d.). *Recurrent Neural Networks (RNN) and Long Short-Term
    Memory (LSTM)* [Video]. YouTube. Available at https://www.youtube.com/watch?v=WCUNPb-5EYI&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington, J. (n.d.). *GloVe: Global vectors for word representation*. The
    Stanford Natural Language Processing Group. Available at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Jason Brownlee. (2020). *Deep Convolutional Neural Network for Sentiment Analysis
    (Text Classification)*. Machine Learning Mastery. Available at [https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evaluation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T., B. (2022, December 9). *Comprehensive guide on Multiclass classification
    metrics*. Medium. Available at [https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Jason Brownlee (2021). *Tour of Evaluation Metrics for Imbalanced Classification*.
    Machine Learning Mastery. Available at https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Korstanje, J. (2021, August 31). *The F1 score*. Medium. Available at [https://towardsdatascience.com/the-f1-score-bec2bbc38aa6](https://towardsdatascience.com/the-f1-score-bec2bbc38aa6).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
