<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer141">
<h1 class="chapter-num er" id="_idParaDest-160"><a id="_idTextAnchor269"/>9</h1>
<h1 id="_idParaDest-161"><a id="_idTextAnchor270"/>Generative Art for NFTs</h1>
<p class="author-quote"><a id="_idTextAnchor271"/>“I use data as a pigment and paint with a painting brush that is assisted by artificial intelligence.”</p>
<p class="author-quote">– Refik Anadol</p>
<p>In this chapter, we’ll take an artistic break and indulge in some creativity. While our previous focus was on analyzing content generated by others on the blockchain, in this chapter, we will be creating our own content to be added to <span class="No-Break">the blockchain.</span></p>
<p>The inclusion of this chapter stems from the recognition that, as data scientists, we might encounter requests to produce or assist in crafting an NFT collection in collaboration with a group of artists. In <a href="B19446_04.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we studied the artistic applications of NFTs and explored notable collections, such as <em class="italic">Bored Ape</em>, which has a total traded volume of 978,382 ETH (approximately USD 1,800 million). We do not know whether they used AI to produce all the images, but they are a good use case of how art can be owned and traded on the blockchain. To be able to participate in that market, we will learn about the entire process, from crafting an image to listing it for sale <span class="No-Break">on OpenSea.</span></p>
<p>One particular collection named <em class="italic">Artsy Monke</em> used AI to create images by combining the Bored Ape collection with 20 curated painting styles. You can find their OpenSea collection <a id="_idIndexMarker527"/>website at <a href="https://opensea.io/collection/artsy-monke">https://opensea.io/collection/artsy-monke</a>. The image on the cover of the book is Artsy <span class="No-Break">Monke #9937.</span></p>
<p>Another example is Refik Anadol’s <em class="italic">Machine Hallucinations</em> collection, which is a collaboration with NASA that uses over two million raw images, recorded by space institutions such as the International Space Station, the Hubble and MRO telescopes across the world into six AI data-created paintings and one sculpture <span class="No-Break">as input.</span></p>
<p>The complete spectrum of tools that AI has enabled is beyond the scope of this chapter. However, we will discuss three practical tools that may be useful if an artist group contacts us to help them build their NFT collection: colorizing, transfer style, and prompt generative art. We will go from edits that do not modify the content and progress to full creation of images. Finally, we will learn how to create a collection on the blockchain and list it <span class="No-Break">for sale.</span></p>
<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Creating with colors – <span class="No-Break">colorization tool</span></li>
<li>Creating with style – style <span class="No-Break">transfer workflow</span></li>
<li>Creating with prompts – <span class="No-Break">text-to-image solutions</span></li>
<li>Monetization – minting and <span class="No-Break">selling NFTs</span></li>
</ul>
<h1 id="_idParaDest-162">Technical requirements<a id="_idTextAnchor272"/></h1>
<p>In this chapter, we’ll<a id="_idTextAnchor273"/> employ distinct tools for each section. For the <em class="italic">colorization</em> segment, we will work with a program named <strong class="old">Style2Paints</strong>. Its documentation can be located at <a href="https://github.com/lllyasviel/style2paints">https://github.com/lllyasviel/style2paints</a> (licensed under Apache 2.0). To download the program, simply go to <a href="https://drive.google.com/open?id=1gmg2wwNIp4qMzxqP12SbcmVAHsLt1iRE">https://drive.google.com/open?id=1gmg2wwNIp4qMzxqP12SbcmVAHsLt1iRE</a>. This will download a <strong class="source-inline">.zip</strong> file onto your computer that needs to <span class="No-Break">be extracted.</span><a id="_idTextAnchor274"/></p>
<p>Moving to the <em class="italic">style transfer</em> segment, we will use a VGG19 model, whose documentation can be found at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19">https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19</a>. It follows the Keras example available <span class="No-Break">at </span><a href="https://keras.io/examples/generative/neural_style_transfer/"><span class="No-Break">https://keras.io/examples/generative/neural_style_transfer/</span></a><span class="No-Break">.</span></p>
<p>For the <em class="italic">text-to-image</em> segment, we will interact with a Leonardo AI platform for which we only need to create an account. Furthermore, we will interact with the OpenSea platform, which will require us to have an active wallet for <span class="No-Break">minting purposes.</span></p>
<p>You can find all the data and code files for this chapter in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter09">https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter09</a>. We recommend that you read through the code files in the <strong class="source-inline">Chapter09</strong> folder to follow along. The NFT collection created in this chapter is accessible <span class="No-Break">at </span><a href="https://opensea.io/collection/mysterious-library"><span class="No-Break">https://opensea.io/collection/mysterious-library</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor275"/>Creating with colors – colorizing</h1>
<p>Colorizing an<a id="_idIndexMarker528"/> image<a id="_idIndexMarker529"/> involves a lot of work for the artistic team. As data scientists, we can assist them with a tool that allows us to paint easily while following their artistic direction. The tool we’re referring to is named <strong class="old">Style2Paints</strong>, a <a id="_idIndexMarker530"/>semi-automatic method for colorization that can produce automatic results when there is no need for color correction. It also provides a functionality to provide hints to the tool for more <span class="No-Break">customized results.</span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor276"/>Hands-on Style2Paints</h2>
<p>Once Style2Paints <a id="_idIndexMarker531"/>has been installed, the main page looks like what’s shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em>. There’s a color style column on the left-hand side and a color palette on <span class="No-Break">the right:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 9.1 – Style2Paints main view" height="871" src="image/B19446_09_01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Style2Paints main view</p>
<p>This tool can be used with color and black-and-white images. Follow <span class="No-Break">these steps:</span></p>
<ol>
<li>To upload an image for colorization, click on the        <span class="No-Break">symbol.</span><div class="IMG---Figure" id="_idContainer121"><img alt="" height="48" role="presentation" src="image/Icon_1.jpg" width="54"/></div></li>
<li>Select the painting region of the image and <span class="No-Break">click </span><span class="No-Break"><strong class="old">OK</strong></span><span class="No-Break">.</span></li>
<li>On the left, we will be offered a list of images that have already been pre-colored that can be clicked and downloaded. For instance, if we upload a basic sketch or “line art,” the tool will suggest some color styles located at the left-hand side of <span class="No-Break">the site.</span></li>
</ol>
<p>Consider the <a id="_idIndexMarker532"/>following line <span class="No-Break">art example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 9.2 – Book binding machine, Joseph William Zaehnsdorf, public domain, via Wikimedia Commons" height="502" src="image/B19446_09_02.jpg" width="426"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Book binding machine, Joseph William Zaehnsdorf, public domain, via Wikimedia Commons</p>
<p>By using these color styles, we can create eight different colored images of the book binder from a single black-and-white image just by clicking on a <span class="No-Break">color combination:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 9.3 – Colorized versions of book binding" height="1032" src="image/B19446_09_03.jpg" width="740"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Colorized versions of book binding</p>
<p>It is also possible <a id="_idIndexMarker533"/>to edit images that already have some color. For example, let’s consider Artsy <span class="No-Break">Monke #9937:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 9.4 – Artsy Monke #9937" height="556" src="image/B19446_09_04.jpg" width="556"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Artsy Monke #9937</p>
<p>We can easily<a id="_idIndexMarker534"/> change the colors that are used by the image by using the color style offering located on the left-hand side of the tool. By clicking on each color combination, the images change. Some examples can be seen in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 9.5 – Colorized Artsy Monke #9937" height="189" src="image/B19446_09_05.jpg" width="723"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Colorized Artsy Monke #9937</p>
<p>It is also possible to manually colorize without the color style suggestions and use a color palette with “hint points,” as the documentation names it. A use case for hints is keeping a certain aesthetic the same or correcting some of the color style suggestions Follow <span class="No-Break">these steps:.</span></p>
<ol>
<li>Select one of the colors on the right-hand side of the tool by clicking <span class="No-Break">on it.</span></li>
<li>Add a dot to the part of the image we want to colorize with the selected color. This is <span class="No-Break">a “hint.”</span></li>
<li>Click on the         icon; the image will reload, painting the selected area with the color <span class="No-Break">we chose.</span><div class="IMG---Figure" id="_idContainer126"><img alt="" height="52" role="presentation" src="image/Icon_2.jpg" width="62"/></div></li>
</ol>
<p>A step-by-step tutorial on how to <a id="_idIndexMarker535"/>use this intuitive tool can be found in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor277"/>Theory</h2>
<p>A <strong class="old">convolutional neural network</strong> (<strong class="old">CNN</strong>) is a <a id="_idIndexMarker536"/>specialized type of deep neural network that’s designed <a id="_idIndexMarker537"/>primarily for analyzing visual data. At a high level, CNNs are inspired by how the human visual system processes information. They consist of layers that automatically learn and detect various features, such as edges, corners, textures, and more complex patterns, from raw pixel data. These learned features are then used for tasks such as image classification, object detection, facial recognition, <span class="No-Break">and more.</span></p>
<p>The following are the key components of <span class="No-Break">a CNN:</span></p>
<ul>
<li><strong class="old">Convolutional layer</strong>: This<a id="_idIndexMarker538"/> is the core of a CNN. It applies a set of learnable filters (also called kernels) to the input image. The layer identifies the distinct features of an image in a process known as <span class="No-Break">feature extraction.</span></li>
<li><strong class="old">Pooling layer</strong>: This<a id="_idIndexMarker539"/> layer reduces the spatial dimensions of the feature maps while retaining important information. There are two types of pooling: max pooling and average pooling. It is usually applied after the convolutional layer to reduce the size of the feature map that was created in the previous layer. After several convolutional and pooling layers, the feature maps are flattened into a one-dimensional vector, which serves as the input to the fully <span class="No-Break">connected layers.</span></li>
<li><strong class="old">Fully connected layers</strong>: These<a id="_idIndexMarker540"/> layers are similar to those in traditional neural networks, connecting <span class="No-Break">separate layers.</span></li>
</ul>
<p>The components we’ve just detailed can be visualized in order in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 9.6 – Structure of a CNN. Photo by Alison Wang in Unsplash" height="617" src="image/B19446_09_06.jpg" width="1359"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Structure of a CNN. Photo by Alison Wang in Unsplash</p>
<p>CNNs are trained <a id="_idIndexMarker541"/>using labeled datasets. During training, the network’s parameters (weights and biases) are updated using optimization algorithms such as gradient descent to minimize a loss function that quantifies the difference between predicted and <span class="No-Break">actual labels.</span></p>
<p>The Style2Paints model is based on a CNN framework trained with the Danbooru database, which has two parts: the draft and refinement processes. According to the <em class="italic">Two-stage Sketch Colorization</em> paper, “<em class="italic">The first drafting stage aggressively splashes colors over the canvas to create a color draft, with the goal of enriching the color variety (…) The second refinement stage corrects the color mistakes, refines details and polishes blurry textures to achieve the final output</em>.” This neural network has been trained to work with color sketches that, by definition, lack some important information, such as shades <span class="No-Break">or textures.</span></p>
<p>It uses <strong class="old">generative adversarial networks</strong> (<strong class="old">GANs</strong>), a <a id="_idIndexMarker542"/>type of CNN for generative modeling. This type of neural network works with two sub-models: a generator and a discriminator. The generator performs an unsupervised task, summarizing the distribution of the training dataset (generally images) and generating synthetic replicas to be analyzed by the discriminator. The discriminator receives the replicas, combined with some samples of the training dataset, and performs a supervised task, classifying between real (the ground truth sample) and fake (the generated by the generator). The model is considered trained when the discriminator cannot identify a generated image from a ground truth one. The generator is then kept to generate new samples of the <span class="No-Break">problem domain.</span></p>
<p>The <a id="_idIndexMarker543"/>training process can be seen in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 9.7 – Steps in the training process" height="1151" src="image/B19446_09_07.jpg" width="890"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Steps in the training process</p>
<p>This can be seen as<a id="_idIndexMarker544"/> two sub-models competing against each other and getting better at generating and discriminating. That is why the word “adversarial” is in its name. An overview of this structure can be found in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
<p>Training GANs require large datasets and a lot of GPU. Videos from the Washington University of Saint Louis have been included in the <em class="italic">Further reading</em> section if you are interested in training your <span class="No-Break">own GAN.</span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor278"/>A note on training datasets</h2>
<p>Better results<a id="_idIndexMarker545"/> will be yielded if the model that’s being used has been trained with the same image style that we are trying to reproduce. For example, if we want to paint with a photographic style, we may try to avoid using <span class="No-Break">anime-trained models.</span></p>
<p>As anticipated, Style2Paints was trained with the<a id="_idIndexMarker546"/> Danbooru dataset, which is a tagged anime dataset that has been evolving and expanding over time. Style2Paints was trained on the 2018 version, but at the time of writing, there is a 2021 version. This dataset contains images accompanied by a JSON file with metadata and tags. The anime art has some common characteristics, such as big expressive eyes with vibrant colors, heightened expressions, and a varied color palette to reflect the atmosphere present in <span class="No-Break">the images.</span></p>
<p>The following are some commonly used <span class="No-Break">image datasets:</span></p>
<ul>
<li><strong class="old">ImageNet</strong>: This <a id="_idIndexMarker547"/>is a compilation of images that follows <a id="_idIndexMarker548"/>the WordNet hierarchy. Each relevant concept in the WordNet collection is a “synonym set” that forms relations with other synsets, establishing a hierarchy of concepts: from general to abstract and specific. The ImageNet project is trying to provide 1,000 images per synset. This dataset is useful for object classification tasks. For more information, <span class="No-Break">visit </span><a href="https://www.image-net.org/"><span class="No-Break">https://www.image-net.org/</span></a><span class="No-Break">.</span></li>
<li><strong class="old">Common Objects in Context</strong> (<strong class="old">COCO</strong>): This is a large-scale dataset that’s been annotated<a id="_idIndexMarker549"/> for object detection <a id="_idIndexMarker550"/>tasks. It contains over 33,000 images, organized into directories, and the annotations are in JSON format and contain the objects and the bounding box coordinates. For more information, <span class="No-Break">visit </span><a href="https://cocodataset.org/#home."><span class="No-Break">https://cocodataset.org/#home.</span></a></li>
<li><strong class="old">MPII Human Pose Database</strong>: This <a id="_idIndexMarker551"/>dataset <a id="_idIndexMarker552"/>has been prepared for use in human pose estimation tasks. It contains approximately 25,000 images reflecting over 410 everyday human activities. For more information, <span class="No-Break">visit </span><a href="http://human-pose.mpi-inf.mpg.de/"><span class="No-Break">http://human-pose.mpi-inf.mpg.de/</span></a><span class="No-Break">.</span></li>
<li><strong class="old">Frames Labeled in Cinema</strong>: This dataset contains images that have been extracted <a id="_idIndexMarker553"/>from <a id="_idIndexMarker554"/>popular Hollywood movies. The images went through multiple processes (from selection to cleaning) before undergoing a final manual review and annotation of body joints. For more information, <span class="No-Break">visit </span><a href="https://bensapp.github.io/flic-dataset.xhtml"><span class="No-Break">https://bensapp.github.io/flic-dataset.xhtml</span></a><span class="No-Break">.</span></li>
<li><strong class="old">Caltech-UCSD Birds-200-2011</strong>: This<a id="_idIndexMarker555"/> dataset<a id="_idIndexMarker556"/> contains close to 12,000 images of 200 categories of birds with test and train subsets. Each image has detailed annotations, including one subcategory label, 15 part locations, 312 binary attributes, and one bounding box. The dataset is available on TensorFlow. For more information, <span class="No-Break">visit </span><a href="https://www.vision.caltech.edu/datasets/cub_200_2011/"><span class="No-Break">https://www.vision.caltech.edu/datasets/cub_200_2011/</span></a><span class="No-Break">.</span></li>
<li><strong class="old">Laion-5b</strong>: This<a id="_idIndexMarker557"/> is the <a id="_idIndexMarker558"/>dataset over which Stable Diffusion (something we’ll review shortly) was trained. It contains 5.85 billion CLIP-filtered image-text pairs via a general crawl of the internet that was done by the German entity LAION. For more information, <span class="No-Break">visit </span><a href="https://laion.ai/blog/laion-5b/"><span class="No-Break">https://laion.ai/blog/laion-5b/</span></a><span class="No-Break">.</span></li>
</ul>
<p>In this section, we<a id="_idIndexMarker559"/> learned how to use a tool that helps in the coloring workflow, automatically or manually. In the following section, we will dig deeper into coloring with a broader impact on the image by transferring a style from one image to the other <span class="No-Break">with AI.</span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor279"/>Creating with style – style transfer</h1>
<p>Another way <a id="_idIndexMarker560"/>we can assist an artistic team<a id="_idIndexMarker561"/> is via style transfer, a process that involves combining <span class="No-Break">two images:</span></p>
<ul>
<li>The style image or <em class="italic">root</em> image, from which we will learn <span class="No-Break">the style</span></li>
<li>The target image, which we will transform with the <span class="No-Break">new style</span></li>
</ul>
<p>The resulting image will retain the core elements of the target image but appear to be painted or printed following the <span class="No-Break">style image.</span></p>
<p>There are several<a id="_idIndexMarker562"/> methods for performing style transfer, including leveraging GANs (described in the previous section), using <strong class="old">Visual Geometry Group</strong> (<strong class="old">VGG</strong>), and employing Stable Diffusion (which we will cover in the <span class="No-Break">next section).</span></p>
<p>In <strong class="source-inline">style_transfer.ipynb</strong>, we will use VGG19, a special type of CNN with 19 layers that has been trained with over a million images from the ImageNet database to extract the style of a Picasso painting and transfer it to a photograph. Picasso belonged to the <strong class="old">cubism</strong> movement, where<a id="_idIndexMarker563"/> the artists applied multiple perspectives, used <a id="_idIndexMarker564"/>geometric shapes, and flattened the picture plane. An interesting article on the defining characteristics of this artistic movement can be found in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
<p>Let’s go through the steps we <span class="No-Break">must follow.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor280"/>Preparation</h2>
<p>First, we <a id="_idIndexMarker565"/>must obtain the tensor representations of the root and target images. The <strong class="source-inline">preprocess_image()</strong> function does this by leveraging the Keras library with the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
def preprocess_image(image_path):
    img = keras.preprocessing.image.load_img(
        image_path, target_size=(img_nrows, img_ncols)
    )
    img = keras.preprocessing.image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return tf.convert_to_tensor(img)</pre> <h2 id="_idParaDest-169"><a id="_idTextAnchor281"/>Model building</h2>
<p>We<a id="_idIndexMarker566"/> build the VGG19 model by setting the ImageNet dataset weights, which means the model will be initialized with weights that have been pre-trained on the ImageNet dataset. The <strong class="source-inline">include_top</strong> parameter is set to <strong class="source-inline">False</strong>, which means the top layers that are responsible for classification are not included in the model. The reason is that we want to use the VGG19 model as a feature extractor rather than for <span class="No-Break">classification purposes:</span></p>
<pre class="console">
model = vgg19.VGG19(weights="imagenet", include_top=False)</pre> <p>The code also extracts the information that’s generated by each layer of the model so that it can be used in the loss functions that we’ll <span class="No-Break">describe here.</span></p>
<p>We define three <span class="No-Break">loss functions:</span></p>
<ul>
<li>The <strong class="old">total variation loss</strong>, which <a id="_idIndexMarker567"/>seeks to ensure the coherence of the final image by measuring the spatial continuity between pixels in the <span class="No-Break">resulting image.</span></li>
<li>The <strong class="old">content loss</strong>, which<a id="_idIndexMarker568"/> aims to minimize the difference between the target image and the resulting image, keeping the composition and high-level representation of the target image and the resulting image similar. The layer over which we calculate this loss is <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">content_layer_name</strong></span><span class="No-Break">.</span></li>
<li>The <strong class="old">style loss</strong>, which<a id="_idIndexMarker569"/> seeks to minimize the difference between the style image and the resulting image. As defined in the Keras documentation, it is “<em class="italic">the sum of L2 distances between the Gram matrices of the representations of the base image and the style reference image</em>.” The style loss focuses on information related to color distribution, texture, brush strokes, and more. The layers over which we calculate this loss are defined in a list <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">style_layer_names</strong></span><span class="No-Break">.</span></li>
</ul>
<p>The style loss<a id="_idIndexMarker570"/> uses a gram matrix (which is essentially a tensor multiplied by its transpose) and is calculated in the <strong class="source-inline">gram_matrix()</strong> function. The rationale behind the gram matrix of a convolutional layer is to combine the style features that are learned among them. For instance, Pablo Picasso's cubism is a combination of colors, shapes, and textures. A synthesis (the gram) of those features measuring the correlation between them will represent <span class="No-Break">Picasso’s style.</span></p>
<p>The <strong class="source-inline">compute_loss</strong> function summarizes the combination of the various losses defined previously, while <strong class="source-inline">compute_loss_and_grads</strong> runs <span class="No-Break">the calculations.</span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor282"/>Training and inference</h2>
<p>The training <a id="_idIndexMarker571"/>process will reduce the style loss and the content loss, which make up the total variation loss. The training <a id="_idIndexMarker572"/>process uses <strong class="old">stochastic gradient descent</strong> (<strong class="old">SGD</strong>) as the optimizer to iteratively decrease <span class="No-Break">the loss.</span></p>
<p>The proposed script saves the image every 100 iterations, allowing us to monitor image variation. The documentation proposes displaying the final image at the end of the training process, which we set at <span class="No-Break">4,000 steps.</span></p>
<p>By using <a id="_idIndexMarker573"/>the <strong class="source-inline">util</strong> function in the notebook named <strong class="source-inline">deprocess_image()</strong>, which rebuilds the image from a tensor into a <strong class="source-inline">.png</strong> file so that it’s ready to be saved and displayed, we can see the style transfer of a Pablo Picasso painting to <span class="No-Break">a photograph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 9.8 – A waterfall in Picasso’s painting style" height="712" src="image/B19446_09_08.jpg" width="623"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – A waterfall in Picasso’s painting style</p>
<p>In the first section, we learned how to modify images automatically by applying color; in this section, we<a id="_idIndexMarker574"/> reviewed how to create images by combining a base image with a specific style. In both cases, we provided the images we wanted to modify. In the next section, we will learn how to create images with a text input <span class="No-Break">or prompt.</span></p>
<h1 id="_idParaDest-171">C<a id="_idTextAnchor283"/>reating with prompts – text to image</h1>
<p>In this section, we <a id="_idIndexMarker575"/>will introduce some services that enable images to be generated based on a <strong class="old">prompt</strong>. A <a id="_idIndexMarker576"/>prompt is a set of natural language instructions that, when fed to the model, generates images. Whatever can be described in words can be transformed into an image. The more descriptive the prompt is, the more unique the output will be. The instructions can include some keywords that will enhance the originality of the created pieces of art, such as the style of the generated image, the aspect ratio, the resolution of the expected images, <span class="No-Break">and more.</span></p>
<p>All of the services we will present use some<a id="_idIndexMarker577"/> form of <strong class="old">diffusion models</strong>, combined with other models to make the image generation process more efficient, clean it from disturbing results (for example, for minor 18), and more. Diffusion models are generative models that try to replicate the training data. During training, the model adds noise to the training dataset and learns how to reverse it to recover the original image. By recovering from the training noise, the model learns the fundamental aspects of an image, as well as how to generate <span class="No-Break">new data.</span></p>
<p>Let’s briefly analyze them one <span class="No-Break">by one.</span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor284"/>DALL.E 2</h2>
<p>OpenAI, the<a id="_idIndexMarker578"/> same research team that developed ChatGPT, developed DALL-E 2, an image generator from text descriptions. According to their documentation, “<em class="italic">DALL·E is a transformer language model. It receives both the text and the image as a single stream of data containing up to 1280 tokens, and is trained using maximum likelihood to generate all of the tokens, one after another</em>.” The model is a 12-billion parameter autoregressive transformer that’s trained on 250 million image-text pairs collected from <span class="No-Break">the internet.</span></p>
<p>DALL-E 2 not only generates images according to a predefined prompt but also enables the user to modify parts of the image or add contextual background to smaller pieces <span class="No-Break">of images.</span></p>
<p>This same <a id="_idIndexMarker579"/>team also designed <strong class="old">contrastive language image pre-training</strong> (<strong class="old">CLIP</strong>), which<a id="_idIndexMarker580"/> allows us to map texts with images and returns the most appropriate caption for an input image. More information can be found at <a href="https://openai.com/research/clip">https://openai.com/research/clip</a>. This takes image tagging and categorization to another level <span class="No-Break">of speed.</span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor285"/>Stable Diffusion</h2>
<p>Stable Diffusion models <a id="_idIndexMarker581"/>are open source and contain code and checkpoints. To enable Stable Diffusion models to be trained on low GPU, they do not train on images but rather on the latent space of the dataset. The models learn from the underlying structure of the dataset instead of processing each image. Training with latent space enables us to feed the model with text and images in the same space that the model will use to regenerate <span class="No-Break">the image.</span></p>
<p>The CLIP model mentioned in the previous section helped train the latest version of Stable Diffusion V2. The link to the repository <span class="No-Break">is </span><a href="https://github.com/Stability-AI/stablediffusion"><span class="No-Break">https://github.com/Stability-AI/stablediffusion</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor286"/>Midjourney</h2>
<p>The base models<a id="_idIndexMarker582"/> that are used by Midjourney are not disclosed, but they are likely a combination of diffusion models, as explained for DALL-E and Stable Diffusion. Midjourney is currently only accessible through a Discord bot on their official Discord server or by inviting the bot to a third-party server. It has <span class="No-Break">no API.</span></p>
<p>This service became popular <span class="No-Break">very fast.</span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor287"/>Leonardo.Ai</h2>
<p>The link to <a id="_idIndexMarker583"/>their page is <a href="https://app.leonardo.ai/">https://app.leonardo.ai/</a>. This tool offers off-the-shelf <a id="_idIndexMarker584"/>models that generate images specifically trained in some of the most common themes, such<a id="_idIndexMarker585"/> as <strong class="old">role-playing games</strong> (<strong class="old">RPGs</strong>) or realistic photographs. It also offers tools to fine-tune models so that they can be adapted to our training datasets and a liberal free tier. Finally, it is developer-friendly with an easy-to-interact API. Each model has a “base model” description based on Stable <span class="No-Break">Diffusion releases.</span></p>
<p>To get started, sign <a id="_idIndexMarker586"/>up on their app and complete the <strong class="old">Get started</strong> survey. There is no need to pay to interact with the basic service, but you must do so to get <span class="No-Break">API keys:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 9.9 – Logging in to Leonardo.Ai for the first time" height="827" src="image/B19446_09_09.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.<a id="_idTextAnchor288"/>9 – Logging in to Leonardo.Ai for the first time</p>
<p>In the home page view, there is a list of models that we can interact with, depending on the type of image we want to generate. As mentioned earlier, each model is optimized for a specific purpose. For example, there is a model specially designed for vintage-style photography or RPG <span class="No-Break">character portraits.</span></p>
<p>We can also see two tabs called <strong class="old">Community Feed</strong> and <strong class="old">Personal Feed</strong>, which show images generated by the community and our <span class="No-Break">images, respectively.</span></p>
<p>If we move to the AI generation tool, we will see several options to choose from on the left-hand side of the view, including <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="old">Number of images</strong>: This allows us to select the number of images we want to generate with <span class="No-Break">each run.</span></li>
<li><strong class="old">Prompt magic</strong>: According to their description, “<em class="italic">Our experimental render pipeline may have better prompt adherence</em>.” The images are more artistic with <span class="No-Break">this enabled.</span></li>
<li><strong class="old">Prompt magic strength</strong>: This option determines the weight of the render <span class="No-Break">detailed previously.</span></li>
<li><strong class="old">Public images</strong>: This option allows us to choose whether we want to share these images with the <span class="No-Break">public feed.</span></li>
<li><strong class="old">Image dimensions</strong>: This option lets us set the size of <span class="No-Break">the images.</span></li>
<li><strong class="old">Guidance scale</strong>: This option determines the weight of the prompt in the final image. It is suggested to keep it at 7 <span class="No-Break">or 8.</span></li>
</ul>
<p>We can also<a id="_idIndexMarker587"/> upload an image to be used as <span class="No-Break">a prompt.</span></p>
<p class="callout-heading">A note on good prompts</p>
<p class="callout"><strong class="old">Vocabulary</strong>: Avoid <a id="_idIndexMarker588"/>the use of “very” or “super” for emphasis. Instead, opt for words that convey the same meaning. For instance, replace “very tired” <span class="No-Break">with “exhausted.”</span></p>
<p class="callout"><strong class="old">Typos</strong>: Refrain <a id="_idIndexMarker589"/>from sending incorrectly spelled words, abbreviations, contractions, or slang as the model may struggle to align them with the dataset on which it <span class="No-Break">was trained.</span></p>
<p class="callout"><strong class="old">Specificity</strong>: Minimize<a id="_idIndexMarker590"/> ambiguity in word choices and unnecessary text. For improved results, opt for expressions such as “cheeseless pizza” instead of “pizza with no cheese.” Utilize negative prompts to exclude specific objects or characteristics from <span class="No-Break">the image.</span></p>
<p class="callout"><strong class="old">Keywords to consider</strong>: Include <a id="_idIndexMarker591"/>hints about the image’s background, style words (such as anime, realistic, paper art, cubism, charcoal painting, folk art, graffiti), lighting (soft, ambient, neon, studio lights), or time of<a id="_idTextAnchor289"/> day (morning, golden <span class="No-Break">hour, midnight).</span></p>
<p>Furthermore, the application helps us with the prompt generation. Within the AI tool that we just described, we can see the <strong class="old">Prompt Generation</strong> tab, which assists us in generating the prompt to obtain the desired image <span class="No-Break">using AI.</span></p>
<h3>Hands-on with Leonardo.Ai</h3>
<p>Let’s try an <a id="_idIndexMarker592"/>exercise with the API. The documentation is available at <a href="https://docs.leonardo.ai/reference/getuserself">https://docs.leonardo.ai/reference/getuserself</a>. It is an easy-to-use API that can be accessed with our well-known <strong class="source-inline">request</strong> library. </p>
<p>The API can help us build the entire pipeline from a prompt to a folder that we can submit to the artistic team for review. The <strong class="source-inline">Leonardo_AI.ipynb</strong> file contains the workflow <span class="No-Break">we’ll explore.</span></p>
<p>Although the API is under development and not all functionalities and models can be invoked programmatically, most of the options described previously can be added as parameters to <span class="No-Break">the payload.</span></p>
<p>Let’s review the following <span class="No-Break">code snippet:</span></p>
<pre class="console">
payload = {
    "prompt": prompt,
    "modelId":"6bef9f1b-29cb-40c7-b9df-32b51c1f67d3",
    "width": 512,
    "height": 512,
    "sd_version": "v2",
    "presetStyle": "LEONARDO",
     "public": False,
    "promptMagic": True
}</pre> <p>To interact with the API, we need to log in and obtain a set of API keys that will pass as authorization in <span class="No-Break">the header.</span></p>
<p>It is important to read the parameters from the website as the documentation for this is not complete. For example, some models are trained with specific image dimensions, so it is better to<a id="_idIndexMarker593"/> input those preferred dimensions in the parameter payload. Additionally, not all the models can be called from the API, and there is no access to the <span class="No-Break">prompt generator.</span></p>
<p>Despite these limitations, this is a great tool that can help us generate high-quality <span class="No-Break">images rapidly.</span></p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor290"/>Minting an NFT collection</h1>
<p>The purpose <a id="_idIndexMarker594"/>of analyzing all the tools in this section is to create or modify images that we can sell or the artistic team we support will sell. Once we have generated the images, we want to “own” them in the Web3 sense, as explained in <a href="B19446_04.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. To achieve this, we will create a collection in <span class="No-Break">a marketplace.</span></p>
<p><strong class="old">Minting</strong> is<a id="_idIndexMarker595"/> the act of creating the digital trace of an item on the blockchain. As we saw when describing ERC 721, it means that the trace will point to a URL containing the stored image. Everything that is stored on the chain pays a <span class="No-Break">gas fee.</span></p>
<p>The concept <a id="_idIndexMarker596"/>of lazy minting has emerged rather recently. <strong class="old">Lazy minting</strong> involves <a id="_idIndexMarker597"/>authorizing the platform to mint the NFT at the moment of the NFT sale and not before. This is important because minting involves gas expenditure, and, in moments of high congestion, gas prices can be high. In addition, lazy minting helps reduce the risk of creating a collection that may not be sold high enough to cover the initial investment. At the time of writing, the main marketplaces, such as OpenSea and Rarible, offer <span class="No-Break">the service.</span></p>
<p>The process consists of the <span class="No-Break">following steps:</span></p>
<ol>
<li>Creators mint an NFT <em class="italic">lazily</em> using a specific smart contract. The smart contract will mint and sell the NFT on our behalf. We provide <span class="No-Break">the authorization.</span></li>
<li>The buyer pays a price that covers the minting costs and the NFT itself when purchasing <span class="No-Break">our NFT.</span></li>
</ol>
<p>This method defers the minting process until just before the NFT is sold, which is an incentive for creators to continue producing and showcasing their art without necessarily paying <span class="No-Break">for gas.</span></p>
<p>Let’s create a collection <span class="No-Break">on OpenSea:</span></p>
<ol>
<li>Go to <a href="https://opensea.io/">https://opensea.io/</a>. To interact with the platform, you will need <span class="No-Break">a wallet.</span></li>
<li>Connect <a id="_idIndexMarker598"/>your wallet and go to the <span class="No-Break">profile options:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 9.10 – Connecting to your wallet" height="96" src="image/B19446_09_10.jpg" width="1232"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – Connecting to your wallet</p>
<ol>
<li value="3">Click on <span class="No-Break"><strong class="old">My Collections</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 9.11 – The My Collections tab" height="788" src="image/B19446_09_11.jpg" width="324"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – The My Collections tab</p>
<ol>
<li value="4">Click on the blue <strong class="old">Create a </strong><span class="No-Break"><strong class="old">collection</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer133">
<img alt="Figure 9.12 – The My Collections page" height="223" src="image/B19446_09_12.jpg" width="783"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – The My Collections page</p>
<ol>
<li value="5">You will <a id="_idIndexMarker599"/>be offered two options: the traditional option (<strong class="old">Deploy your own contract</strong>) and the lazy minting option (<strong class="old">Use the OpenSea contract</strong>). Click on the <span class="No-Break">second option:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 9.13 – Choosing the lazy minting option" height="440" src="image/B19446_09_13.jpg" width="622"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Choosing the lazy minting option</p>
<ol>
<li value="6">A page will open where you can complete the collection’s details. Among them we will find the <span class="No-Break">following details:</span><ol><li class="upper-roman">The name of <span class="No-Break">the collection.</span></li><li class="upper-roman"><span class="No-Break">Description.</span></li><li class="upper-roman">The <span class="No-Break">accepted currency.</span></li><li class="upper-roman">Images for the <span class="No-Break">collection page.</span></li><li class="upper-roman">Author earnings. As we mentioned in <a href="B19446_04.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, it is possible to establish a percentage that the creator will retain each time the NFT <span class="No-Break">is sold.</span></li></ol></li>
</ol>
<p>Once all the <a id="_idIndexMarker600"/>required details have been filled in, click <strong class="old">Save</strong>. Now that we have a collection, we have to add pieces of art <span class="No-Break">to it:</span></p>
<ol>
<li>Go back to your profile and <span class="No-Break">click </span><span class="No-Break"><strong class="old">Create</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 9.14 – Creating an NFT" height="797" src="image/B19446_09_14.jpg" width="341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – Creating an NFT</p>
<ol>
<li value="2">You may require additional authorization to access this panel. If so, you will be taken to the <strong class="old">Create New Item</strong> <span class="No-Break">page: </span><a href="https://opensea.io/asset/create"><span class="No-Break">https://opensea.io/asset/create</span></a><span class="No-Break">.</span></li>
<li>Upload the image, video, or audio that you want to mint. In this example, we will mint one of the images that was generated with the <span class="No-Break"><strong class="source-inline">Leonardo_AI.ipynb</strong></span><span class="No-Break"> notebook:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 9.15 – Details of the new item" height="625" src="image/B19446_09_15.jpg" width="776"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – Details of the new item</p>
<ol>
<li value="4">Start filling<a id="_idIndexMarker601"/> in the <span class="No-Break">required fields:</span><ol><li class="upper-roman"><span class="No-Break">Name</span></li><li class="upper-roman"><span class="No-Break">Description</span></li><li class="upper-roman">Connect it to the collection that was created in <em class="italic">step </em><span class="No-Break"><em class="italic">4 previously</em></span></li><li class="upper-roman">Identify the number of items of the same nature that can <span class="No-Break">be minted</span></li><li class="upper-roman">The network where this item <span class="No-Break">will live</span></li></ol><p class="list-inset">Click <strong class="old">Create</strong>; you will see the <span class="No-Break">following output:</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 9.16 – Resulting message after signing the creation" height="634" src="image/B19446_09_16.jpg" width="639"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fig<a id="_idTextAnchor291"/>ure 9.16 – Resulting message after signing the creation</p>
<ol>
<li value="5">Enable the sale. To do this, navigate to the page of the item and click on the <span class="No-Break"><strong class="old">Sell</strong></span><span class="No-Break"> button:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 9.17 – Sale details" height="245" src="image/B19446_09_17.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – Sale details</p>
<p class="list-inset">New <a id="_idIndexMarker602"/>options will appear for you to choose from. It is possible to choose between a fixed price or an auction with a limited time. If we want to sell for a fixed price, we can follow the <span class="No-Break">next steps.</span></p>
<ol>
<li value="6">Click on <span class="No-Break"><strong class="old">Fixed price</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 9.18 – The Fixed price option" height="302" src="image/B19446_09_18.jpg" width="635"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – The Fixed price option</p>
<ol>
<li value="7">Set a price <a id="_idIndexMarker603"/>in any of the coins or tokens that are acceptable according to what we decided when creating <span class="No-Break">the collection.</span></li>
<li>Below this, we will find a summary of earnings and the fee that OpenSea charges (at the time of writing, this <span class="No-Break">is 2.5%).</span></li>
<li>If we agree, we can click on <strong class="old">Complete listing</strong>. To approve the listing, OpenSea requires <span class="No-Break">our signature.</span></li>
<li>Once signed, we will receive a new notification, informing us that the item has been listed. Now, it is out there in the marketplace and can <span class="No-Break">be purchased!</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 9.19 – The listing for sale has been enabled" height="687" src="image/B19446_09_19.jpg" width="802"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – The listing for sale has been enabled</p>
<p>All the illustrations that were generated in this section are available in The Mysterious Library <span class="No-Break">Collection: </span><a href="https://opensea.io/collection/mysterious-library"><span class="No-Break">https://opensea.io/collection/mysterious-library</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor292"/>Summary</h1>
<p>Throughout this chapter, we explored three different approaches to using AI tools in artistic projects. We examined the use of GAN models for colorizing sketches, explored the VGG19 model for transferring style, and discovered state-of-the-art applications of Stable Diffusion models for generating art based <span class="No-Break">on prompts.</span></p>
<p>Moreover, we learned about the entire workflow, from the finished piece of art to listing the final image on a marketplace. By combining the power of AI and blockchain technology, we now have a range of new opportunities to explore and monetize artistic work in exciting and <span class="No-Break">innovative ways.</span></p>
<p>It is worth noting that questions have arisen concerning the ownership of images generated using AI. This arises from the possibility that these models may have been trained on copyrighted pieces of art without the artist’s consent. Respondents contend that the transformative nature of the model’s outputs, coupled with the fair use argument, could potentially dismiss such accusations. This ongoing issue is yet to be definitively resolved by legal authorities and is likely to exhibit variations across <span class="No-Break">different countries.</span></p>
<p>Having explored the area of NFTs, we’ll now shift our focus to a critical aspect that underpins the integrity of this innovative landscape – fraud detection. In the following chapter, we will analyze another use case where machine learning can help us uncover anomalies and increase <span class="No-Break">transaction security.</span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor293"/>Further reading</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li>Crypto <span class="No-Break">Grims: </span><a href="https://twitter.com/cryptogrims"><span class="No-Break">https://twitter.com/cryptogrims</span></a></li>
<li>Artsy Monke <span class="No-Break">collection: </span><a href="https://opensea.io/assets/ethereum/0xa4bcd3b7f141ba1f08f36033fdfce691565561bc"><span class="No-Break">https://opensea.io/assets/ethereum/0xa4bcd3b7f141ba1f08f36033fdfce691565561bc</span></a><span class="No-Break">.</span></li>
<li>Mishra, M. (2020, September 2). <em class="italic">Convolutional neural networks, explained</em>. Medium. Available <span class="No-Break">at </span><a href="https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939"><span class="No-Break">https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939</span></a><span class="No-Break">.</span></li>
<li>Fortis, S. (n.d.). <em class="italic">Google AI turns all 10,000 BAYC NFTs into machine-made art</em>. Cointelegraph. Available <span class="No-Break">at </span><a href="https://cointelegraph.com/news/google-ai-turns-all-10-000-bayc-nfts-into-machine-made-art"><span class="No-Break">https://cointelegraph.com/news/google-ai-turns-all-10-000-bayc-nfts-into-machine-made-art</span></a><span class="No-Break">.</span></li>
<li>Lllyasviel (n.d.). <em class="italic">style2paints.github.io</em>. Available <span class="No-Break">at </span><a href="https://style2paints.github.io/"><span class="No-Break">https://style2paints.github.io/</span></a><span class="No-Break">.</span></li>
<li>Lllyasviel/style2paints. (n.d.). GitHub. Available <span class="No-Break">at </span><a href="https://github.com/lllyasviel/style2paints"><span class="No-Break">https://github.com/lllyasviel/style2paints</span></a><span class="No-Break">.</span></li>
<li>Tang, J. (2020, October 20). <em class="italic">Attempt to understand an all-star auto-color project—Style2Paints (Part 1)</em>. Medium. Available <span class="No-Break">at </span><a href="https://medium.com/ai-innovation/attempt-to-understand-an-all-star-auto-color-project-style2paints-part-1-84d2e3d96da"><span class="No-Break">https://medium.com/ai-innovation/attempt-to-understand-an-all-star-auto-color-project-style2paints-part-1-84d2e3d96da</span></a><span class="No-Break">.</span></li>
<li>Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and Chunping Liu. (n.d.). <em class="italic">CUHK Computer Science and Engineering</em>. Available <span class="No-Break">at </span><a href="https://www.cse.cuhk.edu.hk/~ttwong/papers/colorize/colorize.pdf"><span class="No-Break">https://www.cse.cuhk.edu.hk/~ttwong/papers/colorize/colorize.pdf</span></a><span class="No-Break">.</span></li>
<li>Nerdy Rodent. (2020, November 19). <em class="italic">Style2Paints – Easily colour any line art using AI</em> [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=cvN9oQfC3w0"><span class="No-Break">https://www.youtube.com/watch?v=cvN9oQfC3w0</span></a><span class="No-Break">.</span></li>
<li>Overview of GAN structure. (n.d.). <em class="italic">Google for Developers</em>. Available <span class="No-Break">at </span><a href="https://developers.google.com/machine-learning/gan/gan_structure"><span class="No-Break">https://developers.google.com/machine-learning/gan/gan_structure</span></a><span class="No-Break">.</span></li>
<li>Prof. Jeff Heaton – Washington University of St. Louis. (2022, January 19). <em class="italic">Introduction to GANS for Image and Data Generation (7.1)</em> [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=hZw-AjbdN5k"><span class="No-Break">https://www.youtube.com/watch?v=hZw-AjbdN5k</span></a><span class="No-Break">.</span></li>
<li>Prof. Jeff Heaton – Washington University of St. Louis. (2021, February 17). <em class="italic">Training a GAN from your Own Images: StyleGAN2 ADA</em> [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=kbDd5lW6rkM"><span class="No-Break">https://www.youtube.com/watch?v=kbDd5lW6rkM</span></a><span class="No-Break">.</span></li>
<li>Prof. Jeff Heaton – Washington University of St. Louis. (2021, May 12). <em class="italic">Training NVIDIA StyleGAN2 ADA under Colab Free and Colab Pro Tricks</em> [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=L3JLzoe-dJU"><span class="No-Break">https://www.youtube.com/watch?v=L3JLzoe-dJU</span></a><span class="No-Break">.</span></li>
<li><em class="italic">T81_558_deep_learning/t81_558_class_07_1_gan_intro.ipynb at master · jeffheaton/t81_558_deep_learning</em>. (n.d.). GitHub. Available <span class="No-Break">at </span><a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb"><span class="No-Break">https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb</span></a><span class="No-Break">.</span></li>
<li><em class="italic">T81_558_deep_learning/t81_558_class_07_2_train_gan.ipynb at master · jeffheaton/t81_558_deep_learning</em>. (n.d.). GitHub. Available <span class="No-Break">at </span><a href="https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb"><span class="No-Break">https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb</span></a><span class="No-Break">.</span></li>
<li>Jason Brownlee PhD. (2019, July 19). <em class="italic">Machine Learning Mastery</em>. Machine Learning Mastery. Available <span class="No-Break">at </span><a href="https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/"><span class="No-Break">https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/</span></a><span class="No-Break">.</span></li>
<li><em class="italic">4 characteristics of cubism and why they are important</em>. (n.d.). Artlex – Art Dictionary and Encyclopedia. Available <span class="No-Break">at </span><a href="https://www.artlex.com/art-movements/cubism/characteristics/"><span class="No-Break">https://www.artlex.com/art-movements/cubism/characteristics/</span></a><span class="No-Break">.</span></li>
<li><em class="italic">Neural style </em><span class="No-Break"><em class="italic">transfer</em></span><span class="No-Break">: </span><a href="https://keras.io/examples/generative/neural_style_transfer/"><span class="No-Break">https://keras.io/examples/generative/neural_style_transfer/</span></a><span class="No-Break">.</span></li>
<li><em class="italic">DALL·E: Creating images from text</em>. (n.d.). OpenAI. Available <span class="No-Break">at </span><a href="https://openai.com/research/dall-e"><span class="No-Break">https://openai.com/research/dall-e</span></a><span class="No-Break">.</span></li>
<li><em class="italic">Zero-shot text-to-Image generation</em>. (n.d.). arXiv.org. Available <span class="No-Break">at </span><a href="https://arxiv.org/abs/2102.12092"><span class="No-Break">https://arxiv.org/abs/2102.12092</span></a><span class="No-Break">.</span></li>
<li>Aleksa Gordić - The AI Epiphany. (2022, September 1). <em class="italic">Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models | ML Coding Series</em> [Video]. YouTube. Available <span class="No-Break">at </span><a href="https://www.youtube.com/watch?v=f6PtJKdey8E"><span class="No-Break">https://www.youtube.com/watch?v=f6PtJKdey8E</span></a><span class="No-Break">.</span></li>
<li><em class="italic">Stability-AI/stablediffusion</em>. (n.d.). GitHub. Available <span class="No-Break">at </span><a href="https://github.com/Stability-AI/stablediffusion"><span class="No-Break">https://github.com/Stability-AI/stablediffusion</span></a><span class="No-Break">.</span></li>
<li><em class="italic">How Stable Diffusion works? Latent Diffusion Models Explained</em>. (2022, December 3). Louis Bouchard. Available <span class="No-Break">at </span><a href="https://www.louisbouchard.ai/latent-diffusion-models/"><span class="No-Break">https://www.louisbouchard.ai/latent-diffusion-models/</span></a><span class="No-Break">.</span></li>
<li>Arya, G. (2023, January 14). <em class="italic">Power of latent diffusion models: Revolutionizing image creation</em>. Analytics Vidhya. Available <span class="No-Break">at </span><a href="https://www.analyticsvidhya.com/blog/2023/01/power-of-latent-diffusion-models-revolutionizing-image-creation/"><span class="No-Break">https://www.analyticsvidhya.com/blog/2023/01/power-of-latent-diffusion-models-revolutionizing-image-creation/</span></a><span class="No-Break">.</span></li>
<li><em class="italic">API Documentation</em>. (n.d.). Leonardo.Ai. Available <span class="No-Break">at </span><a href="https://docs.leonardo.ai/reference/getuserself"><span class="No-Break">https://docs.leonardo.ai/reference/getuserself</span></a><span class="No-Break">.</span></li>
<li>Ashley, K. (2021). <em class="italic">Make art with artificial intelligence: Make and sell your art with AI, blockchain, </em><span class="No-Break"><em class="italic">and NFT</em></span><span class="No-Break">.</span></li>
</ul>
</div>
</div></body></html>