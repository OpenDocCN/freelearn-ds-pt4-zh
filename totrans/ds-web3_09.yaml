- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Art for NFTs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “I use data as a pigment and paint with a painting brush that is assisted by
    artificial intelligence.”
  prefs: []
  type: TYPE_NORMAL
- en: – Refik Anadol
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll take an artistic break and indulge in some creativity.
    While our previous focus was on analyzing content generated by others on the blockchain,
    in this chapter, we will be creating our own content to be added to the blockchain.
  prefs: []
  type: TYPE_NORMAL
- en: The inclusion of this chapter stems from the recognition that, as data scientists,
    we might encounter requests to produce or assist in crafting an NFT collection
    in collaboration with a group of artists. In [*Chapter 4*](B19446_04.xhtml#_idTextAnchor145),
    we studied the artistic applications of NFTs and explored notable collections,
    such as *Bored Ape*, which has a total traded volume of 978,382 ETH (approximately
    USD 1,800 million). We do not know whether they used AI to produce all the images,
    but they are a good use case of how art can be owned and traded on the blockchain.
    To be able to participate in that market, we will learn about the entire process,
    from crafting an image to listing it for sale on OpenSea.
  prefs: []
  type: TYPE_NORMAL
- en: 'One particular collection named *Artsy Monke* used AI to create images by combining
    the Bored Ape collection with 20 curated painting styles. You can find their OpenSea
    collection website at [https://opensea.io/collection/artsy-monke](https://opensea.io/collection/artsy-monke).
    The image on the cover of the book is Artsy Monke #9937.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example is Refik Anadol’s *Machine Hallucinations* collection, which
    is a collaboration with NASA that uses over two million raw images, recorded by
    space institutions such as the International Space Station, the Hubble and MRO
    telescopes across the world into six AI data-created paintings and one sculpture
    as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete spectrum of tools that AI has enabled is beyond the scope of this
    chapter. However, we will discuss three practical tools that may be useful if
    an artist group contacts us to help them build their NFT collection: colorizing,
    transfer style, and prompt generative art. We will go from edits that do not modify
    the content and progress to full creation of images. Finally, we will learn how
    to create a collection on the blockchain and list it for sale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating with colors – colorization tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating with style – style transfer workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating with prompts – text-to-image solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monetization – minting and selling NFTs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll employ distinct tools for each section. For the *colorization*
    segment, we will work with a program named `.zip` file onto your computer that
    needs to be extracted.
  prefs: []
  type: TYPE_NORMAL
- en: Moving to the *style transfer* segment, we will use a VGG19 model, whose documentation
    can be found at [https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19).
    It follows the Keras example available at [https://keras.io/examples/generative/neural_style_transfer/](https://keras.io/examples/generative/neural_style_transfer/).
  prefs: []
  type: TYPE_NORMAL
- en: For the *text-to-image* segment, we will interact with a Leonardo AI platform
    for which we only need to create an account. Furthermore, we will interact with
    the OpenSea platform, which will require us to have an active wallet for minting
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the data and code files for this chapter in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter09](https://github.com/PacktPublishing/Data-Science-for-Web3/tree/main/Chapter09).
    We recommend that you read through the code files in the `Chapter09` folder to
    follow along. The NFT collection created in this chapter is accessible at [https://opensea.io/collection/mysterious-library](https://opensea.io/collection/mysterious-library).
  prefs: []
  type: TYPE_NORMAL
- en: Creating with colors – colorizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Colorizing an image involves a lot of work for the artistic team. As data scientists,
    we can assist them with a tool that allows us to paint easily while following
    their artistic direction. The tool we’re referring to is named **Style2Paints**,
    a semi-automatic method for colorization that can produce automatic results when
    there is no need for color correction. It also provides a functionality to provide
    hints to the tool for more customized results.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on Style2Paints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once Style2Paints has been installed, the main page looks like what’s shown
    in *Figure 9**.1*. There’s a color style column on the left-hand side and a color
    palette on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Style2Paints main view](img/B19446_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Style2Paints main view
  prefs: []
  type: TYPE_NORMAL
- en: 'This tool can be used with color and black-and-white images. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To upload an image for colorization, click on the symbol.![](img/Icon_1.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the painting region of the image and click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left, we will be offered a list of images that have already been pre-colored
    that can be clicked and downloaded. For instance, if we upload a basic sketch
    or “line art,” the tool will suggest some color styles located at the left-hand
    side of the site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider the following line art example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Book binding machine, Joseph William Zaehnsdorf, public domain,
    via Wikimedia Commons](img/B19446_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Book binding machine, Joseph William Zaehnsdorf, public domain,
    via Wikimedia Commons
  prefs: []
  type: TYPE_NORMAL
- en: 'By using these color styles, we can create eight different colored images of
    the book binder from a single black-and-white image just by clicking on a color
    combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Colorized versions of book binding](img/B19446_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Colorized versions of book binding
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to edit images that already have some color. For example,
    let’s consider Artsy Monke #9937:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Artsy Monke #9937](img/B19446_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4 – Artsy Monke #9937'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily change the colors that are used by the image by using the color
    style offering located on the left-hand side of the tool. By clicking on each
    color combination, the images change. Some examples can be seen in *Figure 9**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Colorized Artsy Monke #9937](img/B19446_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 – Colorized Artsy Monke #9937'
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to manually colorize without the color style suggestions
    and use a color palette with “hint points,” as the documentation names it. A use
    case for hints is keeping a certain aesthetic the same or correcting some of the
    color style suggestions Follow these steps:.
  prefs: []
  type: TYPE_NORMAL
- en: Select one of the colors on the right-hand side of the tool by clicking on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a dot to the part of the image we want to colorize with the selected color.
    This is a “hint.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the icon; the image will reload, painting the selected area with the
    color we chose.![](img/Icon_2.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A step-by-step tutorial on how to use this intuitive tool can be found in the
    *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **convolutional neural network** (**CNN**) is a specialized type of deep neural
    network that’s designed primarily for analyzing visual data. At a high level,
    CNNs are inspired by how the human visual system processes information. They consist
    of layers that automatically learn and detect various features, such as edges,
    corners, textures, and more complex patterns, from raw pixel data. These learned
    features are then used for tasks such as image classification, object detection,
    facial recognition, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key components of a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layer**: This is the core of a CNN. It applies a set of learnable
    filters (also called kernels) to the input image. The layer identifies the distinct
    features of an image in a process known as feature extraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling layer**: This layer reduces the spatial dimensions of the feature
    maps while retaining important information. There are two types of pooling: max
    pooling and average pooling. It is usually applied after the convolutional layer
    to reduce the size of the feature map that was created in the previous layer.
    After several convolutional and pooling layers, the feature maps are flattened
    into a one-dimensional vector, which serves as the input to the fully connected
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully connected layers**: These layers are similar to those in traditional
    neural networks, connecting separate layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The components we’ve just detailed can be visualized in order in *Figure 9**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Structure of a CNN. Photo by Alison Wang in Unsplash](img/B19446_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Structure of a CNN. Photo by Alison Wang in Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are trained using labeled datasets. During training, the network’s parameters
    (weights and biases) are updated using optimization algorithms such as gradient
    descent to minimize a loss function that quantifies the difference between predicted
    and actual labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Style2Paints model is based on a CNN framework trained with the Danbooru
    database, which has two parts: the draft and refinement processes. According to
    the *Two-stage Sketch Colorization* paper, “*The first drafting stage aggressively
    splashes colors over the canvas to create a color draft, with the goal of enriching
    the color variety (…) The second refinement stage corrects the color mistakes,
    refines details and polishes blurry textures to achieve the final output*.” This
    neural network has been trained to work with color sketches that, by definition,
    lack some important information, such as shades or textures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It uses **generative adversarial networks** (**GANs**), a type of CNN for generative
    modeling. This type of neural network works with two sub-models: a generator and
    a discriminator. The generator performs an unsupervised task, summarizing the
    distribution of the training dataset (generally images) and generating synthetic
    replicas to be analyzed by the discriminator. The discriminator receives the replicas,
    combined with some samples of the training dataset, and performs a supervised
    task, classifying between real (the ground truth sample) and fake (the generated
    by the generator). The model is considered trained when the discriminator cannot
    identify a generated image from a ground truth one. The generator is then kept
    to generate new samples of the problem domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Steps in the training process](img/B19446_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Steps in the training process
  prefs: []
  type: TYPE_NORMAL
- en: This can be seen as two sub-models competing against each other and getting
    better at generating and discriminating. That is why the word “adversarial” is
    in its name. An overview of this structure can be found in the *Further* *reading*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Training GANs require large datasets and a lot of GPU. Videos from the Washington
    University of Saint Louis have been included in the *Further reading* section
    if you are interested in training your own GAN.
  prefs: []
  type: TYPE_NORMAL
- en: A note on training datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Better results will be yielded if the model that’s being used has been trained
    with the same image style that we are trying to reproduce. For example, if we
    want to paint with a photographic style, we may try to avoid using anime-trained
    models.
  prefs: []
  type: TYPE_NORMAL
- en: As anticipated, Style2Paints was trained with the Danbooru dataset, which is
    a tagged anime dataset that has been evolving and expanding over time. Style2Paints
    was trained on the 2018 version, but at the time of writing, there is a 2021 version.
    This dataset contains images accompanied by a JSON file with metadata and tags.
    The anime art has some common characteristics, such as big expressive eyes with
    vibrant colors, heightened expressions, and a varied color palette to reflect
    the atmosphere present in the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some commonly used image datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ImageNet**: This is a compilation of images that follows the WordNet hierarchy.
    Each relevant concept in the WordNet collection is a “synonym set” that forms
    relations with other synsets, establishing a hierarchy of concepts: from general
    to abstract and specific. The ImageNet project is trying to provide 1,000 images
    per synset. This dataset is useful for object classification tasks. For more information,
    visit [https://www.image-net.org/](https://www.image-net.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common Objects in Context** (**COCO**): This is a large-scale dataset that’s
    been annotated for object detection tasks. It contains over 33,000 images, organized
    into directories, and the annotations are in JSON format and contain the objects
    and the bounding box coordinates. For more information, visit [https://cocodataset.org/#home.](https://cocodataset.org/#home.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MPII Human Pose Database**: This dataset has been prepared for use in human
    pose estimation tasks. It contains approximately 25,000 images reflecting over
    410 everyday human activities. For more information, visit [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frames Labeled in Cinema**: This dataset contains images that have been extracted
    from popular Hollywood movies. The images went through multiple processes (from
    selection to cleaning) before undergoing a final manual review and annotation
    of body joints. For more information, visit [https://bensapp.github.io/flic-dataset.xhtml](https://bensapp.github.io/flic-dataset.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caltech-UCSD Birds-200-2011**: This dataset contains close to 12,000 images
    of 200 categories of birds with test and train subsets. Each image has detailed
    annotations, including one subcategory label, 15 part locations, 312 binary attributes,
    and one bounding box. The dataset is available on TensorFlow. For more information,
    visit [https://www.vision.caltech.edu/datasets/cub_200_2011/](https://www.vision.caltech.edu/datasets/cub_200_2011/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Laion-5b**: This is the dataset over which Stable Diffusion (something we’ll
    review shortly) was trained. It contains 5.85 billion CLIP-filtered image-text
    pairs via a general crawl of the internet that was done by the German entity LAION.
    For more information, visit [https://laion.ai/blog/laion-5b/](https://laion.ai/blog/laion-5b/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned how to use a tool that helps in the coloring workflow,
    automatically or manually. In the following section, we will dig deeper into coloring
    with a broader impact on the image by transferring a style from one image to the
    other with AI.
  prefs: []
  type: TYPE_NORMAL
- en: Creating with style – style transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way we can assist an artistic team is via style transfer, a process
    that involves combining two images:'
  prefs: []
  type: TYPE_NORMAL
- en: The style image or *root* image, from which we will learn the style
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target image, which we will transform with the new style
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting image will retain the core elements of the target image but appear
    to be painted or printed following the style image.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods for performing style transfer, including leveraging
    GANs (described in the previous section), using **Visual Geometry Group** (**VGG**),
    and employing Stable Diffusion (which we will cover in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: In `style_transfer.ipynb`, we will use VGG19, a special type of CNN with 19
    layers that has been trained with over a million images from the ImageNet database
    to extract the style of a Picasso painting and transfer it to a photograph. Picasso
    belonged to the **cubism** movement, where the artists applied multiple perspectives,
    used geometric shapes, and flattened the picture plane. An interesting article
    on the defining characteristics of this artistic movement can be found in the
    *Further* *reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the steps we must follow.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must obtain the tensor representations of the root and target images.
    The `preprocess_image()` function does this by leveraging the Keras library with
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Model building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We build the VGG19 model by setting the ImageNet dataset weights, which means
    the model will be initialized with weights that have been pre-trained on the ImageNet
    dataset. The `include_top` parameter is set to `False`, which means the top layers
    that are responsible for classification are not included in the model. The reason
    is that we want to use the VGG19 model as a feature extractor rather than for
    classification purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code also extracts the information that’s generated by each layer of the
    model so that it can be used in the loss functions that we’ll describe here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define three loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The **total variation loss**, which seeks to ensure the coherence of the final
    image by measuring the spatial continuity between pixels in the resulting image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `content_layer_name`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `style_layer_names`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The style loss uses a gram matrix (which is essentially a tensor multiplied
    by its transpose) and is calculated in the `gram_matrix()` function. The rationale
    behind the gram matrix of a convolutional layer is to combine the style features
    that are learned among them. For instance, Pablo Picasso's cubism is a combination
    of colors, shapes, and textures. A synthesis (the gram) of those features measuring
    the correlation between them will represent Picasso’s style.
  prefs: []
  type: TYPE_NORMAL
- en: The `compute_loss` function summarizes the combination of the various losses
    defined previously, while `compute_loss_and_grads` runs the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Training and inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training process will reduce the style loss and the content loss, which
    make up the total variation loss. The training process uses **stochastic gradient
    descent** (**SGD**) as the optimizer to iteratively decrease the loss.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed script saves the image every 100 iterations, allowing us to monitor
    image variation. The documentation proposes displaying the final image at the
    end of the training process, which we set at 4,000 steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the `util` function in the notebook named `deprocess_image()`, which
    rebuilds the image from a tensor into a `.png` file so that it’s ready to be saved
    and displayed, we can see the style transfer of a Pablo Picasso painting to a
    photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – A waterfall in Picasso’s painting style](img/B19446_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – A waterfall in Picasso’s painting style
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we learned how to modify images automatically by applying
    color; in this section, we reviewed how to create images by combining a base image
    with a specific style. In both cases, we provided the images we wanted to modify.
    In the next section, we will learn how to create images with a text input or prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Creating with prompts – text to image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce some services that enable images to be generated
    based on a **prompt**. A prompt is a set of natural language instructions that,
    when fed to the model, generates images. Whatever can be described in words can
    be transformed into an image. The more descriptive the prompt is, the more unique
    the output will be. The instructions can include some keywords that will enhance
    the originality of the created pieces of art, such as the style of the generated
    image, the aspect ratio, the resolution of the expected images, and more.
  prefs: []
  type: TYPE_NORMAL
- en: All of the services we will present use some form of **diffusion models**, combined
    with other models to make the image generation process more efficient, clean it
    from disturbing results (for example, for minor 18), and more. Diffusion models
    are generative models that try to replicate the training data. During training,
    the model adds noise to the training dataset and learns how to reverse it to recover
    the original image. By recovering from the training noise, the model learns the
    fundamental aspects of an image, as well as how to generate new data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s briefly analyze them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: DALL.E 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI, the same research team that developed ChatGPT, developed DALL-E 2, an
    image generator from text descriptions. According to their documentation, “*DALL·E
    is a transformer language model. It receives both the text and the image as a
    single stream of data containing up to 1280 tokens, and is trained using maximum
    likelihood to generate all of the tokens, one after another*.” The model is a
    12-billion parameter autoregressive transformer that’s trained on 250 million
    image-text pairs collected from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E 2 not only generates images according to a predefined prompt but also
    enables the user to modify parts of the image or add contextual background to
    smaller pieces of images.
  prefs: []
  type: TYPE_NORMAL
- en: This same team also designed **contrastive language image pre-training** (**CLIP**),
    which allows us to map texts with images and returns the most appropriate caption
    for an input image. More information can be found at [https://openai.com/research/clip](https://openai.com/research/clip).
    This takes image tagging and categorization to another level of speed.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stable Diffusion models are open source and contain code and checkpoints. To
    enable Stable Diffusion models to be trained on low GPU, they do not train on
    images but rather on the latent space of the dataset. The models learn from the
    underlying structure of the dataset instead of processing each image. Training
    with latent space enables us to feed the model with text and images in the same
    space that the model will use to regenerate the image.
  prefs: []
  type: TYPE_NORMAL
- en: The CLIP model mentioned in the previous section helped train the latest version
    of Stable Diffusion V2\. The link to the repository is [https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion).
  prefs: []
  type: TYPE_NORMAL
- en: Midjourney
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base models that are used by Midjourney are not disclosed, but they are
    likely a combination of diffusion models, as explained for DALL-E and Stable Diffusion.
    Midjourney is currently only accessible through a Discord bot on their official
    Discord server or by inviting the bot to a third-party server. It has no API.
  prefs: []
  type: TYPE_NORMAL
- en: This service became popular very fast.
  prefs: []
  type: TYPE_NORMAL
- en: Leonardo.Ai
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The link to their page is [https://app.leonardo.ai/](https://app.leonardo.ai/).
    This tool offers off-the-shelf models that generate images specifically trained
    in some of the most common themes, such as **role-playing games** (**RPGs**) or
    realistic photographs. It also offers tools to fine-tune models so that they can
    be adapted to our training datasets and a liberal free tier. Finally, it is developer-friendly
    with an easy-to-interact API. Each model has a “base model” description based
    on Stable Diffusion releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, sign up on their app and complete the **Get started** survey.
    There is no need to pay to interact with the basic service, but you must do so
    to get API keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Logging in to Leonardo.Ai for the first time](img/B19446_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Logging in to Leonardo.Ai for the first time
  prefs: []
  type: TYPE_NORMAL
- en: In the home page view, there is a list of models that we can interact with,
    depending on the type of image we want to generate. As mentioned earlier, each
    model is optimized for a specific purpose. For example, there is a model specially
    designed for vintage-style photography or RPG character portraits.
  prefs: []
  type: TYPE_NORMAL
- en: We can also see two tabs called **Community Feed** and **Personal Feed**, which
    show images generated by the community and our images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we move to the AI generation tool, we will see several options to choose
    from on the left-hand side of the view, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of images**: This allows us to select the number of images we want
    to generate with each run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt magic**: According to their description, “*Our experimental render
    pipeline may have better prompt adherence*.” The images are more artistic with
    this enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt magic strength**: This option determines the weight of the render
    detailed previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Public images**: This option allows us to choose whether we want to share
    these images with the public feed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image dimensions**: This option lets us set the size of the images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidance scale**: This option determines the weight of the prompt in the
    final image. It is suggested to keep it at 7 or 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also upload an image to be used as a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: A note on good prompts
  prefs: []
  type: TYPE_NORMAL
- en: '**Vocabulary**: Avoid the use of “very” or “super” for emphasis. Instead, opt
    for words that convey the same meaning. For instance, replace “very tired” with
    “exhausted.”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Typos**: Refrain from sending incorrectly spelled words, abbreviations, contractions,
    or slang as the model may struggle to align them with the dataset on which it
    was trained.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specificity**: Minimize ambiguity in word choices and unnecessary text. For
    improved results, opt for expressions such as “cheeseless pizza” instead of “pizza
    with no cheese.” Utilize negative prompts to exclude specific objects or characteristics
    from the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keywords to consider**: Include hints about the image’s background, style
    words (such as anime, realistic, paper art, cubism, charcoal painting, folk art,
    graffiti), lighting (soft, ambient, neon, studio lights), or time of day (morning,
    golden hour, midnight).'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the application helps us with the prompt generation. Within the
    AI tool that we just described, we can see the **Prompt Generation** tab, which
    assists us in generating the prompt to obtain the desired image using AI.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on with Leonardo.Ai
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s try an exercise with the API. The documentation is available at [https://docs.leonardo.ai/reference/getuserself](https://docs.leonardo.ai/reference/getuserself).
    It is an easy-to-use API that can be accessed with our well-known `request` library.
  prefs: []
  type: TYPE_NORMAL
- en: The API can help us build the entire pipeline from a prompt to a folder that
    we can submit to the artistic team for review. The `Leonardo_AI.ipynb` file contains
    the workflow we’ll explore.
  prefs: []
  type: TYPE_NORMAL
- en: Although the API is under development and not all functionalities and models
    can be invoked programmatically, most of the options described previously can
    be added as parameters to the payload.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s review the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To interact with the API, we need to log in and obtain a set of API keys that
    will pass as authorization in the header.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to read the parameters from the website as the documentation
    for this is not complete. For example, some models are trained with specific image
    dimensions, so it is better to input those preferred dimensions in the parameter
    payload. Additionally, not all the models can be called from the API, and there
    is no access to the prompt generator.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, this is a great tool that can help us generate high-quality
    images rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Minting an NFT collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of analyzing all the tools in this section is to create or modify
    images that we can sell or the artistic team we support will sell. Once we have
    generated the images, we want to “own” them in the Web3 sense, as explained in
    [*Chapter 4*](B19446_04.xhtml#_idTextAnchor145). To achieve this, we will create
    a collection in a marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: '**Minting** is the act of creating the digital trace of an item on the blockchain.
    As we saw when describing ERC 721, it means that the trace will point to a URL
    containing the stored image. Everything that is stored on the chain pays a gas
    fee.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of lazy minting has emerged rather recently. **Lazy minting** involves
    authorizing the platform to mint the NFT at the moment of the NFT sale and not
    before. This is important because minting involves gas expenditure, and, in moments
    of high congestion, gas prices can be high. In addition, lazy minting helps reduce
    the risk of creating a collection that may not be sold high enough to cover the
    initial investment. At the time of writing, the main marketplaces, such as OpenSea
    and Rarible, offer the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Creators mint an NFT *lazily* using a specific smart contract. The smart contract
    will mint and sell the NFT on our behalf. We provide the authorization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The buyer pays a price that covers the minting costs and the NFT itself when
    purchasing our NFT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This method defers the minting process until just before the NFT is sold, which
    is an incentive for creators to continue producing and showcasing their art without
    necessarily paying for gas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a collection on OpenSea:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://opensea.io/](https://opensea.io/). To interact with the platform,
    you will need a wallet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Connect your wallet and go to the profile options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Connecting to your wallet](img/B19446_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Connecting to your wallet
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **My Collections**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.11 – The My Collections tab](img/B19446_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – The My Collections tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the blue **Create a** **collection** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.12 – The My Collections page](img/B19446_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – The My Collections page
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be offered two options: the traditional option (**Deploy your own
    contract**) and the lazy minting option (**Use the OpenSea contract**). Click
    on the second option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Choosing the lazy minting option](img/B19446_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Choosing the lazy minting option
  prefs: []
  type: TYPE_NORMAL
- en: 'A page will open where you can complete the collection’s details. Among them
    we will find the following details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The name of the collection.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Description.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The accepted currency.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Images for the collection page.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Author earnings. As we mentioned in [*Chapter 4*](B19446_04.xhtml#_idTextAnchor145),
    it is possible to establish a percentage that the creator will retain each time
    the NFT is sold.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once all the required details have been filled in, click **Save**. Now that
    we have a collection, we have to add pieces of art to it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to your profile and click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Creating an NFT](img/B19446_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Creating an NFT
  prefs: []
  type: TYPE_NORMAL
- en: 'You may require additional authorization to access this panel. If so, you will
    be taken to the **Create New Item** page: [https://opensea.io/asset/create](https://opensea.io/asset/create).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload the image, video, or audio that you want to mint. In this example, we
    will mint one of the images that was generated with the `Leonardo_AI.ipynb` notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Details of the new item](img/B19446_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Details of the new item
  prefs: []
  type: TYPE_NORMAL
- en: 'Start filling in the required fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Description
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect it to the collection that was created in *step* *4 previously*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the number of items of the same nature that can be minted
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The network where this item will live
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Create**; you will see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Resulting message after signing the creation](img/B19446_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Resulting message after signing the creation
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable the sale. To do this, navigate to the page of the item and click on
    the **Sell** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Sale details](img/B19446_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Sale details
  prefs: []
  type: TYPE_NORMAL
- en: New options will appear for you to choose from. It is possible to choose between
    a fixed price or an auction with a limited time. If we want to sell for a fixed
    price, we can follow the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Fixed price**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.18 – The Fixed price option](img/B19446_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – The Fixed price option
  prefs: []
  type: TYPE_NORMAL
- en: Set a price in any of the coins or tokens that are acceptable according to what
    we decided when creating the collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Below this, we will find a summary of earnings and the fee that OpenSea charges
    (at the time of writing, this is 2.5%).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we agree, we can click on **Complete listing**. To approve the listing, OpenSea
    requires our signature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once signed, we will receive a new notification, informing us that the item
    has been listed. Now, it is out there in the marketplace and can be purchased!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.19 – The listing for sale has been enabled](img/B19446_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – The listing for sale has been enabled
  prefs: []
  type: TYPE_NORMAL
- en: 'All the illustrations that were generated in this section are available in
    The Mysterious Library Collection: [https://opensea.io/collection/mysterious-library](https://opensea.io/collection/mysterious-library).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we explored three different approaches to using AI
    tools in artistic projects. We examined the use of GAN models for colorizing sketches,
    explored the VGG19 model for transferring style, and discovered state-of-the-art
    applications of Stable Diffusion models for generating art based on prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we learned about the entire workflow, from the finished piece of art
    to listing the final image on a marketplace. By combining the power of AI and
    blockchain technology, we now have a range of new opportunities to explore and
    monetize artistic work in exciting and innovative ways.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that questions have arisen concerning the ownership of images
    generated using AI. This arises from the possibility that these models may have
    been trained on copyrighted pieces of art without the artist’s consent. Respondents
    contend that the transformative nature of the model’s outputs, coupled with the
    fair use argument, could potentially dismiss such accusations. This ongoing issue
    is yet to be definitively resolved by legal authorities and is likely to exhibit
    variations across different countries.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the area of NFTs, we’ll now shift our focus to a critical aspect
    that underpins the integrity of this innovative landscape – fraud detection. In
    the following chapter, we will analyze another use case where machine learning
    can help us uncover anomalies and increase transaction security.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Crypto Grims: [https://twitter.com/cryptogrims](https://twitter.com/cryptogrims)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artsy Monke collection: [https://opensea.io/assets/ethereum/0xa4bcd3b7f141ba1f08f36033fdfce691565561bc](https://opensea.io/assets/ethereum/0xa4bcd3b7f141ba1f08f36033fdfce691565561bc).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra, M. (2020, September 2). *Convolutional neural networks, explained*.
    Medium. Available at [https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939](https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortis, S. (n.d.). *Google AI turns all 10,000 BAYC NFTs into machine-made art*.
    Cointelegraph. Available at [https://cointelegraph.com/news/google-ai-turns-all-10-000-bayc-nfts-into-machine-made-art](https://cointelegraph.com/news/google-ai-turns-all-10-000-bayc-nfts-into-machine-made-art).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lllyasviel (n.d.). *style2paints.github.io*. Available at [https://style2paints.github.io/](https://style2paints.github.io/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lllyasviel/style2paints. (n.d.). GitHub. Available at [https://github.com/lllyasviel/style2paints](https://github.com/lllyasviel/style2paints).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang, J. (2020, October 20). *Attempt to understand an all-star auto-color project—Style2Paints
    (Part 1)*. Medium. Available at [https://medium.com/ai-innovation/attempt-to-understand-an-all-star-auto-color-project-style2paints-part-1-84d2e3d96da](https://medium.com/ai-innovation/attempt-to-understand-an-all-star-auto-color-project-style2paints-part-1-84d2e3d96da).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and Chunping Liu. (n.d.). *CUHK
    Computer Science and Engineering*. Available at [https://www.cse.cuhk.edu.hk/~ttwong/papers/colorize/colorize.pdf](https://www.cse.cuhk.edu.hk/~ttwong/papers/colorize/colorize.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nerdy Rodent. (2020, November 19). *Style2Paints – Easily colour any line art
    using AI* [Video]. YouTube. Available at [https://www.youtube.com/watch?v=cvN9oQfC3w0](https://www.youtube.com/watch?v=cvN9oQfC3w0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of GAN structure. (n.d.). *Google for Developers*. Available at [https://developers.google.com/machine-learning/gan/gan_structure](https://developers.google.com/machine-learning/gan/gan_structure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prof. Jeff Heaton – Washington University of St. Louis. (2022, January 19).
    *Introduction to GANS for Image and Data Generation (7.1)* [Video]. YouTube. Available
    at [https://www.youtube.com/watch?v=hZw-AjbdN5k](https://www.youtube.com/watch?v=hZw-AjbdN5k).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prof. Jeff Heaton – Washington University of St. Louis. (2021, February 17).
    *Training a GAN from your Own Images: StyleGAN2 ADA* [Video]. YouTube. Available
    at [https://www.youtube.com/watch?v=kbDd5lW6rkM](https://www.youtube.com/watch?v=kbDd5lW6rkM).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prof. Jeff Heaton – Washington University of St. Louis. (2021, May 12). *Training
    NVIDIA StyleGAN2 ADA under Colab Free and Colab Pro Tricks* [Video]. YouTube.
    Available at [https://www.youtube.com/watch?v=L3JLzoe-dJU](https://www.youtube.com/watch?v=L3JLzoe-dJU).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T81_558_deep_learning/t81_558_class_07_1_gan_intro.ipynb at master · jeffheaton/t81_558_deep_learning*.
    (n.d.). GitHub. Available at [https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T81_558_deep_learning/t81_558_class_07_2_train_gan.ipynb at master · jeffheaton/t81_558_deep_learning*.
    (n.d.). GitHub. Available at [https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jason Brownlee PhD. (2019, July 19). *Machine Learning Mastery*. Machine Learning
    Mastery. Available at [https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*4 characteristics of cubism and why they are important*. (n.d.). Artlex –
    Art Dictionary and Encyclopedia. Available at [https://www.artlex.com/art-movements/cubism/characteristics/](https://www.artlex.com/art-movements/cubism/characteristics/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural style* *transfer*: [https://keras.io/examples/generative/neural_style_transfer/](https://keras.io/examples/generative/neural_style_transfer/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DALL·E: Creating images from text*. (n.d.). OpenAI. Available at [https://openai.com/research/dall-e](https://openai.com/research/dall-e).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zero-shot text-to-Image generation*. (n.d.). arXiv.org. Available at [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aleksa Gordić - The AI Epiphany. (2022, September 1). *Stable Diffusion: High-Resolution
    Image Synthesis with Latent Diffusion Models | ML Coding Series* [Video]. YouTube.
    Available at [https://www.youtube.com/watch?v=f6PtJKdey8E](https://www.youtube.com/watch?v=f6PtJKdey8E).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stability-AI/stablediffusion*. (n.d.). GitHub. Available at [https://github.com/Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How Stable Diffusion works? Latent Diffusion Models Explained*. (2022, December
    3). Louis Bouchard. Available at [https://www.louisbouchard.ai/latent-diffusion-models/](https://www.louisbouchard.ai/latent-diffusion-models/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arya, G. (2023, January 14). *Power of latent diffusion models: Revolutionizing
    image creation*. Analytics Vidhya. Available at [https://www.analyticsvidhya.com/blog/2023/01/power-of-latent-diffusion-models-revolutionizing-image-creation/](https://www.analyticsvidhya.com/blog/2023/01/power-of-latent-diffusion-models-revolutionizing-image-creation/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*API Documentation*. (n.d.). Leonardo.Ai. Available at [https://docs.leonardo.ai/reference/getuserself](https://docs.leonardo.ai/reference/getuserself).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ashley, K. (2021). *Make art with artificial intelligence: Make and sell your
    art with AI, blockchain,* *and NFT*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
