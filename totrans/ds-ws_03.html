<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer098" class="Content">
			</div>
		</div>
		<div id="_idContainer099" class="Content">
			<h1 id="_idParaDest-69">3. <a id="_idTextAnchor068"/>Binary Classification</h1>
		</div>
		<div id="_idContainer155" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will be using a real-world dataset and a supervised learning technique called classification to generate business outcomes.</p>
			<p class="callout">By the end of this chapter, you will be able to formulate a data science problem statement from a business perspective; build hypotheses from various business drivers influencing a use case and verify the hypotheses using exploratory data analysis; derive features based on intuitions that are derived from exploratory analysis through feature engineering; build binary classification models using a logistic regression function and analyze classification metrics and formulate action plans for the improvement of the model.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Introduction</h1>
			<p>In previous chapters, where an introduction to machine learning was covered, you were introduced to two broad categories of machine learning; supervised learning and unsupervised learning. Supervised learning can be further divided into two types of problem cases, regression and classification. In the last chapter, we covered regression problems. In this chapter, we will peek into the world of classification problems.</p>
			<p>Take a look at the following <em class="italic">Figure 3.1</em>:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="Images/B15019_03_01.jpg" alt="Figure 3.1: Overview of machine learning algorithms&#13;&#10;" width="1653" height="758"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Overview of machine learning algorithms</p>
			<p>Classification problems are the most prevalent use cases you will encounter in the real world. Unlike regression problems, where a real numbered value is predicted, classification problems deal with associating an example to a category. Classification use cases will take forms such as the following:</p>
			<ul>
				<li>Predicting whether a customer will buy the recommended product</li>
				<li>Identifying whether a credit transaction is fraudulent</li>
				<li>Determining whether a patient has a disease</li>
				<li>Analyzing images of animals and predicting whether the image is of a dog, cat, or panda</li>
				<li>Analyzing text reviews and capturing the underlying emotion such as happiness, anger, sorrow, or sarcasm</li>
			</ul>
			<p>If you observe the preceding examples, there is a subtle difference between the first three and the last two. The first three revolve around binary decisions:</p>
			<ul>
				<li>Customers can either buy the product or not.</li>
				<li>Credit card transactions can be fraudulent or legitimate.</li>
				<li>Patients can be diagnosed as positive or negative for a disease.</li>
			</ul>
			<p>Use cases that align with the preceding three genres where a binary decision is made are called binary classification problems. Unlike the first three, the last two associate an example with multiple classes or categories. Such problems are called multiclass classification problems. This chapter will deal with binary classification problems. Multiclass classification will be covered next in <em class="italic">Chapter 4</em>, <em class="italic">Multiclass Classification with RandomForest</em>.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Understanding the Business Context</h1>
			<p>The best way to work using a concept is with an example you can relate to. To understand the business context, let's, for instance, consider the following example.</p>
			<p>The marketing head of the bank where you are a data scientist approaches you with a problem they would like to be addressed. The marketing team recently completed a marketing campaign where they have collated a lot of information on existing customers. They require your help to identify which of these customers are likely to buy a term deposit plan. Based on your assessment of the customer base, the marketing team will chalk out strategies for target marketing. The marketing team has provided access to historical data of past campaigns and their outcomes—that is, whether the targeted customers really bought the term deposits or not. Equipped with the historical data, you have set out on the task to identify the customers with the highest propensity (an inclination) to buy term deposits.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Business Discovery</h2>
			<p>The first process when embarking on a data science problem like the preceding is the business discovery process. This entails understanding various drivers influencing the business problem. Getting to know the business drivers is important as it will help in formulating hypotheses about the business problem, which can be verified during the <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>). The verification of hypotheses will help in formulating intuitions for feature engineering, which will be critical for the veracity of the models that we build.</p>
			<p>Let's understand this process in detail from the context of our use case. The problem statement is to identify those customers who have a propensity to buy term deposits. As you might be aware, term deposits are bank instruments where your money will be locked for a certain period, assuring higher interest rates than saving accounts or interest-bearing checking accounts. From an investment propensity perspective, term deposits are generally popular among risk-averse customers. Equipped with the business context, let's look at some questions on business factors influencing a propensity to buy term deposits:</p>
			<ul>
				<li>Would age be a factor, with more propensity shown by the elderly?</li>
				<li>Is there any relationship between employment status and the propensity to buy term deposits?</li>
				<li>Would the asset portfolio of a customer—that is, house, loan, or higher bank balance—influence the propensity to buy? </li>
				<li>Will demographics such as marital status and education influence the propensity to buy term deposits? If so, how are demographics correlated to a propensity to buy?</li>
			</ul>
			<p>Formulating questions on the business context is critical as this will help in arriving at various trails that we can take when we do exploratory analysis. We will deal with that in the next section. First, let's explore the data related to the preceding business problem.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>Exercise 3.01: Loading and Exploring the Data from the Dataset</h2>
			<p>In this exercise, we will load the dataset in our Colab notebook and do some basic explorations such as printing the dimensions of the dataset using the <strong class="source-inline">.shape()</strong> function and generating summary statistics of the dataset using the <strong class="source-inline">.describe()</strong> function. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset for this exercise is the bank dataset, courtesy of S. Moro, P. Cortez and P. Rita: A Data-Driven Approach to Predict the Success of Bank Telemarketing.</p>
			<p class="callout">It is from the UCI Machine Learning Repository: <a href="https://packt.live/2MItXEl">https://packt.live/2MItXEl</a> and can be downloaded from our GitHub at: <a href="https://packt.live/2Wav1nJ">https://packt.live/2Wav1nJ</a>.</p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li>Open a new Colab notebook.</li>
				<li>Now, <strong class="source-inline">import</strong> <strong class="source-inline">pandas</strong> as <strong class="source-inline">pd</strong> in your Colab notebook:<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the dataset to a variable called <strong class="source-inline">file_url</strong><p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-Data-Science-Workshop/master/Chapter03'\</p><p class="source-code">           '/bank-full.csv'</p></li>
				<li>Now, read the file using the <strong class="source-inline">pd.read_csv()</strong> function from the pandas DataFrame:<p class="source-code"># Loading the data using pandas</p><p class="source-code">bankData = pd.read_csv(file_url, sep=";")</p><p class="source-code">bankData.head()</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">#</strong> symbol in the code snippet above denotes a code comment. Comments are added into code to help explain specific bits of logic.</p><p>The <strong class="source-inline">pd.read_csv()</strong> function's arguments are the filename as a string and the limit separator of a CSV, which is <strong class="source-inline">";"</strong>. After reading the file, the DataFrame is printed using the <strong class="source-inline">.head()</strong> function. Note that the <strong class="source-inline">#</strong> symbol in the code above denotes a comment. Comments are added into code to help explain specific bits of logic. </p><p>You should get the following output:</p><div id="_idContainer101" class="IMG---Figure"><img src="Images/B15019_03_02.jpg" alt="Figure 3.2: Loading data into a Colab notebook&#13;&#10;" width="1635" height="273"/></div><p class="figure-caption">Figure 3.2: Loading data into a Colab notebook</p><p>Here, we loaded the <strong class="source-inline">CSV</strong> file and then stored it as a pandas DataFrame for further analysis.</p></li>
				<li>Next, print the shape of the dataset, as mentioned in the following code snippet:<p class="source-code"># Printing the shape of the data </p><p class="source-code">print(bankData<strong class="bold">.shape</strong>)</p><p>The <strong class="source-inline">.shape</strong> function is used to find the overall shape of the dataset.</p><p>You should get the following output:</p><p class="source-code">(45211, 17)</p></li>
				<li>Now, find the summary of the numerical raw data as a table output using the <strong class="source-inline">.describe()</strong> function in pandas, as mentioned in the following code snippet:<p class="source-code"># Summarizing the statistics of the numerical raw data</p><p class="source-code">bankData<strong class="bold">.describe()</strong></p><p>You should get the following output:</p><div id="_idContainer102" class="IMG---Figure"><img src="Images/B15019_03_03.jpg" alt="Figure 3.3: Loading data into a Colab notebook&#13;&#10;" width="1329" height="464"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.3: Loading data into a Colab notebook</p>
			<p>As seen from the shape of the data, the dataset has <strong class="source-inline">45211</strong> examples with <strong class="source-inline">17</strong> variables. The variable set has both categorical and numerical variables. The preceding summary statistics are derived only for the numerical data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31UQhAU">https://packt.live/31UQhAU</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YdiSAF">https://packt.live/2YdiSAF</a>.</p>
			<p>You have completed the first tasks that are required before embarking on our journey. In this exercise, you have learned how to load data and to derive basic statistics, such as the summary statistics, from the dataset. In the subsequent dataset, we will take a deep dive into the loaded dataset.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Testing Business Hypotheses Using Exploratory Data Analysis</h2>
			<p>In the previous section, you approached the problem statement from a domain perspective, thereby identifying some of the business drivers. Once business drivers are identified, the next step is to evolve some hypotheses about the relationship of these business drivers and the business outcome you have set out to achieve. These hypotheses need to be verified using the data you have. This is where <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) plays a big part in the data science life cycle.</p>
			<p>Let's return to the problem statement we are trying to analyze. From the previous section, we identified some business drivers such as age, demographics, employment status, and asset portfolio, which we feel will influence the propensity for buying a term deposit. Let's go ahead and formulate our hypotheses on some of these business drivers and then verify them using EDA.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Visualization for Exploratory Data Analysis</h2>
			<p>Visualization is imperative for EDA. Effective visualization helps in deriving business intuitions from the data. In this section, we will introduce some of the visualization techniques that will be used for EDA:</p>
			<ul>
				<li><strong class="bold">Line graphs</strong>: Line graphs are one of the simplest forms of visualization. Line graphs are the preferred method for revealing trends in the data. These types of graphs are mostly used for continuous data. We will be generating this graph in <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em>.<p>Here is what a line graph looks like:</p><div id="_idContainer103" class="IMG---Figure"><img src="Images/B15019_03_04.jpg" alt="Figure 3.4: Example of a line graph&#13;&#10;" width="1665" height="585"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.4: Example of a line graph</p>
			<ul>
				<li><strong class="bold">Histograms</strong>: Histograms are plots of the proportion of data along with some specified intervals. They are mostly used for visualizing the distribution of data. Histograms are very effective for identifying whether data distribution is symmetric and for identifying outliers in data. We will be looking at histograms in much more detail later in this chapter.<p>Here is what a histogram looks like:</p><div id="_idContainer104" class="IMG---Figure"><img src="Images/B15019_03_05.jpg" alt="Figure 3.5: Example of a histogram&#13;&#10;" width="1452" height="675"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.5: Example of a histogram</p>
			<ul>
				<li><strong class="bold">Density plots</strong>: Like histograms, density plots are also used for visualizing the distribution of data. However, density plots give a smoother representation of the distribution. We will be looking at this later in this chapter.<p>Here is what a density plot looks like:</p><div id="_idContainer105" class="IMG---Figure"><img src="Images/B15019_03_06.jpg" alt="Figure 3.6: Example of a density plot&#13;&#10;" width="1198" height="602"/></div></li>
			</ul>
			<p class="figure-caption">Figure 3.6: Example of a density plot</p>
			<ul>
				<li><strong class="bold">Stacked bar charts</strong>: A stacked bar chart helps you to visualize the various categories of data, one on top of the other, in order to give you a sense of proportion of the categories; for instance, if you want to plot a bar chart showing the values, <strong class="source-inline">Yes</strong> and <strong class="source-inline">No</strong>, on a single bar. This can be done using the stacked bar chart, which cannot be done on the other charts.<p>Let's create some dummy data and generate a stacked bar chart to check the proportion of jobs in different sectors. </p><p class="callout-heading">Note</p><p class="callout">Do not execute any of the following code snippets until the final step. Enter all the code in the same cell.</p><p>Import the library files required for the task:</p><p class="source-code"># Importing library files</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p>Next, create some sample data detailing a list of jobs:</p><p class="source-code"># Create a simple list of categories</p><p class="source-code">jobList = ['admin','scientist','doctor','management']</p><p>Each job will have two categories to be plotted, <strong class="source-inline">yes</strong> and <strong class="source-inline">No</strong>, with some proportion between <strong class="source-inline">yes</strong> and <strong class="source-inline">No</strong>. These are detailed as follows:</p><p class="source-code"># Getting two categories ( 'yes','No') for each of jobs</p><p class="source-code">jobYes = [20,60,70,40]</p><p class="source-code">jobNo = [80,40,30,60]</p><p>In the next steps, the length of the job list is taken for plotting <strong class="source-inline">xlabels</strong> and then they are arranged using the <strong class="source-inline">np.arange()</strong> function:</p><p class="source-code"># Get the length of x axis labels and arranging its indexes</p><p class="source-code">xlabels = len(jobList)</p><p class="source-code">ind = np.arange(xlabels)</p><p>Next, let's define the width of each bar and do the plotting. In the plot, <strong class="source-inline">p2</strong>, we define that when stacking, <strong class="source-inline">yes</strong> will be at the bottom and <strong class="source-inline">No</strong> at top:</p><p class="source-code"># Get width of each bar</p><p class="source-code">width = 0.35</p><p class="source-code"># Getting the plots</p><p class="source-code">p1 = plt.bar(ind, jobYes, width)</p><p class="source-code">p2 = plt.bar(ind, jobNo, width, bottom=jobYes)</p><p>Define the labels for the <em class="italic">Y</em> axis and the title of the plot:</p><p class="source-code"># Getting the labels for the plots</p><p class="source-code">plt.ylabel('Proportion of Jobs')</p><p class="source-code">plt.title('Job')</p><p>The indexes for the <em class="italic">X</em> and <em class="italic">Y</em> axes are defined next. For the <em class="italic">X</em> axis, the list of jobs are given, and, for the <em class="italic">Y</em> axis, the indices are in proportion from <strong class="source-inline">0</strong> to <strong class="source-inline">100</strong> with an increment of <strong class="source-inline">10</strong> (0, 10, 20, 30, and so on):</p><p class="source-code"># Defining the x label indexes and y label indexes</p><p class="source-code">plt.xticks(ind, jobList)</p><p class="source-code">plt.yticks(np.arange(0, 100, 10))</p><p>The last step is to define the legends and to rotate the axis labels to <strong class="source-inline">90</strong> degrees. The plot is finally displayed:</p><p class="source-code"># Defining the legends</p><p class="source-code">plt.legend((p1[0], p2[0]), ('Yes', 'No'))</p><p class="source-code"># To rotate the axis labels </p><p class="source-code">plt.xticks(rotation=90)</p><p class="source-code">plt.show()</p></li>
			</ul>
			<p>Here is what a stacked bar chart looks like based on the preceding example:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="Images/B15019_03_07.jpg" alt="Figure 3.7: Example of a stacked bar plot&#13;&#10;" width="561" height="448"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: Example of a stacked bar plot</p>
			<p>Let's use these graphs in the following exercises and activities.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Exercise 3.02: Business Hypothesis Testing for Age versus Propensity for a Term Loan</h2>
			<p>The goal of this exercise is to define a hypothesis to check the propensity for an individual to purchase a term deposit plan against their age. We will be using a line graph for this exercise. </p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li value="1">Begin by defining the hypothesis.<p>The first step in the verification process will be to define a hypothesis about the relationship. A hypothesis can be based on your experiences, domain knowledge, some published pieces of knowledge, or your business intuitions.</p><p>Let's first define our hypothesis on age and propensity to buy term deposits: </p><p><em class="italic">The propensity to buy term deposits is more with elderly customers compared to younger ones</em>. This is our hypothesis.</p><p>Now that we have defined our hypothesis, it is time to verify its veracity with the data. One of the best ways to get business intuitions from data is by taking cross-sections of our data and visualizing them. </p></li>
				<li>Import the pandas and altair packages:<p class="source-code">import pandas as pd</p><p class="source-code">import altair as alt</p></li>
				<li>Next, you need to load the dataset, just like you loaded the dataset in <em class="italic">Exercise 3.01</em>,<em class="italic"> Loading and Exploring the Data from the Dataset</em>:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter03/bank-full.csv'</p><p class="source-code">bankData = pd.read_csv(file_url, sep=";")</p><p class="callout-heading">Note</p><p class="callout"><em class="italic">Steps 2-3 </em>will be repeated in the following exercises for this chapter.</p><p>We will be verifying how the purchased term deposits are distributed by age.</p></li>
				<li>Next, we will count the number of records for each age group. We will be using the combination of <strong class="source-inline">.groupby()</strong>, <strong class="source-inline">.agg()</strong>, <strong class="source-inline">.reset_index()</strong> methods from <strong class="source-inline">pandas</strong>.<p class="callout-heading">Note</p><p class="callout">You will see further details of these methods in <em class="italic">Chapter 12</em>, <em class="italic">Feature Engineering</em>.</p><p class="source-code">filter_mask = bankData['y'] == 'yes'</p><p class="source-code">bankSub1 = bankData[filter_mask]\</p><p class="source-code">           .groupby('age')['y'].agg(agegrp='count')\</p><p class="source-code">           .reset_index()</p><p>We first take the pandas <strong class="source-inline">DataFrame</strong>, <strong class="source-inline">bankData</strong>, which we loaded in <em class="italic">Exercise 3.01</em>, <em class="italic">Loading and Exploring the Data from the Dataset</em> and then filter it for all cases where the term deposit is yes using the mask <strong class="source-inline">bankData['y'] == 'yes'</strong>. These cases are grouped through the <strong class="source-inline">groupby()</strong> method and then aggregated according to age through the <strong class="source-inline">agg()</strong> method. Finally we need to use <strong class="source-inline">.reset_index()</strong> to get a well-structure DataFrame that will be stored in a new <strong class="source-inline">DataFrame</strong> called <strong class="source-inline">bankSub1</strong>.</p></li>
				<li>Now, plot a line chart using altair and the <strong class="source-inline">.Chart().mark_line().encode()</strong> methods and we will define the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables, as shown in the following code snippet:<p class="source-code"># Visualising the relationship using altair</p><p class="source-code">alt.Chart(bankSub1).mark_line().encode(x='age', y='agegrp')</p><p>You should get the following output:</p><div id="_idContainer107" class="IMG---Figure"><img src="Images/B15019_03_08.jpg" alt="Figure 3.8: Relationship between age and propensity to purchase&#13;&#10;" width="1665" height="1182"/></div><p class="figure-caption">Figure 3.8: Relationship between age and propensity to purchase</p><p>From the plot, we can see that the highest number of term deposit purchases are done by customers within an age range between 25 and 40, with the propensity to buy tapering off with age.</p><p>This relationship is quite counterintuitive from our assumptions in the hypothesis, right? But, wait a minute, aren't we missing an important point here? We are taking the data based on the absolute count of customers in each age range. If the proportion of banking customers is higher within the age range of 25 to 40, then we are very likely to get a plot like the one that we have got. What we really should plot is the proportion of customers, within each age group, who buy a term deposit.</p><p>Let's look at how we can represent the data by taking the proportion of customers. Just like you did in the earlier steps, we will aggregate the customer propensity with respect to age, and then divide each category of buying propensity by the total number of customers in that age group to get the proportion.</p></li>
				<li>Group the data per age using the <strong class="source-inline">groupby()</strong> method and find the total number of customers under each age group using the <strong class="source-inline">agg()</strong> method:<p class="source-code"># Getting another perspective</p><p class="source-code">ageTot = bankData.groupby('age')['y']\</p><p class="source-code">         .agg(ageTot='count').reset_index()</p><p class="source-code">ageTot.head()</p><p>The output is as follows:</p><div id="_idContainer108" class="IMG---Figure"><img src="Images/B15019_03_09.jpg" alt="Figure 3.9: Customers per age group&#13;&#10;" width="869" height="272"/></div><p class="figure-caption">Figure 3.9: Customers per age group</p></li>
				<li>Now, group the data by both age and propensity of purchase and find the total counts under each category of propensity, which are <strong class="source-inline">yes</strong> and <strong class="source-inline">no</strong>:<p class="source-code"># Getting all the details in one place</p><p class="source-code">ageProp = bankData.groupby(['age','y'])['y']\</p><p class="source-code">          .agg(ageCat='count').reset_index()</p><p class="source-code">ageProp.head()</p><p>The output is as follows:</p><div id="_idContainer109" class="IMG---Figure"><img src="Images/B15019_03_10.jpg" alt="Figure 3.10: Propensity by age group&#13;&#10;" width="879" height="283"/></div><p class="figure-caption">Figure 3.10: Propensity by age group</p></li>
				<li>Merge both of these DataFrames based on the <strong class="source-inline">age</strong> variable using the <strong class="source-inline">pd.merge()</strong> function, and then divide each category of propensity within each age group by the total customers in the respective age group to get the proportion of customers, as shown in the following code snippet:<p class="source-code"># Merging both the data frames</p><p class="source-code">ageComb = pd.merge(ageProp, ageTot,left_on = ['age'], \</p><p class="source-code">                   right_on = ['age'])</p><p class="source-code">ageComb['catProp'] = (ageComb.ageCat/ageComb.ageTot)*100</p><p class="source-code">ageComb.head()</p><p>The output is as follows:</p><div id="_idContainer110" class="IMG---Figure"><img src="Images/B15019_03_11.jpg" alt="Figure 3.11: Merged DataFrames with proportion of customers by age group&#13;&#10;" width="797" height="282"/></div><p class="figure-caption">Figure 3.11: Merged DataFrames with proportion of customers by age group</p></li>
				<li>Now, display the proportion where you plot both categories (yes and no) as separate plots. This can be achieved through a method within <strong class="source-inline">altair</strong> called <strong class="source-inline">facet()</strong>: <p class="source-code"># Visualising the relationship using altair</p><p class="source-code">alt.Chart(ageComb).mark_line()\</p><p class="source-code">   .encode(x='age', y='catProp').facet(column='y')</p><p>This function makes as many plots as there are categories within the variable. Here, we give the <strong class="source-inline">'y'</strong> variable, which is the variable name for the <strong class="source-inline">yes</strong> and <strong class="source-inline">no</strong> categories to the <strong class="source-inline">facet()</strong> function, and we get two different plots: one for <strong class="source-inline">yes</strong> and another for <strong class="source-inline">no</strong>.</p><p>You should get the following output:</p><div id="_idContainer111" class="IMG---Figure"><img src="Images/B15019_03_12.jpg" alt="Figure 3.12: Visualizing normalized relationships&#13;&#10;" width="1665" height="765"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.12: Visualizing normalized relationships</p>
			<p>By the end of this exercise, you were able to get two meaningful plots showing the propensity of people to buy term deposit plans. The final output for this exercise shows two graphs in which the left graph shows the proportion of people who do not buy term deposits and the right one shows those customers who buy term deposits. </p>
			<p>We can see, in the first graph, with the age group beginning from <strong class="source-inline">22</strong> to <strong class="source-inline">60</strong>, individuals would not be inclined to purchase the term deposit. However, in the second graph, we see the opposite, where the age group of <strong class="source-inline">60</strong> and over are much more inclined to purchase the term deposit plan.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iOw7Q4">https://packt.live/3iOw7Q4</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>In the following section, we will begin to analyze our plots based on our intuitions.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Intuitions from the Exploratory Analysis</h2>
			<p>What are the intuitions we can take out of the exercise that we have done so far? We have seen two contrasting plots by taking the proportion of users and without taking the proportions. As you can see, taking the proportion of users is the right approach to get the right perspective in which we must view data. This is more in line with the hypothesis that we have evolved. We can see from the plots that the propensity to buy term deposits is low for age groups from <strong class="source-inline">22</strong> to around <strong class="source-inline">60</strong>. </p>
			<p>After <strong class="source-inline">60</strong>, we see a rising trend in the demand for term deposits. Another interesting fact we can observe is the higher proportion of term deposit purchases for ages younger than <strong class="source-inline">20</strong>.</p>
			<p>In <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em> we discovered how to develop our hypothesis and then verify the hypothesis using EDA. After the following activity, we will delve into another important step in the journey, Feature Engineering.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Activity 3.01: Business Hypothesis Testing to Find Employment Status versus Propensity for Term Deposits</h2>
			<p>You are working as a data scientist for a bank. You are provided with historical data from the management of the bank and are asked to try to formulate a hypothesis between employment status and the propensity to buy term deposits.</p>
			<p>In <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em> we worked on a problem to find the relationship between age and the propensity to buy term deposits. In this activity, we will use a similar route and verify the relationship between employment status and term deposit purchase propensity.</p>
			<p>The steps are as follows:</p>
			<ol>
				<li value="1">Formulate the hypothesis between employment status and the propensity for term deposits. Let the hypothesis be as follows: <em class="italic">High paying employees prefer term deposits than other categories of employees</em>.</li>
				<li>Open a Colab notebook file similar to what was used in <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em> and install and import the necessary libraries such as <strong class="source-inline">pandas</strong> and <strong class="source-inline">altair</strong>.</li>
				<li>From the banking DataFrame, <strong class="source-inline">bankData</strong>, find the distribution of employment status using the <strong class="source-inline">.groupby()</strong>, <strong class="source-inline">.agg()</strong> and <strong class="source-inline">.reset_index()</strong> methods.<p>Group the data with respect to employment status using the <strong class="source-inline">.groupby()</strong> method and find the total count of propensities for each employment status using the <strong class="source-inline">.agg()</strong> method. </p></li>
				<li>Now, merge both DataFrames using the <strong class="source-inline">pd.merge()</strong> function and then find the propensity count by calculating the proportion of propensity for each type of employment status. When creating the new variable for finding the propensity proportion. </li>
				<li>Plot the data and summarize intuitions from the plot using <strong class="source-inline">matplotlib</strong>. Use the stacked bar chart for this activity.<p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">bank-full.csv</strong> dataset to be used in this activity can be found at <a href="https://packt.live/2Wav1nJ">https://packt.live/2Wav1nJ</a>.</p></li>
			</ol>
			<p>Expected output: The final plot of the propensity to buy with respect to employment status will be similar to the following plot:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="Images/B15019_03_13.jpg" alt="Figure 3.13: Visualizing propensity of purchase by job&#13;&#10;" width="497" height="326"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Visualizing propensity of purchase by job</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>Now that we have seen EDA, let's dive into feature engineering.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Feature Engineering </h1>
			<p>In the previous section, we traversed the process of EDA. As part of the earlier process, we tested our business hypotheses by slicing and dicing the data and through visualizations. You might be wondering where we will use the intuitions that we derived from all of the analysis we did. The answer to that question will be addressed in this section.</p>
			<p>Feature engineering is the process of transforming raw variables to create new variables and this will be covered later in the chapter. Feature engineering is one of the most important steps that influence the accuracy of the models that we build. </p>
			<p>There are two broad types of feature engineering:</p>
			<ol>
				<li value="1">Here, we transform raw variables based on intuitions from a business perspective. These intuitions are what we build during the exploratory analysis. </li>
				<li>The transformation of raw variables is done from a statistical and data normalization perspective. </li>
			</ol>
			<p>We will look into each type of feature engineering next.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Feature engineering will be covered in much more detail in <em class="italic">Chapter 12</em>, <em class="italic">Feature Engineering</em>. In this section you will see the purpose of learning about classification.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>Business-Driven Feature Engineering</h2>
			<p>Business-driven feature engineering is the process of transforming raw variables based on business intuitions that were derived during the exploratory analysis. It entails transforming data and creating new variables based on business factors or drivers that influence a business problem. </p>
			<p>In the previous exercises on exploratory analysis, we explored the relationship of a single variable with the dependent variable. In this exercise, we will combine multiple variables and then derive new features. We will explore the relationship between an asset portfolio and the propensity for term deposit purchases. An asset portfolio is the combination of all assets and liabilities the customer has with the bank. We will combine assets and liabilities such as bank balance, home ownership, and loans to get a new feature called an <strong class="bold">asset</strong> index. </p>
			<p>These feature engineering steps will be split into two exercises. In <em class="italic">Exercise 3.03</em>, <em class="italic">Feature Engineering – Exploration of Individual Features</em>, we explore individual variables such as balance, housing, and loans to understand their relationship to a propensity for term deposits. </p>
			<p>In <em class="italic">Exercise 3.04</em>, <em class="italic">Feature Engineering – Creating New Features from Existing Ones, </em>we will transform individual variables and then combine them to form a new feature.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Exercise 3.03: Feature Engineering – Exploration of Individual Features</h2>
			<p>In this exercise, we will explore the relationship between two variables, which are whether an individual owns a house and whether an individual has a loan, to the propensity for term deposit purchases by these individuals.</p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package.<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter03/bank-full.csv'</p></li>
				<li>Read the banking dataset using the <strong class="source-inline">.read_csv()</strong> function:<p class="source-code"># Reading the banking data</p><p class="source-code">bankData = pd.read_csv(file_url, sep=";")</p></li>
				<li>Next, we will find a relationship between housing and the propensity for term deposits, as mentioned in the following code snippet:<p class="source-code"># Relationship between housing and propensity for term deposits</p><p class="source-code">bankData.groupby(['housing', 'y'])['y']\</p><p class="source-code">        .agg(houseTot='count').reset_index()</p><p>You should get the following output:</p><div id="_idContainer113" class="IMG---Figure"><img src="Images/B15019_03_14.jpg" alt="Figure 3.14: Housing status versus propensity to buy term deposits&#13;&#10;" width="1567" height="461"/></div><p class="figure-caption">Figure 3.14: Housing status versus propensity to buy term deposits</p><p>The first part of the code is to group customers based on whether they own a house or not. The count of customers under each category is calculated with the <strong class="source-inline">.agg()</strong> method. From the values, we can see that the propensity to buy term deposits is much higher for people who do not own a house compared with those who do own one: <strong class="source-inline">( 3354 / ( 3354 + 16727) = 17% to  1935 / ( 1935 + 23195) = 8%)</strong>.</p></li>
				<li>Explore the <strong class="source-inline">'loan'</strong> variable to find its relationship with the propensity for a term deposit, as mentioned in the following code snippet:<p class="source-code">"""</p><p class="source-code">Relationship between having a loan and propensity for term </p><p class="source-code">deposits</p><p class="source-code">"""</p><p class="source-code">bankData.groupby(['loan', 'y'])['y']\</p><p class="source-code">        .agg(loanTot='count').reset_index()</p><p class="callout-heading">Note</p><p class="callout">The triple-quotes ( <strong class="source-inline">"""</strong> ) shown in the code snippet above are used to denote the start and end points of a multi-line code comment. This is an alternative to using the <strong class="source-inline">#</strong> symbol. </p><p>You should get the following output:</p><div id="_idContainer114" class="IMG---Figure"><img src="Images/B15019_03_15.jpg" alt="Figure 3.15: Loan versus term deposit propensity&#13;&#10;" width="1525" height="467"/></div><p class="figure-caption">Figure 3.15: Loan versus term deposit propensity</p><p>In the case of loan portfolios, the propensity to buy term deposits is higher for customers without loans: <strong class="source-inline">( 4805 / ( 4805 + 33162) = 12 % to  484/ ( 484 + 6760) =  6%)</strong>.</p><p>Housing and loans were categorical data and finding a relationship was straightforward. However, bank balance data is numerical and to analyze it, we need to have a different strategy. One common strategy is to convert the continuous numerical data into ordinal data and look at how the propensity varies across each category. </p></li>
				<li>To convert numerical values into ordinal values, we first find the quantile values and take them as threshold values. The quantiles are obtained using the following code snippet:<p class="source-code">#Taking the quantiles for 25%, 50% and 75% of the balance data</p><p class="source-code">import numpy as np</p><p class="source-code">np.quantile(bankData['balance'],[0.25,0.5,0.75])</p><p>You should get the following output:</p><div id="_idContainer115" class="IMG---Figure"><img src="Images/B15019_03_16.jpg" alt="Figure 3.16: Quantiles for bank balance data&#13;&#10;" width="1665" height="65"/></div><p class="figure-caption">Figure 3.16: Quantiles for bank balance data</p><p>Quantile values represent certain threshold values for data distribution. For example, when we say the 25<span class="superscript">th</span> quantile percentile, we are talking about a value below which <strong class="source-inline">25%</strong> of the data exists. The quantile can be calculated using the <strong class="source-inline">np.quantile()</strong> function in NumPy. In the code snippet of <em class="italic">Step 4</em>, we calculated the 25<span class="superscript">th</span>, 50<span class="superscript">th</span>, and 75<span class="superscript">th</span> percentiles, which resulted in <strong class="source-inline">72</strong>, <strong class="source-inline">448</strong>, and <strong class="source-inline">1428</strong>.</p></li>
				<li>Now, convert the numerical values of bank balances into categorical values, as mentioned in the following code snippet:<p class="source-code">bankData['balanceClass'] = 'Quant1'</p><p class="source-code">bankData.loc[(bankData['balance'] &gt; 72) \</p><p class="source-code">              &amp; (bankData['balance'] &lt; 448), \</p><p class="source-code">              'balanceClass'] = 'Quant2'</p><p class="source-code">bankData.loc[(bankData['balance'] &gt; 448) \</p><p class="source-code">              &amp; (bankData['balance'] &lt; 1428), \</p><p class="source-code">              'balanceClass'] = 'Quant3'</p><p class="source-code">bankData.loc[bankData['balance'] &gt; 1428, \</p><p class="source-code">             'balanceClass'] = 'Quant4'</p><p class="source-code">bankData.head()</p><p>You should get the following output:</p><div id="_idContainer116" class="IMG---Figure"><img src="Images/B15019_03_17.jpg" alt="Figure 3.17: New features from bank balance data&#13;&#10;" width="1221" height="192"/></div><p class="figure-caption">Figure 3.17: New features from bank balance data</p><p>We did this is by looking at the quantile thresholds we took in the <em class="italic">Step 4</em>, and categorizing the numerical data into the corresponding quantile class. For example, all values lower than the 25<span class="superscript">th</span> quantile value, 72, were classified as <strong class="source-inline">Quant1</strong>, values between 72 and 448 were classified as <strong class="source-inline">Quant2</strong>, and so on. To store the quantile categories, we created a new feature in the bank dataset called <strong class="source-inline">balanceClass</strong> and set its default value to <strong class="source-inline">Quan1</strong>. After this, based on each value threshold, the data points were classified to the respective quantile class.</p></li>
				<li>Next, we need to find the propensity of term deposit purchases based on each quantile the customers fall into. This task is similar to what we did in <em class="italic">Exercise 3.02</em>,<em class="italic"> Business Hypothesis Testing for Age versus Propensity for a Term Loan</em>:<p class="source-code"># Calculating the customers under each quantile </p><p class="source-code">balanceTot = bankData.groupby(['balanceClass'])['y']\</p><p class="source-code">                     .agg(balanceTot='count').reset_index()</p><p class="source-code">balanceTot</p><p>You should get the following output:</p><div id="_idContainer117" class="IMG---Figure"><img src="Images/B15019_03_18.jpg" alt="Figure 3:18: Classification based on quantiles&#13;&#10;" width="1665" height="429"/></div><p class="figure-caption">Figure 3:18: Classification based on quantiles</p></li>
				<li>Calculate the total number of customers categorized by quantile and propensity classification, as mentioned in the following code snippet:<p class="source-code">"""</p><p class="source-code">Calculating the total customers categorised as per quantile </p><p class="source-code">and propensity classification</p><p class="source-code">"""</p><p class="source-code">balanceProp = bankData.groupby(['balanceClass', 'y'])['y']\</p><p class="source-code">                      .agg(balanceCat='count').reset_index()</p><p class="source-code">balanceProp</p><p>You should get the following output:</p><div id="_idContainer118" class="IMG---Figure"><img src="Images/B15019_03_19.jpg" alt="Figure 3.19: Total number of customers categorized by quantile &#13;&#10;and propensity classification&#13;&#10;" width="800" height="413"/></div><p class="figure-caption">Figure 3.19: Total number of customers categorized by quantile and propensity classification</p></li>
				<li>Now, <strong class="source-inline">merge</strong> both DataFrames:<p class="source-code"># Merging both the data frames</p><p class="source-code">balanceComb = pd.merge(balanceProp, balanceTot, \</p><p class="source-code">                       on = ['balanceClass'])</p><p class="source-code">balanceComb['catProp'] = (balanceComb.balanceCat \</p><p class="source-code">                          / balanceComb.balanceTot)*100</p><p class="source-code">balanceComb</p><p>You should get the following output:</p><div id="_idContainer119" class="IMG---Figure"><img src="Images/B15019_03_20.jpg" alt="Figure 3.20: Propensity versus balance category&#13;&#10;" width="816" height="421"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.20: Propensity versus balance category</p>
			<p>From the distribution of data, we can see that, as we move from Quantile 1 to Quantile 4, the proportion of customers who buy term deposits keeps on increasing. For instance, of all of the customers who belong to <strong class="source-inline">Quant 1</strong>, 7.25% have bought term deposits (we get this percentage from <strong class="source-inline">catProp</strong>). This proportion increases to 10.87 % for <strong class="source-inline">Quant 2</strong> and thereafter to 12.52 % and 16.15% for <strong class="source-inline">Quant 3</strong> and <strong class="source-inline">Quant4</strong>, respectively. From this trend, we can conclude that individuals with higher balances have more propensity for term deposits.</p>
			<p>In this exercise, we explored the relationship of each variable to the propensity for term deposit purchases. The overall trend that we can observe is that people with more cash in hand (no loans and a higher balance) have a higher propensity to buy term deposits. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3g7rK0w">https://packt.live/3g7rK0w</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2PZbcNV">https://packt.live/2PZbcNV</a>.</p>
			<p>In the next exercise, we will use these intuitions to derive a new feature.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Exercise 3.04: Feature Engineering – Creating New Features from Existing Ones</h2>
			<p>In this exercise, we will combine the individual variables we analyzed in <em class="italic">Exercise 3.03</em>,<em class="italic"> Feature Engineering – Exploration of Individual Features</em> to derive a new feature called an asset index. One methodology to create an asset index is by assigning weights based on the asset or liability of the customer. </p>
			<p>For instance, a higher bank balance or home ownership will have a positive bearing on the overall asset index and, therefore, will be assigned a higher weight. In contrast, the presence of a loan will be a liability and, therefore, will have to have a lower weight. Let's give a weight of 5 if the customer has a house and 1 in its absence. Similarly, we can give a weight of 1 if the customer has a loan and 5 in case of no loans:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the pandas and numpy package:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p></li>
				<li>Assign the link to the dataset to a variable called 'file_url'.<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter03/bank-full.csv'</p></li>
				<li>Read the banking dataset using the <strong class="source-inline">.read_csv()</strong> function:<p class="source-code"># Reading the banking data</p><p class="source-code">bankData = pd.read_csv(file_url,sep=";")</p></li>
				<li>The first step we will follow is to normalize the numerical variables. This is implemented using the following code snippet:<p class="source-code"># Normalizing data</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">x = bankData[['balance']].values.astype(float)</p></li>
				<li>As the bank balance dataset contains numerical values, we need to first normalize the data. The purpose of normalization is to bring all of the variables that we are using to create the new feature into a common scale. One effective method we can use here for the normalizing function is called <strong class="source-inline">MinMaxScaler()</strong>, which converts all of the numerical data between a scaled range of 0 to 1. The <strong class="source-inline">MinMaxScaler</strong> function is available within the <strong class="source-inline">preprocessing</strong> method in <strong class="source-inline">sklearn</strong>: <p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p></li>
				<li>Transform the balance data by normalizing it with <strong class="source-inline">minmaxScaler</strong>:<p class="source-code">bankData['balanceTran'] = minmaxScaler.fit_transform(x)</p><p>In this step, we created a new feature called <strong class="source-inline">'balanceTran'</strong> to store the normalized bank balance values.</p></li>
				<li>Print the head of the data using the <strong class="source-inline">.head()</strong> function:<p class="source-code">bankData.head()</p><p>You should get the following output:</p><div id="_idContainer120" class="IMG---Figure"><img src="Images/B15019_03_21.jpg" alt="Figure 3.21: Normalizing the bank balance data&#13;&#10;" width="1218" height="189"/></div><p class="figure-caption">Figure 3.21: Normalizing the bank balance data</p></li>
				<li>After creating the normalized variable, add a small value of <strong class="source-inline">0.001</strong> so as to eliminate the 0 values in the variable. This is mentioned in the following code snippet: <p class="source-code"># Adding a small numerical constant to eliminate 0 values</p><p class="source-code">bankData['balanceTran'] = bankData['balanceTran'] + 0.00001</p><p>The purpose of adding this small value is because, in the subsequent steps, we will be multiplying three transformed variables together to form a composite index. The small value is added to avoid the variable values becoming 0 during the multiplying operation. </p></li>
				<li>Now, add two additional columns for introducing the transformed variables for loans and housing, as per the weighting approach discussed at the start of this exercise:<p class="source-code"># Let us transform values for loan data</p><p class="source-code">bankData['loanTran'] = 1</p><p class="source-code"># Giving a weight of 5 if there is no loan</p><p class="source-code">bankData.loc[bankData['loan'] == 'no', 'loanTran'] = 5</p><p class="source-code">bankData.head()</p><p>You should get the following output:</p><div id="_idContainer121" class="IMG---Figure"><img src="Images/B15019_03_22.jpg" alt="Figure 3.22: Additional columns with the transformed variables&#13;&#10;" width="1258" height="188"/></div><p class="figure-caption">Figure 3.22: Additional columns with the transformed variables</p><p>We transformed values for the loan data as per the weighting approach. When a customer has a loan, it is given a weight of <strong class="source-inline">1</strong>, and when there's no loan, the weight assigned is <strong class="source-inline">5</strong>. The value of <strong class="source-inline">1</strong> and <strong class="source-inline">5</strong> are intuitive weights we are assigning. What values we assign can vary based on the business context you may be provided with.</p></li>
				<li>Now, transform values for the <strong class="source-inline">Housing data</strong>, as mentioned here:<p class="source-code"># Let us transform values for Housing data</p><p class="source-code">bankData['houseTran'] = 5</p></li>
				<li>Give a weight of <strong class="source-inline">1</strong> if the customer has a house and print the results, as mentioned in the following code snippet:<p class="source-code">bankData.loc[bankData['housing'] == 'no', 'houseTran'] = 1</p><p class="source-code">print(bankData.head())</p><p> You should get the following output:</p><div id="_idContainer122" class="IMG---Figure"><img src="Images/B15019_03_23.jpg" alt="Figure 3.23: Transforming loan and housing data&#13;&#10;" width="647" height="110"/></div><p class="figure-caption">Figure 3.23: Transforming loan and housing data</p><p>Once all the transformed variables are created, we can multiply all of the transformed variables together to create a new index called <strong class="source-inline">assetIndex</strong>. This is a composite index that represents the combined effect of all three variables.</p></li>
				<li>Now, create a new variable, which is the product of all of the transformed variables:<p class="source-code">""" </p><p class="source-code">Let us now create the new variable which is a product of all </p><p class="source-code">these</p><p class="source-code">"""</p><p class="source-code">bankData['assetIndex'] = bankData['balanceTran'] \</p><p class="source-code">                         * bankData['loanTran'] \</p><p class="source-code">                         * bankData['houseTran']</p><p class="source-code">bankData.head()</p><p>You should get the following output:</p><div id="_idContainer123" class="IMG---Figure"><img src="Images/B15019_03_24.jpg" alt="Figure 3.24: Creating a composite index&#13;&#10;" width="1180" height="146"/></div><p class="figure-caption">Figure 3.24: Creating a composite index</p></li>
				<li>Explore the propensity with respect to the composite index.<p>We observe the relationship between the asset index and the propensity of term deposit purchases. We adopt a similar strategy of converting the numerical values of the asset index into ordinal values by taking the quantiles and then mapping the quantiles to the propensity of term deposit purchases, as mentioned in <em class="italic">Exercise 3.03</em>,<em class="italic"> Feature Engineering – Exploration of Individual Features</em>:</p><p class="source-code"># Finding the quantile</p><p class="source-code">np.quantile(bankData['assetIndex'],[0.25,0.5,0.75])</p><p>You should get the following output:</p><div id="_idContainer124" class="IMG---Figure"><img src="Images/B15019_03_25.jpg" alt="Figure 3.25: Conversion of numerical values into ordinal values&#13;&#10;" width="1340" height="71"/></div><p class="figure-caption">Figure 3.25: Conversion of numerical values into ordinal values</p></li>
				<li>Next, create quantiles from the <strong class="source-inline">assetindex</strong> data, as mentioned in the following code snippet:<p class="source-code">bankData['assetClass'] = 'Quant1'</p><p class="source-code">bankData.loc[(bankData['assetIndex'] &gt; 0.38) \</p><p class="source-code">              &amp; (bankData['assetIndex'] &lt; 0.57), \</p><p class="source-code">              'assetClass'] = 'Quant2'</p><p class="source-code">bankData.loc[(bankData['assetIndex'] &gt; 0.57) \</p><p class="source-code">              &amp; (bankData['assetIndex'] &lt; 1.9), \</p><p class="source-code">              'assetClass'] = 'Quant3'</p><p class="source-code">bankData.loc[bankData['assetIndex'] &gt; 1.9, \</p><p class="source-code">             'assetClass'] = 'Quant4'</p><p class="source-code">bankData.head()</p><p class="source-code">bankData.assetClass[bankData['assetIndex'] &gt; 1.9] = 'Quant4'</p><p class="source-code">bankData.head()</p><p>You should get the following output:</p><div id="_idContainer125" class="IMG---Figure"><img src="Images/B15019_03_26.jpg" alt="Figure 3.26: Quantiles for the asset index&#13;&#10;" width="1562" height="189"/></div><p class="figure-caption">Figure 3.26: Quantiles for the asset index</p></li>
				<li>Calculate the total of each asset class and the category-wise counts, as mentioned in the following code snippet:<p class="source-code"># Calculating total of each asset class</p><p class="source-code">assetTot = bankData.groupby('assetClass')['y']\</p><p class="source-code">                   .agg(assetTot='count').reset_index()</p><p class="source-code"># Calculating the category wise counts</p><p class="source-code">assetProp = bankData.groupby(['assetClass', 'y'])['y']\</p><p class="source-code">                    .agg(assetCat='count').reset_index()</p></li>
				<li>Next, merge both DataFrames:<p class="source-code"># Merging both the data frames</p><p class="source-code">assetComb = pd.merge(assetProp, assetTot, on = ['assetClass'])</p><p class="source-code">assetComb['catProp'] = (assetComb.assetCat \</p><p class="source-code">                        / assetComb.assetTot)*100</p><p class="source-code">assetComb</p><p>You should get the following output:</p><div id="_idContainer126" class="IMG---Figure"><img src="Images/B15019_03_27.jpg" alt="Figure 3.27: Composite index relationship mapping&#13;&#10;" width="968" height="490"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.27: Composite index relationship mapping</p>
			<p>From the new feature we created, we can see that 18.88% (we get this percentage from <strong class="source-inline">catProp</strong>) of customers who are in <strong class="source-inline">Quant2</strong> have bought term deposits compared to 10.57 % for <strong class="source-inline">Quant1</strong>, 8.78% for <strong class="source-inline">Quant3</strong>, and 9.28% for <strong class="source-inline">Quant4</strong>. Since <strong class="source-inline">Quant2</strong> has the highest proportion of customers who have bought term deposits, we can conclude that customers in <strong class="source-inline">Quant2</strong> have higher propensity to purchase the term deposits than all other customers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/316hUrO">https://packt.live/316hUrO</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3kVc7Ny">https://packt.live/3kVc7Ny</a>.</p>
			<p>Similar to the exercise that we just completed, you should think of new variables that can be created from the existing variables based on business intuitions. Creating new features based on business intuitions is the essence of business-driven feature engineering. In the next section, we will look at another type of feature engineering called data-driven feature engineering.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Data-Driven Feature Engineering</h1>
			<p>The previous section dealt with business-driven feature engineering. In addition to features we can derive from the business perspective, it would also be imperative to transform data through feature engineering from the perspective of data structures. We will look into different methods of identifying data structures and take a peek into some data transformation techniques.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>A Quick Peek at Data Types and a Descriptive Summary</h2>
			<p>Looking at the data types such as categorical or numeric and then deriving summary statistics is a good way to take a quick peek into data before you do some of the downstream feature engineering steps. Let's take a look at an example from our dataset:</p>
			<p class="source-code"># Looking at Data types</p>
			<p class="source-code">print(bankData.dtypes)</p>
			<p class="source-code"># Looking at descriptive statistics</p>
			<p class="source-code">print(bankData<strong class="bold">.describe()</strong>)</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="Images/B15019_03_28.jpg" alt="Figure 3.28: Output showing the different data types in the dataset&#13;&#10;" width="520" height="435"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28: Output showing the different data types in the dataset</p>
			<p>In the preceding output, you see the different types of information in the dataset and its corresponding data types. For instance, <strong class="source-inline">age</strong> is an integer and so is <strong class="source-inline">day</strong>.</p>
			<p>The following output is that of a descriptive summary statistic, which displays some of the basic measures such as <strong class="source-inline">mean</strong>, <strong class="source-inline">standard deviation</strong>, <strong class="source-inline">count</strong>, and the <strong class="source-inline">quantile values</strong> of the respective features:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/B15019_03_29.jpg" alt="Figure 3.29: Data types and a descriptive summary&#13;&#10;" width="743" height="553"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29: Data types and a descriptive summary</p>
			<p>The purpose of a descriptive summary is to get a quick feel of the data with respect to the distribution and some basic statistics such as mean and standard deviation. Getting a perspective on the summary statistics is critical for thinking about what kind of transformations are required for each variable. </p>
			<p>For instance, in the earlier exercises, we converted the numerical data into categorical variables based on the quantile values. Intuitions for transforming variables would come from the quick summary statistics that we can derive from the dataset.</p>
			<p>In the following sections, we will be looking at the correlation matrix and visualization.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Correlation Matrix and Visualization</h1>
			<p>Correlation, as you know, is a measure that indicates how two variables fluctuate together. Any correlation value of 1, or near 1, indicates that those variables are highly correlated. Highly correlated variables can sometimes be damaging for the veracity of models and, in many circumstances, we make the decision to eliminate such variables or to combine them to form composite or interactive variables. </p>
			<p>Let's look at how data correlation can be generated and then visualized in the following exercise.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Exercise 3.05: Finding the Correlation in Data to Generate a Correlation Plot Using Bank Data</h2>
			<p>In this exercise, we will be creating a correlation plot and analyzing the results of the bank dataset.</p>
			<p>The following steps will help you to complete the exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook, install the <strong class="source-inline">pandas</strong> packages and load the banking data:<p class="source-code">import pandas as pd</p><p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter03/bank-full.csv'</p><p class="source-code">bankData = pd.read_csv(file_url, sep=";")</p></li>
				<li>Now, <strong class="source-inline">import</strong> the <strong class="source-inline">set_option</strong> library from <strong class="source-inline">pandas</strong>, as mentioned here: <p class="source-code">from pandas import set_option</p><p>The <strong class="source-inline">set_option</strong> function is used to define the display options for many operations.</p></li>
				<li>Next, create a variable that would store numerical variables such as <strong class="source-inline">'age','balance','day','duration','campaign','pdays','previous', </strong>as mentioned in the following code snippet. A correlation plot can be extracted only with numerical data. This is why the numerical data has to be extracted separately: <p class="source-code">bankNumeric = bankData[['age','balance','day','duration',\</p><p class="source-code">                        'campaign','pdays','previous']]</p></li>
				<li>Now, use the <strong class="source-inline">.corr()</strong> function to find the correlation matrix for the dataset:<p class="source-code">set_option('display.width',150)</p><p class="source-code">set_option('precision',3)</p><p class="source-code">bankCorr = bankNumeric.corr(method = 'pearson')</p><p class="source-code">bankCorr</p><p>You should get the following output:</p><div id="_idContainer129" class="IMG---Figure"><img src="Images/B15019_03_30.jpg" alt="Figure 3.30: Correlation matrix&#13;&#10;" width="975" height="442"/></div><p class="figure-caption">Figure 3.30: Correlation matrix</p><p>The method we use for correlation is the <strong class="bold">Pearson</strong> correlation coefficient. We can see from the correlation matrix that the diagonal elements have a correlation of 1. This is because the diagonals are a correlation of a variable with itself, which will always be 1. This is the Pearson correlation coefficient.</p></li>
				<li>Now, plot the data:<p class="source-code">from matplotlib import pyplot</p><p class="source-code">corFig = pyplot.figure()</p><p class="source-code">figAxis = corFig.add_subplot(111)</p><p class="source-code">corAx = figAxis.matshow(bankCorr,vmin=-1,vmax=1)</p><p class="source-code">corFig.colorbar(corAx)</p><p class="source-code">pyplot.show()</p><p>You should get the following output:</p><div id="_idContainer130" class="IMG---Figure"><img src="Images/B15019_03_31.jpg" alt="Figure 3.31: Correlation plot&#13;&#10;" width="438" height="257"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.31: Correlation plot</p>
			<p>We used many plotting parameters in this code block. <strong class="source-inline">pyplot.figure()</strong> is the plotting class that is instantiated. <strong class="source-inline">.add_subplot()</strong> is a grid parameter for the plotting. For example, 111 means a 1 x 1 grid for the first subplot. The <strong class="source-inline">.matshow()</strong> function is to display the plot, and the <strong class="source-inline">vmin</strong> and <strong class="source-inline">vmax</strong> arguments are for normalizing the data in the plot.</p>
			<p>Let's look at the plot of the correlation matrix to visualize the matrix for quicker identification of correlated variables. Some obvious candidates are the high correlation between <strong class="source-inline">'balance'</strong> and <strong class="source-inline">'balanceTran'</strong> and the <strong class="source-inline">'asset index'</strong> with many of the transformed variables that we created in the earlier exercise. Other than that, there aren't many variables that are highly correlated.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3kXr9SK">https://packt.live/3kXr9SK</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gbfbkR">https://packt.live/3gbfbkR</a>.</p>
			<p>In this exercise, we developed a correlation plot that allows us to visualize the correlation between variables.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Skewness of Data</h2>
			<p>Another area for feature engineering is skewness. Skewed data means data that is shifted in one direction or the other. Skewness can cause machine learning models to underperform. Many machine learning models assume normally distributed data or data structures to follow the Gaussian structure. Any deviation from the assumed Gaussian structure, which is the popular bell curve, can affect model performance. A very effective area where we can apply feature engineering is by looking at the skewness of data and then correcting the skewness through normalization of the data. Skewness can be visualized by plotting the data using histograms and density plots. We will investigate each of these techniques.</p>
			<p>Let's take a look at the following example. Here, we use the <strong class="source-inline">.skew()</strong> function to find the skewness in data. For instance, to find the skewness of data in our <strong class="source-inline">bank-full.csv</strong> dataset, we perform the following:</p>
			<p class="source-code"># Skewness of numeric attributes</p>
			<p class="source-code">bankNumeric<strong class="bold">.skew()</strong></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This code refers to the <strong class="source-inline">bankNumeric</strong> data, so you should ensure you are working in the same notebook as the previous exercise.</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="Images/B15019_03_32.jpg" alt="Figure 3.32: Degree of skewness &#13;&#10;" width="889" height="294"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.32: Degree of skewness </p>
			<p>The preceding matrix is the skewness index. Any value closer to 0 indicates a low degree of skewness. Positive values indicate right skew and negative values, left skew. Variables that show higher values of right skew and left skew are candidates for further feature engineering by normalization. Let's now visualize the skewness by plotting histograms and density plots.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Histograms</h2>
			<p>Histograms are an effective way to plot the distribution of data and to identify skewness in data, if any. The histogram outputs of two columns of <strong class="source-inline">bankData</strong> are listed here. The histogram is plotted with the <strong class="source-inline">pyplot</strong> package from <strong class="source-inline">matplotlib</strong> using the <strong class="source-inline">.hist()</strong> function. The number of subplots we want to include is controlled by the <strong class="source-inline">.subplots()</strong> function. <strong class="source-inline">(1,2)</strong> in subplots would mean one row and two columns. The titles are set by the <strong class="source-inline">set_title()</strong> function:</p>
			<p class="source-code"># Histograms</p>
			<p class="source-code">from matplotlib import pyplot as plt</p>
			<p class="source-code">fig, axs = plt.subplots(1,2)</p>
			<p class="source-code">axs[0].hist(bankNumeric['age'])</p>
			<p class="source-code">axs[0].set_title('Distribution of age')</p>
			<p class="source-code">axs[1].hist(bankNumeric['balance'])</p>
			<p class="source-code">axs[1].set_title('Distribution of Balance')</p>
			<p class="source-code"># Ensure plots do not overlap</p>
			<p class="source-code">plt.tight_layout()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/B15019_03_33.jpg" alt="Figure 3.33: Code showing the generation of histograms&#13;&#10;" width="452" height="280"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.33: Code showing the generation of histograms</p>
			<p>From the histogram, we can see that the <strong class="source-inline">age</strong> variable has a distribution closer to the bell curve with a lower degree of skewness. In contrast, the asset index shows a relatively higher right skew, which makes it a more probable candidate for normalization. </p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Density Plots</h2>
			<p>Density plots help in visualizing the distribution of data. A density plot can be created using the <strong class="source-inline">kind = 'density'</strong> parameter: </p>
			<p class="source-code">from matplotlib import pyplot as plt</p>
			<p class="source-code"># Density plots</p>
			<p class="source-code">bankNumeric['age'].plot(kind = 'density', subplots = False, \</p>
			<p class="source-code">                        layout = (1,1))</p>
			<p class="source-code">plt.title('Age Distribution')</p>
			<p class="source-code">plt.xlabel('Age')</p>
			<p class="source-code">plt.ylabel('Normalised age distribution')</p>
			<p class="source-code">pyplot.show()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="Images/B15019_03_34.jpg" alt="Figure 3.34: Code showing the generation of a density plot&#13;&#10;" width="421" height="279"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.34: Code showing the generation of a density plot</p>
			<p>Density plots help in getting a smoother visualization of the distribution of the data. From the density plot of Age, we can see that it has a distribution similar to a bell curve.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Other Feature Engineering Methods</h2>
			<p>So far, we were looking at various descriptive statistics and visualizations that are precursors for applying many feature engineering techniques on data structures. We investigated one such feature engineering technique in <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em> where we applied the <strong class="bold">min max</strong> scaler for normalizing data. </p>
			<p>We will now look into two other similar data transformation techniques, namely, standard scaler and normalizer. Standard scaler standardizes data to a mean of 0 and standard deviation of 1. The mean is the average of the data and the standard deviation is a measure of the spread of data. By standardizing to the same mean and standard deviation, comparison across different distributions of data is enabled.</p>
			<p>The normalizer function normalizes the length of data. This means that each value in a row is divided by the normalization of the row vector to normalize the row. The normalizer function is applied on the rows while standard scaler is applied columnwise. The normalizer and standard scaler functions are important feature engineering steps that are applied to the data before downstream modeling steps. Let's look at both of these techniques:</p>
			<p class="source-code"># Standardize data (0 mean, 1 stdev)</p>
			<p class="source-code">from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">from numpy import set_printoptions</p>
			<p class="source-code">scaling = StandardScaler().fit(bankNumeric)</p>
			<p class="source-code">rescaledNum = scaling.transform(bankNumeric)</p>
			<p class="source-code">set_printoptions(precision = 3)</p>
			<p class="source-code">print(rescaledNum)</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="Images/B15019_03_35.jpg" alt="Figure 3.35: Output from standardizing the data&#13;&#10;" width="452" height="130"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.35: Output from standardizing the data</p>
			<p>The following code uses the normalizer data transmission techniques:</p>
			<p class="source-code"># Normalizing Data (Length of 1)</p>
			<p class="source-code">from sklearn.preprocessing import Normalizer</p>
			<p class="source-code">normaliser = Normalizer().fit(bankNumeric)</p>
			<p class="source-code">normalisedNum = normaliser.transform(bankNumeric)</p>
			<p class="source-code">set_printoptions(precision = 3)</p>
			<p class="source-code">print(normalisedNum)</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="Images/B15019_03_36.jpg" alt="Figure 3.36 Output by the normalizer&#13;&#10;" width="621" height="134"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.36 Output by the normalizer</p>
			<p>The output from standard scaler is normalized along the columns. The output would have 11 columns corresponding to 11 numeric columns (age, balance, day, duration, and so on). If we observe the output, we can see that each value along a column is normalized so as to have a mean of 0 and standard deviation of 1. By transforming data in this way, we can easily compare across columns. </p>
			<p>For instance, in the <strong class="source-inline">age</strong> variable, we have data ranging from 18 up to 95. In contrast, for the balance data, we have data ranging from -8,019 to 102,127. We can see that both of these variables have different ranges of data that cannot be compared. The standard scaler function converts these data points at very different scales into a common scale so as to compare the distribution of data. Normalizer rescales each row so as to have a vector with a length of 1. </p>
			<p>The big question we have to think about is why do we have to standardize or normalize data? Many machine learning algorithms converge faster when the features are of a similar scale or are normally distributed. Standardizing is more useful in algorithms that assume input variables to have a Gaussian structure. Algorithms such as linear regression, logistic regression, and linear discriminate analysis fall under this genre. Normalization techniques would be more congenial for sparse datasets (datasets with lots of zeros) when using algorithms such as k-nearest neighbor or neural networks.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Summarizing Feature Engineering</h2>
			<p>In this section, we investigated the process of feature engineering from a business perspective and data structure perspective. Feature engineering is a very important step in the life cycle of a data science project and helps determine the veracity of the models that we build. As seen in <em class="italic">Exercise 3.02</em>, <em class="italic">Business Hypothesis Testing for Age versus Propensity for a Term Loan</em> we translated our understanding of the domain and our intuitions to build intelligent features. Let's summarize the processes that we followed:</p>
			<ol>
				<li value="1">We obtain intuitions from a business perspective through EDA</li>
				<li>Based on the business intuitions, we devised a new feature that is a combination of three other variables.</li>
				<li>We verified the influence of constituent variables of the new feature and devised an approach for weights to be applied.</li>
				<li>Converted ordinal data into corresponding weights. </li>
				<li>Transformed numerical data by normalizing them using an appropriate normalizer.</li>
				<li>Combined all three variables into a new feature.</li>
				<li>Observed the relationship between the composite index and the propensity to purchase term deposits and derived our intuitions.</li>
				<li>Explored techniques for visualizing and extracting summary statistics from data.</li>
				<li>Identified techniques for transforming data into feature engineered data structures.</li>
			</ol>
			<p>Now that we have completed the feature engineering step, the next question is where do we go from here and what is the relevance of the new feature we created? As you will see in the subsequent sections, the new features that we created will be used for the modeling process. The preceding exercises are an example of a trail we can follow in creating new features. There will be multiple trails like these, which should be thought of as based on more domain knowledge and understanding. The veracity of the models that we build will be dependent on all such intelligent features we can build by translating business knowledge into data. </p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Building a Binary Classification Model Using the Logistic Regression Function</h2>
			<p>The essence of data science is about mapping a business problem into its data elements and then transforming those data elements to get our desired business outcomes. In the previous sections, we discussed how we do the necessary transformation on the data elements. The right transformation of the data elements can highly influence the generation of the right business outcomes by the downstream modeling process.</p>
			<p>Let's look at the business outcome generation process from the perspective of our use case. The desired business outcome, in our use case, is to identify those customers who are likely to buy a term deposit. To correctly identify which customers are likely to buy a term deposit, we first need to learn the traits or features that, when present in a customer, helps in the identification process. This learning of traits is what is achieved through machine learning.</p>
			<p>By now, you may have realized that the goal of machine learning is to estimate a mapping function (<em class="italic">f</em>) between an output variable and input variables. In mathematical form, this can be written as follows:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="Images/B15019_03_37.jpg" alt="Figure 3.37: A mapping function in mathematical form&#13;&#10;" width="962" height="40"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.37: A mapping function in mathematical form</p>
			<p>Let's look at this equation from the perspective of our use case. </p>
			<p><em class="italic">Y</em> is the dependent variable, which is our prediction as to whether a customer has the probability to buy a term deposit or not. </p>
			<p><em class="italic">X</em> is the independent variable(s), which are those attributes such as age, education, and marital status and are part of the dataset.</p>
			<p><em class="italic">f()</em> is a function that connects various attributes of the data to the probability or whether a customer will buy a term deposit or not. This function is learned during the machine learning process. This function is a combination of different coefficients or parameters applied to each of the attributes to get the probability of term deposit purchases. Let's unravel this concept using a simple example of our bank data use case.</p>
			<p>For simplicity, let's assume that we have only two attributes, age and bank balance. Using these, we have to predict whether a customer is likely to buy a term deposit or not. Let the age be 40 years and the balance $1,000. With all of these attribute values, let's assume that the mapping equation is as follows:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="Images/B15019_03_38.jpg" alt="Figure 3.38: Updated mapping equation&#13;&#10;" width="1665" height="86"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.38: Updated mapping equation</p>
			<p>Using the preceding equation, we get the following:</p>
			<p><em class="italic">Y = 0.1 + 0.4 * 40 + 0.002 * 1000</em></p>
			<p><em class="italic">Y = 18.1</em></p>
			<p>Now, you might be wondering, we are getting a real number and how does this represent a decision of whether a customer will buy a term deposit or not? This is where the concept of a decision boundary comes in. Let's also assume that, on analyzing the data, we have also identified that if the value of <em class="italic">Y</em> goes above 15 (an assumed value in this case), then the customer is likely to buy the term deposit, otherwise they will not buy a term deposit. This means that, as per this example, the customer is likely to buy a term deposit. </p>
			<p>Let's now look at the dynamics in this example and try to decipher the concepts. The values such as 0.1, 0.4, and 0.002, which are applied to each of the attributes, are the coefficients. These coefficients, along with the equation connecting the coefficients and the variables, are the functions that we are learning from the data. The essence of machine learning is to learn all of these from the provided data. All of these coefficients along with the functions can also be called by another common name called the <strong class="bold">model</strong>. A model is an approximation of the data generation process. During machine learning, we are trying to get as close to the real model that has generated the data we are analyzing. To learn or estimate the data generating models, we use different machine learning algorithms. </p>
			<p>Machine learning models can be broadly classified into two types, parametric models and non-parametric models. Parametric models are where we assume the form of the function we are trying to learn and then learn the coefficients from the training data. By assuming a form for the function, we simplify the learning process. </p>
			<p>To understand the concept better, let's take the example of a linear model. For a linear model, the mapping function takes the following form:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="Images/B15019_03_39.jpg" alt="Figure 3.39: Linear model mapping function&#13;&#10;" width="1665" height="100"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.39: Linear model mapping function</p>
			<p>The terms <em class="italic">C</em><em class="italic">0</em>, <em class="italic">M</em><em class="italic">1</em>, and <em class="italic">M</em><em class="italic">2</em> are the coefficients of the line that influences the intercept and slope of the line. <em class="italic">X</em><em class="italic">1</em> and <em class="italic">X</em><em class="italic">2</em> are the input variables. What we are doing here is that we assume that the data generating model is a linear model and then, using the data, we estimate the coefficients, which will enable the generation of the predictions. By assuming the data generating model, we have simplified the whole learning process. However, these simple processes also come with their pitfalls. Only if the underlying function is linear or similar to linear will we get good results. If the assumptions about the form are wrong, we are bound to get bad results. </p>
			<p>Some examples of parametric models include:</p>
			<ul>
				<li>Linear and logistic regression</li>
				<li>Naïve Bayes</li>
				<li>Linear support vector machines</li>
				<li>Perceptron</li>
			</ul>
			<p>Machine learning models that do not make strong assumptions on the function are called non-parametric models. In the absence of an assumed form, non-parametric models are free to learn any functional form from the data. Non-parametric models generally require a lot of training data to estimate the underlying function. Some examples of non-parametric models include the following:</p>
			<ul>
				<li>Decision trees</li>
				<li>K –nearest neighbors</li>
				<li>Neural networks</li>
				<li>Support vector machines with Gaussian kernels</li>
			</ul>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Logistic Regression Demystified</h2>
			<p>Logistic regression is a linear model similar to the linear regression that was covered in the previous chapter. At the core of logistic regression is the sigmoid function, which quashes any real-valued number to a value between 0 and 1, which renders this function ideal for predicting probabilities. The mathematical equation for a logistic regression function can be written as follows:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="Images/B15019_03_40.jpg" alt="Figure 3.40: Logistic regression function&#13;&#10;" width="1665" height="255"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.40: Logistic regression function</p>
			<p>Here, <em class="italic">Y</em> is the probability of whether a customer is likely to buy a term deposit or not. </p>
			<p>The terms <em class="italic">C0 + M1 * X1 + M2 * X2</em> are very similar to the ones we have seen in the linear regression function, covered in an earlier chapter. As you would have learned by now, a linear regression function gives a real-valued output. To transform the real-valued output into a probability, we use the logistic function, which has the following form:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="Images/B15019_03_41.jpg" alt="Figure 3.41: An expression to transform the real-valued output to a probability &#13;&#10;" width="1665" height="208"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.41: An expression to transform the real-valued output to a probability </p>
			<p>Here, <em class="italic">e</em> is the natural logarithm. We will not dive deep into the math behind this; however, let's realize that, using the logistic function, we can transform the real-valued output into a probability function.</p>
			<p>Let's now look at the logistic regression function from the business problem that we are trying to solve. In the business problem, we are trying to predict the probability of whether a customer would buy a term deposit or not. To do that, let's return to the example we derived from the problem statement:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="Images/B15019_03_42.jpg" alt="Figure 3.42: The logistic regression function updated with &#13;&#10;the business problem statement&#13;&#10;" width="1059" height="64"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.42: The logistic regression function updated with the business problem statement</p>
			<p>Adding the following values, we get <em class="italic">Y = 0.1 + 0.4 * 40 + 0.002 * 100</em>.</p>
			<p>To get the probability, we must transform this problem statement using the logistic function, as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="Images/B15019_03_43.jpg" alt="Figure 3.43: Transformed problem statement to find the probability &#13;&#10;of using the logistic function&#13;&#10;" width="1665" height="232"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.43: Transformed problem statement to find the probability of using the logistic function</p>
			<p>In applying this, we get a value of <em class="italic">Y = 1</em>, which is a 100% probability that the customer will buy the term deposit. As discussed in the previous example, the coefficients of the model such as 0.1, 0.4, and 0.002 are what we learn using the logistic regression algorithm during the training process. </p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Metrics for Evaluating Model Performance</h2>
			<p>As a data scientist, you always have to make decisions on the models you build. These evaluations are done based on various metrics on the predictions. In this section, we introduce some of the important metrics that are used for evaluating the performance of models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Model performance will be covered in much more detail in <em class="italic">Chapter 6</em>, <em class="italic">How to Assess Performance</em>. This section provides you with an introduction to work with classification models.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Confusion Matrix</h2>
			<p>As you will have learned, we evaluate a model based on its performance on a test set. A test set will have its labels, which we call the ground truth, and, using the model, we also generate predictions for the test set. The evaluation of model performance is all about comparison of the ground truth and the predictions. Let's see this in action with a dummy test set:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="Images/B15019_03_44.jpg" alt="Figure 3.44: Confusion matrix generation&#13;&#10;" width="785" height="411"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.44: Confusion matrix generation</p>
			<p>The preceding table shows a dummy dataset with seven examples. The second column is the ground truth, which are the actual labels, and the third column contains the results of our predictions. From the data, we can see that four have been correctly classified and three were misclassified.</p>
			<p>A confusion matrix generates the resultant comparison between prediction and ground truth, as represented in the following table:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="Images/B15019_03_45.jpg" alt="Figure 3.45: Confusion matrix&#13;&#10;" width="1665" height="259"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.45: Confusion matrix</p>
			<p>As you can see from the table, there are five examples whose labels (ground truth) are<strong class="source-inline"> Yes</strong> and the balance is two examples that have the labels of<strong class="source-inline"> No</strong>. </p>
			<p>The first row of the confusion matrix is the evaluation of the label <strong class="source-inline">Yes</strong>. <strong class="source-inline">True positive</strong> shows those examples whose ground truth and predictions are <strong class="source-inline">Yes</strong> (examples 1, 3, and 5). <strong class="source-inline">False negative</strong> shows those examples whose ground truth is <strong class="source-inline">Yes</strong> and who have been wrongly predicted as <strong class="source-inline">No</strong> (examples 2 and 7).</p>
			<p>Similarly, the second row of the confusion matrix evaluates the performance of the label <strong class="source-inline">No</strong>. <strong class="source-inline">False positive</strong> are those examples whose ground truth is <strong class="source-inline">No</strong> and who have been wrongly classified as <strong class="source-inline">Yes</strong> (example 6). <strong class="source-inline">True negative</strong> examples are those examples whose ground truth and predictions are both <strong class="source-inline">No</strong> (example 4).</p>
			<p>The generation of a confusion matrix is used for calculating many of the matrices such as accuracy and classification reports, which are explained later. It is based on metrics such as accuracy or other detailed metrics shown in the classification report such as precision or recall the models for testing. We generally pick models where these metrics are the highest.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Accuracy</h2>
			<p>Accuracy is the first level of evaluation, which we will resort to in order to have a quick check on model performance. Referring to the preceding table, accuracy can be represented as follows:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="Images/B15019_03_46.jpg" alt="Figure 3.46: A function that represents accuracy&#13;&#10;" width="1558" height="142"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.46: A function that represents accuracy</p>
			<p>Accuracy is the proportion of correct predictions out of all of the predictions.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>Classification Report</h2>
			<p>A classification report outputs three key metrics: <strong class="bold">precision</strong>, <strong class="bold">recall</strong>, and the <strong class="bold">F1 score</strong>. </p>
			<p>Precision is the ratio of true positives to the sum of true positives and false positives:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="Images/B15019_03_47.jpg" alt="Figure 3.47: The precision ratio&#13;&#10;" width="1665" height="134"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.47: The precision ratio</p>
			<p>Precision is the indicator that tells you, out of all of the positives that were predicted, how many were true positives.</p>
			<p>Recall is the ratio of true positives to the sum of true positives and false negatives:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="Images/B15019_03_48.jpg" alt="Figure 3.48: The recall ratio&#13;&#10;" width="1665" height="143"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.48: The recall ratio</p>
			<p>Recall manifests the ability of the model to identify all true positives.</p>
			<p>The F1 score is a weighted score of both precision and recall. An F1 score of 1 indicates the best performance and 0 indicates the worst performance.</p>
			<p>In the next section, let's take a look at data preprocessing, which is an important process to work with data and come to conclusions in data analysis.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Data Preprocessing</h2>
			<p>Data preprocessing has an important role to play in the life cycle of data science projects. These processes are often the most time-consuming part of the data science life cycle. Careful implementation of the preprocessing steps is critical and will have a strong bearing on the results of the data science project.</p>
			<p>The various preprocessing steps include the following:</p>
			<ul>
				<li><strong class="bold">Data loading</strong>: This involves loading the data from different sources into the notebook.</li>
				<li><strong class="bold">Data cleaning</strong>: Data cleaning process entails removing anomalies, for instance, special characters, duplicate data, and identification of missing data from the available dataset. Data cleaning is one of the most time-consuming steps in the data science process.</li>
				<li><strong class="bold">Data imputation</strong>: Data imputation is filling missing data with new data points.</li>
				<li><strong class="bold">Converting data types</strong>: Datasets will have different types of data such as numerical data, categorical data, and character data. Running models will necessitate the transformation of data types. <p class="callout-heading">Note</p><p class="callout">Data processing will be covered in depth in the following chapters of this book.</p></li>
			</ul>
			<p>We will implement some of these preprocessing steps in the subsequent sections and in <em class="italic">Exercise 3.06</em>, <em class="italic">A Logistic Regression Model for Predicting the Propensity of Term Deposit Purchases in a Bank</em>.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>Exercise 3.06: A Logistic Regression Model for Predicting the Propensity of Term Deposit Purchases in a Bank</h2>
			<p>In this exercise, we will build a logistic regression model, which will be used for predicting the propensity of term deposit purchases. This exercise will have three parts. The first part will be the preprocessing of the data, the second part will deal with the training process, and the last part will be spent on prediction, analysis of metrics, and deriving strategies for further improvement of the model.</p>
			<p>You begin with data preprocessing.</p>
			<p>In this part, we will first load the data, convert the ordinal data into dummy data, and then split the data into training and test sets for the subsequent training phase:</p>
			<ol>
				<li value="1">Open a Colab notebook, mount the drives, install necessary packages, and load the data, as in previous exercises:<p class="source-code">import pandas as pd</p><p class="source-code">import altair as alt</p><p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter03/bank-full.csv'</p><p class="source-code">bankData = pd.read_csv(file_url, sep=";")</p></li>
				<li>Now, load the library functions and data:<p class="source-code">from sklearn.linear_model import <strong class="bold">LogisticRegression</strong></p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Now, find the data types:<p class="source-code">bankData<strong class="bold">.dtypes</strong></p><p>You should get the following output:</p><div id="_idContainer148" class="IMG---Figure"><img src="Images/B15019_03_49.jpg" alt="Figure 3.49: Data types&#13;&#10;" width="1159" height="760"/></div><p class="figure-caption">Figure 3.49: Data types</p></li>
				<li>Convert the ordinal data into dummy data.<p>As you can see in the dataset, we have two types of data: the numerical data and the ordinal data. Machine learning algorithms need numerical representation of data and, therefore, we must convert the ordinal data into a numerical form by creating dummy variables. The dummy variable will have values of either 1 or 0 corresponding to whether that category is present or not. The function we use for converting ordinal data into numerical form is <strong class="source-inline">pd.get_dummies()</strong>. This function converts the data structure into a long form or horizontal form. So, if there are three categories in a variable, there will be three new variables created as dummy variables corresponding to each of the categories. </p><p>The value against each variable would be either 1 or 0, depending on whether that category was present in the variable as an example. Let's look at the code for doing that:</p><p class="source-code">"""</p><p class="source-code">Converting all the categorical variables to dummy variables</p><p class="source-code">"""</p><p class="source-code">bankCat = pd.get_dummies\</p><p class="source-code">          (bankData[['job','marital',\</p><p class="source-code">                     'education','default','housing',\</p><p class="source-code">                     'loan','contact','month','poutcome']])</p><p class="source-code">bankCat.shape</p><p>You should get the following output:</p><p class="source-code">(45211, 44)</p><p>We now have a new subset of the data corresponding to the categorical data that was converted into numerical form. Also, we had some numerical variables in the original dataset, which did not need any transformation. The transformed categorical data and the original numerical data have to be combined to get all of the original features. To combine both, let's first extract the numerical data from the original DataFrame.</p></li>
				<li>Now, separate the numerical variables:<p class="source-code">bankNum = bankData[['age','balance','day','duration',\</p><p class="source-code">                    'campaign','pdays','previous']]</p><p class="source-code">bankNum.shape</p><p>You should get the following output:</p><p class="source-code">(45211, 7)</p></li>
				<li>Now, prepare the <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> variables and print the <strong class="source-inline">Y</strong> shape. The <strong class="source-inline">X</strong> variable is the concatenation of the transformed categorical variable and the separated numerical data:<p class="source-code"># Preparing the X variables</p><p class="source-code">X = pd.concat([bankCat, bankNum], axis=1)</p><p class="source-code">print(X.shape)</p><p class="source-code"># Preparing the Y variable</p><p class="source-code">Y = bankData['y']</p><p class="source-code">print(Y.shape)</p><p class="source-code">X.head()</p><p>The output shown below is truncated:</p><div id="_idContainer149" class="IMG---Figure"><img src="Images/B15019_03_50.jpg" alt="Figure 3.50 Combining categorical and numerical DataFrames&#13;&#10;" width="1375" height="421"/></div><p class="figure-caption">Figure 3.50 Combining categorical and numerical DataFrames</p><p>Once the DataFrame is created, we can split the data into training and test sets. We specify the proportion in which the DataFrame must be split into training and test sets.</p></li>
				<li>Split the data into training and test sets:<p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X, Y, test_size=0.3, \</p><p class="source-code">                                    random_state=123)</p><p>Now, the data is all prepared for the modeling task. Next, we begin with modeling.</p><p>In this part, we will train the model using the training set we created in the earlier step. First, we call the <strong class="source-inline">logistic regression </strong>function and then fit the model with the training set data.</p></li>
				<li>Define the <strong class="source-inline">LogisticRegression</strong> function:<p class="source-code">bankModel = LogisticRegression()</p><p class="source-code">bankModel.fit(X_train, y_train)</p><p>You should get the following output:</p><div id="_idContainer150" class="IMG---Figure"><img src="Images/B15019_03_51.jpg" alt="Figure 3.51: Parameters of the model that fits&#13;&#10;" width="707" height="82"/></div><p class="figure-caption">Figure 3.51: Parameters of the model that fits</p></li>
				<li>Now, that the model is created, use it for predicting on the test sets and then getting the accuracy level of the predictions:<p class="source-code">pred = bankModel.predict(X_test)</p><p class="source-code">print('Accuracy of Logistic regression model' \</p><p class="source-code">      'prediction on test set: {:.2f}'\</p><p class="source-code">      .format(bankModel.score(X_test, y_test)))</p><p>You should get the following output:</p><div id="_idContainer151" class="IMG---Figure"><img src="Images/B15019_03_52.jpg" alt="Figure 3.52: Prediction with the model&#13;&#10;" width="918" height="37"/></div><p class="figure-caption">Figure 3.52: Prediction with the model</p></li>
				<li>From an initial look, an accuracy metric of 90% gives us the impression that the model has done a decent job of approximating the data generating process. Or is it otherwise? Let's take a closer look at the details of the prediction by generating the metrics for the model. We will use two metric-generating functions, the confusion matrix and classification report:<p class="source-code"># Confusion Matrix for the model</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get the following output in the following format; however, the values can vary as the modeling task will involve variability:</p><div id="_idContainer152" class="IMG---Figure"><img src="Images/B15019_03_53.jpg" alt="Figure 3.53: Generation of the confusion matrix&#13;&#10;" width="609" height="60"/></div><p class="figure-caption">Figure 3.53: Generation of the confusion matrix</p><p class="callout-heading">Note</p><p class="callout">The end results that you get will be different from what you see here as it depends on the system you are using. This is because the modeling part is stochastic in nature and there will always be differences.</p></li>
				<li>Next, let's generate a <strong class="source-inline">classification_report</strong>:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get a similar output; however, with different values due to variability in the modeling process:</p><div id="_idContainer153" class="IMG---Figure"><img src="Images/B15019_03_54.jpg" alt="Figure 3.54: Confusion matrix and classification report&#13;&#10;" width="676" height="219"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.54: Confusion matrix and classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CGFYYU">https://packt.live/2CGFYYU</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aDq8KX">https://packt.live/3aDq8KX</a>.</p>
			<p>From the metrics, we can see that, out of the total 11,998 examples of <strong class="source-inline">no</strong>, 11,754 were correctly classified as <strong class="source-inline">no</strong> and the balance, 244, were classified as <strong class="source-inline">yes</strong>. This gives a recall value of <em class="italic">11,754/11,998</em>, which is nearly 98%. From a precision perspective, out of the total 12,996 examples that were predicted as <strong class="source-inline">no</strong>, only 11,754 of them were really <strong class="source-inline">no</strong>, which takes our precision to 11,754/12,996 or 90%.</p>
			<p>However, the metrics for <strong class="source-inline">yes</strong> give a different picture. Out of the total 1,566 cases of <strong class="source-inline">yes</strong>, only 324 were correctly identified as <strong class="source-inline">yes</strong>. This gives us a recall of <em class="italic">324/1,566 = 21%</em>. The precision is <em class="italic">324 / (324 + 244) = 57%</em>.</p>
			<p>From an overall accuracy level, this can be calculated as follows: correctly classified <em class="italic">examples / total examples = (11754 + 324) / 13564 = 89%</em>.</p>
			<p>The metrics might seem good when you look only at the accuracy level. However, looking at the details, we can see that the classifier, in fact, is doing a poor job of classifying the <strong class="source-inline">yes</strong> cases. The classifier has been trained to predict mostly <strong class="source-inline">no</strong> values, which from a business perspective is useless. From a business perspective, we predominantly want the <strong class="source-inline">yes</strong> estimates, so that we can target those cases for focused marketing to try to sell term deposits. However, with the results we have, we don't seem to have done a good job in helping the business to increase revenue from term deposit sales. </p>
			<p>In this exercise, we have preprocessed data, then we performed the training process, and finally, we found useful prediction, analysis of metrics, and deriving strategies for further improvement of the model.</p>
			<p>What we have now built is the first model or a benchmark model. The next step is to try to improve on the benchmark model through different strategies. One such strategy is to feature engineer variables and build new models with new features. Let's achieve that in the next activity.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Activity 3.02: Model Iteration 2 – Logistic Regression Model with Feature Engineered Variables</h2>
			<p>As the data scientist of the bank, you created a benchmark model to predict which customers are likely to buy a term deposit. However, management wants to improve the results you got in the benchmark model. In <em class="italic">Exercise 3.04</em>, <em class="italic">Feature Engineering – Creating New Features from Existing Ones, </em>you discussed the business scenario with the marketing and operations teams and created a new variable, <strong class="source-inline">assetIndex</strong>, by feature engineering three raw variables. You are now fitting another logistic regression model on the feature engineered variables and are trying to improve the results. </p>
			<p>In this activity, you will be feature engineering some of the variables to verify their effects on the predictions. </p>
			<p>The steps are as follows:</p>
			<ol>
				<li value="1">Open the Colab notebook used for the feature engineering in <em class="italic">Exercise 3.04</em>, <em class="italic">Feature Engineering – Creating New Features from Existing Ones,</em> and execute all of the steps from that exercise.</li>
				<li>Create dummy variables for the categorical variables using the <strong class="source-inline">pd.get_dummies()</strong> function. Exclude original raw variables such as loan and housing, which were used to create the new variable, <strong class="source-inline">assetIndex</strong>.</li>
				<li>Select the numerical variables including the new feature engineered variable, <strong class="source-inline">assetIndex</strong>, that was created.</li>
				<li>Transform some of the numerical variables by normalizing them using the <strong class="source-inline">MinMaxScaler()</strong> function.</li>
				<li>Concatenate the numerical variables and categorical variables using the <strong class="source-inline">pd.concat()</strong> function and then create <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> variables.</li>
				<li>Split the dataset using the <strong class="source-inline">train_test_split()</strong> function and then fit a new model using the <strong class="source-inline">LogisticRegression()</strong> model on the new features.</li>
				<li>Analyze the results after generating the confusion matrix and classification report.<p>You should get the following output:</p><div id="_idContainer154" class="IMG---Figure"><img src="Images/B15019_03_55.jpg" alt="Figure 3.55: Expected output with the classification report&#13;&#10;" width="470" height="143"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.55: Expected output with the classification report</p>
			<p>The classification report will be similar to the one shown here. However, the values can differ due to the variability in the modeling process.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>Let's now discuss the next steps that need to be adopted in order to improve upon the metrics we got from our two iterations.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Next Steps</h2>
			<p>The next obvious question we can ask is where do we go from all of the processes that we have implemented in this chapter? Let's discuss strategies that we can adopt for further improvement:</p>
			<ul>
				<li><strong class="bold">Class imbalance</strong>: Class imbalance implies use cases where one class outnumbers the other class(es) in the dataset. In the dataset that we used for training, out of the total 31,647 examples, 27,953 or 88% of them belonged to the <strong class="source-inline">no</strong> class. When there are class imbalances, there is a high likelihood that the classifier overfits to the majority class. This is what we have seen in our example. This is also the reason why we shouldn't draw our conclusions on the performance of our classifier by only looking at the accuracy values. <p>Class imbalance is very prevalent in many use cases such as fraud detection, medical diagnostics, and customer churn, to name a few. There are different strategies for addressing use cases where there are class imbalances. We will deal with class imbalance scenarios in <em class="italic">Chapter 13</em>, <em class="italic">Imbalanced Datasets</em>.</p></li>
				<li><strong class="bold">Feature engineering</strong>: Data science is an iterative science. Getting the desired outcome will depend on the variety of experiments we undertake. One big area to make improvements in the initial model is to make changes to the raw variables through feature engineering. We dealt with feature engineering and built a model using feature engineered variables. In building the new features, we followed a trail of creating a new feature related to the asset portfolio. Similarly, there would be other trails that we could follow from a business perspective, which have the potential to yield more features similar to what we created. Identification of such trails would depend on extending the business knowledge we apply through the hypotheses we formulate and the exploratory analysis we do to validate those business hypotheses. A very potent way to improve the veracity of the models is to identify more business trails and then build models through innovative feature engineering.</li>
				<li><strong class="bold">Model selection strategy</strong>: When we discussed parametric and non-parametric models, we touched upon the point that if the real data generation process is not similar to the model that we have assumed, we will get poor results. In our case, we assumed linearity and, therefore, adopted a linear model. What if the real data generation process is not linear? Or, what if there are other parametric or non-parametric models that are much more suitable for this use case? These are all considerations when we try to analyze results and try to improve the model. We must adopt a strategy called model spot checking, which entails working out the use case with different models and checking the initial metrics before adopting a model for the use case. In subsequent chapters, we will discuss other modeling techniques and it will be advisable to try out this use case with other types of models to spot check which modeling technique is more apt for this use case.</li>
			</ul>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Summary</h1>
			<p>In this chapter, we learned about binary classification using logistic regression from the perspective of solving a use case. Let's summarize our learnings in this chapter. We were introduced to classification problems and specifically binary classification problems. We also looked at the classification problem from the perspective of predicting term deposit propensity through a business discovery process. In the business discovery process, we identified different business drivers that influence business outcomes.</p>
			<p>Intuitions derived from the exploratory analysis were used to create new features from the raw variables. A benchmark logistic regression model was built, and the metrics were analyzed to identify a future course of action, and we iterated on the benchmark model by building a second model by incorporating the feature engineered variables. </p>
			<p>Having equipped yourselves to solve binary classification problems, it is time to take the next step forward. In the next chapter, you will deal with multiclass classification, where you will be introduced to different techniques for solving such problems.</p>
		</div>
		<div>
			<div id="_idContainer156" class="Content">
			</div>
		</div>
	</div></body></html>