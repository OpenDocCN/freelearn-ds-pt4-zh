<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer203" class="Content">
			</div>
		</div>
		<div id="_idContainer204" class="Content">
			<h1 id="_idParaDest-118">5. <a id="_idTextAnchor117"/>Performing Your First Cluster Analysis</h1>
		</div>
		<div id="_idContainer260" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will introduce you to unsupervised learning tasks, where algorithms have to automatically learn patterns from data by themselves as no target variables are defined beforehand. We will focus specifically on the k-means algorithm, and see how to standardize and process data for use in cluster analysis.</p>
			<p class="callout">By the end of this chapter, you will be able to load and visualize data and clusters with scatter plots; prepare data for cluster analysis; perform centroid clustering with k-means; interpret clustering results and determine the optimal number of clusters for a given dataset.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor118"/>Introduction</h1>
			<p>The previous chapters introduced you to very popular and extremely powerful machine learning algorithms. They all have one thing in common, which is that they belong to the same category of algorithms: supervised learning. This kind of algorithm tries to learn patterns based on a specified outcome column (target variable) such as sales, employee churn, or class of customer.</p>
			<p>But what if you don't have such a variable in your dataset or you don't want to specify a target variable? Will you still be able to run some machine learning algorithms on it and find interesting patterns? The answer is yes, with the use of clustering algorithms that belong to the unsupervised learning category.</p>
			<p>Clustering algorithms are very popular in the data science industry for grouping similar data points and detecting outliers. For instance, clustering algorithms can be used by banks for fraud detection by identifying unusual clusters from the data. They can also be used by e-commerce companies to identify groups of users with similar browsing behaviors, as in the following figures:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="Images/B15019_05_01.jpg" alt="Figure 5.1: Example of data on customers with similar browsing behaviors &#13;&#10;without clustering analysis performed&#13;&#10;" width="1480" height="1063"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1: Example of data on customers with similar browsing behaviors without clustering analysis performed</p>
			<p>Clustering analysis performed on this data would uncover natural patterns by grouping similar data points such that you may get the following result:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="Images/B15019_05_02.jpg" alt="Figure 5.2: Clustering analysis performed on the data on customers &#13;&#10;with similar browsing behaviors&#13;&#10;" width="1531" height="1084"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2: Clustering analysis performed on the data on customers with similar browsing behaviors</p>
			<p>The data is now segmented into three customer groups depending on their recurring visits and time spent on the website, and different marketing plans can then be used for each of these groups in order to maximize sales. </p>
			<p>In this chapter, you will learn how to perform such analysis using a very famous clustering algorithm called k-means.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Clustering with k-means</h1>
			<p>k-means is one of the most popular clustering algorithms (if not the most popular) among data scientists due to its simplicity and high performance. Its origins date back as early as 1956, when a famous mathematician named Hugo Steinhaus laid its foundations, but it was a decade later that another researcher called James MacQueen named this approach k-means.</p>
			<p>The objective of k-means is to group similar data points (or observations) together that will form a cluster. Think of it as grouping elements close to each other (we will define how to measure closeness later in this chapter). For example, if you were manually analyzing user behavior on a mobile app, you might end up grouping customers who log in quite frequently, or users who make bigger in-app purchases, together. This is the kind of grouping that clustering algorithms such as k-means will automatically find for you from the data. </p>
			<p>In this chapter, we will be working with an open source dataset shared publicly by the <strong class="bold">Australian Taxation Office</strong> (<strong class="bold">ATO</strong>). The dataset contains statistics about each postcode (<em class="italic">also known as a zip code, which is an identification code used for sorting mail by area</em>) in Australia during the financial year of 2014-15.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Australian Taxation Office (ATO) dataset can be found in the Packt GitHub repository here: <a href="https://packt.live/340xO5t">https://packt.live/340xO5t</a>.</p>
			<p class="callout">The source of the dataset can be found here: <a href="https://packt.live/361i1p3">https://packt.live/361i1p3</a>.</p>
			<p>We will perform cluster analysis on this dataset for two specific variables (or columns): <strong class="source-inline">Average net tax</strong> and <strong class="source-inline">Average total deductions</strong>. Our objective is to find groups (or clusters) of postcodes sharing similar patterns in terms of tax received and money deducted. Here is a scatter plot of these two variables:</p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="Images/B15019_05_03.jpg" alt="Figure 5.3: Scatter plot of the ATO dataset&#13;&#10;" width="1431" height="1048"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3: Scatter plot of the ATO dataset</p>
			<p>As part of being a data scientist, you need to analyze the graphs achieved from this dataset and come to conclusions. Let's say you have to analyze manually potential groupings of observations from this dataset. One potential result could be as follows:</p>
			<ul>
				<li>All the data points in the bottom-left corner could be grouped together (average net tax from 0 to 40,000).</li>
				<li>A second group could be all the data points in the center area (average net tax from 40,000 to 60,000 and average total deductions below 10,000).</li>
				<li>Finally, all the remaining data points could be grouped together.</li>
			</ul>
			<p>But rather having you to manually guess these groupings, it will be better if we can use an algorithm to do it for us. This is what we are going to see in practice in the following exercise, where we'll perform clustering analysis on this dataset. </p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>Exercise 5.01: Performing Your First Clustering Analysis on the ATO Dataset</h2>
			<p>In this exercise, we will be using k-means clustering on the ATO dataset and observing the different clusters that the dataset divides itself into, after which we will conclude by analyzing the output:</p>
			<ol>
				<li>Open a new Colab notebook.</li>
				<li>Next, load the required Python packages: <strong class="source-inline">pandas</strong> and <strong class="source-inline">KMeans</strong> from <strong class="source-inline">sklearn.cluster</strong>. <p>We will be using the <strong class="source-inline">import</strong> function from Python:</p><p class="callout-heading">Note</p><p class="callout">You can create short aliases for the packages you will be calling quite often in your script with the function mentioned in the following code snippet.</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="callout-heading">Note</p><p class="callout">We will be looking into <strong class="source-inline">KMeans</strong> (from <strong class="source-inline">sklearn.cluster</strong>), which you have used in the code here, later in the chapter for a more detailed explanation of it.</p></li>
				<li>Next, create a variable containing the link to the file. We will call this variable <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter05/DataSet/taxstats2015.csv'</p><p>In the next step, we will use the <strong class="source-inline">pandas</strong> package to load our data into a DataFrame (think of it as a table, like on an Excel spreadsheet, with a row index and column names). </p><p>Our input file is in <strong class="source-inline">CSV</strong> format, and <strong class="source-inline">pandas</strong> has a method that can directly read this format, which is <strong class="source-inline">.read_csv()</strong>. </p></li>
				<li>Use the <strong class="source-inline">usecols</strong> parameter to subset only the columns we need rather than loading the entire dataset. We just need to provide a list of the column names we are interested in, which are mentioned in the following code snippet:<p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average net tax', \</p><p class="source-code">                          'Average total deductions'])</p><p>Now we have loaded the data into a <strong class="source-inline">pandas</strong> DataFrame.</p></li>
				<li>Next, let's display the first 5 rows of this DataFrame , using the method <strong class="source-inline">.head()</strong>:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer208" class="IMG---Figure"><img src="Images/B15019_05_04.jpg" alt="Figure 5.4: The first five rows of the ATO DataFrame&#13;&#10;" width="1006" height="364"/></div><p class="figure-caption">Figure 5.4: The first five rows of the ATO DataFrame</p></li>
				<li>Now, to output the last 5 rows, we use <strong class="source-inline">.tail()</strong>:<p class="source-code">df.tail()</p><p>You should get the following output:</p><div id="_idContainer209" class="IMG---Figure"><img src="Images/B15019_05_05.jpg" alt="Figure 5.5: The last five rows of the ATO DataFrame&#13;&#10;" width="1039" height="368"/></div><p class="figure-caption">Figure 5.5: The last five rows of the ATO DataFrame</p><p>Now that we have our data, let's jump straight to what we want to do: find clusters. </p><p>As you saw in the previous chapters, <strong class="source-inline">sklearn</strong> provides the exact same APIs for training different machine learning algorithms, such as:</p><ul><li>Instantiate an algorithm with the specified hyperparameters (here it will be KMeans(hyperparameters)).</li><li>Fit the model with the training data with the method <strong class="source-inline">.fit()</strong>.</li><li>Predict the result with the given input data with the method <strong class="source-inline">.predict()</strong>.<p class="callout-heading">Note</p><p class="callout">Here, we will use all the default values for the k-means hyperparameters except for the <strong class="source-inline">random_state</strong> one. Specifying a fixed random state (also called a <strong class="bold">seed</strong>) will help us to get reproducible results every time we have to rerun our code.</p></li></ul></li>
				<li>Instantiate k-means with a random state of <strong class="source-inline">42</strong> and save it into a variable called <strong class="source-inline">kmeans</strong>:<p class="source-code">kmeans = KMeans(random_state=42)</p></li>
				<li>Now feed k-means with our training data. To do so, we need to get only the variables (or columns) used for fitting the model. In our case, the variables are <strong class="source-inline">'Average net tax'</strong> and <strong class="source-inline">'Average total deductions'</strong>, and they are saved in a new variable called <strong class="source-inline">X</strong>:<p class="source-code">X = df[['Average net tax', 'Average total deductions']]</p></li>
				<li>Now fit <strong class="source-inline">kmeans</strong> with this training data:<p class="source-code">kmeans.fit(X)</p><p>You should get the following output:</p><div id="_idContainer210" class="IMG---Figure"><img src="Images/B15019_05_06.jpg" alt="Figure 5.6: Summary of the fitted kmeans and its hyperparameters&#13;&#10;" width="1244" height="112"/></div><p class="figure-caption">Figure 5.6: Summary of the fitted kmeans and its hyperparameters</p><p>We just ran our first clustering algorithm in just a few lines of code. </p></li>
				<li>See which cluster each data point belongs to by using the <strong class="source-inline">.predict()</strong> method:<p class="source-code">y_preds = kmeans.predict(X)</p><p class="source-code">y_preds</p><p>You should get the following output:</p><div id="_idContainer211" class="IMG---Figure"><img src="Images/B15019_05_07.jpg" alt="Figure 5.7: Output of the k-means predictions &#13;&#10;" width="1045" height="58"/></div><p class="figure-caption">Figure 5.7: Output of the k-means predictions </p><p class="callout-heading">Note</p><p class="callout">Although we set a <strong class="source-inline">random_state</strong> value, you may still get an output with different cluster numbers than the one shown above. This will depend on the version of scikit-learn you are using. The output above was generated using version 0.22.2. You can find out which version you are using by executing the following code:</p><p class="callout"><strong class="source-inline">import sklearn</strong></p><p class="callout"><strong class="source-inline">sklearn.__version__</strong></p></li>
				<li>Now, add these predictions into the original DataFrame and take a look at the first five postcodes: <p class="source-code">df['cluster'] = y_preds</p><p class="source-code">df.head()</p><p class="callout-heading">Note </p><p class="callout">The predictions from the sklearn <strong class="source-inline">predict()</strong> method are in the exact same order as the input data. So, the first prediction will correspond to the first row of your DataFrame.</p><p>You should get the following output:</p><div id="_idContainer212" class="IMG---Figure"><img src="Images/B15019_05_08.jpg" alt="Figure 5.8: Cluster number assigned to the first five postcodes&#13;&#10;" width="961" height="361"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.8: Cluster number assigned to the first five postcodes</p>
			<p>Our k-means model has grouped the first two rows into the same cluster, <strong class="source-inline">6</strong>. We can see these two observations both have average net tax values around 28,000. The last three data points have been assigned to different clusters (<strong class="source-inline">5</strong>, <strong class="source-inline">7</strong>, and <strong class="source-inline">2</strong>, respectively) and we can see their values for both average total deductions and average net tax are very different from each other. It seems that lower values are grouped into cluster <strong class="source-inline">5</strong> while higher values are classified into cluster <strong class="source-inline">7</strong>. We are starting to build our understanding of how k-means has decided to group the observations from this dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2DUCWAZ">https://packt.live/2DUCWAZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2EgduFS">https://packt.live/2EgduFS</a>.</p>
			<p>This is a great start. You have learned how to train (or fit) a k-means model in a few lines of code. Now we can start diving deeper into the magic behind k-means.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor121"/>Interpreting k-means Results</h1>
			<p>After training our k-means algorithm, we will likely be interested in analyzing its results in more detail. Remember, the objective of cluster analysis is to group observations with similar patterns together. But how can we see whether the groupings found by the algorithm are meaningful? We will be looking at this in this section by using the dataset results we just generated. </p>
			<p>One way of investigating this is to analyze the dataset row by row with the assigned cluster for each observation. This can be quite tedious, especially if the size of your dataset is quite big, so it would be better to have a kind of summary of the cluster results. </p>
			<p>If you are familiar with Excel spreadsheets, you are probably thinking about using a pivot table to get the average of the variables for each cluster. In SQL, you would have probably used a <strong class="source-inline">GROUP BY</strong> statement. If you are not familiar with either of these, you may think of grouping each cluster together and then calculating the average for each of them. The good news is that this can be easily achieved with the <strong class="source-inline">pandas</strong> package in Python. Let's see how this can be done with an example.</p>
			<p>To create a pivot table similar to an Excel one, we will be using the <strong class="source-inline">pivot_table()</strong> method from <strong class="source-inline">pandas</strong>. We need to specify the following parameters for this method:</p>
			<ul>
				<li><strong class="source-inline">values</strong>: This parameter corresponds to the numerical columns you want to calculate summaries for (or aggregations), such as getting averages or counts. In an Excel pivot table, it is also called <strong class="source-inline">values</strong>. In our dataset, we will use the <strong class="source-inline">Average net tax</strong> and <strong class="source-inline">Average total deductions</strong> variables.</li>
				<li><strong class="source-inline">index</strong>: This parameter is used to specify the columns you want to see summaries for. In our case, it will be the <strong class="source-inline">cluster</strong> column. In a pivot table in Excel, this corresponds with the <strong class="source-inline">Rows</strong> field.</li>
				<li><strong class="source-inline">aggfunc</strong>: This is where you will specify the aggregation functions you want to summarize the data with, such as getting averages or counts. In Excel, this is the <strong class="source-inline">Summarize by</strong> option in the <strong class="source-inline">values</strong> field. An example of how to use the <strong class="source-inline">aggfunc</strong> method is shown below.<p class="callout-heading">Note</p><p class="callout">Run the code below in the same notebook as you used for the previous exercise.</p></li>
			</ul>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">df.pivot_table(values=['Average net tax', \</p>
			<p class="source-code">                       'Average total deductions'], \</p>
			<p class="source-code">               index='cluster', aggfunc=np.mean)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will be using the <strong class="source-inline">numpy</strong> implementation of <strong class="source-inline">mean()</strong> as it is more optimized for pandas DataFrames.</p>
			<p> </p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="Images/B15019_05_09.jpg" alt="Figure 5.9: Output of the pivot_table function&#13;&#10;" width="852" height="470"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9: Output of the pivot_table function</p>
			<p>In this summary, we can see that the algorithm has grouped the data into eight clusters (clusters 0 to 7). Cluster 0 has the lowest average net tax and total deductions amounts among all the clusters, while cluster 4 has the highest values. With this pivot table, we are able to compare clusters between them using their summarized values.</p>
			<p>Using an aggregated view of clusters is a good way of seeing the difference between them, but it is not the only way. Another possibility is to visualize clusters in a graph. This is exactly what we are going to do now.</p>
			<p>You may have heard of different visualization packages, such as <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">seaborn</strong>, and <strong class="source-inline">bokeh</strong>, but in this chapter, we will be using the <strong class="source-inline">altair</strong> package because it is quite simple to use (its API is very similar to <strong class="source-inline">sklearn</strong>). Let's import it first:</p>
			<p class="source-code">import altair as alt</p>
			<p>Then, we will instantiate a <strong class="source-inline">Chart()</strong> object with our DataFrame and save it into a variable called <strong class="source-inline">chart</strong>:</p>
			<p class="source-code">chart = alt.Chart(df)</p>
			<p>Now we will specify the type of graph we want, a scatter plot, with the <strong class="source-inline">.mark_circle()</strong> method and will save it into a new variable called <strong class="source-inline">scatter_plot</strong>:</p>
			<p class="source-code">scatter_plot = chart.mark_circle()</p>
			<p>Finally, we need to configure our scatter plot by specifying the names of the columns that will be our <strong class="source-inline">x</strong>- and <strong class="source-inline">y</strong>-axes on the graph. We also tell the scatter plot to color each point according to its cluster value with the <strong class="source-inline">color</strong> option:</p>
			<p class="source-code">scatter_plot.encode(x='Average net tax', \</p>
			<p class="source-code">                    y='Average total deductions', \</p>
			<p class="source-code">                    color='cluster:N')</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You may have noticed that we added <strong class="source-inline">:N</strong> at the end of the <strong class="source-inline">cluster</strong> column name. This extra parameter is used in <strong class="source-inline">altair</strong> to specify the type of value for this column. <strong class="source-inline">:N</strong> means the information contained in this column is categorical. <strong class="source-inline">altair</strong> automatically defines the color scheme to be used depending on the type of a column.</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="Images/B15019_05_10.jpg" alt="Figure 5.10: Scatter plot of the clusters&#13;&#10;" width="1665" height="1020"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10: Scatter plot of the clusters</p>
			<p>We can now easily see what the clusters in this graph are and how they differ from each other. We can clearly see that k-means assigned data points to each cluster mainly based on the x-axis variable, which is <strong class="source-inline">Average net tax</strong>. The boundaries of the clusters are vertical straight lines. For instance, the boundary separating the yellow and orange clusters is roughly around 18,000. Observations below this limit are assigned to the red cluster (2) and those above to the purple cluster (6).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can see the high quality color images for this Workshop at: <a href="https://packt.live/30O91Bd%20">https://packt.live/30O91Bd</a></p>
			<p>If you have used visualization tools such as <strong class="bold">Tableau</strong> or <strong class="bold">Power BI</strong>, you might feel a bit frustrated as this graph is static, you can't hover over each data point to get more information and find, for instance, what is the limit separating the orange cluster from the pink one. But this can be easily achieved with altair and this is one of the reasons we chose to use it We can add some interactions to your chart with minimal code changes. </p>
			<p>Let's say we want to add a tooltip that will display the values for the two columns of interest: the postcode and the assigned cluster. With <strong class="source-inline">altair</strong>, we just need to add a parameter called <strong class="source-inline">tooltip</strong> in the <strong class="source-inline">encode()</strong> method with a list of corresponding column names and call the <strong class="source-inline">interactive()</strong> method just after, as seen in the following code snippet:</p>
			<p class="source-code">scatter_plot.encode(x='Average net tax', \</p>
			<p class="source-code">                    y='Average total deductions', \</p>
			<p class="source-code">                    color='cluster:N', \</p>
			<p class="source-code">                    tooltip=['Postcode', \</p>
			<p class="source-code">                             'cluster', 'Average net tax', \</p>
			<p class="source-code">                             'Average total deductions'])\</p>
			<p class="source-code">                    .interactive()</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="Images/B15019_05_11.jpg" alt="Figure 5.11: Interactive scatter plot of the clusters with tooltip&#13;&#10;" width="1080" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11: Interactive scatter plot of the clusters with tooltip</p>
			<p>Now we can easily hover over and inspect the data points near the cluster boundaries and find out that the threshold used to differentiate the purple cluster (6) from the red one (2) is close to 32,000 in <strong class="source-inline">'Average Net Tax'</strong>. </p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Exercise 5.02: Clustering Australian Postcodes by Business Income and Expenses</h2>
			<p>In this exercise, we will learn how to perform clustering analysis with k-means and visualize its results based on postcode values sorted by business income and expenses. The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook for this exercise.</li>
				<li>Now <strong class="source-inline">import</strong> the required packages (<strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, <strong class="source-inline">altair</strong>, and <strong class="source-inline">numpy</strong>):<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">import altair as alt</p><p class="source-code">import numpy as np</p></li>
				<li>Assign the link to the ATO dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter05/DataSet/taxstats2015.csv'</p></li>
				<li>Using the <strong class="source-inline">read_csv</strong> method from the pandas package, load the dataset with only the following columns with the <strong class="source-inline">use_cols</strong> parameter: <strong class="source-inline">'Postcode'</strong>, <strong class="source-inline">'Average total business income'</strong>, and <strong class="source-inline">'Average total business expenses'</strong>:<p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average total business income', \</p><p class="source-code">                          'Average total business expenses'])</p></li>
				<li>Display the last 10 rows from the ATO dataset using the <strong class="source-inline">.tail()</strong> method from pandas:<p class="source-code">df.tail(10)</p><p>You should get the following output: </p><div id="_idContainer216" class="IMG---Figure"><img src="Images/B15019_05_12.jpg" alt="Figure 5.12: The last 10 rows of the ATO dataset&#13;&#10;" width="1005" height="514"/></div><p class="figure-caption">Figure 5.12: The last 10 rows of the ATO dataset</p></li>
				<li>Extract the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expenses'</strong> columns using the following pandas column subsetting syntax: <strong class="source-inline">dataframe_name[&lt;list_of_columns&gt;]</strong>. Then, save them into a new variable called <strong class="source-inline">X</strong>: <p class="source-code">X = df[['Average total business income', \</p><p class="source-code">        'Average total business expenses']]</p></li>
				<li>Now fit <strong class="source-inline">kmeans</strong> with this new variable using a value of <strong class="source-inline">8</strong> for the <strong class="source-inline">random_state</strong> hyperparameter:<p class="source-code">kmeans = KMeans(random_state=8)</p><p class="source-code">kmeans.fit(X)</p><p>You should get the following output: </p><div id="_idContainer217" class="IMG---Figure"><img src="Images/B15019_05_13.jpg" alt="Figure 5.13: Summary of the fitted kmeans and its hyperparameters&#13;&#10;" width="1665" height="171"/></div><p class="figure-caption">Figure 5.13: Summary of the fitted kmeans and its hyperparameters</p></li>
				<li>Using the <strong class="source-inline">predict</strong> method from the <strong class="source-inline">sklearn</strong> package, predict the clustering assignment from the input variable, <strong class="source-inline">(X)</strong>, save the results into a new variable called <strong class="source-inline">y_preds</strong>, and display the last <strong class="source-inline">10</strong> predictions:<p class="source-code">y_preds = kmeans.predict(X)</p><p class="source-code">y_preds[-10:]</p><p>You should get the following output: </p><div id="_idContainer218" class="IMG---Figure"><img src="Images/B15019_05_14.jpg" alt="Figure 5.14: Results of the clusters assigned to the last 10 observations&#13;&#10;" width="1048" height="59"/></div><p class="figure-caption">Figure 5.14: Results of the clusters assigned to the last 10 observations</p></li>
				<li>Save the predicted clusters back to the DataFrame by creating a new column called <strong class="source-inline">'cluster'</strong> and print the last <strong class="source-inline">10</strong> rows of the DataFrame using the <strong class="source-inline">.tail()</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">df['cluster'] = y_preds</p><p class="source-code">df.tail(10)</p><p>You should get the following output: </p><div id="_idContainer219" class="IMG---Figure"><img src="Images/B15019_05_15.jpg" alt="Figure 5.15: The last 10 rows of the ATO dataset with the added cluster column&#13;&#10;" width="1088" height="529"/></div><p class="figure-caption">Figure 5.15: The last 10 rows of the ATO dataset with the added cluster column</p></li>
				<li>Generate a pivot table with the averages of the two columns for each cluster value using the <strong class="source-inline">pivot_table</strong> method from the <strong class="source-inline">pandas</strong> package with the following parameters:<p>Provide the names of the columns to be aggregated, <strong class="source-inline">'Average total business income'</strong> and<strong class="source-inline"> 'Average total business expenses'</strong>, to the parameter values.</p><p>Provide the name of the column to be grouped, <strong class="source-inline">'cluster'</strong>, to the parameter index.</p><p>Use the <strong class="source-inline">.mean</strong> method from NumPy (<strong class="source-inline">np</strong>) as the aggregation function for the <strong class="source-inline">aggfunc</strong> parameter:</p><p class="source-code">df.pivot_table(values=['Average total business income', \</p><p class="source-code">                       'Average total business expenses'], \</p><p class="source-code">               index='cluster', aggfunc=np.mean)</p><p>You should get the following output: </p><div id="_idContainer220" class="IMG---Figure"><img src="Images/B15019_05_16.jpg" alt="Figure 5.16: Output of the pivot_table function&#13;&#10;" width="889" height="483"/></div><p class="figure-caption">Figure 5.16: Output of the pivot_table function</p></li>
				<li>Now let's plot the clusters using an interactive scatter plot. First, use <strong class="source-inline">Chart()</strong> and <strong class="source-inline">mark_circle()</strong> from the <strong class="source-inline">altair</strong> package to instantiate a scatter plot graph:<p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p></li>
				<li>Use the <strong class="source-inline">encode</strong> and <strong class="source-inline">interactive</strong> methods from <strong class="source-inline">altair</strong> to specify the display of the scatter plot and its interactivity options with the following parameters:<p>Provide the name of the <strong class="source-inline">'Average total business income'</strong> column to the <strong class="source-inline">x</strong> parameter (the x-axis).</p><p>Provide the name of the <strong class="source-inline">'Average total business expenses'</strong> column to the <strong class="source-inline">y</strong> parameter (the y-axis).</p><p>Provide the name of the <strong class="source-inline">cluster:N</strong> column to the <strong class="source-inline">color</strong> parameter (providing a different color for each group).</p><p>Provide these column names –  <strong class="source-inline">'Postcode'</strong>, <strong class="source-inline">'cluster'</strong>, <strong class="source-inline">'Average total business income'</strong>, and <strong class="source-inline">'Average total business expenses'</strong> – to the <strong class="source-inline">'tooltip'</strong> parameter (this being the information displayed by the tooltip):</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster:N', tooltip = ['Postcode', \</p><p class="source-code">                                                  'cluster', \</p><p class="source-code">                    'Average total business income', \</p><p class="source-code">                    'Average total business expenses'])\</p><p class="source-code">                    .interactive()</p><p>You should get the following output: </p><div id="_idContainer221" class="IMG---Figure"><img src="Images/B15019_05_17.jpg" alt="Figure 5.17: Interactive scatter plot of the clusters&#13;&#10;" width="1665" height="1103"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.17: Interactive scatter plot of the clusters</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hacbXA">https://packt.live/3hacbXA</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>We can see that k-means has grouped the observations into eight different clusters based on the value of the two variables (<strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expense'</strong>). For instance, all the low-value data points have been assigned to cluster 0, while the ones with extremely high values belong to cluster 4. So, k-means has grouped the data points that share similar behaviors.</p>
			<p>You just successfully completed a cluster analysis and visualized its results. You learned how to load a real-world dataset, fit k-means, and display a scatter plot. This is a great start, and we will be delving into more details on how to improve the performance of your model in the sections to come in this chapter.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Choosing the Number of Clusters</h1>
			<p>In the previous sections, we saw how easy it is to fit the k-means algorithm on a given dataset. In our ATO dataset, we found 8 different clusters that were mainly defined by the values of the <strong class="source-inline">Average net tax</strong> variable. </p>
			<p>But you may have asked yourself: "<em class="italic">Why 8 clusters? Why not 3 or 15 clusters?</em>" These are indeed excellent questions. The short answer is that we used k-means' default value for the hyperparameter <strong class="source-inline">n_cluster</strong>, defining the number of clusters to be found, as 8.</p>
			<p>As you will recall from <em class="italic">Chapter 2</em>, <em class="italic">Regression</em>, and <em class="italic">Chapter 4</em>, <em class="italic">Multiclass Classification with RandomForest</em>, the value of a hyperparameter isn't learned by the algorithm but has to be set arbitrarily by you prior to training. For k-means, <strong class="source-inline">n_cluster</strong> is one of the most important hyperparameters you will have to tune. Choosing a low value will lead k-means to group many data points together, even though they are very different from each other. On the other hand, choosing a high value may force the algorithm to split close observations into multiple ones, even though they are very similar.</p>
			<p>Looking at the scatter plot from the ATO dataset, eight clusters seems to be a lot. On the graph, some of the clusters look very close to each other and have similar values. Intuitively, just by looking at the plot, you could have said that there were between two and four different clusters. As you can see, this is quite suggestive, and it would be great if there was a function that could help us to define the right number of clusters for a dataset. Such a method does indeed exist, and it is called the <strong class="bold">Elbow</strong> method. </p>
			<p>This method assesses the compactness of clusters, the objective being to minimize a value known as <strong class="bold">inertia</strong>. More details and an explanation about this will be provided later in this chapter. For now, think of inertia as a value that says, for a group of data points, how far from each other or how close to each other they are.</p>
			<p>Let's apply this method to our ATO dataset. First, we will define the range of cluster numbers we want to evaluate (between 1 and 10) and save them in a DataFrame called <strong class="source-inline">clusters</strong>. We will also create an empty list called <strong class="source-inline">inertia</strong>, where we will store our calculated values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Open the notebook you were using for <em class="italic">Exercise 5.01</em>, <em class="italic">Performing Your First Clustering Analysis on the ATO Dataset</em>, execute the code you already entered, and then continue at the end of the notebook with the following code.</p>
			<p class="source-code">clusters = pd.DataFrame()</p>
			<p class="source-code">clusters['cluster_range'] = range(1, 10)</p>
			<p class="source-code">inertia = []</p>
			<p>Next, we will create a <strong class="source-inline">for</strong> loop that will iterate over the range, fit a k-means model with the specified number of <strong class="source-inline">clusters</strong>, extract the <strong class="source-inline">inertia</strong> value, and store it in our list, as in the following code snippet:</p>
			<p class="source-code">for k in clusters['cluster_range']:</p>
			<p class="source-code">    kmeans = KMeans(n_clusters=k, random_state=8).fit(X)</p>
			<p class="source-code">    inertia.append(kmeans.inertia_)</p>
			<p>Now we can use our list of <strong class="source-inline">inertia</strong> values in the <strong class="source-inline">clusters</strong> DataFrame:</p>
			<p class="source-code">clusters['inertia'] = inertia</p>
			<p class="source-code">clusters</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="Images/B15019_05_18.jpg" alt="Figure 5.18: Dataframe containing inertia values for our clusters&#13;&#10;" width="1154" height="572"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18: Dataframe containing inertia values for our clusters</p>
			<p>Then, we need to plot a line chart using <strong class="source-inline">altair</strong> with the <strong class="source-inline">mark_line()</strong> method. We will specify the <strong class="source-inline">'cluster_range'</strong> column as our x-axis and <strong class="source-inline">'inertia'</strong> as our y-axis, as in the following code snippet:</p>
			<p class="source-code">alt.Chart(clusters).mark_line()\</p>
			<p class="source-code">                   .encode(x='cluster_range', y='inertia')</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="Images/B15019_05_19.jpg" alt="Figure 5.19: Plotting the Elbow method&#13;&#10;" width="1063" height="723"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19: Plotting the Elbow method</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You don't have to save each of the <strong class="source-inline">altair</strong> objects in a separate variable; you can just append the methods one after the other with "<strong class="source-inline">.".</strong></p>
			<p>Now that we have plotted the inertia value against the number of clusters, we need to find the optimal number of clusters. What we need to do is to find the inflection point in the graph, where the inertia value starts to decrease more slowly (that is, where the slope of the line almost reaches a 45-degree angle). Finding the right <strong class="bold">inflection point</strong> can be a bit tricky. If you picture this line chart as an arm, what we want is to find the center of the Elbow (now you know where the name for this method comes from). So, looking at our example, we will say that the optimal number of clusters is three. If we kept adding more clusters, the inertia would not decrease drastically and add any value. This is the reason why we want to find the middle of the Elbow as the inflection point.</p>
			<p>Now let's retrain our <strong class="source-inline">Kmeans</strong> with this hyperparameter and plot the clusters as shown in the following code snippet:</p>
			<p class="source-code">kmeans = KMeans(random_state=42, n_clusters=3)</p>
			<p class="source-code">kmeans.fit(X)</p>
			<p class="source-code">df['cluster2'] = kmeans.predict(X)</p>
			<p class="source-code">scatter_plot.encode(x='Average net tax', \</p>
			<p class="source-code">                    y='Average total deductions', \</p>
			<p class="source-code">                    color='cluster2:N', \</p>
			<p class="source-code">                    tooltip=['Postcode', 'cluster', \</p>
			<p class="source-code">                             'Average net tax', \</p>
			<p class="source-code">                             'Average total deductions'])\</p>
			<p class="source-code">                    .interactive()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="Images/B15019_05_20.jpg" alt="Figure 5.20: Scatter plot of the three clusters&#13;&#10;" width="1098" height="717"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20: Scatter plot of the three clusters</p>
			<p>This is very different compared to our initial results. Looking at the three clusters, we can see that:</p>
			<ul>
				<li>The first cluster (red) represents postcodes with low values for both average net tax and total deductions.</li>
				<li>The second cluster (blue) is for medium average net tax and low average total deductions.</li>
				<li>The third cluster (orange) is grouping all postcodes with average net tax values above 35,000.<p class="callout-heading">Note</p><p class="callout">It is worth noticing that the data points are more spread in the third cluster; this may indicate that there are some outliers in this group.</p></li>
			</ul>
			<p>This example showed us how important it is to define the right number of clusters before training a k-means algorithm if we want to get meaningful groups from data. We used a method called the Elbow method to find this optimal number.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor124"/>Exercise 5.03: Finding the Optimal Number of Clusters</h2>
			<p>In this exercise, we will apply the Elbow method to the same data as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, to find the optimal number of clusters, before fitting a k-means model:</p>
			<ol>
				<li value="1">Open a new Colab notebook for this exercise.</li>
				<li>Now <strong class="source-inline">import</strong> the required packages (<strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">altair</strong>):<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">import altair as alt</p><p>Next, we will load the dataset and select the same columns as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, and print the first five rows.</p></li>
				<li>Assign the link to the ATO dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter05/DataSet/taxstats2015.csv'</p></li>
				<li>Using the <strong class="source-inline">.read_csv()</strong> method from the pandas package, load the dataset with only the following columns using the <strong class="source-inline">use_cols</strong> parameter: <strong class="source-inline">'Postcode'</strong>, <strong class="source-inline">'Average total business income'</strong>, and <strong class="source-inline">'Average total business expenses'</strong>:<p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average total business income', \</p><p class="source-code">                          'Average total business expenses'])</p></li>
				<li>Display the first five rows of the DataFrame with the <strong class="source-inline">.head()</strong> method from the pandas package:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer225" class="IMG---Figure"><img src="Images/B15019_05_21.jpg" alt="Figure 5.21: The first five rows of the ATO DataFrame&#13;&#10;" width="1151" height="325"/></div><p class="figure-caption">Figure 5.21: The first five rows of the ATO DataFrame</p></li>
				<li>Assign the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expenses'</strong> columns to a new variable called <strong class="source-inline">X</strong>:<p class="source-code">X = df[['Average total business income', \</p><p class="source-code">        'Average total business expenses']]</p></li>
				<li>Create an empty pandas DataFrame called <strong class="source-inline">clusters</strong> and an empty list called <strong class="source-inline">inertia</strong>:<p class="source-code">clusters = pd.DataFrame()</p><p class="source-code">inertia = []</p><p>Now, use the <strong class="source-inline">range</strong> function to generate a list containing the range of cluster numbers, from <strong class="source-inline">1</strong> to <strong class="source-inline">15</strong>, and assign it to a new column called <strong class="source-inline">'cluster_range'</strong> from the <strong class="source-inline">'clusters'</strong> DataFrame:</p><p class="source-code">clusters['cluster_range'] = range(1, 15)</p></li>
				<li>Create a <strong class="source-inline">for</strong> loop to go through each cluster number and fit a k-means model accordingly, then append the <strong class="source-inline">inertia</strong> values using the <strong class="source-inline">'inertia_'</strong> parameter with the <strong class="source-inline">'inertia'</strong> list:<p class="source-code">for k in clusters['cluster_range']:</p><p class="source-code">    kmeans = KMeans(n_clusters=k).fit(X)</p><p class="source-code">    inertia.append(kmeans.inertia_)</p></li>
				<li>Assign the <strong class="source-inline">inertia</strong> list to a new column called <strong class="source-inline">'inertia'</strong> from the <strong class="source-inline">clusters</strong> DataFrame and display its content: <p class="source-code">clusters['inertia'] = inertia</p><p class="source-code">clusters</p><p>You should get the following output: </p><div id="_idContainer226" class="IMG---Figure"><img src="Images/B15019_05_22.jpg" alt="Figure 5.22: Plotting the Elbow method&#13;&#10;" width="743" height="621"/></div><p class="figure-caption">Figure 5.22: Plotting the Elbow method</p></li>
				<li>Now use <strong class="source-inline">mark_line()</strong> and <strong class="source-inline">encode()</strong> from the <strong class="source-inline">altair</strong> package to plot the Elbow graph with <strong class="source-inline">'cluster_range'</strong> as the x-axis and <strong class="source-inline">'inertia'</strong> as the y-axis:<p class="source-code">alt.Chart(clusters).mark_line()\</p><p class="source-code">   .encode(alt.X('cluster_range'), alt.Y('inertia'))</p><p>You should get the following output: </p><div id="_idContainer227" class="IMG---Figure"><img src="Images/B15019_05_23.jpg" alt="Figure 5.23: Plotting the Elbow method&#13;&#10;" width="1665" height="1124"/></div><p class="figure-caption">Figure 5.23: Plotting the Elbow method</p></li>
				<li>Looking at the Elbow plot, identify the optimal number of clusters, and assign this value to a variable called <strong class="source-inline">optim_cluster</strong>:<p class="source-code">optim_cluster = 4</p></li>
				<li>Train a k-means model with this number of clusters and a <strong class="source-inline">random_state</strong> value of <strong class="source-inline">42</strong> using the <strong class="source-inline">fit</strong> method from <strong class="source-inline">sklearn</strong>:<p class="source-code">kmeans = KMeans(random_state=42, n_clusters=optim_cluster)</p><p class="source-code">kmeans.fit(X)</p></li>
				<li>Now, using the <strong class="source-inline">predict</strong> method from <strong class="source-inline">sklearn</strong>, get the predicted assigned cluster for each data point contained in the <strong class="source-inline">X</strong> variable and save the results into a new column called <strong class="source-inline">'cluster2'</strong> from the <strong class="source-inline">df</strong> DataFrame:<p class="source-code">df['cluster2'] = kmeans.predict(X)</p></li>
				<li>Display the first five rows of the <strong class="source-inline">df</strong> DataFrame using the <strong class="source-inline">head</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">df.head()</p><p>You should get the following output: </p><div id="_idContainer228" class="IMG---Figure"><img src="Images/B15019_05_24.jpg" alt="Figure 5.24: The first five rows with the cluster predictions&#13;&#10;" width="1066" height="291"/></div><p class="figure-caption">Figure 5.24: The first five rows with the cluster predictions</p></li>
				<li>Now plot the scatter plot using the <strong class="source-inline">mark_circle()</strong> and <strong class="source-inline">encode()</strong> methods from the <strong class="source-inline">altair</strong> package. Also, to add interactiveness, use the <strong class="source-inline">tooltip</strong> parameter and the <strong class="source-inline">interactive()</strong> method from the <strong class="source-inline">altair</strong> package as shown in the following code snippet:<p class="source-code">alt.Chart(df).mark_circle()\</p><p class="source-code">             .encode\</p><p class="source-code">              (x='Average total business income', \</p><p class="source-code">               y='Average total business expenses', \</p><p class="source-code">               color='cluster2:N', \</p><p class="source-code">               tooltip=['Postcode', 'cluster2', \</p><p class="source-code">                        'Average total business income',\</p><p class="source-code">                        'Average total business expenses'])\</p><p class="source-code">             .interactive()</p><p>You should get the following output: </p><div id="_idContainer229" class="IMG---Figure"><img src="Images/B15019_05_25.jpg" alt="Figure 5.25: Scatter plot of the four clusters&#13;&#10;" width="1665" height="1090"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.25: Scatter plot of the four clusters</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iK1rPQ">https://packt.live/3iK1rPQ</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>You just learned how to find the optimal number of clusters before fitting a k-means model. The data points are grouped into four different clusters in our output here:</p>
			<ul>
				<li>Cluster 0 (blue) is for all the observations with average total business income values lower than 100,000 and average total business expense values lower than 80,000.</li>
				<li>Cluster 1 (orange) is grouping data points that have an average total business income value lower than 180,000 and average total business expense values lower than 160,000.</li>
				<li>Cluster 3 (cyan) is for data points that have an average total business income value lower than 370,000 and average total business expense values lower than 330,000.</li>
				<li>Cluster 2 (red) is for data points with extreme values – those with average total business income values higher than 370,000 and average total business expense values higher than 330,000.</li>
			</ul>
			<p>The results from <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, have eight different clusters, and some of them are very similar to each other. Here, you saw that having the optimal number of clusters provides better differentiation between the groups, and this is why it is one of the most important hyperparameters to be tuned for k-means. In the next section, we will look at two other important hyperparameters for initializing k-means.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Initializing Clusters</h1>
			<p>Since the beginning of this chapter, we've been referring to k-means every time we've fitted our clustering algorithms. But you may have noticed in each model summary that there was a hyperparameter called <strong class="source-inline">init</strong> with the default value as k-means++. We were, in fact, using k-means++ all this time.</p>
			<p>The difference between k-means and k-means++ is in how they initialize clusters at the start of the training. k-means randomly chooses the center of each cluster (called the <strong class="bold">centroid</strong>) and then assigns each data point to its nearest cluster. If this cluster initialization is chosen incorrectly, this may lead to non-optimal grouping at the end of the training process. For example, in the following graph, we can clearly see the three natural groupings of the data, but the algorithm didn't succeed in identifying them properly:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="Images/B15019_05_26.jpg" alt="Figure 5.26: Example of non-optimal clusters being found&#13;&#10;" width="1531" height="1056"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26: Example of non-optimal clusters being found</p>
			<p>k-means++ is an attempt to find better clusters at initialization time. The idea behind it is to choose the first cluster randomly and then pick the next ones, those further away, using a probability distribution from the remaining data points. Even though k-means++ tends to get better results compared to the original k-means, in some cases, it can still lead to non-optimal clustering.</p>
			<p>Another hyperparameter data scientists can use to lower the risk of incorrect clusters is <strong class="source-inline">n_init</strong>. This corresponds to the number of times k-means is run with different initializations, the final model being the best run. So, if you have a high number for this hyperparameter, you will have a higher chance of finding the optimal clusters, but the downside is that the training time will be longer. So, you have to choose this value carefully, especially if you have a large dataset.</p>
			<p>Let's try this out on our ATO dataset by having a look at the following example. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Open the notebook you were using for <em class="italic">Exercise 5.01</em>, <em class="italic">Performing Your First Clustering Analysis on the ATO Dataset,</em> and earlier examples. Execute the code you already entered, and then continue at the end of the notebook with the following code.</p>
			<p>First, let's run only one iteration using random initialization:</p>
			<p class="source-code">kmeans = KMeans(random_state=14, n_clusters=3, \</p>
			<p class="source-code">                init='random', n_init=1)</p>
			<p class="source-code">kmeans.fit(X)</p>
			<p>As usual, we want to visualize our clusters with a scatter plot, as defined in the following code snippet:</p>
			<p class="source-code">df['cluster3'] = kmeans.predict(X)</p>
			<p class="source-code">alt.Chart(df).mark_circle()\</p>
			<p class="source-code">             .encode(x='Average net tax', \</p>
			<p class="source-code">                     y='Average total deductions', \</p>
			<p class="source-code">                     color='cluster3:N', \</p>
			<p class="source-code">                     tooltip=['Postcode', 'cluster', \</p>
			<p class="source-code">                              'Average net tax', \</p>
			<p class="source-code">                              'Average total deductions']) \</p>
			<p class="source-code">             .interactive()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="Images/B15019_05_27.jpg" alt="Figure 5.27: Clustering results with n_init as 1 and init as random&#13;&#10;" width="1092" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.27: Clustering results with n_init as 1 and init as random</p>
			<p>Overall, the result is very close to that of our previous run. It is worth noticing that the boundaries between the clusters are slightly different.</p>
			<p>Now let's try with five iterations (using the <strong class="source-inline">n_init</strong> hyperparameter) and k-means++ initialization (using the <strong class="source-inline">init</strong> hyperparameter):</p>
			<p class="source-code">kmeans = KMeans(random_state=14, n_clusters=3, \</p>
			<p class="source-code">                init='k-means++', n_init=5)</p>
			<p class="source-code">kmeans.fit(X)</p>
			<p class="source-code">df['cluster4'] = kmeans.predict(X)</p>
			<p class="source-code">alt.Chart(df).mark_circle()\</p>
			<p class="source-code">             .encode(x='Average net tax', \</p>
			<p class="source-code">                     y='Average total deductions', \</p>
			<p class="source-code">                     color='cluster4:N', \</p>
			<p class="source-code">                     tooltip=['Postcode', 'cluster', \</p>
			<p class="source-code">                              'Average net tax', \</p>
			<p class="source-code">                              'Average total deductions'])\</p>
			<p class="source-code">                    .interactive()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="Images/B15019_05_28.jpg" alt="Figure 5.28: Clustering results with n_init as 5 and init as k-means++&#13;&#10;" width="1636" height="1058"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.28: Clustering results with n_init as 5 and init as k-means++</p>
			<p>Here, the results are very close to the original run with 10 iterations. This means that we didn't have to run so many iterations for k-means to converge and could have saved some time with a lower number.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor126"/>Exercise 5.04: Using Different Initialization Parameters to Achieve a Suitable Outcome</h2>
			<p>In this exercise, we will use the same data as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, and try different values for the <strong class="source-inline">init</strong> and <strong class="source-inline">n_init</strong> hyperparameters and see how they affect the final clustering result:</p>
			<ol>
				<li value="1">Open a new Colab notebook. </li>
				<li>Import the required packages, which are <strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">altair</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">import altair as alt</p></li>
				<li>Assign the link to the ATO dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter05/DataSet/taxstats2015.csv'</p></li>
				<li>Load the dataset and select the same columns as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, and <em class="italic">Exercise 5.03</em>, <em class="italic">Finding the Optimal Number of Clusters</em>, using the <strong class="source-inline">read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average total business income', \</p><p class="source-code">                          'Average total business expenses'])</p></li>
				<li>Assign the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expenses'</strong> columns to a new variable called <strong class="source-inline">X</strong>:<p class="source-code">X = df[['Average total business income', \</p><p class="source-code">        'Average total business expenses']]</p></li>
				<li>Fit a k-means model with <strong class="source-inline">n_init</strong> equal to <strong class="source-inline">1</strong> and a random <strong class="source-inline">init</strong>:<p class="source-code">kmeans = KMeans(random_state=1, n_clusters=4, \</p><p class="source-code">                init='random', n_init=1)</p><p class="source-code">kmeans.fit(X)</p></li>
				<li>Using the <strong class="source-inline">predict</strong> method from the <strong class="source-inline">sklearn</strong> package, predict the clustering assignment from the input variable, <strong class="source-inline">(X)</strong>, and save the results into a new column called <strong class="source-inline">'cluster3'</strong> in the DataFrame:<p class="source-code">df['cluster3'] = kmeans.predict(X)</p></li>
				<li>Plot the clusters using an interactive scatter plot. First, use <strong class="source-inline">Chart()</strong> and <strong class="source-inline">mark_circle()</strong> from the <strong class="source-inline">altair</strong> package to instantiate a scatter plot graph, as shown in the following code snippet:<p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p></li>
				<li>Use the <strong class="source-inline">encode</strong> and <strong class="source-inline">interactive</strong> methods from <strong class="source-inline">altair</strong> to specify the display of the scatter plot and its interactivity options with the following parameters:<p>Provide the name of the <strong class="source-inline">'Average total business income'</strong> column to the <strong class="source-inline">x</strong> parameter (x-axis).</p><p>Provide the name of the <strong class="source-inline">'Average total business expenses'</strong> column to the <strong class="source-inline">y</strong> parameter (y-axis).</p><p>Provide the name of the <strong class="source-inline">'cluster3:N'</strong> column to the <strong class="source-inline">color</strong> parameter (which defines the different colors for each group).</p><p>Provide these column names –  <strong class="source-inline">'Postcode'</strong>, <strong class="source-inline">'cluster3'</strong>, <strong class="source-inline">'Average total business income'</strong>, and <strong class="source-inline">'Average total business expenses'</strong> – to the <strong class="source-inline">tooltip</strong> parameter:</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster3:N', \</p><p class="source-code">                    tooltip=['Postcode', 'cluster3', \</p><p class="source-code">                             'Average total business income', \</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p>You should get the following output:</p><div id="_idContainer233" class="IMG---Figure"><img src="Images/B15019_05_29.jpg" alt="Figure 5.29: Clustering results with n_init as 1 and init as random&#13;&#10;" width="1665" height="1090"/></div><p class="figure-caption">Figure 5.29: Clustering results with n_init as 1 and init as random</p></li>
				<li>Repeat <em class="italic">Steps 5</em> to <em class="italic">8</em> but with different k-means hyperparameters, <strong class="source-inline">n_init=10</strong> and random <strong class="source-inline">init</strong>, as shown in the following code snippet:<p class="source-code">kmeans = KMeans(random_state=1, n_clusters=4, \</p><p class="source-code">                init='random', n_init=10)</p><p class="source-code">kmeans.fit(X)</p><p class="source-code">df['cluster4'] = kmeans.predict(X)</p><p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster4:N',</p><p class="source-code">                    tooltip=['Postcode', 'cluster4', \</p><p class="source-code">                             'Average total business income', \</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p>You should get the following output: </p><div id="_idContainer234" class="IMG---Figure"><img src="Images/B15019_05_30.jpg" alt="Figure 5.30: Clustering results with n_init as 10 and init as random&#13;&#10;" width="1665" height="1090"/></div><p class="figure-caption">Figure 5.30: Clustering results with n_init as 10 and init as random</p></li>
				<li>Again, repeat <em class="italic">Steps 5 </em>to <em class="italic">8</em> but with different k-means hyperparameters – <strong class="source-inline">n_init=100</strong> and random <strong class="source-inline">init</strong>:<p class="source-code">kmeans = KMeans(random_state=1, n_clusters=4, \</p><p class="source-code">                init='random', n_init=100)</p><p class="source-code">kmeans.fit(X)</p><p class="source-code">df['cluster5'] = kmeans.predict(X)</p><p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster5:N', \</p><p class="source-code">                    tooltip=['Postcode', 'cluster5', \</p><p class="source-code">                    'Average total business income', \</p><p class="source-code">                    'Average total business expenses'])\</p><p class="source-code">            .interactive()</p><p>You should get the following output: </p></li>
			</ol>
			<p> </p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="Images/B15019_05_31.jpg" alt="Figure 5.31: Clustering results with n_init as 10 and init as random&#13;&#10;" width="1665" height="1090"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.31: Clustering results with n_init as 10 and init as random</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/315yGqP">https://packt.live/315yGqP</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>You just learned how to tune the two main hyperparameters responsible for initializing k-means clusters. You have seen in this exercise that increasing the number of iterations with <strong class="source-inline">n_init</strong> didn't have much impact on the clustering result for this dataset. </p>
			<p>In this case, it is better to use a lower value for this hyperparameter as it will speed up the training time. But for a different dataset, you may face a case where the results differ drastically depending on the <strong class="source-inline">n_init</strong> value. In such a case, you will have to find a value of <strong class="source-inline">n_init</strong> that is not too small but also not too big. You want to find the sweet spot where the results do not change much compared to the last result obtained with a different value.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor127"/>Calculating the Distance to the Centroid</h1>
			<p>We've talked a lot about similarities between data points in the previous sections, but we haven't really defined what this means. You have probably guessed that it has something to do with how close or how far observations are from each other. You are heading in the right direction. It has to do with some sort of distance measure between two points. The one used by k-means is called <strong class="bold">squared Euclidean distance</strong> and its formula is:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="Images/B15019_05_32.jpg" alt="Figure 5.32: The squared Euclidean distance formula&#13;&#10;" width="974" height="131"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32: The squared Euclidean distance formula</p>
			<p>If you don't have a statistical background, this formula may look intimidating, but it is actually very simple. It is the sum of the squared difference between the data coordinates. Here, <em class="italic">x</em> and <em class="italic">y</em> are two data points and the index, <em class="italic">i</em>, represents the number of coordinates. If the data has two dimensions, <em class="italic">i</em> equals 2. Similarly, if there are three dimensions, then <em class="italic">i</em> will be 3.</p>
			<p>Let's apply this formula to the ATO dataset. </p>
			<p>First, we will grab the values needed – that is, the coordinates from the first two observations – and print them:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Open the notebook you were using for <em class="italic">Exercise 5.01</em>, <em class="italic">Performing Your First Clustering Analysis on the ATO Dataset</em>, and earlier examples. Execute the code you already entered, and then continue at the end of the notebook with the following code.</p>
			<p class="source-code">x = X.iloc[0,].values</p>
			<p class="source-code">y = X.iloc[1,].values</p>
			<p class="source-code">print(x)</p>
			<p class="source-code">print(y)</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="Images/B15019_05_33.jpg" alt="Figure 5.33: Extracting the first two observations from the ATO dataset&#13;&#10;" width="969" height="70"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33: Extracting the first two observations from the ATO dataset</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In pandas, the <strong class="source-inline">iloc</strong> method is used to subset the rows or columns of a DataFrame by index. For instance, if we wanted to grab row number 888 and column number 6, we would use the following syntax: <strong class="source-inline">dataframe.iloc[888, 6]</strong>.</p>
			<p>The coordinates for <strong class="source-inline">x</strong> are <strong class="source-inline">(27555, 2071)</strong> and the coordinates for <strong class="source-inline">y</strong> are <strong class="source-inline">(28142, 3804)</strong>. Here, the formula is telling us to calculate the squared difference between each axis of the two data points and sum them:</p>
			<p class="source-code">squared_euclidean = (x[0] - y[0])**2 + (x[1] - y[1])**2</p>
			<p class="source-code">print(squared_euclidean)</p>
			<p>You should get the following output:</p>
			<p class="source-code">3347858</p>
			<p>k-means uses this metric to calculate the distance between each data point and the center of its assigned cluster (also called the centroid). Here is the basic logic behind this algorithm:</p>
			<ol>
				<li value="1">Choose the centers of the clusters (the centroids) randomly.</li>
				<li>Assign each data point to the nearest centroid using the squared Euclidean distance.</li>
				<li>Update each centroid's coordinates to the newly calculated center of the data points assigned to it.</li>
				<li>Repeat <em class="italic">Steps 2</em> and <em class="italic">3</em> until the clusters converge (that is, until the cluster assignment doesn't change anymore) or until the maximum number of iterations has been reached.</li>
			</ol>
			<p>That's it. The k-means algorithm is as simple as that. We can extract the centroids after fitting a k-means model with <strong class="source-inline">cluster_centers_</strong>. </p>
			<p>Let's see how we can plot the centroids in an example.</p>
			<p>First, we fit a k-means model as shown in the following code snippet:</p>
			<p class="source-code">kmeans = KMeans(random_state=42, n_clusters=3, \</p>
			<p class="source-code">                init='k-means++', n_init=5)</p>
			<p class="source-code">kmeans.fit(X)</p>
			<p class="source-code">df['cluster6'] = kmeans.predict(X)</p>
			<p>Now extract the <strong class="source-inline">centroids</strong> into a DataFrame and print them:</p>
			<p class="source-code">centroids = kmeans.cluster_centers_</p>
			<p class="source-code">centroids = pd.DataFrame(centroids, \</p>
			<p class="source-code">                         columns=['Average net tax', \</p>
			<p class="source-code">                                  'Average total deductions'])</p>
			<p class="source-code">print(centroids)</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="Images/B15019_05_34.jpg" alt="Figure 5.34: Coordinates of the three centroids&#13;&#10;" width="1035" height="148"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.34: Coordinates of the three centroids</p>
			<p>We will plot the usual scatter plot but will assign it to a variable called <strong class="source-inline">chart1</strong>:</p>
			<p class="source-code">chart1 = alt.Chart(df).mark_circle()\</p>
			<p class="source-code">            .encode(x='Average net tax', \</p>
			<p class="source-code">                    y='Average total deductions', \</p>
			<p class="source-code">                    color='cluster6:N', \</p>
			<p class="source-code">                    tooltip=['Postcode', 'cluster6', \</p>
			<p class="source-code">                             'Average net tax', \</p>
			<p class="source-code">                             'Average total deductions'])\</p>
			<p class="source-code">                   .interactive()</p>
			<p class="source-code">chart1</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="Images/B15019_05_35.jpg" alt="Figure 5.35: Scatter plot of the clusters&#13;&#10;" width="1092" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.35: Scatter plot of the clusters</p>
			<p>Now, to create a second scatter plot only for the centroids called <strong class="source-inline">chart2</strong>:</p>
			<p class="source-code">chart2 = alt.Chart(centroids).mark_circle(size=100)\</p>
			<p class="source-code">            .encode(x='Average net tax', \</p>
			<p class="source-code">                    y='Average total deductions', \</p>
			<p class="source-code">                    color=alt.value('black'), \</p>
			<p class="source-code">                    tooltip=['Average net tax', \</p>
			<p class="source-code">                             'Average total deductions'])\</p>
			<p class="source-code">                   .interactive()</p>
			<p class="source-code">chart2</p>
			<p> You should get the following output: </p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="Images/B15019_05_36.jpg" alt="Figure 5.36: Scatter plot of the centroids&#13;&#10;" width="1431" height="1056"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.36: Scatter plot of the centroids</p>
			<p>And now we combine the two charts, which is extremely easy with <strong class="source-inline">altair</strong>:</p>
			<p class="source-code">chart1 + chart2</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="Images/B15019_05_37.jpg" alt="Figure 5.37: Scatter plot of the clusters and their centroids&#13;&#10;" width="1092" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.37: Scatter plot of the clusters and their centroids</p>
			<p>Now we can easily see which centroids the observations are closest to.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Exercise 5.05: Finding the Closest Centroids in Our Dataset</h2>
			<p>In this exercise, we will be coding the first iteration of k-means in order to assign data points to their closest cluster centroids. The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Now <strong class="source-inline">import</strong> the required packages, which are <strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">altair</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">import altair as alt</p></li>
				<li>Load the dataset and select the same columns as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, using the <strong class="source-inline">read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter05/DataSet/taxstats2015.csv'</p><p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average total business income', \</p><p class="source-code">                          'Average total business expenses'])</p></li>
				<li>Assign the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expenses'</strong> columns to a new variable called <strong class="source-inline">X</strong>:<p class="source-code">X = df[['Average total business income', \</p><p class="source-code">        'Average total business expenses']]</p></li>
				<li>Now, calculate the minimum and maximum using the <strong class="source-inline">min()</strong> and <strong class="source-inline">max()</strong> values of the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business income'</strong> variables, as shown in the following code snippet:<p class="source-code">business_income_min = df['Average total business income'].min()</p><p class="source-code">business_income_max = df['Average total business income'].max()</p><p class="source-code">business_expenses_min = df['Average total business expenses']\</p><p class="source-code">                        .min()</p><p class="source-code">business_expenses_max = df['Average total business expenses']\</p><p class="source-code">                        .max()</p></li>
				<li>Print the values of these four variables, which are the minimum and maximum values of the two variables:<p class="source-code">print(business_income_min)</p><p class="source-code">print(business_income_max)</p><p class="source-code">print(business_expenses_min)</p><p class="source-code">print(business_expenses_max)</p><p>You should get the following output:</p><p class="source-code">0</p><p class="source-code">876324</p><p class="source-code">0</p><p class="source-code">884659</p></li>
				<li>Now import the <strong class="source-inline">random</strong> package and use the <strong class="source-inline">seed()</strong> method to set a seed of <strong class="source-inline">42</strong>, as shown in the following code snippet:<p class="source-code">import random</p><p class="source-code">random.seed(42)</p></li>
				<li>Create an empty pandas DataFrame and assign it to a variable called <strong class="source-inline">centroids</strong>:<p class="source-code">centroids = pd.DataFrame()</p></li>
				<li>Generate four random values using the <strong class="source-inline">sample()</strong> method from the <strong class="source-inline">random</strong> package with possible values between the minimum and maximum values of the <strong class="source-inline">'Average total business expenses'</strong> column using <strong class="source-inline">range()</strong> and store the results in a new column called <strong class="source-inline">'Average total business income'</strong> from the <strong class="source-inline">centroids</strong> DataFrame:<p class="source-code">centroids\</p><p class="source-code">['Average total business income'] = random.sample\</p><p class="source-code">                                    (range\</p><p class="source-code">                                    (business_income_min, \</p><p class="source-code">                                     business_income_max), 4)</p></li>
				<li>Repeat the same process to generate <strong class="source-inline">4</strong> random values for  <strong class="source-inline">'Average total business expenses'</strong>:<p class="source-code">centroids\</p><p class="source-code">['Average total business expenses'] = random.sample\</p><p class="source-code">                                      (range\</p><p class="source-code">                                      (business_expenses_min,\</p><p class="source-code">                                       business_expenses_max), 4)</p></li>
				<li>Create a new column called <strong class="source-inline">'cluster'</strong> from the <strong class="source-inline">centroids</strong> DataFrame using the <strong class="source-inline">.index </strong>attributes from the pandas package and print this DataFrame:<p class="source-code">centroids['cluster'] = centroids.index</p><p class="source-code">centroids</p><p>You should get the following output: </p><div id="_idContainer242" class="IMG---Figure"><img src="Images/B15019_05_38.jpg" alt="Figure 5.38: Coordinates of the four random centroids&#13;&#10;" width="1139" height="265"/></div><p class="figure-caption">Figure 5.38: Coordinates of the four random centroids</p></li>
				<li>Create a scatter plot with the <strong class="source-inline">altair</strong> package to display the data contained in the <strong class="source-inline">df</strong> DataFrame and save it in a variable called <strong class="source-inline">'chart1'</strong>:<p class="source-code">chart1 = alt.Chart(df.head()).mark_circle()\</p><p class="source-code">            .encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color=alt.value('orange'), \</p><p class="source-code">                    tooltip=['Postcode', \</p><p class="source-code">                             'Average total business income', \</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p></li>
				<li>Now create a second scatter plot using the <strong class="source-inline">altair</strong> package to display the centroids and save it in a variable called <strong class="source-inline">'chart2'</strong>:<p class="source-code">chart2 = alt.Chart(centroids).mark_circle(size=100)\</p><p class="source-code">            .encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color=alt.value('black'), \</p><p class="source-code">                    tooltip=['cluster', \</p><p class="source-code">                             'Average total business income',\</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p></li>
				<li>Display the two charts together using the altair syntax: <strong class="source-inline">&lt;chart&gt; + &lt;chart&gt;</strong>:<p class="source-code">chart1 + chart2</p><p>You should get the following output: </p><div id="_idContainer243" class="IMG---Figure"><img src="Images/B15019_05_39.jpg" alt="Figure 5.39: Scatter plot of the random centroids and the first five observations&#13;&#10;" width="1665" height="1201"/></div><p class="figure-caption">Figure 5.39: Scatter plot of the random centroids and the first five observations</p></li>
				<li>Define a function that will calculate the <strong class="source-inline">squared_euclidean</strong> distance and return its value. This function will take the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> coordinates of a data point and a centroid:<p class="source-code">def squared_euclidean(data_x, data_y, \</p><p class="source-code">                      centroid_x, centroid_y, ):</p><p class="source-code">    return (data_x - centroid_x)**2 + (data_y - centroid_y)**2</p></li>
				<li>Using the <strong class="source-inline">.at</strong> method from the pandas package, extract the first row's <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> coordinates and save them in two variables called <strong class="source-inline">data_x</strong> and <strong class="source-inline">data_y</strong>:<p class="source-code">data_x = df.at[0, 'Average total business income']</p><p class="source-code">data_y = df.at[0, 'Average total business expenses']</p></li>
				<li>Using a <strong class="source-inline">for</strong> loop or list comprehension, calculate the <strong class="source-inline">squared_euclidean</strong> distance of the first observation (using its <strong class="source-inline">data_x</strong> and <strong class="source-inline">data_y</strong> coordinates) against the <strong class="source-inline">4</strong> different centroids contained in <strong class="source-inline">centroids</strong>, save the result in a variable called <strong class="source-inline">distance</strong>, and display it:<p class="source-code">distances = [squared_euclidean\</p><p class="source-code">             (data_x, data_y, centroids.at\</p><p class="source-code">              [i, 'Average total business income'], \</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">              'Average total business expenses']) \</p><p class="source-code">              for i in range(4)]</p><p class="source-code">distances</p><p>You should get the following output:</p><p class="source-code">[215601466600, 10063365460, 34245932020, 326873037866]</p></li>
				<li>Use the <strong class="source-inline">index</strong> method from the list containing the <strong class="source-inline">squared_euclidean</strong> distances to find the cluster with the shortest distance, as shown in the following code snippet:<p class="source-code">cluster_index = distances.index(min(distances))</p></li>
				<li>Save the <strong class="source-inline">cluster</strong> index in a column called <strong class="source-inline">'cluster'</strong> from the <strong class="source-inline">df</strong> DataFrame for the first observation using the <strong class="source-inline">.at</strong> method from the pandas package:<p class="source-code">df.at[0, 'cluster'] = cluster_index</p></li>
				<li>Display the first five rows of <strong class="source-inline">df</strong> using the <strong class="source-inline">head()</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer244" class="IMG---Figure"><img src="Images/B15019_05_40.jpg" alt="Figure 5.40: The first five rows of the ATO DataFrame with the assigned &#13;&#10;cluster number for the first row&#13;&#10;" width="1282" height="312"/></div><p class="figure-caption">Figure 5.40: The first five rows of the ATO DataFrame with the assigned cluster number for the first row</p></li>
				<li>Repeat <em class="italic">Steps 15</em> to <em class="italic">19</em> for the next <strong class="source-inline">4</strong> rows to calculate their distances from the centroids and find the cluster with the smallest distance value: <p class="source-code">distances = [squared_euclidean\</p><p class="source-code">             (df.at[1, 'Average total business income'], \</p><p class="source-code">              df.at[1, 'Average total business expenses'], \</p><p class="source-code">              centroids.at[i, 'Average total business income'],\</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">                           'Average total business expenses'])\</p><p class="source-code">             for i in range(4)]</p><p class="source-code">df.at[1, 'cluster'] = distances.index(min(distances))</p><p class="source-code">distances = [squared_euclidean\</p><p class="source-code">             (df.at[2, 'Average total business income'], \</p><p class="source-code">              df.at[2, 'Average total business expenses'], \</p><p class="source-code">              centroids.at[i, 'Average total business income'],\</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">                           'Average total business expenses'])\</p><p class="source-code">             for i in range(4)]</p><p class="source-code">df.at[2, 'cluster'] = distances.index(min(distances))</p><p class="source-code">distances = [squared_euclidean\</p><p class="source-code">             (df.at[3, 'Average total business income'], \</p><p class="source-code">              df.at[3, 'Average total business expenses'], \</p><p class="source-code">              centroids.at[i, 'Average total business income'],\</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">                           'Average total business expenses'])\</p><p class="source-code">             for i in range(4)]</p><p class="source-code">df.at[3, 'cluster'] = distances.index(min(distances))</p><p class="source-code">distances = [squared_euclidean\</p><p class="source-code">             (df.at[4, 'Average total business income'], \</p><p class="source-code">              df.at[4, 'Average total business expenses'], \</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">              'Average total business income'], \</p><p class="source-code">              centroids.at[i, \</p><p class="source-code">              'Average total business expenses']) \</p><p class="source-code">             for i in range(4)]</p><p class="source-code">df.at[4, 'cluster'] = distances.index(min(distances))</p><p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer245" class="IMG---Figure"><img src="Images/B15019_05_41.jpg" alt="Figure 5.41: The first five rows of the ATO DataFrame and their assigned clusters&#13;&#10;" width="1216" height="322"/></div><p class="figure-caption">Figure 5.41: The first five rows of the ATO DataFrame and their assigned clusters</p></li>
				<li>Finally, plot the centroids and the first <strong class="source-inline">5</strong> rows of the dataset using the <strong class="source-inline">altair</strong> package as in <em class="italic">Steps 12</em> to <em class="italic">13</em>:<p class="source-code">chart1 = alt.Chart(df.head()).mark_circle()\</p><p class="source-code">            .encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster:N', \</p><p class="source-code">                    tooltip=['Postcode', 'cluster', \</p><p class="source-code">                             'Average total business income', \</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p class="source-code">chart2 = alt.Chart(centroids).mark_circle(size=100)\</p><p class="source-code">            .encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color=alt.value('black'), \</p><p class="source-code">                    tooltip=['cluster', \</p><p class="source-code">                             'Average total business income',\</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p class="source-code">chart1 + chart2</p><p>You should get the following output:</p></li>
			</ol>
			<p> </p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="Images/B15019_05_42.jpg" alt="Figure 5.42: Scatter plot of the random centroids and the first five observations&#13;&#10;" width="1665" height="1101"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.42: Scatter plot of the random centroids and the first five observations</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iPQo7L">https://packt.live/3iPQo7L</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>In this final result, we can see where the four clusters have been placed in the graph and which cluster the five data points have been assigned to:</p>
			<ul>
				<li>The two data points in the bottom-left corner have been assigned to cluster 2, which corresponds to the one with a centroid of coordinates of 26,000 (average total business income) and 234,000 (average total business expense). It is the closest centroid for these two points.</li>
				<li>The two observations in the middle are very close to the centroid with coordinates of 116,000 (average total business income) and 256,000 (average total business expense), which corresponds to cluster 1.</li>
				<li>The observation at the top has been assigned to cluster 0, whose centroid has coordinates of 670,000 (average total business income) and 288,000 (average total business expense).</li>
			</ul>
			<p>You just re-implemented a big part of the k-means algorithm from scratch. You went through how to randomly initialize centroids (cluster centers), calculate the squared Euclidean distance for some data points, find their closest centroid, and assign them to the corresponding cluster. This wasn't easy, but you made it.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/>Standardizing Data</h1>
			<p>You've already learned a lot about the k-means algorithm, and we are close to the end of this chapter. In this final section, we will not talk about another hyperparameter (you've already been through the main ones) but a very important topic: <strong class="bold">data processing</strong>.</p>
			<p>Fitting a k-means algorithm is extremely easy. The trickiest part is making sure the resulting clusters are meaningful for your project, and we have seen how we can tune some hyperparameters to ensure this. But handling input data is as important as all the steps you have learned about so far. If your dataset is not well prepared, even if you find the best hyperparameters, you will still get some bad results.</p>
			<p>Let's have another look at our ATO dataset. In the previous section, <em class="italic">Calculating the Distance to the Centroid</em>, we found three different clusters, and they were mainly defined by the <strong class="source-inline">'Average net tax'</strong> variable. It was as if k-means didn't take into account the second variable, <strong class="source-inline">'Average total deductions'</strong>, at all. This is in fact due to these two variables having very different ranges of values and the way that squared Euclidean distance is calculated.</p>
			<p>Squared Euclidean distance is weighted more toward high-value variables. Let's take an example to illustrate this point with two data points called A and B with respective x and y coordinates of (1, 50000) and (100, 100000). The squared Euclidean distance between A and B will be (100000 - 50000)^2 + (100 - 1)^2. We can clearly see that the result will be mainly driven by the difference between 100,000 and 50,000: 50,000^2. The difference of 100 minus 1 (99^2) will account for very little in the final result.</p>
			<p>But if you look at the ratio between 100,000 and 50,000, it is a factor of 2 (100,000 / 50,000 = 2), while the ratio between 100 and 1 is a factor of 100 (100 / 1 = 100). Does it make sense for the higher-value variable to "dominate" the clustering result? It really depends on your project, and this situation may be intended. But if you want things to be fair between the different axes, it's preferable to bring them all into a similar range of values before fitting a k-means model. This is the reason why you should always consider standardizing your data before running your k-means algorithm.</p>
			<p>There are multiple ways to standardize data, and we will have a look at the two most popular ones: <strong class="bold">min-max scaling</strong> and <strong class="bold">z-score</strong>. Luckily for us, the <strong class="source-inline">sklearn</strong> package has an implementation for both methods.</p>
			<p>The formula for min-max scaling is very simple: on each axis, you need to remove the minimum value for each data point and divide the result by the difference between the maximum and minimum values. The scaled data will have values ranging between 0 and 1:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="Images/B15019_05_43.jpg" alt="Figure 5.43: Min-max scaling formula&#13;&#10;" width="1217" height="130"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.43: Min-max scaling formula</p>
			<p>Let's look at min-max scaling with <strong class="source-inline">sklearn</strong> in the following example. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Open the notebook you were using for <em class="italic">Exercise 5.01</em>, <em class="italic">Performing Your First Clustering Analysis on the ATO Dataset</em>, and earlier examples. Execute the code you already entered, and then continue at the end of the notebook with the following code.</p>
			<p>First, we import the relevant class and instantiate an object:</p>
			<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p>
			<p class="source-code">min_max_scaler = MinMaxScaler()</p>
			<p>Then, we fit it to our dataset:</p>
			<p class="source-code">min_max_scaler.fit(X)</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="Images/B15019_05_44.jpg" alt="Figure 5.44: Min-max scaling summary&#13;&#10;" width="780" height="60"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.44: Min-max scaling summary</p>
			<p>And finally, call the <strong class="source-inline">transform()</strong> method to standardize the data:</p>
			<p class="source-code">X_min_max = min_max_scaler.transform(X)</p>
			<p class="source-code">X_min_max</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="Images/B15019_05_45.jpg" alt="Figure 5.45: Min-max-scaled data&#13;&#10;" width="880" height="258"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.45: Min-max-scaled data</p>
			<p>Now we print the minimum and maximum values of the min-max-scaled data for both axes:</p>
			<p class="source-code">X_min_max[:,0].min(), X_min_max[:,0].max(), \</p>
			<p class="source-code">X_min_max[:,1].min(), X_min_max[:,1].max()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="Images/B15019_05_46.jpg" alt="Figure 5.46: Minimum and maximum values of the min-max-scaled data&#13;&#10;" width="846" height="50"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.46: Minimum and maximum values of the min-max-scaled data</p>
			<p>We can see that both axes now have their values sitting between 0 and 1.</p>
			<p>The <strong class="bold">z-score</strong> is calculated by removing the overall average from the data point and dividing the result by the standard deviation for each axis. The distribution of the standardized data will have a mean of 0 and a standard deviation of 1:</p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="Images/B15019_05_47.jpg" alt="Figure 5.47: Z-score formula&#13;&#10;" width="1665" height="216"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.47: Z-score formula</p>
			<p>To apply it with <strong class="source-inline">sklearn</strong>, first, we have to import the relevant <strong class="source-inline">StandardScaler</strong> class and instantiate an object:</p>
			<p class="source-code">from sklearn.preprocessing import StandardScaler</p>
			<p class="source-code">standard_scaler = StandardScaler()</p>
			<p>This time, instead of calling <strong class="source-inline">fit()</strong> and then <strong class="source-inline">transform()</strong>, we use the <strong class="source-inline">fit_transform()</strong> method:</p>
			<p class="source-code">X_scaled = standard_scaler.fit_transform(X)</p>
			<p class="source-code">X_scaled</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="Images/B15019_05_48.jpg" alt="Figure 5.48: Z-score-standardized data&#13;&#10;" width="838" height="254"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.48: Z-score-standardized data</p>
			<p>Now we'll look at the minimum and maximum values for each axis:</p>
			<p class="source-code">X_scaled[:,0].min(), X_scaled[:,0].max(), \</p>
			<p class="source-code">X_scaled[:,1].min(), X_scaled[:,1].max()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="Images/B15019_05_49.jpg" alt="Figure 5.49: Minimum and maximum values of the z-score-standardized data&#13;&#10;" width="808" height="144"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.49: Minimum and maximum values of the z-score-standardized data</p>
			<p>The value ranges for both axes are much lower now and we can see that their maximum values are around 9 and 18, which indicates that there are some extreme outliers in the data.</p>
			<p>Now, to fit a k-means model and plot a scatter plot on the z-score-standardized data with the following code snippet:</p>
			<p class="source-code">kmeans = KMeans(random_state=42, n_clusters=3, \</p>
			<p class="source-code">                init='k-means++', n_init=5)</p>
			<p class="source-code">kmeans.fit(X_scaled)</p>
			<p class="source-code">df['cluster7'] = kmeans.predict(X_scaled)</p>
			<p class="source-code">alt.Chart(df).mark_circle()\</p>
			<p class="source-code">             .encode(x='Average net tax', \</p>
			<p class="source-code">                     y='Average total deductions', \</p>
			<p class="source-code">                     color='cluster7:N', \</p>
			<p class="source-code">                     tooltip=['Postcode', 'cluster7', \</p>
			<p class="source-code">                              'Average net tax', \</p>
			<p class="source-code">                              'Average total deductions'])\</p>
			<p class="source-code">                    .interactive()</p>
			<p>You should get the following output: </p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="Images/B15019_05_50.jpg" alt="Figure 5.50: Scatter plot of the standardized data&#13;&#10;" width="1092" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.50: Scatter plot of the standardized data</p>
			<p>k-means results are very different from the standardized data. Now we can see that there are two main clusters (blue and red) and their boundaries are not straight vertical lines anymore but diagonal. So, k-means is actually taking into consideration both axes now. The orange cluster contains much fewer data points compared to previous iterations, and it seems it is grouping all the extreme outliers with high values together. If your project was about detecting anomalies, you would have found a way here to easily separate outliers from "normal" observations.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor130"/>Exercise 5.06: Standardizing the Data from Our Dataset</h2>
			<p>In this final exercise, we will standardize the data using min-max scaling and the z-score and fit a k-means model for each method and see their impact on k-means:</p>
			<ol>
				<li value="1">Open a new Colab notebook. </li>
				<li>Now import the required <strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">altair</strong> packages:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">import altair as alt </p></li>
				<li>Load the dataset and select the same columns as in <em class="italic">Exercise 5.02</em>, <em class="italic">Clustering Australian Postcodes by Business Income and Expenses</em>, using the <strong class="source-inline">read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package:<p class="source-code">file_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter05/DataSet/taxstats2015.csv'</p><p class="source-code">df = pd.read_csv(file_url, \</p><p class="source-code">                 usecols=['Postcode', \</p><p class="source-code">                          'Average total business income', \</p><p class="source-code">                          'Average total business expenses'])</p></li>
				<li>Assign the <strong class="source-inline">'Average total business income'</strong> and <strong class="source-inline">'Average total business expenses'</strong> columns to a new variable called <strong class="source-inline">X</strong>:<p class="source-code">X = df[['Average total business income', \</p><p class="source-code">        'Average total business expenses']]</p></li>
				<li>Import the <strong class="source-inline">MinMaxScaler</strong> and <strong class="source-inline">StandardScaler</strong> classes from <strong class="source-inline">sklearn</strong>:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p></li>
				<li>Instantiate and fit <strong class="source-inline">MinMaxScaler</strong> with the data:<p class="source-code">min_max_scaler = MinMaxScaler()</p><p class="source-code">min_max_scaler.fit(X)</p><p>You should get the following output:</p><div id="_idContainer255" class="IMG---Figure"><img src="Images/B15019_05_51.jpg" alt="Figure 5.51: Summary of the min-max scaler&#13;&#10;" width="757" height="32"/></div><p class="figure-caption">Figure 5.51: Summary of the min-max scaler</p></li>
				<li>Perform the min-max scaling transformation and save the data into a new variable called <strong class="source-inline">X_min_max</strong>:<p class="source-code">X_min_max = min_max_scaler.transform(X)</p><p class="source-code">X_min_max</p><p>You should get the following output:</p><div id="_idContainer256" class="IMG---Figure"><img src="Images/B15019_05_52.jpg" alt="Figure 5.52: Min-max-scaled data&#13;&#10;" width="1168" height="355"/></div><p class="figure-caption">Figure 5.52: Min-max-scaled data</p></li>
				<li>Fit a k-means model on the scaled data with the following hyperparameters: <strong class="source-inline">random_state=1</strong>, <strong class="source-inline">n_clusters=4, init='k-means++', n_init=5</strong>, as shown in the following code snippet:<p class="source-code">kmeans = KMeans(random_state=1, n_clusters=4, \</p><p class="source-code">                init='k-means++', n_init=5)</p><p class="source-code">kmeans.fit(X_min_max)</p></li>
				<li>Assign the k-means predictions of each value of <strong class="source-inline">X</strong> in a new column called <strong class="source-inline">'cluster8'</strong> in the <strong class="source-inline">df</strong> DataFrame:<p class="source-code">df['cluster8'] = kmeans.predict(X_min_max)</p></li>
				<li>Plot the k-means results into a scatter plot using the <strong class="source-inline">altair</strong> package:<p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses',\</p><p class="source-code">                    color='cluster8:N',\</p><p class="source-code">                    tooltip=['Postcode', 'cluster8', \</p><p class="source-code">                             'Average total business income',\</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p>You should get the following output: </p><div id="_idContainer257" class="IMG---Figure"><img src="Images/B15019_05_53.jpg" alt="Figure 5.53: Scatter plot of k-means results using the min-max-scaled data&#13;&#10;" width="1665" height="1090"/></div><p class="figure-caption">Figure 5.53: Scatter plot of k-means results using the min-max-scaled data</p></li>
				<li>Re-train the k-means model but on the z-score-standardized data with the same hyperparameter values, <strong class="source-inline">random_state=1, n_clusters=4, init='k-means++', n_init=5</strong>:<p class="source-code">standard_scaler = StandardScaler()</p><p class="source-code">X_scaled = standard_scaler.fit_transform(X)</p><p class="source-code">kmeans = KMeans(random_state=1, n_clusters=4, \</p><p class="source-code">                init='k-means++', n_init=5)</p><p class="source-code">kmeans.fit(X_scaled)</p></li>
				<li>Assign the k-means predictions of each value of <strong class="source-inline">X_scaled</strong> in a new column called <strong class="source-inline">'cluster9' </strong>in the <strong class="source-inline">df</strong> DataFrame:<p class="source-code">df['cluster9'] = kmeans.predict(X_scaled)</p></li>
				<li>Plot the k-means results in a scatter plot using the <strong class="source-inline">altair</strong> package:<p class="source-code">scatter_plot = alt.Chart(df).mark_circle()</p><p class="source-code">scatter_plot.encode(x='Average total business income', \</p><p class="source-code">                    y='Average total business expenses', \</p><p class="source-code">                    color='cluster9:N', \</p><p class="source-code">                    tooltip=['Postcode', 'cluster9', \</p><p class="source-code">                             'Average total business income',\</p><p class="source-code">                             'Average total business expenses'])\</p><p class="source-code">                   .interactive()</p><p>You should get the following output: </p><div id="_idContainer258" class="IMG---Figure"><img src="Images/B15019_05_54.jpg" alt="Figure 5.54: Scatter plot of k-means results using the z-score-standardized data&#13;&#10;" width="1665" height="1030"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.54: Scatter plot of k-means results using the z-score-standardized data</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Q2BSND">https://packt.live/2Q2BSND</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>The k-means clustering results are very similar between min-max and z-score standardization, which are the outputs for <em class="italic">Steps 10</em> and <em class="italic">13</em>. Compared to the results in <em class="italic">Exercise 5.04</em>, <em class="italic">Using Different Initialization Parameters to Achieve a Suitable Outcome</em>, we can see that the boundaries between clusters 1 and 2 are slightly lower after standardization. The reason why these results are very close to each other is due to the fact that the range of values for the two variables (average total business income and average total business expenses) are almost identical: between 0 and 900,000. Therefore, k-means is not putting more weight toward one of these variables.</p>
			<p>You've completed the final exercise of this chapter. You have learned how to preprocess data before fitting a k-means model with two very popular methods: min-max scaling and z-score.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/>Activity 5.01: Perform Customer Segmentation Analysis in a Bank Using k-means</h2>
			<p>You are working for an international bank. The credit department is reviewing its offerings and wants to get a better understanding of its current customers. You have been tasked with performing customer segmentation analysis. You will perform cluster analysis with k-means to identify groups of similar customers.</p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Download the dataset and load it into Python.</li>
				<li>Read the CSV file using the <strong class="source-inline">read_csv()</strong> method.<p class="callout-heading">Note</p><p class="callout">This dataset is in the <strong class="source-inline">.dat</strong> file format. You can still load the file using <strong class="source-inline">read_csv()</strong> but you will need to specify the following parameter: <strong class="source-inline">header=None, sep= '\s\s+' and prefix='X'</strong>.</p></li>
				<li>You will be using the fourth and tenth columns (<strong class="source-inline">X3</strong> and <strong class="source-inline">X9</strong>). Extract these.</li>
				<li>Perform data standardization by instantiating a <strong class="source-inline">StandardScaler</strong> object.</li>
				<li>Analyze and define the optimal number of clusters.</li>
				<li>Fit a k-means algorithm with the number of clusters you've defined.</li>
				<li>Create a scatter plot of the clusters.<p class="callout-heading">Note</p><p class="callout">This is the German Credit Dataset from the UCI Machine Learning Repository. It is available from the Packt repository at <a href="https://packt.live/31L2c2b">https://packt.live/31L2c2b</a>.Even though all the columns in this dataset are integers, most of them are actually categorical variables. The data in these columns is not continuous. Only two variables are really numeric. Those are the ones you will use for your clustering.</p></li>
			</ol>
			<p>You should get something similar to the following output:</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="Images/B15019_05_55.jpg" alt="Figure 5.55: Scatter plot of the four clusters found&#13;&#10;" width="1665" height="1071"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.55: Scatter plot of the four clusters found</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Summary</h1>
			<p>You are now ready to perform cluster analysis with the k-means algorithm on your own dataset. This type of analysis is very popular in the industry for segmenting customer profiles as well as detecting suspicious transactions or anomalies.</p>
			<p>We learned about a lot of different concepts, such as centroids and squared Euclidean distance. We went through the main k-means hyperparameters: <strong class="source-inline">init</strong> (initialization method), <strong class="source-inline">n_init</strong> (number of initialization runs), <strong class="source-inline">n_clusters</strong> (number of clusters), and <strong class="source-inline">random_state</strong> (specified seed). We also discussed the importance of choosing the optimal number of clusters, initializing centroids properly, and standardizing data. You have learned how to use the following Python packages: <strong class="source-inline">pandas</strong>, <strong class="source-inline">altair</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">KMeans</strong>.</p>
			<p>In this chapter, we only looked at k-means, but it is not the only clustering algorithm. There are quite a lot of algorithms that use different approaches, such as hierarchical clustering, principal component analysis, and the Gaussian mixture model, to name a few. If you are interested in this field, you now have all the basic knowledge you need to explore these other algorithms on your own. </p>
			<p>Next, you will see how we can assess the performance of these models and what tools can be used to make them even better.</p>
		</div>
		<div>
			<div id="_idContainer261" class="Content">
			</div>
		</div>
	</div></body></html>