<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer262" class="Content">
			</div>
		</div>
		<div id="_idContainer263" class="Content">
			<h1 id="_idParaDest-134">6. <a id="_idTextAnchor133"/>How to Assess Performance</h1>
		</div>
		<div id="_idContainer311" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will introduce you to model evaluation, where you evaluate or assess the performance of each model that you train before you decide to put it into production. By the end of this chapter, you will be able to create an evaluation dataset. You will be equipped to assess the performance of linear regression models using <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>) and <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). You will also be able to evaluate the performance of logistic regression models using accuracy, precision, recall, and F1 score.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Introduction</h1>
			<p>When you assess the performance of a model, you look at certain measurements or values that tell you how well the model is performing under certain conditions, and that helps you make an informed decision about whether or not to make use of the model that you have trained in the real world. Some of the measurements you will encounter in this chapter are MAE, precision, recall, and R<span class="superscript">2</span> score.</p>
			<p>You learned how to train a regression model in <em class="italic">Chapter 2, Regression</em>, and how to train classification models in <em class="italic">Chapter 3, Binary Classification</em>. Consider the task of predicting whether or not a customer is likely to purchase a term deposit, which you addressed in <em class="italic">Chapter 3, Binary Classification</em>. You have learned how to train a model to perform this sort of classification. You are now concerned with how useful this model might be. You might start by training one model, and then evaluating how often the predictions from that model are correct. You might then proceed to train more models and evaluate whether they perform better than previous models you have trained.</p>
			<p>You have already seen an example of splitting data using <strong class="source-inline">train_test_split</strong> in <em class="italic">Exercise 3.06</em>, <em class="italic">A Logistic Regression Model for Predicting the Propensity of Term Deposit Purchases in a Bank</em>. You will go further into the necessity and application of splitting data in <em class="italic">Chapter 7, The Generalization of Machine Learning Models</em>, but for now, you should note that it is important to split your data into one set that is used for training a model, and a second set that is used for validating the model. It is this validation step that helps you decide whether or not to put a model into production. </p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>Splitting Data</h1>
			<p>You will learn more about splitting data in <em class="italic">Chapter 7, The Generalization of Machine Learning Models</em>, where we will cover the following:</p>
			<ul>
				<li>Simple data splits using <strong class="source-inline">train_test_split</strong></li>
				<li>Multiple data splits using cross-validation</li>
			</ul>
			<p>For now, you will learn how to split data using a function from <strong class="source-inline">sklearn</strong> called <strong class="source-inline">train_test_split</strong>.</p>
			<p>It is very important that you do not use all of your data to train a model. You must set aside some data for validation, and this data must not have been used previously for training. When you train a model, it tries to generate an equation that fits your data. The longer you train, the more complex the equation becomes so that it passes through as many of the data points as possible.</p>
			<p>When you shuffle the data and set some aside for validation, it ensures that the model learns to not overfit the hypotheses you are trying to generate.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Exercise 6.01: Importing and Splitting Data</h2>
			<p>In this exercise, you will import data from a repository and split it into a training and an evaluation set to train a model. Splitting your data is required so that you can evaluate the model later. This exercise will get you familiar with the process of splitting data; this is something you will be doing frequently.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Car dataset that you will be using in this chapter can be found in our GitHub repository: <a href="https://packt.live/30I594E">https://packt.live/30I594E</a>. </p>
			<p class="callout">It was taken from the UCI Machine Learning Repository.</p>
			<p>This dataset is about cars. A text file is provided with the following information:</p>
			<ul>
				<li><strong class="source-inline">buying</strong> – the cost of purchasing this vehicle</li>
				<li><strong class="source-inline">maint</strong> – the maintenance cost of the vehicle</li>
				<li><strong class="source-inline">doors</strong> – the number of doors the vehicle has</li>
				<li><strong class="source-inline">persons</strong> – the number of persons the vehicle is capable of transporting</li>
				<li><strong class="source-inline">lug_boot</strong> – the cargo capacity of the vehicle</li>
				<li><strong class="source-inline">safety</strong> – the safety rating of the vehicle</li>
				<li><strong class="source-inline">car</strong> – this is the category that the model attempts to predict</li>
			</ul>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li>Open a new Colab notebook.</li>
				<li>Import the required libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p>You started by importing a library called <strong class="source-inline">pandas</strong> in the first line. This library is useful for reading files into a data structure that is called a <strong class="source-inline">DataFrame</strong>, which you have used in previous chapters. This structure is like a spreadsheet or a table with rows and columns that we can manipulate. Because you might need to reference the library lots of times, we have created an alias for it, <strong class="source-inline">pd</strong>.</p><p>In the second line, you import a function called <strong class="source-inline">train_test_split</strong> from a module called <strong class="source-inline">model_selection</strong>, which is within <strong class="source-inline">sklearn</strong>. This function is what you will make use of to split the data that you read in using <strong class="source-inline">pandas</strong>.</p></li>
				<li>Create a Python list:<p class="source-code"># data doesn't have headers, so let's create headers</p><p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p><p>The data that you are reading in is stored as a CSV file. </p><p>The browser will download the file to your computer. You can open the file using a text editor. If you do, you will see something similar to the following:</p><div id="_idContainer264" class="IMG---Figure"><img src="Images/B15019_06_01.jpg" alt="Figure 6.1: The car dataset without headers&#13;&#10;" width="467" height="346"/></div><p class="figure-caption">Figure 6.1: The car dataset without headers</p><p class="callout-heading">Note</p><p class="callout">Alternatively, you can enter the dataset URL in the browser to view the dataset.</p><p><strong class="source-inline">CSV</strong> files normally have the name of each column written in the first row of the data. For instance, have a look at this dataset's CSV file, which you used in <em class="italic">Chapter 3, Binary Classification</em>:</p><div id="_idContainer265" class="IMG---Figure"><img src="Images/B15019_06_02.jpg" alt="Figure 6.2: CSV file without headers&#13;&#10;" width="1292" height="424"/></div><p class="figure-caption">Figure 6.2: CSV file without headers</p><p>But, in this case, the column name is missing. That is not a problem, however. The code in this step creates a Python list called <strong class="source-inline">_headers</strong> that contains the name of each column. You will supply this list when you read in the data in the next step.</p></li>
				<li>Read the data:<p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p><p>In this step, the code reads in the file using a function called <strong class="source-inline">read_csv</strong>. The first parameter, <strong class="source-inline">'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter06/Dataset/car.data'</strong>, is mandatory and is the location of the file. In our case, the file is on the internet. It can also be optionally downloaded, and we can then point to the local file's location.</p><p>The second parameter (<strong class="source-inline">names=_headers</strong>) asks the function to add the row headers to the data after reading it in. The third parameter (<strong class="source-inline">index_col=None</strong>) asks the function to generate a new index for the table because the data doesn't contain an index. The function will produce a DataFrame, which we assign to a variable called <strong class="source-inline">df</strong>.</p></li>
				<li>Print out the top five records:<p class="source-code">df.head()</p><p>The code in this step is used to print the top five rows of the DataFrame. The output from that operation is shown in the following screenshot:</p><div id="_idContainer266" class="IMG---Figure"><img src="Images/B15019_06_03.jpg" alt="Figure 6.3: The top five rows of the DataFrame&#13;&#10;" width="746" height="234"/></div><p class="figure-caption">Figure 6.3: The top five rows of the DataFrame</p></li>
				<li>Create a training and an evaluation DataFrame:<p class="source-code">training, evaluation = train_test_split(df, test_size=0.3, \</p><p class="source-code">                                        random_state=0)</p><p>The preceding code will split the DataFrame containing your data into two new DataFrames. The first is called <strong class="source-inline">training</strong> and is used for training the model. The second is called <strong class="source-inline">evaluation</strong> and will be further split into two in the next step. We mentioned earlier that you must separate your dataset into a training and an evaluation dataset, the former for training your model and the latter for evaluating your model.</p><p>At this point, the <strong class="source-inline">train_test_split</strong> function takes two parameters. The first parameter is the data we want to split. The second is the ratio we would like to split it by. What we have done is specified that we want our evaluation data to be 30% of our data.</p><p class="callout-heading">Note</p><p class="callout">The third parameter random_state is set to 0 to ensure reproducibility of results.</p></li>
				<li>Create a validation and test dataset:<p class="source-code">validation, test = train_test_split(evaluation, test_size=0.5, \</p><p class="source-code">                                    random_state=0)</p><p>This code is similar to the code in <em class="italic">Step 6</em>. In this step, the code splits our evaluation data into two equal parts because we specified <strong class="source-inline">0.5</strong>, which means <strong class="source-inline">50%</strong>. </p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3haKvl3">https://packt.live/3haKvl3</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3g8zI9R">https://packt.live/3g8zI9R</a>.</p><p>In previous chapters, we've seen how to split your data into train and test sets, but here, we'll go one step further and split it into three: one to train a model, one to evaluate the model during training, and one to evaluate the model before putting it into production.</p><p>Now that you have your data split into different sets, you may proceed to train and evaluate models. </p></li>
			</ol>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor137"/>Assessing Model Performance for Regression Models</h1>
			<p>When you create a regression model, you create a model that predicts a continuous numerical variable, as you learned in <em class="italic">Chapter 2, Regression</em>. When you set aside your evaluation dataset, you have something that you can use to compare the quality of your model.</p>
			<p>What you need to do to assess your model quality is compare the quality of your prediction to what is called the ground truth, which is the actual observed value that you are trying to predict. Take a look at <em class="italic">Figure 6.4</em>, in which the first column contains the ground truth (called actuals) and the second column contains the predicted values:</p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="Images/B15019_06_04.jpg" alt="Figure 6.4: Actual versus predicted values&#13;&#10;" width="563" height="197"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4: Actual versus predicted values</p>
			<p>Line <strong class="source-inline">0</strong> in the output compares the actual value in our evaluation dataset to what our model predicted. The actual value from our evaluation dataset is <strong class="source-inline">4.891</strong>. The value that the model predicted is <strong class="source-inline">4.132270</strong>.</p>
			<p>Line <strong class="source-inline">1</strong> compares the actual value of <strong class="source-inline">4.194</strong> to what the model predicted, which is <strong class="source-inline">4.364320</strong>.</p>
			<p>In practice, the evaluation dataset will contain a lot of records, so you will not be making this comparison visually. Instead, you will make use of some equations. </p>
			<p>You would carry out this comparison by computing the loss. The loss is the difference between the actuals and the predicted values in the preceding screenshot. In data mining, it is called a <strong class="bold">distance measure</strong>. There are various approaches to computing distance measures that give rise to different loss functions. Two of these are:</p>
			<ul>
				<li>Manhattan distance</li>
				<li>Euclidean distance</li>
			</ul>
			<p>There are various loss functions for regression, but in this book, we will be looking at two of the commonly used loss functions for regression, which are:</p>
			<ul>
				<li>Mean absolute error (MAE) – this is based on Manhattan distance</li>
				<li>Mean squared error (MSE) – this is based on Euclidean distance</li>
			</ul>
			<p>The goal of these functions is to measure the usefulness of your models by giving you a numerical value that shows how much deviation there is between the ground truths and the predicted values from your models.</p>
			<p>Your mission is to train new models with consistently lower errors. Before we do that, let's have a quick introduction to some data structures.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Data Structures – Vectors and Matrices</h2>
			<p>In this section, we will look at different data structures, as follows.</p>
			<h3 id="_idParaDest-140"><a id="_idTextAnchor139"/>Scalars</h3>
			<p>A scalar variable is a simple number, such as 23. Whenever you make use of numbers on their own, they are scalars. You assign them to variables, such as in the following expression:</p>
			<p class="source-code">temperature = 23</p>
			<p>If you had to store the temperature for 5 days, you would need to store the values in 5 different values, such as in the following code snippet:</p>
			<p class="source-code">temp_1 = 23</p>
			<p class="source-code">temp_2 = 24</p>
			<p class="source-code">temp_3 = 23</p>
			<p class="source-code">temp_4 = 22</p>
			<p class="source-code">temp_5 = 22</p>
			<p>In data science, you will frequently work with a large number of data points, such as hourly temperature measurements for an entire year. A more efficient way of storing lots of values is called a vector. Let's look at vectors in the next topic.</p>
			<h3 id="_idParaDest-141"><a id="_idTextAnchor140"/>Vectors</h3>
			<p>A vector is a collection of scalars. Consider the five temperatures in the previous code snippet. A vector is a data type that lets you collect all of the previous temperatures in one variable that supports arithmetic operations. Vectors look similar to Python lists and can be created from Python lists. Consider the following code snippet for creating a Python list:</p>
			<p class="source-code">temps_list = [23, 24, 23, 22, 22]</p>
			<p>You can create a vector from the list using the <strong class="source-inline">.array()</strong> method from <strong class="source-inline">numpy</strong> by first importing <strong class="source-inline">numpy</strong> and then using the following snippet:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">temps_ndarray = np.array(temps_list)</p>
			<p>You can proceed to verify the data type using the following code snippet:</p>
			<p class="source-code">print(type(temps_ndarray))</p>
			<p>The code snippet will cause the compiler to print out the following:</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="Images/B15019_06_05.jpg" alt="Figure 6.5: The temps_ndarray vector data type&#13;&#10;" width="1002" height="48"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5: The temps_ndarray vector data type</p>
			<p>You may inspect the contents of the vector using the following code snippet:</p>
			<p class="source-code">print(temps_ndarray)</p>
			<p>This generates the following output:</p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="Images/B15019_06_06.jpg" alt="Figure 6.6: The temps_ndarray vector&#13;&#10;" width="959" height="42"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6: The temps_ndarray vector</p>
			<p>Note that the output contains single square brackets, <strong class="source-inline">[</strong> and <strong class="source-inline">]</strong>, and the numbers are separated by spaces. This is different from the output from a Python list, which you can obtain using the following code snippet:</p>
			<p class="source-code">print(temps_list)</p>
			<p>The code snippet yields the following output:</p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="Images/B15019_06_07.jpg" alt="Figure 6.7: List of elements in temps_list&#13;&#10;" width="977" height="42"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7: List of elements in temps_list</p>
			<p>Note that the output contains single square brackets, <strong class="source-inline">[</strong> and <strong class="source-inline">]</strong>, and the numbers are separated by commas.</p>
			<p>Vectors have a shape and a dimension. Both of these can be determined by using the following code snippet:</p>
			<p class="source-code">print(temps_ndarray.shape)</p>
			<p>The output is a Python data structure called a <strong class="bold">tuple</strong> and looks like this:</p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="Images/B15019_06_08.jpg" alt="Figure 6.8: Shape of the temps_ndarray vector&#13;&#10;" width="1665" height="85"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8: Shape of the temps_ndarray vector</p>
			<p>Notice that the output consists of brackets, <strong class="source-inline">(</strong> and <strong class="source-inline">)</strong>, with a number and a comma. The single number followed by a comma implies that this object has only one dimension. The value of the number is the number of elements. The output is read as "a vector with five elements." This is very important because it is very different from a matrix, which we will discuss next.</p>
			<h3 id="_idParaDest-142"><a id="_idTextAnchor141"/>Matrices</h3>
			<p>A matrix is also made up of scalars but is different from a scalar in the sense that a matrix has both rows and columns3</p>
			<p>There are times when you need to convert between vectors and matrices. Let's revisit <strong class="source-inline">temps_ndarray</strong>. You may recall that it has five elements because the shape was <strong class="source-inline">(5,)</strong>. To convert it into a matrix with five rows and one column, you would use the following snippet:</p>
			<p class="source-code">temps_matrix = temps_ndarray.reshape(-1, 1)</p>
			<p>The code snippet makes use of the <strong class="source-inline">.reshape()</strong> method. The first parameter, <strong class="source-inline">-1</strong>, instructs the interpreter to keep the first dimension constant. The second parameter, <strong class="source-inline">1</strong>, instructs the interpreter to add a new dimension. This new dimension is the column. To see the new shape, use the following snippet:</p>
			<p class="source-code">print(temps_matrix.shape)</p>
			<p>You will get the following output:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="Images/B15019_06_09.jpg" alt="Figure 6.9: Shape of the matrix&#13;&#10;" width="1665" height="75"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9: Shape of the matrix</p>
			<p>Notice that the tuple now has two numbers, <strong class="source-inline">5</strong> and <strong class="source-inline">1</strong>. The first number, <strong class="source-inline">5</strong>, represents the rows, and the second number, <strong class="source-inline">1</strong>, represents the columns. You can print out the value of the matrix using the following snippet:</p>
			<p class="source-code">print(temps_matrix)</p>
			<p>The output of the code is as follows:</p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="Images/B15019_06_10.jpg" alt="Figure 6.10: Elements of the matrix&#13;&#10;" width="1662" height="356"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10: Elements of the matrix</p>
			<p>Notice that the output is different from that of the vector. First, we have an outer set of square brackets. Then, each row has its element enclosed in square brackets. Each row contains only one number because the matrix has only one column.</p>
			<p>You may reshape the matrix to contain <strong class="source-inline">1</strong> row and <strong class="source-inline">5</strong> columns and print out the value using the following code snippet:</p>
			<p class="source-code">print(temps_matrix.reshape(1,5))</p>
			<p>The output will be as follows:</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="Images/B15019_06_11.jpg" alt="Figure 6.11: Reshaping the matrix&#13;&#10;" width="794" height="46"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11: Reshaping the matrix</p>
			<p>Notice that you now have all the numbers on one row because this matrix has one row and five columns. The outer square brackets represent the matrix, while the inner square brackets represent the row.</p>
			<p>Finally, you can convert the matrix back into a vector by dropping the column using the following snippet:</p>
			<p class="source-code">vector = temps_matrix.reshape(-1)</p>
			<p>You can print out the value of the vector to confirm that you get the following:</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="Images/B15019_06_12.jpg" alt="Figure 6.12: The value of the vector&#13;&#10;" width="907" height="48"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12: The value of the vector</p>
			<p>Notice that you now have only one set of square brackets. You still have the same number of elements.</p>
			<p>Let's now look at an important metric – R<span class="superscript">2</span> score.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>R<span class="superscript">2</span> Score</h2>
			<p>The R<span class="superscript">2</span> score (pronounced "r squared") is sometimes called the "score" and measures the coefficient of determination of the model. Think of it as the model's ability to make good and reliable predictions. This measure is accessed using the <strong class="source-inline">score()</strong> method of the model and is available for every model.</p>
			<p>Your goal is to train successive models with a higher R<span class="superscript">2</span> score. The R<span class="superscript">2</span> score has a range between <strong class="bold">0</strong> and <strong class="bold">1</strong>. Your goal is to try and get the model to have a score that is close to <strong class="bold">1</strong>.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Exercise 6.02: Computing the R<span class="superscript">2</span> Score of a Linear Regression Model</h2>
			<p>As mentioned in the preceding sections, R<span class="superscript">2</span> score is an important factor in evaluating the performance of a model. Thus, in this exercise, we will be creating a linear regression model and then calculating the R<span class="superscript">2</span> score for it.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The fish toxicity dataset that you will be using in this chapter can be found in our GitHub repository: <a href="https://packt.live/2sNChvv">https://packt.live/2sNChvv</a>.</p>
			<p class="callout">This dataset was taken from the UCI Machine Learning Repository: <a href="https://packt.live/2TSyJTB">https://packt.live/2TSyJTB</a>.</p>
			<p>The following attributes are useful for our task:</p>
			<ul>
				<li>CIC0: information indices</li>
				<li>SM1_Dz(Z): 2D matrix-based descriptors</li>
				<li>GATS1i: 2D autocorrelations</li>
				<li>NdsCH: Pimephales promelas</li>
				<li>NdssC: atom-type counts</li>
				<li>MLOGP: molecular properties</li>
				<li>Quantitative response, LC50 [-LOG(mol/L)]: This attribute represents the concentration that causes death in 50% of test fish over a test duration of 96 hours.</li>
			</ul>
			<p>The following steps will help you to complete the exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook to write and execute your code.</li>
				<li>Next, import the libraries mentioned in the following code snippet:<p class="source-code"># import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p>In this step, you import <strong class="source-inline">pandas</strong>, which you will use to read your data. You also import <strong class="source-inline">train_test_split()</strong>, which you will use to split your data into training and validation sets, and you import <strong class="source-inline">LinearRegression</strong>, which you will use to train your model.</p></li>
				<li>Now, read the data from the dataset:<p class="source-code"># column headers</p><p class="source-code">_headers = ['CIC0', 'SM1', 'GATS1i', 'NdsCH', 'Ndssc', \</p><p class="source-code">            'MLOGP', 'response']</p><p class="source-code"># read in data</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/'\</p><p class="source-code">                 'qsar_fish_toxicity.csv', \</p><p class="source-code">                 names=_headers, sep=';')</p><p>In this step, you create a Python list to hold the names of the columns in your data. You do this because the CSV file containing the data does not have a first row that contains the column headers. You proceed to read in the file and store it in a variable called <strong class="source-inline">df</strong> using the <strong class="source-inline">read_csv()</strong> method in pandas. You specify the list containing column headers by passing it into the <strong class="source-inline">names</strong> parameter. This CSV uses semi-colons as column separators, so you specify that using the <strong class="source-inline">sep</strong> parameter. You can use <strong class="source-inline">df.head()</strong> to see what the DataFrame looks like:</p><div id="_idContainer276" class="IMG---Figure"><img src="Images/B15019_06_13.jpg" alt="Figure 6.13: The first five rows of the DataFrame&#13;&#10;" width="657" height="204"/></div><p class="figure-caption">Figure 6.13: The first five rows of the DataFrame</p></li>
				<li>Split the data into features and labels and into training and evaluation datasets:<p class="source-code"># Let's split our data</p><p class="source-code">features = df.drop('response', axis=1).values</p><p class="source-code">labels = df[['response']].values</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                random_state=0)</p><p>In this step, you create two <strong class="source-inline">numpy</strong> arrays called <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>. You then proceed to split them twice. The first split produces a <strong class="source-inline">training</strong> set and an <strong class="source-inline">evaluation</strong> set. The second split creates a <strong class="source-inline">validation</strong> set and a <strong class="source-inline">test</strong> set.</p></li>
				<li>Create a linear regression model:<p class="source-code">model = LinearRegression()</p><p>In this step, you create an instance of <strong class="source-inline">LinearRegression</strong> and store it in a variable called <strong class="source-inline">model</strong>. You will make use of this to train on the training dataset.</p></li>
				<li>Train the model:<p class="source-code">model.fit(X_train, y_train)</p><p>In this step, you train the model using the <strong class="source-inline">fit()</strong> method and the training dataset that you made in <em class="italic">Step 4</em>. The first parameter is the <strong class="source-inline">features</strong> NumPy array, and the second parameter is <strong class="source-inline">labels</strong>.</p><p>You should get an output similar to the following:</p><div id="_idContainer277" class="IMG---Figure"><img src="Images/B15019_06_14.jpg" alt="Figure 6.14: Training the model&#13;&#10;" width="772" height="32"/></div><p class="figure-caption">Figure 6.14: Training the model</p></li>
				<li>Make a prediction, as shown in the following code snippet:<p class="source-code">y_pred = model.predict(X_val)</p><p>In this step, you make use of the validation dataset to make a prediction. This is stored in <strong class="source-inline">y_pred</strong>.</p></li>
				<li>Compute the R<span class="superscript">2</span> score:<p class="source-code">r2 = model.score(X_val, y_val)</p><p class="source-code">print('R^2 score: {}'.format(r2))</p><p>In this step, you compute <strong class="source-inline">r2</strong>, which is the R<span class="superscript">2</span> score of the model. The R<span class="superscript">2</span> score is computed using the <strong class="source-inline">score()</strong> method of the model. The next line causes the interpreter to print out the R<span class="superscript">2</span> score. </p><p>The output is similar to the following:</p><div id="_idContainer278" class="IMG---Figure"><img src="Images/B15019_06_15.jpg" alt="Figure 6.15: R2 score&#13;&#10;" width="1077" height="38"/></div><p class="figure-caption">Figure 6.15: R2 score</p><p class="callout-heading">Note</p><p class="callout">The MAE and R<span class="superscript">2</span> score may vary depending on the distribution of the datasets.</p></li>
				<li>You see that the R<span class="superscript">2</span> score we achieved is <strong class="source-inline">0.56238</strong>, which is not close to 1. In the next step, we will be making comparisons.</li>
				<li>Compare the predictions to the actual ground truth:<p class="source-code">_ys = pd.DataFrame(dict(actuals=y_val.reshape(-1), \</p><p class="source-code">                        predicted=y_pred.reshape(-1)))</p><p class="source-code">_ys.head()</p><p>In this step, you take a cursory look at the predictions compared to the ground truth. In <em class="italic">Step 8</em>, you will have noticed that the R<span class="superscript">2</span> score you computed for the model is far from perfect (perfect is a score of 1). In this step, in the first line, you create a DataFrame by making use of the <strong class="source-inline">DataFrame</strong> method in pandas. You provide a dictionary as an argument. The dictionary has two keys: <strong class="source-inline">actuals</strong> and <strong class="source-inline">predicted</strong>. <strong class="source-inline">actuals</strong> contains <strong class="source-inline">y_vals</strong>, which is the actual labels in the validation dataset. <strong class="source-inline">predicted</strong> contains <strong class="source-inline">y_pred</strong>, which contains the predictions. Both <strong class="source-inline">y_vals</strong> and <strong class="source-inline">y_pred</strong> are two-dimensional matrices, so you reshape them to 1D vectors by using <strong class="source-inline">.reshape(-1)</strong>, which drops the second axis.</p><p>The second line causes the interpreter to display the top five records. </p><p>The output looks similar to the following:</p><div id="_idContainer279" class="IMG---Figure"><img src="Images/B15019_06_16.jpg" alt="Figure 6.16: The actual versus predicted values of the model&#13;&#10;" width="652" height="231"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.16: The actual versus predicted values of the model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31aw6QE">https://packt.live/31aw6QE</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aASLbE">https://packt.live/3aASLbE</a>.</p>
			<p>In this exercise, we computed the R<span class="superscript">2</span> score, which is an evaluation metric that can be used for comparing models.</p>
			<p>In the next topic, we will be looking at the mean absolute error, which is another evaluation metric. </p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Mean Absolute Error</h2>
			<p>The <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>) is an evaluation metric for regression models that measures the absolute distance between your predictions and the ground truth. The absolute distance is the distance regardless of the sign, whether positive or negative. For example, if the ground truth is 6 and you predict 5, the distance is 1. However, if you predict 7, the distance becomes -1. The absolute distance, without taking the signs into consideration, is 1 in both cases. This is called the <strong class="bold">magnitude</strong>. The MAE is computed by summing all of the magnitudes and dividing by the number of observations.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Exercise 6.03: Computing the MAE of a Model</h2>
			<p>The goal of this exercise is to find the score and loss of a model using the same dataset as <em class="italic">Exercise 6.02</em>, <em class="italic">Computing the R2 Score of a Linear Regression Model</em>.</p>
			<p>In this exercise, we will be calculating the MAE of a model.</p>
			<p>The following steps will help you with this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import the necessary libraries:<p class="source-code"># Import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.metrics import mean_absolute_error</p><p>In this step, you import the function called <strong class="source-inline">mean_absolute_error</strong> from <strong class="source-inline">sklearn.metrics</strong>.</p></li>
				<li>Import the data:<p class="source-code"># column headers</p><p class="source-code">_headers = ['CIC0', 'SM1', 'GATS1i', 'NdsCH', 'Ndssc', \</p><p class="source-code">            'MLOGP', 'response']</p><p class="source-code"># read in data</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/'\</p><p class="source-code">                 'qsar_fish_toxicity.csv', \</p><p class="source-code">                 names=_headers, sep=';')</p><p>In the preceding code, you read in your data. This data is hosted online and contains some information about fish toxicity. The data is stored as a CSV but does not contain any headers. Also, the columns in this file are not separated by a comma, but rather by a semi-colon. The Python list called <strong class="source-inline">_headers</strong> contains the names of the column headers.</p><p>In the next line, you make use of the function called <strong class="source-inline">read_csv</strong>, which is contained in the <strong class="source-inline">pandas</strong> library, to load the data. The first parameter specifies the file location. The second parameter specifies the Python list that contains the names of the columns in the data. The third parameter specifies the character that is used to separate the columns in the data.</p></li>
				<li>Split the data into <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong> and into training and evaluation sets:<p class="source-code"># Let's split our data</p><p class="source-code">features = df.drop('response', axis=1).values</p><p class="source-code">labels = df[['response']].values</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                random_state=0)</p><p>In this step, you split your data into training, validation, and test datasets. In the first line, you create a <strong class="source-inline">numpy</strong> array in two steps. In the first step, the <strong class="source-inline">drop</strong> method takes a parameter with the name of the column to drop from the DataFrame. In the second step, you use <strong class="source-inline">values</strong> to convert the DataFrame into a two-dimensional <strong class="source-inline">numpy</strong> array that is a tabular structure with rows and columns. This array is stored in a variable called <strong class="source-inline">features</strong>.</p><p>In the second line, you convert the column into a <strong class="source-inline">numpy</strong> array that contains the label that you would like to predict. You do this by picking out the column from the DataFrame and then using <strong class="source-inline">values</strong> to convert it into a <strong class="source-inline">numpy</strong> array.</p><p>In the third line, you split the <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong> using <strong class="source-inline">train_test_split</strong> and a ratio of 80:20. The training data is contained in <strong class="source-inline">X_train</strong> for the features and <strong class="source-inline">y_train</strong> for the labels. The evaluation dataset is contained in <strong class="source-inline">X_eval</strong> and <strong class="source-inline">y_eval</strong>.</p><p>In the fourth line, you split the evaluation dataset into validation and testing using <strong class="source-inline">train_test_split</strong>. Because you don't specify the <strong class="source-inline">test_size</strong>, a value of <strong class="source-inline">25%</strong> is used. The validation data is stored in <strong class="source-inline">X_val </strong>and <strong class="source-inline">y_val</strong>, while the test data is stored in <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong>.</p></li>
				<li>Create a simple linear regression model and train it:<p class="source-code"># create a simple Linear Regression model</p><p class="source-code">model = LinearRegression()</p><p class="source-code"># train the model</p><p class="source-code">model.fit(X_train, y_train)</p><p>In this step, you make use of your training data to train a model. In the first line, you create an instance of <strong class="source-inline">LinearRegression</strong>, which you call <strong class="source-inline">model</strong>. In the second line, you train the model using <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong>. <strong class="source-inline">X_train</strong> contains the <strong class="source-inline">features</strong>, while <strong class="source-inline">y_train</strong> contains the <strong class="source-inline">labels</strong>.</p></li>
				<li>Now predict the values of our validation dataset:<p class="source-code"># let's use our model to predict on our validation dataset</p><p class="source-code">y_pred = model.predict(X_val)</p><p>At this point, your model is ready to use. You make use of the <strong class="source-inline">predict</strong> method to predict on your data. In this case, you are passing <strong class="source-inline">X_val</strong> as a parameter to the function. Recall that <strong class="source-inline">X_va</strong>l is your validation dataset. The result is assigned to a variable called <strong class="source-inline">y_pred</strong> and will be used in the next step to compute the MAE of the model.</p></li>
				<li>Compute the MAE:<p class="source-code"># Let's compute our MEAN ABSOLUTE ERROR</p><p class="source-code">mae = mean_absolute_error(y_val, y_pred)</p><p class="source-code">print('MAE: {}'.format(mae))</p><p>In this step, you compute the MAE of the model by using the <strong class="source-inline">mean_absolute_error</strong> function and passing in <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong>. <strong class="source-inline">y_val</strong> is the label that was provided with your training data, and <strong class="source-inline">y_pred </strong>is the prediction from the model. The preceding code should give you an MAE value of ~ 0.72434:</p><div id="_idContainer280" class="IMG---Figure"><img src="Images/B15019_06_17.jpg" alt="Figure 6.17 MAE score&#13;&#10;" width="580" height="33"/></div><p class="figure-caption">Figure 6.17 MAE score</p><p>Both <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong> are a <strong class="source-inline">numpy</strong> array that contains the same number of elements. The <strong class="source-inline">mean_absolute_error</strong> function subtracts <strong class="source-inline">y_pred</strong> from <strong class="source-inline">y_val</strong>. This results in a new array. The elements in the resulting array have the absolute function applied to them so that all negative signs are dropped. The average of the elements is then computed.</p></li>
				<li>Compute the R<span class="superscript">2</span> score of the model:<p class="source-code"># Let's get the R2 score</p><p class="source-code">r2 = model.score(X_val, y_val)</p><p class="source-code">print('R^2 score: {}'.format(r2))</p><p>You should get an output similar to the following:</p><div id="_idContainer281" class="IMG---Figure"><img src="Images/B15019_06_18.jpg" alt="Figure 6.18: The R2 score of the model&#13;&#10;" width="670" height="44"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.18: The R2 score of the model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The MAE and R<span class="superscript">2</span> score may vary depending on the distribution of the datasets.</p>
			<p>A higher R<span class="superscript">2</span> score means a better model and uses an equation that computes the coefficient of determination.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer  to <a href="https://packt.live/349mG9P">https://packt.live/349mG9P</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aA1rza">https://packt.live/3aA1rza</a>.</p>
			<p>In this exercise, we have calculated the MAE, which is a significant parameter when it comes to evaluating models.</p>
			<p>You will now train a second model and compare its R<span class="superscript">2</span> score and MAE to the first model to evaluate which is a better performing model.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/>Exercise 6.04: Computing the Mean Absolute Error of a Second Model</h2>
			<p>In this exercise, we will be engineering new features and finding the score and loss of a new model.</p>
			<p>The following steps will help you with this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import the required libraries:<p class="source-code"># Import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.metrics import mean_absolute_error</p><p class="source-code"># pipeline</p><p class="source-code">from sklearn.pipeline import Pipeline</p><p class="source-code"># preprocessing</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.preprocessing import PolynomialFeatures</p><p>In the first step, you will import libraries such as <strong class="source-inline">train_test_split</strong>, <strong class="source-inline">LinearRegression</strong>, and <strong class="source-inline">mean_absolute_error</strong>. We make use of a pipeline to quickly transform our features and engineer new features using <strong class="source-inline">MinMaxScaler</strong> and <strong class="source-inline">PolynomialFeatures</strong>. <strong class="source-inline">MinMaxScaler</strong> reduces the variance in your data by adjusting all values to a range between 0 and 1. It does this by subtracting the mean of the data and dividing by the range, which is the minimum value subtracted from the maximum value. <strong class="source-inline">PolynomialFeatures</strong> will engineer new features by raising the values in a column up to a certain power and creating new columns in your DataFrame to accommodate them.</p></li>
				<li>Read in the data from the dataset:<p class="source-code"># column headers</p><p class="source-code">_headers = ['CIC0', 'SM1', 'GATS1i', 'NdsCH', 'Ndssc', \</p><p class="source-code">            'MLOGP', 'response']</p><p class="source-code"># read in data</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/'\</p><p class="source-code">                 'qsar_fish_toxicity.csv', \</p><p class="source-code">                 names=_headers, sep=';')</p><p>In this step, you will read in your data. While the data is stored in a CSV, it doesn't have a first row that lists the names of the columns. The Python list called <strong class="source-inline">_headers</strong> will hold the column names that you will supply to the <strong class="source-inline">pandas</strong> method called <strong class="source-inline">read_csv</strong>.</p><p>In the next line, you call the <strong class="source-inline">read_csv</strong> <strong class="source-inline">pandas</strong> method and supply the location and name of the file to be read in, along with the header names and the file separator. Columns in the file are separated with a semi-colon.</p></li>
				<li>Split the data into training and evaluation sets:<p class="source-code"># Let's split our data</p><p class="source-code">features = df.drop('response', axis=1).values</p><p class="source-code">labels = df[['response']].values</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                random_state=0)</p><p>In this step, you begin by splitting the DataFrame called <strong class="source-inline">df</strong> into two. The first DataFrame is called <strong class="source-inline">features</strong> and contains all of the independent variables that you will use to make your predictions. The second is called <strong class="source-inline">labels</strong> and contains the values that you are trying to predict.</p><p>In the third line, you split <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong> into four sets using <strong class="source-inline">train_test_split</strong>. <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> contain 80% of the data and are used for training your model. <strong class="source-inline">X_eval</strong> and <strong class="source-inline">y_eval</strong> contain the remaining 20%.</p><p>In the fourth line, you split <strong class="source-inline">X_eval</strong> and <strong class="source-inline">y_eval</strong> into two additional sets. <strong class="source-inline">X_val</strong> and <strong class="source-inline">y_val</strong> contain 75% of the data because you did not specify a ratio or size. <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong> contain the remaining 25%.</p></li>
				<li>Create a pipeline:<p class="source-code"># create a pipeline and engineer quadratic features</p><p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(2)),\</p><p class="source-code">         ('model', LinearRegression())]</p><p>In this step, you begin by creating a Python list called <strong class="source-inline">steps</strong>. The list contains three tuples, each one representing a transformation of a model. The first tuple represents a scaling operation. The first item in the tuple is the name of the step, which you call <strong class="source-inline">scaler</strong>. This uses <strong class="source-inline">MinMaxScaler</strong> to transform the data. The second, called <strong class="source-inline">poly</strong>, creates additional features by crossing the columns of data up to the degree that you specify. In this case, you specify <strong class="source-inline">2</strong>, so it crosses these columns up to a power of 2. Next comes your <strong class="source-inline">LinearRegression</strong> model.</p></li>
				<li>Create a pipeline:<p class="source-code"># create a simple Linear Regression model with a pipeline</p><p class="source-code">model = Pipeline(steps)</p><p>In this step, you create an instance of <strong class="source-inline">Pipeline</strong> and store it in a variable called <strong class="source-inline">model</strong>. <strong class="source-inline">Pipeline</strong> performs a series of transformations, which are specified in the steps you defined in the previous step. This operation works because the transformers (<strong class="source-inline">MinMaxScaler</strong> and <strong class="source-inline">PolynomialFeatures</strong>) implement two methods called <strong class="source-inline">fit()</strong> and <strong class="source-inline">fit_transform()</strong>. You may recall from previous examples that models are trained using the <strong class="source-inline">fit()</strong> method that <strong class="source-inline">LinearRegression</strong> implements.</p></li>
				<li>Train the model:<p class="source-code"># train the model</p><p class="source-code">model.fit(X_train, y_train)</p><p>On the next line, you call the <strong class="source-inline">fit</strong> method and provide <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> as parameters. Because the model is a pipeline, three operations will happen. First, <strong class="source-inline">X_train</strong> will be scaled. Next, additional features will be engineered. Finally, training will happen using the <strong class="source-inline">LinearRegression</strong> model. The output from this step is similar to the following:</p><div id="_idContainer282" class="IMG---Figure"><img src="Images/B15019_06_19.jpg" alt="Figure 6.19: Training the model&#13;&#10;" width="1225" height="312"/></div><p class="figure-caption">Figure 6.19: Training the model</p></li>
				<li>Predict using the validation dataset:<p class="source-code"># let's use our model to predict on our validation dataset</p><p class="source-code">y_pred = model.predict(X_val)</p></li>
				<li>Compute the MAE of the model:<p class="source-code"># Let's compute our MEAN ABSOLUTE ERROR</p><p class="source-code">mae = mean_absolute_error(y_val, y_pred)</p><p class="source-code">print('MAE: {}'.format(mae))</p><p>In the first line, you make use of <strong class="source-inline">mean_absolute_error</strong> to compute the mean absolute error. You supply <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong>, and the result is stored in the <strong class="source-inline">mae</strong> variable. In the following line, you print out <strong class="source-inline">mae</strong>:</p><div id="_idContainer283" class="IMG---Figure"><img src="Images/B15019_06_20.jpg" alt="Figure 6.20: MAE score&#13;&#10;" width="844" height="34"/></div><p class="figure-caption">Figure 6.20: MAE score</p><p>The loss that you compute at this step is called a validation loss because you make use of the validation dataset. This is different from a training loss that is computed using the training dataset. This distinction is important to note as you study other documentation or books, which might refer to both.</p></li>
				<li>Compute the R<span class="superscript">2</span> score:<p class="source-code"># Let's get the R2 score</p><p class="source-code">r2 = model.score(X_val, y_val)</p><p class="source-code">print('R^2 score: {}'.format(r2))</p><p>In the final two lines, you compute the R<span class="superscript">2</span> score and also display it, as shown in the following screenshot:</p><div id="_idContainer284" class="IMG---Figure"><img src="Images/B15019_06_21.jpg" alt="Figure 6.21: R2 score&#13;&#10;" width="931" height="39"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.21: R2 score</p>
			<p>At this point, you should see a difference between the <strong class="source-inline">R</strong><span class="superscript">2</span> score and the MAE of the first model and the second model (in the first model, the <strong class="source-inline">MAE</strong> and <strong class="source-inline">R</strong><span class="superscript">2</span> scores were <strong class="source-inline">0.781629</strong> and <strong class="source-inline">0.498688</strong> respectively).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2EjCaNn">https://packt.live/2EjCaNn</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Yb5vRd">https://packt.live/2Yb5vRd</a>.</p>
			<p>In this exercise, you engineered new features that give you a model with a hypothesis of a higher polynomial degree. This model should perform better than simpler models up to a certain point. After engineering and training the new model, you computed the R<span class="superscript">2</span> score and MAE, which you can use to compare this model with the model you trained previously. We can conclude that this model is better as it has a higher R<span class="superscript">2</span> score and a lower MAE.</p>
			<h3 id="_idParaDest-148"><a id="_idTextAnchor147"/>Other Evaluation Metrics</h3>
			<p>While we made use of <strong class="source-inline">mean_absolute_error</strong>, there are other model evaluation functions for regression. Recall that these are all cost (or loss) functions. These include <strong class="source-inline">max_error</strong>, <strong class="source-inline">mean_squared_error</strong>, <strong class="source-inline">mean_squared_log_error</strong>, and <strong class="source-inline">median_absolute_error</strong>. If you are working on a project with a data scientist, they will normally be responsible for telling you what evaluation metric to make use of. If not, then you can choose any metric of your liking.</p>
			<p>The MAE is computed by subtracting every prediction from the ground truth, finding the absolute value, summing all the absolute values, and dividing by the number of observations. This type of distance measure is called Manhattan distance in data mining.</p>
			<p>The <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) is computed by taking the squares of the differences between the ground truths and the predictions, summing them, and then dividing by the number of observations. The MSE is large, and sometimes the square root of this is used, which is the <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>).</p>
			<p>The <strong class="bold">mean squared logarithmic error</strong> (<strong class="bold">MSLE</strong>) introduces logarithms into the equation by adding one to both the ground truth and the prediction before taking the logarithms, then squaring the differences, then summing them, and dividing by the number of observations. MSLE has the property of having a lower cost for predictions that are above the ground truth than for those that are below it.</p>
			<p>Finally, <strong class="source-inline">median_absolute_error</strong> finds the median value of the absolute errors, which are the differences between the ground truths and the predictions.</p>
			<p>Let's now move toward evaluating the performance of classification models.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>Assessing Model Performance for Classification Models</h1>
			<p>Classification models are used for predicting which class a group of features will fall under. You learned to create binary classification models in <em class="italic">Chapter 3</em>, <em class="italic">Binary Classification</em>, and multi-class classification models in <em class="italic">Chapter 4, Multiclass Classification with RandomForest</em>.</p>
			<p>When you consider a classification model, you might start to ask yourself how accurate the model is. But how do you evaluate accuracy?</p>
			<p>You need to create a classification model before you can start assessing it.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Exercise 6.05: Creating a Classification Model for Computing Evaluation Metrics</h2>
			<p>In this exercise, you will create a classification model that you will make use of later on for model assessment.</p>
			<p>You will make use of the cars dataset from the UCI Machine Learning Repository. You will use this dataset to classify cars as either acceptable or unacceptable based on the following categorical features:</p>
			<ul>
				<li><strong class="source-inline">buying</strong>: the purchase price of the car</li>
				<li><strong class="source-inline">maint</strong>: the maintenance cost of the car</li>
				<li><strong class="source-inline">doors</strong>: the number of doors on the car</li>
				<li><strong class="source-inline">persons</strong>: the carrying capacity of the vehicle</li>
				<li><strong class="source-inline">lug_boot</strong>: the size of the luggage boot</li>
				<li><strong class="source-inline">safety</strong>: the estimated safety of the car<p class="callout-heading">Note</p><p class="callout">You can find the dataset here: <a href="https://packt.live/30I594E">https://packt.live/30I594E</a>.</p></li>
			</ul>
			<p>The following steps will help you achieve the task:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the libraries you will need:<p class="source-code"># import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p>In this step, you import <strong class="source-inline">pandas</strong> and alias it as <strong class="source-inline">pd</strong>. <strong class="source-inline">pandas</strong> is needed for reading data into a DataFrame. You also import <strong class="source-inline">train_test_split</strong>, which is needed for splitting your data into training and evaluation datasets. Finally, you also import the <strong class="source-inline">LogisticRegression</strong> class.</p></li>
				<li>Import your data:<p class="source-code"># data doesn't have headers, so let's create headers</p><p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p><p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p><p class="source-code">df.head()</p><p>In this step, you create a Python list called <strong class="source-inline">_headers</strong> to hold the names of the columns in the file you will be importing because the file doesn't have a header. You  then proceed to read the file into a DataFrame named <strong class="source-inline">df</strong> by using <strong class="source-inline">pd.read_csv</strong> and specifying the file location as well as the list containing the file headers. Finally, you display the first five rows using <strong class="source-inline">df.head()</strong>. </p><p>You should get an output similar to the following:</p><div id="_idContainer285" class="IMG---Figure"><img src="Images/B15019_06_22.jpg" alt="Figure 6.22: Inspecting the DataFrame&#13;&#10;" width="694" height="229"/></div><p class="figure-caption">Figure 6.22: Inspecting the DataFrame</p></li>
				<li>Encode categorical variables as shown in the following code snippet:<p class="source-code"># encode categorical variables</p><p class="source-code">_df = pd.get_dummies(df, columns=['buying', 'maint', 'doors',\</p><p class="source-code">                                  'persons', 'lug_boot', \</p><p class="source-code">                                  'safety'])</p><p class="source-code">_df.head()</p><p>In this step, you convert categorical columns into numeric columns using a technique called one-hot encoding. You saw an example of this in <em class="italic">Step 13</em> of <em class="italic">Exercise 3.04</em>, <em class="italic">Feature Engineering – Creating New Features from Existing Ones</em>. You need to do this because the inputs to your model must be numeric. You get numeric variables from categorical variables using <strong class="source-inline">get_dummies</strong> from the <strong class="source-inline">pandas</strong> library. You provide your DataFrame as input and specify the columns to be encoded. You assign the result to a new DataFrame called <strong class="source-inline">_df</strong>, and then inspect the result using <strong class="source-inline">head()</strong>.</p><p>The output should now resemble the following screenshot:</p><div id="_idContainer286" class="IMG---Figure"><img src="Images/B15019_06_23.jpg" alt="Figure 6.23: Encoding categorical variables&#13;&#10;" width="1133" height="250"/></div><p class="figure-caption">Figure 6.23: Encoding categorical variables</p><p class="callout-heading">Note</p><p class="callout">The output has been truncated for presentation purposes. Please find the complete output at <a href="https://packt.live/3aBNlg7">https://packt.live/3aBNlg7</a>.</p></li>
				<li>Split the data into training and validation sets:<p class="source-code"># split data into training and evaluation datasets</p><p class="source-code">features = _df.drop('car', axis=1).values</p><p class="source-code">labels = _df['car'].values</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.3, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                test_size=0.5, \</p><p class="source-code">                                                random_state=0)</p><p>In this step, you begin by extracting your feature columns and your labels into two NumPy arrays called <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>. You then proceed to extract 70% into <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong>, with the remaining 30% going into <strong class="source-inline">X_eval</strong> and <strong class="source-inline">y_eval</strong>. You then further split <strong class="source-inline">X_eval</strong> and <strong class="source-inline">y_eval</strong> into two equal parts and assign those to <strong class="source-inline">X_val</strong> and <strong class="source-inline">y_val</strong> for validation, and <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong> for testing much later.</p></li>
				<li>Train a logistic regression model:<p class="source-code"># train a Logistic Regression model</p><p class="source-code">model = LogisticRegression()</p><p class="source-code">model.fit(X_train, y_train)</p><p>In this step, you create an instance of <strong class="source-inline">LogisticRegression</strong> and train the model on your training data by passing in <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> to the <strong class="source-inline">fit</strong> method. </p><p>You should get an output that looks similar to the following:</p><div id="_idContainer287" class="IMG---Figure"><img src="Images/B15019_06_24.jpg" alt="Figure 6.24: Training a logistic regression model&#13;&#10;" width="1473" height="221"/></div><p class="figure-caption">Figure 6.24: Training a logistic regression model</p></li>
				<li>Make a prediction:<p class="source-code"># make predictions for the validation set</p><p class="source-code">y_pred = model.predict(X_val)</p><p>In this step, you make a prediction on the validation dataset, <strong class="source-inline">X_val</strong>, and store the result in <strong class="source-inline">y_pred</strong>. A look at the first 10 predictions (by executing <strong class="source-inline">y_pred[0:9]</strong>) should provide an output similar to the following:</p><div id="_idContainer288" class="IMG---Figure"><img src="Images/B15019_06_25.jpg" alt="Figure 6.25: Prediction for the validation set&#13;&#10;" width="722" height="56"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.25: Prediction for the validation set</p>
			<p>This model works because you are able to use it to make predictions. The predictions classify each car as acceptable (<strong class="source-inline">acc</strong>) or unacceptable (<strong class="source-inline">unacc</strong>) based on the features of the car. At this point, you are ready to apply various assessments to the model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3aBNlg7">https://packt.live/3aBNlg7</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/34eg7CH">https://packt.live/34eg7CH</a>.</p>
			<p>Thus, we have successfully created a classification model to make predictions, and we will assess the performance of the model in future exercises.</p>
			<p>In this exercise, we trained this logistic regression model once so that we don't need to do it repeatedly because of the number of steps involved. In the next section, you will be looking at confusion matrices.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>The Confusion Matrix</h1>
			<p>You encountered the confusion matrix in <em class="italic">Chapter 3, Binary Classification</em>. You may recall that the confusion matrix compares the number of classes that the model predicted against the actual occurrences of those classes in the validation dataset. The output is a square matrix that has the number of rows and columns equal to the number of classes you are predicting. The columns represent the actual values, while the rows represent the predictions. You get a confusion matrix by using <strong class="source-inline">confusion_matrix</strong> from <strong class="source-inline">sklearn.metrics</strong>.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor151"/>Exercise 6.06: Generating a Confusion Matrix for the Classification Model</h2>
			<p>The goal of this exercise is to create a confusion matrix for the classification model you trained in <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you achieve the task:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import <strong class="source-inline">confusion_matrix</strong>:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p>In this step, you import <strong class="source-inline">confusion_matrix</strong> from <strong class="source-inline">sklearn.metrics</strong>. This function will let you generate a confusion matrix.</p></li>
				<li>Generate a confusion matrix:<p class="source-code">confusion_matrix(y_val, y_pred)</p><p>In this step, you generate a confusion matrix by supplying <strong class="source-inline">y_val</strong>, the actual classes, and <strong class="source-inline">y_pred</strong>, the predicted classes. </p><p>The output should look similar to the following:</p><div id="_idContainer289" class="IMG---Figure"><img src="Images/B15019_06_26.jpg" alt="Figure 6.26: Confusion matrix&#13;&#10;" width="588" height="104"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.26: Confusion matrix</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hbreQz">https://packt.live/3hbreQz</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2EebSMD">https://packt.live/2EebSMD</a>.</p>
			<p>We can see that our data has four classes. The first column shows all of the data that should belong to the first class. The first row shows the number of predictions that were correctly placed in the first class. In this example, that number is <strong class="source-inline">41</strong>. The second row shows the number of predictions that were placed in the second class but should have been in the first class. In this example, that number is <strong class="source-inline">7</strong>. In the third row, you see the number of items that were predicted to be in the third class but should have been in the first class. That number is <strong class="source-inline">7</strong>. Finally, in the fourth row, you see the number of items that were wrongly classified into the fourth class when they should have been in the first class. In this case, the number is <strong class="source-inline">1</strong>.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor152"/>More on the Confusion Matrix </h2>
			<p>The confusion matrix helps you analyze the impact of the choices you would have to make if you put the model into production. Let's consider the example of predicting the presence of a disease based on the inputs to the model. This is a binary classification problem, where 1 implies that the disease is present and 0 implies the disease is absent. The confusion matrix for this model would have two columns and two rows. </p>
			<p>The first column would show the items that fall into class <strong class="bold">0</strong>. The first row would show the items that were correctly classified into class <strong class="bold">0</strong> and are called <strong class="source-inline">true negatives</strong>. The second row would show the items that were wrongly classified as <strong class="bold">1</strong> but should have been <strong class="bold">0</strong>. These are <strong class="source-inline">false positives</strong>.</p>
			<p>The second column would show the items that fall into class <strong class="bold">1</strong>. The first row would show the items that were wrongly classified into class 0 when they should have been <strong class="bold">1</strong> and are called<strong class="source-inline"> false negatives</strong>. Finally, the second row shows items that were correctly classified into class 1 and are called <strong class="source-inline">true positives</strong>.</p>
			<p>False positives are the cases in which the samples were wrongly predicted to be infected when they are actually healthy. The implication of this is that these cases would be treated for a disease that they do not have.</p>
			<p>False negatives are the cases that were wrongly predicted to be healthy when they actually have the disease. The implication of this is that these cases would not be treated for a disease that they actually have.</p>
			<p>The question you need to ask about this model depends on the nature of the disease and requires domain expertise about the disease. For example, if the disease is contagious, then the untreated cases will be released into the general population and could infect others. What would be the implication of this versus placing cases into quarantine and observing them for symptoms?</p>
			<p>On the other hand, if the disease is not contagious, the question becomes that of the implications of treating people for a disease they do not have versus the implications of not treating cases of a disease.</p>
			<p>It should be clear that there isn't a definite answer to these questions. The model would need to be tuned to provide performance that is acceptable to the users.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>Precision</h2>
			<p>Precision was introduced in <em class="italic">Chapter 3, Binary Classification</em>; however, we will be looking at it in more detail in this chapter. The precision is the total number of cases that were correctly classified as positive (called <strong class="bold">true positive</strong> and abbreviated as <strong class="bold">TP</strong>) divided by the total number of cases in that prediction (that is, the total number of entries in the row, both correctly classified (TP) and wrongly classified (FP) from the confusion matrix). Suppose 10 entries were classified as positive. If 7 of the entries were actually positive, then TP would be 7 and FP would be 3. The precision would, therefore, be 0.7. The equation is given as follows:</p>
			<div>
				<div id="_idContainer290" class="IMG---Figure">
					<img src="Images/B15019_06_27.jpg" alt="Figure 6.27: Equation for precision&#13;&#10;" width="1665" height="163"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.27: Equation for precision</p>
			<p>In the preceding equation:</p>
			<ul>
				<li><strong class="source-inline">tp</strong> is true positive – the number of predictions that were correctly classified as belonging to that class.</li>
				<li><strong class="source-inline">fp</strong> is false positive – the number of predictions that were wrongly classified as belonging to that class.</li>
				<li>The function in <strong class="source-inline">sklearn.metrics</strong> to compute precision is called <strong class="source-inline">precision_score</strong>. Go ahead and give it a try.</li>
			</ul>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>Exercise 6.07: Computing Precision for the Classification Model</h2>
			<p>In this exercise, you will be computing the precision for the classification model you trained in <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you achieve the task:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">from sklearn.metrics import precision_score</p><p>In this step, you import <strong class="source-inline">precision_score</strong> from <strong class="source-inline">sklearn.metrics</strong>.</p></li>
				<li>Next, compute the precision score as shown in the following code snippet:<p class="source-code">precision_score(y_val, y_pred, average='macro')</p><p>In this step, you compute the precision score using <strong class="source-inline">precision_score</strong>.</p><p>The output is a floating-point number between 0 and 1. It might look like this:</p><div id="_idContainer291" class="IMG---Figure"><img src="Images/B15019_06_28.jpg" alt="Figure 6.28: Precision score&#13;&#10;" width="526" height="41"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.28: Precision score</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The precision score can vary depending on the data.</p>
			<p>In this exercise, you see the precision score for the classification model is <strong class="source-inline">0.9245</strong>. <strong class="bold">92%</strong> could be a good score and is acceptable in some domains, but is a low score in certain domains. So there is scope for improvement.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3kROW6R">https://packt.live/3kROW6R</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aCS8ye">https://packt.live/3aCS8ye</a>.</p>
			<p>Think of the precision score as asking how often does this model make the correct prediction for a class? The value needs to be much closer to 1 than the score we just achieved.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Recall</h2>
			<p>Recall is the total number of predictions that were true divided by the number of predictions for the class, both true and false. Think of it as the true positive divided by the sum of entries in the column. The equation is given as follows:</p>
			<div>
				<div id="_idContainer292" class="IMG---Figure">
					<img src="Images/B15019_06_29.jpg" alt="Figure 6.29: Equation for recall&#13;&#10;" width="1665" height="163"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.29: Equation for recall</p>
			<p>The function for this is <strong class="source-inline">recall_score</strong>, which is available from <strong class="source-inline">sklearn.metrics</strong>.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor156"/>Exercise 6.08: Computing Recall for the Classification Model</h2>
			<p>The goal of this exercise is to compute the recall for the classification model you trained in <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you accomplish the task:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Now, import the required libraries:<p class="source-code">from sklearn.metrics import recall_score</p><p>In this step, you import <strong class="source-inline">recall_score</strong> from <strong class="source-inline">sklearn.metrics</strong>. This is the function that you will make use of in the second step.</p></li>
				<li>Compute the recall:<p class="source-code">recall_score(y_val, y_pred, average='macro')</p><p>In this step, you compute the recall by using <strong class="source-inline">recall_score</strong>. You need to specify <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong> as parameters to the function. The documentation for <strong class="source-inline">recall_score</strong> explains the values that you can supply to <strong class="source-inline">average</strong>. If your model does binary prediction and the labels are <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, you can set <strong class="source-inline">average</strong> to <strong class="source-inline">binary</strong>. Other options are <strong class="source-inline">micro</strong>, <strong class="source-inline">macro</strong>, <strong class="source-inline">weighted</strong>, and <strong class="source-inline">samples</strong>. You should read the documentation to see what they do. </p><p>You should get an output that looks like the following:</p><div id="_idContainer293" class="IMG---Figure"><img src="Images/B15019_06_30.jpg" alt="Figure 6.30: Recall score&#13;&#10;" width="530" height="41"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.30: Recall score</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The recall score can vary, depending on the data.</p>
			<p>As you can see, we have calculated the recall score in the exercise, which is <strong class="source-inline">0.622</strong>. This means that of the total number of classes that were predicted, <strong class="source-inline">62%</strong> of them were correctly predicted. On its own, this value might not mean much until it is compared to the recall score from another model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31axPp6">https://packt.live/31axPp6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YdNv8O">https://packt.live/2YdNv8O</a>.</p>
			<p>Let's now move toward calculating the F1 score, which also helps greatly in evaluating the model performance, which in turn aids in making better decisions when choosing models.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>F1 Score</h2>
			<p>The F1 score is another important parameter that helps us to evaluate the model performance. It considers the contribution of both precision and recall using the following equation:</p>
			<div>
				<div id="_idContainer294" class="IMG---Figure">
					<img src="Images/B15019_06_31.jpg" alt="Figure 6.31: F1 score&#13;&#10;" width="1665" height="180"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.31: F1 score</p>
			<p>The F1 score ranges from 0 to 1, with 1 being the best possible score. You compute the F1 score using <strong class="source-inline">f1_score</strong> from <strong class="source-inline">sklearn.metrics</strong>.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>Exercise 6.09: Computing the F1 Score for the Classification Model</h2>
			<p>In this exercise, you will compute the F1 score for the classification model you trained in <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you accomplish the task:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import the necessary modules:<p class="source-code">from sklearn.metrics import f1_score</p><p>In this step, you import the <strong class="source-inline">f1_score</strong> method from <strong class="source-inline">sklearn.metrics</strong>. This score will let you compute evaluation metrics.</p></li>
				<li>Compute the F1 score:<p class="source-code">f1_score(y_val, y_pred, average='macro')</p><p>In this step, you compute the F1 score by passing in <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong>. You also specify <strong class="source-inline">average='macro'</strong> because this is not binary classification. </p><p>You should get an output similar to the following:</p><div id="_idContainer295" class="IMG---Figure"><img src="Images/B15019_06_32.jpg" alt="Figure 6.32: F1 score&#13;&#10;" width="620" height="31"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.32: F1 score</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iWCqkq">https://packt.live/3iWCqkq</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Q84epY">https://packt.live/2Q84epY</a>.</p>
			<p>By the end of this exercise, you will see that the <strong class="source-inline">F1</strong> score we achieved is <strong class="source-inline">0.6746</strong>. There is a lot of room for improvement, and you would engineer new features and train a new model to try and get a better F1 score.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>Accuracy</h2>
			<p>Accuracy is an evaluation metric that is applied to classification models. It is computed by counting the number of labels that were correctly predicted, meaning that the predicted label is exactly the same as the ground truth. The <strong class="source-inline">accuracy_score()</strong> function exists in <strong class="source-inline">sklearn.metrics</strong> to provide this value.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Exercise 6.10: Computing Model Accuracy for the Classification Model</h2>
			<p>The goal of this exercise is to compute the accuracy score of the model trained in <em class="italic">Exercise 6.04</em>, <em class="italic">Computing the Mean Absolute Error of a Second Model</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you accomplish the task:</p>
			<ol>
				<li value="1">Continue from where the code for <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, ends in your notebook.</li>
				<li>Import <strong class="source-inline">accuracy_score()</strong>:<p class="source-code">from sklearn.metrics import accuracy_score</p><p>In this step, you import <strong class="source-inline">accuracy_score()</strong>, which you will use to compute the model accuracy.</p></li>
				<li>Compute the accuracy:<p class="source-code">_accuracy = accuracy_score(y_val, y_pred)</p><p class="source-code">print(_accuracy)</p><p>In this step, you compute the model accuracy by passing in <strong class="source-inline">y_val</strong> and <strong class="source-inline">y_pred</strong> as parameters to <strong class="source-inline">accuracy_score()</strong>. The interpreter assigns the result to a variable called <strong class="source-inline">c</strong>. The <strong class="source-inline">print()</strong> method causes the interpreter to render the value of <strong class="source-inline">_accuracy</strong>. </p><p>The result is similar to the following:</p><div id="_idContainer296" class="IMG---Figure"><img src="Images/B15019_06_33.jpg" alt="Figure 6.33 Accuracy score&#13;&#10;" width="576" height="38"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.33 Accuracy score</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Q6K5Ao">https://packt.live/2Q6K5Ao</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ye55JT">https://packt.live/2Ye55JT</a>.</p>
			<p>Thus, we have successfully calculated the accuracy of the model as being <strong class="source-inline">0.876</strong>. The goal of this exercise is to show you how to compute the accuracy of a model and to compare this accuracy value to that of another model that you will train in the future.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/>Logarithmic Loss</h2>
			<p>The logarithmic loss (or log loss) is the loss function for categorical models. It is also called categorical cross-entropy. It seeks to penalize incorrect predictions. The <strong class="source-inline">sklearn</strong> documentation defines it as "the negative log-likelihood of the true values given your model predictions."</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Exercise 6.11: Computing the Log Loss for the Classification Model</h2>
			<p>The goal of this exercise is to predict the log loss of the model trained in <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.05, Creating a Classification Model for Computing Evaluation Metrics.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.05</em> and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you accomplish the task:</p>
			<ol>
				<li value="1">Open your Colab notebook and continue from where <em class="italic">Exercise 6.05</em>, <em class="italic">Creating a Classification Model for Computing Evaluation Metrics</em>, stopped.</li>
				<li>Import the required libraries:<p class="source-code">from sklearn.metrics import log_loss</p><p>In this step, you import <strong class="source-inline">log_loss()</strong> from <strong class="source-inline">sklearn.metrics</strong>.</p></li>
				<li>Compute the log loss:<p class="source-code">_loss = log_loss(y_val, model.predict_proba(X_val))</p><p class="source-code">print(_loss)</p></li>
			</ol>
			<p>In this step, you compute the log loss and store it in a variable called <strong class="source-inline">_loss</strong>. You need to observe something very important: previously, you made use of <strong class="source-inline">y_val</strong>, the ground truths, and <strong class="source-inline">y_pred</strong>, the predictions.</p>
			<p>In this step, you do not make use of predictions. Instead, you make use of predicted probabilities. You see that in the code where you specify <strong class="source-inline">model.predict_proba()</strong>. You specify the validation dataset and it returns the predicted probabilities.</p>
			<p>The <strong class="source-inline">print()</strong> function causes the interpreter to render the log loss. </p>
			<p>This should look like the following:</p>
			<div>
				<div id="_idContainer297" class="IMG---Figure">
					<img src="Images/B15019_06_34.jpg" alt="Figure 6.34: Log loss output&#13;&#10;" width="564" height="30"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.34: Log loss output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The value of loss can vary for different data.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Q5plZR">https://packt.live/2Q5plZR</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/34eMIsm">https://packt.live/34eMIsm</a>.</p>
			<p>Thus, we have successfully calculated the <strong class="source-inline">log_loss</strong> for a classification model.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor163"/>Receiver Operating Characteristic Curve</h1>
			<p>Recall the True Positive Rate, which we discussed earlier. It is also called <strong class="bold">sensitivity</strong>. Also recall that what we try to do with a logistic regression model is find a threshold value such that above that threshold value, we predict that our input falls into a certain class, and below that threshold, we predict that it doesn't.</p>
			<p>The Receiver Operating Characteristic (ROC) curve is a plot that shows how the true positive and false positive rates vary for a model as the threshold is changed.</p>
			<p>Let's do an exercise to enhance our understanding of the ROC curve.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>Exercise 6.12: Computing and Plotting ROC Curve for a Binary Classification Problem</h2>
			<p>The goal of this exercise is to plot the ROC curve for a binary classification problem. The data for this problem is used to predict whether or not a mother will require a caesarian section to give birth.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that you will be using in this chapter can be found in our GitHub repository: <a href="https://packt.live/36dyEg5">https://packt.live/36dyEg5</a>.</p>
			<p class="callout">From the UCI Machine Learning Repository, the abstract for this dataset follows: "This dataset contains information about caesarian section results of 80 pregnant women with the most important characteristics of delivery problems in the medical field." The attributes of interest are age, delivery number, delivery time, blood pressure, and heart status.</p>
			<p>The following steps will help you accomplish this task:</p>
			<ol>
				<li value="1">Open a Colab notebook file.</li>
				<li>Import the required libraries:<p class="source-code"># import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.metrics import roc_curve</p><p class="source-code">from sklearn.metrics import auc</p><p>In this step, you import <strong class="source-inline">pandas</strong>, which you will use to read in data. You also import <strong class="source-inline">train_test_split</strong> for creating training and validation datasets, and <strong class="source-inline">LogisticRegression</strong> for creating a model.</p></li>
				<li>Read in the data:<p class="source-code"># data doesn't have headers, so let's create headers</p><p class="source-code">_headers = ['Age', 'Delivery_Nbr', 'Delivery_Time', \</p><p class="source-code">            'Blood_Pressure', 'Heart_Problem', 'Caesarian']</p><p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/caesarian.csv.arff',\</p><p class="source-code">                 names=_headers, index_col=None, skiprows=15)</p><p class="source-code">df.head()</p><p class="source-code"># target column is 'Caesarian'</p><p>In this step, you read in your data. The dataset has an interesting format. The bottom part contains the data in CSV format, but the upper part contains some file descriptors. If you download and open the file from <a href="https://packt.live/38qJe4A">https://packt.live/38qJe4A</a> and open the file using Notepad, you will see the following:</p><div id="_idContainer298" class="IMG---Figure"><img src="Images/B15019_06_35.jpg" alt="Figure 6.35: Reading the dataset&#13;&#10;" width="1077" height="711"/></div><p class="figure-caption">Figure 6.35: Reading the dataset</p><p>You will need to do a few things to work with this file. Skip 15 rows and specify the column headers and read the file without an index.</p><p>The code shows how you do that by creating a Python list to hold your column headers and then read in the file using <strong class="source-inline">read_csv()</strong>. The parameters that you pass in are the file's location, the column headers as a Python list, the name of the index column (in this case, it is None), and the number of rows to skip.</p><p>The <strong class="source-inline">head()</strong> method will print out the top five rows and should look similar to the following:</p><div id="_idContainer299" class="IMG---Figure"><img src="Images/B15019_06_36.jpg" alt="Figure 6.36: The top five rows of the DataFrame&#13;&#10;" width="569" height="180"/></div><p class="figure-caption">Figure 6.36: The top five rows of the DataFrame</p></li>
				<li>Split the data:<p class="source-code"># target column is 'Caesarian'</p><p class="source-code">features = df.drop(['Caesarian'], axis=1).values</p><p class="source-code">labels = df[['Caesarian']].values</p><p class="source-code"># split 80% for training and 20% into an evaluation set</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">"""</p><p class="source-code">further split the evaluation set into validation and test sets </p><p class="source-code">of 10% each</p><p class="source-code">"""</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                test_size=0.5, \</p><p class="source-code">                                                random_state=0)</p><p>In this step, you begin by creating two <strong class="source-inline">numpy</strong> arrays, which you call <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>. You then split these arrays into a <strong class="source-inline">training</strong> and an <strong class="source-inline">evaluation</strong> dataset. You further split the <strong class="source-inline">evaluation</strong> dataset into <strong class="source-inline">validation</strong> and <strong class="source-inline">test</strong> datasets.</p></li>
				<li>Now, train and fit a logistic regression model:<p class="source-code">model = LogisticRegression()</p><p class="source-code">model.fit(X_train, y_train)</p><p>In this step, you begin by creating an instance of a logistic regression model. You then proceed to train or fit the model on the training dataset. </p><p>The output should be similar to the following:</p><div id="_idContainer300" class="IMG---Figure"><img src="Images/B15019_06_37.jpg" alt="Figure 6.37: Training a logistic regression model&#13;&#10;" width="742" height="122"/></div><p class="figure-caption">Figure 6.37: Training a logistic regression model</p></li>
				<li>Predict the probabilities, as shown in the following code snippet:<p class="source-code">y_proba = model.predict_proba(X_val)</p><p>In this step, the model predicts the probabilities for each entry in the validation dataset. It stores the results in <strong class="source-inline">y_proba</strong>.</p></li>
				<li>Compute the true positive rate, the false positive rate, and the thresholds:<p class="source-code">_false_positive, _true_positive, _thresholds = roc_curve\</p><p class="source-code">                                               (y_val, \</p><p class="source-code">                                                y_proba[:, 0])</p><p>In this step, you make a call to <strong class="source-inline">roc_curve()</strong> and specify the ground truth and the first column of the predicted probabilities. The result is a tuple of false positive rate, true positive rate, and thresholds.</p></li>
				<li>Explore the false positive rates:<p class="source-code">print(_false_positive)</p><p>In this step, you instruct the interpreter to print out the false positive rate. The output should be similar to the following:</p><div id="_idContainer301" class="IMG---Figure"><img src="Images/B15019_06_38.jpg" alt="Figure 6.38: False positive rates&#13;&#10;" width="522" height="35"/></div><p class="figure-caption">Figure 6.38: False positive rates</p><p class="callout-heading">Note</p><p class="callout">The false positive rates can vary, depending on the data.</p></li>
				<li>Explore the true positive rates:<p class="source-code">print(_true_positive)</p><p>In this step, you instruct the interpreter to print out the true positive rates. This should be similar to the following:</p><p> </p><div id="_idContainer302" class="IMG---Figure"><img src="Images/B15019_06_39.jpg" alt="Figure 6.39: True positive rates&#13;&#10;" width="668" height="48"/></div><p class="figure-caption">Figure 6.39: True positive rates</p></li>
				<li>Explore the thresholds:<p class="source-code">print(_thresholds)</p><p>In this step, you instruct the interpreter to display the thresholds. The output should be similar to the following:</p><div id="_idContainer303" class="IMG---Figure"><img src="Images/B15019_06_40.jpg" alt="Figure 6.40: Thresholds&#13;&#10;" width="669" height="55"/></div><p class="figure-caption">Figure 6.40: Thresholds</p></li>
				<li>Now, plot the ROC curve:<p class="source-code"># Plot the RoC</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">plt.plot(_false_positive, _true_positive, lw=2, \</p><p class="source-code">         label='Receiver Operating Characteristic')</p><p class="source-code">plt.xlim(0.0, 1.2)</p><p class="source-code">plt.ylim(0.0, 1.2)</p><p class="source-code">plt.xlabel('False Positive Rate')</p><p class="source-code">plt.ylabel('True Positive Rate')</p><p class="source-code">plt.title('Receiver Operating Characteristic')</p><p class="source-code">plt.show()</p><p>In this step, you import <strong class="source-inline">matplotlib.pyplot</strong> as your plotting library. You alias it as <strong class="source-inline">plt</strong>. You then proceed to plot a line chart by specifying the false positive rates and true positive rates. The rest of the code decorates the chart with a title and labels for the horizontal and vertical axes. </p><p>The output should look similar to the following:</p><div id="_idContainer304" class="IMG---Figure"><img src="Images/B15019_06_41.jpg" alt="Figure 6.41: ROC curve&#13;&#10;" width="1043" height="587"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.41: ROC curve</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/322jiLa">https://packt.live/322jiLa</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/324ii9s">https://packt.live/324ii9s</a>.</p>
			<p>In this exercise, you learned to plot how the true positive rate and false positive rate of the model vary as you change the prediction threshold. Recall that what the model does is output a value between 0 and 1. This value is called a logit. Your job as a data scientist is to decide on a threshold value, for example, 0.5. If the logit is above that threshold, you predict that the input falls into one class (positive, if it is a positive-or-negative prediction). If the logit is below the threshold, you will predict that the input belongs to the negative class.</p>
			<p>For example, if your threshold is 0.5, then a logit of 0.33 is predicted as negative, while a logit of 0.80 is predicted as positive.</p>
			<p>However, if your threshold is 0.95, then a logit of 0.33 is predicted as negative, and a logit of 0.80 is still predicted as negative.</p>
			<p>Now, recall that what you want your model to do is correctly classify as many data points as possible. Classification is controlled by your chosen threshold value. The logit (predicted probability) from the model will always be the same, but the class assigned to the prediction will depend on the threshold.</p>
			<p>As you vary the threshold, the predictions change, and the number of true positives and true negatives changes.</p>
			<p>The RoC shows you how the percentage of true positives and true negatives changes as the threshold varies from 0 to 1.</p>
			<p>The higher the threshold, the more confident the model needs to be before you classify a prediction as positive. Recall that the logit is the probability that the input belongs to a class and is a confidence score from 0 to 1.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>Area Under the ROC Curve</h1>
			<p>The <strong class="bold">Area Under the Receiver Operating Characteristic Curve</strong> (<strong class="bold">ROC AUC</strong>) is a measure of the likelihood that the model will rank a randomly chosen positive example higher than a randomly chosen negative example. Another way of putting it is to say that the higher this measure is, the better the model is at predicting a negative class as negative, and a positive class as positive. The value ranges from 0 to 1. If the AUC is 0.6, it means that the model has a 60% probability of correctly distinguishing a negative class from a positive class based on the inputs. This measure is used to compare models.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>Exercise 6.13: Computing the ROC AUC for the Caesarian Dataset</h2>
			<p>The goal of this exercise is to compute the ROC AUC for the binary classification model that you trained in <em class="italic">Exercise 6.12</em>, <em class="italic">Computing and Plotting ROC Curve for a Binary Classification Problem</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should continue this exercise in the same notebook as that used in <em class="italic">Exercise 6.12, Computing and Plotting ROC Curve for a Binary Classification Problem.</em> If you wish to use a new notebook, make sure you copy and run the entire code from <em class="italic">Exercise 6.12</em> and then begin with the execution of the code of this exercise. </p>
			<p>The following steps will help you accomplish the task:</p>
			<ol>
				<li value="1">Open a Colab notebook to the code for <em class="italic">Exercise 6.12</em>, <em class="italic">Computing and Plotting ROC Curve for a Binary Classification Problem,</em> and continue writing your code.</li>
				<li>Predict the probabilities:<p class="source-code">y_proba = model.predict_proba(X_val)</p><p>In this step, you compute the probabilities of the classes in the validation dataset. You store the result in <strong class="source-inline">y_proba</strong>.</p></li>
				<li>Compute the ROC AUC:<p class="source-code">from sklearn.metrics import roc_auc_score</p><p class="source-code">_auc = roc_auc_score(y_val, y_proba[:, 0])</p><p class="source-code">print(_auc)</p><p>In this step, you compute the ROC AUC and store the result in <strong class="source-inline">_auc</strong>. You then proceed to print this value out. The result should look similar to the following:</p><div id="_idContainer305" class="IMG---Figure"><img src="Images/B15019_06_42.jpg" alt="Figure 6.42: Computing the ROC AUC&#13;&#10;" width="1507" height="42"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.42: Computing the ROC AUC</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The AUC can be different, depending on the data.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/32jCrIT">https://packt.live/32jCrIT</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/319zoDy">https://packt.live/319zoDy</a>.</p>
			<p>In this exercise, you learned to compute the ROC AUC, which is the measure of the likelihood that the model will rank a randomly chosen positive example higher than a randomly chosen negative example. In this example, the AUC is 0.1944, and there is room for improvement with this model. </p>
			<p>When you are done selecting a model, you might be interested in saving it for use in the future. The next topic explores saving and loading models.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/>Saving and Loading Models</h1>
			<p>You will eventually need to transfer some of the models you have trained to a different computer so they can be put into production. There are various utilities for doing this, but the one we will discuss is called <strong class="source-inline">joblib</strong>.</p>
			<p><strong class="source-inline">joblib</strong> supports saving and loading models, and it saves the models in a format that is supported by other machine learning architectures, such as <strong class="source-inline">ONNX</strong>.</p>
			<p><strong class="source-inline">joblib</strong> is found in the <strong class="source-inline">sklearn.externals</strong> module.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>Exercise 6.14: Saving and Loading a Model</h2>
			<p>In this exercise, you will train a simple model and use it for prediction. You will then proceed to save the model and then load it back in. You will use the loaded model for a second prediction, and then compare the predictions from the first model to those from the second model. You will make use of the car dataset for this exercise.</p>
			<p>The following steps will guide you toward the goal:</p>
			<ol>
				<li value="1">Open a Colab notebook.</li>
				<li>Import the required libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression</p></li>
				<li>Read in the data:<p class="source-code">_headers = ['CIC0', 'SM1', 'GATS1i', 'NdsCH', 'Ndssc', \</p><p class="source-code">            'MLOGP', 'response']</p><p class="source-code"># read in data</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter06/Dataset/'\</p><p class="source-code">                 'qsar_fish_toxicity.csv', \</p><p class="source-code">                 names=_headers, sep=';')</p></li>
				<li>Inspect the data:<p class="source-code">df.head()</p><p>The output should be similar to the following:</p><div id="_idContainer306" class="IMG---Figure"><img src="Images/B15019_06_43.jpg" alt="Figure 6.43: Inspecting the first five rows of the DataFrame&#13;&#10;" width="582" height="182"/></div><p class="figure-caption">Figure 6.43: Inspecting the first five rows of the DataFrame</p></li>
				<li>Split the data into <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>, and into training and validation sets:<p class="source-code">features = df.drop('response', axis=1).values</p><p class="source-code">labels = df[['response']].values</p><p class="source-code">X_train, X_eval, y_train, y_eval = train_test_split\</p><p class="source-code">                                   (features, labels, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=0)</p><p class="source-code">X_val, X_test, y_val, y_test = train_test_split(X_eval, y_eval,\</p><p class="source-code">                                                random_state=0)</p></li>
				<li>Create a linear regression model:<p class="source-code">model = LinearRegression()</p><p class="source-code">print(model)</p><p>The output will be as follows: </p><div id="_idContainer307" class="IMG---Figure"><img src="Images/B15019_06_44.jpg" alt="Figure 6.44: Training a linear regression model&#13;&#10;" width="1074" height="38"/></div><p class="figure-caption">Figure 6.44: Training a linear regression model</p></li>
				<li>Fit the training data to the model:<p class="source-code">model.fit(X_train, y_train)</p></li>
				<li>Use the model for prediction:<p class="source-code">y_pred = model.predict(X_val)</p></li>
				<li>Import <strong class="source-inline">joblib</strong>:<p class="source-code">from sklearn.externals import joblib</p></li>
				<li>Save the model:<p class="source-code">joblib.dump(model, './model.joblib')</p><p>The output should be similar to the following:</p><div id="_idContainer308" class="IMG---Figure"><img src="Images/B15019_06_45.jpg" alt="Figure 6.45: Saving the model&#13;&#10;" width="886" height="49"/></div><p class="figure-caption">Figure 6.45: Saving the model</p></li>
				<li>Load it as a new model:<p class="source-code">m2 = joblib.load('./model.joblib')</p></li>
				<li>Use the new model for predictions:<p class="source-code">m2_preds = m2.predict(X_val)</p></li>
				<li>Compare the predictions:<p class="source-code">ys = pd.DataFrame(dict(predicted=y_pred.reshape(-1), \</p><p class="source-code">                       m2=m2_preds.reshape(-1)))</p><p class="source-code">ys.head()</p><p>The output should be similar to the following:</p><div id="_idContainer309" class="IMG---Figure"><img src="Images/B15019_06_46.jpg" alt="Figure 6.46: Comparing predictions&#13;&#10;" width="627" height="218"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.46: Comparing predictions</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/322VxTb">https://packt.live/322VxTb</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aAUz4q">https://packt.live/3aAUz4q</a>.</p>
			<p>You can see that the predictions from the model before it was saved are exactly the same as the predictions from the model after it was saved and loaded back in. It is safe to conclude that saving and loading models does not affect their quality.</p>
			<p>In this exercise, you learned how to save and load models. You also checked and confirmed that the model predictions remain the same even when you save and load them.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Activity 6.01: Train Three Different Models and Use Evaluation Metrics to Pick the Best Performing Model</h2>
			<p>You work as a data scientist at a bank. The bank would like to implement a model that predicts the likelihood of a customer purchasing a term deposit. The bank provides you with a dataset, which is the same as the one in <em class="italic">Chapter 3</em>, <em class="italic">Binary Classification</em>. You have previously learned how to train a logistic regression model for binary classification. You have also heard about other non-parametric modeling techniques and would like to try out a decision tree as well as a random forest to see how well they perform against the logistic regression models you have been training.</p>
			<p>In this activity, you will train a logistic regression model and compute a classification report. You will then proceed to train a decision tree classifier and compute a classification report. You will compare the models using the classification reports. Finally, you will train a random forest classifier and generate the classification report. You will then compare the logistic regression model with the random forest using the classification reports to determine which model you should put into production.</p>
			<p>The steps to accomplish this task are:</p>
			<ol>
				<li value="1">Open a Colab notebook.</li>
				<li>Load the necessary libraries.</li>
				<li>Read in the data.</li>
				<li>Explore the data.</li>
				<li>Convert categorical variables using <strong class="source-inline">pandas.get_dummies()</strong>.</li>
				<li>Prepare the <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> variables.</li>
				<li>Split the data into training and evaluation sets.</li>
				<li>Create an instance of <strong class="source-inline">LogisticRegression</strong>.</li>
				<li>Fit the training data to the <strong class="source-inline">LogisticRegression</strong> model.</li>
				<li>Use the evaluation set to make a prediction.</li>
				<li>Use the prediction from the <strong class="source-inline">LogisticRegression</strong> model to compute the classification report.</li>
				<li>Create an instance of <strong class="source-inline">DecisionTreeClassifier</strong>:<p class="source-code">dt_model = DecisionTreeClassifier(max_depth= 6)</p></li>
				<li>Fit the training data to the <strong class="source-inline">DecisionTreeClassifier</strong> model:<p class="source-code">dt_model.fit(train_X, train_y)</p></li>
				<li>Using the <strong class="source-inline">DecisionTreeClassifier</strong> model, make a prediction on the evaluation dataset:<p class="source-code">dt_preds = dt_model.predict(val_X)</p></li>
				<li>Use the prediction from the <strong class="source-inline">DecisionTreeClassifier</strong> model to compute the classification report:<p class="source-code">dt_report = classification_report(val_y, dt_preds)</p><p class="source-code">print(dt_report)</p><p class="callout-heading">Note</p><p class="callout">We will be studying decision trees in detail in <em class="italic">Chapter 7, The Generalization of Machine Learning Models</em>.</p></li>
				<li>Compare the classification report from the linear regression model and the classification report from the decision tree classifier to determine which is the better model.</li>
				<li>Create an instance of <strong class="source-inline">RandomForestClassifier</strong>.</li>
				<li>Fit the training data to the <strong class="source-inline">RandomForestClassifier</strong> model.</li>
				<li>Using the <strong class="source-inline">RandomForestClassifier</strong> model, make a prediction on the evaluation dataset.</li>
				<li>Using the prediction from the random forest classifier, compute the classification report.</li>
				<li>Compare the classification report from the linear regression model with the classification report from the random forest classifier to decide which model to keep or improve upon.</li>
				<li>Compare the R<span class="superscript">2</span> scores of all three models. The output should be similar to the following:<div id="_idContainer310" class="IMG---Figure"><img src="Images/B15019_06_47.jpg" alt="Figure 6.47: Comparing the R2 scores&#13;&#10;" width="1389" height="38"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.47: Comparing the R<span class="superscript">2</span> scores</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Summary</h1>
			<p>In this chapter we observed that some of the evaluation metrics for classification models require a binary classification model. We saw that when we worked with more than two classes, we were required to use the one-versus-all approach. The one-versus-all approach builds one model for each class and tries to predict the probability that the input belongs to a specific class. We saw that once this was done, we then predicted that the input belongs to the class where the model has the highest prediction probability. We also split our evaluation dataset into two, it's because <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong> are used once for a final evaluation of the model's performance. You can make use of them before putting your model into production to see how the model would perform in a production environment.</p>
			<p>You have learned how to assess the quality of a regression model by observing how the loss changes. You saw examples using the MAE, and also learned of the existence of MSE. You also learned about how to assess the quality of classification models in the activity. In the next chapter, you will learn how to train multiple models using cross-validation and also implement regularization techniques.</p>
		</div>
	</div></body></html>