- en: 6\. How to Assess Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to model evaluation, where you evaluate or assess
    the performance of each model that you train before you decide to put it into
    production. By the end of this chapter, you will be able to create an evaluation
    dataset. You will be equipped to assess the performance of linear regression models
    using **mean absolute error** (**MAE**) and **mean squared error** (**MSE**).
    You will also be able to evaluate the performance of logistic regression models
    using accuracy, precision, recall, and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you assess the performance of a model, you look at certain measurements
    or values that tell you how well the model is performing under certain conditions,
    and that helps you make an informed decision about whether or not to make use
    of the model that you have trained in the real world. Some of the measurements
    you will encounter in this chapter are MAE, precision, recall, and R2 score.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to train a regression model in *Chapter 2, Regression*, and
    how to train classification models in *Chapter 3, Binary Classification*. Consider
    the task of predicting whether or not a customer is likely to purchase a term
    deposit, which you addressed in *Chapter 3, Binary Classification*. You have learned
    how to train a model to perform this sort of classification. You are now concerned
    with how useful this model might be. You might start by training one model, and
    then evaluating how often the predictions from that model are correct. You might
    then proceed to train more models and evaluate whether they perform better than
    previous models you have trained.
  prefs: []
  type: TYPE_NORMAL
- en: You have already seen an example of splitting data using `train_test_split`
    in *Exercise 3.06*, *A Logistic Regression Model for Predicting the Propensity
    of Term Deposit Purchases in a Bank*. You will go further into the necessity and
    application of splitting data in *Chapter 7, The Generalization of Machine Learning
    Models*, but for now, you should note that it is important to split your data
    into one set that is used for training a model, and a second set that is used
    for validating the model. It is this validation step that helps you decide whether
    or not to put a model into production.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will learn more about splitting data in *Chapter 7, The Generalization
    of Machine Learning Models*, where we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple data splits using `train_test_split`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple data splits using cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For now, you will learn how to split data using a function from `sklearn` called
    `train_test_split`.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important that you do not use all of your data to train a model.
    You must set aside some data for validation, and this data must not have been
    used previously for training. When you train a model, it tries to generate an
    equation that fits your data. The longer you train, the more complex the equation
    becomes so that it passes through as many of the data points as possible.
  prefs: []
  type: TYPE_NORMAL
- en: When you shuffle the data and set some aside for validation, it ensures that
    the model learns to not overfit the hypotheses you are trying to generate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.01: Importing and Splitting Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will import data from a repository and split it into a
    training and an evaluation set to train a model. Splitting your data is required
    so that you can evaluate the model later. This exercise will get you familiar
    with the process of splitting data; this is something you will be doing frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The Car dataset that you will be using in this chapter can be found in our
    GitHub repository: [https://packt.live/30I594E](https://packt.live/30I594E).'
  prefs: []
  type: TYPE_NORMAL
- en: It was taken from the UCI Machine Learning Repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is about cars. A text file is provided with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buying` – the cost of purchasing this vehicle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maint` – the maintenance cost of the vehicle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doors` – the number of doors the vehicle has'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persons` – the number of persons the vehicle is capable of transporting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lug_boot` – the cargo capacity of the vehicle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety` – the safety rating of the vehicle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`car` – this is the category that the model attempts to predict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps will help you complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You started by importing a library called `pandas` in the first line. This library
    is useful for reading files into a data structure that is called a `DataFrame`,
    which you have used in previous chapters. This structure is like a spreadsheet
    or a table with rows and columns that we can manipulate. Because you might need
    to reference the library lots of times, we have created an alias for it, `pd`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the second line, you import a function called `train_test_split` from a module
    called `model_selection`, which is within `sklearn`. This function is what you
    will make use of to split the data that you read in using `pandas`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a Python list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The data that you are reading in is stored as a CSV file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The browser will download the file to your computer. You can open the file
    using a text editor. If you do, you will see something similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1: The car dataset without headers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.1: The car dataset without headers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Alternatively, you can enter the dataset URL in the browser to view the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`CSV` files normally have the name of each column written in the first row
    of the data. For instance, have a look at this dataset''s CSV file, which you
    used in *Chapter 3, Binary Classification*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.2: CSV file without headers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.2: CSV file without headers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: But, in this case, the column name is missing. That is not a problem, however.
    The code in this step creates a Python list called `_headers` that contains the
    name of each column. You will supply this list when you read in the data in the
    next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, the code reads in the file using a function called `read_csv`.
    The first parameter, `'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Science-Workshop/master/Chapter06/Dataset/car.data'`,
    is mandatory and is the location of the file. In our case, the file is on the
    internet. It can also be optionally downloaded, and we can then point to the local
    file's location.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second parameter (`names=_headers`) asks the function to add the row headers
    to the data after reading it in. The third parameter (`index_col=None`) asks the
    function to generate a new index for the table because the data doesn't contain
    an index. The function will produce a DataFrame, which we assign to a variable
    called `df`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print out the top five records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code in this step is used to print the top five rows of the DataFrame.
    The output from that operation is shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3: The top five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.3: The top five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a training and an evaluation DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code will split the DataFrame containing your data into two new
    DataFrames. The first is called `training` and is used for training the model.
    The second is called `evaluation` and will be further split into two in the next
    step. We mentioned earlier that you must separate your dataset into a training
    and an evaluation dataset, the former for training your model and the latter for
    evaluating your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, the `train_test_split` function takes two parameters. The first
    parameter is the data we want to split. The second is the ratio we would like
    to split it by. What we have done is specified that we want our evaluation data
    to be 30% of our data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The third parameter random_state is set to 0 to ensure reproducibility of results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a validation and test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code is similar to the code in *Step 6*. In this step, the code splits
    our evaluation data into two equal parts because we specified `0.5`, which means
    `50%`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3haKvl3](https://packt.live/3haKvl3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3g8zI9R](https://packt.live/3g8zI9R).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In previous chapters, we''ve seen how to split your data into train and test
    sets, but here, we''ll go one step further and split it into three: one to train
    a model, one to evaluate the model during training, and one to evaluate the model
    before putting it into production.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have your data split into different sets, you may proceed to train
    and evaluate models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assessing Model Performance for Regression Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you create a regression model, you create a model that predicts a continuous
    numerical variable, as you learned in *Chapter 2, Regression*. When you set aside
    your evaluation dataset, you have something that you can use to compare the quality
    of your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'What you need to do to assess your model quality is compare the quality of
    your prediction to what is called the ground truth, which is the actual observed
    value that you are trying to predict. Take a look at *Figure 6.4*, in which the
    first column contains the ground truth (called actuals) and the second column
    contains the predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Actual versus predicted values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4: Actual versus predicted values'
  prefs: []
  type: TYPE_NORMAL
- en: Line `0` in the output compares the actual value in our evaluation dataset to
    what our model predicted. The actual value from our evaluation dataset is `4.891`.
    The value that the model predicted is `4.132270`.
  prefs: []
  type: TYPE_NORMAL
- en: Line `1` compares the actual value of `4.194` to what the model predicted, which
    is `4.364320`.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the evaluation dataset will contain a lot of records, so you will
    not be making this comparison visually. Instead, you will make use of some equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You would carry out this comparison by computing the loss. The loss is the
    difference between the actuals and the predicted values in the preceding screenshot.
    In data mining, it is called a **distance measure**. There are various approaches
    to computing distance measures that give rise to different loss functions. Two
    of these are:'
  prefs: []
  type: TYPE_NORMAL
- en: Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various loss functions for regression, but in this book, we will
    be looking at two of the commonly used loss functions for regression, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error (MAE) – this is based on Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean squared error (MSE) – this is based on Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of these functions is to measure the usefulness of your models by giving
    you a numerical value that shows how much deviation there is between the ground
    truths and the predicted values from your models.
  prefs: []
  type: TYPE_NORMAL
- en: Your mission is to train new models with consistently lower errors. Before we
    do that, let's have a quick introduction to some data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Data Structures – Vectors and Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at different data structures, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A scalar variable is a simple number, such as 23\. Whenever you make use of
    numbers on their own, they are scalars. You assign them to variables, such as
    in the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you had to store the temperature for 5 days, you would need to store the
    values in 5 different values, such as in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In data science, you will frequently work with a large number of data points,
    such as hourly temperature measurements for an entire year. A more efficient way
    of storing lots of values is called a vector. Let's look at vectors in the next
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A vector is a collection of scalars. Consider the five temperatures in the
    previous code snippet. A vector is a data type that lets you collect all of the
    previous temperatures in one variable that supports arithmetic operations. Vectors
    look similar to Python lists and can be created from Python lists. Consider the
    following code snippet for creating a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can create a vector from the list using the `.array()` method from `numpy`
    by first importing `numpy` and then using the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can proceed to verify the data type using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The code snippet will cause the compiler to print out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: The temps_ndarray vector data type'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.5: The temps_ndarray vector data type'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may inspect the contents of the vector using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: The temps_ndarray vector'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.6: The temps_ndarray vector'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the output contains single square brackets, `[` and `]`, and the
    numbers are separated by spaces. This is different from the output from a Python
    list, which you can obtain using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The code snippet yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: List of elements in temps_list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.7: List of elements in temps_list'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the output contains single square brackets, `[` and `]`, and the numbers
    are separated by commas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vectors have a shape and a dimension. Both of these can be determined by using
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a Python data structure called a **tuple** and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: Shape of the temps_ndarray vector'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.8: Shape of the temps_ndarray vector'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the output consists of brackets, `(` and `)`, with a number and
    a comma. The single number followed by a comma implies that this object has only
    one dimension. The value of the number is the number of elements. The output is
    read as "a vector with five elements." This is very important because it is very
    different from a matrix, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A matrix is also made up of scalars but is different from a scalar in the sense
    that a matrix has both rows and columns3
  prefs: []
  type: TYPE_NORMAL
- en: 'There are times when you need to convert between vectors and matrices. Let''s
    revisit `temps_ndarray`. You may recall that it has five elements because the
    shape was `(5,)`. To convert it into a matrix with five rows and one column, you
    would use the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The code snippet makes use of the `.reshape()` method. The first parameter,
    `-1`, instructs the interpreter to keep the first dimension constant. The second
    parameter, `1`, instructs the interpreter to add a new dimension. This new dimension
    is the column. To see the new shape, use the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: Shape of the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9: Shape of the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the tuple now has two numbers, `5` and `1`. The first number, `5`,
    represents the rows, and the second number, `1`, represents the columns. You can
    print out the value of the matrix using the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10: Elements of the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.10: Elements of the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the output is different from that of the vector. First, we have
    an outer set of square brackets. Then, each row has its element enclosed in square
    brackets. Each row contains only one number because the matrix has only one column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may reshape the matrix to contain `1` row and `5` columns and print out
    the value using the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11: Reshaping the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.11: Reshaping the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that you now have all the numbers on one row because this matrix has
    one row and five columns. The outer square brackets represent the matrix, while
    the inner square brackets represent the row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can convert the matrix back into a vector by dropping the column
    using the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can print out the value of the vector to confirm that you get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12: The value of the vector'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12: The value of the vector'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that you now have only one set of square brackets. You still have the
    same number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at an important metric – R2 score.
  prefs: []
  type: TYPE_NORMAL
- en: R2 Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The R2 score (pronounced "r squared") is sometimes called the "score" and measures
    the coefficient of determination of the model. Think of it as the model's ability
    to make good and reliable predictions. This measure is accessed using the `score()`
    method of the model and is available for every model.
  prefs: []
  type: TYPE_NORMAL
- en: Your goal is to train successive models with a higher R2 score. The R2 score
    has a range between **0** and **1**. Your goal is to try and get the model to
    have a score that is close to **1**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.02: Computing the R2 Score of a Linear Regression Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the preceding sections, R2 score is an important factor in evaluating
    the performance of a model. Thus, in this exercise, we will be creating a linear
    regression model and then calculating the R2 score for it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The fish toxicity dataset that you will be using in this chapter can be found
    in our GitHub repository: [https://packt.live/2sNChvv](https://packt.live/2sNChvv).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset was taken from the UCI Machine Learning Repository: [https://packt.live/2TSyJTB](https://packt.live/2TSyJTB).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following attributes are useful for our task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CIC0: information indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SM1_Dz(Z): 2D matrix-based descriptors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GATS1i: 2D autocorrelations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NdsCH: Pimephales promelas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NdssC: atom-type counts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLOGP: molecular properties'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantitative response, LC50 [-LOG(mol/L)]: This attribute represents the concentration
    that causes death in 50% of test fish over a test duration of 96 hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook to write and execute your code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, import the libraries mentioned in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `pandas`, which you will use to read your data. You
    also import `train_test_split()`, which you will use to split your data into training
    and validation sets, and you import `LinearRegression`, which you will use to
    train your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, read the data from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you create a Python list to hold the names of the columns in
    your data. You do this because the CSV file containing the data does not have
    a first row that contains the column headers. You proceed to read in the file
    and store it in a variable called `df` using the `read_csv()` method in pandas.
    You specify the list containing column headers by passing it into the `names`
    parameter. This CSV uses semi-colons as column separators, so you specify that
    using the `sep` parameter. You can use `df.head()` to see what the DataFrame looks
    like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13: The first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.13: The first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into features and labels and into training and evaluation datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you create two `numpy` arrays called `features` and `labels`.
    You then proceed to split them twice. The first split produces a `training` set
    and an `evaluation` set. The second split creates a `validation` set and a `test`
    set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you create an instance of `LinearRegression` and store it in a
    variable called `model`. You will make use of this to train on the training dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you train the model using the `fit()` method and the training
    dataset that you made in *Step 4*. The first parameter is the `features` NumPy
    array, and the second parameter is `labels`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.14: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a prediction, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you make use of the validation dataset to make a prediction. This
    is stored in `y_pred`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the R2 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute `r2`, which is the R2 score of the model. The R2 score
    is computed using the `score()` method of the model. The next line causes the
    interpreter to print out the R2 score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.15: R2 score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.15: R2 score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The MAE and R2 score may vary depending on the distribution of the datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You see that the R2 score we achieved is `0.56238`, which is not close to 1\.
    In the next step, we will be making comparisons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare the predictions to the actual ground truth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you take a cursory look at the predictions compared to the ground
    truth. In *Step 8*, you will have noticed that the R2 score you computed for the
    model is far from perfect (perfect is a score of 1). In this step, in the first
    line, you create a DataFrame by making use of the `DataFrame` method in pandas.
    You provide a dictionary as an argument. The dictionary has two keys: `actuals`
    and `predicted`. `actuals` contains `y_vals`, which is the actual labels in the
    validation dataset. `predicted` contains `y_pred`, which contains the predictions.
    Both `y_vals` and `y_pred` are two-dimensional matrices, so you reshape them to
    1D vectors by using `.reshape(-1)`, which drops the second axis.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second line causes the interpreter to display the top five records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output looks similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.16: The actual versus predicted values of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.16: The actual versus predicted values of the model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31aw6QE](https://packt.live/31aw6QE).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3aASLbE](https://packt.live/3aASLbE).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we computed the R2 score, which is an evaluation metric that
    can be used for comparing models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next topic, we will be looking at the mean absolute error, which is another
    evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Absolute Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **mean absolute error** (**MAE**) is an evaluation metric for regression
    models that measures the absolute distance between your predictions and the ground
    truth. The absolute distance is the distance regardless of the sign, whether positive
    or negative. For example, if the ground truth is 6 and you predict 5, the distance
    is 1\. However, if you predict 7, the distance becomes -1\. The absolute distance,
    without taking the signs into consideration, is 1 in both cases. This is called
    the **magnitude**. The MAE is computed by summing all of the magnitudes and dividing
    by the number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.03: Computing the MAE of a Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to find the score and loss of a model using the
    same dataset as *Exercise 6.02*, *Computing the R2 Score of a Linear Regression
    Model*.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we will be calculating the MAE of a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you with this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import the function called `mean_absolute_error` from `sklearn.metrics`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you read in your data. This data is hosted online and
    contains some information about fish toxicity. The data is stored as a CSV but
    does not contain any headers. Also, the columns in this file are not separated
    by a comma, but rather by a semi-colon. The Python list called `_headers` contains
    the names of the column headers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next line, you make use of the function called `read_csv`, which is contained
    in the `pandas` library, to load the data. The first parameter specifies the file
    location. The second parameter specifies the Python list that contains the names
    of the columns in the data. The third parameter specifies the character that is
    used to separate the columns in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into `features` and `labels` and into training and evaluation
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you split your data into training, validation, and test datasets.
    In the first line, you create a `numpy` array in two steps. In the first step,
    the `drop` method takes a parameter with the name of the column to drop from the
    DataFrame. In the second step, you use `values` to convert the DataFrame into
    a two-dimensional `numpy` array that is a tabular structure with rows and columns.
    This array is stored in a variable called `features`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the second line, you convert the column into a `numpy` array that contains
    the label that you would like to predict. You do this by picking out the column
    from the DataFrame and then using `values` to convert it into a `numpy` array.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the third line, you split the `features` and `labels` using `train_test_split`
    and a ratio of 80:20\. The training data is contained in `X_train` for the features
    and `y_train` for the labels. The evaluation dataset is contained in `X_eval`
    and `y_eval`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the fourth line, you split the evaluation dataset into validation and testing
    using `train_test_split`. Because you don't specify the `test_size`, a value of
    `25%` is used. The validation data is stored in `X_val` and `y_val`, while the
    test data is stored in `X_test` and `y_test`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a simple linear regression model and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you make use of your training data to train a model. In the first
    line, you create an instance of `LinearRegression`, which you call `model`. In
    the second line, you train the model using `X_train` and `y_train`. `X_train`
    contains the `features`, while `y_train` contains the `labels`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now predict the values of our validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this point, your model is ready to use. You make use of the `predict` method
    to predict on your data. In this case, you are passing `X_val` as a parameter
    to the function. Recall that `X_va`l is your validation dataset. The result is
    assigned to a variable called `y_pred` and will be used in the next step to compute
    the MAE of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the MAE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you compute the MAE of the model by using the `mean_absolute_error`
    function and passing in `y_val` and `y_pred`. `y_val` is the label that was provided
    with your training data, and `y_pred` is the prediction from the model. The preceding
    code should give you an MAE value of ~ 0.72434:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.17 MAE score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.17 MAE score
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both `y_val` and `y_pred` are a `numpy` array that contains the same number
    of elements. The `mean_absolute_error` function subtracts `y_pred` from `y_val`.
    This results in a new array. The elements in the resulting array have the absolute
    function applied to them so that all negative signs are dropped. The average of
    the elements is then computed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the R2 score of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.18: The R2 score of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.18: The R2 score of the model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The MAE and R2 score may vary depending on the distribution of the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A higher R2 score means a better model and uses an equation that computes the
    coefficient of determination.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/349mG9P](https://packt.live/349mG9P).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3aA1rza](https://packt.live/3aA1rza).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we have calculated the MAE, which is a significant parameter
    when it comes to evaluating models.
  prefs: []
  type: TYPE_NORMAL
- en: You will now train a second model and compare its R2 score and MAE to the first
    model to evaluate which is a better performing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.04: Computing the Mean Absolute Error of a Second Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will be engineering new features and finding the score
    and loss of a new model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you with this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the first step, you will import libraries such as `train_test_split`, `LinearRegression`,
    and `mean_absolute_error`. We make use of a pipeline to quickly transform our
    features and engineer new features using `MinMaxScaler` and `PolynomialFeatures`.
    `MinMaxScaler` reduces the variance in your data by adjusting all values to a
    range between 0 and 1\. It does this by subtracting the mean of the data and dividing
    by the range, which is the minimum value subtracted from the maximum value. `PolynomialFeatures`
    will engineer new features by raising the values in a column up to a certain power
    and creating new columns in your DataFrame to accommodate them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read in the data from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you will read in your data. While the data is stored in a CSV,
    it doesn't have a first row that lists the names of the columns. The Python list
    called `_headers` will hold the column names that you will supply to the `pandas`
    method called `read_csv`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next line, you call the `read_csv` `pandas` method and supply the location
    and name of the file to be read in, along with the header names and the file separator.
    Columns in the file are separated with a semi-colon.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into training and evaluation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you begin by splitting the DataFrame called `df` into two. The
    first DataFrame is called `features` and contains all of the independent variables
    that you will use to make your predictions. The second is called `labels` and
    contains the values that you are trying to predict.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the third line, you split `features` and `labels` into four sets using `train_test_split`.
    `X_train` and `y_train` contain 80% of the data and are used for training your
    model. `X_eval` and `y_eval` contain the remaining 20%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the fourth line, you split `X_eval` and `y_eval` into two additional sets.
    `X_val` and `y_val` contain 75% of the data because you did not specify a ratio
    or size. `X_test` and `y_test` contain the remaining 25%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you begin by creating a Python list called `steps`. The list contains
    three tuples, each one representing a transformation of a model. The first tuple
    represents a scaling operation. The first item in the tuple is the name of the
    step, which you call `scaler`. This uses `MinMaxScaler` to transform the data.
    The second, called `poly`, creates additional features by crossing the columns
    of data up to the degree that you specify. In this case, you specify `2`, so it
    crosses these columns up to a power of 2\. Next comes your `LinearRegression`
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you create an instance of `Pipeline` and store it in a variable
    called `model`. `Pipeline` performs a series of transformations, which are specified
    in the steps you defined in the previous step. This operation works because the
    transformers (`MinMaxScaler` and `PolynomialFeatures`) implement two methods called
    `fit()` and `fit_transform()`. You may recall from previous examples that models
    are trained using the `fit()` method that `LinearRegression` implements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On the next line, you call the `fit` method and provide `X_train` and `y_train`
    as parameters. Because the model is a pipeline, three operations will happen.
    First, `X_train` will be scaled. Next, additional features will be engineered.
    Finally, training will happen using the `LinearRegression` model. The output from
    this step is similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.19: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict using the validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the MAE of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the first line, you make use of `mean_absolute_error` to compute the mean
    absolute error. You supply `y_val` and `y_pred`, and the result is stored in the
    `mae` variable. In the following line, you print out `mae`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.20: MAE score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.20: MAE score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The loss that you compute at this step is called a validation loss because you
    make use of the validation dataset. This is different from a training loss that
    is computed using the training dataset. This distinction is important to note
    as you study other documentation or books, which might refer to both.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the R2 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the final two lines, you compute the R2 score and also display it, as shown
    in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.21: R2 score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.21: R2 score'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should see a difference between the `R`2 score and the MAE
    of the first model and the second model (in the first model, the `MAE` and `R`2
    scores were `0.781629` and `0.498688` respectively).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2EjCaNn](https://packt.live/2EjCaNn).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Yb5vRd](https://packt.live/2Yb5vRd).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you engineered new features that give you a model with a hypothesis
    of a higher polynomial degree. This model should perform better than simpler models
    up to a certain point. After engineering and training the new model, you computed
    the R2 score and MAE, which you can use to compare this model with the model you
    trained previously. We can conclude that this model is better as it has a higher
    R2 score and a lower MAE.
  prefs: []
  type: TYPE_NORMAL
- en: Other Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we made use of `mean_absolute_error`, there are other model evaluation
    functions for regression. Recall that these are all cost (or loss) functions.
    These include `max_error`, `mean_squared_error`, `mean_squared_log_error`, and
    `median_absolute_error`. If you are working on a project with a data scientist,
    they will normally be responsible for telling you what evaluation metric to make
    use of. If not, then you can choose any metric of your liking.
  prefs: []
  type: TYPE_NORMAL
- en: The MAE is computed by subtracting every prediction from the ground truth, finding the
    absolute value, summing all the absolute values, and dividing by the number of
    observations. This type of distance measure is called Manhattan distance in data
    mining.
  prefs: []
  type: TYPE_NORMAL
- en: The **mean squared error** (**MSE**) is computed by taking the squares of the
    differences between the ground truths and the predictions, summing them, and then
    dividing by the number of observations. The MSE is large, and sometimes the square
    root of this is used, which is the **root mean squared error** (**RMSE**).
  prefs: []
  type: TYPE_NORMAL
- en: The **mean squared logarithmic error** (**MSLE**) introduces logarithms into
    the equation by adding one to both the ground truth and the prediction before
    taking the logarithms, then squaring the differences, then summing them, and dividing
    by the number of observations. MSLE has the property of having a lower cost for
    predictions that are above the ground truth than for those that are below it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `median_absolute_error` finds the median value of the absolute errors,
    which are the differences between the ground truths and the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move toward evaluating the performance of classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Model Performance for Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification models are used for predicting which class a group of features
    will fall under. You learned to create binary classification models in *Chapter
    3*, *Binary Classification*, and multi-class classification models in *Chapter
    4, Multiclass Classification with RandomForest*.
  prefs: []
  type: TYPE_NORMAL
- en: When you consider a classification model, you might start to ask yourself how
    accurate the model is. But how do you evaluate accuracy?
  prefs: []
  type: TYPE_NORMAL
- en: You need to create a classification model before you can start assessing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.05: Creating a Classification Model for Computing Evaluation Metrics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will create a classification model that you will make
    use of later on for model assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will make use of the cars dataset from the UCI Machine Learning Repository.
    You will use this dataset to classify cars as either acceptable or unacceptable
    based on the following categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buying`: the purchase price of the car'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maint`: the maintenance cost of the car'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doors`: the number of doors on the car'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persons`: the carrying capacity of the vehicle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lug_boot`: the size of the luggage boot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety`: the estimated safety of the car'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can find the dataset here: [https://packt.live/30I594E](https://packt.live/30I594E).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following steps will help you achieve the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the libraries you will need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `pandas` and alias it as `pd`. `pandas` is needed for
    reading data into a DataFrame. You also import `train_test_split`, which is needed
    for splitting your data into training and evaluation datasets. Finally, you also
    import the `LogisticRegression` class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import your data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you create a Python list called `_headers` to hold the names of
    the columns in the file you will be importing because the file doesn't have a
    header. You  then proceed to read the file into a DataFrame named `df` by using
    `pd.read_csv` and specifying the file location as well as the list containing
    the file headers. Finally, you display the first five rows using `df.head()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22: Inspecting the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.22: Inspecting the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Encode categorical variables as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you convert categorical columns into numeric columns using a technique
    called one-hot encoding. You saw an example of this in *Step 13* of *Exercise
    3.04*, *Feature Engineering – Creating New Features from Existing Ones*. You need
    to do this because the inputs to your model must be numeric. You get numeric variables
    from categorical variables using `get_dummies` from the `pandas` library. You
    provide your DataFrame as input and specify the columns to be encoded. You assign
    the result to a new DataFrame called `_df`, and then inspect the result using
    `head()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output should now resemble the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.23: Encoding categorical variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.23: Encoding categorical variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output has been truncated for presentation purposes. Please find the complete
    output at [https://packt.live/3aBNlg7](https://packt.live/3aBNlg7).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you begin by extracting your feature columns and your labels into
    two NumPy arrays called `features` and `labels`. You then proceed to extract 70%
    into `X_train` and `y_train`, with the remaining 30% going into `X_eval` and `y_eval`.
    You then further split `X_eval` and `y_eval` into two equal parts and assign those
    to `X_val` and `y_val` for validation, and `X_test` and `y_test` for testing much
    later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train a logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you create an instance of `LogisticRegression` and train the model
    on your training data by passing in `X_train` and `y_train` to the `fit` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output that looks similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.24: Training a logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.24: Training a logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you make a prediction on the validation dataset, `X_val`, and
    store the result in `y_pred`. A look at the first 10 predictions (by executing
    `y_pred[0:9]`) should provide an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.25: Prediction for the validation set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.25: Prediction for the validation set'
  prefs: []
  type: TYPE_NORMAL
- en: This model works because you are able to use it to make predictions. The predictions
    classify each car as acceptable (`acc`) or unacceptable (`unacc`) based on the
    features of the car. At this point, you are ready to apply various assessments
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3aBNlg7](https://packt.live/3aBNlg7).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/34eg7CH](https://packt.live/34eg7CH).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have successfully created a classification model to make predictions,
    and we will assess the performance of the model in future exercises.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we trained this logistic regression model once so that we
    don't need to do it repeatedly because of the number of steps involved. In the
    next section, you will be looking at confusion matrices.
  prefs: []
  type: TYPE_NORMAL
- en: The Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You encountered the confusion matrix in *Chapter 3, Binary Classification*.
    You may recall that the confusion matrix compares the number of classes that the
    model predicted against the actual occurrences of those classes in the validation
    dataset. The output is a square matrix that has the number of rows and columns
    equal to the number of classes you are predicting. The columns represent the actual
    values, while the rows represent the predictions. You get a confusion matrix by
    using `confusion_matrix` from `sklearn.metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.06: Generating a Confusion Matrix for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to create a confusion matrix for the classification
    model you trained in *Exercise 6.05*, *Creating a Classification Model for Computing
    Evaluation Metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05*, *Creating a Classification Model for Computing Evaluation Metrics*, and
    then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you achieve the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `confusion_matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `confusion_matrix` from `sklearn.metrics`. This function
    will let you generate a confusion matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you generate a confusion matrix by supplying `y_val`, the actual
    classes, and `y_pred`, the predicted classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output should look similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.26: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.26: Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hbreQz](https://packt.live/3hbreQz).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2EebSMD](https://packt.live/2EebSMD).
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our data has four classes. The first column shows all of the
    data that should belong to the first class. The first row shows the number of
    predictions that were correctly placed in the first class. In this example, that
    number is `41`. The second row shows the number of predictions that were placed
    in the second class but should have been in the first class. In this example,
    that number is `7`. In the third row, you see the number of items that were predicted
    to be in the third class but should have been in the first class. That number
    is `7`. Finally, in the fourth row, you see the number of items that were wrongly
    classified into the fourth class when they should have been in the first class.
    In this case, the number is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: More on the Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The confusion matrix helps you analyze the impact of the choices you would have
    to make if you put the model into production. Let's consider the example of predicting
    the presence of a disease based on the inputs to the model. This is a binary classification
    problem, where 1 implies that the disease is present and 0 implies the disease
    is absent. The confusion matrix for this model would have two columns and two
    rows.
  prefs: []
  type: TYPE_NORMAL
- en: The first column would show the items that fall into class `true negatives`.
    The second row would show the items that were wrongly classified as `false positives`.
  prefs: []
  type: TYPE_NORMAL
- en: The second column would show the items that fall into class `false negatives`.
    Finally, the second row shows items that were correctly classified into class
    1 and are called `true positives`.
  prefs: []
  type: TYPE_NORMAL
- en: False positives are the cases in which the samples were wrongly predicted to
    be infected when they are actually healthy. The implication of this is that these
    cases would be treated for a disease that they do not have.
  prefs: []
  type: TYPE_NORMAL
- en: False negatives are the cases that were wrongly predicted to be healthy when
    they actually have the disease. The implication of this is that these cases would
    not be treated for a disease that they actually have.
  prefs: []
  type: TYPE_NORMAL
- en: The question you need to ask about this model depends on the nature of the disease
    and requires domain expertise about the disease. For example, if the disease is
    contagious, then the untreated cases will be released into the general population
    and could infect others. What would be the implication of this versus placing
    cases into quarantine and observing them for symptoms?
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the disease is not contagious, the question becomes that
    of the implications of treating people for a disease they do not have versus the
    implications of not treating cases of a disease.
  prefs: []
  type: TYPE_NORMAL
- en: It should be clear that there isn't a definite answer to these questions. The
    model would need to be tuned to provide performance that is acceptable to the
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Precision was introduced in *Chapter 3, Binary Classification*; however, we
    will be looking at it in more detail in this chapter. The precision is the total
    number of cases that were correctly classified as positive (called **true positive**
    and abbreviated as **TP**) divided by the total number of cases in that prediction
    (that is, the total number of entries in the row, both correctly classified (TP)
    and wrongly classified (FP) from the confusion matrix). Suppose 10 entries were
    classified as positive. If 7 of the entries were actually positive, then TP would
    be 7 and FP would be 3\. The precision would, therefore, be 0.7\. The equation
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27: Equation for precision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.27: Equation for precision'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tp` is true positive – the number of predictions that were correctly classified
    as belonging to that class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fp` is false positive – the number of predictions that were wrongly classified
    as belonging to that class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function in `sklearn.metrics` to compute precision is called `precision_score`.
    Go ahead and give it a try.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 6.07: Computing Precision for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will be computing the precision for the classification
    model you trained in *Exercise 6.05*, *Creating a Classification Model for Computing
    Evaluation Metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05*, *Creating a Classification Model for Computing Evaluation Metrics*, and
    then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you achieve the task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `precision_score` from `sklearn.metrics`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, compute the precision score as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute the precision score using `precision_score`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is a floating-point number between 0 and 1\. It might look like
    this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.28: Precision score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.28: Precision score'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The precision score can vary depending on the data.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you see the precision score for the classification model is
    `0.9245`. **92%** could be a good score and is acceptable in some domains, but
    is a low score in certain domains. So there is scope for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3kROW6R](https://packt.live/3kROW6R).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3aCS8ye](https://packt.live/3aCS8ye).
  prefs: []
  type: TYPE_NORMAL
- en: Think of the precision score as asking how often does this model make the correct
    prediction for a class? The value needs to be much closer to 1 than the score
    we just achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall is the total number of predictions that were true divided by the number
    of predictions for the class, both true and false. Think of it as the true positive
    divided by the sum of entries in the column. The equation is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29: Equation for recall'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.29: Equation for recall'
  prefs: []
  type: TYPE_NORMAL
- en: The function for this is `recall_score`, which is available from `sklearn.metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.08: Computing Recall for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to compute the recall for the classification model
    you trained in *Exercise 6.05*, *Creating a Classification Model for Computing
    Evaluation Metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05*, *Creating a Classification Model for Computing Evaluation Metrics*, and
    then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `recall_score` from `sklearn.metrics`. This is the
    function that you will make use of in the second step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the recall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute the recall by using `recall_score`. You need to specify
    `y_val` and `y_pred` as parameters to the function. The documentation for `recall_score`
    explains the values that you can supply to `average`. If your model does binary
    prediction and the labels are `0` and `1`, you can set `average` to `binary`.
    Other options are `micro`, `macro`, `weighted`, and `samples`. You should read
    the documentation to see what they do.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output that looks like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.30: Recall score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.30: Recall score'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The recall score can vary, depending on the data.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have calculated the recall score in the exercise, which is
    `0.622`. This means that of the total number of classes that were predicted, `62%`
    of them were correctly predicted. On its own, this value might not mean much until
    it is compared to the recall score from another model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31axPp6](https://packt.live/31axPp6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YdNv8O](https://packt.live/2YdNv8O).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move toward calculating the F1 score, which also helps greatly in
    evaluating the model performance, which in turn aids in making better decisions
    when choosing models.
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The F1 score is another important parameter that helps us to evaluate the model
    performance. It considers the contribution of both precision and recall using
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31: F1 score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.31: F1 score'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score ranges from 0 to 1, with 1 being the best possible score. You compute
    the F1 score using `f1_score` from `sklearn.metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.09: Computing the F1 Score for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will compute the F1 score for the classification model
    you trained in *Exercise 6.05*, *Creating a Classification Model for Computing
    Evaluation Metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05*, *Creating a Classification Model for Computing Evaluation Metrics*, and
    then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import the `f1_score` method from `sklearn.metrics`. This
    score will let you compute evaluation metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the F1 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute the F1 score by passing in `y_val` and `y_pred`. You
    also specify `average='macro'` because this is not binary classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.32: F1 score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.32: F1 score'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iWCqkq](https://packt.live/3iWCqkq).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Q84epY](https://packt.live/2Q84epY).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this exercise, you will see that the `F1` score we achieved is
    `0.6746`. There is a lot of room for improvement, and you would engineer new features
    and train a new model to try and get a better F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accuracy is an evaluation metric that is applied to classification models. It
    is computed by counting the number of labels that were correctly predicted, meaning
    that the predicted label is exactly the same as the ground truth. The `accuracy_score()`
    function exists in `sklearn.metrics` to provide this value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.10: Computing Model Accuracy for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to compute the accuracy score of the model trained
    in *Exercise 6.04*, *Computing the Mean Absolute Error of a Second Model*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05*, *Creating a Classification Model for Computing Evaluation Metrics*, and
    then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Continue from where the code for *Exercise 6.05*, *Creating a Classification
    Model for Computing Evaluation Metrics*, ends in your notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `accuracy_score()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `accuracy_score()`, which you will use to compute the
    model accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute the model accuracy by passing in `y_val` and `y_pred`
    as parameters to `accuracy_score()`. The interpreter assigns the result to a variable
    called `c`. The `print()` method causes the interpreter to render the value of
    `_accuracy`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The result is similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.33 Accuracy score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.33 Accuracy score
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Q6K5Ao](https://packt.live/2Q6K5Ao).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Ye55JT](https://packt.live/2Ye55JT).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have successfully calculated the accuracy of the model as being `0.876`.
    The goal of this exercise is to show you how to compute the accuracy of a model
    and to compare this accuracy value to that of another model that you will train
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic Loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logarithmic loss (or log loss) is the loss function for categorical models.
    It is also called categorical cross-entropy. It seeks to penalize incorrect predictions.
    The `sklearn` documentation defines it as "the negative log-likelihood of the
    true values given your model predictions."
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.11: Computing the Log Loss for the Classification Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to predict the log loss of the model trained in
    *Exercise 6.05*, *Creating a Classification Model for Computing Evaluation Metrics*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.05, Creating a Classification Model for Computing Evaluation Metrics.* If you
    wish to use a new notebook, make sure you copy and run the entire code from *Exercise
    6.05* and then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your Colab notebook and continue from where *Exercise 6.05*, *Creating
    a Classification Model for Computing Evaluation Metrics*, stopped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `log_loss()` from `sklearn.metrics`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the log loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you compute the log loss and store it in a variable called `_loss`.
    You need to observe something very important: previously, you made use of `y_val`,
    the ground truths, and `y_pred`, the predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, you do not make use of predictions. Instead, you make use of predicted
    probabilities. You see that in the code where you specify `model.predict_proba()`.
    You specify the validation dataset and it returns the predicted probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The `print()` function causes the interpreter to render the log loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'This should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.34: Log loss output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_06_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.34: Log loss output'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The value of loss can vary for different data.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Q5plZR](https://packt.live/2Q5plZR).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/34eMIsm](https://packt.live/34eMIsm).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have successfully calculated the `log_loss` for a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver Operating Characteristic Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall the True Positive Rate, which we discussed earlier. It is also called
    **sensitivity**. Also recall that what we try to do with a logistic regression
    model is find a threshold value such that above that threshold value, we predict
    that our input falls into a certain class, and below that threshold, we predict
    that it doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: The Receiver Operating Characteristic (ROC) curve is a plot that shows how the
    true positive and false positive rates vary for a model as the threshold is changed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's do an exercise to enhance our understanding of the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.12: Computing and Plotting ROC Curve for a Binary Classification
    Problem'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to plot the ROC curve for a binary classification
    problem. The data for this problem is used to predict whether or not a mother
    will require a caesarian section to give birth.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that you will be using in this chapter can be found in our GitHub
    repository: [https://packt.live/36dyEg5](https://packt.live/36dyEg5).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the UCI Machine Learning Repository, the abstract for this dataset follows:
    "This dataset contains information about caesarian section results of 80 pregnant
    women with the most important characteristics of delivery problems in the medical
    field." The attributes of interest are age, delivery number, delivery time, blood
    pressure, and heart status.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish this task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `pandas`, which you will use to read in data. You also
    import `train_test_split` for creating training and validation datasets, and `LogisticRegression`
    for creating a model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you read in your data. The dataset has an interesting format.
    The bottom part contains the data in CSV format, but the upper part contains some
    file descriptors. If you download and open the file from [https://packt.live/38qJe4A](https://packt.live/38qJe4A)
    and open the file using Notepad, you will see the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.35: Reading the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.35: Reading the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will need to do a few things to work with this file. Skip 15 rows and specify
    the column headers and read the file without an index.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code shows how you do that by creating a Python list to hold your column
    headers and then read in the file using `read_csv()`. The parameters that you
    pass in are the file's location, the column headers as a Python list, the name
    of the index column (in this case, it is None), and the number of rows to skip.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `head()` method will print out the top five rows and should look similar
    to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.36: The top five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.36: The top five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you begin by creating two `numpy` arrays, which you call `features`
    and `labels`. You then split these arrays into a `training` and an `evaluation`
    dataset. You further split the `evaluation` dataset into `validation` and `test`
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, train and fit a logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you begin by creating an instance of a logistic regression model.
    You then proceed to train or fit the model on the training dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.37: Training a logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.37: Training a logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict the probabilities, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, the model predicts the probabilities for each entry in the validation
    dataset. It stores the results in `y_proba`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the true positive rate, the false positive rate, and the thresholds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you make a call to `roc_curve()` and specify the ground truth
    and the first column of the predicted probabilities. The result is a tuple of
    false positive rate, true positive rate, and thresholds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Explore the false positive rates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you instruct the interpreter to print out the false positive
    rate. The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.38: False positive rates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.38: False positive rates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The false positive rates can vary, depending on the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Explore the true positive rates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you instruct the interpreter to print out the true positive rates.
    This should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.39: True positive rates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.39: True positive rates'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Explore the thresholds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you instruct the interpreter to display the thresholds. The output
    should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.40: Thresholds'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.40: Thresholds'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, plot the ROC curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you import `matplotlib.pyplot` as your plotting library. You alias
    it as `plt`. You then proceed to plot a line chart by specifying the false positive
    rates and true positive rates. The rest of the code decorates the chart with a
    title and labels for the horizontal and vertical axes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output should look similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.41: ROC curve'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.41: ROC curve'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/322jiLa](https://packt.live/322jiLa).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/324ii9s](https://packt.live/324ii9s).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you learned to plot how the true positive rate and false positive
    rate of the model vary as you change the prediction threshold. Recall that what
    the model does is output a value between 0 and 1\. This value is called a logit.
    Your job as a data scientist is to decide on a threshold value, for example, 0.5\.
    If the logit is above that threshold, you predict that the input falls into one
    class (positive, if it is a positive-or-negative prediction). If the logit is
    below the threshold, you will predict that the input belongs to the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if your threshold is 0.5, then a logit of 0.33 is predicted as
    negative, while a logit of 0.80 is predicted as positive.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your threshold is 0.95, then a logit of 0.33 is predicted as negative,
    and a logit of 0.80 is still predicted as negative.
  prefs: []
  type: TYPE_NORMAL
- en: Now, recall that what you want your model to do is correctly classify as many
    data points as possible. Classification is controlled by your chosen threshold
    value. The logit (predicted probability) from the model will always be the same,
    but the class assigned to the prediction will depend on the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: As you vary the threshold, the predictions change, and the number of true positives
    and true negatives changes.
  prefs: []
  type: TYPE_NORMAL
- en: The RoC shows you how the percentage of true positives and true negatives changes
    as the threshold varies from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the threshold, the more confident the model needs to be before you
    classify a prediction as positive. Recall that the logit is the probability that
    the input belongs to a class and is a confidence score from 0 to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Area Under the ROC Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Area Under the Receiver Operating Characteristic Curve** (**ROC AUC**)
    is a measure of the likelihood that the model will rank a randomly chosen positive
    example higher than a randomly chosen negative example. Another way of putting
    it is to say that the higher this measure is, the better the model is at predicting
    a negative class as negative, and a positive class as positive. The value ranges
    from 0 to 1\. If the AUC is 0.6, it means that the model has a 60% probability
    of correctly distinguishing a negative class from a positive class based on the
    inputs. This measure is used to compare models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.13: Computing the ROC AUC for the Caesarian Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to compute the ROC AUC for the binary classification
    model that you trained in *Exercise 6.12*, *Computing and Plotting ROC Curve for
    a Binary Classification Problem*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should continue this exercise in the same notebook as that used in *Exercise
    6.12, Computing and Plotting ROC Curve for a Binary Classification Problem.* If
    you wish to use a new notebook, make sure you copy and run the entire code from
    *Exercise 6.12* and then begin with the execution of the code of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Colab notebook to the code for *Exercise 6.12*, *Computing and Plotting
    ROC Curve for a Binary Classification Problem,* and continue writing your code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Predict the probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, you compute the probabilities of the classes in the validation
    dataset. You store the result in `y_proba`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute the ROC AUC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you compute the ROC AUC and store the result in `_auc`. You then
    proceed to print this value out. The result should look similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.42: Computing the ROC AUC'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.42: Computing the ROC AUC'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The AUC can be different, depending on the data.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/32jCrIT](https://packt.live/32jCrIT).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/319zoDy](https://packt.live/319zoDy).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you learned to compute the ROC AUC, which is the measure of
    the likelihood that the model will rank a randomly chosen positive example higher
    than a randomly chosen negative example. In this example, the AUC is 0.1944, and
    there is room for improvement with this model.
  prefs: []
  type: TYPE_NORMAL
- en: When you are done selecting a model, you might be interested in saving it for
    use in the future. The next topic explores saving and loading models.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will eventually need to transfer some of the models you have trained to
    a different computer so they can be put into production. There are various utilities
    for doing this, but the one we will discuss is called `joblib`.
  prefs: []
  type: TYPE_NORMAL
- en: '`joblib` supports saving and loading models, and it saves the models in a format
    that is supported by other machine learning architectures, such as `ONNX`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`joblib` is found in the `sklearn.externals` module.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.14: Saving and Loading a Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will train a simple model and use it for prediction. You
    will then proceed to save the model and then load it back in. You will use the
    loaded model for a second prediction, and then compare the predictions from the
    first model to those from the second model. You will make use of the car dataset
    for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you toward the goal:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.43: Inspecting the first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.43: Inspecting the first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into `features` and `labels`, and into training and validation
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.44: Training a linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.44: Training a linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training data to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the model for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `joblib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.45: Saving the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.45: Saving the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load it as a new model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the new model for predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compare the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.46: Comparing predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_06_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.46: Comparing predictions'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/322VxTb](https://packt.live/322VxTb).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3aAUz4q](https://packt.live/3aAUz4q).
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the predictions from the model before it was saved are exactly
    the same as the predictions from the model after it was saved and loaded back
    in. It is safe to conclude that saving and loading models does not affect their
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you learned how to save and load models. You also checked
    and confirmed that the model predictions remain the same even when you save and
    load them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Train Three Different Models and Use Evaluation Metrics to Pick
    the Best Performing Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You work as a data scientist at a bank. The bank would like to implement a model
    that predicts the likelihood of a customer purchasing a term deposit. The bank
    provides you with a dataset, which is the same as the one in *Chapter 3*, *Binary
    Classification*. You have previously learned how to train a logistic regression
    model for binary classification. You have also heard about other non-parametric
    modeling techniques and would like to try out a decision tree as well as a random
    forest to see how well they perform against the logistic regression models you
    have been training.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you will train a logistic regression model and compute a classification
    report. You will then proceed to train a decision tree classifier and compute
    a classification report. You will compare the models using the classification
    reports. Finally, you will train a random forest classifier and generate the classification
    report. You will then compare the logistic regression model with the random forest
    using the classification reports to determine which model you should put into
    production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to accomplish this task are:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert categorical variables using `pandas.get_dummies()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the `X` and `y` variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and evaluation sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of `LogisticRegression`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the training data to the `LogisticRegression` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the evaluation set to make a prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the prediction from the `LogisticRegression` model to compute the classification
    report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an instance of `DecisionTreeClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the training data to the `DecisionTreeClassifier` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the `DecisionTreeClassifier` model, make a prediction on the evaluation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the prediction from the `DecisionTreeClassifier` model to compute the classification
    report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will be studying decision trees in detail in *Chapter 7, The Generalization
    of Machine Learning Models*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compare the classification report from the linear regression model and the classification
    report from the decision tree classifier to determine which is the better model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of `RandomForestClassifier`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the training data to the `RandomForestClassifier` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `RandomForestClassifier` model, make a prediction on the evaluation dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the prediction from the random forest classifier, compute the classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the classification report from the linear regression model with the
    classification report from the random forest classifier to decide which model
    to keep or improve upon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare the R2 scores of all three models. The output should be similar to the following:![Figure
    6.47: Comparing the R2 scores'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15019_06_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.47: Comparing the R2 scores'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found at the following address: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we observed that some of the evaluation metrics for classification
    models require a binary classification model. We saw that when we worked with
    more than two classes, we were required to use the one-versus-all approach. The
    one-versus-all approach builds one model for each class and tries to predict the
    probability that the input belongs to a specific class. We saw that once this
    was done, we then predicted that the input belongs to the class where the model
    has the highest prediction probability. We also split our evaluation dataset into
    two, it's because `X_test` and `y_test` are used once for a final evaluation of
    the model's performance. You can make use of them before putting your model into
    production to see how the model would perform in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: You have learned how to assess the quality of a regression model by observing
    how the loss changes. You saw examples using the MAE, and also learned of the
    existence of MSE. You also learned about how to assess the quality of classification
    models in the activity. In the next chapter, you will learn how to train multiple
    models using cross-validation and also implement regularization techniques.
  prefs: []
  type: TYPE_NORMAL
