<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer312" class="Content">
			<p class="chapter-number">7</p>
		</div>
		<div id="_idContainer313" class="Content">
			<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>7. The Generalization of Machine Learning Models</h1>
		</div>
		<div id="_idContainer394" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will teach you how to make use of the data you have to train better models by either splitting your data if it is sufficient or making use of cross-validation if it is not. By the end of this chapter, you will know how to split your data into training, validation, and test datasets. You will be able to identify the ratio in which data has to be split and also consider certain features while splitting. You will also be able to implement cross-validation to use limited data for testing and use regularization to reduce overfitting in models.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor172"/>Introduction</h1>
			<p>In the previous chapter, you learned about model assessment using various metrics such as R2 score, MAE, and accuracy. These metrics help you decide which models to keep and which ones to discard. In this chapter, you will learn some more techniques for training better models.</p>
			<p>Generalization deals with getting your models to perform well enough on data points that they have not encountered in the past (that is, during training). We will address two specific areas:</p>
			<ul>
				<li>How to make use of as much of your data as possible to train a model</li>
				<li>How to reduce overfitting in a model</li>
			</ul>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor173"/>Overfitting</h1>
			<p>A model is said to overfit the training data when it generates a hypothesis that accounts for every example. What this means is that it correctly predicts the outcome of every example. The problem with this scenario is that the model equation becomes extremely complex, and such models have been observed to be incapable of correctly predicting new observations. </p>
			<p>Overfitting occurs when a model has been over-engineered. Two of the ways in which this could occur are:</p>
			<ul>
				<li>The model is trained on too many features.</li>
				<li>The model is trained for too long.</li>
			</ul>
			<p>We'll discuss each of these two points in the following sections.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/>Training on Too Many Features</h2>
			<p>When a model trains on too many features, the hypothesis becomes extremely complicated. Consider a case in which you have one column of features and you need to generate a hypothesis. This would be a simple linear equation, as shown here:</p>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="Images/B15019_07_01.jpg" alt="Figure 7.1: Equation for a hypothesis for a line&#13;&#10;" width="1665" height="59"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1: Equation for a hypothesis for a line</p>
			<p>Now, consider a case in which you have two columns, and in which you cross the columns by multiplying them. The hypothesis becomes the following:</p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="Images/B15019_07_02.jpg" alt="Figure 7.2: Equation for a hypothesis for a curve&#13;&#10;" width="1665" height="79"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2: Equation for a hypothesis for a curve</p>
			<p>While the first equation yields a line, the second equation yields a curve, because it is now a quadratic equation. But the same two features could become even more complicated depending on how you engineer your features. Consider the following equation: </p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="Images/B15019_07_03.jpg" alt="Figure 7.3: Cubic equation for a hypothesis&#13;&#10;" width="1514" height="96"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3: Cubic equation for a hypothesis</p>
			<p>The same set of features has now given rise to a cubic equation. This equation will have the property of having a large number of weights, for example:</p>
			<ul>
				<li>The simple linear equation has one weight and one bias.</li>
				<li>The quadratic equation has three weights and one bias.</li>
				<li>The cubic equation has five weights and one bias.</li>
			</ul>
			<p>One solution to overfitting as a result of too many features is to eliminate certain features. The technique for this is called lasso regression.</p>
			<p>A second solution to overfitting as a result of too many features is to provide more data to the model. This might not always be a feasible option, but where possible, it is always a good idea to do so.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>Training for Too Long</h2>
			<p>The model starts training by initializing the vector of weights such that all values are equal to zero. During training, the weights are updated according to the gradient update rule. This systematically adds or subtracts a small value to each weight. As training progresses, the magnitude of the weights increases. If the model trains for too long, these model weights become too large.</p>
			<p>The solution to overfitting as a result of large weights is to reduce the magnitude of the weights to as close to zero as possible. The technique for this is called ridge regression.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Underfitting</h1>
			<p>Consider an alternative situation in which the data has 10 features, but you only make use of 1 feature. Your model hypothesis would still be the following:</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="Images/B15019_07_04.jpg" alt="Figure 7.4: Equation for a hypothesis for a line&#13;&#10;" width="1588" height="81"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4: Equation for a hypothesis for a line</p>
			<p>However, that is the equation of a straight line, but your model is probably ignoring a lot of information. The model is over-simplified and is said to underfit the data.</p>
			<p>The solution to underfitting is to provide the model with more features, or conversely, less data to train on; but more features is the better approach.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Data</h1>
			<p>In the world of machine learning, the data that you have is not used in its entirety to train your model. Instead, you need to separate your data into three sets, as mentioned here:</p>
			<ul>
				<li>A training dataset, which is used to train your model and measure the training loss.</li>
				<li>An evaluation or validation dataset, which you use to measure the validation loss of the model to see whether the validation loss continues to reduce as well as the training loss.</li>
				<li>A test dataset for final testing to see how well the model performs before you put it into production.</li>
			</ul>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>The Ratio for Dataset Splits</h2>
			<p>The evaluation dataset is set aside from your entire training data and is never used for training. There are various schools of thought around the particular ratio that is set aside for evaluation, but it generally ranges from a high of 30% to a low of 10%. This evaluation dataset is normally further split into a validation dataset that is used during training and a test dataset that is used at the end for a sanity check. If you are using 10% for evaluation, you might set 5% aside for validation and the remaining 5% for testing. If using 30%, you might set 20% aside for validation and 10% for testing.</p>
			<p>To summarize, you might split your data into 70% for training, 20% for validation, and 10% for testing, or you could split your data into 80% for training, 15% for validation, and 5% for test. Or, finally, you could split your data into 90% for training, 5% for validation, and 5% for testing.</p>
			<p>The choice of what ratio to use is dependent on the amount of data that you have. If you are working with 100,000 records, for example, then 20% validation would give you 20,000 records. However, if you were working with 100,000,000 records, then 5% would give you 5 million records for validation, which would be more than sufficient.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/>Creating Dataset Splits</h2>
			<p>At a very basic level, splitting your data involves random sampling. Let's say you have 10 items in a bowl. To get 30% of the items, you would reach in and take any 3 items at random. </p>
			<p>In the same way, because you are writing code, you could do the following:</p>
			<ol>
				<li>Create a Python list.</li>
				<li>Place 10 numbers in the list.</li>
				<li>Generate 3 non-repeating random whole numbers from 0 to 9.</li>
				<li>Pick items whose indices correspond to the random numbers previously generated.<div id="_idContainer318" class="IMG---Figure"><img src="Images/B15019_07_05.jpg" alt="Figure 7.5: Visualization of data splitting&#13;&#10;" width="1277" height="300"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.5: Visualization of data splitting</p>
			<p>This is something you will only do once for a particular dataset. You might write a function for it. If it is something that you need to do repeatedly and you also need to handle advanced functionality, you might want to write a class for it.</p>
			<p><strong class="source-inline">sklearn</strong> has a class called <strong class="source-inline">train_test_split</strong>, which provides the functionality for splitting data. It is available as <strong class="source-inline">sklearn.model_selection.train_test_split</strong>. This function will let you split a DataFrame into two parts.</p>
			<p>Have a look at the following exercise on importing and splitting data.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Exercise 7.01: Importing and Splitting Data</h2>
			<p>The goal of this exercise is to import data from a repository and to split it into a training and an evaluation set.</p>
			<p>We will be using the Cars dataset from the UCI Machine Learning Repository. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the dataset here: <a href="https://packt.live/2RE5rWi">https://packt.live/2RE5rWi</a></p>
			<p class="callout">The dataset can also be found on our GitHub, here: <a href="https://packt.live/36cvyc4">https://packt.live/36cvyc4</a></p>
			<p>You will be using this dataset throughout the exercises in this chapter.</p>
			<p>This dataset is about the cost of owning cars with certain attributes. The abstract from the website states: "<em class="italic">Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods</em>." Here are some of the key attributes of this dataset:</p>
			<p class="source-code">CAR car acceptability</p>
			<p class="source-code">. PRICE overall price</p>
			<p class="source-code">. . buying buying price</p>
			<p class="source-code">. . maint price of the maintenance</p>
			<p class="source-code">. TECH technical characteristics</p>
			<p class="source-code">. . COMFORT comfort</p>
			<p class="source-code">. . . doors number of doors</p>
			<p class="source-code">. . . persons capacity in terms of persons to carry</p>
			<p class="source-code">. . . lug_boot the size of luggage boot</p>
			<p class="source-code">. . safety estimated safety of the car</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import the necessary libraries:<p class="source-code"># import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p>In this step, you have imported <strong class="source-inline">pandas</strong> and aliased it as <strong class="source-inline">pd</strong>. As you know, <strong class="source-inline">pandas</strong> is required to read in the file. You also import <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn.model_selection</strong> to split the data into two parts.</p></li>
				<li>Before reading the file into your notebook, open and inspect the file (<strong class="source-inline">car.data</strong>) with an editor. You should see an output similar to the following:<div id="_idContainer319" class="IMG---Figure"><img src="Images/B15019_07_06.jpg" alt="Figure 7.6: Car data&#13;&#10;" width="794" height="612"/></div><p class="figure-caption">Figure 7.6: Car data</p><p>You will notice from the preceding screenshot that the file doesn't have a first row containing the headers.</p></li>
				<li>Create a Python list to hold the headers for the data:<p class="source-code"># data doesn't have headers, so let's create headers</p><p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p></li>
				<li>Now, import the data as shown in the following code snippet:<p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter07/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p><p>You then proceed to import the data into a variable called <strong class="source-inline">df</strong> by using <strong class="source-inline">pd.read_csv</strong>. You specify the location of the data file, as well as the list of column headers. You also specify that the data does not have a column index.</p></li>
				<li>Show the top five records:<p class="source-code">df.info()</p><p>In order to get information about the columns in the data as well as the number of records, you make use of the <strong class="source-inline">info()</strong> method. You should get an output similar to the following:</p><div id="_idContainer320" class="IMG---Figure"><img src="Images/B15019_07_07.jpg" alt="Figure 7.7: The top five records of the DataFrame&#13;&#10;" width="675" height="250"/></div><p> </p><p class="figure-caption">Figure 7.7: The top five records of the DataFrame</p><p>The <strong class="source-inline">RangeIndex</strong> value shows the number of records, which is <strong class="source-inline">1728</strong>.</p></li>
				<li>Now, you need to split the data contained in <strong class="source-inline">df</strong> into a training dataset and an evaluation dataset:<p class="source-code">#split the data into 80% for training and 20% for evaluation</p><p class="source-code">training_df, eval_df = train_test_split(df, train_size=0.8, \</p><p class="source-code">                                        random_state=0)</p><p>In this step, you make use of <strong class="source-inline">train_test_split</strong> to create two new DataFrames called <strong class="source-inline">training_df</strong> and <strong class="source-inline">eval_df</strong>. </p><p>You specify a value of <strong class="source-inline">0.8</strong> for <strong class="source-inline">train_size</strong> so that <strong class="source-inline">80%</strong> of the data is assigned to <strong class="source-inline">training_df</strong>.</p><p><strong class="source-inline">random_state</strong> ensures that your experiments are reproducible. Without <strong class="source-inline">random_state</strong>, the data is split differently every time using a different random number. With <strong class="source-inline">random_state</strong>, the data is split the same way every time. We will be studying <strong class="source-inline">random_state</strong> in depth in the next chapter.</p></li>
				<li>Check the information of <strong class="source-inline">training_df</strong>:<p class="source-code">training_df.info()</p><p>In this step, you make use of <strong class="source-inline">.info()</strong> to get the details of <strong class="source-inline">training_df</strong>. This will print out the column names as well as the number of records. </p><p>You should get an output similar to the following:</p><div id="_idContainer321" class="IMG---Figure"><img src="Images/B15019_07_08.jpg" alt="Figure 7.8: Information on training_df&#13;&#10;" width="595" height="230"/></div><p class="figure-caption">Figure 7.8: Information on training_df</p><p>You should observe that the column names match those in <strong class="source-inline">df</strong>, but you should have <strong class="source-inline">80%</strong> of the records that you did in <strong class="source-inline">df</strong>, which is <strong class="source-inline">1382</strong> out of <strong class="source-inline">1728</strong>.</p></li>
				<li>Check the information on <strong class="source-inline">eval_df</strong>:<p class="source-code">eval_df.info()</p><p>In this step, you print out the information about <strong class="source-inline">eval_df</strong>. This will give you the column names and the number of records. The output should be similar to the following:</p><div id="_idContainer322" class="IMG---Figure"><img src="Images/B15019_07_09.jpg" alt="Figure 7.9: Information on eval_df&#13;&#10;" width="626" height="229"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.9: Information on eval_df</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3294avL">https://packt.live/3294avL</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2E8FHhT">https://packt.live/2E8FHhT</a>.</p>
			<p>Now you know how to split your data. Whenever you split your data, the records are going to be exactly the same. You could repeat the exercise a number of times and notice the range of entries in the index for <strong class="source-inline">eval_df</strong>.</p>
			<p>The implication of this is that you cannot repeat your experiments. If you run the same code, you will get different results every time. Also, if you share your code with your colleagues, they will get different results. This is because the compiler makes use of random numbers. </p>
			<p>These random numbers are not actually random but make use of something called a pseudo-random number generator. The generator has a pre-determined set of random numbers that it uses, and as a result, you can specify a random state that will cause it to use a particular set of random numbers.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/>Random State</h1>
			<p>The key to reproducing the same results is called random state. You simply specify a number, and whenever that number is used, the same results will be produced. This works because computers don't have an actual random number generator. Instead, they have a pseudo-random number generator. This means that you can generate the same sequence of random numbers if you set a random state.</p>
			<p>Consider the following figure as an example. The columns are your random states. If you pick 0 as the random state, the following numbers will be generated: 41, 52, 38, 56…</p>
			<p>However, if you pick 1 as the random state, a different set of numbers will be generated, and so on.</p>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="Images/B15019_07_10.jpg" alt="Figure 7.10: Numbers generated using random state&#13;&#10;" width="566" height="499"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10: Numbers generated using random state</p>
			<p>In the previous exercise, you set the random state to 0 so that the experiment was repeatable.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/>Exercise 7.02: Setting a Random State When Splitting Data</h2>
			<p>The goal of this exercise is to have a reproducible way of splitting the data that you imported in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We going to refactor the code from the previous exercise. Hence, if you are using a new Colab notebook then make sure you copy the code from the previous exercise. Alternatively, you can make a copy of the notebook used in <em class="italic">Exercise 7.01</em> and use the revised the code as suggested in the following steps.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Continue from the previous <em class="italic">Exercise 7.01</em> notebook.</li>
				<li>Set the random state as <strong class="source-inline">1</strong> and split the data:<p class="source-code">"""</p><p class="source-code">split the data into 80% for training and 20% for evaluation </p><p class="source-code">using a random state</p><p class="source-code">"""</p><p class="source-code">training_df, eval_df = train_test_split(df, train_size=0.8, \</p><p class="source-code">                                        random_state=1)</p><p>In this step, you specify a <strong class="source-inline">random_state</strong> value of 1 to the <strong class="source-inline">train_test_split</strong> function.</p></li>
				<li>Now, view the top five records in <strong class="source-inline">training_df</strong>:<p class="source-code">#view the head of training_eval</p><p class="source-code">training_df.head()</p><p>In this step, you print out the first five records in <strong class="source-inline">training_df</strong>. </p><p>The output should be similar to the following:</p><div id="_idContainer324" class="IMG---Figure"><img src="Images/B15019_07_11.jpg" alt="Figure 7.11: The top five rows for the training evaluation set&#13;&#10;" width="1011" height="344"/></div><p class="figure-caption">Figure 7.11: The top five rows for the training evaluation set</p></li>
				<li>View the top five records in <strong class="source-inline">eval_df</strong>:<p class="source-code">#view the top of eval_df</p><p class="source-code">eval_df.head()</p><p>In this step, you print out the first five records in <strong class="source-inline">eval_df</strong>. </p><p>The output should be similar to the following:</p><div id="_idContainer325" class="IMG---Figure"><img src="Images/B15019_07_12.jpg" alt="Figure 7.12: The top five rows of eval_df&#13;&#10;" width="945" height="333"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.12: The top five rows of eval_df</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Q6Jb7e">https://packt.live/2Q6Jb7e</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2EjFvMp">https://packt.live/2EjFvMp</a>.</p>
			<p>The goal of this exercise is to get reproducible splits. If you run the code, you will get the same records in both <strong class="source-inline">training_df</strong> and <strong class="source-inline">eval_df</strong>. You may proceed to run that code a few times on every system and verify that you get the same records in both datasets.</p>
			<p>Whenever you change <strong class="source-inline">random_state</strong>, you will get a different set of training and validation data. </p>
			<p>But how do you find the best dataset split to train your model? When you don't have a lot of data, the recommended approach is to make use of all of your data. </p>
			<p>But how do you retain validation data if you make use of all of your data?</p>
			<p>The answer is to split the data into a number of parts. This approach is called cross-validation, which we will be looking at in the next section.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Cross-Validation</h1>
			<p>Consider an example where you split your data into five parts of 20% each. You would then make use of four parts for training and one part for evaluation. Because you have five parts, you can make use of the data five times, each time using one part for validation and the remaining data for training. </p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="Images/B15019_07_13.jpg" alt="Figure 7.13: Cross-validation&#13;&#10;" width="1559" height="405"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13: Cross-validation</p>
			<p>Cross-validation is an approach to splitting your data where you make multiple splits and then make use of some of them for training and the rest for validation. You then make use of all of the combinations of data to train multiple models. </p>
			<p>This approach is called n-fold cross-validation or k-fold cross-validation.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on k-fold cross-validation, refer to <a href="https://packt.live/36eXyfi">https://packt.live/36eXyfi</a>.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>KFold</h2>
			<p>The <strong class="source-inline">KFold</strong> class in <strong class="source-inline">sklearn.model_selection</strong> returns a generator that provides a tuple with two indices, one for training and another for testing or validation. A generator function lets you declare a function that behaves like an iterator, thus letting you use it in a loop.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor185"/>Exercise 7.03: Creating a Five-Fold Cross-Validation Dataset</h2>
			<p>The goal of this exercise is to create a five-fold cross-validation dataset from the data that you imported in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are using a new Colab notebook then make sure you copy the code from <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>. Alternatively, you can make a copy of the notebook used in <em class="italic">Exercise 7.01</em> and then use the code as suggested in the following steps.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Continue from the notebook file of <em class="italic">Exercise 7.01.</em></li>
				<li>Import all the necessary libraries:<p class="source-code">from sklearn.model_selection import KFold</p><p>In this step, you import <strong class="source-inline">KFold</strong> from <strong class="source-inline">sklearn.model_selection</strong>.</p></li>
				<li>Now create an instance of the class:<p class="source-code">_kf = KFold(n_splits=5)</p><p>In this step, you create an instance of <strong class="source-inline">KFold</strong> and assign it to a variable called <strong class="source-inline">_kf</strong>. You specify a value of <strong class="source-inline">5</strong> for the <strong class="source-inline">n_splits</strong> parameter so that it splits the dataset into five parts.</p></li>
				<li>Now split the data as shown in the following code snippet:<p class="source-code">indices = _kf.split(df)</p><p>In this step, you call the <strong class="source-inline">split</strong> method, which is <strong class="source-inline">.split()</strong> on <strong class="source-inline">_kf</strong>. The result is stored in a variable called <strong class="source-inline">indices</strong>.</p></li>
				<li>Find out what data type <strong class="source-inline">indices</strong> has:<p class="source-code">print(type(indices))</p><p>In this step, you inspect the call to split the output returns. </p><p>The output should be a <strong class="source-inline">generator</strong>, as seen in the following output:</p><div id="_idContainer327" class="IMG---Figure"><img src="Images/B15019_07_14.jpg" alt="Figure 7.14: Data type for indices&#13;&#10;" width="1508" height="76"/></div><p class="figure-caption">Figure 7.14: Data type for indices</p></li>
				<li>Get the first set of indices:<p class="source-code">#first set</p><p class="source-code">train_indices, val_indices = next(indices)</p><p>In this step, you make use of the <strong class="source-inline">next()</strong> Python function on the generator function. Using <strong class="source-inline">next()</strong> is the way that you get a generator to return results to you. You asked for five splits, so you can call <strong class="source-inline">next()</strong> five times on this particular generator. Calling <strong class="source-inline">next()</strong> a sixth time will cause the Python runtime to raise an exception.</p><p>The call to <strong class="source-inline">next()</strong> yields a tuple. In this case, it is a pair of indices. The first one contains your training indices and the second one contains your validation indices. You assign these to <strong class="source-inline">train_indices</strong> and <strong class="source-inline">val_indices</strong>.</p></li>
				<li>Create a training dataset as shown in the following code snippet:<p class="source-code">train_df = df.drop(val_indices)</p><p class="source-code">train_df.info()</p><p>In this step, you create a new DataFrame called <strong class="source-inline">train_df</strong> by dropping the validation indices from <strong class="source-inline">df</strong>, the DataFrame that contains all of the data. This is a subtractive operation similar to what is done in set theory. The <strong class="source-inline">df</strong> set is a union of <strong class="source-inline">train</strong> and <strong class="source-inline">val</strong>. Once you know what <strong class="source-inline">val</strong> is, you can work backward to determine <strong class="source-inline">train</strong> by subtracting <strong class="source-inline">val</strong> from <strong class="source-inline">df</strong>. If you consider <strong class="source-inline">df</strong> to be a set called <strong class="source-inline">A</strong>, <strong class="source-inline">val</strong> to be a set called <strong class="source-inline">B</strong>, and train to be a set called <strong class="source-inline">C</strong>, then the following holds true:</p><div id="_idContainer328" class="IMG---Figure"><img src="Images/B15019_07_15.jpg" alt="Figure 7.15: Dataframe A&#13;&#10;" width="1665" height="40"/></div><p class="figure-caption">Figure 7.15: Dataframe A</p><p>Similarly, set <strong class="source-inline">C</strong> can be the difference between set <strong class="source-inline">A</strong> and set <strong class="source-inline">B</strong>, as depicted in the following:</p><div id="_idContainer329" class="IMG---Figure"><img src="Images/B15019_07_16.jpg" alt="Figure 7.16: Dataframe C&#13;&#10;" width="1555" height="41"/></div><p class="figure-caption">Figure 7.16: Dataframe C</p><p>The way to accomplish this with a pandas DataFrame is to drop the rows with the indices of the elements of <strong class="source-inline">B</strong> from <strong class="source-inline">A</strong>, which is what you see in the preceding code snippet.</p><p>You can see the result of this by calling the <strong class="source-inline">info()</strong> method on the new DataFrame.</p><p>The result of that call should be similar to the following screenshot:</p><div id="_idContainer330" class="IMG---Figure"><img src="Images/B15019_07_17.jpg" alt="Figure 7.17: Information on the new dataframe&#13;&#10;" width="1222" height="538"/></div><p class="figure-caption">Figure 7.17: Information on the new dataframe</p></li>
				<li>Create a validation dataset:<p class="source-code">val_df = df.drop(train_indices)</p><p class="source-code">val_df.info()</p><p>In this step, you create the <strong class="source-inline">val_df</strong> validation dataset by dropping the training indices from the <strong class="source-inline">df</strong> DataFrame. Again, you can see the details of this new DataFrame by calling the <strong class="source-inline">info()</strong> method. </p><p>The output should be similar to the following:</p><div id="_idContainer331" class="IMG---Figure"><img src="Images/B15019_07_18.jpg" alt="Figure 7.18: Information for the validation dataset&#13;&#10;" width="1570" height="526"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.18: Information for the validation dataset</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3kRRaDf">https://packt.live/3kRRaDf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3kTNPnf">https://packt.live/3kTNPnf</a>.</p>
			<p>You could program all of the preceding in a loop so that you do not need to manually make a call to <strong class="source-inline">next()</strong> five times. This is what we will be doing in the next exercise.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>Exercise 7.04: Creating a Five-Fold Cross-Validation Dataset Using a Loop for Calls</h2>
			<p>The goal of this exercise is to create a five-fold cross-validation dataset from the data that you imported in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>. You will make use of a loop for calls to the generator function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are using a new Colab notebook then make sure you copy the code from <em class="italic">Exercise 7.01, Importing and Splitting Data</em>. Alternatively, you can make a copy of the notebook used in <em class="italic">Exercise 7.01</em> and then use the code as suggested in the following steps. The link to notebook for this exercise can be found here: <a href="https://packt.live/3g83AmU">https://packt.live/3g83AmU</a></p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook and repeat the steps you used to import data in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>.</li>
				<li>Define the number of splits you would like:<p class="source-code">from sklearn.model_selection import KFold</p><p class="source-code">#define number of splits</p><p class="source-code">n_splits = 5</p><p>In this step, you set the number of splits to <strong class="source-inline">5</strong>. You store this in a variable called <strong class="source-inline">n_splits</strong>.</p></li>
				<li>Create an instance of <strong class="source-inline">Kfold</strong>:<p class="source-code">#create an instance of KFold</p><p class="source-code">_kf = KFold(n_splits=n_splits)</p><p>In this step, you create an instance of <strong class="source-inline">Kfold</strong>. You assign this instance to a variable called <strong class="source-inline">_kf</strong>.</p></li>
				<li>Generate the split indices:<p class="source-code">#create splits as _indices</p><p class="source-code">_indices = _kf.split(df)</p><p>In this step, you call the <strong class="source-inline">split()</strong> method on <strong class="source-inline">_kf</strong>, which is the instance of <strong class="source-inline">KFold</strong> that you defined earlier. You provide <strong class="source-inline">df</strong> as a parameter so that the splits are performed on the data contained in the DataFrame called <strong class="source-inline">df</strong>. The resulting generator is stored as <strong class="source-inline">_indices</strong>.</p></li>
				<li>Create two Python lists:<p class="source-code">_t, _v = [], []</p><p>In this step, you create two Python lists. The first is called <strong class="source-inline">_t</strong> and holds the training DataFrames, and the second is called <strong class="source-inline">_v</strong> and holds the validation DataFrames.</p></li>
				<li>Iterate over the generator and create DataFrames called <strong class="source-inline">train_idx</strong>, <strong class="source-inline">val_idx</strong>, <strong class="source-inline">_train_df</strong> and <strong class="source-inline">_val_df</strong>:<p class="source-code">#iterate over _indices</p><p class="source-code">for i in range(n_splits):</p><p class="source-code">    train_idx, val_idx = next(_indices)</p><p class="source-code">    _train_df = df.drop(val_idx)</p><p class="source-code">    _t.append(_train_df)</p><p class="source-code">    _val_df = df.drop(train_idx)</p><p class="source-code">    _v.append(_val_df)</p><p>In this step, you create a loop using <strong class="source-inline">range</strong> to determine the number of iterations. You specify the number of iterations by providing <strong class="source-inline">n_splits</strong> as a parameter to <strong class="source-inline">range()</strong>. On every iteration, you execute <strong class="source-inline">next()</strong> on the <strong class="source-inline">_indices</strong> generator and store the results in <strong class="source-inline">train_idx</strong> and <strong class="source-inline">val_idx</strong>. You then proceed to create <strong class="source-inline">_train_df</strong> by dropping the validation indices, <strong class="source-inline">val_idx</strong>, from <strong class="source-inline">df</strong>. You also create <strong class="source-inline">_val_df</strong> by dropping the training indices from <strong class="source-inline">df</strong>.</p></li>
				<li>Iterate over the training list:<p class="source-code">for d in _t:</p><p class="source-code">    print(d.info())</p><p>In this step, you verify that the compiler created the DataFrames. You do this by iterating over the list and using the <strong class="source-inline">.info()</strong> method to print out the details of each element. The output is similar to the following screenshot, which is incomplete due to the size of the output. Each element in the list is a DataFrame with 1,382 entries:</p><div id="_idContainer332" class="IMG---Figure"><img src="Images/B15019_07_19.jpg" alt="Figure 7.19: Iterating over the training list&#13;&#10;" width="1075" height="682"/></div><p class="figure-caption">Figure 7.19: Iterating over the training list</p><p class="callout-heading">Note</p><p class="callout">The preceding output is a truncated version of the actual output.</p></li>
				<li>Iterate over the validation list:<p class="source-code">for d in _v:</p><p class="source-code">    print(d.info())</p><p>In this step, you iterate over the validation list and make use of <strong class="source-inline">.info()</strong> to print out the details of each element. The output is similar to the following screenshot, which is incomplete due to the size. Each element is a DataFrame with 346 entries:</p><div id="_idContainer333" class="IMG---Figure"><img src="Images/B15019_07_20.jpg" alt="Figure 7.20: Iterating over the validation list&#13;&#10;" width="611" height="532"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.20: Iterating over the validation list</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding output is a truncated version of the actual output.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3g83AmU">https://packt.live/3g83AmU</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3iXwEPR">https://packt.live/3iXwEPR</a>.</p>
			<p>In this exercise, you have learned how to use a loop for k-fold cross-validation to extract training and validation datasets. You can make use of these datasets to train and evaluate multiple models.</p>
			<p>The essence of creating cross-validation datasets is that you can train and evaluate multiple models. What if you didn't have to train those models in a loop? </p>
			<p>The good news is that you can avoid training multiple models in a loop because if you did that, you would need arrays to track lots of metrics.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor187"/>cross_val_score</h1>
			<p>The <strong class="source-inline">cross_val_score()</strong> function is available in <strong class="source-inline">sklearn.model_selection</strong>. Up until this point, you have learned how to create cross-validation datasets in a loop. If you made use of that approach, you would need to keep track of all of the models that you are training and evaluating inside of that loop. </p>
			<p><strong class="source-inline">cross_val_score</strong> takes care of the following:</p>
			<ul>
				<li>Creating cross-validation datasets</li>
				<li>Training models by fitting them to the training data</li>
				<li>Evaluating the models on the validation data</li>
				<li>Returning a list of the R2 score of each model that is trained</li>
			</ul>
			<p>For all of the preceding actions to happen, you will need to provide the following inputs:</p>
			<ul>
				<li>An instance of an estimator (for example, <strong class="source-inline">LinearRegression</strong>)</li>
				<li>The original dataset</li>
				<li>The number of splits to create (which is also the number of models that will be trained and evaluated)</li>
			</ul>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor188"/>Exercise 7.05: Getting the Scores from Five-Fold Cross-Validation</h2>
			<p>The goal of this exercise is to create a five-fold cross-validation dataset from the data that you imported in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>. You will then use <strong class="source-inline">cross_val_score</strong> to get the scores of models trained on those datasets.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are using a new Colab notebook then make sure you copy the code from <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>. Alternatively, you can make a copy of the notebook used in <em class="italic">Exercise 7.01</em> and then use the revised code as suggested in the following steps. The link to notebook for this exercise can be found here: <a href="https://packt.live/2DWTkAY">https://packt.live/2DWTkAY</a>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook and repeat <em class="italic">steps 1-6</em> that you took to import data in <em class="italic">Exercise 7.01</em>, <em class="italic">Importing and Splitting Data</em>.</li>
				<li>Encode the categorical variables in the dataset:<p class="source-code"># encode categorical variables</p><p class="source-code">_df = pd.get_dummies(df, columns=['buying', 'maint', 'doors', \</p><p class="source-code">                                  'persons', 'lug_boot', \</p><p class="source-code">                                  'safety'])</p><p class="source-code">_df.head()</p><p>In this step, you make use of <strong class="source-inline">pd.get_dummies()</strong> to convert categorical variables into an encoding. You store the result in a new DataFrame variable called <strong class="source-inline">_df</strong>. You then proceed to take a look at the first five records.</p><p>The result should look similar to the following:</p><div id="_idContainer334" class="IMG---Figure"><img src="Images/B15019_07_21.jpg" alt="Figure 7.21: Encoding categorical variables&#13;&#10;" width="1128" height="365"/></div><p class="figure-caption">Figure 7.21: Encoding categorical variables</p></li>
				<li>Split the data into features and labels:<p class="source-code"># separate features and labels DataFrames</p><p class="source-code">features = _df.drop(['car'], axis=1).values</p><p class="source-code">labels = _df[['car']].values</p><p>In this step, you create a <strong class="source-inline">features</strong> DataFrame by dropping <strong class="source-inline">car</strong> from <strong class="source-inline">_df</strong>. You also create <strong class="source-inline">labels</strong> by selecting only <strong class="source-inline">car</strong> in a new DataFrame. Here, a feature and a label are similar in the Cars dataset.</p></li>
				<li>Create an instance of the <strong class="source-inline">LogisticRegression</strong> class to be used later:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code"># create an instance of LogisticRegression</p><p class="source-code">_lr = LogisticRegression()</p><p>In this step, you import <strong class="source-inline">LogisticRegression</strong> from <strong class="source-inline">sklearn.linear_model</strong>. We use <strong class="source-inline">LogisticRegression</strong> because it lets us create a classification model, as you learned in <em class="italic">Chapter 3, Binary Classification</em>. You then proceed to create an instance and store it as <strong class="source-inline">_lr</strong>.</p></li>
				<li>Import the <strong class="source-inline">cross_val_score</strong> function:<p class="source-code">from sklearn.model_selection import cross_val_score</p><p>In this step now, you import <strong class="source-inline">cross_val_score</strong>, which you will make use of to compute the scores of the models.</p></li>
				<li>Compute the cross-validation scores:<p class="source-code">_scores = cross_val_score(_lr, features, labels, cv=5)</p><p>In this step, you the compute cross-validation scores and store the result in a Python list, which you call <strong class="source-inline">_scores</strong>. You do this using <strong class="source-inline">cross_cal_score</strong>. The function requires the following four parameters: the model to make use of (in our case, it's called <strong class="source-inline">_lr</strong>); the features of the dataset; the labels of the dataset; and the number of cross-validation splits to create (five, in our case).</p></li>
				<li>Now, display the scores as shown in the following code snippet:<p class="source-code">print(_scores)</p><p>In this step, you display the scores using <strong class="source-inline">print()</strong>. </p><p>The output should look similar to the following:</p><div id="_idContainer335" class="IMG---Figure"><img src="Images/B15019_07_22.jpg" alt="Figure 7.22: Printing the cross-validation scores&#13;&#10;" width="1438" height="56"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.22: Printing the cross-validation scores</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You may get slightly different outputs but the best score should belong to second split.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2DWTkAY">https://packt.live/2DWTkAY</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/34d5aS8">https://packt.live/34d5aS8</a>.</p>
			<p>In the preceding output, you see that the Python list stored in <strong class="source-inline">variable _scores</strong> contains five results. Each result is the R2 score of a <strong class="source-inline">LogisticRegression</strong> model. As mentioned before the exercise, the data will be split into five sets, and each combination of the five sets will be used to train and evaluate a model, after which the R2 score is computed.</p>
			<p>You should observe from the preceding example that the same model trained on five different datasets yields different scores. This implies the importance of your data as well as how it is split.</p>
			<p>By completing this exercise, we see that the best score is <strong class="bold">0.832</strong>, which belongs to the<strong class="bold"> </strong>second split. This is our conclusion here.</p>
			<p>You have seen that cross-validation yields different models. </p>
			<p>But how do you get the best model to work with? There are some models or estimators with in-built cross-validation. Let's explain those.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor189"/>Understanding Estimators That Implement CV</h2>
			<p>The goal of using cross-validation is to find the best performing model using the data that you have. The process for this is:</p>
			<ol>
				<li value="1">Split the data using something like <strong class="source-inline">Kfold()</strong>.</li>
				<li>Iterate over the number of splits and create an estimator.</li>
				<li>Train and evaluate each estimator.</li>
				<li>Pick the estimator with the best metrics to use. You have already seen various approaches to doing that.</li>
			</ol>
			<p>Cross-validation is a popular technique, so estimators exist for cross-validation. For example, <strong class="source-inline">LogisticRegressionCV</strong> exists as a class that implements cross-validation inside <strong class="source-inline">LogisticRegression</strong>. When you make use of <strong class="source-inline">LogisticRegressionCV</strong>, it returns an instance of <strong class="source-inline">LogisticRegression</strong>. The instance it returns is the best performing instance.</p>
			<p>When you create an instance of <strong class="source-inline">LogisticRegressionCV</strong>, you will need to specify the number of <strong class="source-inline">cv</strong> parts that you want. For example, if you set <strong class="source-inline">cv</strong> to <strong class="source-inline">3</strong>, <strong class="source-inline">LogisticRegressionCV</strong> will train three instances of <strong class="source-inline">LogisticRegression</strong> and then evaluate them and return the best performing instance.</p>
			<p>You do not have to make use of <strong class="source-inline">LogisticRegressionCV</strong>. You can continue to make use of <strong class="source-inline">LogisticRegression</strong> with <strong class="source-inline">Kfold</strong> and iterations. <strong class="source-inline">LogisticRegressionCV</strong> simply exists as a convenience.</p>
			<p>In a similar manner, <strong class="source-inline">LinearRegressionCV</strong> exists as a convenient way of implementing cross-validation using <strong class="source-inline">LinearRegression</strong>.</p>
			<p>So, just to be clear, you do not have to use convenience methods such as <strong class="source-inline">LogisticRegressionCV</strong>. Also, they are not a replacement for their primary implementations, such as <strong class="source-inline">LogisticRegression</strong>. Instead, you make use of the convenience methods when you need to implement cross-validation but would like to cut out the four preceding steps.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>LogisticRegressionCV</h1>
			<p><strong class="source-inline">LogisticRegressionCV</strong> is a class that implements cross-validation inside it. This class will train multiple <strong class="source-inline">LogisticRegression</strong> models and return the best one.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor191"/>Exercise 7.06: Training a Logistic Regression Model Using Cross-Validation</h2>
			<p>The goal of this exercise is to train a logistic regression model using cross-validation and get the optimal R2 result. We will be making use of the Cars dataset that you worked with previously.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1"> Open a new Colab notebook.</li>
				<li> Import the necessary libraries:<p class="source-code"># import libraries</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p>In this step, you import <strong class="source-inline">pandas</strong> and alias it as <strong class="source-inline">pd</strong>. You will make use of pandas to read in the file you will be working with.</p></li>
				<li>Create headers for the data:<p class="source-code"># data doesn't have headers, so let's create headers</p><p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p><p>In this step, you start by creating a Python list to hold the <strong class="source-inline">headers</strong> column for the file you will be working with. You store this list as <strong class="source-inline">_headers</strong>.</p></li>
				<li>Read the data:<p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter07/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p><p>You then proceed to read in the file and store it as <strong class="source-inline">df</strong>. This is a DataFrame.</p></li>
				<li>Print out the top five records:<p class="source-code">df.info()</p><p>Finally, you look at the summary of the DataFrame using <strong class="source-inline">.info()</strong>. </p><p>The output looks similar to the following:</p><div id="_idContainer336" class="IMG---Figure"><img src="Images/B15019_07_23.jpg" alt="Figure 7.23: The top five records of the dataframe&#13;&#10;" width="666" height="267"/></div><p class="figure-caption">Figure 7.23: The top five records of the dataframe</p></li>
				<li>Encode the categorical variables as shown in the following code snippet:<p class="source-code"># encode categorical variables</p><p class="source-code">_df = pd.get_dummies(df, columns=['buying', 'maint', 'doors', \</p><p class="source-code">                                  'persons', 'lug_boot', \</p><p class="source-code">                                  'safety'])</p><p class="source-code">_df.head()</p><p>In this step, you convert categorical variables into encodings using the <strong class="source-inline">get_dummies()</strong> method from pandas. You supply the original DataFrame as a parameter and also specify the columns you would like to encode. </p><p>Finally, you take a peek at the top five rows. The output looks similar to the following:</p><div id="_idContainer337" class="IMG---Figure"><img src="Images/B15019_07_24.jpg" alt="Figure 7.24: Encoding categorical variables&#13;&#10;" width="793" height="245"/></div><p class="figure-caption">Figure 7.24: Encoding categorical variables</p></li>
				<li>Split the DataFrame into features and labels:<p class="source-code"># separate features and labels DataFrames</p><p class="source-code">features = _df.drop(['car'], axis=1).values</p><p class="source-code">labels = _df[['car']].values</p><p>In this step, you create two NumPy arrays. The first, called <strong class="source-inline">features</strong>, contains the independent variables. The second, called <strong class="source-inline">labels</strong>, contains the values that the model learns to predict. These are also called <strong class="source-inline">targets</strong>.</p></li>
				<li>Import logistic regression with cross-validation:<p class="source-code">from sklearn.linear_model import LogisticRegressionCV</p><p>In this step, you import the <strong class="source-inline">LogisticRegressionCV</strong> class.</p></li>
				<li>Instantiate <strong class="source-inline">LogisticRegressionCV</strong> as shown in the following code snippet:<p class="source-code">model = LogisticRegressionCV(max_iter=2000, multi_class='auto',\</p><p class="source-code">                             cv=5)</p><p>In this step, you create an instance of <strong class="source-inline">LogisticRegressionCV</strong>. You specify the following parameters:</p><p><strong class="source-inline">max_iter</strong> : You set this to <strong class="source-inline">2000</strong> so that the trainer continues training for <strong class="source-inline">2000</strong> iterations to find better weights.</p><p><strong class="source-inline">multi_class</strong>: You set this to <strong class="source-inline">auto</strong> so that the model automatically detects that your data has more than two classes.</p><p><strong class="source-inline">cv</strong>: You set this to <strong class="source-inline">5</strong>, which is the number of cross-validation sets you would like to train on.</p></li>
				<li>Now fit the model:<p class="source-code">model.fit(features, labels.ravel())</p><p>In this step, you train the model. You pass in <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>. Because <strong class="source-inline">labels</strong> is a 2D array, you make use of <strong class="source-inline">ravel()</strong> to convert it into a 1D array or vector. </p><p>The interpreter produces an output similar to the following:</p><div id="_idContainer338" class="IMG---Figure"><img src="Images/B15019_07_25.jpg" alt="Figure 7.25: Fitting the model&#13;&#10;" width="790" height="161"/></div><p class="figure-caption">Figure 7.25: Fitting the model</p><p>In the preceding output, you see that the model fits the training data. The output shows you the parameters that were used in training, so you are not taken by surprise. Notice, for example, that <strong class="source-inline">max_iter</strong> is <strong class="source-inline">2000</strong>, which is the value that you set. Other parameters you didn't set make use of default values, which you can find out more about from the documentation.</p></li>
				<li>Evaluate the training R2:<p class="source-code">print(model.score(features, labels.ravel()))</p><p>In this step, we make use of the training dataset to compute the R2 score. While we didn't set aside a specific validation dataset, it is important to note that the model only saw 80% of our training data, so it still has new data to work with for this evaluation. </p><p>The output looks similar to the following:</p><div id="_idContainer339" class="IMG---Figure"><img src="Images/B15019_07_26.jpg" alt="Figure 7.26: Computing the R2 score&#13;&#10;" width="1361" height="54"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.26: Computing the R2 score</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/34eD1du">https://packt.live/34eD1du</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Yey40k">https://packt.live/2Yey40k</a>.</p>
			<p>In the preceding output, you see that the final model has an <strong class="source-inline">R2</strong> score of <strong class="source-inline">0.95</strong>, which is a good score.</p>
			<p>At this point, you should see a much better <strong class="source-inline">R2</strong> score than you have previously encountered.</p>
			<p>What if you were working with other types of models that don't have cross-validation built into them? Can you make use of cross-validation to train models and find the best one? Let's find out.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor192"/>Hyperparameter Tuning with GridSearchCV</h1>
			<p><strong class="source-inline">GridSearchCV</strong> will take a model and parameters and train one model for each permutation of the parameters. At the end of the training, it will provide access to the parameters and the model scores. This is called hyperparameter tuning and you will be looking at this in much more depth in <em class="italic">Chapter 8, Hyperparameter Tuning</em>.</p>
			<p>The usual practice is to make use of a small training set to find the optimal parameters using hyperparameter tuning and then to train a final model with all of the data.</p>
			<p>Before the next exercise, let's take a brief look at decision trees, which are a type of model or estimator.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/>Decision Trees</h2>
			<p>A decision tree works by generating a separating hyperplane or a threshold for the features in data. It does this by considering every feature and finding the correlation between the spread of the values in that feature and the label that you are trying to predict.</p>
			<p>Consider the following data about balloons. The label you need to predict is called <strong class="source-inline">inflated</strong>. This dataset is used for predicting whether the balloon is inflated or deflated given the features. The features are:</p>
			<ul>
				<li><strong class="source-inline">color</strong></li>
				<li><strong class="source-inline">size</strong></li>
				<li><strong class="source-inline">act</strong></li>
				<li><strong class="source-inline">age</strong></li>
			</ul>
			<p>The following table displays the distribution of features:</p>
			<div>
				<div id="_idContainer340" class="IMG---Figure">
					<img src="Images/B15019_07_27.jpg" alt="Figure 7.27: Tabular data for balloon features&#13;&#10;" width="566" height="548"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.27: Tabular data for balloon features</p>
			<p>Now consider the following charts, which are visualized depending on the spread of the features against the label:</p>
			<ul>
				<li>If you consider the <strong class="source-inline">Color</strong> feature, the values are <strong class="source-inline">PURPLE</strong> and <strong class="source-inline">YELLOW</strong>, but the number of observations is the same, so you can't infer whether the balloon is inflated or not based on the color, as you can see in the following figure:<div id="_idContainer341" class="IMG---Figure"><img src="Images/B15019_07_28.jpg" alt="Figure 7.28: Barplot for the color feature&#13;&#10;" width="716" height="364"/></div></li>
			</ul>
			<p class="figure-caption">Figure 7.28: Barplot for the color feature</p>
			<ul>
				<li>The <strong class="source-inline">Size</strong> feature has two values: <strong class="source-inline">LARGE</strong> and <strong class="source-inline">SMALL</strong>. These are equally spread, so we can't infer whether the balloon is inflated or not based on the color, as you can see in the following figure:<div id="_idContainer342" class="IMG---Figure"><img src="Images/B15019_07_29.jpg" alt="Figure 7.29: Barplot for the size feature&#13;&#10;" width="730" height="362"/></div></li>
			</ul>
			<p class="figure-caption">Figure 7.29: Barplot for the size feature</p>
			<ul>
				<li>The <strong class="source-inline">Act</strong> feature has two values: <strong class="source-inline">DIP</strong> and <strong class="source-inline">STRETCH</strong>. You can see from the chart that the majority of the <strong class="source-inline">STRETCH</strong> values are inflated. If you had to make a guess, you could easily say that if <strong class="source-inline">Act</strong> is <strong class="source-inline">STRETCH</strong>, then the balloon is inflated. Consider the following figure:<div id="_idContainer343" class="IMG---Figure"><img src="Images/B15019_07_30.jpg" alt="Figure 7.30: Barplot for the act feature&#13;&#10;" width="1270" height="823"/></div></li>
			</ul>
			<p class="figure-caption">Figure 7.30: Barplot for the act feature</p>
			<ul>
				<li>Finally, the <strong class="source-inline">Age</strong> feature also has two values: <strong class="source-inline">ADULT</strong> and <strong class="source-inline">CHILD</strong>. It's also visible from the chart that the <strong class="source-inline">ADULT</strong> value constitutes the majority of inflated balloons:<div id="_idContainer344" class="IMG---Figure"><img src="Images/B15019_07_31.jpg" alt="Figure 7.31: Barplot for the age feature&#13;&#10;" width="1352" height="771"/></div></li>
			</ul>
			<p class="figure-caption">Figure 7.31: Barplot for the age feature</p>
			<p>The two features that are useful to the decision tree are <strong class="source-inline">Act</strong> and <strong class="source-inline">Age</strong>. The tree could start by considering whether <strong class="source-inline">Act</strong> is <strong class="source-inline">STRETCH</strong>. If it is, the prediction will be true. This tree would look like the following figure:</p>
			<div>
				<div id="_idContainer345" class="IMG---Figure">
					<img src="Images/B15019_07_32.jpg" alt="Figure 7.32: Decision tree with depth=1&#13;&#10;" width="761" height="178"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.32: Decision tree with depth=1</p>
			<p>The left side evaluates to the condition being false, and the right side evaluates to the condition being true. This tree has a depth of 1. F means that the prediction is false, and T means that the prediction is true.</p>
			<p>To get better results, the decision tree could introduce a second level. The second level would utilize the <strong class="source-inline">Age</strong> feature and evaluate whether the value is <strong class="source-inline">ADULT</strong>. It would look like the following figure:</p>
			<div>
				<div id="_idContainer346" class="IMG---Figure">
					<img src="Images/B15019_07_33.jpg" alt="Figure 7.33: Decision tree with depth=2&#13;&#10;" width="782" height="310"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.33: Decision tree with depth=2</p>
			<p>This tree has a depth of 2. At the first level, it predicts true if <strong class="source-inline">Act</strong> is <strong class="source-inline">STRETCH</strong>. If <strong class="source-inline">Act</strong> is not <strong class="source-inline">STRETCH</strong>, it checks whether <strong class="source-inline">Age</strong> is <strong class="source-inline">ADULT</strong>. If it is, it predicts true, otherwise, it predicts false.</p>
			<p>The decision tree can have as many levels as you like but starts to overfit at a certain point. As with everything in data science, the optimal depth depends on the data and is a hyperparameter, meaning you need to try different values to find the optimal one.</p>
			<p>In the following exercise, we will be making use of grid search with cross-validation to find the best parameters for a decision tree estimator.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor194"/>Exercise 7.07: Using Grid Search with Cross-Validation to Find the Best Parameters for a Model</h2>
			<p>The goal of this exercise is to make use of grid search to find the best parameters for a <strong class="source-inline">DecisionTree</strong> classifier. We will be making use of the Cars dataset that you worked with previously.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a Colab notebook file.</li>
				<li>Import <strong class="source-inline">pandas</strong>:<p class="source-code">import pandas as pd</p><p>In this step, you import <strong class="source-inline">pandas</strong>. You alias it as <strong class="source-inline">pd</strong>. <strong class="source-inline">Pandas</strong> is used to read in the data you will work with subsequently.</p></li>
				<li>Create <strong class="source-inline">headers</strong>:<p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p></li>
				<li>Read in the <strong class="source-inline">headers</strong>:<p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter07/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p></li>
				<li>Inspect the top five records:<p class="source-code">df.info()</p><p>The output looks similar to the following:</p><div id="_idContainer347" class="IMG---Figure"><img src="Images/B15019_07_34.jpg" alt="Figure 7.34: The top five records of the dataframe&#13;&#10;" width="662" height="265"/></div><p class="figure-caption">Figure 7.34: The top five records of the dataframe</p></li>
				<li>Encode the categorical variables:<p class="source-code">_df = pd.get_dummies(df, columns=['buying', 'maint', 'doors',\</p><p class="source-code">                                  'persons', 'lug_boot', \</p><p class="source-code">                                  'safety'])</p><p class="source-code">_df.head()</p><p>In this step, you utilize <strong class="source-inline">.get_dummies()</strong> to convert the categorical variables into encodings. The <strong class="source-inline">.head()</strong> method instructs the Python interpreter to output the top five columns. </p><p>The output is similar to the following:</p><div id="_idContainer348" class="IMG---Figure"><img src="Images/B15019_07_35.jpg" alt="Figure 7.35: Encoding categorical variables&#13;&#10;" width="793" height="242"/></div><p class="figure-caption">Figure 7.35: Encoding categorical variables</p></li>
				<li>Separate <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>:<p class="source-code">features = _df.drop(['car'], axis=1).values</p><p class="source-code">labels = _df[['car']].values</p><p>In this step, you create two <strong class="source-inline">numpy</strong> arrays, <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>, the first containing independent variables or predictors, and the second containing dependent variables or targets.</p></li>
				<li>Import more libraries – <strong class="source-inline">numpy</strong>, <strong class="source-inline">DecisionTreeClassifier</strong>, and <strong class="source-inline">GridSearchCV</strong>:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.tree import DecisionTreeClassifier</p><p class="source-code">from sklearn.model_selection import GridSearchCV</p><p>In this step, you import <strong class="source-inline">numpy</strong>. NumPy is a numerical computation library. You alias it as <strong class="source-inline">np</strong>. You also import <strong class="source-inline">DecisionTreeClassifier</strong>, which you use to create decision trees. Finally, you import <strong class="source-inline">GridSearchCV</strong>, which will use cross-validation to train multiple models.</p></li>
				<li>Instantiate the decision tree:<p class="source-code">clf = DecisionTreeClassifier()</p><p>In this step, you create an instance of <strong class="source-inline">DecisionTreeClassifier</strong> as <strong class="source-inline">clf</strong>. This instance will be used repeatedly by the grid search.</p></li>
				<li>Create parameters – <strong class="source-inline">max_depth</strong>:<p class="source-code">params = {'max_depth': np.arange(1, 8)}</p><p>In this step, you create a dictionary of parameters. There are two parts to this dictionary:</p><p>The key of the dictionary is a parameter that is passed into the model. In this case, <strong class="source-inline">max_depth</strong> is a parameter that <strong class="source-inline">DecisionTreeClassifier</strong> takes.</p><p>The value is a Python list that grid search iterates over and passes to the model. In this case, we create an array that starts at 1 and ends at 7, inclusive.</p></li>
				<li>Instantiate the grid search as shown in the following code snippet:<p class="source-code">clf_cv = GridSearchCV(clf, param_grid=params, cv=5)</p><p>In this step, you create an instance of <strong class="source-inline">GridSearchCV</strong>. The first parameter is the model to train. The second parameter is the parameters to search over. The third parameter is the number of cross-validation splits to create.</p></li>
				<li>Now train the models:<p class="source-code">clf_cv.fit(features, labels)</p><p>In this step, you train the models using the features and labels. Depending on the type of model, this could take a while. Because we are using a decision tree, it trains quickly. </p><p>The output is similar to the following:</p><div id="_idContainer349" class="IMG---Figure"><img src="Images/B15019_07_36.jpg" alt="Figure 7.36: Training the model&#13;&#10;" width="790" height="436"/></div><p class="figure-caption">Figure 7.36: Training the model</p><p>You can learn a lot by reading the output, such as the number of cross-validation datasets created (called <strong class="source-inline">cv</strong> and equal to <strong class="source-inline">5</strong>), the estimator used (<strong class="source-inline">DecisionTreeClassifier</strong>), and the parameter search space (called <strong class="source-inline">param_grid</strong>).</p></li>
				<li>Print the best parameter:<p class="source-code">print("Tuned Decision Tree Parameters: {}"\</p><p class="source-code">      .format(clf_cv.best_params_))</p><p>In this step, you print out what the best parameter is. In this case, what we were looking for was the best <strong class="source-inline">max_depth</strong>. The output looks like the following:</p><div id="_idContainer350" class="IMG---Figure"><img src="Images/B15019_07_37.jpg" alt="Figure 7.37: Printing the best parameter&#13;&#10;" width="600" height="28"/></div><p class="figure-caption">Figure 7.37: Printing the best parameter</p><p>In the preceding output, you see that the best performing model is one with a <strong class="source-inline">max_depth</strong> of <strong class="source-inline">2</strong>.</p><p>Accessing <strong class="source-inline">best_params_</strong> lets you train another model with the best-known parameters using a larger training dataset.</p></li>
				<li>Print the best <strong class="source-inline">R2</strong>:<p class="source-code">print("Best score is {}".format(clf_cv.best_score_))</p><p>In this step, you print out the <strong class="source-inline">R2</strong> score of the best performing model. </p><p>The output is similar to the following:</p><p class="source-code">Best score is 0.7777777777777778</p><p>In the preceding output, you see that the best performing model has an <strong class="source-inline">R2</strong> score of <strong class="source-inline">0.778</strong>.</p></li>
				<li>Access the best model:<p class="source-code">model = clf_cv.best_estimator_</p><p class="source-code">model</p><p>In this step, you access the best model (or estimator) using <strong class="source-inline">best_estimator_</strong>. This will let you analyze the model, or optionally use it to make predictions and find other metrics. Instructing the Python interpreter to print the best estimator will yield an output similar to the following:</p><div id="_idContainer351" class="IMG---Figure"><img src="Images/B15019_07_38.jpg" alt="Figure 7.38: Accessing the model&#13;&#10;" width="780" height="153"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.38: Accessing the model</p>
			<p>In the preceding output, you see that the best model is <strong class="source-inline">DecisionTreeClassifier</strong> with a <strong class="source-inline">max_depth</strong> of <strong class="source-inline">2</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2E6TdCD">https://packt.live/2E6TdCD</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aCg30V">https://packt.live/3aCg30V</a>.</p>
			<p>Grid search is one of the first techniques that is taught for hyperparameter tuning. However, as the search space increases in size, it quickly becomes expensive. The search space increases as you increase the parameter options because every possible combination of parameter options is considered.</p>
			<p>Consider the case in which the model (or estimator) takes more than one parameter. The search space becomes a multiple of the number of parameters. For example, if we want to train a random forest classifier, we will need to specify the number of trees in the forest, as well as the max depth. If we specified a max depth of 1, 2, and 3, and a forest with 1,000, 2,000, and 3,000 trees, we would need to train 9 different estimators. If we added any more parameters (or hyperparameters), our search space would increase geometrically.</p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Hyperparameter Tuning with RandomizedSearchCV</h1>
			<p>Grid search goes over the entire search space and trains a model or estimator for every combination of parameters. Randomized search goes over only some of the combinations. This is a more optimal use of resources and still provides the benefits of hyperparameter tuning and cross-validation. You will be looking at this in depth in <em class="italic">Chapter 8, Hyperparameter Tuning</em>.</p>
			<p>Have a look at the following exercise. </p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/>Exercise 7.08: Using Randomized Search for Hyperparameter Tuning</h2>
			<p>The goal of this exercise is to perform hyperparameter tuning using randomized search and cross-validation.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Import <strong class="source-inline">pandas</strong>:<p class="source-code">import pandas as pd</p><p>In this step, you import <strong class="source-inline">pandas</strong>. You will make use of it in the next step.</p></li>
				<li>Create <strong class="source-inline">headers</strong>:<p class="source-code">_headers = ['buying', 'maint', 'doors', 'persons', \</p><p class="source-code">            'lug_boot', 'safety', 'car']</p></li>
				<li>Read in the data:<p class="source-code"># read in cars dataset</p><p class="source-code">df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                 'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                 'master/Chapter07/Dataset/car.data', \</p><p class="source-code">                 names=_headers, index_col=None)</p></li>
				<li>Check the first five rows:<p class="source-code">df.info()</p><p>You need to provide a Python list of column headers because the data does not contain column headers. You also inspect the DataFrame that you created. </p><p>The output is similar to the following:</p><div id="_idContainer352" class="IMG---Figure"><img src="Images/B15019_07_39.jpg" alt="Figure 7.39: The top five rows of the DataFrame&#13;&#10;" width="655" height="263"/></div><p class="figure-caption">Figure 7.39: The top five rows of the DataFrame</p></li>
				<li>Encode categorical variables as shown in the following code snippet:<p class="source-code">_df = pd.get_dummies(df, columns=['buying', 'maint', 'doors',\</p><p class="source-code">                                  'persons', 'lug_boot', \</p><p class="source-code">                                  'safety'])</p><p class="source-code">_df.head()</p><p>In this step, you find a numerical representation of text data using one-hot encoding. The operation results in a new DataFrame. You will see that the resulting data structure looks similar to the following:</p><div id="_idContainer353" class="IMG---Figure"><img src="Images/B15019_07_40.jpg" alt="Figure 7.40: Encoding categorical variables&#13;&#10;" width="790" height="237"/></div><p class="figure-caption">Figure 7.40: Encoding categorical variables</p></li>
				<li>Separate the data into independent and dependent variables, which are the <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>:<p class="source-code">features = _df.drop(['car'], axis=1).values</p><p class="source-code">labels = _df[['car']].values</p><p>In this step, you separate the DataFrame into two <strong class="source-inline">numpy</strong> arrays called <strong class="source-inline">features</strong> and <strong class="source-inline">labels</strong>. <strong class="source-inline">Features</strong> contains the independent variables, while <strong class="source-inline">labels</strong> contains the target or dependent variables.</p></li>
				<li>Import additional libraries – <strong class="source-inline">numpy</strong>, <strong class="source-inline">RandomForestClassifier</strong>, and <strong class="source-inline">RandomizedSearchCV</strong>:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p>In this step, you import <strong class="source-inline">numpy</strong> for numerical computations, <strong class="source-inline">RandomForestClassifier</strong> to create an ensemble of estimators, and <strong class="source-inline">RandomizedSearchCV</strong> to perform a randomized search with cross-validation.</p></li>
				<li>Create an instance of <strong class="source-inline">RandomForestClassifier</strong>:<p class="source-code">clf = RandomForestClassifier()</p><p>In this step, you instantiate <strong class="source-inline">RandomForestClassifier</strong>. A random forest classifier is a voting classifier. It makes use of multiple decision trees, which are trained on different subsets of the data. The results from the trees contribute to the output of the random forest by using a voting mechanism.</p></li>
				<li>Specify the parameters:<p class="source-code">params = {'n_estimators':[500, 1000, 2000], \</p><p class="source-code">          'max_depth': np.arange(1, 8)}</p><p><strong class="source-inline">RandomForestClassifier</strong> accepts many parameters, but we specify two: the number of trees in the forest, called <strong class="source-inline">n_estimators</strong>, and the depth of the nodes in each tree, called <strong class="source-inline">max_depth</strong>.</p></li>
				<li>Instantiate a randomized search:<p class="source-code">clf_cv = RandomizedSearchCV(clf, param_distributions=params, \</p><p class="source-code">                            cv=5)</p><p>In this step, you specify three parameters when you instantiate the <strong class="source-inline">clf</strong> class, the estimator, or model to use, which is a random forest classifier, <strong class="source-inline">param_distributions</strong>, the parameter search space, and <strong class="source-inline">cv</strong>, the number of cross-validation datasets to create.</p></li>
				<li>Perform the search:<p class="source-code">clf_cv.fit(features, labels.ravel())</p><p>In this step, you perform the search by calling <strong class="source-inline">fit()</strong>. This operation trains different models using the cross-validation datasets and various combinations of the hyperparameters. The output from this operation is similar to the following:</p><div id="_idContainer354" class="IMG---Figure"><img src="Images/B15019_07_41.jpg" alt="Figure 7.41: Output of the search operation&#13;&#10;" width="787" height="627"/></div><p class="figure-caption">Figure 7.41: Output of the search operation</p><p>In the preceding output, you see that the randomized search will be carried out using cross-validation with five splits (<strong class="source-inline">cv=5</strong>). The estimator to be used is <strong class="source-inline">RandomForestClassifier</strong>.</p></li>
				<li>Print the best parameter combination:<p class="source-code">print("Tuned Random Forest Parameters: {}"\</p><p class="source-code">      .format(clf_cv.best_params_))</p><p>In this step, you print out the best hyperparameters. </p><p>The output is similar to the following:</p><div id="_idContainer355" class="IMG---Figure"><img src="Images/B15019_07_42.jpg" alt="Figure 7.42: Printing the best parameter combination&#13;&#10;" width="677" height="33"/></div><p class="figure-caption">Figure 7.42: Printing the best parameter combination</p><p>In the preceding output, you see that the best estimator is a Random Forest classifier with 1,000 trees (<strong class="source-inline">n_estimators=1000</strong>) and <strong class="source-inline">max_depth=5</strong>. You can print the best score by executing <strong class="source-inline">print("Best score is {}".format(clf_cv.best_score_))</strong>. For this exercise, this value is ~ <strong class="source-inline">0.76</strong>.</p></li>
				<li>Inspect the best model:<p class="source-code">model = clf_cv.best_estimator_</p><p class="source-code">model</p><p>In this step, you find the best performing estimator (or model) and print out its details. The output is similar to the following:</p><div id="_idContainer356" class="IMG---Figure"><img src="Images/B15019_07_43.jpg" alt="Figure 7.43:  Inspecting the model&#13;&#10;" width="789" height="221"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.43:  Inspecting the model</p>
			<p>In the preceding output, you see that the best estimator is <strong class="source-inline">RandomForestClassifier</strong> with <strong class="source-inline">n_estimators=1000</strong> and <strong class="source-inline">max_depth=5</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3aDFijn">https://packt.live/3aDFijn</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3kWMQ5r">https://packt.live/3kWMQ5r</a>.</p>
			<p>In this exercise, you learned to make use of cross-validation and random search to find the best model using a combination of hyperparameters. This process is called hyperparameter tuning, in which you find the best combination of hyperparameters to use to train the model that you will put into production.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor197"/>Model Regularization with Lasso Regression</h1>
			<p>As mentioned at the beginning of this chapter models can overfit training data. One reason for this is having too many features with large coefficients (also called weights). The key to solving this type of overfitting problem is reducing the magnitude of the coefficients.</p>
			<p>You may recall that weights are optimized during model training. One method for optimizing weights is called gradient descent. The gradient update rule makes use of a differentiable loss function. Examples of differentiable loss functions are:</p>
			<ul>
				<li>Mean Absolute Error (MAE)</li>
				<li>Mean Squared Error (MSE)</li>
			</ul>
			<p>For lasso regression, a penalty is introduced in the loss function. The technicalities of this implementation are hidden by the class. The penalty is also called a regularization parameter.</p>
			<p>Consider the following exercise in which you over-engineer a model to introduce overfitting, and then use lasso regression to get better results.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor198"/>Exercise 7.09: Fixing Model Overfitting Using Lasso Regression</h2>
			<p>The goal of this exercise is to teach you how to identify when your model starts overfitting, and to use lasso regression to fix overfitting in your model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The data you will be making use of is the Combined Cycle Power Plant Data Set from the UCI Machine Learning Repository. It contains 9568 data points collected from a Combined Cycle Power Plant. Features include temperature, pressure, humidity, and exhaust vacuum. These are used to predict the net hourly electrical energy output of the plant. See the following link: <a href="https://packt.live/2v9ohwK">https://packt.live/2v9ohwK</a>.</p>
			<p>The attribute information states "Features consist of hourly average ambient variables:</p>
			<ul>
				<li>Temperature (T) in the range 1.81°C and 37.11°C,</li>
				<li>Ambient Pressure (AP) in the range 992.89-1033.30 millibar,</li>
				<li>Relative Humidity (RH) in the range 25.56% to 100.16%</li>
				<li>Exhaust Vacuum (V) in the range 25.36-81.56 cm Hg</li>
				<li>Net hourly electrical energy output (EP) 420.26-495.76 MW</li>
			</ul>
			<p>The averages are taken from various sensors located around the plant that record the ambient variables every second. The variables are given without normalization."</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a Colab notebook.</li>
				<li>Import the required libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression, Lasso</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">from sklearn.pipeline import Pipeline</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler, \</p><p class="source-code">PolynomialFeatures</p></li>
				<li>Read in the data:<p class="source-code">_df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                  'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                  'master/Chapter07/Dataset/ccpp.csv')</p></li>
				<li>Inspect the DataFrame:<p class="source-code">_df.info()</p><p>The <strong class="source-inline">.info()</strong> method prints out a summary of the DataFrame, including the names of the columns and the number of records. The output might be similar to the following:</p><div id="_idContainer357" class="IMG---Figure"><img src="Images/B15019_07_44.jpg" alt="Figure 7.44: Inspecting the dataframe&#13;&#10;" width="1206" height="363"/></div><p class="figure-caption">Figure 7.44: Inspecting the dataframe</p><p>You can see from the preceding figure that the DataFrame has 5 columns and 9,568 records. You can see that all columns contain numeric data and that the columns have the following names: <strong class="source-inline">AT</strong>, <strong class="source-inline">V</strong>, <strong class="source-inline">AP</strong>, <strong class="source-inline">RH</strong>, and <strong class="source-inline">PE</strong>.</p></li>
				<li>Extract features into a column called <strong class="source-inline">X</strong>:<p class="source-code">X = _df.drop(['PE'], axis=1).values</p></li>
				<li>Extract labels into a column called <strong class="source-inline">y</strong>:<p class="source-code">y = _df['PE'].values</p></li>
				<li>Split the data into training and evaluation sets:<p class="source-code">train_X, eval_X, train_y, eval_y = train_test_split\</p><p class="source-code">                                   (X, y, train_size=0.8, \</p><p class="source-code">                                    random_state=0)</p></li>
				<li>Create an instance of a <strong class="source-inline">LinearRegression</strong> model:<p class="source-code">lr_model_1 = LinearRegression()</p></li>
				<li>Fit the model on the training data:<p class="source-code">lr_model_1.fit(train_X, train_y)</p><p>The output from this step should look similar to the following:</p><div id="_idContainer358" class="IMG---Figure"><img src="Images/B15019_07_45.jpg" alt="Figure 7.45: Fitting the model on training data&#13;&#10;" width="916" height="33"/></div><p class="figure-caption">Figure 7.45: Fitting the model on training data</p></li>
				<li>Use the model to make predictions on the evaluation dataset:<p class="source-code">lr_model_1_preds = lr_model_1.predict(eval_X)</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of the model:<p class="source-code">print('lr_model_1 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_1.score(eval_X, eval_y)))</p><p>The output of this step should look similar to the following:</p><div id="_idContainer359" class="IMG---Figure"><img src="Images/B15019_07_46.jpg" alt="Figure 7.46: Printing the R2 score&#13;&#10;" width="542" height="33"/></div><p class="figure-caption">Figure 7.46: Printing the R2 score</p><p>You will notice that the <strong class="source-inline">R2</strong> score for this model is <strong class="source-inline">0.926</strong>. You will make use of this figure to compare with the next model you train. Recall that this is an evaluation metric.</p></li>
				<li>Print out the Mean Squared Error (MSE) of this model:<p class="source-code">print('lr_model_1 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_1_preds)))</p><p>The output of this step should look similar to the following:</p><div id="_idContainer360" class="IMG---Figure"><img src="Images/B15019_07_47.jpg" alt="Figure 7.47: Printing the MSE&#13;&#10;" width="570" height="36"/></div><p class="figure-caption">Figure 7.47: Printing the MSE</p><p>You will notice that the MSE is <strong class="source-inline">21.675</strong>. This is an evaluation metric that you will use to compare this model to subsequent models.</p><p>The first model was trained on four features. You will now train a new model on four cubed features.</p></li>
				<li>Create a list of tuples to serve as a pipeline:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=3)),\</p><p class="source-code">         ('lr', LinearRegression())]</p><p>In this step, you create a list with three tuples. The first tuple represents a scaling operation that makes use of <strong class="source-inline">MinMaxScaler</strong>. The second tuple represents a feature engineering step and makes use of <strong class="source-inline">PolynomialFeatures</strong>. The third tuple represents a <strong class="source-inline">LinearRegression</strong> model.</p><p>The first element of the tuple represents the name of the step, while the second element represents the class that performs a transformation or an estimator.</p></li>
				<li>Create an instance of a pipeline:<p class="source-code">lr_model_2 = Pipeline(steps)</p></li>
				<li>Train the instance of the pipeline:<p class="source-code">lr_model_2.fit(train_X, train_y)</p><p>The pipeline implements a <strong class="source-inline">.fit()</strong> method, which is also implemented in all instances of transformers and estimators. The <strong class="source-inline">.fit()</strong> method causes <strong class="source-inline">.fit_transform()</strong> to be called on transformers, and causes <strong class="source-inline">.fit()</strong> to be called on estimators. The output of this step is similar to the following:</p><div id="_idContainer361" class="IMG---Figure"><img src="Images/B15019_07_48.jpg" alt="Figure 7.48: Training the instance of the pipeline&#13;&#10;" width="915" height="213"/></div><p class="figure-caption">Figure 7.48: Training the instance of the pipeline</p><p>You can see from the output that a pipeline was trained. You can see that the steps are made up of <strong class="source-inline">MinMaxScaler</strong> and <strong class="source-inline">PolynomialFeatures</strong>, and that the final step is made up of <strong class="source-inline">LinearRegression</strong>.</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of the model:<p class="source-code">print('lr_model_2 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_2.score(eval_X, eval_y)))</p><p>The output is similar to the following:</p><div id="_idContainer362" class="IMG---Figure"><img src="Images/B15019_07_49.jpg" alt="Figure 7.49: The R2 score of the model&#13;&#10;" width="550" height="36"/></div><p class="figure-caption">Figure 7.49: The R2 score of the model</p><p>You can see from the preceding that the <strong class="source-inline">R2</strong> score is <strong class="source-inline">0.944</strong>, which is better than the <strong class="source-inline">R2</strong> score of the first model, which was <strong class="source-inline">0.932</strong>. You can start to observe that the metrics suggest that this model is better than the first one.</p></li>
				<li>Use the model to predict on the evaluation data:<p class="source-code">lr_model_2_preds = lr_model_2.predict(eval_X)</p></li>
				<li>Print the MSE of the second model:<p class="source-code">print('lr_model_2 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_2_preds)))</p><p>The output is similar to the following:</p><div id="_idContainer363" class="IMG---Figure"><img src="Images/B15019_07_50.jpg" alt="Figure 7.50: The MSE of the second model&#13;&#10;" width="545" height="46"/></div><p class="figure-caption">Figure 7.50: The MSE of the second model</p><p>You can see from the output that the MSE of the second model is <strong class="source-inline">16.27</strong>. This is less than the MSE of the first model, which is <strong class="source-inline">19.73</strong>. You can safely conclude that the second model is better than the first.</p></li>
				<li>Inspect the model coefficients (also called weights):<p class="source-code">print(lr_model_2[-1].coef_)</p><p>In this step, you will note that <strong class="source-inline">lr_model_2</strong> is a pipeline. The final object in this pipeline is the model, so you make use of list addressing to access this by setting the index of the list element to <strong class="source-inline">-1</strong>.</p><p>Once you have the model, which is the final element in the pipeline, you make use of <strong class="source-inline">.coef_</strong> to get the model coefficients. The output is similar to the following:</p><div id="_idContainer364" class="IMG---Figure"><img src="Images/B15019_07_51.jpg" alt="Figure 7.51: Print the model coefficients&#13;&#10;" width="643" height="204"/></div><p class="figure-caption">Figure 7.51: Print the model coefficients</p><p>You will note from the preceding output that the majority of the values are in the tens, some values are in the hundreds, and one value has a really small magnitude.</p></li>
				<li>Check for the number of coefficients in this model:<p class="source-code">print(len(lr_model_2[-1].coef_))</p><p>The output for this step is similar to the following:</p><p class="source-code">35</p><p>You can see from the preceding screenshot that the second model has <strong class="source-inline">35</strong> coefficients.</p></li>
				<li>Create a <strong class="source-inline">steps</strong> list with <strong class="source-inline">PolynomialFeatures</strong> of degree <strong class="source-inline">10</strong>:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=10)),\</p><p class="source-code">         ('lr', LinearRegression())]</p></li>
				<li>Create a third model from the preceding steps:<p class="source-code">lr_model_3 = Pipeline(steps)</p></li>
				<li>Fit the third model on the training data:<p class="source-code">lr_model_3.fit(train_X, train_y)</p><p>The output from this step is similar to the following:</p><div id="_idContainer365" class="IMG---Figure"><img src="Images/B15019_07_52.jpg" alt="Figure 7.52: Fitting the third model on the data&#13;&#10;" width="906" height="212"/></div><p class="figure-caption">Figure 7.52: Fitting the third model on the data</p><p>You can see from the output that the pipeline makes use of <strong class="source-inline">PolynomialFeatures</strong> of degree <strong class="source-inline">10</strong>. You are doing this in the hope of getting a better model.</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of this model:<p class="source-code">print('lr_model_3 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_3.score(eval_X, eval_y)))</p><p>The output of this model is similar to the following:</p><div id="_idContainer366" class="IMG---Figure"><img src="Images/B15019_07_53.jpg" alt="Figure 7.53: R2 score of the model&#13;&#10;" width="520" height="32"/></div><p class="figure-caption">Figure 7.53: R2 score of the model</p><p>You can see from the preceding figure that the R2 score is now <strong class="source-inline">0.56</strong>. The previous model had an <strong class="source-inline">R2</strong> score of <strong class="source-inline">0.944</strong>. This model has an R2 score that is considerably worse than the one of the previous model, <strong class="source-inline">lr_model_2</strong>. This happens when your model is overfitting.</p></li>
				<li>Use <strong class="source-inline">lr_model_3</strong> to predict on evaluation data:<p class="source-code">lr_model_3_preds = lr_model_3.predict(eval_X)</p></li>
				<li>Print out the MSE for <strong class="source-inline">lr_model_3</strong>:<p class="source-code">print('lr_model_3 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_3_preds)))</p><p>The output for this step might be similar to the following:</p><div id="_idContainer367" class="IMG---Figure"><img src="Images/B15019_07_54.jpg" alt="Figure 7.54: The MSE of the model&#13;&#10;" width="527" height="30"/></div><p class="figure-caption">Figure 7.54: The MSE of the model</p><p>You can see from the preceding figure that the MSE is also considerably worse. The MSE is <strong class="source-inline">126.25</strong>, as compared to <strong class="source-inline">16.27</strong> for the previous model.</p></li>
				<li>Print out the number of coefficients (also called weights) in this model:<p class="source-code">print(len(lr_model_3[-1].coef_))</p><p>The output might resemble the following:</p><div id="_idContainer368" class="IMG---Figure"><img src="Images/B15019_07_55.jpg" alt="Figure 7.55: Printing the number of coefficients&#13;&#10;" width="1665" height="65"/></div><p class="figure-caption">Figure 7.55: Printing the number of coefficients</p><p>You can see that the model has 1,001 coefficients.</p></li>
				<li>Inspect the first 35 coefficients to get a sense of the individual magnitudes:<p class="source-code">print(lr_model_3[-1].coef_[:35])</p><p>The output might be similar to the following:</p><div id="_idContainer369" class="IMG---Figure"><img src="Images/B15019_07_56.jpg" alt="Figure 7.56: Inspecting the first 35 coefficients&#13;&#10;" width="638" height="200"/></div><p class="figure-caption">Figure 7.56: Inspecting the first 35 coefficients</p><p>You can see from the output that the coefficients have significantly larger magnitudes than the coefficients from <strong class="source-inline">lr_model_2</strong>.</p><p>In the next steps, you will train a lasso regression model on the same set of features to reduce overfitting.</p></li>
				<li>Create a list of steps for the pipeline you will create later on:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=10)),\</p><p class="source-code">         ('lr', Lasso(alpha=0.01))]</p><p>You create a list of steps for the pipeline you will create. Note that the third step in this list is an instance of lasso. The parameter called <strong class="source-inline">alpha</strong> in the call to <strong class="source-inline">Lasso()</strong> is the regularization parameter. You can play around with any values from 0 to 1 to see how it affects the performance of the model that you train.</p></li>
				<li>Create an instance of a pipeline:<p class="source-code">lasso_model = Pipeline(steps)</p></li>
				<li>Fit the pipeline on the training data:<p class="source-code">lasso_model.fit(train_X, train_y)</p><p>The output from this operation might be similar to the following:</p><div id="_idContainer370" class="IMG---Figure"><img src="Images/B15019_07_57.jpg" alt="Figure 7.57: Fitting the pipeline on the training data&#13;&#10;" width="885" height="256"/></div><p class="figure-caption">Figure 7.57: Fitting the pipeline on the training data</p><p>You can see from the output that the pipeline trained a lasso model in the final step. The regularization parameter was <strong class="source-inline">0.01</strong> and the model trained for a maximum of 1,000 iterations.</p></li>
				<li>Print the <strong class="source-inline">R2</strong> score of <strong class="source-inline">lasso_model</strong>:<p class="source-code">print('lasso_model R2 Score: {}'\</p><p class="source-code">      .format(lasso_model.score(eval_X, eval_y)))</p><p>The output of this step might be similar to the following:</p><div id="_idContainer371" class="IMG---Figure"><img src="Images/B15019_07_58.jpg" alt="Figure 7.58: R2 score&#13;&#10;" width="644" height="38"/></div><p class="figure-caption">Figure 7.58: R2 score</p><p>You can see that the <strong class="source-inline">R2</strong> score has climbed back up to <strong class="source-inline">0.94</strong>, which is considerably better than the score of <strong class="source-inline">0.56</strong> that <strong class="source-inline">lr_model_3</strong> had. This is already looking like a better model.</p></li>
				<li>Use <strong class="source-inline">lasso_model</strong> to predict on the evaluation data:<p class="source-code">lasso_preds = lasso_model.predict(eval_X)</p></li>
				<li>Print the MSE of <strong class="source-inline">lasso_model</strong>:<p class="source-code">print('lasso_model MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lasso_preds)))</p><p>The output might be similar to the following: </p><div id="_idContainer372" class="IMG---Figure"><img src="Images/B15019_07_59.jpg" alt="Figure 7.59: MSE of lasso model&#13;&#10;" width="663" height="33"/></div><p class="figure-caption">Figure 7.59: MSE of lasso model</p><p>You can see from the output that the MSE is <strong class="source-inline">17.01</strong>, which is way lower than the MSE value of <strong class="source-inline">126.25</strong> that <strong class="source-inline">lr_model_3</strong> had. You can safely conclude that this is a much better model.</p></li>
				<li>Print out the number of coefficients in <strong class="source-inline">lasso_model</strong>:<p class="source-code">print(len(lasso_model[-1].coef_))</p><p>The output might be similar to the following:</p><p class="source-code">1001</p><p>You can see that this model has 1,001 coefficients, which is the same number of coefficients that <strong class="source-inline">lr_model_3</strong> had.</p></li>
				<li>Print out the values of the first 35 coefficients:<p class="source-code">print(lasso_model[-1].coef_[:35])</p><p>The output might be similar to the following:</p><div id="_idContainer373" class="IMG---Figure"><img src="Images/B15019_07_60.jpg" alt="Figure 7.60: Printing the values of 35 coefficients&#13;&#10;" width="659" height="156"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.60: Printing the values of 35 coefficients</p>
			<p>You can see from the preceding output that some of the coefficients are set to <strong class="source-inline">0</strong>. This has the effect of ignoring the corresponding column of data in the input. You can also see that the remaining coefficients have magnitudes of less than 100. This goes to show that the model is no longer overfitting.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/319S6en">https://packt.live/319S6en</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/319AAXD">https://packt.live/319AAXD</a>.</p>
			<p>This exercise taught you how to fix overfitting by using <strong class="source-inline">LassoRegression</strong> to train a new model.</p>
			<p>In the next section, you will learn about using ridge regression to solve overfitting in a model.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor199"/>Ridge Regression</h1>
			<p>You just learned about lasso regression, which introduces a penalty and tries to eliminate certain features from the data. Ridge regression takes an alternative approach by introducing a penalty that penalizes large weights. As a result, the optimization process tries to reduce the magnitude of the coefficients without completely eliminating them.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Exercise 7.10: Fixing Model Overfitting Using Ridge Regression</h2>
			<p>The goal of this exercise is to teach you how to identify when your model starts overfitting, and to use ridge regression to fix overfitting in your model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You will be using the same dataset as in <em class="italic">Exercise 7.09</em>, <em class="italic">Fixing Model Overfitting Using Lasso Regression.</em></p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a Colab notebook.</li>
				<li>Import the required libraries:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.linear_model import LinearRegression, Ridge</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">from sklearn.pipeline import Pipeline</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler, \</p><p class="source-code">PolynomialFeatures</p></li>
				<li>Read in the data:<p class="source-code">_df = pd.read_csv('https://raw.githubusercontent.com/'\</p><p class="source-code">                  'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">                  'master/Chapter07/Dataset/ccpp.csv')</p></li>
				<li>Inspect the DataFrame:<p class="source-code">_df.info()</p><p>The <strong class="source-inline">.info()</strong> method prints out a summary of the DataFrame, including the names of the columns and the number of records. The output might be similar to the following:</p><div id="_idContainer374" class="IMG---Figure"><img src="Images/B15019_07_61.jpg" alt="Figure 7.61: Inspecting the dataframe&#13;&#10;" width="980" height="416"/></div><p class="figure-caption">Figure 7.61: Inspecting the dataframe</p><p>You can see from the preceding figure that the DataFrame has 5 columns and 9,568 records. You can see that all columns contain numeric data and that the columns have the names: <strong class="source-inline">AT</strong>, <strong class="source-inline">V</strong>, <strong class="source-inline">AP</strong>, <strong class="source-inline">RH</strong>, and <strong class="source-inline">PE</strong>.</p></li>
				<li>Extract features into a column called <strong class="source-inline">X</strong>:<p class="source-code">X = _df.drop(['PE'], axis=1).values</p></li>
				<li>Extract labels into a column called <strong class="source-inline">y</strong>:<p class="source-code">y = _df['PE'].values</p></li>
				<li>Split the data into training and evaluation sets:<p class="source-code">train_X, eval_X, train_y, eval_y = train_test_split\</p><p class="source-code">                                   (X, y, train_size=0.8, \</p><p class="source-code">                                    random_state=0)</p></li>
				<li>Create an instance of a <strong class="source-inline">LinearRegression</strong> model:<p class="source-code">lr_model_1 = LinearRegression()</p></li>
				<li>Fit the model on the training data:<p class="source-code">lr_model_1.fit(train_X, train_y)</p><p>The output from this step should look similar to the following:</p><div id="_idContainer375" class="IMG---Figure"><img src="Images/B15019_07_62.jpg" alt="Figure 7.62: Fitting the model on data&#13;&#10;" width="910" height="32"/></div><p class="figure-caption">Figure 7.62: Fitting the model on data</p></li>
				<li>Use the model to make predictions on the evaluation dataset:<p class="source-code">lr_model_1_preds = lr_model_1.predict(eval_X)</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of the model:<p class="source-code">print('lr_model_1 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_1.score(eval_X, eval_y)))</p><p>The output of this step should look similar to the following:</p><div id="_idContainer376" class="IMG---Figure"><img src="Images/B15019_07_63.jpg" alt="Figure 7.63: R2 score&#13;&#10;" width="796" height="31"/></div><p class="figure-caption">Figure 7.63: R2 score</p><p>You will notice that the R2 score for this model is <strong class="source-inline">0.933</strong>. You will make use of this figure to compare it with the next model you train. Recall that this is an evaluation metric.</p></li>
				<li>Print out the MSE of this model:<p class="source-code">print('lr_model_1 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_1_preds)))</p><p>The output of this step should look similar to the following:</p><div id="_idContainer377" class="IMG---Figure"><img src="Images/B15019_07_64.jpg" alt="Figure 7.64: The MSE of the model&#13;&#10;" width="669" height="34"/></div><p class="figure-caption">Figure 7.64: The MSE of the model</p><p>You will notice that the MSE is <strong class="source-inline">19.734</strong>. This is an evaluation metric that you will use to compare this model to subsequent models.</p><p>The first model was trained on four features. You will now train a new model on four cubed features.</p></li>
				<li>Create a list of tuples to serve as a pipeline:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=3)),\</p><p class="source-code">         ('lr', LinearRegression())]</p><p>In this step, you create a list with three tuples. The first tuple represents a scaling operation that makes use of <strong class="source-inline">MinMaxScaler</strong>. The second tuple represents a feature engineering step and makes use of <strong class="source-inline">PolynomialFeatures</strong>. The third tuple represents a <strong class="source-inline">LinearRegression</strong> model.</p><p>The first element of the tuple represents the name of the step, while the second element represents the class that performs a transformation or an estimation.</p></li>
				<li>Create an instance of a pipeline:<p class="source-code">lr_model_2 = Pipeline(steps)</p></li>
				<li>Train the instance of the pipeline:<p class="source-code">lr_model_2.fit(train_X, train_y)</p><p>The pipeline implements a <strong class="source-inline">.fit()</strong> method, which is also implemented in all instances of transformers and estimators. The <strong class="source-inline">.fit()</strong> method causes <strong class="source-inline">.fit_transform()</strong> to be called on transformers, and causes <strong class="source-inline">.fit()</strong> to be called on estimators. The output of this step is similar to the following:</p><div id="_idContainer378" class="IMG---Figure"><img src="Images/B15019_07_65.jpg" alt="Figure 7.65: Training the instance of a pipeline&#13;&#10;" width="912" height="219"/></div><p class="figure-caption">Figure 7.65: Training the instance of a pipeline</p><p>You can see from the output that a pipeline was trained. You can see that the steps are made up of <strong class="source-inline">MinMaxScaler</strong> and <strong class="source-inline">PolynomialFeatures</strong>, and that the final step is made up of <strong class="source-inline">LinearRegression</strong>.</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of the model:<p class="source-code">print('lr_model_2 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_2.score(eval_X, eval_y)))</p><p>The output is similar to the following:</p><div id="_idContainer379" class="IMG---Figure"><img src="Images/B15019_07_66.jpg" alt="Figure 7.66: R2 score&#13;&#10;" width="688" height="34"/></div><p class="figure-caption">Figure 7.66: R2 score</p><p>You can see from the preceding that the R2 score is <strong class="source-inline">0.944</strong>, which is better than the R2 score of the first model, which was <strong class="source-inline">0.933</strong>. You can start to observe that the metrics suggest that this model is better than the first one.</p></li>
				<li>Use the model to predict on the evaluation data:<p class="source-code">lr_model_2_preds = lr_model_2.predict(eval_X)</p></li>
				<li>Print the MSE of the second model:<p class="source-code">print('lr_model_2 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_2_preds)))</p><p>The output is similar to the following:</p><div id="_idContainer380" class="IMG---Figure"><img src="Images/B15019_07_67.jpg" alt="Figure 7.67: The MSE of the model&#13;&#10;" width="602" height="32"/></div><p class="figure-caption">Figure 7.67: The MSE of the model</p><p>You can see from the output that the MSE of the second model is <strong class="source-inline">16.272</strong>. This is less than the MSE of the first model, which is <strong class="source-inline">19.734</strong>. You can safely conclude that the second model is better than the first.</p></li>
				<li>Inspect the model coefficients (also called weights):<p class="source-code">print(lr_model_2[-1].coef_)</p><p>In this step, you will note that <strong class="source-inline">lr_model_2</strong> is a pipeline. The final object in this pipeline is the model, so you make use of list addressing to access this by setting the index of the list element to <strong class="source-inline">-1</strong>.</p><p>Once you have the model, which is the final element in the pipeline, you make use of <strong class="source-inline">.coef_</strong> to get the model coefficients. The output is similar to the following:</p><div id="_idContainer381" class="IMG---Figure"><img src="Images/B15019_07_68.jpg" alt="Figure 7.68: Printing model coefficients&#13;&#10;" width="905" height="261"/></div><p class="figure-caption">Figure 7.68: Printing model coefficients</p><p>You will note from the preceding output that the majority of the values are in the tens, some values are in the hundreds, and one value has a really small magnitude.</p></li>
				<li>Check the number of coefficients in this model:<p class="source-code">print(len(lr_model_2[-1].coef_))</p><p>The output of this step is similar to the following:</p><div id="_idContainer382" class="IMG---Figure"><img src="Images/B15019_07_69.jpg" alt="Figure 7.69: Checking the number of coefficients&#13;&#10;" width="1665" height="67"/></div><p class="figure-caption">Figure 7.69: Checking the number of coefficients</p><p>You will see from the preceding that the second model has 35 coefficients.</p></li>
				<li>Create a <strong class="source-inline">steps</strong> list with <strong class="source-inline">PolynomialFeatures</strong> of degree <strong class="source-inline">10</strong>:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=10)),\</p><p class="source-code">         ('lr', LinearRegression())]</p></li>
				<li>Create a third model from the preceding steps:<p class="source-code">lr_model_3 = Pipeline(steps)</p></li>
				<li>Fit the third model on the training data:<p class="source-code">lr_model_3.fit(train_X, train_y)</p><p>The output from this step is similar to the following:</p><div id="_idContainer383" class="IMG---Figure"><img src="Images/B15019_07_70.jpg" alt="Figure 7.70: Fitting lr_model_3 on the training data&#13;&#10;" width="909" height="212"/></div><p class="figure-caption">Figure 7.70: Fitting lr_model_3 on the training data</p><p>You can see from the output that the pipeline makes use of <strong class="source-inline">PolynomialFeatures</strong> of degree <strong class="source-inline">10</strong>. You are doing this in the hope of getting a better model.</p></li>
				<li>Print out the <strong class="source-inline">R2</strong> score of this model:<p class="source-code">print('lr_model_3 R2 Score: {}'\</p><p class="source-code">      .format(lr_model_3.score(eval_X, eval_y)))</p><p>The output of this model is similar to the following:</p><div id="_idContainer384" class="IMG---Figure"><img src="Images/B15019_07_71.jpg" alt="Figure 7.71: R2 score&#13;&#10;" width="735" height="33"/></div><p class="figure-caption">Figure 7.71: R2 score</p><p>You can see from the preceding figure that the <strong class="source-inline">R2</strong> score is now <strong class="source-inline">0.568</strong> The previous model had an <strong class="source-inline">R2</strong> score of <strong class="source-inline">0.944</strong>. This model has an <strong class="source-inline">R2</strong> score that is worse than the one of the previous model, <strong class="source-inline">lr_model_2</strong>. This happens when your model is overfitting.</p></li>
				<li>Use <strong class="source-inline">lr_model_3</strong> to predict on evaluation data:<p class="source-code">lr_model_3_preds = lr_model_3.predict(eval_X)</p></li>
				<li>Print out the MSE for <strong class="source-inline">lr_model_3</strong>:<p class="source-code">print('lr_model_3 MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, lr_model_3_preds)))</p><p>The output of this step might be similar to the following:</p><div id="_idContainer385" class="IMG---Figure"><img src="Images/B15019_07_72.jpg" alt="Figure 7.72: The MSE of lr_model_3&#13;&#10;" width="650" height="28"/></div><p class="figure-caption">Figure 7.72: The MSE of lr_model_3</p><p>You can see from the preceding figure that the MSE is also worse. The MSE is <strong class="source-inline">126.254</strong>, as compared to <strong class="source-inline">16.271</strong> for the previous model.</p></li>
				<li>Print out the number of coefficients (also called weights) in this model:<p class="source-code">print(len(lr_model_3[-1].coef_))</p><p>The output might resemble the following:</p><p class="source-code">1001</p><p>You can see that the model has <strong class="source-inline">1,001</strong> coefficients.</p></li>
				<li>Inspect the first <strong class="source-inline">35</strong> coefficients to get a sense of the individual magnitudes:<p class="source-code">print(lr_model_3[-1].coef_[:35])</p><p>The output might be similar to the following:</p><div id="_idContainer386" class="IMG---Figure"><img src="Images/B15019_07_73.jpg" alt="Figure 7.73: Inspecting 35 coefficients&#13;&#10;" width="891" height="261"/></div><p class="figure-caption">Figure 7.73: Inspecting 35 coefficients</p><p>You can see from the output that the coefficients have significantly larger magnitudes than the coefficients from <strong class="source-inline">lr_model_2</strong>.</p><p>In the next steps, you will train a ridge regression model on the same set of features to reduce overfitting.</p></li>
				<li>Create a list of steps for the pipeline you will create later on:<p class="source-code">steps = [('scaler', MinMaxScaler()),\</p><p class="source-code">         ('poly', PolynomialFeatures(degree=10)),\</p><p class="source-code">         ('lr', Ridge(alpha=0.9))]</p><p>You create a list of steps for the pipeline you will create. Note that the third step in this list is an instance of <strong class="source-inline">Ridge</strong>. The parameter called <strong class="source-inline">alpha</strong> in the call to <strong class="source-inline">Ridge()</strong> is the regularization parameter. You can play around with any values from 0 to 1 to see how it affects the performance of the model that you train.</p></li>
				<li>Create an instance of a pipeline:<p class="source-code">ridge_model = Pipeline(steps)</p></li>
				<li>Fit the pipeline on the training data:<p class="source-code">ridge_model.fit(train_X, train_y)</p><p>The output of this operation might be similar to the following:</p><div id="_idContainer387" class="IMG---Figure"><img src="Images/B15019_07_74.jpg" alt="Figure 7.74: Fitting the pipeline on training data&#13;&#10;" width="915" height="256"/></div><p class="figure-caption">Figure 7.74: Fitting the pipeline on training data</p><p>You can see from the output that the pipeline trained a ridge model in the final step. The regularization parameter was <strong class="source-inline">0</strong>.</p></li>
				<li>Print the R2 score of <strong class="source-inline">ridge_model</strong>:<p class="source-code">print('ridge_model R2 Score: {}'\</p><p class="source-code">      .format(ridge_model.score(eval_X, eval_y)))</p><p>The output of this step might be similar to the following:</p><div id="_idContainer388" class="IMG---Figure"><img src="Images/B15019_07_75.jpg" alt="Figure 7.75: R2 score&#13;&#10;" width="588" height="31"/></div><p class="figure-caption">Figure 7.75: R2 score</p><p>You can see that the R2 score has climbed back up to <strong class="source-inline">0.945</strong>, which is way better than the score of <strong class="source-inline">0.568</strong> that <strong class="source-inline">lr_model_3</strong> had. This is already looking like a better model.</p></li>
				<li>Use <strong class="source-inline">ridge_model</strong> to predict on the evaluation data:<p class="source-code">ridge_model_preds = ridge_model.predict(eval_X)</p></li>
				<li>Print the MSE of <strong class="source-inline">ridge_model</strong>:<p class="source-code">print('ridge_model MSE: {}'\</p><p class="source-code">      .format(mean_squared_error(eval_y, ridge_model_preds)))</p><p>The output might be similar to the following:</p><div id="_idContainer389" class="IMG---Figure"><img src="Images/B15019_07_76.jpg" alt="Figure 7.76: The MSE of ridge_model&#13;&#10;" width="699" height="25"/></div><p class="figure-caption">Figure 7.76: The MSE of ridge_model</p><p>You can see from the output that the MSE is <strong class="source-inline">16.030</strong>, which is lower than the MSE value of <strong class="source-inline">126.254</strong> that <strong class="source-inline">lr_model_3</strong> had. You can safely conclude that this is a much better model.</p></li>
				<li>Print out the number of coefficients in <strong class="source-inline">ridge_model</strong>:<p class="source-code">print(len(ridge_model[-1].coef_))</p><p>The output might be similar to the following:</p><div id="_idContainer390" class="IMG---Figure"><img src="Images/B15019_07_77.jpg" alt="Figure 7.77: The number of coefficients in the ridge model&#13;&#10;" width="770" height="30"/></div><p class="figure-caption">Figure 7.77: The number of coefficients in the ridge model</p><p>You can see that this model has <strong class="source-inline">1001</strong> coefficients, which is the same number of coefficients that <strong class="source-inline">lr_model_3</strong> had.</p></li>
				<li>Print out the values of the first 35 coefficients:<p class="source-code">print(ridge_model[-1].coef_[:35])</p><p>The output might be similar to the following:</p><div id="_idContainer391" class="IMG---Figure"><img src="Images/B15019_07_78.jpg" alt="Figure 7.78: The values of the first 35 coefficients&#13;&#10;" width="900" height="208"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.78: The values of the first 35 coefficients</p>
			<p>You can see from the preceding output that the coefficient values no longer have large magnitudes. A lot of the coefficients have a magnitude that is less than 10, with none we can see exceeding 100. This goes to show that the model is no longer overfitting.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3248PPx">https://packt.live/3248PPx</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2E4LWDu">https://packt.live/2E4LWDu</a>.</p>
			<p>This exercise taught you how to fix overfitting by using <strong class="source-inline">RidgeRegression</strong> to train a new model.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/>Activity 7.01: Find an Optimal Model for Predicting the Critical Temperatures of Superconductors</h2>
			<p>You work as a data scientist for a cable manufacturer. Management has decided to start shipping low-resistance cables to clients around the world. To ensure that the right cables are shipped to the right countries, they would like to predict the critical temperatures of various cables based on certain observed readings.</p>
			<p>In this activity, you will train a linear regression model and compute the R2 score and the MSE. You will proceed to engineer new features using polynomial features of degree 3. You will compare the R2 score and MSE of this new model to those of the first model to determine overfitting. You will then use regularization to train a model that generalizes to previously unseen data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You will find the dataset required for the activity here: <a href="https://packt.live/2tJFVqu">https://packt.live/2tJFVqu</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="https://packt.live/3ay3aoe">https://packt.live/3ay3aoe</a>.</p>
			<p class="callout">Citation:</p>
			<p class="callout">Hamidieh, Kam, A data-driven statistical model for predicting the critical temperature of a superconductor, Computational Materials Science, Volume 154, November 2018, pages 346-354.</p>
			<p>The steps to accomplish this task are:</p>
			<ol>
				<li value="1">Open a Colab notebook.</li>
				<li>Load the necessary libraries.</li>
				<li>Read in the data from the <strong class="source-inline">superconduct</strong> folder.</li>
				<li>Prepare the <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> variables.</li>
				<li>Split the data into training and evaluation sets.</li>
				<li>Create a baseline linear regression model.</li>
				<li>Print out the R2 score and MSE of the model.</li>
				<li>Create a pipeline to engineer polynomial features and train a linear regression model.</li>
				<li>Print out the R2 score and MSE.</li>
				<li>Determine that this new model is overfitting.</li>
				<li>Create a pipeline to engineer polynomial features and train a ridge or lasso model.</li>
				<li>Print out the R2 score and MSE.<p>The output will be as follows:</p><div id="_idContainer392" class="IMG---Figure"><img src="Images/B15019_07_79.jpg" alt="Figure 7.79: The R2 score and MSE of the ridge model&#13;&#10;" width="601" height="28"/></div><p class="figure-caption">Figure 7.79: The R2 score and MSE of the ridge model</p></li>
				<li>Determine that this model is no longer overfitting. This is the model to put into production.<p>The coefficients for the ridge model are as shown in the following figure:</p><div id="_idContainer393" class="IMG---Figure"><img src="Images/B15019_07_80.jpg" alt="Figure 7.80: The coefficients for the ridge model&#13;&#10;" width="873" height="230"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.80: The coefficients for the ridge model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor202"/>Summary</h1>
			<p>In this chapter, we studied the importance of withholding some of the available data to evaluate models. We also learned how to make use of all of the available data with a technique called cross-validation to find the best performing model from a set of models you are training. We also made use of evaluation metrics to determine when a model starts to overfit and made use of ridge and lasso regression to fix a model that is overfitting.</p>
			<p>In the next chapter, we will go into hyperparameter tuning in depth. You will learn about various techniques for finding the best hyperparameters to train your models.</p>
		</div>
	</div></body></html>