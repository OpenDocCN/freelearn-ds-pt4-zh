["```py\nCAR car acceptability\n. PRICE overall price\n. . buying buying price\n. . maint price of the maintenance\n. TECH technical characteristics\n. . COMFORT comfort\n. . . doors number of doors\n. . . persons capacity in terms of persons to carry\n. . . lug_boot the size of luggage boot\n. . safety estimated safety of the car\n```", "```py\n    # import libraries\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    # data doesn't have headers, so let's create headers\n    _headers = ['buying', 'maint', 'doors', 'persons', \\\n                'lug_boot', 'safety', 'car']\n    ```", "```py\n    # read in cars dataset\n    df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                     'PacktWorkshops/The-Data-Science-Workshop/'\\\n                     'master/Chapter07/Dataset/car.data', \\\n                     names=_headers, index_col=None)\n    ```", "```py\n    df.info()\n    ```", "```py\n    #split the data into 80% for training and 20% for evaluation\n    training_df, eval_df = train_test_split(df, train_size=0.8, \\\n                                            random_state=0)\n    ```", "```py\n    training_df.info()\n    ```", "```py\n    eval_df.info()\n    ```", "```py\n    \"\"\"\n    split the data into 80% for training and 20% for evaluation \n    using a random state\n    \"\"\"\n    training_df, eval_df = train_test_split(df, train_size=0.8, \\\n                                            random_state=1)\n    ```", "```py\n    #view the head of training_eval\n    training_df.head()\n    ```", "```py\n    #view the top of eval_df\n    eval_df.head()\n    ```", "```py\n    from sklearn.model_selection import KFold\n    ```", "```py\n    _kf = KFold(n_splits=5)\n    ```", "```py\n    indices = _kf.split(df)\n    ```", "```py\n    print(type(indices))\n    ```", "```py\n    #first set\n    train_indices, val_indices = next(indices)\n    ```", "```py\n    train_df = df.drop(val_indices)\n    train_df.info()\n    ```", "```py\n    info() method on the new DataFrame.\n    ```", "```py\n    val_df = df.drop(train_indices)\n    val_df.info()\n    ```", "```py\n    from sklearn.model_selection import KFold\n    #define number of splits\n    n_splits = 5\n    ```", "```py\n    #create an instance of KFold\n    _kf = KFold(n_splits=n_splits)\n    ```", "```py\n    #create splits as _indices\n    _indices = _kf.split(df)\n    ```", "```py\n    _t, _v = [], []\n    ```", "```py\n    #iterate over _indices\n    for i in range(n_splits):\n        train_idx, val_idx = next(_indices)\n        _train_df = df.drop(val_idx)\n        _t.append(_train_df)\n        _val_df = df.drop(train_idx)\n        _v.append(_val_df)\n    ```", "```py\n    for d in _t:\n        print(d.info())\n    ```", "```py\n    for d in _v:\n        print(d.info())\n    ```", "```py\n    # encode categorical variables\n    _df = pd.get_dummies(df, columns=['buying', 'maint', 'doors', \\\n                                      'persons', 'lug_boot', \\\n                                      'safety'])\n    _df.head()\n    ```", "```py\n    # separate features and labels DataFrames\n    features = _df.drop(['car'], axis=1).values\n    labels = _df[['car']].values\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    # create an instance of LogisticRegression\n    _lr = LogisticRegression()\n    ```", "```py\n    from sklearn.model_selection import cross_val_score\n    ```", "```py\n    _scores = cross_val_score(_lr, features, labels, cv=5)\n    ```", "```py\n    print(_scores)\n    ```", "```py\n    # import libraries\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    # data doesn't have headers, so let's create headers\n    _headers = ['buying', 'maint', 'doors', 'persons', \\\n                'lug_boot', 'safety', 'car']\n    ```", "```py\n    # read in cars dataset\n    df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                     'PacktWorkshops/The-Data-Science-Workshop/'\\\n                     'master/Chapter07/Dataset/car.data', \\\n                     names=_headers, index_col=None)\n    ```", "```py\n    df.info()\n    ```", "```py\n    # encode categorical variables\n    _df = pd.get_dummies(df, columns=['buying', 'maint', 'doors', \\\n                                      'persons', 'lug_boot', \\\n                                      'safety'])\n    _df.head()\n    ```", "```py\n    # separate features and labels DataFrames\n    features = _df.drop(['car'], axis=1).values\n    labels = _df[['car']].values\n    ```", "```py\n    from sklearn.linear_model import LogisticRegressionCV\n    ```", "```py\n    model = LogisticRegressionCV(max_iter=2000, multi_class='auto',\\\n                                 cv=5)\n    ```", "```py\n    model.fit(features, labels.ravel())\n    ```", "```py\n    print(model.score(features, labels.ravel()))\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    _headers = ['buying', 'maint', 'doors', 'persons', \\\n                'lug_boot', 'safety', 'car']\n    ```", "```py\n    # read in cars dataset\n    df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                     'PacktWorkshops/The-Data-Science-Workshop/'\\\n                     'master/Chapter07/Dataset/car.data', \\\n                     names=_headers, index_col=None)\n    ```", "```py\n    df.info()\n    ```", "```py\n    _df = pd.get_dummies(df, columns=['buying', 'maint', 'doors',\\\n                                      'persons', 'lug_boot', \\\n                                      'safety'])\n    _df.head()\n    ```", "```py\n    features = _df.drop(['car'], axis=1).values\n    labels = _df[['car']].values\n    ```", "```py\n    import numpy as np\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.model_selection import GridSearchCV\n    ```", "```py\n    clf = DecisionTreeClassifier()\n    ```", "```py\n    params = {'max_depth': np.arange(1, 8)}\n    ```", "```py\n    clf_cv = GridSearchCV(clf, param_grid=params, cv=5)\n    ```", "```py\n    clf_cv.fit(features, labels)\n    ```", "```py\n    print(\"Tuned Decision Tree Parameters: {}\"\\\n          .format(clf_cv.best_params_))\n    ```", "```py\n    print(\"Best score is {}\".format(clf_cv.best_score_))\n    ```", "```py\n    Best score is 0.7777777777777778\n    ```", "```py\n    model = clf_cv.best_estimator_\n    model\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    _headers = ['buying', 'maint', 'doors', 'persons', \\\n                'lug_boot', 'safety', 'car']\n    ```", "```py\n    # read in cars dataset\n    df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                     'PacktWorkshops/The-Data-Science-Workshop/'\\\n                     'master/Chapter07/Dataset/car.data', \\\n                     names=_headers, index_col=None)\n    ```", "```py\n    df.info()\n    ```", "```py\n    _df = pd.get_dummies(df, columns=['buying', 'maint', 'doors',\\\n                                      'persons', 'lug_boot', \\\n                                      'safety'])\n    _df.head()\n    ```", "```py\n    features = _df.drop(['car'], axis=1).values\n    labels = _df[['car']].values\n    ```", "```py\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import RandomizedSearchCV\n    ```", "```py\n    clf = RandomForestClassifier()\n    ```", "```py\n    params = {'n_estimators':[500, 1000, 2000], \\\n              'max_depth': np.arange(1, 8)}\n    ```", "```py\n    clf_cv = RandomizedSearchCV(clf, param_distributions=params, \\\n                                cv=5)\n    ```", "```py\n    clf_cv.fit(features, labels.ravel())\n    ```", "```py\n    print(\"Tuned Random Forest Parameters: {}\"\\\n          .format(clf_cv.best_params_))\n    ```", "```py\n    model = clf_cv.best_estimator_\n    model\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LinearRegression, Lasso\n    from sklearn.metrics import mean_squared_error\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import MinMaxScaler, \\\n    PolynomialFeatures\n    ```", "```py\n    _df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                      'PacktWorkshops/The-Data-Science-Workshop/'\\\n                      'master/Chapter07/Dataset/ccpp.csv')\n    ```", "```py\n    _df.info()\n    ```", "```py\n    X = _df.drop(['PE'], axis=1).values\n    ```", "```py\n    y = _df['PE'].values\n    ```", "```py\n    train_X, eval_X, train_y, eval_y = train_test_split\\\n                                       (X, y, train_size=0.8, \\\n                                        random_state=0)\n    ```", "```py\n    lr_model_1 = LinearRegression()\n    ```", "```py\n    lr_model_1.fit(train_X, train_y)\n    ```", "```py\n    lr_model_1_preds = lr_model_1.predict(eval_X)\n    ```", "```py\n    print('lr_model_1 R2 Score: {}'\\\n          .format(lr_model_1.score(eval_X, eval_y)))\n    ```", "```py\n    print('lr_model_1 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_1_preds)))\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=3)),\\\n             ('lr', LinearRegression())]\n    ```", "```py\n    lr_model_2 = Pipeline(steps)\n    ```", "```py\n    lr_model_2.fit(train_X, train_y)\n    ```", "```py\n    print('lr_model_2 R2 Score: {}'\\\n          .format(lr_model_2.score(eval_X, eval_y)))\n    ```", "```py\n    lr_model_2_preds = lr_model_2.predict(eval_X)\n    ```", "```py\n    print('lr_model_2 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_2_preds)))\n    ```", "```py\n    print(lr_model_2[-1].coef_)\n    ```", "```py\n    print(len(lr_model_2[-1].coef_))\n    ```", "```py\n    35\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=10)),\\\n             ('lr', LinearRegression())]\n    ```", "```py\n    lr_model_3 = Pipeline(steps)\n    ```", "```py\n    lr_model_3.fit(train_X, train_y)\n    ```", "```py\n    print('lr_model_3 R2 Score: {}'\\\n          .format(lr_model_3.score(eval_X, eval_y)))\n    ```", "```py\n    lr_model_3_preds = lr_model_3.predict(eval_X)\n    ```", "```py\n    print('lr_model_3 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_3_preds)))\n    ```", "```py\n    print(len(lr_model_3[-1].coef_))\n    ```", "```py\n    print(lr_model_3[-1].coef_[:35])\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=10)),\\\n             ('lr', Lasso(alpha=0.01))]\n    ```", "```py\n    lasso_model = Pipeline(steps)\n    ```", "```py\n    lasso_model.fit(train_X, train_y)\n    ```", "```py\n    print('lasso_model R2 Score: {}'\\\n          .format(lasso_model.score(eval_X, eval_y)))\n    ```", "```py\n    lasso_preds = lasso_model.predict(eval_X)\n    ```", "```py\n    print('lasso_model MSE: {}'\\\n          .format(mean_squared_error(eval_y, lasso_preds)))\n    ```", "```py\n    print(len(lasso_model[-1].coef_))\n    ```", "```py\n    1001\n    ```", "```py\n    print(lasso_model[-1].coef_[:35])\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LinearRegression, Ridge\n    from sklearn.metrics import mean_squared_error\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import MinMaxScaler, \\\n    PolynomialFeatures\n    ```", "```py\n    _df = pd.read_csv('https://raw.githubusercontent.com/'\\\n                      'PacktWorkshops/The-Data-Science-Workshop/'\\\n                      'master/Chapter07/Dataset/ccpp.csv')\n    ```", "```py\n    _df.info()\n    ```", "```py\n    X = _df.drop(['PE'], axis=1).values\n    ```", "```py\n    y = _df['PE'].values\n    ```", "```py\n    train_X, eval_X, train_y, eval_y = train_test_split\\\n                                       (X, y, train_size=0.8, \\\n                                        random_state=0)\n    ```", "```py\n    lr_model_1 = LinearRegression()\n    ```", "```py\n    lr_model_1.fit(train_X, train_y)\n    ```", "```py\n    lr_model_1_preds = lr_model_1.predict(eval_X)\n    ```", "```py\n    print('lr_model_1 R2 Score: {}'\\\n          .format(lr_model_1.score(eval_X, eval_y)))\n    ```", "```py\n    print('lr_model_1 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_1_preds)))\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=3)),\\\n             ('lr', LinearRegression())]\n    ```", "```py\n    lr_model_2 = Pipeline(steps)\n    ```", "```py\n    lr_model_2.fit(train_X, train_y)\n    ```", "```py\n    print('lr_model_2 R2 Score: {}'\\\n          .format(lr_model_2.score(eval_X, eval_y)))\n    ```", "```py\n    lr_model_2_preds = lr_model_2.predict(eval_X)\n    ```", "```py\n    print('lr_model_2 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_2_preds)))\n    ```", "```py\n    print(lr_model_2[-1].coef_)\n    ```", "```py\n    print(len(lr_model_2[-1].coef_))\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=10)),\\\n             ('lr', LinearRegression())]\n    ```", "```py\n    lr_model_3 = Pipeline(steps)\n    ```", "```py\n    lr_model_3.fit(train_X, train_y)\n    ```", "```py\n    print('lr_model_3 R2 Score: {}'\\\n          .format(lr_model_3.score(eval_X, eval_y)))\n    ```", "```py\n    lr_model_3_preds = lr_model_3.predict(eval_X)\n    ```", "```py\n    print('lr_model_3 MSE: {}'\\\n          .format(mean_squared_error(eval_y, lr_model_3_preds)))\n    ```", "```py\n    print(len(lr_model_3[-1].coef_))\n    ```", "```py\n    1001\n    ```", "```py\n    print(lr_model_3[-1].coef_[:35])\n    ```", "```py\n    steps = [('scaler', MinMaxScaler()),\\\n             ('poly', PolynomialFeatures(degree=10)),\\\n             ('lr', Ridge(alpha=0.9))]\n    ```", "```py\n    ridge_model = Pipeline(steps)\n    ```", "```py\n    ridge_model.fit(train_X, train_y)\n    ```", "```py\n    print('ridge_model R2 Score: {}'\\\n          .format(ridge_model.score(eval_X, eval_y)))\n    ```", "```py\n    ridge_model_preds = ridge_model.predict(eval_X)\n    ```", "```py\n    print('ridge_model MSE: {}'\\\n          .format(mean_squared_error(eval_y, ridge_model_preds)))\n    ```", "```py\n    print(len(ridge_model[-1].coef_))\n    ```", "```py\n    print(ridge_model[-1].coef_[:35])\n    ```"]