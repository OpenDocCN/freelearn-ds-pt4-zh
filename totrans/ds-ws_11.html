<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer526" class="Content">
			</div>
		</div>
		<div id="_idContainer527" class="Content">
			<h1 id="_idParaDest-259"><a id="_idTextAnchor258"/>11. Data Preparation</h1>
		</div>
		<div id="_idContainer583" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">By the end of this chapter you will be able to filter DataFrames with specific conditions; remove duplicate or irrelevant records or columns; convert variables into different data types; replace values in a column and handle missing values and outlier observations.</p>
			<p class="callout">This chapter will introduce you to the main techniques you can use to handle data issues in order to achieve high quality for your dataset prior to modeling it.</p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/>Introduction</h1>
			<p>In the previous chapter, you saw how critical it was to get a very good understanding of your data and learned about different techniques and tools to achieve this goal. While performing <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>) on a given <strong class="bold">dataset</strong>, you may find some potential issues that need to be addressed before the modeling stage. This is exactly the topic that will be covered in this chapter. You will learn how you can handle some of the most frequent data quality issues and prepare the dataset properly.</p>
			<p>This chapter will introduce you to the issues that you will encounter frequently during your data scientist career (such as <strong class="bold">duplicated</strong> <strong class="bold">rows</strong>, incorrect data types, incorrect values, and missing values) and you will learn about the techniques you can use to easily fix them. But be careful – some issues that you come across don't necessarily need to be fixed. Some of the suspicious or unexpected values you find may be genuine from a business point of view. This includes values that crop up very rarely but are totally genuine. Therefore, it is extremely important to get confirmation either from your stakeholder or the data engineering team before you alter the dataset. It is your responsibility to make sure you are making the right decisions for the business while preparing the dataset. </p>
			<p>For instance, in <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em>, you looked at the <em class="italic">Online Retail dataset</em>, which had some negative values in the <strong class="source-inline">Quantity</strong> column. Here, we expected only positive values. But before fixing this issue straight away (by either dropping the records or transforming them into positive values), it is preferable to get in touch with your stakeholders first and get confirmation that these values are not significant for the business. They may tell you that these values are extremely important as they represent returned items and cost the company a lot of money, so they want to analyze these cases in order to reduce these numbers. If you had moved to the data cleaning stage straight away, you would have missed this critical piece of information and potentially came up with incorrect results.</p>
			<h1 id="_idParaDest-261"><a id="_idTextAnchor260"/>Handling Row Duplication</h1>
			<p>Most of the time, the datasets you will receive or have access to will not have been 100% cleaned. They usually have some issues that need to be fixed. One of these issues could be duplicated rows. Row duplication means that several observations contain the exact same information in the dataset. With the <strong class="source-inline">pandas</strong> package, it is extremely easy to find these cases. </p>
			<p>Let's use the example that we saw in <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em>.</p>
			<p>Start by <strong class="bold">importing</strong> the dataset into a DataFrame:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">file_url = 'https://github.com/PacktWorkshops/'\</p>
			<p class="source-code">           'The-Data-Science-Workshop/blob/'\</p>
			<p class="source-code">           'master/Chapter10/dataset/'\</p>
			<p class="source-code">           'Online%20Retail.xlsx?raw=true'</p>
			<p class="source-code">df = pd.read_excel(file_url)</p>
			<p>The <strong class="source-inline">duplicated()</strong> method from <strong class="source-inline">pandas</strong> checks whether any of the rows are duplicates and returns a <strong class="bold">boolean</strong> value for each row, <strong class="source-inline">True</strong> if the row is a duplicate and <strong class="source-inline">False</strong> if not:</p>
			<p class="source-code">df.duplicated()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer528" class="IMG---Figure">
					<img src="Images/B15019_11_01.jpg" alt="Figure 11.1: Output of the duplicated() method&#13;&#10;" width="1496" height="634"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1: Output of the duplicated() method</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The outputs in this chapter have been truncated to effectively use the page area.</p>
			<p>In Python, the <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> binary values correspond to the numerical values 1 and 0, respectively. To find out how many rows have been identified as duplicates, you can use the <strong class="source-inline">sum()</strong> method on the output of <strong class="source-inline">duplicated()</strong>. This will add all the 1s (that is, <strong class="source-inline">True</strong> values) and gives us the count of duplicates:</p>
			<p class="source-code">df.duplicated().sum()</p>
			<p>You should get the following output:</p>
			<p class="source-code">5268</p>
			<p>Since the output of the <strong class="source-inline">duplicated()</strong> method is a <strong class="source-inline">pandas</strong> series of binary values for each row, you can also use it to subset the rows of a DataFrame. The <strong class="source-inline">pandas</strong> package provides different APIs for subsetting a DataFrame, as follows:</p>
			<ul>
				<li>df[&lt;rows&gt; or &lt;columns&gt;]</li>
				<li>df.loc[&lt;rows&gt;, &lt;columns&gt;]</li>
				<li>df.iloc[&lt;rows&gt;, &lt;columns&gt;]</li>
			</ul>
			<p>The first API subsets the DataFrame by <strong class="bold">row</strong> or <strong class="bold">column</strong>. To filter specific columns, you can provide a list that contains their names. For instance, if you want to keep only the variables, that is, <strong class="source-inline">InvoiceNo</strong>, <strong class="source-inline">StockCode</strong>, <strong class="source-inline">InvoiceDate</strong>, and <strong class="source-inline">CustomerID</strong>, you need to use the following code:</p>
			<p class="source-code">df[['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer529" class="IMG---Figure">
					<img src="Images/B15019_11_02.jpg" alt="Figure 11.2: Subsetting columns&#13;&#10;" width="813" height="505"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2: Subsetting columns</p>
			<p>If you only want to filter the rows that are considered duplicates, you can use the same API call with the output of the <strong class="source-inline">duplicated()</strong> method. It will only keep the rows with <strong class="bold">True</strong> as a value:</p>
			<p class="source-code">df[df.duplicated()]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer530" class="IMG---Figure">
					<img src="Images/B15019_11_03.jpg" alt="Figure 11.3: Subsetting the duplicated rows&#13;&#10;" width="1398" height="493"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3: Subsetting the duplicated rows</p>
			<p>If you want to subset the rows and columns at the same time, you must use one of the other two available APIs: <strong class="source-inline">.loc</strong> or <strong class="source-inline">.iloc</strong>. These APIs do the exact same thing but <strong class="source-inline">.loc</strong> uses labels or names while <strong class="source-inline">.iloc</strong> only takes indices as input. You will use the <strong class="source-inline">.loc</strong> API to subset the duplicated rows and keep only the selected four columns, as shown in the previous example:</p>
			<p class="source-code">df.loc[df.duplicated(), ['InvoiceNo', 'StockCode', \</p>
			<p class="source-code">                         'InvoiceDate', 'CustomerID']]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer531" class="IMG---Figure">
					<img src="Images/B15019_11_04.jpg" alt="Figure 11.4: Subsetting the duplicated rows and selected columns using .loc&#13;&#10;" width="730" height="503"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4: Subsetting the duplicated rows and selected columns using .loc</p>
			<p>This preceding output shows that the first few duplicates are row numbers <strong class="source-inline">517</strong>, <strong class="source-inline">527</strong>, <strong class="source-inline">537</strong>, and so on. By default, <strong class="source-inline">pandas</strong> doesn't mark the first occurrence of duplicates as duplicates: all the same, duplicates will have a value of <strong class="source-inline">True</strong> except for the first occurrence. You can change this behavior by specifying the <strong class="source-inline">keep</strong> parameter. If you want to keep the last duplicate, you need to specify <strong class="source-inline">keep='last'</strong>:</p>
			<p class="source-code">df.loc[df.duplicated(keep='last'), ['InvoiceNo', 'StockCode', \</p>
			<p class="source-code">                                    'InvoiceDate', 'CustomerID']]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer532" class="IMG---Figure">
					<img src="Images/B15019_11_05.jpg" alt="Figure 11.5: Subsetting the last duplicated rows&#13;&#10;" width="826" height="500"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5: Subsetting the last duplicated rows</p>
			<p>As you can see from the previous outputs, row <strong class="source-inline">485</strong> has the same value as row <strong class="source-inline">539</strong>. As expected, row <strong class="source-inline">539</strong> is not marked as a duplicate anymore. If you want to mark all the duplicate records as duplicates, you will have to use <strong class="source-inline">keep=False</strong>:</p>
			<p class="source-code">df.loc[df.duplicated(keep=False), ['InvoiceNo', 'StockCode',\</p>
			<p class="source-code">                                   'InvoiceDate', 'CustomerID']]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer533" class="IMG---Figure">
					<img src="Images/B15019_11_06.jpg" alt="Figure 11.6: Subsetting all the duplicated rows&#13;&#10;" width="792" height="496"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6: Subsetting all the duplicated rows</p>
			<p>This time, rows <strong class="source-inline">485</strong> and <strong class="source-inline">539</strong> have been listed as duplicates. Now that you know how to identify duplicate observations, you can decide whether you wish to remove them from the dataset. As we mentioned previously, you must be careful when changing the data. You may want to confirm with the business that they are comfortable with you doing so. You will have to explain the reason why you want to remove these rows. In the Online Retail dataset, if you take rows <strong class="source-inline">485</strong> and <strong class="source-inline">539</strong> as an example, these two observations are identical. From a business perspective, this means that a specific customer (<strong class="source-inline">CustomerID 17908</strong>) has bought the same item (<strong class="source-inline">StockCode 22111</strong>) at the exact same date and time (<strong class="source-inline">InvoiceDate 2010-12-01 11:45:00</strong>) on the same invoice (<strong class="source-inline">InvoiceNo 536409</strong>). This is highly suspicious. When you're talking with the business, they may tell you that new software was released at that time and there was a bug that was splitting all the purchased items into single-line items. </p>
			<p>In this case, you know that you shouldn't remove these rows. On the other hand, they may tell you that duplication shouldn't happen and that it may be due to human error as the data was entered or during the data extraction step. Let's assume this is the case; now, it is safe for you to remove these rows. </p>
			<p>To do so, you can use the <strong class="source-inline">drop_duplicates()</strong> method from <strong class="source-inline">pandas</strong>. It has the same <strong class="source-inline">keep</strong> parameter as <strong class="source-inline">duplicated()</strong>, which specifies which duplicated record you want to keep or if you want to remove all of them. In this case, we want to keep at least one duplicate row. Here, we want to keep the first occurrence: </p>
			<p class="source-code">df.drop_duplicates(keep='first')</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer534" class="IMG---Figure">
					<img src="Images/B15019_11_07.jpg" alt="Figure 11.7: Dropping duplicate rows with keep='first'&#13;&#10;" width="1106" height="221"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7: Dropping duplicate rows with keep='first'</p>
			<p>The output of this method is a new DataFrame that contains unique records where only the first occurrence of duplicates has been kept. If you want to replace the existing DataFrame rather than getting a new DataFrame, you need to use the <strong class="source-inline">inplace=True</strong> parameter.</p>
			<p>The <strong class="source-inline">drop_duplicates()</strong> and <strong class="source-inline">duplicated()</strong> methods also have another very useful parameter: <strong class="source-inline">subset</strong>. This parameter allows you to specify the list of columns to consider while looking for duplicates. By default, all the columns of a DataFrame are used to find duplicate rows. Let's see how many duplicate rows there are while only looking at the <strong class="source-inline">InvoiceNo</strong>, <strong class="source-inline">StockCode</strong>, <strong class="source-inline">invoiceDate</strong>, and <strong class="source-inline">CustomerID</strong> columns:</p>
			<p class="source-code">df.duplicated(subset=['InvoiceNo', 'StockCode', 'InvoiceDate',\</p>
			<p class="source-code">                      'CustomerID'], keep='first').sum()</p>
			<p>You should get the following output:</p>
			<p class="source-code">10677</p>
			<p>By looking only at these four columns instead of all of them, we can see that the number of duplicate rows has increased from <strong class="source-inline">5268</strong> to <strong class="source-inline">10677</strong>. This means that there are rows that have the exact same values as these four columns but have different values in other columns, which means they may be different records. In this case, it is better to use all the columns to identify duplicate records.</p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor261"/>Exercise 11.01: Handling Duplicates in a Breast Cancer Dataset</h2>
			<p>In this exercise, you will learn how to identify duplicate records and how to handle such issues so that the dataset only contains <strong class="bold">unique</strong> records. Let's get started:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that we're using in this exercise is the Breast Cancer Detection dataset, which has been shared by Dr. William H. Wolberg from the University of Wisconsin Hospitals and is hosted by the UCI Machine Learning Repository. The attribute information for this dataset can be found here: <a href="https://packt.live/39LaIDx">https://packt.live/39LaIDx</a>.</p>
			<p class="callout">This dataset can also be found in this book's GitHub repository: <a href="https://packt.live/2QqbHBC">https://packt.live/2QqbHBC</a>.</p>
			<ol>
				<li>Open a new <strong class="bold">Colab</strong> notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package:<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the <strong class="source-inline">Breast Cancer</strong> dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter11/dataset/'\</p><p class="source-code">           'breast-cancer-wisconsin.data'</p></li>
				<li>Using the <strong class="source-inline">read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package, load the dataset into a new variable called <strong class="source-inline">df</strong> with the <strong class="source-inline">header=None</strong> parameter. We're doing this because this file doesn't contain column names:<p class="source-code">df = pd.read_csv(file_url, header=None)</p></li>
				<li>Create a variable called <strong class="source-inline">col_names</strong> that contains the names of the columns: <strong class="source-inline">Sample code number, Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses</strong>, and <strong class="source-inline">Class</strong>:<p class="callout-heading">Note</p><p class="callout">Information about the names that have been specified in this file can be found here: <a href="https://packt.live/39J7hgT">https://packt.live/39J7hgT</a>.</p><p class="source-code">col_names = ['Sample code number','Clump Thickness',\</p><p class="source-code">             'Uniformity of Cell Size',\</p><p class="source-code">             'Uniformity of Cell Shape',\</p><p class="source-code">             'Marginal Adhesion','Single Epithelial Cell Size',\</p><p class="source-code">             'Bare Nuclei','Bland Chromatin',\</p><p class="source-code">             'Normal Nucleoli','Mitoses','Class'] </p></li>
				<li>Assign the column names of the DataFrame using the <strong class="source-inline">columns</strong> attribute:<p class="source-code">df.columns = col_names</p></li>
				<li>Display the shape of the DataFrame using the <strong class="source-inline">.shape</strong> attribute:<p class="source-code">df.shape</p><p>You should get the following output:</p><p class="source-code">(699, 11)</p><p>This DataFrame contains <strong class="source-inline">699</strong> rows and <strong class="source-inline">11</strong> columns.</p></li>
				<li>Display the first five rows of the DataFrame using the <strong class="source-inline">head()</strong> method:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer535" class="IMG---Figure"><img src="Images/B15019_11_08.jpg" alt="Figure 11.8: The first five rows of the Breast Cancer dataset&#13;&#10;" width="1397" height="397"/></div><p class="figure-caption">Figure 11.8: The first five rows of the Breast Cancer dataset</p><p>All the variables are numerical. The Sample code number column is an identifier for the measurement samples.</p></li>
				<li>Find the number of duplicate rows using the <strong class="source-inline">duplicated()</strong> and <strong class="source-inline">sum()</strong> methods:<p class="source-code">df.duplicated().sum()</p><p>You should get the following output:</p><p class="source-code">8</p><p>Looking at the 11 columns in this dataset, we can see that there are <strong class="source-inline">8</strong> duplicate rows.</p></li>
				<li>Display the duplicate rows using the <strong class="source-inline">loc()</strong> and <strong class="source-inline">duplicated()</strong> methods:<p class="source-code">df.loc[df.duplicated()]</p><p>You should get the following output:</p><div id="_idContainer536" class="IMG---Figure"><img src="Images/B15019_11_09.jpg" alt="Figure 11.9: Duplicate records&#13;&#10;" width="805" height="326"/></div><p class="figure-caption">Figure 11.9: Duplicate records</p><p>The following rows are duplicates: <strong class="source-inline">208</strong>, <strong class="source-inline">253</strong>, <strong class="source-inline">254</strong>, <strong class="source-inline">258</strong>, <strong class="source-inline">272</strong>, <strong class="source-inline">338</strong>, <strong class="source-inline">561</strong>, and <strong class="source-inline">684</strong>.</p></li>
				<li>Display the duplicate rows just like we did in <em class="italic">Step 9</em>, but with the <strong class="source-inline">keep='last'</strong> parameter instead:<p class="source-code">df.loc[df.duplicated(keep='last')]</p><p>You should get the following output:</p><div id="_idContainer537" class="IMG---Figure"><img src="Images/B15019_11_10.jpg" alt="Figure 11.10: Duplicate records with keep='last'&#13;&#10;" width="801" height="328"/></div><p class="figure-caption">Figure 11.10: Duplicate records with keep='last'</p><p>By using the <strong class="source-inline">keep='last'</strong> parameter, the following rows are considered duplicates: <strong class="source-inline">42</strong>, <strong class="source-inline">62</strong>, <strong class="source-inline">168</strong>, <strong class="source-inline">207</strong>, <strong class="source-inline">267</strong>, <strong class="source-inline">314</strong>, <strong class="source-inline">560</strong>, and <strong class="source-inline">683</strong>. By comparing this output to the one from the previous step, we can see that rows 253 and 42 are identical.</p></li>
				<li>Remove the duplicate rows using the <strong class="source-inline">drop_duplicates()</strong> method along with the <strong class="source-inline">keep='first'</strong> parameter and save this into a new DataFrame called <strong class="source-inline">df_unique</strong>:<p class="source-code">df_unique = df.drop_duplicates(keep='first')</p></li>
				<li>Display the shape of <strong class="source-inline">df_unique</strong> with the <strong class="source-inline">.shape</strong> attribute:<p class="source-code">df_unique.shape</p><p>You should get the following output:</p><p class="source-code">(691, 11)</p><p>Now that we have removed the eight duplicate records, only <strong class="source-inline">691</strong> rows remain. Now, the dataset only contains unique observations.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2EiArYI">https://packt.live/2EiArYI</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/349tLat">https://packt.live/349tLat</a>.</p></li>
			</ol>
			<p>In this exercise, you learned how to identify and remove duplicate records from a real-world dataset.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor262"/>Converting Data Types</h1>
			<p>Another problem you may face in a project is incorrect data types being inferred for some columns. As we saw in <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em>, the <strong class="source-inline">pandas</strong> package provides us with a very easy way to display the data type of each column using the <strong class="source-inline">.dtypes</strong> attribute. You may be wondering, when did <strong class="source-inline">pandas</strong> identify the type of each column? The types are detected when you load the dataset into a <strong class="source-inline">pandas</strong> DataFrame using methods such as <strong class="source-inline">read_csv()</strong>, <strong class="source-inline">read_excel()</strong>, and so on. </p>
			<p>When you've done this, <strong class="source-inline">pandas</strong> will try its best to automatically find the best type according to the values contained in each column. Let's see how this works on the <strong class="source-inline">Online Retail</strong> dataset.</p>
			<p>First, you must import <strong class="source-inline">pandas</strong>:</p>
			<p class="source-code">import pandas as pd</p>
			<p>Then, you need to assign the URL to the dataset to a new variable:</p>
			<p class="source-code">file_url = 'https://github.com/PacktWorkshops/'\</p>
			<p class="source-code">           'The-Data-Science-Workshop/blob/'\</p>
			<p class="source-code">           'master/Chapter10/dataset/'\</p>
			<p class="source-code">           'Online%20Retail.xlsx?raw=true'</p>
			<p>Let's load the dataset into a <strong class="source-inline">pandas</strong> DataFrame using <strong class="source-inline">read_excel()</strong>:</p>
			<p class="source-code">df = pd.read_excel(file_url)</p>
			<p>Finally, let's print the data type of each column:</p>
			<p class="source-code">df.dtypes</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer538" class="IMG---Figure">
					<img src="Images/B15019_11_11.jpg" alt="Figure 11.11: The data type of each column of the Online Retail dataset&#13;&#10;" width="986" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11: The data type of each column of the Online Retail dataset</p>
			<p>The preceding output shows the data types that have been assigned to each column. <strong class="source-inline">Quantity</strong>, <strong class="source-inline">UnitPrice</strong>, and <strong class="source-inline">CustomerID</strong> have been identified as numerical variables (<strong class="source-inline">int64</strong>, <strong class="source-inline">float64</strong>), <strong class="source-inline">InvoiceDate</strong> is a <strong class="source-inline">datetime</strong> variable, and all the other columns are considered text (<strong class="source-inline">object</strong>). This is not too bad. <strong class="source-inline">pandas</strong> did a great job of recognizing non-text columns.</p>
			<p>But what if you want to change the types of some columns? You have two ways to achieve this.</p>
			<p>The first way is to reload the dataset, but this time, you will need to specify the data types of the columns of interest using the <strong class="source-inline">dtype</strong> parameter. This parameter takes a dictionary with the column names as keys and the correct data types as values, such as {'col1': np.float64, 'col2': np.int32}, as input. Let's try this on <strong class="source-inline">CustomerID</strong>. We know this isn't a numerical variable as it contains a unique <strong class="bold">identifier</strong> (code). Here, we are going to change its type to <strong class="bold">object</strong>:</p>
			<p class="source-code">df = pd.read_excel(file_url, dtype={'CustomerID': 'category'})</p>
			<p class="source-code">df.dtypes</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer539" class="IMG---Figure">
					<img src="Images/B15019_11_12.jpg" alt="Figure 11.12: The data types of each column after converting CustomerID&#13;&#10;" width="1073" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12: The data types of each column after converting CustomerID</p>
			<p>As you can see, the data type for <strong class="source-inline">CustomerID</strong> has effectively changed to a <strong class="source-inline">category</strong> type. </p>
			<p>Now, let's look at the second way of converting a single column into a different type. In <strong class="source-inline">pandas</strong>, you can use the <strong class="source-inline">astype()</strong> method and specify the new data type that it will be converted into as its <strong class="bold">parameter</strong>. It will return a new column (a new <strong class="source-inline">pandas</strong> series, to be more precise), so you need to reassign it to the same column of the DataFrame. For instance, if you want to change the <strong class="source-inline">InvoiceNo</strong> column into a categorical variable, you would do the following:</p>
			<p class="source-code">df['InvoiceNo'] = df['InvoiceNo'].astype('category')</p>
			<p class="source-code">df.dtypes</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer540" class="IMG---Figure">
					<img src="Images/B15019_11_13.jpg" alt="Figure 11.13: The data types of each column after converting InvoiceNo&#13;&#10;" width="960" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13: The data types of each column after converting InvoiceNo</p>
			<p>As you can see, the data type for <strong class="source-inline">InvoiceNo</strong> has changed to a categorical variable. The difference between <strong class="source-inline">object</strong> and <strong class="source-inline">category</strong> is that the latter has a finite number of possible values (also called discrete variables). Once these have been changed into categorical variables, <strong class="source-inline">pandas</strong> will automatically list all the values. They can be accessed using the <strong class="source-inline">.cat.categories</strong> attribute:</p>
			<p class="source-code">df['InvoiceNo'].cat.categories</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer541" class="IMG---Figure">
					<img src="Images/B15019_11_14.jpg" alt="Figure 11.14: List of categories (possible values) for the InvoiceNo categorical variable&#13;&#10;" width="1218" height="210"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14: List of categories (possible values) for the InvoiceNo categorical variable</p>
			<p><strong class="source-inline">pandas</strong> has identified that there are 25,900 different values in this column and has listed all of them. Depending on the data type that's assigned to a variable, <strong class="source-inline">pandas</strong> provides different attributes and methods that are very handy for data transformation or feature engineering (this will be covered in <em class="italic">Chapter 12</em>, <em class="italic">Feature Engineering</em>).</p>
			<p>As a final note, you may be wondering when you would use the first way of changing the types of certain columns (while loading the dataset). To find out the current type of each variable, you must load the data first, so why will you need to reload the data again with new data types? It will be easier to change the type with the <strong class="source-inline">astype()</strong> method after the first load. There are a few reasons why you would use it. One reason could be that you have already explored the dataset on a different tool, such as Excel, and already know what the correct data types are. </p>
			<p>The second reason could be that your dataset is big, and you cannot load it in its entirety. As you may have noticed, by default, <strong class="source-inline">pandas</strong> use 64-bit encoding for numerical variables. This requires a lot of memory and may be overkill. </p>
			<p>For example, the <strong class="source-inline">Quantity</strong> column has an int64 data type, which means that the range of possible values is -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. However, in <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em> while analyzing the distribution of this column, you learned that the range of values for this column is only from -80,995 to 80,995. You don't need to use so much space. By reducing the data type of this variable to int32 (which ranges from -2,147,483,648 to 2,147,483,647), you may be able to reload the entire dataset.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor263"/>Exercise 11.02: Converting Data Types for the Ames Housing Dataset</h2>
			<p>In this exercise, you will prepare a dataset by converting its variables into the correct data types.</p>
			<p>You will use the Ames Housing dataset to do this, which we also used in <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em>. For more information about this dataset, refer to the following note. Let's get started:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that's being used in this exercise is the Ames Housing dataset, which has been compiled by Dean De Cock: <a href="https://packt.live/2QTbTbq">https://packt.live/2QTbTbq</a>.</p>
			<p class="callout">For your convenience, this dataset has been uploaded to this book's GitHub repository: <a href="https://packt.live/2ZUk4bz">https://packt.live/2ZUk4bz</a>.</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package:<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the Ames dataset to a variable called <strong class="source-inline">file_url</strong>: <p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter10/dataset/ames_iowa_housing.csv'</p></li>
				<li>Using the <strong class="source-inline">read_csv</strong> method from the <strong class="source-inline">pandas</strong> package, load the dataset into a new variable called <strong class="source-inline">df</strong>:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Print the data type of each column using the <strong class="source-inline">dtypes</strong> attribute:<p class="source-code">df.dtypes</p><p>You should get the following output:</p><div id="_idContainer542" class="IMG---Figure"><img src="Images/B15019_11_15.jpg" alt="Figure 11.15: List of columns and their assigned data types&#13;&#10;" width="1000" height="347"/></div><p class="figure-caption">Figure 11.15: List of columns and their assigned data types</p><p class="callout-heading">Note</p><p class="callout">The preceding output has been truncated.</p><p>From <em class="italic">Chapter 10</em>, <em class="italic">Analyzing a Dataset</em> you know that the <strong class="source-inline">Id</strong>, <strong class="source-inline">MSSubClass</strong>, <strong class="source-inline">OverallQual</strong>, and <strong class="source-inline">OverallCond</strong> columns have been incorrectly classified as numerical variables. They have a finite number of unique values and you can't perform any mathematical operations on them. For example, it doesn't make sense to add, remove, multiply, or divide two different values from the <strong class="source-inline">Id</strong> column. Therefore, you need to convert them into categorical variables.</p></li>
				<li>Using the <strong class="source-inline">astype()</strong> method, convert the <strong class="source-inline">'Id'</strong> column into a categorical variable, as shown in the following code snippet:<p class="source-code">df['Id'] = df['Id'].astype('category')</p></li>
				<li>Convert the <strong class="source-inline">'MSSubClass'</strong>, <strong class="source-inline">'OverallQual'</strong>, and <strong class="source-inline">'OverallCond'</strong> columns into categorical variables, like we did in the previous step:<p class="source-code">df['MSSubClass'] = df['MSSubClass'].astype('category')</p><p class="source-code">df['OverallQual'] = df['OverallQual'].astype('category')</p><p class="source-code">df['OverallCond'] = df['OverallCond'].astype('category')</p></li>
				<li>Create a for loop that will iterate through the four categorical columns <strong class="source-inline">('Id', 'MSSubClass', 'OverallQual', </strong>and<strong class="source-inline"> 'OverallCond'</strong>) and print their names and categories using the <strong class="source-inline">.cat.categories</strong> attribute:<p class="source-code">for col_name in ['Id', 'MSSubClass', 'OverallQual', \</p><p class="source-code">                 'OverallCond']:</p><p class="source-code">    print(col_name)</p><p class="source-code">    print(df[col_name].cat.categories)</p><p>You should get the following output:</p><div id="_idContainer543" class="IMG---Figure"><img src="Images/B15019_11_16.jpg" alt="Figure 11.16: List of categories for the four newly converted variables&#13;&#10;" width="1155" height="285"/></div><p class="figure-caption">Figure 11.16: List of categories for the four newly converted variables</p><p>Now, these four columns have been converted into categorical variables. From the output of <em class="italic">Step 5</em>, we can see that there are a lot of variables of the <strong class="source-inline">object</strong> type. Let's have a look at them and see if they need to be converted as well.</p></li>
				<li>Create a new DataFrame called <strong class="source-inline">obj_df</strong> that will only contain variables of the <strong class="source-inline">object</strong> type using the <strong class="source-inline">select_dtypes</strong> method along with the <strong class="source-inline">include='object'</strong> parameter:<p class="source-code">obj_df = df.select_dtypes(include='object')</p></li>
				<li>Create a new variable called <strong class="source-inline">obj_cols</strong> that contains a list of column names from the <strong class="source-inline">obj_df</strong> DataFrame using the <strong class="source-inline">.columns</strong> attribute and display its content:<p class="source-code">obj_cols = obj_df.columns</p><p class="source-code">obj_cols</p><p>You should get the following output:</p><div id="_idContainer544" class="IMG---Figure"><img src="Images/B15019_11_17.jpg" alt="Figure 11.17: List of variables of the 'object' type&#13;&#10;" width="1002" height="261"/></div><p class="figure-caption">Figure 11.17: List of variables of the 'object' type</p></li>
				<li>Like we did in <em class="italic">Step 8</em>, create a <strong class="source-inline">for</strong> loop that will iterate through the column names contained in <strong class="source-inline">obj_cols</strong> and print their names and unique values using the <strong class="source-inline">unique()</strong> method:<p class="source-code">for col_name in obj_cols:</p><p class="source-code">    print(col_name)</p><p class="source-code">    print(df[col_name].unique())</p><p>You should get the following output:</p><div id="_idContainer545" class="IMG---Figure"><img src="Images/B15019_11_18.jpg" alt="Figure 11.18: List of unique values for each variable of the 'object' type&#13;&#10;" width="658" height="368"/></div><p class="figure-caption">Figure 11.18: List of unique values for each variable of the 'object' type</p><p>As you can see, all these columns have a finite number of unique values that are composed of text, which shows us that they are categorical variables.</p></li>
				<li>Now, create a <strong class="source-inline">for</strong> loop that will iterate through the column names contained in <strong class="source-inline">obj_cols</strong> and convert each of them into a categorical variable using the <strong class="source-inline">astype()</strong> method:<p class="source-code">for col_name in obj_cols:</p><p class="source-code">    df[col_name] = df[col_name].astype('category')</p></li>
				<li>Print the data type of each column using the <strong class="source-inline">dtypes</strong> attribute:<p class="source-code">df.dtypes</p><p>You should get the following output:</p><div id="_idContainer546" class="IMG---Figure"><img src="Images/B15019_11_19.jpg" alt="Figure 11.19: List of variables and their new data types&#13;&#10;" width="771" height="336"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.19: List of variables and their new data types</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding output has been truncated.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2FvR8R6">https://packt.live/2FvR8R6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3aAmKka">https://packt.live/3aAmKka</a>.</p>
			<p>You have successfully converted the columns that have incorrect data types (numerical or object) into categorical variables. Your dataset is now one step closer to being prepared for modeling.</p>
			<p>In the next section, we will look at handling incorrect values.</p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor264"/>Handling Incorrect Values</h1>
			<p>Another issue you may face with a new dataset is incorrect values for some of the observations in the dataset. Sometimes, this is due to a syntax error; for instance, the name of a country may be written all in lower case, all in upper case, as a title (where only the first letter is capitalized), or may even be abbreviated. France may take different values, such as 'France', 'FRANCE', 'france', 'FR', and so on. If you define 'France' as the standard format, then all the other variants are considered incorrect values in the dataset and need to be fixed.</p>
			<p>If this kind of issue is not handled before the modeling phase, it can lead to incorrect results. The model will think these different variants are completely different values and may pay less attention to these values since they have separated frequencies. For instance, let's say that 'France' represents 2% of the value, 'FRANCE' 2% and 'FR' 1%. You know that these values correspond to the same country and should represent 5% of the values, but the model will consider them as different countries.</p>
			<p>Let's learn how to detect such issues in real life by using the <strong class="source-inline">Online Retail</strong> dataset. </p>
			<p>First, you need to load the data into a <strong class="source-inline">pandas</strong> DataFrame:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">file_url = 'https://github.com/PacktWorkshops/'\</p>
			<p class="source-code">           'The-Data-Science-Workshop/blob/'\</p>
			<p class="source-code">           'master/Chapter10/dataset/'\</p>
			<p class="source-code">           'Online%20Retail.xlsx?raw=true'</p>
			<p class="source-code">df = pd.read_excel(file_url)</p>
			<p>In this dataset, there are two variables that seem to be related to each other: <strong class="source-inline">StockCode</strong> and <strong class="source-inline">Description</strong>. The first one contains the identifier code of the items sold and the other one contains their descriptions. However, if you look at some of the examples, such as <strong class="source-inline">StockCode 23131</strong>, the <strong class="source-inline">Description</strong> column has different values:</p>
			<p class="source-code">df.loc[df['StockCode'] == 23131, 'Description'].unique()</p>
			<p>You should get the following output</p>
			<div>
				<div id="_idContainer547" class="IMG---Figure">
					<img src="Images/B15019_11_20.jpg" alt="Figure 11.20: List of unique values for the Description column and StockCode 23131&#13;&#10;" width="1185" height="109"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.20: List of unique values for the Description column and StockCode 23131</p>
			<p>There are multiple issues in the preceding output. One issue is that the word <strong class="source-inline">Mistletoe</strong> has been misspelled so that it reads <strong class="source-inline">Miseltoe</strong>. The other errors are unexpected values and missing values, which will be covered in the next section. It seems that the <strong class="source-inline">Description</strong> column has been used to record comments such as <strong class="source-inline">had been put aside</strong>. </p>
			<p>Let's focus on the misspelling issue. What we need to do here is modify the incorrect spelling and replace it with the correct value. First, let's create a new column called <strong class="source-inline">StockCodeDescription</strong>, which is an exact copy of the <strong class="source-inline">Description</strong> column:</p>
			<p class="source-code">df['StockCodeDescription'] = df['Description']</p>
			<p>You will use this new column to fix the misspelling issue. To do this, use the subsetting technique you learned about earlier in this chapter. You need to use <strong class="source-inline">.loc</strong> and filter the rows and columns you want, that is, all rows with <strong class="source-inline">StockCode == 21131</strong> and <strong class="source-inline">Description == MISELTOE HEART WREATH CREAM</strong> and the <strong class="source-inline">Description</strong> column:</p>
			<p class="source-code">df.loc[(df['StockCode'] == 23131) \</p>
			<p class="source-code">        &amp; (df['StockCodeDescription'] \</p>
			<p class="source-code">           == 'MISELTOE HEART WREATH CREAM'), \</p>
			<p class="source-code">        'StockCodeDescription'] = 'MISTLETOE HEART WREATH CREAM'</p>
			<p>If you reprint the value for this issue, you will see that the misspelling value has been fixed and is not present anymore:</p>
			<p class="source-code">df.loc[df['StockCode'] == 23131, 'StockCodeDescription'].unique()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer548" class="IMG---Figure">
					<img src="Images/B15019_11_21.jpg" alt="Figure 11.21: List of unique values for the Description column and StockCode 23131 &#13;&#10;after fixing the first misspelling issue&#13;&#10;" width="1252" height="75"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.21: List of unique values for the Description column and StockCode 23131 after fixing the first misspelling issue</p>
			<p>As you can see, there are still five different values for this product, but for one of them, that is, <strong class="source-inline">MISTLETOE</strong>, has been spelled incorrectly: <strong class="source-inline">MISELTOE</strong>.</p>
			<p>This time, rather than looking at an exact match (a word must be the same as another one), we will look at performing a partial match (part of a word will be present in another word). In our case, instead of looking at the spelling of <strong class="source-inline">MISELTOE</strong>, we will only look at <strong class="source-inline">MISEL</strong>. The <strong class="source-inline">pandas</strong> package provides a method called <strong class="source-inline">.str.contains()</strong> that we can use to look for observations that partially match with a given expression. </p>
			<p>Let's use this to see if we have the same misspelling issue (<strong class="source-inline">MISEL</strong>) in the entire dataset. You will need to add one additional parameter since this method doesn't handle missing values. You will also have to subset the rows that don't have missing values for the <strong class="source-inline">Description</strong> column. This can be done by providing the <strong class="source-inline">na=False</strong> parameter to the <strong class="source-inline">.str.contains()</strong> method:</p>
			<p class="source-code">df.loc[df['StockCodeDescription']\</p>
			<p class="source-code">  .str.contains('MISEL', na=False),]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer549" class="IMG---Figure">
					<img src="Images/B15019_11_22.jpg" alt="Figure 11.22: Displaying all the rows containing the misspelling 'MISELTOE'&#13;&#10;" width="944" height="460"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.22: Displaying all the rows containing the misspelling 'MISELTOE'</p>
			<p>This misspelling issue (<strong class="source-inline">MISELTOE</strong>) is not only related to <strong class="source-inline">StockCode 23131</strong>, but also to other items. You will need to fix all of these using the <strong class="source-inline">str.replace()</strong> method. This method takes the string of characters to be replaced and the replacement string as parameters:</p>
			<p class="source-code">df['StockCodeDescription'] = df['StockCodeDescription']\</p>
			<p class="source-code">                             .str.replace\</p>
			<p class="source-code">                             ('MISELTOE', 'MISTLETOE')</p>
			<p>Now, if you print all the rows that contain the misspelling of <strong class="source-inline">MISEL</strong>, you will see that no such rows exist anymore: </p>
			<p class="source-code">df.loc[df['StockCodeDescription']\</p>
			<p class="source-code">  .str.contains('MISEL', na=False),]</p>
			<p>You should get the following output</p>
			<div>
				<div id="_idContainer550" class="IMG---Figure">
					<img src="Images/B15019_11_23.jpg" alt="Figure 11.23: Displaying all the rows containing the misspelling MISELTOE after cleaning up&#13;&#10;" width="1665" height="37"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.23: Displaying all the rows containing the misspelling MISELTOE after cleaning up</p>
			<p>You just saw how easy it is to clean observations that have incorrect values using the <strong class="source-inline">.str.contains</strong> and <strong class="source-inline">.str.replace()</strong> methods that are provided by the <strong class="source-inline">pandas</strong> package. These methods can only be used for variables containing strings, but the same logic can be applied to numerical variables and can also be used to handle extreme values or outliers. You can use the ==, &gt;, &lt;, &gt;=, or &lt;= operator to subset the rows you want and then replace the observations with the correct values.</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor265"/>Exercise 11.03: Fixing Incorrect Values in the State Column</h2>
			<p>In this exercise, you will clean the <strong class="source-inline">State</strong> variable in a modified version of a dataset by listing all the finance officers in the USA. We are doing this because the dataset contains some incorrect values. Let's get started:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The original dataset was shared by Forest Gregg and Derek Eder and can be found at <a href="https://packt.live/2rTJVns">https://packt.live/2rTJVns</a>.</p>
			<p class="callout">The modified dataset that we're using here is available in this book's GitHub repository: <a href="https://packt.live/2MZJsrk">https://packt.live/2MZJsrk</a>.</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package:<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter11/dataset/officers.csv'</p></li>
				<li>Using the <strong class="source-inline">read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package, load the dataset into a new variable called <strong class="source-inline">df</strong>:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Print the first five rows of the DataFrame using the <strong class="source-inline">.head()</strong> method:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer551" class="IMG---Figure"><img src="Images/B15019_11_24.jpg" alt="Figure 11.24: The first five rows of the finance officers dataset&#13;&#10;" width="752" height="260"/></div><p class="figure-caption">Figure 11.24: The first five rows of the finance officers dataset</p></li>
				<li>Print out all the unique values of the <strong class="source-inline">State</strong> variable:<p class="source-code">df['State'].unique()</p><p>You should get the following output:</p><div id="_idContainer552" class="IMG---Figure"><img src="Images/B15019_11_25.jpg" alt="Figure 11.25: List of unique values in the State column&#13;&#10;" width="1219" height="177"/></div><p class="figure-caption">Figure 11.25: List of unique values in the State column</p><p>All the states have been encoded into a two-capitalized character format. As you can see, there are some incorrect values with non-capitalized characters, such as <strong class="source-inline">il</strong> and <strong class="source-inline">iL</strong> (they look like spelling errors for Illinois), and unexpected values such as <strong class="source-inline">8I</strong>, <strong class="source-inline">I</strong>, and <strong class="source-inline">60</strong>. In the next few steps, you are going to fix these issues.</p></li>
				<li>Print out the rows that have the <strong class="source-inline">il</strong> value in the <strong class="source-inline">State</strong> column using the <strong class="source-inline">pandas</strong> <strong class="source-inline">.str.contains()</strong> method and the subsetting API, that is, DataFrame [condition]. You will also have to set the <strong class="source-inline">na</strong> parameter to <strong class="source-inline">False</strong> in <strong class="source-inline">str.contains()</strong> in order to exclude observations with missing values:<p class="source-code">df[df['State'].str.contains('il', na=False)]</p><p>You should get the following output:</p><div id="_idContainer553" class="IMG---Figure"><img src="Images/B15019_11_26.jpg" alt="Figure 11.26: Observations with a value of il&#13;&#10;" width="816" height="296"/></div><p class="figure-caption">Figure 11.26: Observations with a value of il</p><p>As you can see, all the cities with the <strong class="source-inline">il</strong> value are from the state of Illinois. So, the correct <strong class="source-inline">State</strong> value should be <strong class="source-inline">IL</strong>. You may be thinking that the following values are also referring to Illinois: <strong class="source-inline">Il</strong>, <strong class="source-inline">iL</strong>, and <strong class="source-inline">Il</strong>. We'll have a look at them next.</p></li>
				<li>Now, create a <strong class="source-inline">for</strong> loop that will iterate through the following values in the <strong class="source-inline">State</strong> column: <strong class="source-inline">Il</strong>, <strong class="source-inline">iL</strong>, <strong class="source-inline">Il</strong>. Then, print out the values of the City and State variables using the <strong class="source-inline">pandas</strong> method for subsetting, that is, <strong class="source-inline">.loc()</strong>: DataFrame.loc[row_condition, column condition]. Do this for each observation:<p class="source-code">for state in ['Il', 'iL', 'Il']:</p><p class="source-code">    print(df.loc[df['State'] == state, ['City', 'State']])</p><p>You should get the following output:</p><div id="_idContainer554" class="IMG---Figure"><img src="Images/B15019_11_27.jpg" alt="Figure 11.27: Observations with the il value&#13;&#10;" width="734" height="708"/></div><p class="figure-caption">Figure 11.27: Observations with the il value</p><p class="callout-heading">Note</p><p class="callout">The preceding output has been truncated.</p><p>As you can see, all these cities belong to the state of Illinois. Let's replace them with the correct values.</p></li>
				<li>Create a condition mask (<strong class="source-inline">il_mask</strong>) to subset all the rows that contain the four incorrect values (<strong class="source-inline">il</strong>, <strong class="source-inline">Il</strong>, <strong class="source-inline">iL</strong>, and <strong class="source-inline">Il</strong>) by using the <strong class="source-inline">isin()</strong> method and a list of these values as a parameter. Then, save the result into a variable called <strong class="source-inline">il_mask</strong>:<p class="source-code">il_mask = df['State'].isin(['il', 'Il', 'iL', 'Il'])</p></li>
				<li>Print the number of rows that match the condition we set in <strong class="source-inline">il_mask</strong> using the <strong class="source-inline">.sum()</strong> method. This will sum all the rows that have a value of <strong class="source-inline">True</strong> (they match the condition):<p class="source-code">il_mask.sum()</p><p>You should get the following output:</p><p class="source-code">672</p></li>
				<li>Using the <strong class="source-inline">pandas</strong> <strong class="source-inline">.loc()</strong> method, subset the rows with the <strong class="source-inline">il_mask</strong> condition mask and replace the value of the <strong class="source-inline">State</strong> column with <strong class="source-inline">IL</strong>:<p class="source-code">df.loc[il_mask, 'State'] = 'IL'</p></li>
				<li>Print out all the unique values of the <strong class="source-inline">State</strong> variable once more:<p class="source-code">df['State'].unique()</p><p>You should get the following output:</p><div id="_idContainer555" class="IMG---Figure"><img src="Images/B15019_11_28.jpg" alt="Figure 11.28: List of unique values for the 'State' column&#13;&#10;" width="1219" height="177"/></div><p class="figure-caption">Figure 11.28: List of unique values for the 'State' column</p><p>As you can see, the four incorrect values are not present anymore. Let's have a look at the other remaining incorrect values: <strong class="source-inline">II</strong>, <strong class="source-inline">I</strong>, <strong class="source-inline">8I</strong>, and <strong class="source-inline">60</strong>. We will look at dealing <strong class="source-inline">II</strong> in the next step.</p><p>Print out the rows that have a value of <strong class="source-inline">II</strong> into the <strong class="source-inline">State</strong> column using the <strong class="source-inline">pandas</strong> subsetting API, that is, DataFrame.loc[row_condition, column_condition]:</p><p class="source-code">df.loc[df['State'] == 'II',]</p><p>You should get the following output:</p><div id="_idContainer556" class="IMG---Figure"><img src="Images/B15019_11_29.jpg" alt="Figure 11.29: Subsetting the rows with a value of IL in the State column&#13;&#10;" width="785" height="144"/></div><p class="figure-caption">Figure 11.29: Subsetting the rows with a value of IL in the State column</p><p>There are only two cases where the <strong class="source-inline">II</strong> value has been used for the <strong class="source-inline">State</strong> column and both have <strong class="source-inline">Bloomington</strong> as the city, which is in Illinois. Here, the correct <strong class="source-inline">State</strong> value should be <strong class="source-inline">IL</strong>.</p></li>
				<li>Now, create a <strong class="source-inline">for</strong> loop that iterates through the three incorrect values (<strong class="source-inline">I</strong>, <strong class="source-inline">8I</strong>, and <strong class="source-inline">60</strong>) and print out the subsetted rows using the same logic that we used in <em class="italic">Step 12</em>. Only display the <strong class="source-inline">City</strong> and <strong class="source-inline">State</strong> columns:<p class="source-code">for val in ['I', '8I', '60']:</p><p class="source-code">    print(df.loc[df['State'] == val, ['City', 'State']])</p><p>You should get the following output:</p><div id="_idContainer557" class="IMG---Figure"><img src="Images/B15019_11_30.jpg" alt="Figure 11.30: Observations with incorrect values (I, 8I, and 60)&#13;&#10;" width="1135" height="211"/></div><p class="figure-caption">Figure 11.30: Observations with incorrect values (I, 8I, and 60)</p><p>All the observations that have incorrect values are cities in Illinois. Let's fix them now.</p></li>
				<li>Create a <strong class="source-inline">for</strong> loop that iterates through the four incorrect values (<strong class="source-inline">II</strong>, <strong class="source-inline">I</strong>, <strong class="source-inline">8I</strong>, and <strong class="source-inline">60</strong>) and reuse the subsetting logic from <em class="italic">Step 12</em> to replace the value in <strong class="source-inline">State</strong> with <strong class="source-inline">IL</strong>:<p class="source-code">for val in ['II', 'I', '8I', '60']:</p><p class="source-code">    df.loc[df['State'] == val, 'State'] = 'IL'</p></li>
				<li>Print out all the unique values of the <strong class="source-inline">State</strong> variable:<p class="source-code">df['State'].unique()</p><p>You should get the following output:</p><div id="_idContainer558" class="IMG---Figure"><img src="Images/B15019_11_31.jpg" alt="Figure 11.31: List of unique values for the State column&#13;&#10;"/></div><p class="figure-caption">Figure 11.31: List of unique values for the State column</p><p>You fixed the issues for the state of Illinois. However, there are two more incorrect values in this column: <strong class="source-inline">In</strong> and <strong class="source-inline">ng</strong>.</p></li>
				<li>Repeat <em class="italic">Step 13</em>, but iterate through the <strong class="source-inline">In</strong> and <strong class="source-inline">ng</strong> values instead:<p class="source-code">for val in ['In', 'ng']:</p><p class="source-code">    print(df.loc[df['State'] == val, ['City', 'State']])</p><p>You should get the following output:</p><div id="_idContainer559" class="IMG---Figure"><img src="Images/B15019_11_32.jpg" alt="Figure 11.32: Observations with incorrect values (In, ng)&#13;&#10;" width="960" height="177"/></div><p class="figure-caption">Figure 11.32: Observations with incorrect values (In, ng)</p><p>The rows that have the <strong class="source-inline">ng</strong> value in <strong class="source-inline">State</strong> are missing values. We will cover this topic in the next section. The observation that has <strong class="source-inline">In</strong> as its <strong class="source-inline">State</strong> is a city in Indiana, so the correct value should be <strong class="source-inline">IN</strong>. Let's fix this.</p></li>
				<li>Subset the rows containing the <strong class="source-inline">In</strong> value in <strong class="source-inline">State</strong> using the <strong class="source-inline">.loc()</strong> and <strong class="source-inline">.str.contains()</strong> methods and replace the state value with <strong class="source-inline">IN</strong>. Don't forget to specify the <strong class="source-inline">na=False</strong> parameter as <strong class="source-inline">.str.contains()</strong>:<p class="source-code">df.loc[df['State']\</p><p class="source-code">  .str.contains('In', na=False), 'State'] = 'IN'</p><p>Print out all the unique values of the <strong class="source-inline">State</strong> variable:</p><p class="source-code">df['State'].unique()</p><p>You should get the following output:</p><div id="_idContainer560" class="IMG---Figure"><img src="Images/B15019_11_31.jpg" alt="Figure 11.33: List of unique values for the State column&#13;&#10;" width="1269" height="143"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.33: List of unique values for the State column</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/319pfGX">https://packt.live/319pfGX</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2E8ICHn">https://packt.live/2E8ICHn</a>.</p>
			<p>You just fixed all the incorrect values for the <strong class="source-inline">State</strong> variable using the methods provided by the <strong class="source-inline">pandas</strong> package. In the next section, we are going to look at handling missing values.</p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor266"/>Handling Missing Values</h1>
			<p>So far, you have looked at a variety of issues when it comes to datasets. Now it is time to discuss another issue that occurs quite frequently: missing values. As you may have guessed, this type of issue means that certain values are missing for certain variables. </p>
			<p>The <strong class="source-inline">pandas</strong> package provides a method that we can use to identify missing values in a DataFrame: <strong class="source-inline">.isna()</strong>. Let's see it in action on the <strong class="source-inline">Online Retail</strong> dataset. First, you need to import <strong class="source-inline">pandas</strong> and load the data into a DataFrame:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">file_url = 'https://github.com/PacktWorkshops/'\</p>
			<p class="source-code">           'The-Data-Science-Workshop/blob/'\</p>
			<p class="source-code">           'master/Chapter10/dataset/'\</p>
			<p class="source-code">           'Online%20Retail.xlsx?raw=true'</p>
			<p class="source-code">df = pd.read_excel(file_url)</p>
			<p>The <strong class="source-inline">.isna()</strong> method returns a <strong class="source-inline">pandas</strong> series with a binary value for each cell of a DataFrame and states whether it is missing a value (<strong class="source-inline">True</strong>) or not (<strong class="source-inline">False</strong>):</p>
			<p class="source-code">df.isna()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer561" class="IMG---Figure">
					<img src="Images/B15019_11_34.jpg" alt="Figure 11.34: Output of the .isna() method&#13;&#10;" width="1043" height="350"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.34: Output of the .isna() method</p>
			<p>As we saw previously, we can give the output of a binary variable to the <strong class="source-inline">.sum()</strong> method, which will add all the <strong class="source-inline">True</strong> values together (cells that have missing values) and provide a summary for each column:</p>
			<p class="source-code">df.isna().sum()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer562" class="IMG---Figure">
					<img src="Images/B15019_11_35.jpg" alt="Figure 11.35: Summary of missing values for each variable&#13;&#10;" width="1033" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.35: Summary of missing values for each variable</p>
			<p>As you can see, there are <strong class="source-inline">1454</strong> missing values in the <strong class="source-inline">Description</strong> column and <strong class="source-inline">135080</strong> in the <strong class="source-inline">CustomerID</strong> column. Let's have a look at the missing value observations for <strong class="source-inline">Description</strong>. You can use the output of the <strong class="source-inline">.isna()</strong> method to subset the rows with missing values:</p>
			<p class="source-code">df[df['Description'].isna()]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer563" class="IMG---Figure">
					<img src="Images/B15019_11_36.jpg" alt="Figure 11.36: Subsetting the rows with missing values for Description&#13;&#10;" width="982" height="242"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.36: Subsetting the rows with missing values for Description</p>
			<p>From the preceding output, you can see that all the rows with missing values have <strong class="source-inline">0.0</strong> as the unit price and are missing the <strong class="source-inline">CustomerID</strong> column. In a real project, you will have to discuss these cases with the business and check whether these transactions are genuine or not. If the business confirms that these observations are irrelevant, then you will need to remove them from the dataset.</p>
			<p>The <strong class="source-inline">pandas</strong> package provides a method that we can use to easily remove missing values: <strong class="source-inline">.dropna()</strong>. This method returns a new DataFrame without all the rows that have missing values. By default, it will look at all the columns. You can specify a list of columns for it to look for with the <strong class="source-inline">subset</strong> parameter:</p>
			<p class="source-code">df.dropna(subset=['Description'])</p>
			<p>This method returns a new DataFrame with no missing values for the specified columns. If you want to replace the original dataset directly, you can use the <strong class="source-inline">inplace=True</strong> parameter:</p>
			<p class="source-code">df.dropna(subset=['Description'], inplace=True)</p>
			<p>Now, look at the summary of the missing values for each variable:</p>
			<p class="source-code">df.isna().sum()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer564" class="IMG---Figure">
					<img src="Images/B15019_11_37.jpg" alt="Figure 11.37: Summary of missing values for each variable&#13;&#10;" width="1041" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.37: Summary of missing values for each variable</p>
			<p>As you can see, there are no more missing values in the <strong class="source-inline">Description</strong> column. Let's have a look at the <strong class="source-inline">CustomerID</strong> column:</p>
			<p class="source-code">df[df['CustomerID'].isna()]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer565" class="IMG---Figure">
					<img src="Images/B15019_11_38.jpg" alt="Figure 11.38: Rows with missing values in CustomerID&#13;&#10;" width="1447" height="531"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.38: Rows with missing values in CustomerID</p>
			<p>This time, all the transactions look normal, except they are missing values for the <strong class="source-inline">CustomerID</strong> column; all the other variables have been filled in with values that seem genuine. There is no other way to infer the missing values for the <strong class="source-inline">CustomerID</strong> column. These rows represent almost 25% of the dataset, so we can't remove them. </p>
			<p>However, most algorithms require a value for each observation, so you need to provide one for these cases. We will use the <strong class="source-inline">.fillna()</strong> method from <strong class="source-inline">pandas</strong> to do this. Provide the value to be imputed as <strong class="source-inline">Missing</strong> and use <strong class="source-inline">inplace=True</strong> as a parameter:</p>
			<p class="source-code">df['CustomerID'].fillna('Missing', inplace=True)</p>
			<p class="source-code">df[1443:1448]</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer566" class="IMG---Figure">
					<img src="Images/B15019_11_39.jpg" alt="Figure 11.39: Examples of rows where missing values for CustomerID &#13;&#10;have been replaced with Missing&#13;&#10;" width="1665" height="499"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.39: Examples of rows where missing values for CustomerID have been replaced with Missing</p>
			<p>Let's see if we have any missing values in the dataset:</p>
			<p class="source-code">df.isna().sum()</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer567" class="IMG---Figure">
					<img src="Images/B15019_11_40.jpg" alt="Figure 11.40: Summary of missing values for each variable&#13;&#10;" width="1050" height="313"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.40: Summary of missing values for each variable</p>
			<p>You have successfully fixed all the missing values in this dataset. These methods also work when we want to handle missing numerical variables. We will look at this in the following exercise. All you need to do is provide a numerical value when you want to impute a value with <strong class="source-inline">.fillna()</strong>.</p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor267"/>Exercise 11.04: Fixing Missing Values for the Horse Colic Dataset</h2>
			<p>In this exercise, you will be cleaning out all the missing values for all the numerical variables in the <strong class="source-inline">Horse Colic</strong> dataset.</p>
			<p>Colic is a painful condition that horses can suffer from, and this dataset contains various pieces of information related to specific cases of this condition. You can use the link provided in the Note section if you want to find out more about the dataset's attributes. Let's get started:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is from the UCI Machine Learning Repository. The attributes information can be found at <a href="https://packt.live/2MZwSrW">https://packt.live/2MZwSrW</a>.</p>
			<p class="callout">For your convenience, the dataset file that we'll be using in this exercise has been uploaded to this book's GitHub repository: <a href="https://packt.live/35qESZq">https://packt.live/35qESZq</a>.</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package:<p class="source-code">import pandas as pd</p></li>
				<li>Assign the link to the dataset to a variable called <strong class="source-inline">file_url</strong>:<p class="source-code">file_url = 'http://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Data-Science-Workshop/'\</p><p class="source-code">           'master/Chapter11/dataset/horse-colic.data'</p></li>
				<li>Using the <strong class="source-inline">.read_csv()</strong> method from the <strong class="source-inline">pandas</strong> package, load the dataset into a new variable called <strong class="source-inline">df</strong> and specify the <strong class="source-inline">header=None</strong>,<strong class="source-inline"> sep='\s+'</strong>, and<strong class="source-inline"> prefix='X'</strong> parameters:<p class="source-code">df = pd.read_csv(file_url, header=None, \</p><p class="source-code">                 sep='\s+', prefix='X')</p></li>
				<li>Print the first five rows of the DataFrame using the <strong class="source-inline">.head()</strong> method:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer568" class="IMG---Figure"><img src="Images/B15019_11_41.jpg" alt="Figure 11.41: The first five rows of the Horse Colic dataset&#13;&#10;" width="1193" height="364"/></div><p class="figure-caption">Figure 11.41: The first five rows of the Horse Colic dataset</p><p>As you can see, the authors have used the <strong class="source-inline">?</strong> character for missing values, but the <strong class="source-inline">pandas</strong> package thinks that this is a normal value. You need to transform them into missing values.</p></li>
				<li>Reload the dataset into a <strong class="source-inline">pandas</strong> DataFrame using the <strong class="source-inline">.read_csv()</strong> method, but this time, add the <strong class="source-inline">na_values='?'</strong> parameter in order to specify that this value needs to be treated as a missing value:<p class="source-code">df = pd.read_csv(file_url, header=None, sep='\s+', \</p><p class="source-code">                 prefix='X', na_values='?')</p></li>
				<li>Print the first five rows of the DataFrame using the <strong class="source-inline">.head()</strong> method:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer569" class="IMG---Figure"><img src="Images/B15019_11_42.jpg" alt="Figure 11.42: The first five rows of the Horse Colic dataset&#13;&#10;" width="1005" height="270"/></div><p class="figure-caption">Figure 11.42: The first five rows of the Horse Colic dataset</p><p>Now, you can see that <strong class="source-inline">pandas</strong> have converted all the <strong class="source-inline">?</strong> values into missing values.</p></li>
				<li>Print the data type of each column using the <strong class="source-inline">dtypes</strong> attribute:<p class="source-code">df.dtypes</p><p>You should get the following output:</p><div id="_idContainer570" class="IMG---Figure"><img src="Images/B15019_11_43.jpg" alt="Figure 11.43: Data type of each column&#13;&#10;" width="955" height="476"/></div><p class="figure-caption">Figure 11.43: Data type of each column</p></li>
				<li>Print the number of missing values for each column by combining the <strong class="source-inline">.isna()</strong> and <strong class="source-inline">.sum()</strong> methods:<p class="source-code">df.isna().sum()</p><p>You should get the following output:</p><div id="_idContainer571" class="IMG---Figure"><img src="Images/B15019_11_44.jpg" alt="Figure 11.44: Number of missing values for each column&#13;&#10;" width="910" height="375"/></div><p class="figure-caption">Figure 11.44: Number of missing values for each column</p></li>
				<li>Create a condition mask called <strong class="source-inline">x0_mask</strong> so that you can find the missing values in the <strong class="source-inline">X0</strong> column using the <strong class="source-inline">.isna()</strong> method:<p class="source-code">x0_mask = df['X0'].isna()</p></li>
				<li>Display the number of missing values for this column by using the <strong class="source-inline">.sum()</strong> method on <strong class="source-inline">x0_mask</strong>:<p class="source-code">x0_mask.sum()</p><p>You should get the following output:</p><p class="source-code">1</p><p>Here, you got the exact same number of missing values for <strong class="source-inline">X0</strong> that you did in <em class="italic">Step 9</em>.</p></li>
				<li>Extract the mean of <strong class="source-inline">X0</strong> using the <strong class="source-inline">.median()</strong> method and store it in a new variable called <strong class="source-inline">x0_median</strong>. Print its value:<p class="source-code">x0_median = df['X0'].median()</p><p class="source-code">print(x0_median)</p><p>You should get the following output:</p><p class="source-code">1.0</p><p>The median value for this column is <strong class="source-inline">1</strong>. You will replace all the missing values with this value in the <strong class="source-inline">X0</strong> column.</p></li>
				<li>Replace all the missing values in the <strong class="source-inline">X0</strong> variable with their median using the <strong class="source-inline">.fillna()</strong> method, along with the <strong class="source-inline">inplace=True</strong> parameter:<p class="source-code">df['X0'].fillna(x0_median, inplace=True)</p></li>
				<li>Print the number of missing values for <strong class="source-inline">X0</strong> by combining the <strong class="source-inline">.isna()</strong> and <strong class="source-inline">.sum()</strong> methods:<p class="source-code">df['X0'].isna().sum()</p><p>You should get the following output:</p><p class="source-code">0</p><p>There are no more missing values in the variables.</p></li>
				<li>Create a <strong class="source-inline">for</strong> loop that will iterate through all the columns of the DataFrame. In the for loop, calculate the median for each and save them into a variable called <strong class="source-inline">col_median</strong>. Then, impute missing values with this median value using the <strong class="source-inline">.fillna()</strong> method, along with the <strong class="source-inline">inplace=True</strong> parameter, and print the name of the column and its median value:<p class="source-code">for col_name in df.columns:</p><p class="source-code">    col_median = df[col_name].median()</p><p class="source-code">    df[col_name].fillna(col_median, inplace=True)</p><p class="source-code">    print(col_name)</p><p class="source-code">    print(col_median)</p><p>You should get the following output:</p><div id="_idContainer572" class="IMG---Figure"><img src="Images/B15019_11_45.jpg" alt="Figure 11.45: Median values for each column&#13;&#10;" width="1665" height="678"/></div><p class="figure-caption">Figure 11.45: Median values for each column</p></li>
				<li>Print the number of missing values for each column by combining the <strong class="source-inline">.isna()</strong> and <strong class="source-inline">.sum()</strong> methods:<p class="source-code">df.isna().sum()</p><p>You should get the following output:</p><div id="_idContainer573" class="IMG---Figure"><img src="Images/B15019_11_46.jpg" alt="Figure 11.46: Number of missing values for each column&#13;&#10;" width="1665" height="633"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.46: Number of missing values for each column</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/34c1zUd">https://packt.live/34c1zUd</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/324mHt0">https://packt.live/324mHt0</a>.</p>
			<p>You have successfully fixed the missing values for all the numerical variables using the methods provided by the <strong class="source-inline">pandas</strong> package: <strong class="source-inline">.isna()</strong> and <strong class="source-inline">.fillna()</strong>.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor268"/>Activity 11.01: Preparing the Speed Dating Dataset</h2>
			<p>As an entrepreneur, you are planning to launch a new dating app into the market. The key feature that will differentiate your app from other competitors will be your high performing user-matching algorithm. Before building this model, you have partnered with a speed dating company to collect data from real events. You just received the dataset from your partner company but realized it is not as clean as you expected; there are missing and incorrect values. Your task is to fix the main data quality issues in this dataset.</p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Download and load the dataset into Python using <strong class="source-inline">.read_csv()</strong>.</li>
				<li>Print out the dimensions of the DataFrame using <strong class="source-inline">.shape</strong>.</li>
				<li>Check for duplicate rows by using <strong class="source-inline">.duplicated()</strong> and <strong class="source-inline">.sum()</strong> on all the columns.</li>
				<li>Check for duplicate rows by using <strong class="source-inline">.duplicated() </strong>and <strong class="source-inline">.sum()</strong> for the identifier columns (<strong class="source-inline">iid</strong>, <strong class="source-inline">id</strong>, <strong class="source-inline">partner</strong>, and <strong class="source-inline">pid</strong>).</li>
				<li>Check for unexpected values for the following numerical variables: <strong class="source-inline">'imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping',</strong> and <strong class="source-inline">'yoga'</strong>.</li>
				<li>Replace the identified incorrect values.</li>
				<li>Check the data type of the different columns using <strong class="source-inline">.dtypes</strong>.</li>
				<li>Change the data types to categorical for the columns that don't contain numerical values using <strong class="source-inline">.astype()</strong>.</li>
				<li>Check for any missing values using <strong class="source-inline">.isna()</strong> and <strong class="source-inline">.sum()</strong> for each numerical variable.</li>
				<li>Replace the missing values for each numerical variable with their corresponding mean or median values using <strong class="source-inline">.fillna()</strong>, <strong class="source-inline">.mean()</strong>, and <strong class="source-inline">.median()</strong>.<p class="callout-heading">Note</p><p class="callout">The dataset for this activity can be found in this book's GitHub repository: <a href="https://packt.live/36u0jtR">https://packt.live/36u0jtR</a>.</p><p class="callout">The original dataset has been shared by Ray Fisman and Sheena Iyengar from Columbia Business School: <a href="https://packt.live/2Fp5rUg">https://packt.live/2Fp5rUg</a>.</p><p class="callout">The authors have provided a very useful document that describes the dataset and its features: <a href="https://packt.live/2Qrp7gD">https://packt.live/2Qrp7gD</a>.</p></li>
			</ol>
			<p>You should get the following output. The figure represents the number of rows with unexpected values for <strong class="source-inline">imprace</strong> and a list of unexpected values:</p>
			<div>
				<div id="_idContainer574" class="IMG---Figure">
					<img src="Images/B15019_11_47.jpg" alt="Figure 11.47: Number of rows with unexpected values for 'imprace' &#13;&#10;and a list of unexpected values&#13;&#10;" width="1665" height="181"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.47: Number of rows with unexpected values for 'imprace' and a list of unexpected values</p>
			<p>The following figure illustrates the number of rows with unexpected values and a list of unexpected values for each column:</p>
			<div>
				<div id="_idContainer575" class="IMG---Figure">
					<img src="Images/B15019_11_48.jpg" alt="Figure 11.48: Number of rows with unexpected values and &#13;&#10;a list of unexpected values for each column&#13;&#10;" width="1517" height="494"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.48: Number of rows with unexpected values and a list of unexpected values for each column</p>
			<p>The following figure illustrates a list of unique values for gaming:</p>
			<div>
				<div id="_idContainer576" class="IMG---Figure">
					<img src="Images/B15019_11_49.jpg" alt="Figure 11.49: List of unique values for gaming&#13;&#10;" width="881" height="75"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.49: List of unique values for gaming</p>
			<p>The following figure displays the data types of each column:</p>
			<div>
				<div id="_idContainer577" class="IMG---Figure">
					<img src="Images/B15019_11_50.jpg" alt="Figure 11.50: Data types of each column&#13;&#10;" width="865" height="240"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.50: Data types of each column</p>
			<p>The following figure displays the updated data types of each column:</p>
			<div>
				<div id="_idContainer578" class="IMG---Figure">
					<img src="Images/B15019_11_51.jpg" alt="Figure 11.51: Data types of each column&#13;&#10;" width="942" height="279"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.51: Data types of each column</p>
			<p>The following figure displays the number of missing values for numerical variables:</p>
			<div>
				<div id="_idContainer579" class="IMG---Figure">
					<img src="Images/B15019_11_52.jpg" alt="Figure 11.52: Number of missing values for numerical variables&#13;&#10;" width="924" height="278"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.52: Number of missing values for numerical variables</p>
			<p>The following figure displays the list of unique values for <strong class="source-inline">int_corr</strong>:</p>
			<div>
				<div id="_idContainer580" class="IMG---Figure">
					<img src="Images/B15019_11_53.jpg" alt="Figure 11.53: List of unique values for 'int_corr'&#13;&#10;" width="876" height="259"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.53: List of unique values for 'int_corr'</p>
			<p>The following figure displays the list of unique values for numerical variables:</p>
			<div>
				<div id="_idContainer581" class="IMG---Figure">
					<img src="Images/B15019_11_54.jpg" alt="Figure 11.54: List of unique values for numerical variables&#13;&#10;" width="916" height="278"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.54: List of unique values for numerical variables</p>
			<p>The following figure displays the number of missing values for numerical variables:</p>
			<div>
				<div id="_idContainer582" class="IMG---Figure">
					<img src="Images/B15019_11_55.jpg" alt="Figure 11.55: Number of missing values for numerical variables&#13;&#10;" width="1330" height="375"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.55: Number of missing values for numerical variables</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found at the following address: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<h1 id="_idParaDest-270"><a id="_idTextAnchor269"/>Summary</h1>
			<p>In this chapter, you learned how important it is to prepare any given dataset and fix the main quality issues it has. This is critical because the cleaner a dataset is, the easier it will be for any machine learning model to easily learn about the relevant patterns. On top of this, most algorithms can't handle issues such as missing values, so they must be handled prior to the modeling phase. In this chapter, you covered the most frequent issues that are faced in data science projects: duplicate rows, incorrect data types, unexpected values, and missing values. </p>
			<p>The goal of this chapter was to introduce you to the concepts that will help you to spot some of these issues and easily fix them so that you have the basic toolkit to be able to handle other cases. As a final note, throughout this chapter, we emphasized how important it is to discuss the issues you find with the business or the data engineering team you are working with. For instance, if you've detected unexpected values in a dataset, you may want to confirm that they don't have any special meaning from a business point of view before removing or replacing them.</p>
			<p>You also need to be very careful when fixing issues: you don't want to alter the dataset too much so that it creates additional unexpected patterns. This is exactly why it is recommended that you replace any of the missing values of numerical variables with their <strong class="bold">mean</strong> or <strong class="bold">median</strong>. Otherwise, you will change its distribution drastically. For example, if the values of a variable are between 0 and 10, replacing all the missing values with -999 will drastically change their mean and <strong class="bold">standard</strong> <strong class="bold">deviation</strong>. </p>
			<p>In the next chapter, we will discuss the interesting topic of feature engineering.</p>
		</div>
		<div>
			<div id="_idContainer584" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer585" class="Content">
			</div>
		</div>
	</div></body></html>