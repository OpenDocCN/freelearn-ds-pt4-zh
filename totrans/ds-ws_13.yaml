- en: 13\. Imbalanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to identify use cases where datasets
    are likely to be imbalanced; formulate strategies for dealing with imbalanced
    datasets; build classification models, such as logistic regression models, after
    balancing datasets; and analyze classification metrics to validate whether adopted
    strategies are yielding the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be dealing with imbalanced datasets, which are very
    prevalent in real-life scenarios. You will be using techniques such as `SMOTE`,
    `MSMOTE`, and random undersampling to address imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, *Chapter 12*, *Feature Engineering*, where we dealt
    with data points related to dates, we were addressing scenarios pertaining to
    features. In this chapter, we will deal with scenarios where the proportions of
    examples in the overall dataset pose challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Let's revisit the dataset we dealt with in *Chapter 3*, *Binary Classification*,
    in which the examples pertaining to 'No' for term deposits far outnumbered the
    ones with 'Yes' with a ratio of 88% to 12%. We also determined that one reason
    for suboptimal results with a logistic regression model on that dataset was the
    skewed proportion of examples. Datasets like the one we analyzed in *Chapter 3*,
    *Binary Classification,* which are called imbalanced datasets, are very common
    in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the use cases where we encounter imbalanced datasets include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection for credit cards or insurance claims
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical diagnoses where we must detect the presence of rare diseases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intrusion detection in networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all of these use cases, we can see that what we really want to detect will
    be minority cases. For instance, in domains such as the medical diagnosis of rare
    diseases, examples where rare diseases exist could even be less than 1% of the
    total examples. One inherent characteristic of use cases with imbalanced datasets
    is that the quality of the classifier is not apparent if the right metric is not
    used. This makes the problem of imbalanced datasets really challenging.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss strategies for identifying imbalanced datasets
    and ways to mitigate the effects of imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Business Context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The business head of the bank for which you are working as a data scientist
    recently raised the alarm about the results of the term deposit propensity model
    that you built in *Chapter 3*, *Binary Classification*. It has been observed that
    a large proportion of customers who were identified as potential cases for targeted
    marketing for term deposits have turned down the offer. This has made a big dent
    in the sales team's metrics on upselling and cross-selling. The business team
    urgently requires your help in fixing the issue to meet the required sales targets
    for the quarter. Don't worry, though â€“ this is the problem that we will be solving
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: First, we begin with an analysis of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13.01: Benchmarking the Logistic Regression Model on the Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be analyzing the problem of predicting whether a
    customer will buy a term deposit. For this, you will be fitting a logistic regression
    model, as you did in *Chapter 3*, *Binary Classification*, and you will look closely
    at the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset you will be using in this exercise can be found on our GitHub repository:
    [https://packt.live/2twFgIM](https://packt.live/2twFgIM).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new notebook in Google Colab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, import `pandas` and load the data from the GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, load the data using `pandas`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1: The first 5 rows of bankData'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.1: The first 5 rows of bankData'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, to break the dataset down further, let's perform some feature-engineering
    steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalize the numerical features (age, balance, and duration) through scaling,
    which was covered in *Chapter 3*, *Binary Classification*. Enter the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we used a scaling function called `RobustScaler()`
    to scale the numerical data. `RobustScaler()` is a scaling function similar to
    `MinMaxScaler` in *Chapter 3*, *Binary Classification*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After scaling the numerical data, we convert each of the columns to a scaled
    version, as in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, drop the original features after we introduce the scaled features using
    the `.drop()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the first five columns using the `.head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2: bankData with scaled features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.2: bankData with scaled features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The categorical features in the dataset must be converted into numerical values
    by transforming them into dummy values, which was covered in *Chapter 3*, *Binary
    Classification*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert all the categorical variables to dummy variables using the `.get_dummies()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate the numerical data and observe the shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the categorical values are transformed, they must be combined with the
    scaled numerical values of the data frame to get the feature-engineered dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the independent variables, `X`, and dependent variables, `Y`, from the
    combined dataset for modeling, as in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.3: The independent variables and the combined data (truncated)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.3: The independent variables and the combined data (truncated)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are now ready for the modeling task. Let's first import the necessary packages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, `import` the necessary functions of `train_test_split()` and `LogisticRegression`
    from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into train and test sets at a `test_size = 0.3` variable in
    the splitting function. We also set `random_state` for the reproducibility of
    the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, fit the model using `.fit` on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.4: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.4: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the model is fit, let's now predict the test set and generate the metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, find the prediction on the test set and print the accuracy scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use both the `confusion_matrix()` and `classification_report()` functions
    to generate the metrics for further analysis, which we will cover in the *Analysis
    of the Result* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.5: Metrics showing the accuracy result along with the confusion
    matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.5: Metrics showing the accuracy result along with the confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will get metrics similar to the following. However, the values will vary
    due to the variability in the modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we have found a report that may have caused the issue with
    the number of customers expected to purchase the term deposit plan. From the metrics,
    we can see that the number of values for `No` is relatively higher than that for
    `Yes`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hapnvB](https://packt.live/3hapnvB).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hh6Xta](https://packt.live/3hh6Xta).
  prefs: []
  type: TYPE_NORMAL
- en: To understand more about the reasons behind the skewed results, we will analyze
    these metrics in detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the Result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To analyze the results obtained in the previous section, let''s expand the
    confusion matrix in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6: Confusion matrix of the resulting metrics obtained'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.6: Confusion matrix of the resulting metrics obtained'
  prefs: []
  type: TYPE_NORMAL
- en: 'We enter the values `11707`, `291`, `1060`, and `506` from the output we got
    from the previous exercise. We then place these values as shown in the diagram.
    We will represent the propensity to take a term deposit (`No`) as the positive
    class and the other as the negative class. So, from the confusion matrix, we can
    calculate the accuracy measures, which were covered in *Chapter 3*, *Binary Classification*.
    The accuracy of the model is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7: Accuracy of a model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.7: Accuracy of a model'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, it will be (11707 + 506) / (11707 + 1060 + 291 + 506), or 90%.
  prefs: []
  type: TYPE_NORMAL
- en: From the accuracy perspective, the model would seem like it is doing a reasonable
    job. However, the reality might be quite different. To find out what's really
    the case, let's look at the precision and recall values, which are available from
    the classification report we obtained. The formulae for precision for any class
    was covered in *Chapter 3*, *Binary Classification*
  prefs: []
  type: TYPE_NORMAL
- en: 'The precision value of any class is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8: Precision of a model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.8: Precision of a model'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, for the positive class, the precision is *TP/(TP + FP)*, which
    is 11707/ (11707 + 1060), which comes to approximately 92%.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the negative class, the precision could be written as *TN / (TN
    + FN)*, which is 506 / (506 + 291), which comes to approximately 63%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the recall value for any class can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9: Recalling a model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.9: Recalling a model'
  prefs: []
  type: TYPE_NORMAL
- en: The recall value for the positive class, *TP / (TP + FN)* = 11707 / (11707 +
    291), comes to approximately 98%.
  prefs: []
  type: TYPE_NORMAL
- en: The recall value for the negative class, *TN / (TN + FP)* = 506 / (506 + 1060),
    comes to approximately 32%.
  prefs: []
  type: TYPE_NORMAL
- en: Recall indicates the ability of the classifier to correctly identify the respective
    classes. From the metrics, we see that the model that we built does a good job
    of identifying the positive classes, but does a very poor job of correctly identifying
    the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: Why do you think that the classifier is biased toward one class? The answer
    to this can be unearthed by looking at the class balance in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will generate the percentages of the classes in the training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: From this, we can see that the majority of the training set (88%) is made up
    of the positive class. This imbalance is one of the major reasons behind the poor
    metrics that we have had with the logistic regression classifier we have selected.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the challenges of imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of Imbalanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen from the classifier example, one of the biggest challenges with imbalanced
    datasets is the bias toward the majority class, which ended up being 88% in the
    previous example. This will result in suboptimal results. However, what makes
    such cases even more challenging is the deceptive nature of results if the right
    metric is not used.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take, for example, a dataset where the negative class is around 99% and
    the positive class is 1% (as in a use case where a rare disease has to be detected,
    for instance).
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we had a poor classifier that was capable of only predicting the negative
    class; we would get the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10: Confusion matrix of the poor classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.10: Confusion matrix of the poor classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the confusion matrix, let''s calculate the accuracy measures. Have a look
    at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With such a classifier, if we were to use a metric such as accuracy, we still
    would get a result of around 99%, which, in normal circumstances, would look outstanding.
    However, in this case, the classifier is doing a bad job. Think of the real-life
    impact of using such a classifier and a metric such as accuracy. The impact on
    patients who have rare diseases and who get wrongly classified as not having the
    disease could be fatal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is important to identify cases with imbalanced datasets and equally
    important to pick the right metric for analyzing such datasets. The right metric
    in this example would have been to look at the recall values for both the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: From the recall values, we could have identified the bias of the classifier
    toward the majority class, prompting us to look at strategies for mitigating such
    biases, which is the next topic we will focus on.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for Dealing with Imbalanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have identified the challenges of imbalanced datasets, let''s look
    at strategies for combatting imbalanced datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11: Strategies for dealing with imbalanced datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.11: Strategies for dealing with imbalanced datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting More Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having encountered an imbalanced dataset, one of the first questions you need
    to ask is whether it is possible to get more data. This might appear naÃ¯ve, but
    collecting more data, especially from the minority class, and then balancing the
    dataset should be the first strategy for addressing the class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many circumstances, collecting more data, especially from minority classes,
    can be challenging as data points for the minority class will be very minimal.
    In such circumstances, we need to adopt different strategies to work with our
    constraints and still strive to balance our dataset. One effective strategy is
    to resample our dataset to make the dataset more balanced. Resampling would mean
    taking samples from the available dataset to create a new dataset, thereby making
    the new dataset balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the idea in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12: Random undersampling of the majority class'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.12: Random undersampling of the majority class'
  prefs: []
  type: TYPE_NORMAL
- en: As seen in *Figure 13.8*, the idea behind resampling is to randomly pick samples
    from the majority class to make the final dataset more balanced. In the diagram,
    we can see that the minority class has the same number of examples as the original
    dataset and that the majority class is under-sampled to make the final dataset
    more balanced. Resampling examples of this type is called random undersampling
    as we are undersampling the majority class. We will perform random undersampling
    in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13.02: Implementing Random Undersampling and Classification on Our
    Banking Dataset to Find the Optimal Result'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will undersample the majority class (propensity `''No''`)
    and then make the dataset balanced. On the new balanced dataset, you will fit
    a logistic regression model and then analyze the results:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset you will be using in this exercise can be found on our GitHub repository:
    [https://packt.live/2twFgIM](https://packt.live/2twFgIM).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook for this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the initial 12 steps of *Exercise 13.01*, *Benchmarking the Logistic
    Regression Model on the Dataset*, such that the dataset is split into training
    and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, join the `X` and `y` variables for the training set before resampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we concatenated the `X_train` and `y_train` datasets to one single
    dataset. This is done to make the resampling process in the subsequent steps easier.
    To concatenate the two datasets, we use the `.concat()` function from `pandas`.
    In the code, we use `axis = 1` to indicate that the concatenation is done horizontally,
    which is along the columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, display the new data with the `.head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should get the following output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.13: Displaying the first five rows of the dataset using .head()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.13: Displaying the first five rows of the dataset using .head()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding output shows some of the columns of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's move onto separating the minority and majority classes into separate
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What we will do next is separate the minority class and the majority class.
    This is required because we have to sample separately from the majority class
    to make a balanced dataset. To separate the minority class, we have to identify
    the indexes of the dataset where the dataset has 'yes.' The indexes are identified
    using `.index()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once those indexes are identified, they are separated from the main dataset
    using the `.loc()` function and stored in a new variable for the minority class.
    The shape of the minority dataset is also printed. A similar process is followed
    for the majority class and, after these two steps, we have two datasets: one for
    the minority class and one for the majority class.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, find the indexes of the sample dataset where the propensity is `yes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate by the minority class as in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, find the indexes of the majority class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Separate by the majority class as in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.14: Output after separating the majority classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.14: Output after separating the majority classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the majority class is separated, we can proceed with sampling from the
    majority class. Once the sampling is done, the shape of the majority class dataset
    and its head are printed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Take a random sample equal to the length of the minority class to make the dataset
    balanced.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the samples using the `.sample()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The number of examples that are sampled is equal to the number of examples in
    the minority class. This is implemented with the parameters `(n=len(ind))`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that sampling is done, the shape of the majority class dataset and its
    head is printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.15: Output showing the shape of the majority class dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.15: Output showing the shape of the majority class dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we move onto preparing the new training data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After preparing the individual dataset, we can now concatenate them together
    using the `pd.concat()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, we are concatenating in the vertical direction and, therefore,
    `axis = 0` is used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, shuffle the dataset so that both the minority and majority classes are
    evenly distributed using the `shuffle()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.16: Output after shuffling the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.16: Output after shuffling the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, separate the shuffled dataset into the independent variables, `X_trainNew`,
    and dependent variables, `y_trainNew`. The separation is to be done using the
    index features `0` to `51` for the dependent variables using the `.iloc()` function
    in `pandas`. The dependent variables are separated by sub-setting with the column
    name `''y''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.17: Shuffling the dataset into independent variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.17: Shuffling the dataset into independent variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, fit the model on the new data and generate the confusion matrix and classification
    report for our analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, define the `LogisticRegression` function with the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.18: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.18: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, perform the prediction on the test with the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`{:.2f}''.format` is used to print the string values along with the accuracy
    score, which is output from `bankModel1.score(X_test, y_test)`. In this, `2f`
    means a numerical score with two decimals.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, generate the confusion matrix for the model and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.19: Confusion matrix for the model obtained'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.19: Confusion matrix for the model obtained'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The values can vary in the output as the modeling process is subject to variation.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/348njjY](https://packt.live/348njjY).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/318R81I](https://packt.live/318R81I).
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's analyze the results and compare them with those of the benchmark logistic
    regression model that we built at the beginning of this chapter. In the benchmark
    model, we had the problem of the model being biased toward the majority class
    with a very low recall value for the `yes` cases.
  prefs: []
  type: TYPE_NORMAL
- en: Now, by balancing the dataset, we have seen that the recall for the minority
    class has improved tremendously, from a low of `0.32` to around `0.82`. This means
    that by balancing the dataset, the classifier has improved its ability to identify
    negative cases.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can see that our overall accuracy has taken a hit. From a high of
    around 90%, it has come down to around 85%. One major area where accuracy has
    taken a hit is the number of false positives, which are those `No` cases that
    were wrongly predicted as `Yes`.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the result from a business perspective, this is a much better scenario
    than the one we got in the benchmark model. In the benchmark model, out of the
    total 1,566 `Yes` cases, only 506 were correctly identified. However, after balancing,
    we were able to identify 1,277 out of 1,566 customers from the dataset who were
    likely to buy term deposits, which can potentially result in a better conversion
    rate. However, the flip side of this is that the sales team will also have to
    spend a lot of time on customers who are unlikely to buy term deposits. From the
    confusion matrix, we can see that false negatives have gone up to 1,795 from the
    earlier 291 we got in the benchmark model. Ideally, we would want quadrants 2
    and 3 to come down in favor of the other two quadrants.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Synthetic Samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at the undersampling method, where we downsized
    the majority class to make the dataset balanced. However, when undersampling,
    we reduced the size of the dataset. In many circumstances, downsizing the dataset
    can have adverse effects on the predictive power of the classifier. An effective
    way to counter the downsizing of the dataset is to oversample the minority class.
    Oversampling is done by generating new synthetic data points similar to those
    of the minority class, thereby balancing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two very popular methods for generating such synthetic points are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synthetic Minority Oversampling Technique** (**SMOTE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modified SMOTE** (**MSMOTE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The way the `SMOTE` algorithm generates synthetic data is by looking at the
    neighborhood of minority classes and generating new data points within the neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.20: Dataset with two classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.20: Dataset with two classes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain the concept of generating synthetic datasets with a pictorial
    representation. Let''s assume that *Figure 13.15* represents a dataset with two
    classes: the grey circles represent the minority class, and the black circles
    represent the majority class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In creating synthetic points, an imaginary line connecting all the minority
    samples in the neighborhood is created and new data points are generated on this
    line, as shown in *Figure 13.16*, thereby balancing the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.21: Connecting samples in a neighborhood'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.21: Connecting samples in a neighborhood'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, `MSMOTE` is an advancement over the `SMOTE` algorithm and has a different
    approach to generating synthetic points. `MSMOTE` classifies the minority class
    into three distinct groups: **security samples**, **border samples**, and **latent
    noise samples**. Different strategies are adopted to generate neighborhood points
    based on the group each minority class falls into.'
  prefs: []
  type: TYPE_NORMAL
- en: We will see the implementation of both `SMOTE` and `MSMOTE` in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of SMOTE and MSMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`SMOTE` and `MSMOTE` can be implemented from a package called `smote-variants`
    in Python. The library can be installed through `pip install` in the Colab notebook
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: More details on the package and its different variations can be obtained at
    [https://packt.live/2QsNhat](https://packt.live/2QsNhat).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now implement both these methods and analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13.03: Implementing SMOTE on Our Banking Dataset to Find the Optimal
    Result'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will generate synthetic samples of the minority class
    using `SMOTE` and then make the dataset balanced. Then, on the new balanced dataset,
    we will fit a logistic regression model and analyze the results:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement all the steps of *Exercise 13.01*, *Benchmarking the Logistic Regression
    Model on the Dataset*, until the splitting of the train and test sets (*Step 12*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, print the count of both the classes before we oversample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The counts mentioned in this output can vary because of a variability in the
    sampling process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will be oversampling the training set using `SMOTE`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Begin by importing `sv` and `numpy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The library files that are required for oversampling the training set include
    the `smote_variants` library, which we installed earlier. This is imported as
    `sv`. The other library that is required is `numpy`, as the training set will
    have to be given a `numpy` array for the `smote_variants` library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, instantiate the `SMOTE` library to a variable called `oversampler` using
    the `sv.SMOTE()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a common way of instantiating any of the variants of `SMOTE` from the
    `smote_variants` library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, sample the process using the `.sample()` function of `oversampler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both the `X` and `y` variables are converted to `numpy` arrays before applying
    the `.sample()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, print the shapes of the new `X` and `y` variables and the `counts` of
    the classes. You will note that the size of the overall dataset has increased
    from the earlier count of around 31,647 (3694 + 27953) to 55,906\. The increase
    in size can be attributed to the fact that the minority class has been oversampled
    from 3,694 to 27,953:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The counts mentioned in this output can vary because of variability in the sampling
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have generated synthetic points using `SMOTE` and balanced the dataset,
    let's fit a logistic regression model on the new sample and analyze the results
    using a confusion matrix and a classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `LogisticRegression` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, predict using `.predict` on the test set, as mentioned in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, `print` the accuracy values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, generate `ConfusionMatrix` for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The matrix is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate `Classification_report` for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.22: Classification report for the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.22: Classification report for the model'
  prefs: []
  type: TYPE_NORMAL
- en: From the generated metrics, we can see that the results are very similar to
    the undersampling results, with the exception that the recall value of the `'Yes'`
    cases has reduced from `0.82` to around `0.80`. The results that are generated
    vary from one use case to the next. `SMOTE` and its variants have been proven
    to have robust results in balancing data and are therefore the most popular methods
    used when encountering use cases with highly imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The values can vary in the output as the modeling process is subject to variation.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Ycxu34](https://packt.live/2Ycxu34).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2FDvTgo](https://packt.live/2FDvTgo).
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will be implementing `MSMOTE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13.04: Implementing MSMOTE on Our Banking Dataset to Find the Optimal
    Result'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will generate synthetic samples of the minority class using
    `MSMOTE` and then make the dataset balanced. Then, on the new balanced dataset,
    we will fit a logistic regression model and analyze the results. This exercise
    will be very similar to the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: Implement all the steps of *Exercise 13.01*, *Benchmarking the Logistic Regression
    Model on the Dataset*, until the splitting of the train and test sets (*Step 12*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, print the count of both the classes before we oversample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The counts mentioned in this output can vary because of variability in the sampling
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will be oversampling the training set using `MSMOTE`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Begin by importing the `sv` and `numpy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The library files that are required for oversampling the training set include
    the `smote_variants` library, which we installed earlier. This is imported as
    `sv`. The other library that is required is `numpy`, as the training set will
    have to be given a `numpy` array for the `smote_variants` library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, instantiate the `MSMOTE` library to a variable called `oversampler` using
    the `sv.MSMOTE()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, sample the process using the `.sample()` function of `oversampler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have generated synthetic points using `MSMOTE` and balanced the
    dataset, let's fit a logistic regression model on the new sample and analyze the
    results using a confusion matrix and a classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `LogisticRegression` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, predict using `.predict` on the test set as in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, `print` the accuracy values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the `ConfusionMatrix` for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The matrix should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the `Classification_report` for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.23: Classification report for the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_13_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 13.23: Classification report for the model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The values can vary in the output as the modeling process is subject to variation.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/34bCWHd](https://packt.live/34bCWHd).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Edccvh](https://packt.live/2Edccvh).
  prefs: []
  type: TYPE_NORMAL
- en: From the implementation of `MSMOTE`, it is seen that the metrics have degraded
    compared to the `SMOTE` implementation from *Exercise 13.03*, *Implementing SMOTE
    on Our Banking Dataset to Find the Optimal Result*. We can then conclude that
    `MSMOTE` might not be the best method for this use case.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Balancing Techniques on a Telecom Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have seen different balancing techniques, let''s apply these techniques
    to a new dataset that is related to the churn of telecom customers. This dataset
    is available at the following link: [https://packt.live/37IvqSX](https://packt.live/37IvqSX).'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset has various variables related to the usage level of a mobile connection,
    such as total call minutes, call charges, calls made during certain periods of
    the day, details of international calls, and details of calls to customer services.
  prefs: []
  type: TYPE_NORMAL
- en: The problem statement is to predict whether a customer will churn. This dataset
    is a highly imbalanced one, with the cases where customers churn being the minority.
    You will be using this dataset in the following activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13.01: Finding the Best Balancing Technique by Fitting a Classifier
    on the Telecom Churn Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are working as a data scientist for a telecom company. You have encountered
    a dataset that is highly imbalanced, and you want to correct the class imbalance
    before fitting the classifier to analyze the churn. You know different methods
    for correcting the imbalance in datasets and you want to compare them to find
    the best method before fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you need to implement all of the three methods that you have
    come across so far and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will be using the telecom churn dataset that you used in *Chapter 10*, *Analyzing
    a Dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: Use the `MinMaxscaler` function to scale the dataset instead of the robust scaler
    function you have been using so far. Compare the methods based on the results
    you get by fitting a logistic regression model on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement all the initial steps, which include installing smote-variants and
    loading the data using pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the numerical raw data using the `MinMaxScaler()` function we learned
    about in *Chapter 3, Binary Classification*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create dummy data for the categorical variables using the `pd.get_dummies()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the numerical data from the original data frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concatenate numerical data and dummy categorical data using the `pd.concat()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the earlier dataset into train and test sets using the `train_test_split()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the dataset is imbalanced, you need to perform the various techniques
    mentioned in the following steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the undersampling method, find the index of the minority class using the
    `.index()` function and separate the minority class. After that, sample the majority
    class and make the majority dataset equal to the minority class using the `.sample()`
    function. Concatenate both the minority and under-sampled majority class to form
    a new dataset. Shuffle the dataset and separate the `X` and `Y` variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the under-sampled dataset and name it `churnModel1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the `SMOTE` method, create the oversampler using the `sv.SMOTE()` function
    and create the new `X` and `Y` training sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model using `SMOTE` and name it `churnModel2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the `smote-variant` library and instantiate the `MSMOTE` algorithm using
    the `sv.MSMOTE()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the oversampled data using the oversampler. Please note that the `X`
    and `y` variables have to be converted to a `numpy` array before oversampling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the logistic regression model using the `MSMOTE` dataset and name the model
    `churnModel3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the three separate predictions for each model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate separate accuracy metrics, classification reports, and confusion matrices
    for each of the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the results and select the best method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: The final metrics that you can expect will be similar to what you see here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Undersampling Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.24: Undersampling output report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.24: Undersampling output report'
  prefs: []
  type: TYPE_NORMAL
- en: '**SMOTE Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.25: SMOTE output report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.25: SMOTE output report'
  prefs: []
  type: TYPE_NORMAL
- en: '**MSMOTE Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.26: MSMOTE output report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_13_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13.26: MSMOTE output report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will have different output as the modeling is stochastic in nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about imbalanced datasets and strategies for addressing
    imbalanced datasets. We introduced the use cases where imbalanced datasets would
    be encountered. We looked at the challenges posed by imbalanced datasets and we
    were introduced to the metrics that should be used in the case of an imbalanced
    dataset. We formulated strategies for dealing with imbalanced datasets and implemented
    different strategies, such as random undersampling and oversampling, for balancing
    datasets. We then fit different models after balancing the datasets and analyzed
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing datasets is a very effective way to improve the performance of your
    classifiers. However, it should be noted that there could be a degradation of
    overall accuracy measures for the majority class due to balancing. What strategies
    to adopt in what situations should be arrived at based on the problem statement
    and also after rigorous experiments for those problem statements.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned about methods for dealing with imbalanced datasets, we will now
    be introduced to another important technique that is prevalent in many modern
    datasets called dimensionality reduction. Different techniques for dimensionality
    reduction will be addressed in *Chapter 14*, *Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
