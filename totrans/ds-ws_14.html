<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer665" class="Content">
			</div>
		</div>
		<div id="_idContainer666" class="Content">
			<h1 id="_idParaDest-303"><a id="_idTextAnchor302"/>14. Dimensionality Reduction</h1>
		</div>
		<div id="_idContainer703" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces dimensionality reduction in data science. You will be using the Internet Advertisements dataset to analyze and evaluate different techniques in dimensionality reduction. By the end of this chapter, you will be able to analyze datasets with high dimensions and deal with the challenges posed by these datasets. As well as applying different dimensionality reduction techniques to large datasets, you will fit models based on those datasets and analyze their results. By the end of this chapter, you will be able to deal with huge datasets in the real world.</p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor303"/>Introduction</h1>
			<p>In the previous chapter on balancing datasets, we dealt with the Bank Marketing dataset, which had 18 variables. We were able to load that dataset very easily, fit a model, and get results. But have you considered the scenario when the number of variables you have to deal with is large, say around 18 million instead of the 18 you dealt with in the last chapter? How do you load such large datasets and analyze them? How do you deal with the computing resources required for modeling with such large datasets? </p>
			<p>This is the reality in some modern-day datasets in domains such as:</p>
			<ul>
				<li>Healthcare, where genetics datasets can have millions of features</li>
				<li>High-resolution imaging datasets</li>
				<li>Web data related to advertisements, ranking, and crawling</li>
			</ul>
			<p>When dealing with such huge datasets, many challenges can arise:</p>
			<ul>
				<li>Storage and computation challenges: Large datasets with high dimensions require a lot of storage and expensive computational resources for analysis.</li>
				<li>Exploration challenges: When trying to explore data and derive insights, high-dimensional data can be really cumbersome.</li>
				<li>Algorithm challenges: Many algorithms do not scale well in high-dimensional settings.</li>
			</ul>
			<p>So, what is the solution when we have to deal with high-dimensional data? This is where dimensionality reduction techniques come to the fore, which we will explore in this chapter. </p>
			<p>Dimensionality reduction aims to reduce the dimensions of datasets to get over the challenges posed by high-dimensional data. In this chapter, we will examine some of the popular dimensionality reduction techniques:</p>
			<ul>
				<li>Backward feature elimination or recursive feature elimination</li>
				<li>Forward feature selection</li>
				<li><strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>)</li>
				<li><strong class="bold">Independent Component Analysis</strong> (<strong class="bold">ICA</strong>)</li>
				<li>Factor analysis</li>
			</ul>
			<p>Let's first examine our business context and then apply these techniques to the problem statement.</p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor304"/>Business Context</h2>
			<p>The marketing head of your company comes to you with a problem she has been grappling with. Many customers have been complaining about the browsing experience of your company's website because of the number of advertisements that pop up during browsing. Your company wants to build an engine on your web server that identifies potential advertisements and then eliminates them even before they pop up. </p>
			<p>To help you to achieve this, you have been given a dataset that contains a set of possible advertisements on a variety of web pages. The features of the dataset represent the geometry of the images in the possible adverts, as well as phrases occurring in the URL, image URLs, anchor text, and words occurring near the anchor text. This dataset has also been labeled, with each possible ad given a label that says whether it is actually an advertisement or not. Using this dataset, you have to build a model that predicts whether something is an advertisement or not. You may think that this is a relatively simple problem that could be solved with any binary classification algorithm. However, there is a challenge in the dataset. The dataset has a large number of features. You have set out to solve this high-dimensional dataset challenge.</p>
			<p>This dataset is uploaded in the GitHub repository for working through all the subsequent exercises. The attributes of the dataset are available in the following link: <a href="https://packt.live/36rqiCg">https://packt.live/36rqiCg</a>.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor305"/>Exercise 14.01: Loading and Cleaning the Dataset</h2>
			<p>In this exercise, we will download the dataset, load it in our Colab notebook, and do some basic explorations, such as printing the dimensions of the dataset using the <strong class="source-inline">.shape()</strong> and <strong class="source-inline">.describe()</strong> functions, and also cleaning the dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">internet_ads</strong> dataset has been uploaded to our GitHub repository and can be accessed at <a href="https://packt.live/2sPaVF6">https://packt.live/2sPaVF6</a>.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li>Open a new Colab notebook file.</li>
				<li>Now, <strong class="source-inline">import pandas</strong> into your Colab notebook:<p class="source-code">import pandas as pd</p></li>
				<li>Next, set the path of the drive where the <strong class="source-inline">ad.Data</strong> file is uploaded, as shown in the following code snippet:<p class="source-code"># Defining file name of the GitHub repository</p><p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p></li>
				<li>Read the file using the <strong class="source-inline">pd.read_csv()</strong> function from the pandas data frame: <p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">adData.head()</p><p>The <strong class="source-inline">pd.read_csv()</strong> function's arguments are the filename as a string and the limit separator of a CSV file, which is <strong class="source-inline">","</strong>. Please note that as there are no headers for the dataset. We specifically mention this using the <strong class="source-inline">header = None</strong> command. The last argument, <strong class="source-inline">error_bad_lines=False</strong>, is to skip any errors in the format of the file and then load data.</p><p>After reading the file, the data frame is printed using the <strong class="source-inline">.head()</strong> function.</p><p>You should get the following output:</p><div id="_idContainer667" class="IMG---Figure"><img src="Images/B15019_14_01.jpg" alt="Figure 14.1: Loading data into the Colab notebook&#13;&#10;" width="1343" height="206"/></div><p class="figure-caption">Figure 14.1: Loading data into the Colab notebook</p></li>
				<li>Now, print the shape of the dataset, as shown in the following code snippet:<p class="source-code"># Printing the shape of the data</p><p class="source-code">print(adData.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 1559)</p><p>From the shape, we can see that we have a large number of features, <strong class="source-inline">1559</strong>.</p></li>
				<li>Find the summary of the numerical features of the raw data using the <strong class="source-inline">.describe()</strong> function in pandas, as shown in the following code snippet:<p class="source-code"># Summarizing the statistics of the numerical raw data</p><p class="source-code">adData.describe()</p><p>You should get the following output:</p><div id="_idContainer668" class="IMG---Figure"><img src="Images/B15019_14_02.jpg" alt="Figure 14.2: Loading data into the Colab notebook&#13;&#10;" width="1310" height="279"/></div><p class="figure-caption">Figure 14.2: Loading data into the Colab notebook</p><p>As we saw from the shape of the data, the dataset has <strong class="source-inline">3279</strong> examples with <strong class="source-inline">1559 </strong>variables. The variable set has both categorical and numerical variables. The summary statistics are only derived for numerical data.</p></li>
				<li>Separate the dependent and independent variables from our dataset, as shown in the following code snippet:<p class="source-code"># Separate the dependent and independent variables</p><p class="source-code"># Preparing the X variables</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">print(X.shape)</p><p class="source-code"># Preparing the Y variable</p><p class="source-code">Y = adData[1558]</p><p class="source-code">print(Y.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 1558)</p><p class="source-code">(3279, )</p><p>As seen earlier, there are <strong class="source-inline">1559</strong> features in the dataset. The first <strong class="source-inline">1558</strong> features are independent variables. They are separated from the initial <strong class="source-inline">adData</strong> data frame using the <strong class="source-inline">.loc()</strong> function and give the indexes of the corresponding features (<strong class="source-inline">0</strong> to <strong class="source-inline">1557</strong>). The independent variables are loaded into a new variable called <strong class="source-inline">X</strong>. The dependent variable, which is the label of the dataset, is loaded in variable <strong class="source-inline">Y</strong>. The shapes of the dependent and independent variables are also printed.</p></li>
				<li>Print the first <strong class="source-inline">15</strong> examples of the independent variables:<p class="source-code"># Printing the head of the independent variables</p><p class="source-code">X.head(15)</p><p>You can print as many rows of the data by defining the number within the <strong class="source-inline">head()</strong> function. Here, we have printed out the first <strong class="source-inline">15</strong> rows of the data. </p><p>The output is as follows:</p><div id="_idContainer669" class="IMG---Figure"><img src="Images/B15019_14_03.jpg" alt="Figure 14.3: First 15 examples of independent variables&#13;&#10;" width="1351" height="430"/></div><p class="figure-caption">Figure 14.3: First 15 examples of independent variables</p><p>From the output, we can see that there are many missing values in the dataset, which are represented by <strong class="source-inline">?</strong>. For further analysis, we have to remove these special characters and then replace those cells with assumed values. One popular method of replacing special characters is to impute the mean of the respective feature. Let's adopt this strategy. However, before doing that, let's look at the data types for this dataset to adopt a suitable replacement strategy.</p></li>
				<li>Print the data types of the dataset:<p class="source-code"># Printing the data types</p><p class="source-code">print(X.dtypes)</p><p>We should get the following output:</p><div id="_idContainer670" class="IMG---Figure"><img src="Images/B15019_14_04.jpg" alt="Figure 14.4: The data types in our dataset&#13;&#10;" width="1084" height="462"/></div><p class="figure-caption">Figure 14.4: The data types in our dataset</p><p>From the output, we can see that the first four columns are of the object type, which refers to string data, and the others are integer data. When replacing the special characters in the data, we need to be cognizant of the data types.</p></li>
				<li>Replace special characters with <strong class="source-inline">NaN</strong> values for the first four columns.<p>Replace the special characters in the first four columns, which are of the object type, with <strong class="source-inline">NaN</strong> values. <strong class="source-inline">NaN</strong> is an abbreviation for "not a number." Replacing special characters with <strong class="source-inline">NaN</strong> values makes it easy to further impute data. </p><p>This is achieved through the following code snippet:</p><p class="source-code">"""</p><p class="source-code">Replacing special characters in first 3 columns </p><p class="source-code">which are of type object</p><p class="source-code">"""</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'nan')\</p><p class="source-code">               .values.astype(float)</p><p class="source-code">print(X.head(15))</p><p>To replace the first three columns, we loop through the columns using the <strong class="source-inline">for()</strong> loop and also using the <strong class="source-inline">range()</strong> function. Since the first three columns are of the <strong class="source-inline">object</strong> or <strong class="source-inline">string</strong> type, we use the <strong class="source-inline">.str.replace()</strong> function, which stands for "string replace". After replacing the special characters, <strong class="source-inline">?</strong>, of the data with <strong class="source-inline">nan</strong>, we convert the data type to <strong class="source-inline">float</strong> with the <strong class="source-inline">.values.astype(float)</strong> function, which is required for further processing. By printing the first 15 examples, we can see that all special characters have been replaced with <strong class="source-inline">nan</strong> or <strong class="source-inline">NaN</strong> values</p><p>You should get the following output:</p><div id="_idContainer671" class="IMG---Figure"><img src="Images/B15019_14_05.jpg" alt="Figure 14.5: After replacing special characters with NaN&#13;&#10;" width="612" height="258"/></div><p class="figure-caption">Figure 14.5: After replacing special characters with NaN</p></li>
				<li>Now, replace special characters for the integer features.<p>As in <em class="italic">Step 9</em>, let's also replace the special characters from the features of the <strong class="source-inline">int64</strong> data type with the following code snippet:</p><p class="source-code">"""</p><p class="source-code">Replacing special characters in the remaining </p><p class="source-code">columns which are of type integer</p><p class="source-code">"""</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN').values.astype(float)</p><p class="callout-heading">Note</p><p class="callout">For the integer features, we do not have <strong class="source-inline">.str</strong> before the <strong class="source-inline">.replace()</strong> function, as these features are integer values and not string values.</p></li>
				<li>Now, impute the mean of each column for the <strong class="source-inline">NaN</strong> values.<p>Now that we have replaced special characters in the data with <strong class="source-inline">NaN</strong> values, we can use the <strong class="source-inline">fillna()</strong> function in pandas to replace the <strong class="source-inline">NaN</strong> values with the mean of the column. This is executed using the following code snippet:</p><p class="source-code">import numpy as np</p><p class="source-code"># Impute the 'NaN'  with mean of the values</p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">print(X.head(15))</p><p>In the preceding code snippet, the <strong class="source-inline">.mean()</strong> function calculates the mean of each column and then replaces the <strong class="source-inline">nan</strong> values with the mean of the column.</p><p>You should get the following output:</p><div id="_idContainer672" class="IMG---Figure"><img src="Images/B15019_14_06.jpg" alt="Figure 14.6: Mean of the NaN columns&#13;&#10;" width="615" height="290"/></div><p class="figure-caption">Figure 14.6: Mean of the NaN columns</p></li>
				<li>Scale the dataset using the <strong class="source-inline">minmaxScaler()</strong> function.<p>As in <em class="italic">Chapter 3</em>, <em class="italic">Binary Classification</em>, scaling data is useful in the modeling step. Let's scale the dataset using the <strong class="source-inline">minmaxScaler()</strong> function as learned in <em class="italic">Chapter 3</em>, <em class="italic">Binary Classification</em>. </p><p>This is shown in the following code snippet:</p><p class="source-code"># Scaling the data sets</p><p class="source-code"># Import library function</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code"># Creating the scaling function</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code"># Transforming with the scaler function</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p><p class="source-code"># Printing the output</p><p class="source-code">X_tran.head() </p><p>You should get the following output. Here, we have displayed the first 24 columns:</p><div id="_idContainer673" class="IMG---Figure"><img src="Images/B15019_14_07.jpg" alt="Figure 14.7: Scaling the dataset using the MinMaxScaler() function&#13;&#10;" width="1000" height="222"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.7: Scaling the dataset using the MinMaxScaler() function</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Yi7Nym">https://packt.live/2Yi7Nym</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Q6l9ZZ">https://packt.live/2Q6l9ZZ</a>.</p>
			<p>You have come to the end of the first exercise. In this exercise, we loaded the dataset, extracted summary statistics, cleaned the data, and also scaled the data. You can see that in the final output, all the raw values have been transformed into scaled values.</p>
			<p>In the next section, let's try to augment this dataset with many more features so that this becomes a massive dataset and then fit a simple logistic regression model on this dataset as a benchmark model. </p>
			<p>Let's first see how data can be augmented with an example.</p>
			<h1 id="_idParaDest-307"><a id="_idTextAnchor306"/>Creating a High-Dimensional Dataset</h1>
			<p>In the earlier section, we worked with a dataset that has around <strong class="source-inline">1,558</strong> features. In order to demonstrate the challenges with high-dimensional datasets, let's create an extremely high dimensional dataset from the internet dataset that we already have. </p>
			<p>This we will achieve by replicating the existing number of features multiple times so that the dataset becomes really large. To replicate the dataset, we will use a function called <strong class="source-inline">np.tile()</strong>, which copies a data frame multiple times across the axes we want. We will also calculate the time it takes for any activity using the <strong class="source-inline">time()</strong> function.  </p>
			<p>Let's look at both these functions in action with a toy example.</p>
			<p>You begin by importing the necessary library functions:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p>Then, to create a dummy data frame, we will use a small dataset with two rows and three columns for this example. We use the <strong class="source-inline">pd.np.array()</strong> function to create a data frame:</p>
			<p class="source-code"># Creating a simple data frame</p>
			<p class="source-code">df = pd.np.array([[1, 2, 3], [4, 5, 6]])</p>
			<p class="source-code">print(df.shape)</p>
			<p class="source-code">df</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer674" class="IMG---Figure">
					<img src="Images/B15019_14_08.jpg" alt="Figure 14.8: Array for the sample dummy data frame&#13;&#10;" width="1280" height="132"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.8: Array for the sample dummy data frame</p>
			<p>Next, you replicate the dummy data frame and this replication of the columns is done using the <strong class="source-inline">pd.np.tile()</strong> function in the following code snippet:</p>
			<p class="source-code"># Replicating the data frame and noting the time</p>
			<p class="source-code">import time</p>
			<p class="source-code"># Starting a timing function</p>
			<p class="source-code">t0=time.time()</p>
			<p class="source-code">Newdf = pd.DataFrame(pd.np.tile(df, (1, 5)))</p>
			<p class="source-code">print(Newdf.shape)</p>
			<p class="source-code">print(Newdf)</p>
			<p class="source-code"># Finding the end time</p>
			<p class="source-code">print("Total time:", round(time.time()-t0, 3), "s")</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer675" class="IMG---Figure">
					<img src="Images/B15019_14_09.jpg" alt="Figure 14.9: Replication of the data frame&#13;&#10;" width="1532" height="261"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.9: Replication of the data frame</p>
			<p>As we can see in the snippet, the <strong class="source-inline">pd.np.tile()</strong> function accepts two sets of arguments. The first one is the data frame, <strong class="source-inline">df</strong>, that we want to replicate. The next argument, <strong class="source-inline">(1,5)</strong>, defines which axes we want to replicate. In this example, we define that the rows will remain as is because of the <strong class="source-inline">1</strong> argument, and the columns will be replicated <strong class="source-inline">5</strong> times with the <strong class="source-inline">5</strong> argument. We can see from the <strong class="source-inline">shape()</strong> function that the original data frame, which was of shape <strong class="source-inline">(2,3)</strong>, has been transformed into a data frame with a shape of <strong class="source-inline">(2,15)</strong>. </p>
			<p>Calculating the total time is done using the <strong class="source-inline">time</strong> library. To start the timing, we invoke the <strong class="source-inline">time.time()</strong> function. In the example, we store the initial time in a variable called <strong class="source-inline">t0</strong> and then subtract this from the end time to find the total time it takes for the process. Thus we have augmented and added more data frames to our exiting internet ads dataset.</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor307"/>Activity 14.01: Fitting a Logistic Regression Model on a HighDimensional Dataset</h2>
			<p>You want to test the performance of your models when the dataset is large. To do this, you are artificially augmenting the internet ads dataset so that the dataset is 300 times bigger in dimension than the original dataset. You will be fitting a logistic regression model on this new dataset and then observe the results.</p>
			<p><strong class="bold">Hint</strong>: In this activity, we will use a notebook similar to <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, and we will also be fitting a logistic regression model as done in <em class="italic">Chapter 3</em>, <em class="italic">Binary Classification</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will be using the same ads dataset for this activity.</p>
			<p class="callout">The <strong class="source-inline">internet_ads</strong> dataset has been uploaded to our GitHub repository and can be accessed at <a href="https://packt.live/2sPaVF6">https://packt.live/2sPaVF6</a>.</p>
			<p>The steps to complete this activity are as follows:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Implement all steps from <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, until the normalization of data. Derive the transformed independent <strong class="source-inline">X_tran</strong> variable.</li>
				<li>Create a high-dimensional dataset by replicating the columns 300 times using the <strong class="source-inline">pd.np.tile()</strong> function. Print the shape of the new dataset and observe the number of features in the new dataset.</li>
				<li>Split the dataset into train and test sets.</li>
				<li>Fit a logistic regression model on the new dataset and note the time it takes to fit the model. Note the color change for the indicator for RAM on your Colab notebook.<p><strong class="bold">Expected Output</strong>:</p><p>You should get output similar to the following after fitting the logistic regression model on the new dataset:</p><p class="source-code">Total training time: 23.86 s</p><div id="_idContainer676" class="IMG---Figure"><img src="Images/B15019_14_10.jpg" alt="Figure 14.10: Google Colab RAM utilization&#13;&#10;" width="854" height="101"/></div><p class="figure-caption">Figure 14.10: Google Colab RAM utilization</p></li>
				<li>Predict on the test set and print the classification report and confusion matrix.<p>You should get the following output:</p><div id="_idContainer677" class="IMG---Figure"><img src="Images/B15019_14_11.jpg" alt="Figure 14.11: Confusion matrix and the classification report results&#13;&#10;" width="970" height="331"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.11: Confusion matrix and the classification report results</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to the activity can be found here: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>In this activity, you will have created a high-dimensional dataset by replicating the columns of the existing database and identified that the resource utilization is quite high with this high dimensional dataset. The resource utilization indicator changed its color to orange because of the large dimensions. The longer time, <strong class="source-inline">23.86</strong> seconds, taken for modeling was also noticed on this dataset. You will have also predicted on the test set to get an accuracy level of around <strong class="source-inline">97%</strong> using the logistic regression model.</p>
			<p>First, you need to know why the color of RAM utilization on Colab changed to orange. Because of the huge dataset we created by replication, Colab had to use access RAM, due to which the color changed to orange.</p>
			<p>But, out of curiosity, what do you think the impact will be on the RAM utilization if you increased the replication from 300 to 500? Let's have a look at the following example.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You don't need to perform this on your Colab notebook.</p>
			<p>We begin by defining the path of the dataset for the GitHub repository to our "ads" dataset:</p>
			<p class="source-code"># Defining the file name from GitHub</p>
			<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p>
			<p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p>
			<p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p>
			<p>Next, we simply load the data using pandas:</p>
			<p class="source-code"># import pandas as pd</p>
			<p class="source-code"># Loading the data using pandas</p>
			<p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p>
			<p class="source-code">                     error_bad_lines=False)</p>
			<p>Create a high-dimensional dataset with a scaling factor of <strong class="source-inline">500</strong>:</p>
			<p class="source-code"># Creating a high dimension dataset</p>
			<p class="source-code">X_hd = pd.DataFrame(pd.np.tile(adData, (1, 500)))</p>
			<p>You will see the following output:</p>
			<div>
				<div id="_idContainer678" class="IMG---Figure">
					<img src="Images/B15019_14_12.jpg" alt="Figure 14.12: Colab crashing&#13;&#10;" width="1159" height="67"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.12: Colab crashing</p>
			<p>From the output, you can see that the session crashes because all the RAM provided by Colab has been used. The session will restart, and you will lose all your variables. Hence, it is always good to be mindful of the resources you are provided with, along with the dataset. As a data scientist, if you feel that a dataset is huge with many features but the resources to process that dataset are limited, you need to get in touch with the organization and get the required resources or build an appropriate strategy to address these high-dimensional datasets.</p>
			<h1 id="_idParaDest-309"><a id="_idTextAnchor308"/>Strategies for Addressing High-Dimensional Datasets</h1>
			<p>In <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a High-Dimensional Dataset</em>, we witnessed the challenges of high-dimensional datasets. We saw how the resources were challenged when the replication factor was 300. You also saw that the notebook crashes when the replication factor is increased to 500. When the replication factor was 500, the number of features was around 750,000. In our case, our resources would fail to scale up even before we hit the 1 million mark on the number of features. Some modern-day datasets sometimes have hundreds of millions, or in many cases billions, of features. Imagine the kind of resources and time it would take to get any actionable insights from the dataset.</p>
			<p>Luckily, we have many robust methods for addressing high-dimensional datasets. Many of these techniques are very effective and have helped to address the challenges raised by huge datasets.</p>
			<p>Let's look at some of the techniques for dealing with high-dimensional datasets. In <em class="italic">Figure 14.14</em>, you can see the strategies we will be coming across in this chapter to deal with such datasets:</p>
			<div>
				<div id="_idContainer679" class="IMG---Figure">
					<img src="Images/B15019_14_13.jpg" alt="Figure 14.13: Strategies to address high dimensional datasets&#13;&#10;" width="1067" height="270"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.13: Strategies to address high dimensional datasets</p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor309"/>Backward Feature Elimination (Recursive Feature Elimination)</h2>
			<p>The mechanism behind the backward feature elimination algorithm is the recursive elimination of features and building a model on those features that remain after all the elimination. </p>
			<p>Let's look under the hood of this algorithm step by step:</p>
			<ol>
				<li value="1">Initially, at a given iteration, the selected classification algorithm is first trained on all the <strong class="source-inline">n</strong> features available. For example, let's take the case of the original dataset we had, which had <strong class="source-inline">1,558</strong> features. The algorithm starts off with all the <strong class="source-inline">1,558</strong> features in the first iteration.</li>
				<li>In the next step, we remove one feature at a time and train a model with the remaining <strong class="source-inline">n-1</strong> features. This process is repeated <strong class="source-inline">n</strong> times. For example, we first remove feature 1 and then fit a model using all the remaining 1,557 variables. In the next iteration, we use feature <strong class="source-inline">1</strong> and instead, we eliminate feature <strong class="source-inline">2</strong> and then fit the model. This process is repeated <strong class="source-inline">n</strong> times (<strong class="source-inline">1,558</strong>) times. </li>
				<li>For each of the models fitted, the performance of the model (using measures such as accuracy) is calculated.</li>
				<li>The feature whose replacement has resulted in the smallest change in performance is removed permanently and <em class="italic">Step 2</em> is repeated with <strong class="source-inline">n-1</strong> features. </li>
				<li>The process is then repeated with <strong class="source-inline">n-2</strong> features and so on. </li>
				<li>The algorithm keeps on eliminating features until the threshold number of features we require is reached.</li>
			</ol>
			<p>Let's take a look at the backward feature elimination algorithm in action for the augmented ads dataset in the next exercise.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor310"/>Exercise 14.02: Dimensionality Reduction Using Backward Feature Elimination</h2>
			<p>In this exercise, we will fit a logistic regression model after eliminating features using the backward elimination technique to find the accuracy of the model. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Implement all the initial steps similar to <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, until scaling the dataset using the <strong class="source-inline">minmaxscaler()</strong> function:<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p><p class="source-code">import pandas as pd</p><p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">Y = adData[1558]</p><p class="source-code">import numpy as np</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'NaN').values.astype(float)</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN').values.astype(float)</p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Next, create a high-dimensional dataset. We'll augment the dataset artificially by a factor of <strong class="source-inline">2</strong>. The process of backward feature elimination is a very compute-intensive process, and using higher dimensions will involve a longer processing time. This is why the augmenting factor has been kept at <strong class="source-inline">2</strong>. This is implemented using the following code snippet:<p class="source-code"># Creating a high dimension data set</p><p class="source-code">X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 2)))</p><p class="source-code">print(X_hd.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 3116)</p></li>
				<li>Define the backward elimination model. Backward elimination works by providing two arguments to the <strong class="source-inline">RFE()</strong> function, which is the model we want to try (logistic regression in our case) and the number of features we want the dataset to be reduced to. This is implemented as follows:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.feature_selection import RFE</p><p class="source-code"># Defining the Classification function</p><p class="source-code">backModel = LogisticRegression()</p><p class="source-code">"""</p><p class="source-code">Reducing dimensionality to 250 features for the </p><p class="source-code">backward elimination model</p><p class="source-code">"""</p><p class="source-code">rfe = RFE(backModel, 250)</p><p>In this implementation, the number of features that we have given, <strong class="source-inline">250</strong>, is identified through trial and error. The process is to first assume an arbitrary number of features and then, based on the final metrics, arrive at the most optimum number of features for the model. In this implementation, our first assumption of <strong class="source-inline">250</strong> implies that we want the backward elimination model to start eliminating features until we get the best <strong class="source-inline">250</strong> features.</p></li>
				<li>Fit the backward elimination method to identify the best <strong class="source-inline">250</strong> features.<p>We are now ready to fit the backward elimination method on the higher-dimensional dataset. We will also note the time it takes for backward elimination to work. This is implemented using the following code snippet:</p><p class="source-code"># Fitting the rfe for selecting the top 250 features</p><p class="source-code">import time</p><p class="source-code">t0 = time.time()</p><p class="source-code">rfe = rfe.fit(X_hd, Y)</p><p class="source-code">t1 = time.time()</p><p class="source-code">print("Backward Elimination time:", \</p><p class="source-code">      round(t1-t0, 3), "s")</p><p>Fitting the backward elimination method is done using the <strong class="source-inline">.fit()</strong> function. We give the independent and dependent training sets. </p><p class="callout-heading">Note</p><p class="callout">The backward elimination method is a compute-intensive process, and therefore this process will take a lot of time to execute. The larger the number of features, the longer it will take.</p><p>The time for backward elimination is at the end of the notifications:</p><div id="_idContainer680" class="IMG---Figure"><img src="Images/B15019_14_14.jpg" alt="Figure 14.14: The time taken for the backward elimination process&#13;&#10;" width="598" height="278"/></div><p class="figure-caption">Figure 14.14: The time taken for the backward elimination process</p><p>You can see that the backward elimination process to find the best <strong class="source-inline">250</strong> features has taken <strong class="source-inline">230.35</strong> seconds to implement. </p></li>
				<li>Display the features identified using the backward elimination method. We can display the <strong class="source-inline">250</strong> features that were identified using the backward elimination process using the <strong class="source-inline">get_support()</strong> function. This is implemented as follows:<p class="source-code"># Getting the indexes of the features used</p><p class="source-code">rfe.get_support(indices = True)</p><p>You should get the following output:</p><div id="_idContainer681" class="IMG---Figure"><img src="Images/B15019_14_15.jpg" alt="Figure 14.15: The identified features being displayed&#13;&#10;" width="622" height="383"/></div><p class="figure-caption">Figure 14.15: The identified features being displayed</p><p>These are the best <strong class="source-inline">250</strong> features that were finally selected using the backward elimination process from the entire dataset.</p></li>
				<li>Now, split the dataset into training and testing sets for modeling:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_hd, Y, test_size=0.3,\</p><p class="source-code">                                    random_state=123)</p><p class="source-code">print('Training set shape',X_train.shape)</p><p class="source-code">print('Test set shape',X_test.shape)</p><p>You should get the following output:</p><p class="source-code">Training set shape (2295, 3116)</p><p class="source-code">Test set shape (984, 3116)</p><p>From the output, you see the shapes of both the training set and testing sets.</p></li>
				<li>Transform the train and test sets. In <em class="italic">step 5</em>, we identified the top <strong class="source-inline">250</strong> features through backward elimination. Now we need to reduce the train and test sets to those top <strong class="source-inline">250</strong> features. This is done using the <strong class="source-inline">.transform()</strong> function. This is implemented using the following code snippet:<p class="source-code"># Transforming both train and test sets</p><p class="source-code">X_train_tran = rfe.transform(X_train)</p><p class="source-code">X_test_tran = rfe.transform(X_test)</p><p class="source-code">print("Training set shape",X_train_tran.shape)</p><p class="source-code">print("Test set shape",X_test_tran.shape)</p><p>You should get the following output:</p><p class="source-code">Training set shape (2295, 250)</p><p class="source-code">Test set shape (984, 250)</p><p>We can see that both the training set and test sets have been reduced to the <strong class="source-inline">250</strong> best features.</p></li>
				<li>Fit a logistic regression model on the training set and note the time:<p class="source-code"># Fitting the logistic regression model</p><p class="source-code">import time</p><p class="source-code"># Defining the LogisticRegression function</p><p class="source-code">RfeModel = LogisticRegression()</p><p class="source-code"># Starting a timing function</p><p class="source-code">t0=time.time()</p><p class="source-code"># Fitting the model</p><p class="source-code">RfeModel.fit(X_train_tran, y_train)</p><p class="source-code"># Finding the end time</p><p class="source-code">print("Total training time:", \</p><p class="source-code">      round(time.time()-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">Total training time: 0.016 s</p><p>As expected, the total time it takes to fit a model on a reduced set of features is much lower than the time it took for the larger dataset in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset</em>, which was <strong class="source-inline">23.86</strong> seconds. This is a great improvement.</p></li>
				<li>Now, predict on the test set and print the accuracy metrics, as shown in the following code snippet:<p class="source-code"># Predicting on the test set and getting the accuracy</p><p class="source-code">pred = RfeModel.predict(X_test_tran)</p><p class="source-code">print('Accuracy of Logistic regression model after '\</p><p class="source-code">      'backward elimination: {:.2f}'\</p><p class="source-code">      .format(RfeModel.score(X_test_tran, y_test)))</p><p>You should get the following output:</p><div id="_idContainer682" class="IMG---Figure"><img src="Images/B15019_14_16.jpg" alt="Figure 14.16: The achieved accuracy of the logistic regression model&#13;&#10;" width="1360" height="48"/></div><p class="figure-caption">Figure 14.16: The achieved accuracy of the logistic regression model</p><p>You can see that the accuracy measure for this model has improved compared to the one we got for the model with higher dimensionality, which was <strong class="source-inline">0.97</strong> in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset</em>. This increase could be attributed to the identification of non-correlated features from the complete feature set, which could have boosted the performance of the model.</p></li>
				<li>Print the confusion matrix:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get the following output:</p><div id="_idContainer683" class="IMG---Figure"><img src="Images/B15019_14_17.jpg" alt="Figure 14.17: Confusion matrix&#13;&#10;" width="1436" height="122"/></div><p class="figure-caption">Figure 14.17: Confusion matrix</p></li>
				<li>Printing the classification report:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code"># Getting the Classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get the following output:</p><div id="_idContainer684" class="IMG---Figure"><img src="Images/B15019_14_18.jpg" alt="Figure 14.18: Classification matrix&#13;&#10;" width="1235" height="322"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.18: Classification matrix</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ca5k6">https://packt.live/31ca5k6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/329yJkF">https://packt.live/329yJkF</a>.</p>
			<p>As we can see from the backward elimination process, we were able to get an improved accuracy of <strong class="source-inline">98%</strong> with <strong class="source-inline">250</strong> features, compared to the model in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset,</em> where an artificially enhanced dataset was used and got an accuracy of <strong class="source-inline">97%</strong>.</p>
			<p>However, it should be noted that dimensionality reduction techniques should not be viewed as a method to improve the performance of any model. Dimensionality reduction techniques have to be viewed from the perspective of enabling us to fit a model on datasets with large numbers of features. When dimensions increase, fitting the model becomes intractable. This can be observed if the scaling factor used in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset,</em> was to be increased from <strong class="source-inline">300</strong> to <strong class="source-inline">500</strong>. In such cases, fitting a model wouldn't happen with the current set of resources. </p>
			<p>Dimensionality reduction aids in such scenarios by reducing the number of features, thereby enabling the fitting of a model on reduced dimensions without a large degradation of performance, and can sometimes lead to an improvement in results. However, it should also be noted that methods such as backward elimination are compute-intensive processes. You would have observed this phenomenon as to the time it takes in identifying the top 250 features when the scaling factor was just 2. With much higher scaling factors, it will take far more time and resources to identify the top 250 features.</p>
			<p>Having seen the backward elimination method, let's now look at the next technique, which is forward feature selection.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor311"/>Forward Feature Selection</h2>
			<p>Forward feature selection works in the reverse order as backward elimination. In this process, we start off with an initial feature, and features are added one by one until no improvement in performance is achieved. The detailed process is as follows:</p>
			<ol>
				<li value="1">Start model building with one feature. </li>
				<li>Iterate the model building process <em class="italic">n</em> times, each time selecting one feature at a time. The feature that gives the highest improvement in performance is selected.</li>
				<li>Once the first feature is selected, it is the time to select the second feature. The process for selecting the second feature proceeds exactly the same as <em class="italic">step 2</em>. The remaining <em class="italic">n-1</em> features are iterated along with the first feature and the performance on the model is observed. The feature that produces the biggest improvement in model performance is selected as the second feature.</li>
				<li>The iteration of features will continue until a threshold number of features we have determined is extracted. </li>
				<li>The set of final features selected will be the ones that give the maximum model performance. </li>
			</ol>
			<p>Let's now implement this algorithm in the next exercise.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor312"/>Exercise 14.03: Dimensionality Reduction Using Forward Feature Selection</h2>
			<p>In this exercise, we will fit a logistic regression model by selecting the optimum features through forward feature selection and observing the performance of the model. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Implement all the initial steps similar to <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, up until scaling the dataset using <strong class="source-inline">MinMaxScaler()</strong>:<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p><p class="source-code">import pandas as pd</p><p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">Y = adData[1558]</p><p class="source-code">import numpy as np</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'NaN')\</p><p class="source-code">               .values.astype(float)</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN').values.astype(float)</p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Create a high-dimensional dataset. Now, augment the dataset artificially to a factor of <strong class="source-inline">50</strong>. Augmenting the dataset to higher factors will result in the notebook crashing because of lack of memory. This is implemented using the following code snippet:<p class="source-code"># Creating a high dimension dataset</p><p class="source-code">X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))</p><p class="source-code">print(X_hd.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 77900)</p></li>
				<li>Split the high dimensional dataset into training and testing sets:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_hd, Y, test_size=0.3, \</p><p class="source-code">                                    random_state=123)</p></li>
				<li>Now we define the threshold features. Once the train and test sets are created, the next step is to import the feature selection function, <strong class="source-inline">SelectKBest</strong>. The argument we give to this function is the number of features we want. The features are selected through experimentation and, as a first step, we assume a threshold value. In this example, we assume a threshold value of <strong class="source-inline">250</strong>. This is implemented using the following code snippet:<p class="source-code">from sklearn.feature_selection import SelectKBest</p><p class="source-code"># feature extraction</p><p class="source-code">feats = SelectKBest(k=250)</p></li>
				<li>Iterate and get the best set of threshold features. Based on the threshold set of features we defined, we have to fit the training set and get the best set of threshold features. Fitting on the training set is done using the <strong class="source-inline">.fit()</strong> function. We also note the time it takes to find the best set of features. This is executed using the following code snippet:<p class="source-code"># Fitting the features for training set</p><p class="source-code">import time</p><p class="source-code">t0 = time.time()</p><p class="source-code">fit = feats.fit(X_train, y_train)</p><p class="source-code">t1 = time.time()</p><p class="source-code">print("Forward selection fitting time:", \</p><p class="source-code">      round(t1-t0, 3), "s")</p><p>You should get something similar to the following output:</p><p class="source-code">Forward selection fitting time: 2.682 s</p><p>We can see that the forward selection method has taken around <strong class="source-inline">2.68</strong> seconds, which is much lower than the backward selection method.</p></li>
				<li>Create new training and test sets. Once we have identified the best set of features, we have to modify our training and test sets so that they have only those selected features. This is accomplished using the <strong class="source-inline">.transform()</strong> function:<p class="source-code"># Creating new training set and test sets </p><p class="source-code">features_train = fit.transform(X_train)</p><p class="source-code">features_test = fit.transform(X_test)</p></li>
				<li>Let's verify the shapes of the train and test sets before transformation and after transformation:<p class="source-code">"""</p><p class="source-code">Printing the shape of training and test sets </p><p class="source-code">before transformation</p><p class="source-code">"""</p><p class="source-code">print('Train shape before transformation',\</p><p class="source-code">      X_train.shape)</p><p class="source-code">print('Test shape before transformation',\</p><p class="source-code">      X_test.shape)</p><p class="source-code">"""</p><p class="source-code">Printing the shape of training and test sets </p><p class="source-code">after transformation</p><p class="source-code">"""</p><p class="source-code">print('Train shape after transformation',\</p><p class="source-code">      features_train.shape)</p><p class="source-code">print('Test shape after transformation',\</p><p class="source-code">      features_test.shape)</p><p>You should get the following output:</p><div id="_idContainer685" class="IMG---Figure"><img src="Images/B15019_14_19.jpg" alt="Figure 14.19: Shape of the training and testing datasets&#13;&#10;" width="1449" height="189"/></div><p class="figure-caption">Figure 14.19: Shape of the training and testing datasets</p><p>You can see that both the training and test sets are reduced to <strong class="source-inline">250</strong> features each.</p></li>
				<li>Let's now fit a logistic regression model on the transformed dataset and note the time it takes to fit the model:<p class="source-code"># Fitting a Logistic Regression Model</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">import time</p><p class="source-code">t0 = time.time()</p><p class="source-code">forwardModel = LogisticRegression()</p><p class="source-code">forwardModel.fit(features_train, y_train)</p><p class="source-code">t1 = time.time()</p></li>
				<li>Print the total time:<p class="source-code">print("Total training time:", round(t1-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">Total training time: 0.035 s</p><p>You can see that the training time is much less than the model that was fit in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset</em>, which was <strong class="source-inline">23.86</strong> seconds. This shorter time is attributed to the number of features in the forward selection model.</p></li>
				<li>Now, perform predictions on the test set and print the accuracy metrics:<p class="source-code"># Predicting with the forward model</p><p class="source-code">pred = forwardModel.predict(features_test)</p><p class="source-code">print('Accuracy of Logistic regression'\</p><p class="source-code">      ' model prediction on test set: {:.2f}'</p><p class="source-code">      .format(forwardModel.score(features_test, y_test)))</p><p>You should get the following output:</p><p class="source-code">Accuracy of Logistic regression model prediction on test set: 0.94</p></li>
				<li>Print the confusion matrix:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get something similar to the following output:</p><div id="_idContainer686" class="IMG---Figure"><img src="Images/B15019_14_20.jpg" alt="Figure 14.20: Resulting confusion matrix&#13;&#10;" width="1665" height="112"/></div><p class="figure-caption">Figure 14.20: Resulting confusion matrix</p></li>
				<li>Print the classification report:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code"># Getting the Classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get something similar to the following output:</p><div id="_idContainer687" class="IMG---Figure"><img src="Images/B15019_14_21.jpg" alt="Figure 14.21: Resulting classification report&#13;&#10;" width="1199" height="318"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.21: Resulting classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YhQE7X">https://packt.live/2YhQE7X</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>As we can see from the forward selection process, we were able to get an accuracy score of <strong class="source-inline">94%</strong> with <strong class="source-inline">250</strong> features. This score is lower than the one that was achieved with the backward elimination method (<strong class="source-inline">98%</strong>) and also the benchmark model (<strong class="source-inline">97%</strong>) built in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset</em>. However, the time taken to find the best features (2.68 seconds) was substantially less than the backward elimination method (230.35 seconds).</p>
			<p>In the next section, we will be looking at Principal Component Analysis (PCA).</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor313"/>Principal Component Analysis (PCA)</h2>
			<p>PCA is a very effective dimensionality reduction technique that achieves dimensionality reduction without compromising on the information content of the data. The basic idea behind PCA is to first identify correlations among different variables within the dataset. Once correlations are identified, the algorithm decides to eliminate the variables in such a way that the variability of the data is maintained. In other words, PCA aims to find uncorrelated sources of data.</p>
			<p>Implementing PCA on raw variables results in transforming them into a completely new set of variables called principal components. Each of these components represents variability in data along an axes that are orthogonal to each other. This means that the first axis is fit in the direction where the maximum variability of data is present. After this, the second axis is selected in such a way that the axis is orthogonal (perpendicular) to the first selected axis and also covers the next highest variability.</p>
			<p>Let's look at the idea of PCA with an example. </p>
			<p>We will create a sample dataset with 2 variables and 100 random data points in each variable. Random data points are created using the <strong class="source-inline">rand()</strong> function. This is implemented in the following code:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code"># Setting the seed for reproducibility</p>
			<p class="source-code">seed = np.random.RandomState(123)</p>
			<p class="source-code"># Generating an array of random numbers</p>
			<p class="source-code">X = seed.rand(100,2)</p>
			<p class="source-code"># Printing the shape of the dataset</p>
			<p class="source-code">X.shape</p>
			<p>The resulting output is: <strong class="source-inline">(100, 2)</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A random state is defined using the <strong class="source-inline">RandomState(123)</strong> function. This is defined to ensure that anyone who reproduces this example gets the same output.</p>
			<p>Let's visualize this data using <strong class="source-inline">matplotlib</strong>:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">%matplotlib inline</p>
			<p class="source-code">plt.scatter(X[:, 0], X[:, 1])</p>
			<p class="source-code">plt.axis('equal')</p>
			<p>You should get the following output:</p>
			<p class="source-code">(-0.04635361265714105,</p>
			<p class="source-code"> 1.0325632864350174,</p>
			<p class="source-code"> -0.003996887112708292,</p>
			<p class="source-code"> 1.0429468329457663)</p>
			<div>
				<div id="_idContainer688" class="IMG---Figure">
					<img src="Images/B15019_14_22.jpg" alt="Figure 14.22: Visualization of the data&#13;&#10;" width="814" height="392"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.22: Visualization of the data</p>
			<p>In the graph, we can see that the data is evenly spread out.</p>
			<p>Let's now find the principal components for this dataset. We will reduce this two-dimensional dataset into a one-dimensional dataset. In other words, we will reduce the original dataset into one of its principal components.</p>
			<p>This is implemented in code as follows:</p>
			<p class="source-code">from sklearn.decomposition import PCA</p>
			<p class="source-code"># Defining one component</p>
			<p class="source-code">pca = PCA(n_components=1)</p>
			<p class="source-code"># Fitting the PCA function</p>
			<p class="source-code">pca.fit(X)</p>
			<p class="source-code"># Getting the new dataset</p>
			<p class="source-code">X_pca = pca.transform(X)</p>
			<p class="source-code"># Printing the shapes</p>
			<p class="source-code">print("Original data set:   ", X.shape)</p>
			<p class="source-code">print("Data set after transformation:", X_pca.shape)</p>
			<p>You should get the following output:</p>
			<p class="source-code">original shape: (100, 2)</p>
			<p class="source-code">transformed shape: (100, 1)</p>
			<p>As we can see in the code, we first define the number of components using the <strong class="source-inline">'n_components' = 1</strong> argument. After this, the PCA algorithm is fit on the input dataset. After fitting on the input data, the initial dataset is transformed into a new dataset with only one variable, which is its principal component.</p>
			<p>The algorithm transforms the original dataset into its first principal component by using an axis where the data has the largest variability. </p>
			<p>To visualize this concept, let's reverse the transformation of the <strong class="source-inline">X_pca</strong> dataset to its original form and then visualize this data along with the original data. To reverse the transformation, we use the <strong class="source-inline">.inverse_transform()</strong> function:</p>
			<p class="source-code"># Reversing the transformation and plotting </p>
			<p class="source-code">X_reverse = pca.inverse_transform(X_pca)</p>
			<p class="source-code"># Plotting the original data</p>
			<p class="source-code">plt.scatter(X[:, 0], X[:, 1], alpha=0.1)</p>
			<p class="source-code"># Plotting the reversed data</p>
			<p class="source-code">plt.scatter(X_reverse[:, 0], X_reverse[:, 1], alpha=0.9)</p>
			<p class="source-code">plt.axis('equal');</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer689" class="IMG---Figure">
					<img src="Images/B15019_14_23.jpg" alt="Figure 14.23: Plot with reverse transformation&#13;&#10;" width="777" height="390"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.23: Plot with reverse transformation</p>
			<p>As we can see in the plot, the data points in orange represent an axis with the highest variability. All the data points were projected to that axis to generate the first principal component. </p>
			<p>The data points that are generated when transforming into various principal components will be very different from the original data points before transformation. Each principal component will be in an axis that is orthogonal (perpendicular) to the other principal component. If a second principal component was generated for the preceding example, the second principal component would be along an axis indicated by the blue arrow in the graph. The way we pick the number of principal components for model building is by selecting the number of components that explains a certain threshold of variability. </p>
			<p>For example, if there were originally 1,000 features and we reduced it to 100 principal components, and then we find that out of the 100 principal components the first 75 components explain 90% of the variability of data, we would pick those 75 components to build the model. This process is called picking principal components with the percentage of variance explained.</p>
			<p>Let's now see how to use PCA as a tool for dimensionality reduction in our use case.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor314"/>Exercise 14.04: Dimensionality Reduction Using PCA</h2>
			<p>In this exercise, we will fit a logistic regression model by selecting the principal components that explain the maximum variability of the data. We will also observe the performance of the feature selection and model building process. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file. </li>
				<li>Implement the initial steps from <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, up until scaling the dataset using the <strong class="source-inline">minmaxscaler()</strong> function:<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p><p class="source-code">import pandas as pd</p><p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">Y = adData[1558]</p><p class="source-code">import numpy as np</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'NaN').values.astype(float)</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN').values.astype(float)</p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Create a high-dimensional dataset. Let's now augment the dataset artificially to a factor of 50. Augmenting the dataset to higher factors will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:<p class="source-code"># Creating a high dimension data set</p><p class="source-code">X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))</p><p class="source-code">print(X_hd.shape)</p><p>You should get the following output</p><p class="source-code">(3279, 77900)</p></li>
				<li>Let's split the high-dimensional dataset to training and test sets:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_hd, Y, test_size=0.3, \</p><p class="source-code">                                    random_state=123)</p></li>
				<li>Let's now fit the PCA function on the training set. This is done using the <strong class="source-inline">.fit()</strong> function, as shown in the following snippet. We will also note the time it takes to fit the PCA model on the dataset:<p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">import time</p><p class="source-code">t0 = time.time()</p><p class="source-code">pca = PCA().fit(X_train)</p><p class="source-code">t1 = time.time()</p><p class="source-code">print("PCA fitting time:", round(t1-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">PCS fitting time: 179.545 s</p><p>We can see that the time taken to fit the PCA function on the dataset is less than the backward elimination model (230.35 seconds) and higher than the forward selection method (2.682 seconds).</p></li>
				<li>We will now determine the number of principal components by plotting the cumulative variance explained by all the principal components. The variance explained is determined by the <strong class="source-inline">pca.explained_variance_ratio_</strong> method. This is plotted in <strong class="source-inline">matplotlib</strong> using the following code snippet:<p class="source-code">%matplotlib inline</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">plt.plot(np.cumsum(pca.explained_variance_ratio_))</p><p class="source-code">plt.xlabel('Number of Principal Components')</p><p class="source-code">plt.ylabel('Cumulative explained variance');</p><p>In the code, the <strong class="source-inline">np.cumsum()</strong> function is used to get the cumulative variance of each principal component.</p><p>You will get the following plot as output:</p><div id="_idContainer690" class="IMG---Figure"><img src="Images/B15019_14_24.jpg" alt="Figure 14.24: The variance graph&#13;&#10;" width="917" height="444"/></div><p class="figure-caption">Figure 14.24: The variance graph</p><p>From the plot, we can see that the first <strong class="source-inline">250</strong> principal components explain more than <strong class="source-inline">90%</strong> of the variance. Based on this graph, we can decide how many principal components we want to have depending on the variability it explains. Let's select <strong class="source-inline">250</strong> components for fitting our model.</p></li>
				<li>Now that we have identified that <strong class="source-inline">250</strong> components explain a lot of the variability, let's refit the training set for <strong class="source-inline">250</strong> components. This is described in the following code snippet:<p class="source-code"># Defining PCA with 250 components</p><p class="source-code">pca = PCA(n_components=250)</p><p class="source-code"># Fitting PCA on the training set</p><p class="source-code">pca.fit(X_train)</p></li>
				<li>We now transform the training and test sets with the 200 principal components:<p class="source-code"># Transforming training set and test set</p><p class="source-code">X_pca = pca.transform(X_train)</p><p class="source-code">X_test_pca = pca.transform(X_test)</p></li>
				<li>Let's verify the shapes of the train and test sets before transformation and after transformation:<p class="source-code">"""</p><p class="source-code">Printing the shape of train and test sets before </p><p class="source-code">and after transformation</p><p class="source-code">"""</p><p class="source-code">print("original shape of Training set:   ", \</p><p class="source-code">      X_train.shape)</p><p class="source-code">print("original shape of Test set:   ", \</p><p class="source-code">      X_test.shape)</p><p class="source-code">print("Transformed shape of training set:", \</p><p class="source-code">      X_pca.shape)</p><p class="source-code">print("Transformed shape of test set:", \</p><p class="source-code">      X_test_pca.shape)</p><p>You should get the following output:</p><div id="_idContainer691" class="IMG---Figure"><img src="Images/B15019_14_25.jpg" alt="Figure 14.25: Transformed and the original training and testing sets&#13;&#10;" width="1565" height="230"/></div><p class="figure-caption">Figure 14.25: Transformed and the original training and testing sets</p><p>You can see that both the training and test sets are reduced to <strong class="source-inline">250</strong> features each.</p></li>
				<li>Let's now fit the logistic regression model on the transformed dataset and note the time it takes to fit the model:<p class="source-code"># Fitting a Logistic Regression Model</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">import time</p><p class="source-code">pcaModel = LogisticRegression()</p><p class="source-code">t0 = time.time()</p><p class="source-code">pcaModel.fit(X_pca, y_train)</p><p class="source-code">t1 = time.time()</p></li>
				<li>Print the total time:<p class="source-code">print("Total training time:", round(t1-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">Total training time: 0.293 s</p><p>You can see that the training time is much lower than the model that was fit in <em class="italic">Activity 14.01</em>, <em class="italic">Fitting a Logistic Regression Model on a HighDimensional Dataset</em>, which was 23.86 seconds. The shorter time is attributed to the smaller number of features, <strong class="source-inline">250</strong>, selected in PCA.</p></li>
				<li>Now, predict on the test set and print the accuracy metrics:<p class="source-code"># Predicting with the pca model</p><p class="source-code">pred = pcaModel.predict(X_test_pca)</p><p class="source-code">print('Accuracy of Logistic regression model '\</p><p class="source-code">      'prediction on test set: {:.2f}'\</p><p class="source-code">      .format(pcaModel.score(X_test_pca, y_test)))</p><p>You should get the following output:</p><div id="_idContainer692" class="IMG---Figure"><img src="Images/B15019_14_26.jpg" alt="Figure 14.26: Accuracy of the logistic regression model&#13;&#10;" width="1538" height="56"/></div><p class="figure-caption">Figure 14.26: Accuracy of the logistic regression model</p><p>You can see that the accuracy level is better than the benchmark model with all the features (<strong class="source-inline">97%</strong>) and the forward selection model (<strong class="source-inline">94%</strong>).</p></li>
				<li>Print the confusion matrix:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get the following output:</p><div id="_idContainer693" class="IMG---Figure"><img src="Images/B15019_14_27.jpg" alt="Figure 14.27: Resulting confusion matrix&#13;&#10;" width="1665" height="123"/></div><p class="figure-caption">Figure 14.27: Resulting confusion matrix</p></li>
				<li>Print the classification report:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code"># Getting the Classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get the following output:</p><div id="_idContainer694" class="IMG---Figure"><img src="Images/B15019_14_28.jpg" alt="Figure 14.28: Resulting classification matrix&#13;&#10;" width="1124" height="314"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.28: Resulting classification matrix</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iXNVbq">https://packt.live/3iXNVbq</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>As is evident from the results, we get a score of 98%, which is better than the benchmark model. One reason that could be attributed to the higher performance could be the creation of uncorrelated principal components using the PCA method, which has boosted the performance.</p>
			<p>In the next section, we will be looking at Independent Component Analysis (ICA).</p>
			<h2 id="_idParaDest-316"><a id="_idTextAnchor315"/>Independent Component Analysis (ICA)</h2>
			<p>ICA is a technique of dimensionality reduction that conceptually follows a similar path as PCA. Both ICA and PCA try to derive new sources of data by linearly combining the original data.</p>
			<p>However, the difference between them lies in the method they use to find new sources of data. While PCA attempts to find uncorrelated sources of data, ICA attempts to find independent sources of data.</p>
			<p>ICA has a very similar implementation for dimensionality reduction as PCA. </p>
			<p>Let's look at the implementation of ICA for our use case.</p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor316"/>Exercise 14.05: Dimensionality Reduction Using Independent Component Analysis</h2>
			<p>In this exercise, we will fit a logistic regression model using the ICA technique and observe the performance of the model. We will be using the same ads dataset as before, and we will be enhancing it with additional features for this exercise.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Implement all the steps from <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, up until scaling the dataset using <strong class="source-inline">MinMaxScaler()</strong>:<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p><p class="source-code">import pandas as pd</p><p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">Y = adData[1558]</p><p class="source-code">import numpy as np</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'NaN')\</p><p class="source-code">               .values.astype(float)</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN')\</p><p class="source-code">               .values.astype(float)  </p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Let's now augment the dataset artificially to a factor of <strong class="source-inline">50</strong>. Augmenting the dataset to factors that are higher than <strong class="source-inline">50</strong> will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:<p class="source-code"># Creating a high dimension data set</p><p class="source-code">X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))</p><p class="source-code">print(X_hd.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 77900)</p></li>
				<li>Let's split the high-dimensional dataset into training and testing sets:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_hd, Y, test_size=0.3,\</p><p class="source-code">                                    random_state=123)</p></li>
				<li>Let's load the ICA function, <strong class="source-inline">FastICA</strong>, and then define the number of components we require. We will use the same number of components that we used for PCA:<p class="source-code"># Defining the ICA with number of components</p><p class="source-code">from sklearn.decomposition import FastICA </p><p class="source-code">ICA = FastICA(n_components=250, random_state=123)</p></li>
				<li>Once the ICA method is defined, we will fit the method on the training set and also transform the training set to get a new training set with the required number of components. We will also note the time taken for fitting and transforming:<p class="source-code">"""</p><p class="source-code">Fitting the ICA method and transforming the </p><p class="source-code">training set import time</p><p class="source-code">"""</p><p class="source-code">t0 = time.time()</p><p class="source-code">X_ica=ICA.fit_transform(X_train)</p><p class="source-code">t1 = time.time()</p><p class="source-code">print("ICA fitting time:", round(t1-t0, 3), "s")</p><p>In the code, the <strong class="source-inline">.fit()</strong> function is used to fit on the training set and the <strong class="source-inline">transform()</strong> method is used to get a new training set with the required number of features.</p><p>You should get the following output:</p><p class="source-code">ICA fitting time: 203.02 s</p><p>We can see that implementing ICA has taken much more time than PCA (179.54 seconds).</p></li>
				<li>We now transform the test set with the <strong class="source-inline">250</strong> components:<p class="source-code"># Transforming the test set </p><p class="source-code">X_test_ica=ICA.transform(X_test)</p></li>
				<li>Let's verify the shapes of the train and test sets before transformation and after transformation:<p class="source-code">"""</p><p class="source-code">Printing the shape of train and test sets </p><p class="source-code">before and after transformation</p><p class="source-code">"""</p><p class="source-code">print("original shape of Training set:   ", \</p><p class="source-code">      X_train.shape)</p><p class="source-code">print("original shape of Test set:   ", \</p><p class="source-code">      X_test.shape)</p><p class="source-code">print("Transformed shape of training set:", \</p><p class="source-code">      X_ica.shape)</p><p class="source-code">print("Transformed shape of test set:", \</p><p class="source-code">      X_test_ica.shape)</p><p>You should get the following output:</p><div id="_idContainer695" class="IMG---Figure"><img src="Images/B15019_14_29.jpg" alt="Figure 14.29: Shape of the original and transformed datasets&#13;&#10;" width="1657" height="233"/></div><p class="figure-caption">Figure 14.29: Shape of the original and transformed datasets</p><p>You can see that both the training and test sets are reduced to <strong class="source-inline">250</strong> features each.</p></li>
				<li>Let's now fit the logistic regression model on the transformed dataset and note the time it takes:<p class="source-code"># Fitting a Logistic Regression Model</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">import time</p><p class="source-code">icaModel = LogisticRegression()</p><p class="source-code">t0 = time.time()</p><p class="source-code">icaModel.fit(X_ica, y_train)</p><p class="source-code">t1 = time.time()</p></li>
				<li>Print the total time:<p class="source-code">print("Total training time:", round(t1-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">Total training time: 0.054 s</p></li>
				<li>Let's now predict on the test set and print the accuracy metrics:<p class="source-code"># Predicting with the ica model</p><p class="source-code">pred = icaModel.predict(X_test_ica)</p><p class="source-code">print('Accuracy of Logistic regression model '\</p><p class="source-code">      'prediction on test set: {:.2f}'\</p><p class="source-code">      .format(icaModel.score(X_test_ica, y_test)))</p><p>You should get the following output:</p><p class="source-code">Accuracy of Logistic regression model prediction on test set: 0.87</p><p>We can see that the ICA model has worse results than other models.</p></li>
				<li>Print the confusion matrix:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get the following output:</p><div id="_idContainer696" class="IMG---Figure"><img src="Images/B15019_14_30.jpg" alt="Figure 14.30: Resulting confusion matrix&#13;&#10;" width="1092" height="83"/></div><p class="figure-caption">Figure 14.30: Resulting confusion matrix</p><p>We can see that the ICA model has done a poor job in classifying the ads. All the examples have been wrongly classified as non-ads. We can conclude that ICA is not suitable for this dataset.</p></li>
				<li>Print the classification report:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code"># Getting the Classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get the following output:</p><div id="_idContainer697" class="IMG---Figure"><img src="Images/B15019_14_31.jpg" alt="Figure 14.31: Resulting classification report&#13;&#10;" width="1215" height="329"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.31: Resulting classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/325H88Q">https://packt.live/325H88Q</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>As we can see, transforming the data to its first 250 independent components did not capture all the necessary variability in the data. This has resulted in the degradation of the classification results for this method. We can conclude that ICA is not suitable for this dataset. </p>
			<p>It was also observed that the time taken to find the best independent features was longer than for PCA. However, it should be noted that different methods vary in results according to the input data. Even though ICA was not suitable for this dataset, it still is a potent method for dimensionality reduction that should be in the repertoire of a data scientist.</p>
			<p>From this exercise, you may come up with a few questions:</p>
			<ul>
				<li>How do you think we can improve the classification results using ICA?</li>
				<li>Increasing the number of components results in a marginal increase in the accuracy metrics.</li>
				<li>Are there any other side effects because of the strategy adopted to improve the results?</li>
			</ul>
			<p>Increasing the number of components also results in a longer training time for the logistic regression model.</p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor317"/>Factor Analysis</h2>
			<p>Factor analysis is a technique that achieves dimensionality reduction by grouping variables that are highly correlated. Let's look at an example from our context of predicting advertisements.</p>
			<p>In our dataset, there could be many features that describe the geometry (the size and shape of an image in the ad) of the images on a web page. These features can be correlated because they refer to specific characteristics of an image.</p>
			<p>Similarly, there could be many features that describe the anchor text or phrases occurring in a URL, which are highly correlated. Factor analysis looks at correlated groups such as these from the data and then groups them into latent factors. Therefore, if there are 10 raw features describing the geometry of an image, factor analysis will group them into one feature that characterizes the geometry of an image. Each of these groups is called factors. As many correlated features are combined to form a group, the resulting number of features will be much smaller in comparison with the original dimensions of the dataset. </p>
			<p>Let's now see how factor analysis can be implemented as a technique for dimensionality reduction.</p>
			<h2 id="_idParaDest-319"><a id="_idTextAnchor318"/>Exercise 14.06: Dimensionality Reduction Using Factor Analysis</h2>
			<p>In this exercise, we will fit a logistic regression model after reducing the original dimensions to some key factors and then observe the performance of the model.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Implement the same initial steps from <em class="italic">Exercise 14.01</em>, <em class="italic">Loading and Cleaning the Dataset</em>, up until scaling the dataset using the <strong class="source-inline">minmaxscaler()</strong> function:<p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter14/Dataset/ad.data'</p><p class="source-code">import pandas as pd</p><p class="source-code">adData = pd.read_csv(filename,sep=",",header = None,\</p><p class="source-code">                     error_bad_lines=False)</p><p class="source-code">X = adData.loc[:,0:1557]</p><p class="source-code">Y = adData[1558]</p><p class="source-code">import numpy as np</p><p class="source-code">for i in range(0,3):</p><p class="source-code">    X[i] = X[i].str.replace("?", 'NaN')\</p><p class="source-code">               .values.astype(float)</p><p class="source-code">for i in range(3,1557):</p><p class="source-code">    X[i] = X[i].replace("?", 'NaN')\</p><p class="source-code">               .values.astype(float)  </p><p class="source-code">for i in range(0,1557):</p><p class="source-code">    X[i] = X[i].fillna(X[i].mean())</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Let's now augment the dataset artificially to a factor of <strong class="source-inline">50</strong>. Augmenting the dataset to factors that are higher than <strong class="source-inline">50</strong> will result in the notebook crashing because of a lack of memory. This is implemented using the following code snippet:<p class="source-code"># Creating a high dimension data set</p><p class="source-code">X_hd = pd.DataFrame(pd.np.tile(X_tran, (1, 50)))</p><p class="source-code">print(X_hd.shape)</p><p>You should get the following output:</p><p class="source-code">(3279, 77900)</p></li>
				<li>Let's split the high-dimensional dataset into train and test sets:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_hd, Y, test_size=0.3,\</p><p class="source-code">                                    random_state=123)</p></li>
				<li>An important step in factor analysis is defining the number of factors in a dataset. This step is achieved through experimentation. In our case, we will arbitrarily assume that there are <strong class="source-inline">20</strong> factors. This is implemented as follows:<p class="source-code"># Defining the number of factors</p><p class="source-code">from sklearn.decomposition import FactorAnalysis</p><p class="source-code">fa = FactorAnalysis(n_components = 20,\</p><p class="source-code">                    random_state=123)</p><p>The number of factors is defined through the <strong class="source-inline">n_components</strong> argument. We also define a random state for reproducibility.</p></li>
				<li>Once the factor method is defined, we will fit the method on the training set and also transform the training set to get a new training set with the required number of factors. We will also note the time it takes to fit the required number of factors:<p class="source-code">"""</p><p class="source-code">Fitting the Factor analysis method and </p><p class="source-code">transforming the training set</p><p class="source-code">"""</p><p class="source-code">import time</p><p class="source-code">t0 = time.time()</p><p class="source-code">X_fac=fa.fit_transform(X_train)</p><p class="source-code">t1 = time.time()</p><p class="source-code">print("Factor analysis fitting time:", \</p><p class="source-code">      round(t1-t0, 3), "s")</p><p>In the code, the <strong class="source-inline">.fit()</strong> function is used to fit on the training set, and the <strong class="source-inline">transform()</strong> method is used to get a new training set with the required number of factors.</p><p>You should get the following output:</p><p class="source-code">Factor analysis fitting time: 130.688 s</p><p>Factor analysis is also a compute-intensive method. This is the reason that only 20 factors were selected. We can see that it has taken <strong class="source-inline">130.688</strong> seconds for <strong class="source-inline">20</strong> factors.</p></li>
				<li>We now transform the test set with the same number of factors:<p class="source-code"># Transforming the test set </p><p class="source-code">X_test_fac=fa.transform(X_test)</p></li>
				<li>Let's verify the shapes of the train and test sets before transformation and after transformation:<p class="source-code">"""</p><p class="source-code">Printing the shape of train and test sets </p><p class="source-code">before and after transformation</p><p class="source-code">"""</p><p class="source-code">print("original shape of Training set:   ", \</p><p class="source-code">      X_train.shape)</p><p class="source-code">print("original shape of Test set:   ", \</p><p class="source-code">      X_test.shape)</p><p class="source-code">print("Transformed shape of training set:", \</p><p class="source-code">      X_fac.shape)</p><p class="source-code">print("Transformed shape of test set:", \</p><p class="source-code">      X_test_fac.shape)</p><p>You should get the following output:</p><div id="_idContainer698" class="IMG---Figure"><img src="Images/B15019_14_32.jpg" alt="Figure 14.32: Original and transformed dataset values&#13;&#10;" width="1214" height="159"/></div><p class="figure-caption">Figure 14.32: Original and transformed dataset values</p><p>You can see that both the training and test sets have been reduced to <strong class="source-inline">20</strong> factors each.</p></li>
				<li>Let's now fit the logistic regression model on the transformed dataset and note the time it takes to fit the model:<p class="source-code"># Fitting a Logistic Regression Model</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">import time</p><p class="source-code">facModel = LogisticRegression()</p><p class="source-code">t0 = time.time()</p><p class="source-code">facModel.fit(X_fac, y_train)</p><p class="source-code">t1 = time.time()</p></li>
				<li>Print the total time:<p class="source-code">print("Total training time:", round(t1-t0, 3), "s")</p><p>You should get the following output:</p><p class="source-code">Total training time: 0.028 s</p><p>We can see that the time it has taken to fit the logistic regression model is comparable with other methods.</p></li>
				<li>Let's now predict on the test set and print the accuracy metrics:<p class="source-code"># Predicting with the factor analysis model</p><p class="source-code">pred = facModel.predict(X_test_fac)</p><p class="source-code">print('Accuracy of Logistic regression '\</p><p class="source-code">      'model prediction on test set: {:.2f}'</p><p class="source-code">      .format(facModel.score(X_test_fac, y_test)))</p><p>You should get the following output:</p><p class="source-code">Accuracy of Logistic regression model prediction on test set: 0.92</p><p>We can see that the factor model has better results than the ICA model, but worse results than the other models.</p></li>
				<li>Print the confusion matrix:<p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get the following output:</p><div id="_idContainer699" class="IMG---Figure"><img src="Images/B15019_14_33.jpg" alt="Figure 14.33: Resulting confusion matrix&#13;&#10;" width="1637" height="111"/></div><p class="figure-caption">Figure 14.33: Resulting confusion matrix</p><p>We can see that the factor model has done a better job at classifying the ads than the ICA model. However, there is still a high number of false positives.</p></li>
				<li>Print the classification report:<p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code"># Getting the Classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get the following output:</p><div id="_idContainer700" class="IMG---Figure"><img src="Images/B15019_14_34.jpg" alt="Figure 14.34: Resulting classification report&#13;&#10;" width="1121" height="317"/></div></li>
			</ol>
			<p class="figure-caption">Figure 14.34: Resulting classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/32b9SNk">https://packt.live/32b9SNk</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, but can be run as usual on Google Colab.</p>
			<p>As we can see in the results, by reducing the variables to just 20 factors, we were able to get a decent classification result. Even though there is degradation on the result, we still have a manageable number of features, which will be able to scale well on any algorithm. The balance between the accuracy measures and the ability to manage features needs to be explored through greater experimentation.</p>
			<p>How do you think we can improve the classification results for factor analysis?</p>
			<p>Well, increasing the number of components results in an increase in the accuracy metrics.</p>
			<h1 id="_idParaDest-320"><a id="_idTextAnchor319"/>Comparing Different Dimensionality Reduction Techniques</h1>
			<p>Now that we have learned different dimensionality reduction techniques, let's apply all of these techniques to a new dataset that we will create from the existing ads dataset. </p>
			<p>We will randomly sample some data points from a known distribution and then add these random samples to the existing dataset to create a new dataset. Let's carry out an experiment to see how a new dataset can be created from an existing dataset.</p>
			<p>We import the necessary libraries:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p>Next, we create a dummy data frame.</p>
			<p>We will use a small dataset with two rows and three columns for this example. We use the <strong class="source-inline">pd.np.array()</strong> function to create a data frame:</p>
			<p class="source-code"># Creating a simple data frame</p>
			<p class="source-code">df = pd.np.array([[1, 2, 3], [4, 5, 6]])</p>
			<p class="source-code">print(df.shape)</p>
			<p class="source-code">df</p>
			<p>You should get the following output:</p>
			<div>
				<div id="_idContainer701" class="IMG---Figure">
					<img src="Images/B15019_14_35.jpg" alt="Figure 14.35: Sample data frame&#13;&#10;" width="1665" height="182"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.35: Sample data frame</p>
			<p>What we will do next is sample some data points with the same shape as the data frame we created. </p>
			<p>Let's sample some data points from a normal distribution that has mean <strong class="source-inline">0</strong> and standard deviation of <strong class="source-inline">0.1</strong>. We touched briefly on normal distributions in <em class="italic">Chapter 3, Binary Classification.</em> A normal distribution has two parameters. The first one is the mean, which is the average of all the data in the distribution, and the second one is standard deviation, which is a measure of how spread out the data points are.</p>
			<p>By assuming a mean and standard deviation, we will be able to draw samples from a normal distribution using the <strong class="source-inline">np.random.normal()</strong> Python function. The arguments that we have to give for this function are the mean, the standard deviation, and the shape of the new dataset. </p>
			<p>Let's see how this is implemented in code:</p>
			<p class="source-code"># Defining the mean and standard deviation</p>
			<p class="source-code">mu, sigma = 0, 0.1 </p>
			<p class="source-code"># Generating random sample</p>
			<p class="source-code">noise = np.random.normal(mu, sigma, [2,3]) </p>
			<p class="source-code">noise.shape</p>
			<p>You should get the following output:</p>
			<p class="source-code">(2, 3)</p>
			<p>As we can see, we give the mean (<strong class="source-inline">mu</strong>), standard deviation (<strong class="source-inline">sigma</strong>), and the shape of the data frame <strong class="source-inline">[2,3]</strong> to generate the new random samples.</p>
			<p>Print the sampled data frame:</p>
			<p class="source-code"># Sampled data frame</p>
			<p class="source-code">noise</p>
			<p>You will get something like the following output:</p>
			<p class="source-code">array([[-0.07175021, -0.21135372,  0.10258917],</p>
			<p class="source-code">       [ 0.03737542,  0.00045449, -0.04866098]])</p>
			<p>The next step is to add the original data frame and the sampled data frame to get the new dataset:</p>
			<p class="source-code"># Creating a new data set by adding sampled data frame</p>
			<p class="source-code">df_new = df + noise</p>
			<p class="source-code">df_new</p>
			<p>You should get something like the following output:</p>
			<p class="source-code">array([[0.92824979, 1.78864628, 3.10258917],</p>
			<p class="source-code">       [4.03737542, 5.00045449, 5.95133902]])</p>
			<p>Having seen how to create a new dataset, let's use this knowledge in the next activity. </p>
			<h2 id="_idParaDest-321"><a id="_idTextAnchor320"/>Activity 14.02: Comparison of Dimensionality Reduction Techniques on the Enhanced Ads Dataset</h2>
			<p>You have learned different dimensionality reduction techniques. You want to determine which is the best technique among them for a dataset you will create.</p>
			<p><strong class="bold">Hint</strong>: In this activity, we will use the different techniques that you have used in all the exercises so far. You will also create a new dataset as we did in the previous section.</p>
			<p>The steps to complete this activity are as follows:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Normalize the original ads data and derive the transformed independent variable, <strong class="source-inline">X_tran</strong>.</li>
				<li>Create a high-dimensional dataset by replicating the columns twice using the <strong class="source-inline">pd.np.tile()</strong> function.</li>
				<li>Create random samples from a normal distribution with mean = 0 and standard deviation = 0.1. Make the new dataset with the same shape as the high-dimensional dataset created in <em class="italic">step 3</em>. </li>
				<li>Add the high dimensional dataset and the random samples to get the new dataset.</li>
				<li>Split the dataset into train and test sets.</li>
				<li>Implement backward elimination with the following steps:<p>Implement the backward elimination step using the <strong class="source-inline">RFE()</strong> function.</p><p>Use logistic regression as the model and select the best <strong class="source-inline">300</strong> features.</p><p>Fit the <strong class="source-inline">RFE()</strong> function on the training set and measure the time it takes to fit the RFE model on the training set.</p><p>Transform the train and test sets with the RFE model.</p><p>Fit a logistic regression model on the transformed training set.</p><p>Predict on the test set and print the accuracy score, confusion matrix, and classification report.</p></li>
				<li>Implement the forward selection technique with the following steps:<p>Define the number of features using the <strong class="source-inline">SelectKBest()</strong> function. Select the best <strong class="source-inline">300</strong> features.</p><p>Fit the forward selection on the training set using the <strong class="source-inline">.fit()</strong> function and note the time taken for the fit.</p><p>Transform both the training and test sets using the <strong class="source-inline">.transform()</strong> function.</p><p>Fit a logistic regression model on the transformed training set.</p><p>Predict on the transformed test set and print the accuracy, confusion matrix, and classification report.</p></li>
				<li>Implement PCA:<p>Define the principal components using the <strong class="source-inline">PCA()</strong> function. Use 300 components.</p><p>Fit <strong class="source-inline">PCA()</strong> on the training set. Note the time.</p><p>Transform both the training set and test set to get the respective number of components for these datasets using the <strong class="source-inline">.transform()</strong> function.</p><p>Fit a logistic regression model on the transformed training set.</p><p>Predict on the transformed test set and print the accuracy, confusion matrix, and classification report.</p></li>
				<li>Implement ICA:<p>Define independent components using the <strong class="source-inline">FastICA()</strong> function using <strong class="source-inline">300</strong> components.</p><p>Fit the independent components on the training set and transform the training set. Note the time for the implementation.</p><p>Transform the test set to get the respective number of components for these datasets using the <strong class="source-inline">.transform()</strong> function.</p><p>Fit a logistic regression model on the transformed training set.</p><p>Predict on the transformed test set and print the accuracy, confusion matrix, and classification report.</p></li>
				<li>Implement factor analysis:<p>Define the number of factors using the <strong class="source-inline">FactorAnalysis()</strong> function and <strong class="source-inline">30</strong> factors.</p><p>Fit the factors on the training set and transform the training set. Note the time for the implementation.</p><p>Transform the test set to get the respective number of components for these datasets using the <strong class="source-inline">.transform()</strong> function.</p><p>Fit a logistic regression model on the transformed training set.</p><p>Predict on the transformed test set and print the accuracy, confusion matrix, and classification report.</p></li>
				<li>Compare the outputs of all the methods.</li>
			</ol>
			<p><strong class="bold">Expected Output</strong>:</p>
			<p>An example summary table of the results is as follows:</p>
			<div>
				<div id="_idContainer702" class="IMG---Figure">
					<img src="Images/B15019_14_36.jpg" alt="Figure 14.36: Summary output of all the reduction techniques&#13;&#10;" width="1253" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.36: Summary output of all the reduction techniques</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to the activity can be found here: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>In this activity, we implemented five different methods of dimensionality reduction with a new dataset that we created from the internet ads dataset. </p>
			<p>From the tabulated results, we can see that three methods (backward elimination, forward selection, and PCA) have got the same accuracy scores. Therefore, the selection criteria for the best method should be based on the time taken to get the reduced dimension. With these criteria, the forward selection method is the best method, followed by PCA. </p>
			<p>For the third place, we should strike a balance between accuracy and the time taken for dimensionality reduction. We can see that factor analysis and backward elimination have very close accuracy scores, 96% and 97% respectively. However, the time taken for backward elimination is quite large compared to factor analysis. Therefore, we should weigh our considerations toward factor analysis as the third best, even though the accuracy is marginally lower than backward elimination. The last spot should go to ICA because the accuracy is far lower than all the other methods.</p>
			<h1 id="_idParaDest-322"><a id="_idTextAnchor321"/>Summary</h1>
			<p>In this chapter, we have learned about various techniques for dimensionality reduction. Let's summarize what we have learned in this chapter.</p>
			<p>At the beginning of the chapter, we were introduced to the challenges inherent with some of the modern-day datasets in terms of scalability. To further learn about these challenges, we downloaded the Internet Advertisement dataset and did an activity where we witnessed the scalability challenges posed by a large dataset. In the activity, we artificially created a large dataset and fit a logistic regression model to it.</p>
			<p>In the subsequent sections, we were introduced to five different methods of dimensionality reduction.</p>
			<p><strong class="bold">Backward feature elimination</strong> worked on the principle of eliminating features one by one until no major degradation of accuracy measures occurred. This method is computationally intensive, but we got better results than the benchmark model.</p>
			<p><strong class="bold">Forward feature selection</strong> goes in the opposite direction as backward elimination and selects one feature at a time to get the best set of features we predetermined. This method is also computationally intensive. We also found out that this method had marginally lower accuracy.</p>
			<p><strong class="bold">Principal component analysis </strong>(<strong class="bold">PCA</strong>) aims at finding components that are orthogonal to each other and that best explain the variability of the data. We had better results with PCA than we got from the benchmark model.</p>
			<p><strong class="bold">Independent component analysis </strong>(<strong class="bold">ICA</strong>) is similar to PCA; however, it differs in terms of the approach to the selection of components. ICA looks for independent components from the dataset. We saw that ICA achieved one of the worst results in our context.</p>
			<p><strong class="bold">Factor analysis</strong> was all about finding factors or groups of correlated features that best described the data. We achieved much better results than ICA with factor analysis.</p>
			<p>The aim of this chapter was to equip you with a set of techniques that help in scenarios when the scalability of models was challenging. The key to getting good results is to understand which method to use in which scenario. This could be achieved with lots of hands-on practice and experimentation with many large datasets. </p>
			<p>Having learned a set of tools to manage scalability in this chapter, we will move on to the next chapter, which addresses the problem of boosting performance. In the next chapter, you will be introduced to a technique called ensemble learning. This technique will help to boost the performance of your machine learning models.</p>
		</div>
		<div>
			<div id="_idContainer704" class="Content">
			</div>
		</div>
	</div></body></html>