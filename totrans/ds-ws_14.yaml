- en: 14\. Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces dimensionality reduction in data science. You will be
    using the Internet Advertisements dataset to analyze and evaluate different techniques
    in dimensionality reduction. By the end of this chapter, you will be able to analyze
    datasets with high dimensions and deal with the challenges posed by these datasets.
    As well as applying different dimensionality reduction techniques to large datasets,
    you will fit models based on those datasets and analyze their results. By the
    end of this chapter, you will be able to deal with huge datasets in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter on balancing datasets, we dealt with the Bank Marketing
    dataset, which had 18 variables. We were able to load that dataset very easily,
    fit a model, and get results. But have you considered the scenario when the number
    of variables you have to deal with is large, say around 18 million instead of
    the 18 you dealt with in the last chapter? How do you load such large datasets
    and analyze them? How do you deal with the computing resources required for modeling
    with such large datasets?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the reality in some modern-day datasets in domains such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare, where genetics datasets can have millions of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-resolution imaging datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web data related to advertisements, ranking, and crawling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When dealing with such huge datasets, many challenges can arise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Storage and computation challenges: Large datasets with high dimensions require
    a lot of storage and expensive computational resources for analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploration challenges: When trying to explore data and derive insights, high-dimensional
    data can be really cumbersome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Algorithm challenges: Many algorithms do not scale well in high-dimensional settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, what is the solution when we have to deal with high-dimensional data? This
    is where dimensionality reduction techniques come to the fore, which we will explore
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction aims to reduce the dimensions of datasets to get over
    the challenges posed by high-dimensional data. In this chapter, we will examine
    some of the popular dimensionality reduction techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Backward feature elimination or recursive feature elimination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent Component Analysis** (**ICA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's first examine our business context and then apply these techniques to
    the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Business Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The marketing head of your company comes to you with a problem she has been
    grappling with. Many customers have been complaining about the browsing experience
    of your company's website because of the number of advertisements that pop up
    during browsing. Your company wants to build an engine on your web server that
    identifies potential advertisements and then eliminates them even before they
    pop up.
  prefs: []
  type: TYPE_NORMAL
- en: To help you to achieve this, you have been given a dataset that contains a set
    of possible advertisements on a variety of web pages. The features of the dataset
    represent the geometry of the images in the possible adverts, as well as phrases
    occurring in the URL, image URLs, anchor text, and words occurring near the anchor
    text. This dataset has also been labeled, with each possible ad given a label
    that says whether it is actually an advertisement or not. Using this dataset,
    you have to build a model that predicts whether something is an advertisement
    or not. You may think that this is a relatively simple problem that could be solved
    with any binary classification algorithm. However, there is a challenge in the
    dataset. The dataset has a large number of features. You have set out to solve
    this high-dimensional dataset challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is uploaded in the GitHub repository for working through all the
    subsequent exercises. The attributes of the dataset are available in the following
    link: [https://packt.live/36rqiCg](https://packt.live/36rqiCg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.01: Loading and Cleaning the Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will download the dataset, load it in our Colab notebook,
    and do some basic explorations, such as printing the dimensions of the dataset
    using the `.shape()` and `.describe()` functions, and also cleaning the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `internet_ads` dataset has been uploaded to our GitHub repository and can
    be accessed at [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, `import pandas` into your Colab notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, set the path of the drive where the `ad.Data` file is uploaded, as shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the file using the `pd.read_csv()` function from the pandas data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `pd.read_csv()` function's arguments are the filename as a string and the
    limit separator of a CSV file, which is `","`. Please note that as there are no
    headers for the dataset. We specifically mention this using the `header = None`
    command. The last argument, `error_bad_lines=False`, is to skip any errors in
    the format of the file and then load data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After reading the file, the data frame is printed using the `.head()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.1: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.1: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, print the shape of the dataset, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the shape, we can see that we have a large number of features, `1559`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the summary of the numerical features of the raw data using the `.describe()`
    function in pandas, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.2: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.2: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we saw from the shape of the data, the dataset has `3279` examples with `1559`
    variables. The variable set has both categorical and numerical variables. The
    summary statistics are only derived for numerical data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the dependent and independent variables from our dataset, as shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As seen earlier, there are `1559` features in the dataset. The first `1558`
    features are independent variables. They are separated from the initial `adData`
    data frame using the `.loc()` function and give the indexes of the corresponding
    features (`0` to `1557`). The independent variables are loaded into a new variable
    called `X`. The dependent variable, which is the label of the dataset, is loaded
    in variable `Y`. The shapes of the dependent and independent variables are also printed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the first `15` examples of the independent variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can print as many rows of the data by defining the number within the `head()`
    function. Here, we have printed out the first `15` rows of the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.3: First 15 examples of independent variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.3: First 15 examples of independent variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the output, we can see that there are many missing values in the dataset,
    which are represented by `?`. For further analysis, we have to remove these special
    characters and then replace those cells with assumed values. One popular method
    of replacing special characters is to impute the mean of the respective feature.
    Let's adopt this strategy. However, before doing that, let's look at the data
    types for this dataset to adopt a suitable replacement strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the data types of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.4: The data types in our dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.4: The data types in our dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the output, we can see that the first four columns are of the object type,
    which refers to string data, and the others are integer data. When replacing the
    special characters in the data, we need to be cognizant of the data types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Replace special characters with `NaN` values for the first four columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the special characters in the first four columns, which are of the object
    type, with `NaN` values. `NaN` is an abbreviation for "not a number." Replacing
    special characters with `NaN` values makes it easy to further impute data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is achieved through the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To replace the first three columns, we loop through the columns using the `for()`
    loop and also using the `range()` function. Since the first three columns are
    of the `object` or `string` type, we use the `.str.replace()` function, which
    stands for "string replace". After replacing the special characters, `?`, of the
    data with `nan`, we convert the data type to `float` with the `.values.astype(float)`
    function, which is required for further processing. By printing the first 15 examples,
    we can see that all special characters have been replaced with `nan` or `NaN`
    values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.5: After replacing special characters with NaN'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.5: After replacing special characters with NaN'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, replace special characters for the integer features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As in *Step 9*, let''s also replace the special characters from the features
    of the `int64` data type with the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the integer features, we do not have `.str` before the `.replace()` function,
    as these features are integer values and not string values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, impute the mean of each column for the `NaN` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have replaced special characters in the data with `NaN` values,
    we can use the `fillna()` function in pandas to replace the `NaN` values with
    the mean of the column. This is executed using the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, the `.mean()` function calculates the mean of
    each column and then replaces the `nan` values with the mean of the column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.6: Mean of the NaN columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.6: Mean of the NaN columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scale the dataset using the `minmaxScaler()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As in *Chapter 3*, *Binary Classification*, scaling data is useful in the modeling
    step. Let's scale the dataset using the `minmaxScaler()` function as learned in
    *Chapter 3*, *Binary Classification*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is shown in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output. Here, we have displayed the first 24 columns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.7: Scaling the dataset using the MinMaxScaler() function'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.7: Scaling the dataset using the MinMaxScaler() function'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yi7Nym](https://packt.live/2Yi7Nym).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Q6l9ZZ](https://packt.live/2Q6l9ZZ).
  prefs: []
  type: TYPE_NORMAL
- en: You have come to the end of the first exercise. In this exercise, we loaded
    the dataset, extracted summary statistics, cleaned the data, and also scaled the
    data. You can see that in the final output, all the raw values have been transformed
    into scaled values.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's try to augment this dataset with many more features
    so that this becomes a massive dataset and then fit a simple logistic regression
    model on this dataset as a benchmark model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first see how data can be augmented with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a High-Dimensional Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the earlier section, we worked with a dataset that has around `1,558` features.
    In order to demonstrate the challenges with high-dimensional datasets, let's create
    an extremely high dimensional dataset from the internet dataset that we already
    have.
  prefs: []
  type: TYPE_NORMAL
- en: This we will achieve by replicating the existing number of features multiple
    times so that the dataset becomes really large. To replicate the dataset, we will
    use a function called `np.tile()`, which copies a data frame multiple times across
    the axes we want. We will also calculate the time it takes for any activity using
    the `time()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at both these functions in action with a toy example.
  prefs: []
  type: TYPE_NORMAL
- en: 'You begin by importing the necessary library functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to create a dummy data frame, we will use a small dataset with two rows
    and three columns for this example. We use the `pd.np.array()` function to create
    a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8: Array for the sample dummy data frame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.8: Array for the sample dummy data frame'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you replicate the dummy data frame and this replication of the columns
    is done using the `pd.np.tile()` function in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9: Replication of the data frame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.9: Replication of the data frame'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the snippet, the `pd.np.tile()` function accepts two sets of
    arguments. The first one is the data frame, `df`, that we want to replicate. The
    next argument, `(1,5)`, defines which axes we want to replicate. In this example,
    we define that the rows will remain as is because of the `1` argument, and the
    columns will be replicated `5` times with the `5` argument. We can see from the
    `shape()` function that the original data frame, which was of shape `(2,3)`, has
    been transformed into a data frame with a shape of `(2,15)`.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the total time is done using the `time` library. To start the timing,
    we invoke the `time.time()` function. In the example, we store the initial time
    in a variable called `t0` and then subtract this from the end time to find the
    total time it takes for the process. Thus we have augmented and added more data
    frames to our exiting internet ads dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 14.01: Fitting a Logistic Regression Model on a HighDimensional Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to test the performance of your models when the dataset is large. To
    do this, you are artificially augmenting the internet ads dataset so that the
    dataset is 300 times bigger in dimension than the original dataset. You will be
    fitting a logistic regression model on this new dataset and then observe the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint**: In this activity, we will use a notebook similar to *Exercise 14.01*,
    *Loading and Cleaning the Dataset*, and we will also be fitting a logistic regression
    model as done in *Chapter 3*, *Binary Classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the same ads dataset for this activity.
  prefs: []
  type: TYPE_NORMAL
- en: The `internet_ads` dataset has been uploaded to our GitHub repository and can
    be accessed at [https://packt.live/2sPaVF6](https://packt.live/2sPaVF6).
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to complete this activity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement all steps from *Exercise 14.01*, *Loading and Cleaning the Dataset*,
    until the normalization of data. Derive the transformed independent `X_tran` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a high-dimensional dataset by replicating the columns 300 times using
    the `pd.np.tile()` function. Print the shape of the new dataset and observe the
    number of features in the new dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the new dataset and note the time it takes
    to fit the model. Note the color change for the indicator for RAM on your Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected Output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get output similar to the following after fitting the logistic regression
    model on the new dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 14.10: Google Colab RAM utilization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.10: Google Colab RAM utilization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the test set and print the classification report and confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.11: Confusion matrix and the classification report results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.11: Confusion matrix and the classification report results'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you will have created a high-dimensional dataset by replicating
    the columns of the existing database and identified that the resource utilization
    is quite high with this high dimensional dataset. The resource utilization indicator
    changed its color to orange because of the large dimensions. The longer time,
    `23.86` seconds, taken for modeling was also noticed on this dataset. You will
    have also predicted on the test set to get an accuracy level of around `97%` using
    the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: First, you need to know why the color of RAM utilization on Colab changed to
    orange. Because of the huge dataset we created by replication, Colab had to use
    access RAM, due to which the color changed to orange.
  prefs: []
  type: TYPE_NORMAL
- en: But, out of curiosity, what do you think the impact will be on the RAM utilization
    if you increased the replication from 300 to 500? Let's have a look at the following
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to perform this on your Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by defining the path of the dataset for the GitHub repository to our
    "ads" dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we simply load the data using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a high-dimensional dataset with a scaling factor of `500`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.12: Colab crashing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.12: Colab crashing'
  prefs: []
  type: TYPE_NORMAL
- en: From the output, you can see that the session crashes because all the RAM provided
    by Colab has been used. The session will restart, and you will lose all your variables.
    Hence, it is always good to be mindful of the resources you are provided with,
    along with the dataset. As a data scientist, if you feel that a dataset is huge
    with many features but the resources to process that dataset are limited, you
    need to get in touch with the organization and get the required resources or build
    an appropriate strategy to address these high-dimensional datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for Addressing High-Dimensional Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Activity 14.01*, *Fitting a Logistic Regression Model on a High-Dimensional
    Dataset*, we witnessed the challenges of high-dimensional datasets. We saw how
    the resources were challenged when the replication factor was 300\. You also saw
    that the notebook crashes when the replication factor is increased to 500\. When
    the replication factor was 500, the number of features was around 750,000\. In
    our case, our resources would fail to scale up even before we hit the 1 million
    mark on the number of features. Some modern-day datasets sometimes have hundreds
    of millions, or in many cases billions, of features. Imagine the kind of resources
    and time it would take to get any actionable insights from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, we have many robust methods for addressing high-dimensional datasets.
    Many of these techniques are very effective and have helped to address the challenges
    raised by huge datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some of the techniques for dealing with high-dimensional datasets.
    In *Figure 14.14*, you can see the strategies we will be coming across in this
    chapter to deal with such datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13: Strategies to address high dimensional datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.13: Strategies to address high dimensional datasets'
  prefs: []
  type: TYPE_NORMAL
- en: Backward Feature Elimination (Recursive Feature Elimination)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mechanism behind the backward feature elimination algorithm is the recursive
    elimination of features and building a model on those features that remain after
    all the elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look under the hood of this algorithm step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, at a given iteration, the selected classification algorithm is first
    trained on all the `n` features available. For example, let's take the case of
    the original dataset we had, which had `1,558` features. The algorithm starts
    off with all the `1,558` features in the first iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, we remove one feature at a time and train a model with the
    remaining `n-1` features. This process is repeated `n` times. For example, we
    first remove feature 1 and then fit a model using all the remaining 1,557 variables.
    In the next iteration, we use feature `1` and instead, we eliminate feature `2`
    and then fit the model. This process is repeated `n` times (`1,558`) times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of the models fitted, the performance of the model (using measures
    such as accuracy) is calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The feature whose replacement has resulted in the smallest change in performance
    is removed permanently and *Step 2* is repeated with `n-1` features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process is then repeated with `n-2` features and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm keeps on eliminating features until the threshold number of features
    we require is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's take a look at the backward feature elimination algorithm in action for
    the augmented ads dataset in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.02: Dimensionality Reduction Using Backward Feature Elimination'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will fit a logistic regression model after eliminating
    features using the backward elimination technique to find the accuracy of the
    model. We will be using the same ads dataset as before, and we will be enhancing
    it with additional features for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement all the initial steps similar to *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, until scaling the dataset using the `minmaxscaler()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a high-dimensional dataset. We''ll augment the dataset artificially
    by a factor of `2`. The process of backward feature elimination is a very compute-intensive
    process, and using higher dimensions will involve a longer processing time. This
    is why the augmenting factor has been kept at `2`. This is implemented using the
    following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the backward elimination model. Backward elimination works by providing
    two arguments to the `RFE()` function, which is the model we want to try (logistic
    regression in our case) and the number of features we want the dataset to be reduced
    to. This is implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this implementation, the number of features that we have given, `250`, is
    identified through trial and error. The process is to first assume an arbitrary
    number of features and then, based on the final metrics, arrive at the most optimum
    number of features for the model. In this implementation, our first assumption
    of `250` implies that we want the backward elimination model to start eliminating
    features until we get the best `250` features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit the backward elimination method to identify the best `250` features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are now ready to fit the backward elimination method on the higher-dimensional
    dataset. We will also note the time it takes for backward elimination to work.
    This is implemented using the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fitting the backward elimination method is done using the `.fit()` function.
    We give the independent and dependent training sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The backward elimination method is a compute-intensive process, and therefore
    this process will take a lot of time to execute. The larger the number of features,
    the longer it will take.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The time for backward elimination is at the end of the notifications:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.14: The time taken for the backward elimination process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.14: The time taken for the backward elimination process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that the backward elimination process to find the best `250` features
    has taken `230.35` seconds to implement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the features identified using the backward elimination method. We can
    display the `250` features that were identified using the backward elimination
    process using the `get_support()` function. This is implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.15: The identified features being displayed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.15: The identified features being displayed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These are the best `250` features that were finally selected using the backward
    elimination process from the entire dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, split the dataset into training and testing sets for modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the output, you see the shapes of both the training set and testing sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transform the train and test sets. In *step 5*, we identified the top `250`
    features through backward elimination. Now we need to reduce the train and test
    sets to those top `250` features. This is done using the `.transform()` function.
    This is implemented using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that both the training set and test sets have been reduced to the
    `250` best features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit a logistic regression model on the training set and note the time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As expected, the total time it takes to fit a model on a reduced set of features
    is much lower than the time it took for the larger dataset in *Activity 14.01*,
    *Fitting a Logistic Regression Model on a HighDimensional Dataset*, which was
    `23.86` seconds. This is a great improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, predict on the test set and print the accuracy metrics, as shown in the
    following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.16: The achieved accuracy of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.16: The achieved accuracy of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that the accuracy measure for this model has improved compared to
    the one we got for the model with higher dimensionality, which was `0.97` in *Activity
    14.01*, *Fitting a Logistic Regression Model on a HighDimensional Dataset*. This
    increase could be attributed to the identification of non-correlated features
    from the complete feature set, which could have boosted the performance of the
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.17: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.17: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Printing the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.18: Classification matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.18: Classification matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31ca5k6](https://packt.live/31ca5k6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/329yJkF](https://packt.live/329yJkF).
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the backward elimination process, we were able to get an
    improved accuracy of `98%` with `250` features, compared to the model in *Activity
    14.01*, *Fitting a Logistic Regression Model on a HighDimensional Dataset,* where
    an artificially enhanced dataset was used and got an accuracy of `97%`.
  prefs: []
  type: TYPE_NORMAL
- en: However, it should be noted that dimensionality reduction techniques should
    not be viewed as a method to improve the performance of any model. Dimensionality
    reduction techniques have to be viewed from the perspective of enabling us to
    fit a model on datasets with large numbers of features. When dimensions increase,
    fitting the model becomes intractable. This can be observed if the scaling factor
    used in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset,* was to be increased from `300` to `500`. In such cases, fitting a model
    wouldn't happen with the current set of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction aids in such scenarios by reducing the number of features,
    thereby enabling the fitting of a model on reduced dimensions without a large
    degradation of performance, and can sometimes lead to an improvement in results.
    However, it should also be noted that methods such as backward elimination are
    compute-intensive processes. You would have observed this phenomenon as to the
    time it takes in identifying the top 250 features when the scaling factor was
    just 2\. With much higher scaling factors, it will take far more time and resources
    to identify the top 250 features.
  prefs: []
  type: TYPE_NORMAL
- en: Having seen the backward elimination method, let's now look at the next technique,
    which is forward feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Feature Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Forward feature selection works in the reverse order as backward elimination.
    In this process, we start off with an initial feature, and features are added
    one by one until no improvement in performance is achieved. The detailed process
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start model building with one feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate the model building process *n* times, each time selecting one feature
    at a time. The feature that gives the highest improvement in performance is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the first feature is selected, it is the time to select the second feature.
    The process for selecting the second feature proceeds exactly the same as *step
    2*. The remaining *n-1* features are iterated along with the first feature and
    the performance on the model is observed. The feature that produces the biggest
    improvement in model performance is selected as the second feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The iteration of features will continue until a threshold number of features
    we have determined is extracted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The set of final features selected will be the ones that give the maximum model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now implement this algorithm in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.03: Dimensionality Reduction Using Forward Feature Selection'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will fit a logistic regression model by selecting the optimum
    features through forward feature selection and observing the performance of the
    model. We will be using the same ads dataset as before, and we will be enhancing
    it with additional features for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement all the initial steps similar to *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, up until scaling the dataset using `MinMaxScaler()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a high-dimensional dataset. Now, augment the dataset artificially to
    a factor of `50`. Augmenting the dataset to higher factors will result in the
    notebook crashing because of lack of memory. This is implemented using the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the high dimensional dataset into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we define the threshold features. Once the train and test sets are created,
    the next step is to import the feature selection function, `SelectKBest`. The
    argument we give to this function is the number of features we want. The features
    are selected through experimentation and, as a first step, we assume a threshold
    value. In this example, we assume a threshold value of `250`. This is implemented
    using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate and get the best set of threshold features. Based on the threshold
    set of features we defined, we have to fit the training set and get the best set
    of threshold features. Fitting on the training set is done using the `.fit()`
    function. We also note the time it takes to find the best set of features. This
    is executed using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get something similar to the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the forward selection method has taken around `2.68` seconds,
    which is much lower than the backward selection method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create new training and test sets. Once we have identified the best set of
    features, we have to modify our training and test sets so that they have only
    those selected features. This is accomplished using the `.transform()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.19: Shape of the training and testing datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.19: Shape of the training and testing datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that both the training and test sets are reduced to `250` features each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now fit a logistic regression model on the transformed dataset and note
    the time it takes to fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the total time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that the training time is much less than the model that was fit
    in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*, which was `23.86` seconds. This shorter time is attributed to the number
    of features in the forward selection model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, perform predictions on the test set and print the accuracy metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get something similar to the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.20: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.20: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get something similar to the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.21: Resulting classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.21: Resulting classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YhQE7X](https://packt.live/2YhQE7X).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the forward selection process, we were able to get an accuracy
    score of `94%` with `250` features. This score is lower than the one that was
    achieved with the backward elimination method (`98%`) and also the benchmark model
    (`97%`) built in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*. However, the time taken to find the best features (2.68 seconds) was
    substantially less than the backward elimination method (230.35 seconds).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at Principal Component Analysis (PCA).
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is a very effective dimensionality reduction technique that achieves dimensionality
    reduction without compromising on the information content of the data. The basic
    idea behind PCA is to first identify correlations among different variables within
    the dataset. Once correlations are identified, the algorithm decides to eliminate
    the variables in such a way that the variability of the data is maintained. In
    other words, PCA aims to find uncorrelated sources of data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing PCA on raw variables results in transforming them into a completely
    new set of variables called principal components. Each of these components represents
    variability in data along an axes that are orthogonal to each other. This means
    that the first axis is fit in the direction where the maximum variability of data
    is present. After this, the second axis is selected in such a way that the axis
    is orthogonal (perpendicular) to the first selected axis and also covers the next
    highest variability.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the idea of PCA with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a sample dataset with 2 variables and 100 random data points
    in each variable. Random data points are created using the `rand()` function.
    This is implemented in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is: `(100, 2)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A random state is defined using the `RandomState(123)` function. This is defined
    to ensure that anyone who reproduces this example gets the same output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize this data using `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.22: Visualization of the data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.22: Visualization of the data'
  prefs: []
  type: TYPE_NORMAL
- en: In the graph, we can see that the data is evenly spread out.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now find the principal components for this dataset. We will reduce this
    two-dimensional dataset into a one-dimensional dataset. In other words, we will
    reduce the original dataset into one of its principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is implemented in code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the code, we first define the number of components using the
    `'n_components' = 1` argument. After this, the PCA algorithm is fit on the input
    dataset. After fitting on the input data, the initial dataset is transformed into
    a new dataset with only one variable, which is its principal component.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm transforms the original dataset into its first principal component
    by using an axis where the data has the largest variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize this concept, let''s reverse the transformation of the `X_pca`
    dataset to its original form and then visualize this data along with the original
    data. To reverse the transformation, we use the `.inverse_transform()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.23: Plot with reverse transformation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.23: Plot with reverse transformation'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the plot, the data points in orange represent an axis with
    the highest variability. All the data points were projected to that axis to generate
    the first principal component.
  prefs: []
  type: TYPE_NORMAL
- en: The data points that are generated when transforming into various principal
    components will be very different from the original data points before transformation.
    Each principal component will be in an axis that is orthogonal (perpendicular)
    to the other principal component. If a second principal component was generated
    for the preceding example, the second principal component would be along an axis
    indicated by the blue arrow in the graph. The way we pick the number of principal
    components for model building is by selecting the number of components that explains
    a certain threshold of variability.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there were originally 1,000 features and we reduced it to 100
    principal components, and then we find that out of the 100 principal components
    the first 75 components explain 90% of the variability of data, we would pick
    those 75 components to build the model. This process is called picking principal
    components with the percentage of variance explained.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how to use PCA as a tool for dimensionality reduction in our use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.04: Dimensionality Reduction Using PCA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will fit a logistic regression model by selecting the principal
    components that explain the maximum variability of the data. We will also observe
    the performance of the feature selection and model building process. We will be
    using the same ads dataset as before, and we will be enhancing it with additional
    features for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement the initial steps from *Exercise 14.01*, *Loading and Cleaning the
    Dataset*, up until scaling the dataset using the `minmaxscaler()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a high-dimensional dataset. Let''s now augment the dataset artificially
    to a factor of 50\. Augmenting the dataset to higher factors will result in the
    notebook crashing because of a lack of memory. This is implemented using the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should get the following output
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s split the high-dimensional dataset to training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now fit the PCA function on the training set. This is done using the
    `.fit()` function, as shown in the following snippet. We will also note the time
    it takes to fit the PCA model on the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the time taken to fit the PCA function on the dataset is less
    than the backward elimination model (230.35 seconds) and higher than the forward
    selection method (2.682 seconds).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will now determine the number of principal components by plotting the cumulative
    variance explained by all the principal components. The variance explained is
    determined by the `pca.explained_variance_ratio_` method. This is plotted in `matplotlib`
    using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the code, the `np.cumsum()` function is used to get the cumulative variance
    of each principal component.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will get the following plot as output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.24: The variance graph'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.24: The variance graph'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the plot, we can see that the first `250` principal components explain
    more than `90%` of the variance. Based on this graph, we can decide how many principal
    components we want to have depending on the variability it explains. Let's select
    `250` components for fitting our model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have identified that `250` components explain a lot of the variability,
    let''s refit the training set for `250` components. This is described in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now transform the training and test sets with the 200 principal components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.25: Transformed and the original training and testing sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.25: Transformed and the original training and testing sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that both the training and test sets are reduced to `250` features each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes to fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the total time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that the training time is much lower than the model that was fit
    in *Activity 14.01*, *Fitting a Logistic Regression Model on a HighDimensional
    Dataset*, which was 23.86 seconds. The shorter time is attributed to the smaller
    number of features, `250`, selected in PCA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, predict on the test set and print the accuracy metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.26: Accuracy of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.26: Accuracy of the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that the accuracy level is better than the benchmark model with
    all the features (`97%`) and the forward selection model (`94%`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.27: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.27: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.28: Resulting classification matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.28: Resulting classification matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iXNVbq](https://packt.live/3iXNVbq).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: As is evident from the results, we get a score of 98%, which is better than
    the benchmark model. One reason that could be attributed to the higher performance
    could be the creation of uncorrelated principal components using the PCA method,
    which has boosted the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at Independent Component Analysis (ICA).
  prefs: []
  type: TYPE_NORMAL
- en: Independent Component Analysis (ICA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ICA is a technique of dimensionality reduction that conceptually follows a similar
    path as PCA. Both ICA and PCA try to derive new sources of data by linearly combining
    the original data.
  prefs: []
  type: TYPE_NORMAL
- en: However, the difference between them lies in the method they use to find new
    sources of data. While PCA attempts to find uncorrelated sources of data, ICA
    attempts to find independent sources of data.
  prefs: []
  type: TYPE_NORMAL
- en: ICA has a very similar implementation for dimensionality reduction as PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the implementation of ICA for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.05: Dimensionality Reduction Using Independent Component Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will fit a logistic regression model using the ICA technique
    and observe the performance of the model. We will be using the same ads dataset
    as before, and we will be enhancing it with additional features for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement all the steps from *Exercise 14.01*, *Loading and Cleaning the Dataset*,
    up until scaling the dataset using `MinMaxScaler()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now augment the dataset artificially to a factor of `50`. Augmenting
    the dataset to factors that are higher than `50` will result in the notebook crashing
    because of a lack of memory. This is implemented using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s split the high-dimensional dataset into training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s load the ICA function, `FastICA`, and then define the number of components
    we require. We will use the same number of components that we used for PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the ICA method is defined, we will fit the method on the training set
    and also transform the training set to get a new training set with the required
    number of components. We will also note the time taken for fitting and transforming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the code, the `.fit()` function is used to fit on the training set and the
    `transform()` method is used to get a new training set with the required number
    of features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that implementing ICA has taken much more time than PCA (179.54 seconds).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now transform the test set with the `250` components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.29: Shape of the original and transformed datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.29: Shape of the original and transformed datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that both the training and test sets are reduced to `250` features each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the total time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now predict on the test set and print the accuracy metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the ICA model has worse results than other models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.30: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.30: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that the ICA model has done a poor job in classifying the ads. All
    the examples have been wrongly classified as non-ads. We can conclude that ICA
    is not suitable for this dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.31: Resulting classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.31: Resulting classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/325H88Q](https://packt.live/325H88Q).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, transforming the data to its first 250 independent components
    did not capture all the necessary variability in the data. This has resulted in
    the degradation of the classification results for this method. We can conclude
    that ICA is not suitable for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It was also observed that the time taken to find the best independent features
    was longer than for PCA. However, it should be noted that different methods vary
    in results according to the input data. Even though ICA was not suitable for this
    dataset, it still is a potent method for dimensionality reduction that should
    be in the repertoire of a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this exercise, you may come up with a few questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you think we can improve the classification results using ICA?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of components results in a marginal increase in the accuracy
    metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any other side effects because of the strategy adopted to improve
    the results?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of components also results in a longer training time for
    the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Factor Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Factor analysis is a technique that achieves dimensionality reduction by grouping
    variables that are highly correlated. Let's look at an example from our context
    of predicting advertisements.
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, there could be many features that describe the geometry (the
    size and shape of an image in the ad) of the images on a web page. These features
    can be correlated because they refer to specific characteristics of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there could be many features that describe the anchor text or phrases
    occurring in a URL, which are highly correlated. Factor analysis looks at correlated
    groups such as these from the data and then groups them into latent factors. Therefore,
    if there are 10 raw features describing the geometry of an image, factor analysis
    will group them into one feature that characterizes the geometry of an image.
    Each of these groups is called factors. As many correlated features are combined
    to form a group, the resulting number of features will be much smaller in comparison
    with the original dimensions of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how factor analysis can be implemented as a technique for dimensionality
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14.06: Dimensionality Reduction Using Factor Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will fit a logistic regression model after reducing the
    original dimensions to some key factors and then observe the performance of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement the same initial steps from *Exercise 14.01*, *Loading and Cleaning
    the Dataset*, up until scaling the dataset using the `minmaxscaler()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now augment the dataset artificially to a factor of `50`. Augmenting
    the dataset to factors that are higher than `50` will result in the notebook crashing
    because of a lack of memory. This is implemented using the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s split the high-dimensional dataset into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An important step in factor analysis is defining the number of factors in a
    dataset. This step is achieved through experimentation. In our case, we will arbitrarily
    assume that there are `20` factors. This is implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The number of factors is defined through the `n_components` argument. We also
    define a random state for reproducibility.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the factor method is defined, we will fit the method on the training set
    and also transform the training set to get a new training set with the required
    number of factors. We will also note the time it takes to fit the required number
    of factors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the code, the `.fit()` function is used to fit on the training set, and the
    `transform()` method is used to get a new training set with the required number
    of factors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Factor analysis is also a compute-intensive method. This is the reason that
    only 20 factors were selected. We can see that it has taken `130.688` seconds
    for `20` factors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now transform the test set with the same number of factors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s verify the shapes of the train and test sets before transformation and
    after transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.32: Original and transformed dataset values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.32: Original and transformed dataset values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that both the training and test sets have been reduced to `20` factors each.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now fit the logistic regression model on the transformed dataset and
    note the time it takes to fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the total time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the time it has taken to fit the logistic regression model is
    comparable with other methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now predict on the test set and print the accuracy metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the factor model has better results than the ICA model, but
    worse results than the other models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.33: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.33: Resulting confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that the factor model has done a better job at classifying the ads
    than the ICA model. However, there is still a high number of false positives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.34: Resulting classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_14_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 14.34: Resulting classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/32b9SNk](https://packt.live/32b9SNk).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, but can
    be run as usual on Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the results, by reducing the variables to just 20 factors,
    we were able to get a decent classification result. Even though there is degradation
    on the result, we still have a manageable number of features, which will be able
    to scale well on any algorithm. The balance between the accuracy measures and
    the ability to manage features needs to be explored through greater experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: How do you think we can improve the classification results for factor analysis?
  prefs: []
  type: TYPE_NORMAL
- en: Well, increasing the number of components results in an increase in the accuracy metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Different Dimensionality Reduction Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned different dimensionality reduction techniques, let's
    apply all of these techniques to a new dataset that we will create from the existing
    ads dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will randomly sample some data points from a known distribution and then
    add these random samples to the existing dataset to create a new dataset. Let's
    carry out an experiment to see how a new dataset can be created from an existing
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create a dummy data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a small dataset with two rows and three columns for this example.
    We use the `pd.np.array()` function to create a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.35: Sample data frame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.35: Sample data frame'
  prefs: []
  type: TYPE_NORMAL
- en: What we will do next is sample some data points with the same shape as the data
    frame we created.
  prefs: []
  type: TYPE_NORMAL
- en: Let's sample some data points from a normal distribution that has mean `0` and
    standard deviation of `0.1`. We touched briefly on normal distributions in *Chapter
    3, Binary Classification.* A normal distribution has two parameters. The first
    one is the mean, which is the average of all the data in the distribution, and
    the second one is standard deviation, which is a measure of how spread out the
    data points are.
  prefs: []
  type: TYPE_NORMAL
- en: By assuming a mean and standard deviation, we will be able to draw samples from
    a normal distribution using the `np.random.normal()` Python function. The arguments
    that we have to give for this function are the mean, the standard deviation, and
    the shape of the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this is implemented in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we give the mean (`mu`), standard deviation (`sigma`), and the
    shape of the data frame `[2,3]` to generate the new random samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the sampled data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to add the original data frame and the sampled data frame
    to get the new dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Having seen how to create a new dataset, let's use this knowledge in the next
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 14.02: Comparison of Dimensionality Reduction Techniques on the Enhanced
    Ads Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have learned different dimensionality reduction techniques. You want to
    determine which is the best technique among them for a dataset you will create.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint**: In this activity, we will use the different techniques that you have
    used in all the exercises so far. You will also create a new dataset as we did
    in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to complete this activity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the original ads data and derive the transformed independent variable,
    `X_tran`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a high-dimensional dataset by replicating the columns twice using the
    `pd.np.tile()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create random samples from a normal distribution with mean = 0 and standard
    deviation = 0.1\. Make the new dataset with the same shape as the high-dimensional
    dataset created in *step 3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the high dimensional dataset and the random samples to get the new dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement backward elimination with the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the backward elimination step using the `RFE()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use logistic regression as the model and select the best `300` features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit the `RFE()` function on the training set and measure the time it takes to
    fit the RFE model on the training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transform the train and test sets with the RFE model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the transformed training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the test set and print the accuracy score, confusion matrix, and
    classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement the forward selection technique with the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the number of features using the `SelectKBest()` function. Select the
    best `300` features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit the forward selection on the training set using the `.fit()` function and
    note the time taken for the fit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transform both the training and test sets using the `.transform()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the transformed training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the principal components using the `PCA()` function. Use 300 components.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit `PCA()` on the training set. Note the time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transform both the training set and test set to get the respective number of
    components for these datasets using the `.transform()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the transformed training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement ICA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define independent components using the `FastICA()` function using `300` components.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit the independent components on the training set and transform the training
    set. Note the time for the implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transform the test set to get the respective number of components for these
    datasets using the `.transform()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the transformed training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement factor analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the number of factors using the `FactorAnalysis()` function and `30` factors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit the factors on the training set and transform the training set. Note the
    time for the implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transform the test set to get the respective number of components for these
    datasets using the `.transform()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the transformed training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predict on the transformed test set and print the accuracy, confusion matrix,
    and classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compare the outputs of all the methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example summary table of the results is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.36: Summary output of all the reduction techniques'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_14_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.36: Summary output of all the reduction techniques'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we implemented five different methods of dimensionality reduction
    with a new dataset that we created from the internet ads dataset.
  prefs: []
  type: TYPE_NORMAL
- en: From the tabulated results, we can see that three methods (backward elimination,
    forward selection, and PCA) have got the same accuracy scores. Therefore, the
    selection criteria for the best method should be based on the time taken to get
    the reduced dimension. With these criteria, the forward selection method is the
    best method, followed by PCA.
  prefs: []
  type: TYPE_NORMAL
- en: For the third place, we should strike a balance between accuracy and the time
    taken for dimensionality reduction. We can see that factor analysis and backward
    elimination have very close accuracy scores, 96% and 97% respectively. However,
    the time taken for backward elimination is quite large compared to factor analysis.
    Therefore, we should weigh our considerations toward factor analysis as the third
    best, even though the accuracy is marginally lower than backward elimination.
    The last spot should go to ICA because the accuracy is far lower than all the
    other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about various techniques for dimensionality
    reduction. Let's summarize what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we were introduced to the challenges inherent
    with some of the modern-day datasets in terms of scalability. To further learn
    about these challenges, we downloaded the Internet Advertisement dataset and did
    an activity where we witnessed the scalability challenges posed by a large dataset.
    In the activity, we artificially created a large dataset and fit a logistic regression
    model to it.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we were introduced to five different methods of
    dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backward feature elimination** worked on the principle of eliminating features
    one by one until no major degradation of accuracy measures occurred. This method
    is computationally intensive, but we got better results than the benchmark model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward feature selection** goes in the opposite direction as backward elimination
    and selects one feature at a time to get the best set of features we predetermined.
    This method is also computationally intensive. We also found out that this method
    had marginally lower accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) aims at finding components that
    are orthogonal to each other and that best explain the variability of the data.
    We had better results with PCA than we got from the benchmark model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent component analysis** (**ICA**) is similar to PCA; however, it
    differs in terms of the approach to the selection of components. ICA looks for
    independent components from the dataset. We saw that ICA achieved one of the worst
    results in our context.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Factor analysis** was all about finding factors or groups of correlated features
    that best described the data. We achieved much better results than ICA with factor analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this chapter was to equip you with a set of techniques that help
    in scenarios when the scalability of models was challenging. The key to getting
    good results is to understand which method to use in which scenario. This could
    be achieved with lots of hands-on practice and experimentation with many large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned a set of tools to manage scalability in this chapter, we will
    move on to the next chapter, which addresses the problem of boosting performance.
    In the next chapter, you will be introduced to a technique called ensemble learning.
    This technique will help to boost the performance of your machine learning models.
  prefs: []
  type: TYPE_NORMAL
