<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer705" class="Content">
			</div>
		</div>
		<div id="_idContainer706" class="Content">
			<h1 id="_idParaDest-323"><a id="_idTextAnchor322"/>15. Ensemble Learning</h1>
		</div>
		<div id="_idContainer742" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">By the end of this chapter, you will be able to describe ensemble learning and apply different ensemble learning techniques to your dataset. You will also be able to fit a dataset on a model and analyze the results after ensemble learning.</p>
			<p class="callout">In this chapter, we will be using the credit card application dataset, where we will try to predict whether a credit card application will be approved.</p>
			<h1 id="_idParaDest-324"><a id="_idTextAnchor323"/>Introduction</h1>
			<p>In the previous chapter, we learned various techniques, such as the backward elimination technique, factor analysis, and so on, that helped us to deal with high-dimensional datasets.</p>
			<p>In this chapter, we will further enhance our repertoire of skills with another set of techniques, called <strong class="bold">ensemble learning</strong>, in which we will be dealing with different ensemble learning techniques such as the following:</p>
			<ul>
				<li>Averaging</li>
				<li>Weighted averaging</li>
				<li>Max voting</li>
				<li>Bagging</li>
				<li>Boosting</li>
				<li>Blending</li>
			</ul>
			<h1 id="_idParaDest-325"><a id="_idTextAnchor324"/>Ensemble Learning</h1>
			<p>Ensemble learning, as the name denotes, is a method that combines several machine learning models to generate a superior model, thereby decreasing variability/variance and bias, and boosting performance.</p>
			<p>Before we explore what ensemble learning is, let's look at the concepts of bias and variance with the help of the classical bias-variance quadrant, as shown here:</p>
			<div>
				<div id="_idContainer707" class="IMG---Figure">
					<img src="Images/B15019_15_01.jpg" alt="Figure 15.1: Bias-variance quadrant&#13;&#10;" width="954" height="423"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.1: Bias-variance quadrant</p>
			<h2 id="_idParaDest-326"><a id="_idTextAnchor325"/>Variance</h2>
			<p>Variance is the measure of how spread out data is. In the context of machine learning, models with high variance imply that the predictions generated on the same test set will differ considerably when different training sets are used to fit the model. The underlying reason for high variability could be attributed to the model being attuned to specific nuances of training data rather than generalizing the relationship between input and output. Ideally, we want every machine learning model to have low variance.</p>
			<h2 id="_idParaDest-327"><a id="_idTextAnchor326"/>Bias</h2>
			<p>Bias is the difference between the ground truth and the average value of our predictions. A low bias will indicate that the predictions are very close to the actual values. A high bias implies that the model has oversimplified the relationship between the inputs and outputs, leading to high error rates on test sets, which again is an undesirable outcome.</p>
			<p><em class="italic">Figure 15.1</em> helps us to visualize the trade-off between bias and variance. The top-left corner is the depiction of a scenario where the bias is high, and the variance is low. The top-right quadrant displays a scenario where both bias and variance are high. From the figure, we can see that when the bias is high, it is further away from the truth, which in this case, is the <em class="italic">bull's eye</em>. The presence of variance is manifested as whether the arrows are spread out or congregated in one spot.</p>
			<p>Ensemble models combine many weaker models that differ in variance and bias, thereby creating a better model, outperforming the individual weaker models. Ensemble models exemplify the adage <em class="italic">the wisdom of the crowds</em>. In this chapter, we will learn about different ensemble techniques, which can be classified into two types, that is, simple and advanced techniques:</p>
			<div>
				<div id="_idContainer708" class="IMG---Figure">
					<img src="Images/B15019_15_02.jpg" alt="Figure 15.2: Different ensemble learning methods&#13;&#10;" width="927" height="227"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.2: Different ensemble learning methods</p>
			<h2 id="_idParaDest-328"><a id="_idTextAnchor327"/>Business Context</h2>
			<p>You are working in the credit card division of your bank. The operations head of your company has requested your help in determining whether a customer is creditworthy or not. You have been provided with credit card operations data. </p>
			<p>This dataset contains credit card applications with around 15 variables. The variables are a mix of continuous and categorical data pertaining to credit card operations. The label for the dataset is a flag, which indicates whether the application has been approved or not. </p>
			<p>You want to fit some benchmark models and try some ensemble learning methods on the dataset to address the problem and come up with a tool for predicting whether or not a given customer should be approved for their credit application.</p>
			<h2 id="_idParaDest-329"><a id="_idTextAnchor328"/>Exercise 15.01: Loading, Exploring, and Cleaning the Data</h2>
			<p>In this exercise, we will download the credit card dataset, load it into our Colab notebook, and perform a few basic explorations. In addition, we will also clean the dataset to remove unwanted characters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that we will be using in this exercise was sourced from the UCI Machine Learning Repository: <a href="https://packt.live/39NCgZ2">https://packt.live/39NCgZ2</a>. You can download the dataset from our GitHub at <a href="https://packt.live/3018OdD">https://packt.live/3018OdD</a>.</p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li>Open a new Colab notebook file.</li>
				<li>Now, import <strong class="source-inline">pandas</strong> into your Colab notebook:<p class="source-code">import pandas as pd</p></li>
				<li>Next, set the path of the GitHub repository where  <strong class="source-inline">crx.data</strong> is uploaded, as mentioned in the following code snippet:<p class="source-code">#Load data from the GitHub repository</p><p class="source-code">filename = 'https://raw.githubusercontent.com'\</p><p class="source-code">           '/PacktWorkshops/The-Data-Science-Workshop'\</p><p class="source-code">           '/master/Chapter15/Dataset/crx.data'</p></li>
				<li>Read the file using the <strong class="source-inline">pd.read_csv()</strong> function from the <strong class="source-inline">pandas</strong> data frame:<p class="source-code">credData = pd.read_csv(filename,sep= ",",\</p><p class="source-code">                       header = None,\</p><p class="source-code">                       na_values =  "?")</p><p class="source-code">credData.head()</p><p>The <strong class="source-inline">pd.read_csv()</strong> function's arguments are the filename as a string and the limit separator of a CSV file, which is <strong class="source-inline">,</strong>. </p><p class="callout-heading">Note</p><p class="callout">There are no headers for the dataset; we specifically mention this using the <strong class="source-inline">header = None</strong> command.</p><p>We replace the missing values represented as <strong class="source-inline">?</strong> in the dataset as <strong class="source-inline">na</strong> values using the <strong class="source-inline">na_values =  "?"</strong> argument. This replacement is for ease of further processing. </p><p>After reading the file, print the data frame using the <strong class="source-inline">.head()</strong> function. You should get the following output:</p><div id="_idContainer709" class="IMG---Figure"><img src="Images/B15019_15_03.jpg" alt="Figure 15.3: Loading data into the Colab notebook&#13;&#10;" width="553" height="183"/></div><p class="figure-caption">Figure 15.3: Loading data into the Colab notebook</p></li>
				<li>Change the classes to <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong>.<p>If you notice in the dataset, the classes represented in column <strong class="source-inline">15</strong> are special characters: <strong class="source-inline">+</strong> for approved and <strong class="source-inline">-</strong> for not approved. You need to change this to numerical values of <strong class="source-inline">1</strong> for approved and <strong class="source-inline">0</strong> for not approved, as shown in the following code snippet:</p><p class="source-code"># Changing the Classes to 1 &amp; 0</p><p class="source-code">credData.loc[credData[15] == '+' , 15] = 1</p><p class="source-code">credData.loc[credData[15] == '-' , 15] = 0</p><p class="source-code">credData.head()</p><p>You should get the following output:</p><div id="_idContainer710" class="IMG---Figure"><img src="Images/B15019_15_04.jpg" alt="Figure 15.4: Data frame after replacing special characters&#13;&#10;" width="538" height="196"/></div><p class="figure-caption">Figure 15.4: Data frame after replacing special characters</p><p>In the preceding code snippet, <strong class="source-inline">.loc()</strong> was used to locate the fifteenth column and replace the <strong class="source-inline">+</strong> and <strong class="source-inline">-</strong> values with <strong class="source-inline">1</strong> and <strong class="source-inline">0</strong>, respectively.</p></li>
				<li>Find the number of <strong class="source-inline">null</strong> values in the dataset.<p>We'll now find the number of <strong class="source-inline">null</strong> values in each of the features using the <strong class="source-inline">.isnull()</strong> function. The <strong class="source-inline">.sum()</strong> function sums up all such null values across each of the columns in the dataset. </p><p>This is represented in the following code snippet:</p><p class="source-code"># Finding number of null values in the data set</p><p class="source-code">credData.isnull().sum()</p><p>You should get the following output:</p><div id="_idContainer711" class="IMG---Figure"><img src="Images/B15019_15_05.jpg" alt="Figure 15.5: Summarizing null values in the dataset&#13;&#10;" width="1154" height="753"/></div><p class="figure-caption">Figure 15.5: Summarizing null values in the dataset</p><p>As seen from the preceding output, there are many columns with <strong class="source-inline">null</strong> values.</p></li>
				<li>Now, print the shape and data types of each column:<p class="source-code"># Printing Shape and data types</p><p class="source-code">print('Shape of raw data set',credData.shape)</p><p class="source-code">print('Data types of data set',credData.dtypes)</p><p>You should get the following output:</p><div id="_idContainer712" class="IMG---Figure"><img src="Images/B15019_15_06.jpg" alt="Figure 15.6: Shape and data types of each column&#13;&#10;" width="816" height="513"/></div><p class="figure-caption">Figure 15.6: Shape and data types of each column</p></li>
				<li>Remove the rows with <strong class="source-inline">na</strong> values.<p>In order to clean the dataset, let's remove all the rows with <strong class="source-inline">na</strong> values using the <strong class="source-inline">.dropna()</strong> function with the following code snippet:</p><p class="source-code"># Dropping all the rows with na values</p><p class="source-code">newcred = credData.dropna(axis = 0)</p><p class="source-code">newcred.shape</p><p>You should get the following output:</p><p class="source-code">(653, 16)</p><p>As you can see, around 37 rows that, which had <strong class="source-inline">na</strong> values, were removed. In the code snippet, we define <strong class="source-inline">axis = 0</strong> in order to denote that the dropping of <strong class="source-inline">na</strong> values should be done along the rows.</p></li>
				<li>Verify that no <strong class="source-inline">null</strong> values exist:<p class="source-code"># Verifying no null values exist</p><p class="source-code">newcred.isnull().sum()</p><p>You should get the following output:</p><div id="_idContainer713" class="IMG---Figure"><img src="Images/B15019_15_07.jpg" alt="Figure 15.7: Verifying that no null values are present&#13;&#10;" width="798" height="483"/></div><p class="figure-caption">Figure 15.7: Verifying that no null values are present</p></li>
				<li>Next, make dummy values from the categorical variables.<p>As you can see from the data types, there are many variables with categorical values. These have to be converted to dummy values using the <strong class="source-inline">pd.get_dummies()</strong> function. This is done using the following code snippet:</p><p class="source-code">"""</p><p class="source-code">Separating the categorical variables to </p><p class="source-code">make dummy variables</p><p class="source-code">"""</p><p class="source-code">credCat = pd.get_dummies(newcred[[0,3,4,5,6,8,9,11,12]])</p></li>
				<li>Separate the numerical variables.<p>We will also be separating the numerical variables from the original dataset to concatenate them with the dummy variables. This step is done as follows:</p><p class="source-code"># Separating the numerical variables</p><p class="source-code">credNum = newcred[[1,2,7,10,13,14]]</p><p class="callout-heading">Note</p><p class="callout">You can view these new DataFrames by running the commands <strong class="source-inline">credCat</strong> and <strong class="source-inline">credNum</strong>.</p></li>
				<li>Create the <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> variables. The dummy variables and the numerical variables will now be concatenated to form the <strong class="source-inline">X</strong> variable. The <strong class="source-inline">y</strong> variable will be created separately by taking the labels of the dataset. Let's see these steps in action in the following code snippet:<p class="source-code">"""</p><p class="source-code">Making the X variable which is a concatenation </p><p class="source-code">of categorical and numerical data</p><p class="source-code">"""</p><p class="source-code">X = pd.concat([credCat,credNum],axis = 1)</p><p class="source-code">print(X.shape)</p><p class="source-code"># Separating the label as y variable</p><p class="source-code">y = pd.Series(newcred[15], dtype="int")</p><p class="source-code">print(y.shape)</p><p>You should get the following output:</p><p class="source-code">(653, 46)</p><p class="source-code">(653,)</p></li>
				<li>Normalize the dataset using the <strong class="source-inline">MinMaxScaler()</strong> function:<p class="source-code"># Normalizing the data sets</p><p class="source-code"># Import library function</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code"># Creating the scaling function</p><p class="source-code">minmaxScaler = preprocessing.MinMaxScaler()</p><p class="source-code"># Transforming with the scaler function</p><p class="source-code">X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))</p></li>
				<li>Split the dataset into training and test sets.<p>As the final step of data preparation, we will now split the dataset into training and test sets using the <strong class="source-inline">train_test_split()</strong> function:</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># Splitting the data into train and test sets</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (X_tran, y, test_size=0.3,\</p><p class="source-code">                                    random_state=123)</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31hM3od">https://packt.live/31hM3od</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3glWdZf">https://packt.live/3glWdZf</a>.</p></li>
			</ol>
			<p>We now have the required dataset ready for further actions. As always, let's start off by fitting a benchmark model using logistic regression on the cleaned dataset. This will be achieved in the next activity.</p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor329"/>Activity 15.01: Fitting a Logistic Regression Model on Credit Card Data</h2>
			<p>You have just cleaned the dataset that you received to predict the creditworthiness of your customers. Before applying ensemble learning methods, you want to fit a benchmark model on the dataset.</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Implement all the appropriate steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, until you have split the dataset into training and test sets.</li>
				<li>Fit a logistic regression model on the training set.</li>
				<li>Get the predictions on the test set.</li>
				<li>Print the confusion matrix and classification report for the benchmark model.<p>You should get an output similar to the following after fitting the logistic regression model on the dataset:</p><div id="_idContainer714" class="IMG---Figure"><img src="Images/B15019_15_08.jpg" alt="Figure 15.8: Expected output after fitting the logistic regression model&#13;&#10;" width="900" height="289"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.8: Expected output after fitting the logistic regression model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please note that you will not get exact values as output due to variability in the prediction process.</p>
			<p class="callout">The solution to this activity can be found here: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>In this activity, we created a benchmark model for comparison with subsequent models. </p>
			<p>As you can see from the output, we achieved an accuracy level of <strong class="source-inline">0.89</strong> with the benchmark model.</p>
			<p>Now that we have fit a benchmark model, we will explore different methods of ensemble learning in the next section, starting with simple methods.</p>
			<h1 id="_idParaDest-331"><a id="_idTextAnchor330"/>Simple Methods for Ensemble Learning</h1>
			<p>As defined earlier in the chapter, ensemble learning is all about combining the strengths of individual models to get a superior model. In this section, we will explore some simple techniques such as the following: </p>
			<ul>
				<li>Averaging</li>
				<li>Weighted averaging</li>
				<li>Max voting</li>
			</ul>
			<p>Let's take a look at each of them in turn.</p>
			<h2 id="_idParaDest-332"><a id="_idTextAnchor331"/>Averaging</h2>
			<p>Averaging is a naïve way of doing ensemble learning; however, it is extremely useful too. The basic idea behind this technique is to take the predictions of multiple individual models and then average the predictions to generate a final prediction. The assumption is that by averaging the predictions of different individual learners, we eliminate the errors made by individual learners, thereby generating a model superior to the base model. One prerequisite to make averaging work is to have the predictions of the base models be uncorrelated. This would mean that the individual models should not make the same kinds of errors. The diversity of the models is a critical aspect to ensure uncorrelated errors.</p>
			<p>When implementing the averaging technique, there are some nuances in predictions that need to be taken care of. When predicting, so far, we have been using the <strong class="source-inline">predict()</strong> function. As you might know by now, the <strong class="source-inline">predict()</strong> function outputs the class that has the highest probability. For example, in our benchmark model, when we predicted on the test set, we got an output of either '1' or '0' for each of the test examples. There is also another way to make predictions, which is by generating the probability of each class. If we were to output the probability of each class for our benchmark model, we would get two outputs for each example corresponding to the probability for each class. This is demonstrated in an example prediction in the following table:</p>
			<div>
				<div id="_idContainer715" class="IMG---Figure">
					<img src="Images/B15019_15_09.jpg" alt="Figure 15.9: An example prediction&#13;&#10;" width="1665" height="209"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.9: An example prediction</p>
			<p>From the preceding example, we can see that each test example has two outputs corresponding to the probabilities of each class. So, for the first example, the prediction would be class '1' as the probability for class <strong class="source-inline">1</strong> (<strong class="source-inline">0.902</strong>) is higher than class <strong class="source-inline">0</strong> (<strong class="source-inline">0.098</strong>). The respective predictions for example <strong class="source-inline">2</strong> and <strong class="source-inline">3 </strong>would be class <strong class="source-inline">0</strong> and class <strong class="source-inline">1</strong> respectively.</p>
			<p>When we generate an ensemble by averaging method, we generate the probability of each class instead of the class predictions. The probability of each class is predicted using a separate function called <strong class="source-inline">predict_proba()</strong>. We will see the implementation of the averaging method in <em class="italic">Exercise 15.02</em>, <em class="italic">Ensemble Model Using the Averaging Technique</em>.</p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor332"/>Exercise 15.02: Ensemble Model Using the Averaging Technique</h2>
			<p>In this exercise, we will implement an ensemble model using the averaging technique. The base models that we will use for this exercise are the logistic regression model, which we used as our benchmark model, and the KNN and random forest models, which were introduced in <em class="italic">Chapter 4</em>,<em class="italic"> Multiclass Classification with RandomForest</em>, and <em class="italic">Chapter 8</em>, <em class="italic">Hyperparameter Tuning</em>:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Execute all the appropriate steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, to split the dataset into train and test sets.</li>
				<li>Let's define the three base models. Import the selected classifiers, which we will use as base models:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">model1 = LogisticRegression(random_state=123)</p><p class="source-code">model2 = KNeighborsClassifier(n_neighbors=5)</p><p class="source-code">model3 = RandomForestClassifier(n_estimators=500)</p></li>
				<li>Fit all three models on the training set:<p class="source-code"># Fitting all three models on the training data</p><p class="source-code">model1.fit(X_train,y_train)</p><p class="source-code">model2.fit(X_train,y_train)</p><p class="source-code">model3.fit(X_train,y_train)</p></li>
				<li>We will now predict the probabilities of each model using the <strong class="source-inline">predict_proba()</strong> function:<p class="source-code">"""</p><p class="source-code">Predicting probabilities of each model </p><p class="source-code">on the test set</p><p class="source-code">"""</p><p class="source-code">pred1=model1.predict_proba(X_test)</p><p class="source-code">pred2=model2.predict_proba(X_test)</p><p class="source-code">pred3=model3.predict_proba(X_test)</p></li>
				<li>Average the predictions generated from all of the three models:<p class="source-code">"""</p><p class="source-code">Calculating the ensemble prediction by </p><p class="source-code">averaging three base model predictions</p><p class="source-code">"""</p><p class="source-code">ensemblepred=(pred1+pred2+pred3)/3</p></li>
				<li>Display the first four rows of the ensemble prediction array:<p class="source-code"># Displaying first 4 rows of the ensemble predictions</p><p class="source-code">ensemblepred[0:4,:]</p><p>You should get an output similar to the following:</p><div id="_idContainer716" class="IMG---Figure"><img src="Images/B15019_15_10.jpg" alt="Figure 15.10: First four rows of ensemble predictions&#13;&#10;" width="861" height="121"/></div><p class="figure-caption">Figure 15.10: First four rows of ensemble predictions</p><p>As you can see from the preceding output, we have two probabilities for each example corresponding to each class.</p></li>
				<li>Print the order of each class from the prediction output. As you can see from <em class="italic">Step 6</em>, the prediction output has two columns corresponding to each class. In order to find the order of the class prediction, we use a method called <strong class="source-inline">.classes_</strong>. This is implemented in the following code snippet:<p class="source-code"># Printing the order of classes for each model</p><p class="source-code">print(model1.classes_)</p><p class="source-code">print(model2.classes_)</p><p class="source-code">print(model3.classes_)</p><p>You should get the following output:</p><div id="_idContainer717" class="IMG---Figure"><img src="Images/B15019_15_11.jpg" alt="Figure 15.11: Order of classes&#13;&#10;" width="772" height="93"/></div><p class="figure-caption">Figure 15.11: Order of classes</p></li>
				<li>We now have to get the final predictions for each example from the output probabilities. The final prediction will be the class with the highest probability. To get the class with the highest probability, we use the <strong class="source-inline">numpy</strong> function, <strong class="source-inline">.argmax()</strong>. This is executed as follows:<p class="source-code">import numpy as np</p><p class="source-code">pred = np.argmax(ensemblepred,axis = 1)</p><p class="source-code">pred</p><p>From the preceding code, <strong class="source-inline">axis = 1</strong> means that we need to find the index of the maximum value across the columns.</p><p>You should get the following output:</p><div id="_idContainer718" class="IMG---Figure"><img src="Images/B15019_15_12.jpg" alt="Figure 15.12: Array output&#13;&#10;" width="1059" height="299"/></div><p class="figure-caption">Figure 15.12: Array output</p></li>
				<li>Generate the confusion matrix for the predictions:<p class="source-code"># Generating confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get an output similar to the following:</p><div id="_idContainer719" class="IMG---Figure"><img src="Images/B15019_15_13.jpg" alt="Figure 15.13: Confusion matrix&#13;&#10;" width="601" height="55"/></div><p class="figure-caption">Figure 15.13: Confusion matrix</p></li>
				<li>Generate a classification report:<p class="source-code"># Generating classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer720" class="IMG---Figure"><img src="Images/B15019_15_14.jpg" alt="Figure 15.14: Classification report&#13;&#10;" width="846" height="233"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.14: Classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31aLa0w">https://packt.live/31aLa0w</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31hMpex">https://packt.live/31hMpex</a>.</p>
			<p>In this exercise, we implemented the averaging technique for ensemble learning. As you can see from the classification report, we managed to improve the accuracy rate from <strong class="source-inline">0.89</strong>, which was achieved with the benchmark model, to <strong class="source-inline">0.91</strong> by averaging. However, it should be noted that ensemble methods are not guaranteed to improve the results in all cases. There could be instances where no improvement in performance would be observed. </p>
			<p>Let's also look at the results from the business context of credit card applications. An accuracy of <strong class="source-inline">91%</strong> means that out of the <strong class="source-inline">196</strong> cases in the test set, we were correctly able to predict whether a customer is creditworthy in <strong class="source-inline">91%</strong> of those cases. It would also be interesting to look at the recall value of class <strong class="source-inline">0</strong>, which is <strong class="source-inline">91%</strong>. In this case, we correctly identified unworthy customers in <strong class="source-inline">91%</strong> of the cases. It is also an area of concern that the balance of <strong class="source-inline">9%</strong> of unworthy customers have been wrongly identified as creditworthy, which is, in fact, a risk to the credit card division. Ideally, we would want to increase the recall value of the non-worthy customers if we want to reduce the risk to the credit card division.</p>
			<p>On the other hand, there is also an issue of maximizing a business opportunity with the correct predictions of creditworthy customers (<strong class="source-inline">1</strong>). We have seen that <strong class="source-inline">91%</strong> of creditworthy customers out of a total of <strong class="source-inline">107</strong> were correctly predicted, which is a positive outcome. The balance of <strong class="source-inline">9%</strong> of creditworthy customers would be a lost business opportunity. </p>
			<p>Fine-tuning models is a balancing act, which a data scientist has to be cognizant about. A data scientist has to act according to the preferences of the business. If the business is risk-averse and would prefer to forego business opportunities, then the models will have to be tuned to increase the recall value of the non-worthy customers. On the other hand, if the business is aggressive and would like to go for growth, then the preferred route would be to improve the recall value of creditworthy customers. At the end of the day, fine-tuning models is a balancing act based on business realities.</p>
			<h2 id="_idParaDest-334"><a id="_idTextAnchor333"/>Weighted Averaging</h2>
			<p>Weighted averaging is an extension of the averaging method that we saw earlier. The major difference in both of these approaches is in the way the combined predictions are generated. In the weighted averaging method, we assign weights to each model's predictions and then generate the combined predictions. The weights are assigned based on our judgment of which model would be the most influential in the ensemble. These weights, which are initially assigned arbitrarily, have to be evolved after a lot of experimentation. To start off, we assume some weights and then iterate with different weights for each model to verify whether we get any improvements in the performance. Let's implement the weighted averaging method in the upcoming exercise.</p>
			<h2 id="_idParaDest-335"><a id="_idTextAnchor334"/>Exercise 15.03: Ensemble Model Using the Weighted Averaging Technique</h2>
			<p>In this exercise, we will implement an ensemble model using the weighted averaging technique. We will use the same base models, logistic regression, KNN, and random forest, which were used in <em class="italic">Exercise 15.02</em>, <em class="italic">Ensemble Model Using the Averaging Technique</em>:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Execute all the steps from <em class="italic">Exercise 15.02</em>, <em class="italic">Ensemble Model Using the Averaging Technique</em>, up until predicting the probabilities of the three models.</li>
				<li>Take the weighted average of the predictions. In the weighted averaging method, weights are assigned arbitrarily based on our judgment of each of the predictions. This is done as follows:<p class="source-code">"""</p><p class="source-code">Calculating the ensemble prediction by applying </p><p class="source-code">weights for each prediction</p><p class="source-code">"""</p><p class="source-code">ensemblepred=(pred1 *0.60 + pred2 * 0.20 + pred3 * 0.20)</p><p>Please note that the weights are assigned in such a way that the sum of all weights becomes <strong class="source-inline">1</strong> (<strong class="source-inline">0.6 + 0.2 + 0.2 = 1</strong>).</p></li>
				<li>Display the first four rows of the ensemble prediction array:<p class="source-code"># Displaying first 4 rows of the ensemble predictions</p><p class="source-code">ensemblepred[0:4,:]</p><p>You should get an output similar to the following:</p><div id="_idContainer721" class="IMG---Figure"><img src="Images/B15019_15_15.jpg" alt="Figure 15.15: Array output for ensemble prediction&#13;&#10;" width="1370" height="226"/></div><p class="figure-caption">Figure 15.15: Array output for ensemble prediction</p><p>As you can see from the output, we have two probabilities for each example corresponding to each class.</p></li>
				<li>Print the order of each class from the prediction output:<p class="source-code"># Printing the order of classes for each model</p><p class="source-code">print(model1.classes_)</p><p class="source-code">print(model2.classes_)</p><p class="source-code">print(model3.classes_)</p><p>You should get the following output:</p><div id="_idContainer722" class="IMG---Figure"><img src="Images/B15019_15_16.jpg" alt="Figure 15.16: Order of class from prediction output&#13;&#10;" width="748" height="93"/></div><p class="figure-caption">Figure 15.16: Order of class from prediction output</p></li>
				<li>Calculate the final predictions from the probabilities.<p>We now have to get the final predictions for each example from the output probabilities using the <strong class="source-inline">np.argmax()</strong> function:</p><p class="source-code">import numpy as np</p><p class="source-code">pred = np.argmax(ensemblepred,axis = 1)</p><p class="source-code">pred</p><p>You should get an output similar to the following:</p><div id="_idContainer723" class="IMG---Figure"><img src="Images/B15019_15_17.jpg" alt="Figure 15.17: Array for final predictions&#13;&#10;" width="1056" height="294"/></div><p class="figure-caption">Figure 15.17: Array for final predictions</p></li>
				<li>Generate the confusion matrix for the predictions:<p class="source-code"># Generating confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get an output similar to the following:</p><div id="_idContainer724" class="IMG---Figure"><img src="Images/B15019_15_18.jpg" alt="Figure 15.18: Confusion matrix&#13;&#10;" width="655" height="55"/></div><p class="figure-caption">Figure 15.18: Confusion matrix</p></li>
				<li>Generating a classification report:<p class="source-code"># Generating classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer725" class="IMG---Figure"><img src="Images/B15019_15_19.jpg" alt="Figure 15.19: Classification report&#13;&#10;" width="946" height="233"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.19: Classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CINUc5">https://packt.live/2CINUc5</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YfrTZK">https://packt.live/2YfrTZK</a>.</p>
			<h3 id="_idParaDest-336"><a id="_idTextAnchor335"/>Iteration 2 with Different Weights</h3>
			<p>From the first iteration, we saw that we got accuracy of <strong class="source-inline">89%</strong>. This metric is a reflection of the weights that we applied in the first iteration. Let's try to change the weights and see what effect it has on the metrics. The process of trying out various weights is based on our judgment of the dataset and the distribution of data. Let's say we feel that the data distribution is more linear, and therefore we decide to increase the weight for the linear regression model and decrease the weights of the other two models. Let's now try the new combination of weights in <em class="italic">iteration 2</em>:</p>
			<ol>
				<li value="1">Take the weighted average of the predictions.<p>In this iteration, we increase the weight of logistic regression prediction from <strong class="source-inline">0.6</strong> to <strong class="source-inline">0.7</strong> and decrease the other two from <strong class="source-inline">0.2</strong> to <strong class="source-inline">0.15</strong>:</p><p class="source-code">"""</p><p class="source-code">Calculating the ensemble prediction by applying </p><p class="source-code">weights for each prediction</p><p class="source-code">"""</p><p class="source-code">ensemblepred=(pred1 *0.70+pred2 * 0.15+pred3 * 0.15)</p></li>
				<li>Calculate the final predictions from the probabilities.<p>We now have to get the final predictions for each example from the output probabilities using the <strong class="source-inline">np.argmax()</strong> function:</p><p class="source-code"># Generating predictions from probabilities</p><p class="source-code">import numpy as np</p><p class="source-code">pred = np.argmax(ensemblepred,axis = 1)</p></li>
				<li>Generate the confusion matrix for the predictions:<p class="source-code"># Generating confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">confusionMatrix = confusion_matrix(y_test, pred)</p><p class="source-code">print(confusionMatrix)</p><p>You should get an output similar to the following:</p><div id="_idContainer726" class="IMG---Figure"><img src="Images/B15019_15_20.jpg" alt="Figure 15.20: Confusion matrix&#13;&#10;" width="786" height="65"/></div><p class="figure-caption">Figure 15.20: Confusion matrix</p></li>
				<li>Generate a classification report:<p class="source-code"># Generating classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer727" class="IMG---Figure"><img src="Images/B15019_15_21.jpg" alt="Figure 15.21: Classification report&#13;&#10;" width="1235" height="325"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.21: Classification report</p>
			<p>In this exercise, we implemented the weighted averaging technique for ensemble learning. We did two iterations with the weights. We saw that in the second iteration, where we increased the weight of the logistic regression prediction from <strong class="source-inline">0.6</strong> to <strong class="source-inline">0.7</strong>, the accuracy actually improved from <strong class="source-inline">0.89</strong> to <strong class="source-inline">0.90</strong>. This is a validation of our assumption about the prominence of the logistic regression model in the ensemble. To check whether there is more room for improvement, we should again change the weights, just like we did in iteration <strong class="source-inline">2</strong>, and then validate against the metrics. We should continue these iterations until there is no further improvement noticed in the metrics.</p>
			<p>Comparing it with the metrics from the averaging method, we can see that the accuracy level has gone down from <strong class="source-inline">0.91</strong> to <strong class="source-inline">0.90</strong>. However, the recall value of class <strong class="source-inline">1</strong> has gone up from <strong class="source-inline">0.91</strong> to <strong class="source-inline">0.92</strong>, and the corresponding value for class <strong class="source-inline">0</strong> has gone down from <strong class="source-inline">0.91</strong> to <strong class="source-inline">0.88</strong>. It could be that the weights that we applied have resulted in a marginal degradation of the results from what we got from the averaging method.</p>
			<p>Looking at the results from a business perspective, we can see that with the increase in the recall value of class <strong class="source-inline">1</strong>, the card division is getting more creditworthy customers. However, this has come at the cost of increasing the risk with more unworthy customers, with <strong class="source-inline">12%</strong> (<strong class="source-inline">100% - 88%</strong>) being tagged as creditworthy customers. </p>
			<h3 id="_idParaDest-337"><a id="_idTextAnchor336"/>Max Voting</h3>
			<p>The max voting method works on the principle of majority rule. In this method, the opinion of the majority rules the roost. In this technique, individual models, or, in ensemble learning jargon, individual learners, are fit on the training set and their predictions are then generated on the test set. Each individual learner's prediction is considered to be a vote. On the test set, whichever class gets the maximum vote is the ultimate winner. Let's demonstrate this with a toy example.</p>
			<p>Let's say we have three individual learners who learned on the training set. Each of them generates their predictions on the test set, which is tabulated in the following table. The predictions are either for class '1' or class '0':</p>
			<div>
				<div id="_idContainer728" class="IMG---Figure">
					<img src="Images/B15019_15_22.jpg" alt="Figure 15.22: Predictions for learners&#13;&#10;" width="1665" height="367"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.22: Predictions for learners</p>
			<p>In the preceding example, we can see that for <strong class="source-inline">Example 1</strong> and <strong class="source-inline">Example 3</strong>, the majority vote is for class '1,' and for the other two examples, the majority of the vote is for class '0'. The final predictions are based on which class gets the majority vote. This method of voting, where we output a class, is called "hard " voting.</p>
			<p>When implementing the max voting method using the <strong class="source-inline">scikit-learn</strong> library, we use a special function called <strong class="source-inline">VotingClassifier()</strong>. We provide individual learners as input to <strong class="source-inline">VotingClassifier</strong> to create the ensemble model. This ensemble model is then used to fit the training set and then is finally used to predict on the test sets. We will explore the dynamics of max voting in <em class="italic">Exercise 15.04</em>, <em class="italic">Ensemble Model Using Max Voting</em>.</p>
			<h2 id="_idParaDest-338"><a id="_idTextAnchor337"/>Exercise 15.04: Ensemble Model Using Max Voting</h2>
			<p>In this exercise, we will implement an ensemble model using the max voting technique. The individual learners we will select are similar to the ones that we chose in the previous exercises, that is, logistic regression, KNN, and random forest:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Execute all the steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, up until the splitting of the dataset into train and test sets.</li>
				<li>We will now import the selected classifiers, which we will use as the individual learners:<p class="source-code">"""</p><p class="source-code">Defining the voting classifier and three </p><p class="source-code">individual learners</p><p class="source-code">"""</p><p class="source-code">from sklearn.ensemble import VotingClassifier</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code"># Defining the models</p><p class="source-code">model1 = LogisticRegression(random_state=123)</p><p class="source-code">model2 = KNeighborsClassifier(n_neighbors=5)</p><p class="source-code">model3 = RandomForestClassifier(n_estimators=500)</p></li>
				<li>Having defined the individual learners, we can now construct the ensemble model using the <strong class="source-inline">VotingClassifier()</strong> function. This is implemented by the following code snippet:<p class="source-code"># Defining the ensemble model using VotingClassifier</p><p class="source-code">model = VotingClassifier(estimators=[('lr', model1),\</p><p class="source-code">                        ('knn', model2),('rf', model3)],\</p><p class="source-code">                         voting= 'hard')</p><p>As you can see from the code snippet, the individual learners are given as input using the <strong class="source-inline">estimators</strong> argument. Estimators take each of the defined individual learners along with the string value to denote which model it is. For example, <strong class="source-inline">lr</strong> denotes logistic regression. Also, note that the voting is "hard, " which means that the output will be class labels and not probabilities.</p></li>
				<li>Fit the training set on the ensemble model:<p class="source-code"># Fitting the model on the training set</p><p class="source-code">model.fit(X_train,y_train)</p></li>
				<li>Print the accuracy scores after training:<p class="source-code">"""</p><p class="source-code">Predicting accuracy on the test set using </p><p class="source-code">.score() function</p><p class="source-code">"""</p><p class="source-code">model.score(X_test,y_test)</p><p>You should get an output similar to the following:</p><p class="source-code">0.9081632653061225</p></li>
				<li>Generate the predictions from the ensemble model on the test set:<p class="source-code"># Generating the predictions on the test set</p><p class="source-code">preds = model.predict(X_test)</p></li>
				<li>Generate the confusion matrix for the predictions:<p class="source-code"># Printing the confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code"># Confusion matrix for the test set</p><p class="source-code">print(confusion_matrix(y_test, preds))</p><p>You should get an output similar to the following:</p><div id="_idContainer729" class="IMG---Figure"><img src="Images/B15019_15_23.jpg" alt="Figure 15.23: Confusion matrix&#13;&#10;" width="1052" height="99"/></div><p class="figure-caption">Figure 15.23: Confusion matrix</p></li>
				<li>Generate the classification report:<p class="source-code"># Printing the classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, preds))</p><p>You should get an output similar to the following:</p><div id="_idContainer730" class="IMG---Figure"><img src="Images/B15019_15_24.jpg" alt="Figure 15.24: Classification report&#13;&#10;" width="1172" height="325"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.24: Classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3aESLr6">https://packt.live/3aESLr6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31cL7RJ">https://packt.live/31cL7RJ</a>.</p>
			<p>In this exercise, we implemented the max voting technique for ensemble learning. As you can see from the classification report, the results are similar to what we got from the averaging method (<strong class="source-inline">0.91</strong>). From a business context, we can see that this result is more balanced. The recall value for worthy customers is high, at <strong class="source-inline">92%</strong>; however, this has come at the cost of more risk by having more unworthy customers. The percentage of unworthy customers is around <strong class="source-inline">10%</strong> in this case (<strong class="source-inline">100%</strong> - <strong class="source-inline">90%</strong>).</p>
			<h1 id="_idParaDest-339"><a id="_idTextAnchor338"/>Advanced Techniques for Ensemble Learning</h1>
			<p>Having learned simple techniques for ensemble learning, let's now explore some advanced techniques. Among the advanced techniques, we will be dealing with three different kinds of ensemble learning:</p>
			<ul>
				<li>Bagging</li>
				<li>Boosting</li>
				<li>Stacking/blending</li>
			</ul>
			<p>Before we deal with each of them, there are some basic dynamics of these advanced ensemble learning techniques that need to be deciphered. As described at the beginning of the chapter, the essence of ensemble learning is in combining individual models to form a superior model. There are some subtle nuances in the way the superior model is generated in the advanced techniques. In these techniques, the individual models or learners generate predictions and those predictions are used to form the final predictions. The individual models or learners, which generate the first set of predictions, are called <strong class="bold">base</strong> <strong class="bold">learners</strong> or <strong class="bold">base</strong> <strong class="bold">estimators</strong> and the model, which is a combination of the predictions of the base learners, is called the <strong class="bold">meta</strong> <strong class="bold">learner</strong> or <strong class="bold">meta estimator</strong>. The way in which the meta learners learn from the base learners differs for each of the advanced techniques. Let's understand each of the advanced techniques in detail.</p>
			<h2 id="_idParaDest-340"><a id="_idTextAnchor339"/>Bagging</h2>
			<p>Bagging is a pseudonym for <strong class="bold">B</strong>ootstrap <strong class="bold">Agg</strong>regat<strong class="bold">ing</strong>. Before we explain how bagging works, let's describe what bootstrapping is. Bootstrapping has its etymological origins in the phrase, <em class="italic">Pulling oneself up by one's bootstrap</em>. The essence of this phrase is to make the best use of the available resources. In the statistical context, bootstrapping entails taking samples from the available dataset by replacement. Let's look at this concept with a toy example. </p>
			<p>Suppose we have a dataset consisting of 10 numbers from 1 to 10. We now need to create 4 different datasets of 10 each from the available dataset. How do we do this? This is where the concept of bootstrapping comes in handy. In this method, we take samples from the available dataset one by one and then replace the number we took before taking the next sample. We continue with this until we get a sample with the number of data points we need. </p>
			<p>As we are replacing each number after it is selected, there is a chance that we might have more than one of a given data point in a sample. This is explained by the following figure:</p>
			<div>
				<div id="_idContainer731" class="IMG---Figure">
					<img src="Images/B15019_15_25.jpg" alt="Figure 15.25: Bootstrapping&#13;&#10;" width="954" height="533"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.25: Bootstrapping</p>
			<p>Now that we have understood bootstrapping, let's apply this concept to a machine learning context. Earlier in the chapter, we discussed that ensemble learning helps in reducing the variance of predictions. One way that variance could be reduced is by averaging out the predictions from multiple learners. In bagging, multiple subsets of the data are created using bootstrapping. On each of these subsets of data, a base learner is fitted and the predictions generated. These predictions from all the base learners are then averaged to get the meta learner or the final predictions. </p>
			<p>When implementing bagging, we use a function called <strong class="source-inline">BaggingClassifier()</strong>, which is available in the <strong class="source-inline">Scikit learn</strong> library. Some of the important arguments that are provided when creating an ensemble model include the following:</p>
			<ul>
				<li><strong class="source-inline">base_estimator</strong>: This argument is to define the base estimator to be used.</li>
				<li><strong class="source-inline">n_estimator</strong>: This argument defines the number of base estimators that will be used in the ensemble.</li>
				<li><strong class="source-inline">max_samples</strong>: The maximum size of the bootstrapped sample for fitting the base estimator is defined using this argument. This is represented as a proportion (0.8, 0.7, and so on).</li>
				<li><strong class="source-inline">max_features</strong>: When fitting multiple individual learners, it has been found that randomly selecting the features to be used in each dataset results in superior performance. The <strong class="source-inline">max_features</strong> argument indicates the number of features to be used. For example, if there were 10 features in the dataset and the <strong class="source-inline">max_features</strong> argument was to be defined as 0.8, then only 8 (0.8 x 10) features would be used to fit a model using the base learner. </li>
			</ul>
			<p>Let's explore ensemble learning with bagging in <em class="italic">Exercise 15.05</em>, <em class="italic">Ensemble Learning Using Bagging</em>.</p>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor340"/>Exercise 15.05: Ensemble Learning Using Bagging</h2>
			<p>In this exercise, we will implement an ensemble model using bagging. The individual learner we will select will be random forest:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Execute all the steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, up until the splitting of the dataset into train and test sets.</li>
				<li>Define the base learner, which will be a random forest classifier:<p class="source-code"># Defining the base learner</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">bl1 = RandomForestClassifier(random_state=123)</p></li>
				<li>Having defined the individual learner, we can now construct the ensemble model using the <strong class="source-inline">BaggingClassifier()</strong> function. This is implemented by the following code snippet:<p class="source-code"># Creating the bagging meta learner</p><p class="source-code">from sklearn.ensemble import BaggingClassifier</p><p class="source-code">baggingLearner = \</p><p class="source-code">BaggingClassifier(base_estimator=bl1, n_estimators=10, \</p><p class="source-code">                  max_samples=0.8, max_features=0.7)</p><p>The arguments that we have given are arbitrary values. The optimal values have to be identified using experimentation.</p></li>
				<li>Fit the training set on the ensemble model:<p class="source-code"># Fitting the model using the meta learner</p><p class="source-code">model = baggingLearner.fit(X_train, y_train)</p></li>
				<li>Generate the predictions from the ensemble model on the test set:<p class="source-code"># Predicting on the test set using the model</p><p class="source-code">pred = model.predict(X_test)</p></li>
				<li>Generate a confusion matrix for the predictions:<p class="source-code"># Printing the confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">print(confusion_matrix(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer732" class="IMG---Figure"><img src="Images/B15019_15_26.jpg" alt="Figure 15.26: Confusion matrix&#13;&#10;" width="1042" height="89"/></div><p class="figure-caption">Figure 15.26: Confusion matrix</p></li>
				<li>Generate the classification report:<p class="source-code"># Printing the classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer733" class="IMG---Figure"><img src="Images/B15019_15_27.jpg" alt="Figure 15.27: Classification report&#13;&#10;" width="1224" height="333"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.27: Classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iYcHZ3">https://packt.live/3iYcHZ3</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hfRCbZ">https://packt.live/3hfRCbZ</a>.</p>
			<p>In this exercise, we implemented ensemble learning using bagging. As you can see from the classification report, we have a slightly lower result (<strong class="source-inline">0.90</strong>) than what we got from the averaging and max voting methods (<strong class="source-inline">0.91</strong>). However, looking at the results from a business perspective, we can see some big swings in the recall values for both of the classes. We see that the recall value of creditworthy customers is around 87%. That means that there is around <strong class="source-inline">13%</strong> of lost opportunities. We can also see that the risks in terms of unworthy customers being identified are also being reduced. The recall value for identifying unworthy customers is around 93%, which means that only around 7% of the unworthy customers are wrongly classified as creditworthy. So, for a business that is more risk-averse, it is recommended that you use this model.</p>
			<h2 id="_idParaDest-342"><a id="_idTextAnchor341"/>Boosting</h2>
			<p>The bagging technique, which we discussed in the last section, can be termed as a parallel learning technique. This means that each base learner is fit independently of the other and their predictions are aggregated. Unlike the bagging method, boosting works in a sequential manner. It works on the principle of correcting the prediction errors of each base learner. The base learners are fit sequentially one after the other. A base learner tries to correct the error generated by the previous learner and this process continues until a superior meta learner is created. The steps involved in the boosting technique are as follows:</p>
			<ol>
				<li value="1">A base learner is fit on a subset of the dataset.</li>
				<li>Once the model is fit, predictions are made on the entire dataset.</li>
				<li>The errors in the predictions are identified by comparing them with the actual labels.</li>
				<li>Those examples that generated the wrong predictions are given larger weights.</li>
				<li>Another base learner is fit on the dataset where the weights of the wrongly predicted examples in the previous step are altered.</li>
				<li>This base learner tries to correct the errors of the earlier model and gives their predictions.</li>
				<li><em class="italic">Steps 4</em>, <em class="italic">5</em>, and <em class="italic">6</em> are repeated until a strong meta learner is generated.</li>
			</ol>
			<p>When implementing the boosting technique, one method we can use is <strong class="source-inline">AdaBoostClassifier()</strong> in scikit-learn. Like the bagging estimator, some of the important arguments for the <strong class="source-inline">AdaBoostClassifier()</strong> method are <strong class="source-inline">base_estimator</strong> and <strong class="source-inline">n_estimators</strong>. We will now implement the boosting algorithm in <em class="italic">Exercise 15.06</em>, <em class="italic">Ensemble Learning Using Boosting</em>.</p>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor342"/>Exercise 15.06: Ensemble Learning Using Boosting</h2>
			<p>In this exercise, we will implement an ensemble model using boosting. The individual learner we will select will be the logistic regression model. The steps for implementing this algorithm are very similar to the bagging algorithm:</p>
			<ol>
				<li value="1">Open a new Colab notebook file.</li>
				<li>Execute all of the steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, up until the splitting of the dataset into train and test sets.</li>
				<li>Define the base learner, which will be a logistic regression classifier:<p class="source-code"># Defining the base learner</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">bl1 = LogisticRegression(random_state=123)</p></li>
				<li>Having defined the individual learner, we can now construct the ensemble model using the <strong class="source-inline">AdaBoostClassifier()</strong> function. This is implemented by the following code snippet:<p class="source-code"># Define the boosting meta learner</p><p class="source-code">from sklearn.ensemble import AdaBoostClassifier</p><p class="source-code">boosting = AdaBoostClassifier(base_estimator=bl1, \</p><p class="source-code">                              n_estimators=200)</p><p>The arguments that we have given are arbitrary values. The optimal values have to be identified using experimentation.</p></li>
				<li>Fit the training set on the ensemble model:<p class="source-code"># Fitting the model on the training set</p><p class="source-code">model = boosting.fit(X_train, y_train)</p></li>
				<li>Generate the predictions from the ensemble model on the test set:<p class="source-code"># Getting the predictions from the boosting model</p><p class="source-code">pred = model.predict(X_test)</p></li>
				<li>Generate a confusion matrix for the predictions:<p class="source-code"># Printing the confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">print(confusion_matrix(y_test, pred))</p><p>You should get a similar output to the following:</p><div id="_idContainer734" class="IMG---Figure"><img src="Images/B15019_15_28.jpg" alt="Figure 15.28: Confusion matrix&#13;&#10;" width="1013" height="82"/></div><p class="figure-caption">Figure 15.28: Confusion matrix</p></li>
				<li>Generate the classification report:<p class="source-code"># Printing the classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get an output similar to the following:</p><div id="_idContainer735" class="IMG---Figure"><img src="Images/B15019_15_29.jpg" alt="Figure 15.29: Classification report&#13;&#10;" width="1158" height="336"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.29: Classification report</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3iUas96">https://packt.live/3iUas96</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2EkkW2J">https://packt.live/2EkkW2J</a>.</p>
			<p>In this exercise, we implemented the ensemble learning model using boosting. As you can see from the classification report, we got very similar results (0.90) to the bagging method, which we implemented in the previous exercise. From a business context, the results are more balanced compared to the results that we got for the bagging method (the recall values of 0.93 and 0.87). Here, we can see that the recall value of the unworthy customers (90%) and that of creditworthy customers (91%) are quite close to each other, which indicates a very balanced result.</p>
			<h2 id="_idParaDest-344"><a id="_idTextAnchor343"/>Stacking</h2>
			<p>Stacking, in principle, works in a similar way to bagging and boosting in that it combines base learners to form a meta learner. However, the approach for getting the meta learners from the base learners differs substantially in stacking. In stacking, the meta learner is fit on the predictions made by the base learners. The stacking algorithm can be explained as follows:</p>
			<ol>
				<li value="1">The training set is split into multiple parts, say, five parts.</li>
				<li>A base learner (say, KNN) is fitted on four parts of the training set and then predicted on the fifth set. This process continues until the base learner predicts on each of the five parts of the training set. All the predictions, which are so generated, are collated to get the predictions for the complete training set.</li>
				<li>The same base learner is then used to generate predictions on the test set  as well.</li>
				<li><em class="italic">Steps 2</em> and <em class="italic">3</em> are then repeated with a different base learner (say, random forest).</li>
				<li>Next enters a new model, which acts as the meta learner (say, logistic regression).</li>
				<li>The meta learner is fit on the predictions generated on the training set by the base learners.</li>
				<li>Once the meta learner is fit on the training set, the same model is used to predict on the predictions generated on the test set by the base learners.<p>All these processes are explained pictorially as follows:</p><div id="_idContainer736" class="IMG---Figure"><img src="Images/B15019_15_30.jpg" alt="Figure 15.30: Process of stacking&#13;&#10;" width="1570" height="1187"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.30: Process of stacking</p>
			<p>The implementation of stacking is done through a function called <strong class="source-inline">StackingClassifier()</strong>. This is available from a package called <strong class="source-inline">mlxtend</strong>. The various arguments for this function are the models that we assign as base learners and the model assigned as a meta learner. The implementation of the stacking technique is executed in <em class="italic">Exercise 15.07</em>, <em class="italic">Ensemble Learning Using Stacking</em>.</p>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor344"/>Exercise 15.07: Ensemble Learning Using Stacking</h2>
			<p>In this exercise, we will implement an ensemble model using stacking. The individual learners we will use are KNN and random forest. Our meta learner will be logistic regression:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Execute all of the steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data</em>, up until the splitting of the dataset into train and test sets.</li>
				<li>Import the base learners and the meta learner. In this implementation, we will be using two base learners (KNN and random forest). The meta learner will be logistic regression:<p class="source-code"># Importing the meta learner and base learners</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">bl1 = KNeighborsClassifier(n_neighbors=5)</p><p class="source-code">bl2 = RandomForestClassifier(random_state=123)</p><p class="source-code">ml = LogisticRegression(random_state=123)</p></li>
				<li>Once the base learners and meta learner are defined, we will proceed to create the stacking classifier:<p class="source-code"># Creating the stacking classifier</p><p class="source-code">from mlxtend.classifier import StackingClassifier</p><p class="source-code">stackclf = StackingClassifier(classifiers=[bl1, bl2],\</p><p class="source-code">                              meta_classifier=ml)</p><p>The arguments that we have been given are the two base learners and the meta learner.</p></li>
				<li>Fit the training set on the ensemble model:<p class="source-code"># Fitting the model on the training set</p><p class="source-code">model = stackclf.fit(X_train, y_train)</p></li>
				<li>Generate the predictions from the ensemble model on the test set:<p class="source-code"># Generating predictions on test set</p><p class="source-code">pred = model.predict(X_test)</p></li>
				<li>Generate the classification report:<p class="source-code"># Printing the classification report</p><p class="source-code">from sklearn.metrics import classification_report</p><p class="source-code">print(classification_report(y_test, pred))</p><p>You should get a similar output to the following:</p><div id="_idContainer737" class="IMG---Figure"><img src="Images/B15019_15_31.jpg" alt="Figure 15.31: Classification report&#13;&#10;" width="1157" height="333"/></div><p class="figure-caption">Figure 15.31: Classification report</p></li>
				<li>Generate a confusion matrix for the predictions:<p class="source-code"># Printing the confusion matrix</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">print(confusion_matrix(y_test, pred))</p><p>You should get a similar output to the following:</p><div id="_idContainer738" class="IMG---Figure"><img src="Images/B15019_15_32.jpg" alt="Figure 15.32: Confusion matrix&#13;&#10;" width="1131" height="89"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.32: Confusion matrix</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31dZRjy">https://packt.live/31dZRjy</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Fwsdgf">https://packt.live/2Fwsdgf</a>.</p>
			<p>In this exercise, we implemented the ensemble learning model using stacking. As you can see from the classification report, we did not get any improvements over the benchmark model using the stacking classifier. The possible reasons for this could range from the choice of base learners and meta learners that we used to the size of the dataset. It should be noted that not all ensemble learners will be the desired silver bullet. There will be many instances where many advanced methods will fail to improve performance. This exercise proved to be one such instance. However, the secret recipe for getting the most optimal method, which boosts performance, is intense experimentation. That is what will yield results. After all, in machine learning, there are no free lunches.</p>
			<p>From a business context, the results have huge implications from a revenue perspective as a large number of creditworthy customers, <strong class="source-inline">16%</strong> (<strong class="source-inline">100%</strong> - <strong class="source-inline">84%</strong>), have been tagged as unworthy. The risk side of recall is quite good with a higher percentage (<strong class="source-inline">92%</strong>) of unworthy customers identified.</p>
			<p>So far, we have seen three advanced techniques for ensemble learning. Let's now apply all of these techniques to the credit card dataset and then pick the best method. We will do this in <em class="italic">Activity 15.02</em>, <em class="italic">Ensemble Model Using the Averaging Technique</em>.</p>
			<h2 id="_idParaDest-346"><a id="_idTextAnchor345"/>Activity 15.02: Comparison of Advanced Ensemble Techniques</h2>
			<p>Scenario: You have tried the benchmark model on the credit card dataset and have got some benchmark metrics. Having learned some advanced ensemble techniques, you want to determine which technique to use for the credit card approval dataset.</p>
			<p>In this activity, you will use all three advanced techniques and compare the results before selecting your final technique.</p>
			<p>The steps are as follows:</p>
			<ol>
				<li value="1">Open a new Colab notebook.</li>
				<li>Implement all steps from <em class="italic">Exercise 15.01</em>, <em class="italic">Loading, Exploring, and Cleaning the Data,</em> up until the splitting of the dataset into train and test sets.</li>
				<li>Implement the bagging technique with the base learner as the logistic regression model. In the bagging classifier, define <strong class="source-inline">n_estimators = 15</strong>, <strong class="source-inline">max_samples = 0.7</strong>, and <strong class="source-inline">max_features = 0.8</strong>. Fit the model on the training set, generate the predictions, and print the confusion matrix and the classification report.</li>
				<li>Implement boosting with random forest as the base learner. In the <strong class="source-inline">AdaBoostClassifier</strong>, define <strong class="source-inline">n_estimators = 300</strong>. Fit the model on the training set, generate the predictions, and print the confusion matrix and classification report.</li>
				<li>Implement the stacking technique. Make the KNN and logistic regression models base learners and random forest a meta learner. Fit the model on the training set, generate the predictions, and print the confusion matrix and classification report.</li>
				<li>Compare the results across all three techniques and select the best technique.</li>
				<li>Output: You should get an output similar to the following for all three methods. Please note you will not get exact values as output due to variability in the prediction process.<p>The output for bagging would be as follows:</p><div id="_idContainer739" class="IMG---Figure"><img src="Images/B15019_15_33.jpg" alt="Figure 15.33: Output for bagging&#13;&#10;" width="1131" height="460"/></div></li>
			</ol>
			<p class="figure-caption">Figure 15.33: Output for bagging</p>
			<p>The output for boosting would be as follows:</p>
			<div>
				<div id="_idContainer740" class="IMG---Figure">
					<img src="Images/B15019_15_34.jpg" alt="Figure 15.34: Output for boosting&#13;&#10;" width="1169" height="419"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.34: Output for boosting</p>
			<p>The output for stacking would be as follows:</p>
			<div>
				<div id="_idContainer741" class="IMG---Figure">
					<img src="Images/B15019_15_35.jpg" alt="Figure 15.35: Output for stacking&#13;&#10;" width="1212" height="410"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.35: Output for stacking</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found here: <a href="https://packt.live/2GbJloz">https://packt.live/2GbJloz</a>.</p>
			<p>In this activity, we implemented all three advanced ensemble techniques on the credit card dataset. Based on the metrics, we found that boosting (<strong class="source-inline">0.91</strong>) has better results than bagging (<strong class="source-inline">0.90</strong>) and stacking (<strong class="source-inline">0.87</strong>). We, therefore, select the boosting algorithm to boost the performance of our models. From a business perspective, the boosting algorithm has generated more balanced results where the recall value of both creditworthy and not creditworthy customers (91%) is similar.</p>
			<h1 id="_idParaDest-347"><a id="_idTextAnchor346"/>Summary</h1>
			<p>In this chapter, we learned about various techniques of ensemble learning. Let's summarize our learning in this chapter.</p>
			<p>At the beginning of the chapter, we were introduced to the concepts of variance and bias and we learned that ensemble learning is a technique that aims to combine individual models to create a superior model, thereby reducing variance and bias and improving performance. To further explore different techniques of ensemble learning, we downloaded the credit card approval dataset. We also fitted a benchmark model using logistic regression.</p>
			<p>In the subsequent sections, we were introduced to six different techniques of ensemble learning; three of them being simple techniques and the remaining three being advanced techniques. The averaging method creates an ensemble by combining the predictions of base learners and averaging the prediction probabilities. We were able to get better results than the benchmark model using this technique. The weighted averaging method is similar to the averaging method. The difference is in the way the predictions are combined. In this method, arbitrary weights are applied to the predictions of individual learners to get the final prediction. Max voting is a technique that arrives at final predictions based on the votes of the majority of the base learners. </p>
			<p>Ensemble learning with bagging leverages bootstrapping techniques. The base learners are fitted on bootstrapped datasets and the results are aggregated to get the meta learner. Boosting is a sequential learner, which works on the principle of error correction of the base learners. We found that boosting techniques produced some superior results for our context. We used the stacking technique to generate the final predictions by learning from the output of the predictions by the base learners. </p>
			<p>The objective of this chapter was to equip you with a repertoire of skills aimed at boosting the performance of your machine learning models. However, it should be noted that there are no silver bullets among machine learning techniques. Not all techniques will work in all scenarios. The secret sauce, which will help you on your journey to being a good data scientist, is the rigor of experimentation with different techniques, use cases, and datasets.</p>
			<p>Having learned a set of tools aimed at boosting performance, you are now well prepared to apply these on a range of real-world projects. This chapter marks the end of the book and is a good opportunity for you to reflect on all the skills you have learned in working your way through the different chapters. However, there are still three bonus chapters available to you at <a href="https://packt.live/2ZagB9y">https://packt.live/2ZagB9y</a>.</p>
		</div>
		<div>
			<div id="_idContainer743" class="Content">
			</div>
		</div>
	</div>
<div id="sbo-rt-content"><nav id="toc" epub:type="toc">
		<h2>Contents</h2>
			<ol>
				<li><a href="B15019_FM_Final_SZ_ePub.xhtml#_idParaDest-1">The Data Science Workshop</a></li>
				<li><a href="B15019_FM_Final_SZ_ePub.xhtml#_idParaDest-2">Second Edition</a></li>
				<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-3">Preface</a>
					<ol>
						<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-4">About the Book</a>
							<ol>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-5">Audience</a></li>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-6">About the Chapters</a></li>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-7">Conventions</a></li>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-8">Code Presentation</a></li>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-9">Setting up Your Environment</a>
									<ol>
										<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-10">How to Set Up Google Colab</a></li>
										<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-11">How to Use Google Colab</a></li>
									</ol>
								</li>
								<li><a href="B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-12">Accessing the Code Files</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-13">1. Introduction to Data Science in Python</a>
					<ol>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-14">Introduction</a></li>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-15">Application of Data Science</a>
							<ol>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-16">What Is Machine Learning?</a>
									<ol>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-17">Supervised Learning</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-18">Unsupervised Learning</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-19">Reinforcement Learning</a></li>
									</ol>
								</li>
							</ol>
						</li>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-20">Overview of Python</a>
							<ol>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-21">Types of Variable</a>
									<ol>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-22">Numeric Variables</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-23">Text Variables</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-24">Python List</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-25">Python Dictionary</a></li>
									</ol>
								</li>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-26">Exercise 1.01: Creating a Dictionary That Will Contain Machine Learning Algorithms</a></li>
							</ol>
						</li>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-27">Python for Data Science</a>
							<ol>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-28">The pandas Package</a>
									<ol>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-29">DataFrame and Series</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-30">CSV Files</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-31">Excel Spreadsheets</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-32">JSON</a></li>
									</ol>
								</li>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-33">Exercise 1.02: Loading Data of Different Formats into a pandas DataFrame</a></li>
							</ol>
						</li>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-34">Scikit-Learn</a>
							<ol>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-35">What Is a Model?</a>
									<ol>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-36">Model Hyperparameters</a></li>
										<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-37">The sklearn API</a></li>
									</ol>
								</li>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-38">Exercise 1.03: Predicting Breast Cancer from a Dataset Using sklearn</a></li>
								<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-39">Activity 1.01: Train a Spam Detector Algorithm</a></li>
							</ol>
						</li>
						<li><a href="B15019_01_Final_SMP_ePub.xhtml#_idParaDest-40">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-41">2. Regression</a>
					<ol>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-42">Introduction</a></li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-43">Simple Linear Regression</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-44">The Method of Least Squares</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-45">Multiple Linear Regression</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-46">Estimating the Regression Coefficients (β0, β1, β2 and β3)</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-47">Logarithmic Transformations of Variables</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-48">Correlation Matrices</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-49">Conducting Regression Analysis Using Python</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-50">Exercise 2.01: Loading and Preparing the Data for Analysis</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-51">The Correlation Coefficient</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-52">Exercise 2.02: Graphical Investigation of Linear Relationships Using Python</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-53">Exercise 2.03: Examining a Possible Log-Linear Relationship Using Python</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-54">The Statsmodels formula API</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-55">Exercise 2.04: Fitting a Simple Linear Regression Model Using the Statsmodels formula API</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-56">Analyzing the Model Summary</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-57">The Model Formula Language</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-58">Intercept Handling</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-59">Activity 2.01: Fitting a Log-Linear Model Using the Statsmodels Formula API</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-60">Multiple Regression Analysis</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-61">Exercise 2.05: Fitting a Multiple Linear Regression Model Using the Statsmodels Formula API</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-62">Assumptions of Regression Analysis</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-63">Activity 2.02: Fitting a Multiple Log-Linear Regression Model</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-64">Explaining the Results of Regression Analysis</a>
							<ol>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-65">Regression Analysis Checks and Balances</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-66">The F-test</a></li>
								<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-67">The t-test</a></li>
							</ol>
						</li>
						<li><a href="B15019_02_Final_SMP_ePub.xhtml#_idParaDest-68">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-69">3. Binary Classification</a>
					<ol>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-70">Introduction</a></li>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-71">Understanding the Business Context</a>
							<ol>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-72">Business Discovery</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-73">Exercise 3.01: Loading and Exploring the Data from the Dataset</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-74">Testing Business Hypotheses Using Exploratory Data Analysis</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-75">Visualization for Exploratory Data Analysis</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-76">Exercise 3.02: Business Hypothesis Testing for Age versus Propensity for a Term Loan</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-77">Intuitions from the Exploratory Analysis</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-78">Activity 3.01: Business Hypothesis Testing to Find Employment Status versus Propensity for Term Deposits</a></li>
							</ol>
						</li>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-79">Feature Engineering </a>
							<ol>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-80">Business-Driven Feature Engineering</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-81">Exercise 3.03: Feature Engineering – Exploration of Individual Features</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-82">Exercise 3.04: Feature Engineering – Creating New Features from Existing Ones</a></li>
							</ol>
						</li>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-83">Data-Driven Feature Engineering</a>
							<ol>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-84">A Quick Peek at Data Types and a Descriptive Summary</a></li>
							</ol>
						</li>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-85">Correlation Matrix and Visualization</a>
							<ol>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-86">Exercise 3.05: Finding the Correlation in Data to Generate a Correlation Plot Using Bank Data</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-87">Skewness of Data</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-88">Histograms</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-89">Density Plots</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-90">Other Feature Engineering Methods</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-91">Summarizing Feature Engineering</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-92">Building a Binary Classification Model Using the Logistic Regression Function</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-93">Logistic Regression Demystified</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-94">Metrics for Evaluating Model Performance</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-95">Confusion Matrix</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-96">Accuracy</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-97">Classification Report</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-98">Data Preprocessing</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-99">Exercise 3.06: A Logistic Regression Model for Predicting the Propensity of Term Deposit Purchases in a Bank</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-100">Activity 3.02: Model Iteration 2 – Logistic Regression Model with Feature Engineered Variables</a></li>
								<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-101">Next Steps</a></li>
							</ol>
						</li>
						<li><a href="B15019_03_Final_SMP_ePub.xhtml#_idParaDest-102">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-103">4. Multiclass Classification with RandomForest</a>
					<ol>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-104">Introduction</a></li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-105">Training a Random Forest Classifier</a></li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-106">Evaluating the Model's Performance</a>
							<ol>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-107">Exercise 4.01: Building a Model for Classifying Animal Type and Assessing Its Performance</a></li>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-108">Number of Trees Estimator</a></li>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-109">Exercise 4.02: Tuning n_estimators to Reduce Overfitting</a></li>
							</ol>
						</li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-110">Maximum Depth</a>
							<ol>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-111">Exercise 4.03: Tuning max_depth to Reduce Overfitting</a></li>
							</ol>
						</li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-112">Minimum Sample in Leaf</a>
							<ol>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-113">Exercise 4.04: Tuning min_samples_leaf</a></li>
							</ol>
						</li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-114">Maximum Features</a>
							<ol>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-115">Exercise 4.05: Tuning max_features</a></li>
								<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-116">Activity 4.01: Train a Random Forest Classifier on the ISOLET Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_04_Final_SZ_ePub.xhtml#_idParaDest-117">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-118">5. Performing Your First Cluster Analysis</a>
					<ol>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-119">Introduction</a></li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-120">Clustering with k-means</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-121">Exercise 5.01: Performing Your First Clustering Analysis on the ATO Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-122">Interpreting k-means Results</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-123">Exercise 5.02: Clustering Australian Postcodes by Business Income and Expenses</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-124">Choosing the Number of Clusters</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-125">Exercise 5.03: Finding the Optimal Number of Clusters</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-126">Initializing Clusters</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-127">Exercise 5.04: Using Different Initialization Parameters to Achieve a Suitable Outcome</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-128">Calculating the Distance to the Centroid</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-129">Exercise 5.05: Finding the Closest Centroids in Our Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-130">Standardizing Data</a>
							<ol>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-131">Exercise 5.06: Standardizing the Data from Our Dataset</a></li>
								<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-132">Activity 5.01: Perform Customer Segmentation Analysis in a Bank Using k-means</a></li>
							</ol>
						</li>
						<li><a href="B15019_05_Final_SZ_ePub.xhtml#_idParaDest-133">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-134">6. How to Assess Performance</a>
					<ol>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-135">Introduction</a></li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-136">Splitting Data</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-137">Exercise 6.01: Importing and Splitting Data</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-138">Assessing Model Performance for Regression Models</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-139">Data Structures – Vectors and Matrices</a>
									<ol>
										<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-140">Scalars</a></li>
										<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-141">Vectors</a></li>
										<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-142">Matrices</a></li>
									</ol>
								</li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-143">R2 Score</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-144">Exercise 6.02: Computing the R2 Score of a Linear Regression Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-145">Mean Absolute Error</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-146">Exercise 6.03: Computing the MAE of a Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-147">Exercise 6.04: Computing the Mean Absolute Error of a Second Model</a>
									<ol>
										<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-148">Other Evaluation Metrics</a></li>
									</ol>
								</li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-149">Assessing Model Performance for Classification Models</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-150">Exercise 6.05: Creating a Classification Model for Computing Evaluation Metrics</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-151">The Confusion Matrix</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-152">Exercise 6.06: Generating a Confusion Matrix for the Classification Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-153">More on the Confusion Matrix </a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-154">Precision</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-155">Exercise 6.07: Computing Precision for the Classification Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-156">Recall</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-157">Exercise 6.08: Computing Recall for the Classification Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-158">F1 Score</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-159">Exercise 6.09: Computing the F1 Score for the Classification Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-160">Accuracy</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-161">Exercise 6.10: Computing Model Accuracy for the Classification Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-162">Logarithmic Loss</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-163">Exercise 6.11: Computing the Log Loss for the Classification Model</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-164">Receiver Operating Characteristic Curve</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-165">Exercise 6.12: Computing and Plotting ROC Curve for a Binary Classification Problem</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-166">Area Under the ROC Curve</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-167">Exercise 6.13: Computing the ROC AUC for the Caesarian Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-168">Saving and Loading Models</a>
							<ol>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-169">Exercise 6.14: Saving and Loading a Model</a></li>
								<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-170">Activity 6.01: Train Three Different Models and Use Evaluation Metrics to Pick the Best Performing Model</a></li>
							</ol>
						</li>
						<li><a href="B15019_06_Final_SZ_ePub.xhtml#_idParaDest-171">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-172">7. The Generalization of Machine Learning Models</a>
					<ol>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-173">Introduction</a></li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-174">Overfitting</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-175">Training on Too Many Features</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-176">Training for Too Long</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-177">Underfitting</a></li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-178">Data</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-179">The Ratio for Dataset Splits</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-180">Creating Dataset Splits</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-181">Exercise 7.01: Importing and Splitting Data</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-182">Random State</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-183">Exercise 7.02: Setting a Random State When Splitting Data</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-184">Cross-Validation</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-185">KFold</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-186">Exercise 7.03: Creating a Five-Fold Cross-Validation Dataset</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-187">Exercise 7.04: Creating a Five-Fold Cross-Validation Dataset Using a Loop for Calls</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-188">cross_val_score</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-189">Exercise 7.05: Getting the Scores from Five-Fold Cross-Validation</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-190">Understanding Estimators That Implement CV</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-191">LogisticRegressionCV</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-192">Exercise 7.06: Training a Logistic Regression Model Using Cross-Validation</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-193">Hyperparameter Tuning with GridSearchCV</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-194">Decision Trees</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-195">Exercise 7.07: Using Grid Search with Cross-Validation to Find the Best Parameters for a Model</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-196">Hyperparameter Tuning with RandomizedSearchCV</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-197">Exercise 7.08: Using Randomized Search for Hyperparameter Tuning</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-198">Model Regularization with Lasso Regression</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-199">Exercise 7.09: Fixing Model Overfitting Using Lasso Regression</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-200">Ridge Regression</a>
							<ol>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-201">Exercise 7.10: Fixing Model Overfitting Using Ridge Regression</a></li>
								<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-202">Activity 7.01: Find an Optimal Model for Predicting the Critical Temperatures of Superconductors</a></li>
							</ol>
						</li>
						<li><a href="B15019_07_Final_SMP_ePub.xhtml#_idParaDest-203">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-204">8. Hyperparameter Tuning</a>
					<ol>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-205">Introduction</a></li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-206">What Are Hyperparameters?</a>
							<ol>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-207">Difference between Hyperparameters and Statistical Model Parameters</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-208">Setting Hyperparameters</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-209">A Note on Defaults</a></li>
							</ol>
						</li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-210">Finding the Best Hyperparameterization</a>
							<ol>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-211">Exercise 8.01: Manual Hyperparameter Tuning for a k-NN Classifier</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-212">Advantages and Disadvantages of a Manual Search</a></li>
							</ol>
						</li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-213">Tuning Using Grid Search</a>
							<ol>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-214">Simple Demonstration of the Grid Search Strategy</a></li>
							</ol>
						</li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-215">GridSearchCV</a>
							<ol>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-216">Tuning using GridSearchCV</a>
									<ol>
										<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-217">Support Vector Machine (SVM) Classifiers</a></li>
									</ol>
								</li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-218">Exercise 8.02: Grid Search Hyperparameter Tuning for an SVM</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-219">Advantages and Disadvantages of Grid Search</a></li>
							</ol>
						</li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-220">Random Search</a>
							<ol>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-221">Random Variables and Their Distributions</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-222">Simple Demonstration of the Random Search Process</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-223">Tuning Using RandomizedSearchCV</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-224">Exercise 8.03: Random Search Hyperparameter Tuning for a Random Forest Classifier</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-225">Advantages and Disadvantages of a Random Search</a></li>
								<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-226">Activity 8.01: Is the Mushroom Poisonous?</a></li>
							</ol>
						</li>
						<li><a href="B15019_08_Final_SZ_ePub.xhtml#_idParaDest-227">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-228">9. Interpreting a Machine Learning Model</a>
					<ol>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-229">Introduction</a></li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-230">Linear Model Coefficients</a>
							<ol>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-231">Exercise 9.01: Extracting the Linear Regression Coefficient </a></li>
							</ol>
						</li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-232">RandomForest Variable Importance</a>
							<ol>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-233">Exercise 9.02: Extracting RandomForest Feature Importance</a></li>
							</ol>
						</li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-234">Variable Importance via Permutation</a>
							<ol>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-235">Exercise 9.03: Extracting Feature Importance via Permutation</a></li>
							</ol>
						</li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-236">Partial Dependence Plots</a>
							<ol>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-237">Exercise 9.04: Plotting Partial Dependence</a></li>
							</ol>
						</li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-238">Local Interpretation with LIME </a>
							<ol>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-239">Exercise 9.05: Local Interpretation with LIME</a></li>
								<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-240">Activity 9.01: Train and Analyze a Network Intrusion Detection Model</a></li>
							</ol>
						</li>
						<li><a href="B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-241">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-242">10. Analyzing a Dataset</a>
					<ol>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-243">Introduction</a></li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-244">Exploring Your Data</a></li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-245">Analyzing Your Dataset</a>
							<ol>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-246">Exercise 10.01: Exploring the Ames Housing Dataset with Descriptive Statistics</a></li>
							</ol>
						</li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-247">Analyzing the Content of a Categorical Variable</a>
							<ol>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-248">Exercise 10.02: Analyzing the Categorical Variables from the Ames Housing Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-249">Summarizing Numerical Variables</a>
							<ol>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-250">Exercise 10.03: Analyzing Numerical Variables from the Ames Housing Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-251">Visualizing Your Data</a>
							<ol>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-252">Using the Altair API</a></li>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-253">Histogram for Numerical Variables</a></li>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-254">Bar Chart for Categorical Variables </a></li>
							</ol>
						</li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-255">Boxplots</a>
							<ol>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-256">Exercise 10.04: Visualizing the Ames Housing Dataset with Altair</a></li>
								<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-257">Activity 10.01: Analyzing Churn Data Using Visual Data Analysis Techniques</a></li>
							</ol>
						</li>
						<li><a href="B15019_10_Final_SZ_ePub.xhtml#_idParaDest-258">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-259">11. Data Preparation</a>
					<ol>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-260">Introduction</a></li>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-261">Handling Row Duplication</a>
							<ol>
								<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-262">Exercise 11.01: Handling Duplicates in a Breast Cancer Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-263">Converting Data Types</a>
							<ol>
								<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-264">Exercise 11.02: Converting Data Types for the Ames Housing Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-265">Handling Incorrect Values</a>
							<ol>
								<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-266">Exercise 11.03: Fixing Incorrect Values in the State Column</a></li>
							</ol>
						</li>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-267">Handling Missing Values</a>
							<ol>
								<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-268">Exercise 11.04: Fixing Missing Values for the Horse Colic Dataset</a></li>
								<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-269">Activity 11.01: Preparing the Speed Dating Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_11_Final_SZ_ePub.xhtml#_idParaDest-270">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-271">12. Feature Engineering</a>
					<ol>
						<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-272">Introduction</a>
							<ol>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-273">Merging Datasets</a>
									<ol>
										<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-274">The Left Join</a></li>
										<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-275">The Right Join</a></li>
									</ol>
								</li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-276">Exercise 12.01: Merging the ATO Dataset with the Postcode Data</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-277">Binning Variables</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-278">Exercise 12.02: Binning the YearBuilt Variable from the AMES Housing Dataset</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-279">Manipulating Dates</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-280">Exercise 12.03: Date Manipulation on Financial Services Consumer Complaints</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-281">Performing Data Aggregation</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-282">Exercise 12.04: Feature Engineering Using Data Aggregation on the AMES Housing Dataset</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-283">Activity 12.01: Feature Engineering on a Financial Dataset</a></li>
								<li><a href="B15019_12_Final_SMP_ePub.xhtml#_idParaDest-284">Summary</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-285">13. Imbalanced Datasets</a>
					<ol>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-286">Introduction</a></li>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-287">Understanding the Business Context</a>
							<ol>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-288">Exercise 13.01: Benchmarking the Logistic Regression Model on the Dataset</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-289">Analysis of the Result</a></li>
							</ol>
						</li>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-290">Challenges of Imbalanced Datasets</a></li>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-291">Strategies for Dealing with Imbalanced Datasets</a>
							<ol>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-292">Collecting More Data</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-293">Resampling Data</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-294">Exercise 13.02: Implementing Random Undersampling and Classification on Our Banking Dataset to Find the Optimal Result</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-295">Analysis</a></li>
							</ol>
						</li>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-296">Generating Synthetic Samples</a>
							<ol>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-297">Implementation of SMOTE and MSMOTE</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-298">Exercise 13.03: Implementing SMOTE on Our Banking Dataset to Find the Optimal Result</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-299">Exercise 13.04: Implementing MSMOTE on Our Banking Dataset to Find the Optimal Result</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-300">Applying Balancing Techniques on a Telecom Dataset</a></li>
								<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-301">Activity 13.01: Finding the Best Balancing Technique by Fitting a Classifier on the Telecom Churn Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_13_Final_SZ_ePub.xhtml#_idParaDest-302">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-303">14. Dimensionality Reduction</a>
					<ol>
						<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-304">Introduction</a>
							<ol>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-305">Business Context</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-306">Exercise 14.01: Loading and Cleaning the Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-307">Creating a High-Dimensional Dataset</a>
							<ol>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-308">Activity 14.01: Fitting a Logistic Regression Model on a HighDimensional Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-309">Strategies for Addressing High-Dimensional Datasets</a>
							<ol>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-310">Backward Feature Elimination (Recursive Feature Elimination)</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-311">Exercise 14.02: Dimensionality Reduction Using Backward Feature Elimination</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-312">Forward Feature Selection</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-313">Exercise 14.03: Dimensionality Reduction Using Forward Feature Selection</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-314">Principal Component Analysis (PCA)</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-315">Exercise 14.04: Dimensionality Reduction Using PCA</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-316">Independent Component Analysis (ICA)</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-317">Exercise 14.05: Dimensionality Reduction Using Independent Component Analysis</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-318">Factor Analysis</a></li>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-319">Exercise 14.06: Dimensionality Reduction Using Factor Analysis</a></li>
							</ol>
						</li>
						<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-320">Comparing Different Dimensionality Reduction Techniques</a>
							<ol>
								<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-321">Activity 14.02: Comparison of Dimensionality Reduction Techniques on the Enhanced Ads Dataset</a></li>
							</ol>
						</li>
						<li><a href="B15019_14_Final_SMP_ePub.xhtml#_idParaDest-322">Summary</a></li>
					</ol>
				</li>
				<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-323">15. Ensemble Learning</a>
					<ol>
						<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-324">Introduction</a></li>
						<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-325">Ensemble Learning</a>
							<ol>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-326">Variance</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-327">Bias</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-328">Business Context</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-329">Exercise 15.01: Loading, Exploring, and Cleaning the Data</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-330">Activity 15.01: Fitting a Logistic Regression Model on Credit Card Data</a></li>
							</ol>
						</li>
						<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-331">Simple Methods for Ensemble Learning</a>
							<ol>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-332">Averaging</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-333">Exercise 15.02: Ensemble Model Using the Averaging Technique</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-334">Weighted Averaging</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-335">Exercise 15.03: Ensemble Model Using the Weighted Averaging Technique</a>
									<ol>
										<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-336">Iteration 2 with Different Weights</a></li>
										<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-337">Max Voting</a></li>
									</ol>
								</li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-338">Exercise 15.04: Ensemble Model Using Max Voting</a></li>
							</ol>
						</li>
						<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-339">Advanced Techniques for Ensemble Learning</a>
							<ol>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-340">Bagging</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-341">Exercise 15.05: Ensemble Learning Using Bagging</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-342">Boosting</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-343">Exercise 15.06: Ensemble Learning Using Boosting</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-344">Stacking</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-345">Exercise 15.07: Ensemble Learning Using Stacking</a></li>
								<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-346">Activity 15.02: Comparison of Advanced Ensemble Techniques</a></li>
							</ol>
						</li>
						<li><a href="B15019_15_Final_SZ_ePub.xhtml#_idParaDest-347">Summary</a></li>
					</ol>
				</li>
			</ol>
		</nav>
		<nav epub:type="landmarks">
		<h2>Landmarks</h2>
			<ol>
				<li><a epub:type="cover" href="Images/cover.xhtml">Cover</a></li>
				<li><a epub:type="toc" href="B15019_FM_Final_SZ_ePub.xhtml#_idContainer004">Table of Contents</a></li>
			</ol>
		</nav>
	</div></body></html>