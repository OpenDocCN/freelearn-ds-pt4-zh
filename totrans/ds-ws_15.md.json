["```py\n    import pandas as pd\n    ```", "```py\n    #Load data from the GitHub repository\n    filename = 'https://raw.githubusercontent.com'\\\n               '/PacktWorkshops/The-Data-Science-Workshop'\\\n               '/master/Chapter15/Dataset/crx.data'\n    ```", "```py\n    credData = pd.read_csv(filename,sep= \",\",\\\n                           header = None,\\\n                           na_values =  \"?\")\n    credData.head()\n    ```", "```py\n    # Changing the Classes to 1 & 0\n    credData.loc[credData[15] == '+' , 15] = 1\n    credData.loc[credData[15] == '-' , 15] = 0\n    credData.head()\n    ```", "```py\n    .loc() was used to locate the fifteenth column and replace the + and - values with 1 and 0, respectively.\n    ```", "```py\n    # Finding number of null values in the data set\n    credData.isnull().sum()\n    ```", "```py\n    # Printing Shape and data types\n    print('Shape of raw data set',credData.shape)\n    print('Data types of data set',credData.dtypes)\n    ```", "```py\n    # Dropping all the rows with na values\n    newcred = credData.dropna(axis = 0)\n    newcred.shape\n    ```", "```py\n    (653, 16)\n    ```", "```py\n    # Verifying no null values exist\n    newcred.isnull().sum()\n    ```", "```py\n    \"\"\"\n    Separating the categorical variables to \n    make dummy variables\n    \"\"\"\n    credCat = pd.get_dummies(newcred[[0,3,4,5,6,8,9,11,12]])\n    ```", "```py\n    # Separating the numerical variables\n    credNum = newcred[[1,2,7,10,13,14]]\n    ```", "```py\n    \"\"\"\n    Making the X variable which is a concatenation \n    of categorical and numerical data\n    \"\"\"\n    X = pd.concat([credCat,credNum],axis = 1)\n    print(X.shape)\n    # Separating the label as y variable\n    y = pd.Series(newcred[15], dtype=\"int\")\n    print(y.shape)\n    ```", "```py\n    (653, 46)\n    (653,)\n    ```", "```py\n    # Normalizing the data sets\n    # Import library function\n    from sklearn import preprocessing\n    # Creating the scaling function\n    minmaxScaler = preprocessing.MinMaxScaler()\n    # Transforming with the scaler function\n    X_tran = pd.DataFrame(minmaxScaler.fit_transform(X))\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    # Splitting the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split\\\n                                       (X_tran, y, test_size=0.3,\\\n                                        random_state=123)\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    model1 = LogisticRegression(random_state=123)\n    model2 = KNeighborsClassifier(n_neighbors=5)\n    model3 = RandomForestClassifier(n_estimators=500)\n    ```", "```py\n    # Fitting all three models on the training data\n    model1.fit(X_train,y_train)\n    model2.fit(X_train,y_train)\n    model3.fit(X_train,y_train)\n    ```", "```py\n    \"\"\"\n    Predicting probabilities of each model \n    on the test set\n    \"\"\"\n    pred1=model1.predict_proba(X_test)\n    pred2=model2.predict_proba(X_test)\n    pred3=model3.predict_proba(X_test)\n    ```", "```py\n    \"\"\"\n    Calculating the ensemble prediction by \n    averaging three base model predictions\n    \"\"\"\n    ensemblepred=(pred1+pred2+pred3)/3\n    ```", "```py\n    # Displaying first 4 rows of the ensemble predictions\n    ensemblepred[0:4,:]\n    ```", "```py\n    # Printing the order of classes for each model\n    print(model1.classes_)\n    print(model2.classes_)\n    print(model3.classes_)\n    ```", "```py\n    import numpy as np\n    pred = np.argmax(ensemblepred,axis = 1)\n    pred\n    ```", "```py\n    # Generating confusion matrix\n    from sklearn.metrics import confusion_matrix\n    confusionMatrix = confusion_matrix(y_test, pred)\n    print(confusionMatrix)\n    ```", "```py\n    # Generating classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    \"\"\"\n    Calculating the ensemble prediction by applying \n    weights for each prediction\n    \"\"\"\n    ensemblepred=(pred1 *0.60 + pred2 * 0.20 + pred3 * 0.20)\n    ```", "```py\n    # Displaying first 4 rows of the ensemble predictions\n    ensemblepred[0:4,:]\n    ```", "```py\n    # Printing the order of classes for each model\n    print(model1.classes_)\n    print(model2.classes_)\n    print(model3.classes_)\n    ```", "```py\n    import numpy as np\n    pred = np.argmax(ensemblepred,axis = 1)\n    pred\n    ```", "```py\n    # Generating confusion matrix\n    from sklearn.metrics import confusion_matrix\n    confusionMatrix = confusion_matrix(y_test, pred)\n    print(confusionMatrix)\n    ```", "```py\n    # Generating classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    \"\"\"\n    Calculating the ensemble prediction by applying \n    weights for each prediction\n    \"\"\"\n    ensemblepred=(pred1 *0.70+pred2 * 0.15+pred3 * 0.15)\n    ```", "```py\n    # Generating predictions from probabilities\n    import numpy as np\n    pred = np.argmax(ensemblepred,axis = 1)\n    ```", "```py\n    # Generating confusion matrix\n    from sklearn.metrics import confusion_matrix\n    confusionMatrix = confusion_matrix(y_test, pred)\n    print(confusionMatrix)\n    ```", "```py\n    # Generating classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    \"\"\"\n    Defining the voting classifier and three \n    individual learners\n    \"\"\"\n    from sklearn.ensemble import VotingClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    # Defining the models\n    model1 = LogisticRegression(random_state=123)\n    model2 = KNeighborsClassifier(n_neighbors=5)\n    model3 = RandomForestClassifier(n_estimators=500)\n    ```", "```py\n    # Defining the ensemble model using VotingClassifier\n    model = VotingClassifier(estimators=[('lr', model1),\\\n                            ('knn', model2),('rf', model3)],\\\n                             voting= 'hard')\n    ```", "```py\n    # Fitting the model on the training set\n    model.fit(X_train,y_train)\n    ```", "```py\n    \"\"\"\n    Predicting accuracy on the test set using \n    .score() function\n    \"\"\"\n    model.score(X_test,y_test)\n    ```", "```py\n    0.9081632653061225\n    ```", "```py\n    # Generating the predictions on the test set\n    preds = model.predict(X_test)\n    ```", "```py\n    # Printing the confusion matrix\n    from sklearn.metrics import confusion_matrix\n    # Confusion matrix for the test set\n    print(confusion_matrix(y_test, preds))\n    ```", "```py\n    # Printing the classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, preds))\n    ```", "```py\n    # Defining the base learner\n    from sklearn.ensemble import RandomForestClassifier\n    bl1 = RandomForestClassifier(random_state=123)\n    ```", "```py\n    # Creating the bagging meta learner\n    from sklearn.ensemble import BaggingClassifier\n    baggingLearner = \\\n    BaggingClassifier(base_estimator=bl1, n_estimators=10, \\\n                      max_samples=0.8, max_features=0.7)\n    ```", "```py\n    # Fitting the model using the meta learner\n    model = baggingLearner.fit(X_train, y_train)\n    ```", "```py\n    # Predicting on the test set using the model\n    pred = model.predict(X_test)\n    ```", "```py\n    # Printing the confusion matrix\n    from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(y_test, pred))\n    ```", "```py\n    # Printing the classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    # Defining the base learner\n    from sklearn.linear_model import LogisticRegression\n    bl1 = LogisticRegression(random_state=123)\n    ```", "```py\n    # Define the boosting meta learner\n    from sklearn.ensemble import AdaBoostClassifier\n    boosting = AdaBoostClassifier(base_estimator=bl1, \\\n                                  n_estimators=200)\n    ```", "```py\n    # Fitting the model on the training set\n    model = boosting.fit(X_train, y_train)\n    ```", "```py\n    # Getting the predictions from the boosting model\n    pred = model.predict(X_test)\n    ```", "```py\n    # Printing the confusion matrix\n    from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(y_test, pred))\n    ```", "```py\n    # Printing the classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    # Importing the meta learner and base learners\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    bl1 = KNeighborsClassifier(n_neighbors=5)\n    bl2 = RandomForestClassifier(random_state=123)\n    ml = LogisticRegression(random_state=123)\n    ```", "```py\n    # Creating the stacking classifier\n    from mlxtend.classifier import StackingClassifier\n    stackclf = StackingClassifier(classifiers=[bl1, bl2],\\\n                                  meta_classifier=ml)\n    ```", "```py\n    # Fitting the model on the training set\n    model = stackclf.fit(X_train, y_train)\n    ```", "```py\n    # Generating predictions on test set\n    pred = model.predict(X_test)\n    ```", "```py\n    # Printing the classification report\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, pred))\n    ```", "```py\n    # Printing the confusion matrix\n    from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(y_test, pred))\n    ```"]