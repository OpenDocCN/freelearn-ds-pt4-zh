- en: 15\. Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to describe ensemble learning and
    apply different ensemble learning techniques to your dataset. You will also be
    able to fit a dataset on a model and analyze the results after ensemble learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using the credit card application dataset, where
    we will try to predict whether a credit card application will be approved.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned various techniques, such as the backward
    elimination technique, factor analysis, and so on, that helped us to deal with
    high-dimensional datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will further enhance our repertoire of skills with another
    set of techniques, called **ensemble learning**, in which we will be dealing with
    different ensemble learning techniques such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning, as the name denotes, is a method that combines several machine
    learning models to generate a superior model, thereby decreasing variability/variance
    and bias, and boosting performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explore what ensemble learning is, let''s look at the concepts of
    bias and variance with the help of the classical bias-variance quadrant, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1: Bias-variance quadrant'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.1: Bias-variance quadrant'
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variance is the measure of how spread out data is. In the context of machine
    learning, models with high variance imply that the predictions generated on the
    same test set will differ considerably when different training sets are used to
    fit the model. The underlying reason for high variability could be attributed
    to the model being attuned to specific nuances of training data rather than generalizing
    the relationship between input and output. Ideally, we want every machine learning
    model to have low variance.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias is the difference between the ground truth and the average value of our
    predictions. A low bias will indicate that the predictions are very close to the
    actual values. A high bias implies that the model has oversimplified the relationship
    between the inputs and outputs, leading to high error rates on test sets, which
    again is an undesirable outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15.1* helps us to visualize the trade-off between bias and variance.
    The top-left corner is the depiction of a scenario where the bias is high, and
    the variance is low. The top-right quadrant displays a scenario where both bias
    and variance are high. From the figure, we can see that when the bias is high,
    it is further away from the truth, which in this case, is the *bull''s eye*. The
    presence of variance is manifested as whether the arrows are spread out or congregated
    in one spot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensemble models combine many weaker models that differ in variance and bias,
    thereby creating a better model, outperforming the individual weaker models. Ensemble
    models exemplify the adage *the wisdom of the crowds*. In this chapter, we will
    learn about different ensemble techniques, which can be classified into two types,
    that is, simple and advanced techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2: Different ensemble learning methods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.2: Different ensemble learning methods'
  prefs: []
  type: TYPE_NORMAL
- en: Business Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are working in the credit card division of your bank. The operations head
    of your company has requested your help in determining whether a customer is creditworthy
    or not. You have been provided with credit card operations data.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains credit card applications with around 15 variables. The
    variables are a mix of continuous and categorical data pertaining to credit card
    operations. The label for the dataset is a flag, which indicates whether the application
    has been approved or not.
  prefs: []
  type: TYPE_NORMAL
- en: You want to fit some benchmark models and try some ensemble learning methods
    on the dataset to address the problem and come up with a tool for predicting whether
    or not a given customer should be approved for their credit application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.01: Loading, Exploring, and Cleaning the Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will download the credit card dataset, load it into our
    Colab notebook, and perform a few basic explorations. In addition, we will also
    clean the dataset to remove unwanted characters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that we will be using in this exercise was sourced from the UCI
    Machine Learning Repository: [https://packt.live/39NCgZ2](https://packt.live/39NCgZ2).
    You can download the dataset from our GitHub at [https://packt.live/3018OdD](https://packt.live/3018OdD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, import `pandas` into your Colab notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, set the path of the GitHub repository where `crx.data` is uploaded, as
    mentioned in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the file using the `pd.read_csv()` function from the `pandas` data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `pd.read_csv()` function's arguments are the filename as a string and the
    limit separator of a CSV file, which is `,`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are no headers for the dataset; we specifically mention this using the
    `header = None` command.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We replace the missing values represented as `?` in the dataset as `na` values
    using the `na_values = "?"` argument. This replacement is for ease of further
    processing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After reading the file, print the data frame using the `.head()` function.
    You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.3: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.3: Loading data into the Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Change the classes to `1` and `0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you notice in the dataset, the classes represented in column `15` are special
    characters: `+` for approved and `-` for not approved. You need to change this
    to numerical values of `1` for approved and `0` for not approved, as shown in
    the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.4: Data frame after replacing special characters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Find the number of `null` values in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll now find the number of `null` values in each of the features using the
    `.isnull()` function. The `.sum()` function sums up all such null values across
    each of the columns in the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is represented in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.5: Summarizing null values in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.5: Summarizing null values in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As seen from the preceding output, there are many columns with `null` values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, print the shape and data types of each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.6: Shape and data types of each column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.6: Shape and data types of each column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Remove the rows with `na` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to clean the dataset, let''s remove all the rows with `na` values
    using the `.dropna()` function with the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, around 37 rows that, which had `na` values, were removed. In
    the code snippet, we define `axis = 0` in order to denote that the dropping of
    `na` values should be done along the rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify that no `null` values exist:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.7: Verifying that no null values are present'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.7: Verifying that no null values are present'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, make dummy values from the categorical variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As you can see from the data types, there are many variables with categorical
    values. These have to be converted to dummy values using the `pd.get_dummies()`
    function. This is done using the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Separate the numerical variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will also be separating the numerical variables from the original dataset
    to concatenate them with the dummy variables. This step is done as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can view these new DataFrames by running the commands `credCat` and `credNum`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the `X` and `y` variables. The dummy variables and the numerical variables
    will now be concatenated to form the `X` variable. The `y` variable will be created
    separately by taking the labels of the dataset. Let''s see these steps in action
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the dataset using the `MinMaxScaler()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Split the dataset into training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the final step of data preparation, we will now split the dataset into training
    and test sets using the `train_test_split()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31hM3od](https://packt.live/31hM3od).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3glWdZf](https://packt.live/3glWdZf).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We now have the required dataset ready for further actions. As always, let's
    start off by fitting a benchmark model using logistic regression on the cleaned
    dataset. This will be achieved in the next activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15.01: Fitting a Logistic Regression Model on Credit Card Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have just cleaned the dataset that you received to predict the creditworthiness
    of your customers. Before applying ensemble learning methods, you want to fit
    a benchmark model on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement all the appropriate steps from *Exercise 15.01*, *Loading, Exploring,
    and Cleaning the Data*, until you have split the dataset into training and test
    sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the predictions on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the confusion matrix and classification report for the benchmark model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get an output similar to the following after fitting the logistic
    regression model on the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.8: Expected output after fitting the logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.8: Expected output after fitting the logistic regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that you will not get exact values as output due to variability
    in the prediction process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we created a benchmark model for comparison with subsequent
    models.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the output, we achieved an accuracy level of `0.89` with
    the benchmark model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have fit a benchmark model, we will explore different methods of
    ensemble learning in the next section, starting with simple methods.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Methods for Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As defined earlier in the chapter, ensemble learning is all about combining
    the strengths of individual models to get a superior model. In this section, we
    will explore some simple techniques such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted averaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at each of them in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Averaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Averaging is a na√Øve way of doing ensemble learning; however, it is extremely
    useful too. The basic idea behind this technique is to take the predictions of
    multiple individual models and then average the predictions to generate a final
    prediction. The assumption is that by averaging the predictions of different individual
    learners, we eliminate the errors made by individual learners, thereby generating
    a model superior to the base model. One prerequisite to make averaging work is
    to have the predictions of the base models be uncorrelated. This would mean that
    the individual models should not make the same kinds of errors. The diversity
    of the models is a critical aspect to ensure uncorrelated errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing the averaging technique, there are some nuances in predictions
    that need to be taken care of. When predicting, so far, we have been using the
    `predict()` function. As you might know by now, the `predict()` function outputs
    the class that has the highest probability. For example, in our benchmark model,
    when we predicted on the test set, we got an output of either ''1'' or ''0'' for
    each of the test examples. There is also another way to make predictions, which
    is by generating the probability of each class. If we were to output the probability
    of each class for our benchmark model, we would get two outputs for each example
    corresponding to the probability for each class. This is demonstrated in an example
    prediction in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.9: An example prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.9: An example prediction'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding example, we can see that each test example has two outputs
    corresponding to the probabilities of each class. So, for the first example, the
    prediction would be class '1' as the probability for class `1` (`0.902`) is higher
    than class `0` (`0.098`). The respective predictions for example `2` and `3` would
    be class `0` and class `1` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: When we generate an ensemble by averaging method, we generate the probability
    of each class instead of the class predictions. The probability of each class
    is predicted using a separate function called `predict_proba()`. We will see the
    implementation of the averaging method in *Exercise 15.02*, *Ensemble Model Using
    the Averaging Technique*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.02: Ensemble Model Using the Averaging Technique'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using the averaging technique.
    The base models that we will use for this exercise are the logistic regression
    model, which we used as our benchmark model, and the KNN and random forest models,
    which were introduced in *Chapter 4*, *Multiclass Classification with RandomForest*,
    and *Chapter 8*, *Hyperparameter Tuning*:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all the appropriate steps from *Exercise 15.01*, *Loading, Exploring,
    and Cleaning the Data*, to split the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s define the three base models. Import the selected classifiers, which
    we will use as base models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit all three models on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now predict the probabilities of each model using the `predict_proba()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Average the predictions generated from all of the three models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the first four rows of the ensemble prediction array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.10: First four rows of ensemble predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.10: First four rows of ensemble predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see from the preceding output, we have two probabilities for each
    example corresponding to each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the order of each class from the prediction output. As you can see from
    *Step 6*, the prediction output has two columns corresponding to each class. In
    order to find the order of the class prediction, we use a method called `.classes_`.
    This is implemented in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.11: Order of classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.11: Order of classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now have to get the final predictions for each example from the output probabilities.
    The final prediction will be the class with the highest probability. To get the
    class with the highest probability, we use the `numpy` function, `.argmax()`.
    This is executed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the preceding code, `axis = 1` means that we need to find the index of
    the maximum value across the columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.12: Array output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.12: Array output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.13: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.13: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.14: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.14: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31aLa0w](https://packt.live/31aLa0w).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31hMpex](https://packt.live/31hMpex).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented the averaging technique for ensemble learning.
    As you can see from the classification report, we managed to improve the accuracy
    rate from `0.89`, which was achieved with the benchmark model, to `0.91` by averaging.
    However, it should be noted that ensemble methods are not guaranteed to improve
    the results in all cases. There could be instances where no improvement in performance
    would be observed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also look at the results from the business context of credit card applications.
    An accuracy of `91%` means that out of the `196` cases in the test set, we were
    correctly able to predict whether a customer is creditworthy in `91%` of those
    cases. It would also be interesting to look at the recall value of class `0`,
    which is `91%`. In this case, we correctly identified unworthy customers in `91%`
    of the cases. It is also an area of concern that the balance of `9%` of unworthy
    customers have been wrongly identified as creditworthy, which is, in fact, a risk
    to the credit card division. Ideally, we would want to increase the recall value
    of the non-worthy customers if we want to reduce the risk to the credit card division.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there is also an issue of maximizing a business opportunity
    with the correct predictions of creditworthy customers (`1`). We have seen that
    `91%` of creditworthy customers out of a total of `107` were correctly predicted,
    which is a positive outcome. The balance of `9%` of creditworthy customers would
    be a lost business opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning models is a balancing act, which a data scientist has to be cognizant
    about. A data scientist has to act according to the preferences of the business.
    If the business is risk-averse and would prefer to forego business opportunities,
    then the models will have to be tuned to increase the recall value of the non-worthy
    customers. On the other hand, if the business is aggressive and would like to
    go for growth, then the preferred route would be to improve the recall value of
    creditworthy customers. At the end of the day, fine-tuning models is a balancing
    act based on business realities.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted Averaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weighted averaging is an extension of the averaging method that we saw earlier.
    The major difference in both of these approaches is in the way the combined predictions
    are generated. In the weighted averaging method, we assign weights to each model's
    predictions and then generate the combined predictions. The weights are assigned
    based on our judgment of which model would be the most influential in the ensemble.
    These weights, which are initially assigned arbitrarily, have to be evolved after
    a lot of experimentation. To start off, we assume some weights and then iterate
    with different weights for each model to verify whether we get any improvements
    in the performance. Let's implement the weighted averaging method in the upcoming
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.03: Ensemble Model Using the Weighted Averaging Technique'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using the weighted averaging
    technique. We will use the same base models, logistic regression, KNN, and random
    forest, which were used in *Exercise 15.02*, *Ensemble Model Using the Averaging
    Technique*:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all the steps from *Exercise 15.02*, *Ensemble Model Using the Averaging
    Technique*, up until predicting the probabilities of the three models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take the weighted average of the predictions. In the weighted averaging method,
    weights are assigned arbitrarily based on our judgment of each of the predictions.
    This is done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that the weights are assigned in such a way that the sum of all
    weights becomes `1` (`0.6 + 0.2 + 0.2 = 1`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the first four rows of the ensemble prediction array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.15: Array output for ensemble prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.15: Array output for ensemble prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see from the output, we have two probabilities for each example corresponding
    to each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the order of each class from the prediction output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.16: Order of class from prediction output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.16: Order of class from prediction output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the final predictions from the probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now have to get the final predictions for each example from the output probabilities
    using the `np.argmax()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.17: Array for final predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.17: Array for final predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.18: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.18: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generating a classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.19: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.19: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2CINUc5](https://packt.live/2CINUc5).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YfrTZK](https://packt.live/2YfrTZK).
  prefs: []
  type: TYPE_NORMAL
- en: Iteration 2 with Different Weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the first iteration, we saw that we got accuracy of `89%`. This metric
    is a reflection of the weights that we applied in the first iteration. Let''s
    try to change the weights and see what effect it has on the metrics. The process
    of trying out various weights is based on our judgment of the dataset and the
    distribution of data. Let''s say we feel that the data distribution is more linear,
    and therefore we decide to increase the weight for the linear regression model
    and decrease the weights of the other two models. Let''s now try the new combination
    of weights in *iteration 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the weighted average of the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this iteration, we increase the weight of logistic regression prediction
    from `0.6` to `0.7` and decrease the other two from `0.2` to `0.15`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Calculate the final predictions from the probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now have to get the final predictions for each example from the output probabilities
    using the `np.argmax()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.20: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.20: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.21: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.21: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented the weighted averaging technique for ensemble
    learning. We did two iterations with the weights. We saw that in the second iteration,
    where we increased the weight of the logistic regression prediction from `0.6`
    to `0.7`, the accuracy actually improved from `0.89` to `0.90`. This is a validation
    of our assumption about the prominence of the logistic regression model in the
    ensemble. To check whether there is more room for improvement, we should again
    change the weights, just like we did in iteration `2`, and then validate against
    the metrics. We should continue these iterations until there is no further improvement
    noticed in the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing it with the metrics from the averaging method, we can see that the
    accuracy level has gone down from `0.91` to `0.90`. However, the recall value
    of class `1` has gone up from `0.91` to `0.92`, and the corresponding value for
    class `0` has gone down from `0.91` to `0.88`. It could be that the weights that
    we applied have resulted in a marginal degradation of the results from what we
    got from the averaging method.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the results from a business perspective, we can see that with the
    increase in the recall value of class `1`, the card division is getting more creditworthy
    customers. However, this has come at the cost of increasing the risk with more
    unworthy customers, with `12%` (`100% - 88%`) being tagged as creditworthy customers.
  prefs: []
  type: TYPE_NORMAL
- en: Max Voting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The max voting method works on the principle of majority rule. In this method,
    the opinion of the majority rules the roost. In this technique, individual models,
    or, in ensemble learning jargon, individual learners, are fit on the training
    set and their predictions are then generated on the test set. Each individual
    learner's prediction is considered to be a vote. On the test set, whichever class
    gets the maximum vote is the ultimate winner. Let's demonstrate this with a toy
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have three individual learners who learned on the training set.
    Each of them generates their predictions on the test set, which is tabulated in
    the following table. The predictions are either for class ''1'' or class ''0'':'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.22: Predictions for learners'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.22: Predictions for learners'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we can see that for `Example 1` and `Example 3`, the
    majority vote is for class '1,' and for the other two examples, the majority of
    the vote is for class '0'. The final predictions are based on which class gets
    the majority vote. This method of voting, where we output a class, is called "hard
    " voting.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing the max voting method using the `scikit-learn` library, we
    use a special function called `VotingClassifier()`. We provide individual learners
    as input to `VotingClassifier` to create the ensemble model. This ensemble model
    is then used to fit the training set and then is finally used to predict on the
    test sets. We will explore the dynamics of max voting in *Exercise 15.04*, *Ensemble
    Model Using Max Voting*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.04: Ensemble Model Using Max Voting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using the max voting
    technique. The individual learners we will select are similar to the ones that
    we chose in the previous exercises, that is, logistic regression, KNN, and random
    forest:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all the steps from *Exercise 15.01*, *Loading, Exploring, and Cleaning
    the Data*, up until the splitting of the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now import the selected classifiers, which we will use as the individual
    learners:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Having defined the individual learners, we can now construct the ensemble model
    using the `VotingClassifier()` function. This is implemented by the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from the code snippet, the individual learners are given as input
    using the `estimators` argument. Estimators take each of the defined individual
    learners along with the string value to denote which model it is. For example,
    `lr` denotes logistic regression. Also, note that the voting is "hard, " which
    means that the output will be class labels and not probabilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training set on the ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the accuracy scores after training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the predictions from the ensemble model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.23: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.23: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.24: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.24: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3aESLr6](https://packt.live/3aESLr6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31cL7RJ](https://packt.live/31cL7RJ).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented the max voting technique for ensemble learning.
    As you can see from the classification report, the results are similar to what
    we got from the averaging method (`0.91`). From a business context, we can see
    that this result is more balanced. The recall value for worthy customers is high,
    at `92%`; however, this has come at the cost of more risk by having more unworthy
    customers. The percentage of unworthy customers is around `10%` in this case (`100%`
    - `90%`).
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Techniques for Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having learned simple techniques for ensemble learning, let''s now explore
    some advanced techniques. Among the advanced techniques, we will be dealing with
    three different kinds of ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking/blending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we deal with each of them, there are some basic dynamics of these advanced
    ensemble learning techniques that need to be deciphered. As described at the beginning
    of the chapter, the essence of ensemble learning is in combining individual models
    to form a superior model. There are some subtle nuances in the way the superior
    model is generated in the advanced techniques. In these techniques, the individual
    models or learners generate predictions and those predictions are used to form
    the final predictions. The individual models or learners, which generate the first
    set of predictions, are called **base** **learners** or **base** **estimators**
    and the model, which is a combination of the predictions of the base learners,
    is called the **meta** **learner** or **meta estimator**. The way in which the
    meta learners learn from the base learners differs for each of the advanced techniques.
    Let's understand each of the advanced techniques in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging is a pseudonym for **B**ootstrap **Agg**regat**ing**. Before we explain
    how bagging works, let's describe what bootstrapping is. Bootstrapping has its
    etymological origins in the phrase, *Pulling oneself up by one's bootstrap*. The
    essence of this phrase is to make the best use of the available resources. In
    the statistical context, bootstrapping entails taking samples from the available
    dataset by replacement. Let's look at this concept with a toy example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a dataset consisting of 10 numbers from 1 to 10\. We now need
    to create 4 different datasets of 10 each from the available dataset. How do we
    do this? This is where the concept of bootstrapping comes in handy. In this method,
    we take samples from the available dataset one by one and then replace the number
    we took before taking the next sample. We continue with this until we get a sample
    with the number of data points we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are replacing each number after it is selected, there is a chance that
    we might have more than one of a given data point in a sample. This is explained
    by the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.25: Bootstrapping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.25: Bootstrapping'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood bootstrapping, let's apply this concept to a machine
    learning context. Earlier in the chapter, we discussed that ensemble learning
    helps in reducing the variance of predictions. One way that variance could be
    reduced is by averaging out the predictions from multiple learners. In bagging,
    multiple subsets of the data are created using bootstrapping. On each of these
    subsets of data, a base learner is fitted and the predictions generated. These
    predictions from all the base learners are then averaged to get the meta learner
    or the final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing bagging, we use a function called `BaggingClassifier()`,
    which is available in the `Scikit learn` library. Some of the important arguments
    that are provided when creating an ensemble model include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_estimator`: This argument is to define the base estimator to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimator`: This argument defines the number of base estimators that will
    be used in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_samples`: The maximum size of the bootstrapped sample for fitting the
    base estimator is defined using this argument. This is represented as a proportion
    (0.8, 0.7, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: When fitting multiple individual learners, it has been found
    that randomly selecting the features to be used in each dataset results in superior
    performance. The `max_features` argument indicates the number of features to be
    used. For example, if there were 10 features in the dataset and the `max_features`
    argument was to be defined as 0.8, then only 8 (0.8 x 10) features would be used
    to fit a model using the base learner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore ensemble learning with bagging in *Exercise 15.05*, *Ensemble
    Learning Using Bagging*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.05: Ensemble Learning Using Bagging'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using bagging. The individual
    learner we will select will be random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all the steps from *Exercise 15.01*, *Loading, Exploring, and Cleaning
    the Data*, up until the splitting of the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the base learner, which will be a random forest classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Having defined the individual learner, we can now construct the ensemble model
    using the `BaggingClassifier()` function. This is implemented by the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The arguments that we have given are arbitrary values. The optimal values have
    to be identified using experimentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training set on the ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the predictions from the ensemble model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.26: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.26: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.27: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.27: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iYcHZ3](https://packt.live/3iYcHZ3).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hfRCbZ](https://packt.live/3hfRCbZ).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented ensemble learning using bagging. As you can
    see from the classification report, we have a slightly lower result (`0.90`) than
    what we got from the averaging and max voting methods (`0.91`). However, looking
    at the results from a business perspective, we can see some big swings in the
    recall values for both of the classes. We see that the recall value of creditworthy
    customers is around 87%. That means that there is around `13%` of lost opportunities.
    We can also see that the risks in terms of unworthy customers being identified
    are also being reduced. The recall value for identifying unworthy customers is
    around 93%, which means that only around 7% of the unworthy customers are wrongly
    classified as creditworthy. So, for a business that is more risk-averse, it is
    recommended that you use this model.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The bagging technique, which we discussed in the last section, can be termed
    as a parallel learning technique. This means that each base learner is fit independently
    of the other and their predictions are aggregated. Unlike the bagging method,
    boosting works in a sequential manner. It works on the principle of correcting
    the prediction errors of each base learner. The base learners are fit sequentially
    one after the other. A base learner tries to correct the error generated by the
    previous learner and this process continues until a superior meta learner is created.
    The steps involved in the boosting technique are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A base learner is fit on a subset of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model is fit, predictions are made on the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The errors in the predictions are identified by comparing them with the actual
    labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Those examples that generated the wrong predictions are given larger weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another base learner is fit on the dataset where the weights of the wrongly
    predicted examples in the previous step are altered.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This base learner tries to correct the errors of the earlier model and gives
    their predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 4*, *5*, and *6* are repeated until a strong meta learner is generated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When implementing the boosting technique, one method we can use is `AdaBoostClassifier()`
    in scikit-learn. Like the bagging estimator, some of the important arguments for
    the `AdaBoostClassifier()` method are `base_estimator` and `n_estimators`. We
    will now implement the boosting algorithm in *Exercise 15.06*, *Ensemble Learning
    Using Boosting*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.06: Ensemble Learning Using Boosting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using boosting. The individual
    learner we will select will be the logistic regression model. The steps for implementing
    this algorithm are very similar to the bagging algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all of the steps from *Exercise 15.01*, *Loading, Exploring, and Cleaning
    the Data*, up until the splitting of the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the base learner, which will be a logistic regression classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Having defined the individual learner, we can now construct the ensemble model
    using the `AdaBoostClassifier()` function. This is implemented by the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The arguments that we have given are arbitrary values. The optimal values have
    to be identified using experimentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training set on the ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the predictions from the ensemble model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.28: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.28: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get an output similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.29: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.29: Classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3iUas96](https://packt.live/3iUas96).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2EkkW2J](https://packt.live/2EkkW2J).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented the ensemble learning model using boosting.
    As you can see from the classification report, we got very similar results (0.90)
    to the bagging method, which we implemented in the previous exercise. From a business
    context, the results are more balanced compared to the results that we got for
    the bagging method (the recall values of 0.93 and 0.87). Here, we can see that
    the recall value of the unworthy customers (90%) and that of creditworthy customers
    (91%) are quite close to each other, which indicates a very balanced result.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stacking, in principle, works in a similar way to bagging and boosting in that
    it combines base learners to form a meta learner. However, the approach for getting
    the meta learners from the base learners differs substantially in stacking. In
    stacking, the meta learner is fit on the predictions made by the base learners.
    The stacking algorithm can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The training set is split into multiple parts, say, five parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A base learner (say, KNN) is fitted on four parts of the training set and then
    predicted on the fifth set. This process continues until the base learner predicts
    on each of the five parts of the training set. All the predictions, which are
    so generated, are collated to get the predictions for the complete training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The same base learner is then used to generate predictions on the test set as
    well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 2* and *3* are then repeated with a different base learner (say, random
    forest).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next enters a new model, which acts as the meta learner (say, logistic regression).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The meta learner is fit on the predictions generated on the training set by
    the base learners.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the meta learner is fit on the training set, the same model is used to
    predict on the predictions generated on the test set by the base learners.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All these processes are explained pictorially as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.30: Process of stacking'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.30: Process of stacking'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of stacking is done through a function called `StackingClassifier()`.
    This is available from a package called `mlxtend`. The various arguments for this
    function are the models that we assign as base learners and the model assigned
    as a meta learner. The implementation of the stacking technique is executed in
    *Exercise 15.07*, *Ensemble Learning Using Stacking*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15.07: Ensemble Learning Using Stacking'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement an ensemble model using stacking. The individual
    learners we will use are KNN and random forest. Our meta learner will be logistic
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute all of the steps from *Exercise 15.01*, *Loading, Exploring, and Cleaning
    the Data*, up until the splitting of the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the base learners and the meta learner. In this implementation, we will
    be using two base learners (KNN and random forest). The meta learner will be logistic
    regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the base learners and meta learner are defined, we will proceed to create
    the stacking classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The arguments that we have been given are the two base learners and the meta
    learner.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the training set on the ensemble model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the predictions from the ensemble model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.31: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.31: Classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a confusion matrix for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.32: Confusion matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.32: Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31dZRjy](https://packt.live/31dZRjy).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Fwsdgf](https://packt.live/2Fwsdgf).
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we implemented the ensemble learning model using stacking.
    As you can see from the classification report, we did not get any improvements
    over the benchmark model using the stacking classifier. The possible reasons for
    this could range from the choice of base learners and meta learners that we used
    to the size of the dataset. It should be noted that not all ensemble learners
    will be the desired silver bullet. There will be many instances where many advanced
    methods will fail to improve performance. This exercise proved to be one such
    instance. However, the secret recipe for getting the most optimal method, which
    boosts performance, is intense experimentation. That is what will yield results.
    After all, in machine learning, there are no free lunches.
  prefs: []
  type: TYPE_NORMAL
- en: From a business context, the results have huge implications from a revenue perspective
    as a large number of creditworthy customers, `16%` (`100%` - `84%`), have been
    tagged as unworthy. The risk side of recall is quite good with a higher percentage
    (`92%`) of unworthy customers identified.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen three advanced techniques for ensemble learning. Let's
    now apply all of these techniques to the credit card dataset and then pick the
    best method. We will do this in *Activity 15.02*, *Ensemble Model Using the Averaging
    Technique*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 15.02: Comparison of Advanced Ensemble Techniques'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scenario: You have tried the benchmark model on the credit card dataset and
    have got some benchmark metrics. Having learned some advanced ensemble techniques,
    you want to determine which technique to use for the credit card approval dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you will use all three advanced techniques and compare the
    results before selecting your final technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement all steps from *Exercise 15.01*, *Loading, Exploring, and Cleaning
    the Data,* up until the splitting of the dataset into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the bagging technique with the base learner as the logistic regression
    model. In the bagging classifier, define `n_estimators = 15`, `max_samples = 0.7`,
    and `max_features = 0.8`. Fit the model on the training set, generate the predictions,
    and print the confusion matrix and the classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement boosting with random forest as the base learner. In the `AdaBoostClassifier`,
    define `n_estimators = 300`. Fit the model on the training set, generate the predictions,
    and print the confusion matrix and classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the stacking technique. Make the KNN and logistic regression models
    base learners and random forest a meta learner. Fit the model on the training
    set, generate the predictions, and print the confusion matrix and classification
    report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the results across all three techniques and select the best technique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output: You should get an output similar to the following for all three methods.
    Please note you will not get exact values as output due to variability in the
    prediction process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output for bagging would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.33: Output for bagging'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15019_15_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 15.33: Output for bagging'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for boosting would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.34: Output for boosting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.34: Output for boosting'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for stacking would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.35: Output for stacking'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15019_15_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15.35: Output for stacking'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found here: [https://packt.live/2GbJloz](https://packt.live/2GbJloz).'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we implemented all three advanced ensemble techniques on the
    credit card dataset. Based on the metrics, we found that boosting (`0.91`) has
    better results than bagging (`0.90`) and stacking (`0.87`). We, therefore, select
    the boosting algorithm to boost the performance of our models. From a business
    perspective, the boosting algorithm has generated more balanced results where
    the recall value of both creditworthy and not creditworthy customers (91%) is
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about various techniques of ensemble learning. Let's
    summarize our learning in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we were introduced to the concepts of variance
    and bias and we learned that ensemble learning is a technique that aims to combine
    individual models to create a superior model, thereby reducing variance and bias
    and improving performance. To further explore different techniques of ensemble
    learning, we downloaded the credit card approval dataset. We also fitted a benchmark
    model using logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we were introduced to six different techniques of
    ensemble learning; three of them being simple techniques and the remaining three
    being advanced techniques. The averaging method creates an ensemble by combining
    the predictions of base learners and averaging the prediction probabilities. We
    were able to get better results than the benchmark model using this technique.
    The weighted averaging method is similar to the averaging method. The difference
    is in the way the predictions are combined. In this method, arbitrary weights
    are applied to the predictions of individual learners to get the final prediction.
    Max voting is a technique that arrives at final predictions based on the votes
    of the majority of the base learners.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning with bagging leverages bootstrapping techniques. The base
    learners are fitted on bootstrapped datasets and the results are aggregated to
    get the meta learner. Boosting is a sequential learner, which works on the principle
    of error correction of the base learners. We found that boosting techniques produced
    some superior results for our context. We used the stacking technique to generate
    the final predictions by learning from the output of the predictions by the base
    learners.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this chapter was to equip you with a repertoire of skills aimed
    at boosting the performance of your machine learning models. However, it should
    be noted that there are no silver bullets among machine learning techniques. Not
    all techniques will work in all scenarios. The secret sauce, which will help you
    on your journey to being a good data scientist, is the rigor of experimentation
    with different techniques, use cases, and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Having learned a set of tools aimed at boosting performance, you are now well
    prepared to apply these on a range of real-world projects. This chapter marks
    the end of the book and is a good opportunity for you to reflect on all the skills
    you have learned in working your way through the different chapters. However,
    there are still three bonus chapters available to you at [https://packt.live/2ZagB9y](https://packt.live/2ZagB9y).
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The Data Science Workshop](B15019_FM_Final_SZ_ePub.xhtml#_idParaDest-1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Second Edition](B15019_FM_Final_SZ_ePub.xhtml#_idParaDest-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Preface](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[About the Book](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-4)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Audience](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-5)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[About the Chapters](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-6)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Conventions](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-7)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Code Presentation](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-8)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Setting up Your Environment](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-9)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How to Set Up Google Colab](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-10)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[How to Use Google Colab](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-11)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Accessing the Code Files](B15019_Preface_Final_SZ_ePub.xhtml#_idParaDest-12)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[1\. Introduction to Data Science in Python](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-13)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-14)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Application of Data Science](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-15)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What Is Machine Learning?](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-16)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Supervised Learning](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-17)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Unsupervised Learning](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-18)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Reinforcement Learning](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-19)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Overview of Python](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-20)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Types of Variable](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-21)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Numeric Variables](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-22)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Text Variables](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-23)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Python List](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-24)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Python Dictionary](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-25)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 1.01: Creating a Dictionary That Will Contain Machine Learning Algorithms](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-26)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Python for Data Science](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-27)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The pandas Package](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-28)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DataFrame and Series](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-29)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[CSV Files](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-30)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Excel Spreadsheets](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-31)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[JSON](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-32)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 1.02: Loading Data of Different Formats into a pandas DataFrame](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-33)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Scikit-Learn](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-34)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What Is a Model?](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-35)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Model Hyperparameters](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-36)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The sklearn API](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-37)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 1.03: Predicting Breast Cancer from a Dataset Using sklearn](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-38)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 1.01: Train a Spam Detector Algorithm](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-39)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_01_Final_SMP_ePub.xhtml#_idParaDest-40)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2\. Regression](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-41)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-42)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simple Linear Regression](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-43)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Method of Least Squares](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-44)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Multiple Linear Regression](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-45)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Estimating the Regression Coefficients (Œ≤0, Œ≤1, Œ≤2 and Œ≤3)](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-46)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Logarithmic Transformations of Variables](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-47)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Correlation Matrices](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-48)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Conducting Regression Analysis Using Python](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-49)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 2.01: Loading and Preparing the Data for Analysis](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-50)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Correlation Coefficient](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-51)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 2.02: Graphical Investigation of Linear Relationships Using Python](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-52)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 2.03: Examining a Possible Log-Linear Relationship Using Python](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-53)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Statsmodels formula API](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-54)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 2.04: Fitting a Simple Linear Regression Model Using the Statsmodels
    formula API](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-55)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analyzing the Model Summary](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-56)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Model Formula Language](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-57)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Intercept Handling](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-58)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 2.01: Fitting a Log-Linear Model Using the Statsmodels Formula API](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-59)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Multiple Regression Analysis](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-60)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 2.05: Fitting a Multiple Linear Regression Model Using the Statsmodels
    Formula API](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-61)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Assumptions of Regression Analysis](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-62)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 2.02: Fitting a Multiple Log-Linear Regression Model](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-63)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Explaining the Results of Regression Analysis](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-64)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Regression Analysis Checks and Balances](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-65)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The F-test](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-66)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The t-test](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-67)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_02_Final_SMP_ePub.xhtml#_idParaDest-68)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3\. Binary Classification](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-69)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-70)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understanding the Business Context](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-71)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Business Discovery](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-72)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.01: Loading and Exploring the Data from the Dataset](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-73)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Testing Business Hypotheses Using Exploratory Data Analysis](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-74)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Visualization for Exploratory Data Analysis](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-75)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.02: Business Hypothesis Testing for Age versus Propensity for a
    Term Loan](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-76)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Intuitions from the Exploratory Analysis](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-77)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 3.01: Business Hypothesis Testing to Find Employment Status versus
    Propensity for Term Deposits](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-78)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Feature Engineering](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-79)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Business-Driven Feature Engineering](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-80)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.03: Feature Engineering ‚Äì Exploration of Individual Features](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-81)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.04: Feature Engineering ‚Äì Creating New Features from Existing Ones](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-82)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data-Driven Feature Engineering](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-83)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Quick Peek at Data Types and a Descriptive Summary](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-84)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Correlation Matrix and Visualization](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-85)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.05: Finding the Correlation in Data to Generate a Correlation Plot
    Using Bank Data](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-86)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Skewness of Data](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-87)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Histograms](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-88)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Density Plots](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-89)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Other Feature Engineering Methods](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-90)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summarizing Feature Engineering](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-91)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Building a Binary Classification Model Using the Logistic Regression Function](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-92)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Logistic Regression Demystified](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-93)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Metrics for Evaluating Model Performance](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-94)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Confusion Matrix](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-95)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Accuracy](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-96)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Classification Report](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-97)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Preprocessing](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-98)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 3.06: A Logistic Regression Model for Predicting the Propensity of
    Term Deposit Purchases in a Bank](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-99)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 3.02: Model Iteration 2 ‚Äì Logistic Regression Model with Feature
    Engineered Variables](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-100)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Next Steps](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-101)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_03_Final_SMP_ePub.xhtml#_idParaDest-102)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4\. Multiclass Classification with RandomForest](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-103)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-104)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training a Random Forest Classifier](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-105)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Evaluating the Model''s Performance](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-106)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 4.01: Building a Model for Classifying Animal Type and Assessing
    Its Performance](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-107)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Number of Trees Estimator](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-108)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 4.02: Tuning n_estimators to Reduce Overfitting](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-109)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Depth](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-110)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 4.03: Tuning max_depth to Reduce Overfitting](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-111)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Minimum Sample in Leaf](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-112)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 4.04: Tuning min_samples_leaf](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-113)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Features](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-114)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 4.05: Tuning max_features](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-115)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 4.01: Train a Random Forest Classifier on the ISOLET Dataset](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-116)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_04_Final_SZ_ePub.xhtml#_idParaDest-117)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5\. Performing Your First Cluster Analysis](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-118)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-119)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Clustering with k-means](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-120)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.01: Performing Your First Clustering Analysis on the ATO Dataset](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-121)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Interpreting k-means Results](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-122)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.02: Clustering Australian Postcodes by Business Income and Expenses](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-123)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Choosing the Number of Clusters](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-124)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.03: Finding the Optimal Number of Clusters](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-125)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Initializing Clusters](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-126)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.04: Using Different Initialization Parameters to Achieve a Suitable
    Outcome](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-127)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Calculating the Distance to the Centroid](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-128)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.05: Finding the Closest Centroids in Our Dataset](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-129)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Standardizing Data](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-130)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 5.06: Standardizing the Data from Our Dataset](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-131)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 5.01: Perform Customer Segmentation Analysis in a Bank Using k-means](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-132)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_05_Final_SZ_ePub.xhtml#_idParaDest-133)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6\. How to Assess Performance](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-134)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-135)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Splitting Data](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-136)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.01: Importing and Splitting Data](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-137)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Assessing Model Performance for Regression Models](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-138)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Structures ‚Äì Vectors and Matrices](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-139)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Scalars](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-140)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Vectors](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-141)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Matrices](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-142)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[R2 Score](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-143)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.02: Computing the R2 Score of a Linear Regression Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-144)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Mean Absolute Error](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-145)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.03: Computing the MAE of a Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-146)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.04: Computing the Mean Absolute Error of a Second Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-147)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Other Evaluation Metrics](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-148)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Assessing Model Performance for Classification Models](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-149)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.05: Creating a Classification Model for Computing Evaluation Metrics](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-150)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Confusion Matrix](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-151)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.06: Generating a Confusion Matrix for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-152)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[More on the Confusion Matrix](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-153)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Precision](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-154)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.07: Computing Precision for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-155)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Recall](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-156)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.08: Computing Recall for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-157)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[F1 Score](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-158)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.09: Computing the F1 Score for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-159)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Accuracy](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-160)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.10: Computing Model Accuracy for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-161)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Logarithmic Loss](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-162)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.11: Computing the Log Loss for the Classification Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-163)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Receiver Operating Characteristic Curve](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-164)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.12: Computing and Plotting ROC Curve for a Binary Classification
    Problem](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-165)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Area Under the ROC Curve](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-166)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.13: Computing the ROC AUC for the Caesarian Dataset](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-167)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Saving and Loading Models](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-168)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 6.14: Saving and Loading a Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-169)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 6.01: Train Three Different Models and Use Evaluation Metrics to
    Pick the Best Performing Model](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-170)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_06_Final_SZ_ePub.xhtml#_idParaDest-171)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7\. The Generalization of Machine Learning Models](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-172)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-173)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Overfitting](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-174)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training on Too Many Features](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-175)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training for Too Long](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-176)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Underfitting](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-177)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-178)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Ratio for Dataset Splits](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-179)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Creating Dataset Splits](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-180)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.01: Importing and Splitting Data](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-181)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Random State](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-182)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.02: Setting a Random State When Splitting Data](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-183)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cross-Validation](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-184)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[KFold](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-185)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.03: Creating a Five-Fold Cross-Validation Dataset](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-186)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.04: Creating a Five-Fold Cross-Validation Dataset Using a Loop
    for Calls](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-187)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[cross_val_score](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-188)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.05: Getting the Scores from Five-Fold Cross-Validation](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-189)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understanding Estimators That Implement CV](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-190)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[LogisticRegressionCV](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-191)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.06: Training a Logistic Regression Model Using Cross-Validation](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-192)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning with GridSearchCV](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-193)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Decision Trees](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-194)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.07: Using Grid Search with Cross-Validation to Find the Best Parameters
    for a Model](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-195)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Hyperparameter Tuning with RandomizedSearchCV](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-196)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.08: Using Randomized Search for Hyperparameter Tuning](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-197)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Model Regularization with Lasso Regression](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-198)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.09: Fixing Model Overfitting Using Lasso Regression](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-199)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Ridge Regression](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-200)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 7.10: Fixing Model Overfitting Using Ridge Regression](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-201)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 7.01: Find an Optimal Model for Predicting the Critical Temperatures
    of Superconductors](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-202)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_07_Final_SMP_ePub.xhtml#_idParaDest-203)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8\. Hyperparameter Tuning](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-204)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-205)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What Are Hyperparameters?](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-206)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Difference between Hyperparameters and Statistical Model Parameters](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-207)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Setting Hyperparameters](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-208)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Note on Defaults](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-209)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Finding the Best Hyperparameterization](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-210)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 8.01: Manual Hyperparameter Tuning for a k-NN Classifier](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-211)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Advantages and Disadvantages of a Manual Search](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-212)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tuning Using Grid Search](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-213)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simple Demonstration of the Grid Search Strategy](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-214)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GridSearchCV](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-215)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tuning using GridSearchCV](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-216)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Support Vector Machine (SVM) Classifiers](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-217)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 8.02: Grid Search Hyperparameter Tuning for an SVM](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-218)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Advantages and Disadvantages of Grid Search](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-219)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Random Search](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-220)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Random Variables and Their Distributions](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-221)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simple Demonstration of the Random Search Process](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-222)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tuning Using RandomizedSearchCV](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-223)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 8.03: Random Search Hyperparameter Tuning for a Random Forest Classifier](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-224)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Advantages and Disadvantages of a Random Search](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-225)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 8.01: Is the Mushroom Poisonous?](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-226)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_08_Final_SZ_ePub.xhtml#_idParaDest-227)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[9\. Interpreting a Machine Learning Model](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-228)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-229)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Linear Model Coefficients](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-230)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 9.01: Extracting the Linear Regression Coefficient](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-231)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[RandomForest Variable Importance](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-232)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 9.02: Extracting RandomForest Feature Importance](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-233)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Variable Importance via Permutation](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-234)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 9.03: Extracting Feature Importance via Permutation](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-235)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Partial Dependence Plots](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-236)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 9.04: Plotting Partial Dependence](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-237)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Local Interpretation with LIME](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-238)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 9.05: Local Interpretation with LIME](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-239)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 9.01: Train and Analyze a Network Intrusion Detection Model](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-240)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_09_Final_SMP_new_ePub.xhtml#_idParaDest-241)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[10\. Analyzing a Dataset](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-242)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-243)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exploring Your Data](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-244)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analyzing Your Dataset](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-245)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 10.01: Exploring the Ames Housing Dataset with Descriptive Statistics](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-246)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analyzing the Content of a Categorical Variable](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-247)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 10.02: Analyzing the Categorical Variables from the Ames Housing
    Dataset](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-248)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summarizing Numerical Variables](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-249)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 10.03: Analyzing Numerical Variables from the Ames Housing Dataset](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-250)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Visualizing Your Data](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-251)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Using the Altair API](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-252)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Histogram for Numerical Variables](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-253)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Bar Chart for Categorical Variables](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-254)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Boxplots](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-255)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 10.04: Visualizing the Ames Housing Dataset with Altair](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-256)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 10.01: Analyzing Churn Data Using Visual Data Analysis Techniques](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-257)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_10_Final_SZ_ePub.xhtml#_idParaDest-258)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[11\. Data Preparation](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-259)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-260)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Handling Row Duplication](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-261)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 11.01: Handling Duplicates in a Breast Cancer Dataset](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-262)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Converting Data Types](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-263)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 11.02: Converting Data Types for the Ames Housing Dataset](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-264)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Handling Incorrect Values](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-265)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 11.03: Fixing Incorrect Values in the State Column](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-266)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Handling Missing Values](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-267)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 11.04: Fixing Missing Values for the Horse Colic Dataset](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-268)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 11.01: Preparing the Speed Dating Dataset](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-269)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_11_Final_SZ_ePub.xhtml#_idParaDest-270)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[12\. Feature Engineering](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-271)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-272)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Merging Datasets](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-273)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Left Join](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-274)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Right Join](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-275)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 12.01: Merging the ATO Dataset with the Postcode Data](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-276)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Binning Variables](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-277)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 12.02: Binning the YearBuilt Variable from the AMES Housing Dataset](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-278)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Manipulating Dates](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-279)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 12.03: Date Manipulation on Financial Services Consumer Complaints](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-280)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Performing Data Aggregation](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-281)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 12.04: Feature Engineering Using Data Aggregation on the AMES Housing
    Dataset](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-282)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 12.01: Feature Engineering on a Financial Dataset](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-283)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_12_Final_SMP_ePub.xhtml#_idParaDest-284)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[13\. Imbalanced Datasets](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-285)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-286)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understanding the Business Context](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-287)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 13.01: Benchmarking the Logistic Regression Model on the Dataset](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-288)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analysis of the Result](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-289)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Challenges of Imbalanced Datasets](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-290)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Strategies for Dealing with Imbalanced Datasets](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-291)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Collecting More Data](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-292)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Resampling Data](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-293)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 13.02: Implementing Random Undersampling and Classification on Our
    Banking Dataset to Find the Optimal Result](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-294)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Analysis](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-295)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Generating Synthetic Samples](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-296)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Implementation of SMOTE and MSMOTE](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-297)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 13.03: Implementing SMOTE on Our Banking Dataset to Find the Optimal
    Result](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-298)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 13.04: Implementing MSMOTE on Our Banking Dataset to Find the Optimal
    Result](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-299)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Applying Balancing Techniques on a Telecom Dataset](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-300)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 13.01: Finding the Best Balancing Technique by Fitting a Classifier
    on the Telecom Churn Dataset](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-301)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_13_Final_SZ_ePub.xhtml#_idParaDest-302)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[14\. Dimensionality Reduction](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-303)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-304)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Business Context](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-305)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.01: Loading and Cleaning the Dataset](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-306)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Creating a High-Dimensional Dataset](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-307)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 14.01: Fitting a Logistic Regression Model on a HighDimensional Dataset](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-308)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Strategies for Addressing High-Dimensional Datasets](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-309)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Backward Feature Elimination (Recursive Feature Elimination)](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-310)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.02: Dimensionality Reduction Using Backward Feature Elimination](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-311)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Forward Feature Selection](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-312)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.03: Dimensionality Reduction Using Forward Feature Selection](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-313)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Principal Component Analysis (PCA)](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-314)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.04: Dimensionality Reduction Using PCA](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-315)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Independent Component Analysis (ICA)](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-316)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.05: Dimensionality Reduction Using Independent Component Analysis](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-317)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Factor Analysis](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-318)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 14.06: Dimensionality Reduction Using Factor Analysis](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-319)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Comparing Different Dimensionality Reduction Techniques](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-320)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 14.02: Comparison of Dimensionality Reduction Techniques on the Enhanced
    Ads Dataset](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-321)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_14_Final_SMP_ePub.xhtml#_idParaDest-322)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[15\. Ensemble Learning](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-323)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Introduction](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-324)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Ensemble Learning](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-325)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Variance](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-326)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Bias](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-327)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Business Context](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-328)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.01: Loading, Exploring, and Cleaning the Data](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-329)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 15.01: Fitting a Logistic Regression Model on Credit Card Data](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-330)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simple Methods for Ensemble Learning](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-331)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Averaging](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-332)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.02: Ensemble Model Using the Averaging Technique](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-333)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Weighted Averaging](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-334)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.03: Ensemble Model Using the Weighted Averaging Technique](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-335)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Iteration 2 with Different Weights](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-336)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Max Voting](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-337)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.04: Ensemble Model Using Max Voting](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-338)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Advanced Techniques for Ensemble Learning](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-339)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Bagging](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-340)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.05: Ensemble Learning Using Bagging](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-341)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Boosting](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-342)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.06: Ensemble Learning Using Boosting](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-343)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Stacking](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-344)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Exercise 15.07: Ensemble Learning Using Stacking](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-345)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Activity 15.02: Comparison of Advanced Ensemble Techniques](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-346)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](B15019_15_Final_SZ_ePub.xhtml#_idParaDest-347)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Landmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Cover](Images/cover.xhtml)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Table of Contents](B15019_FM_Final_SZ_ePub.xhtml#_idContainer004)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
