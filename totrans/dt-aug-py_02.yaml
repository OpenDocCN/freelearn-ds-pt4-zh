- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Biases in Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As **artificial intelligence** (**AI**) becomes embedded in our society, biases
    in AI systems will adversely affect your quality of life. These AI systems, particularly
    in **deep learning** (**DL**) and generative AI, depend on the input data you
    are using to extend data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems rely heavily on data to make decisions, and if the data used to train
    the system is biased, then the AI system will make unfair decisions. It will lead
    to the unjust treatment of individuals or groups and perpetuate systemic inequalities.
    AI plays a decisive role in life-changing decisions, such as how much your monthly
    mortgage insurance rate is, whether you can be approved for a car loan, your application
    qualification for a job, who will receive government assistance, how much you
    pay for milk, what you read on social media newsfeeds, and how much oil your country
    will import or export, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: By learning data biases before diving deep into learning data augmentation,
    you will be able to help develop ethical and fair AI systems that benefit society.
    It will help you make informed decisions about the data they use and prevent the
    perpetuation of existing biases and inequalities. Additionally, understanding
    data bias will help you make informed decisions about the data collection process
    and ensure it’s representative and unbiased.
  prefs: []
  type: TYPE_NORMAL
- en: Data biases may be problematic for data scientists and college students because
    they are seldom discussed or unavailable in college courses. There is no ready-made
    fairness matrix to follow programmatically for data augmentation. Maybe by using
    the latest generative AI, the biases may even originate from computer systems
    and not be so heavily due to humans.
  prefs: []
  type: TYPE_NORMAL
- en: There are many strategies to provide protected and safe software products and
    services, but AI systems require new processes and perspectives. Trustworthy and
    responsible AI is about fairness, ethical design, and minimizing biases. Achieving
    trustworthy AI starts with transparency, datasets, **test, evaluation, validation,
    and verification** (**TEVV**), as defined by the *Standard for Identifying and
    Managing Bias in Artificial Intelligence, National Institute of Standards and
    Technology (NIST)* special publication 1270.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, Twitter corrupted the Microsoft AI chatbot **Tay** in 1 day. Microsoft
    created Tay for online casual and playful conversation. Tay was designed to learn
    and take input from raw, uncurated data and comments from the web. The Twitter
    community thought it would be fun to teach Tay with misogynistic, racist, and
    violent tweets. To this day, Tay is a poster child for lessons learned in data
    bias input for AI. As one blogger put it, “*Flaming garbage pile in, flaming garbage*
    *pile out*.”
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide a crash course on recognizing the differences in **computation**,
    **human**, and **systemic** biases. We will learn about bias but not practice
    how to compute bias programmatically. The fairness and confusion matrixes are
    used to gauge AI’s prediction in terms of **true-positive**, **false-positive**,
    **true-negative**, and **false-negative**. However, the fairness and confusion
    matrixes are used for building AI systems, not data augmentation. While looking
    at real-world text datasets, we will attempt to write Python code for a fairness
    matrix with word counts and misspelled words, but for the most part, we will rely
    on Pluto and your observations to name the biases in image and text data.
  prefs: []
  type: TYPE_NORMAL
- en: The Python code in this chapter will focus on helping you learn how to download
    real-world datasets from the *Kaggle* website. The later chapters will reuse the
    helper and wrapper functions shown in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have a deeper appreciation for a balanced
    dataset. In particular, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Computational biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systemic biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text biases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluto will begin with the easier of the three biases – computational biases.
  prefs: []
  type: TYPE_NORMAL
- en: Computational biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start, a fair warning is that you will not be learning how to write
    Python code to calculate a numeric score for computational bias in datasets. The
    primary focus of this chapter is to help you learn how to fetch real-world datasets
    from the Kaggle website and use observation to spot biases in data. There will
    be some coding to calculate the fairness or balance in the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we will compute the word counts per record and the misspelled words
    in the text datasets.
  prefs: []
  type: TYPE_NORMAL
- en: You may think all biases are the same, but it helps to break them into three
    distinct categories. The bias categories’ differences can be subtle when first
    reading about data biases. One method to help distinguish the differences is to
    think about how you could remove or reduce the error in AI forecasting. For example,
    computational biases can be resolved by changing the datasets, while systemic
    biases can be fixed by changing the deployment and access strategy of the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Computational biases originate from the unevenness in the dataset for the general
    population. In other words, it favors or underrepresents one group or data category.
    The prejudices could be unintentional or deep-seated. The data is skewed higher
    than the usual randomness. As a result, the algorithm will be plagued with higher
    false-positive and false-negative predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset representation** (**DR**) and **machine learning algorithms** (**MLAs**)
    are two types of computation biases. DR is easier to understand and more closely
    related to augmenting data. Many of the examples in this section are from DR biases.
    MLA is specific to a project and can’t be generalized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of computational biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kodak’s Shirley Cards Set Photography’s Skin-Tone Standard* from the mid-1970s
    is one of the more famous examples of technology biases. The **Shirley card**
    from Kodak is used to calibrate the image, such as skin tone and shadow, before
    printing people’s pictures. It is a part of the setting up process and is frequently
    used at the printing facility. Shirley is the name of an employee at Kodak. Because
    of this innocent and unintentional discrimination, for three decades, photos printed
    in the USA did not show the true skin tone of anyone who did not have a white
    skin tone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Open AI DALL-E 2*, from 2022, is an AI model that generates pictures
    from texts. For example, you can type the input as *a hippo eating broccoli wearing
    a pink polka dot swimsuit*, and DALL-E 2 will generate the picture. Even with
    this highly touted technology breakthrough, there are prejudices, as reported
    by the *NBC Tech* news written by Jake Traylor in the article *No quick fix: How
    OpenAI’s DALL-E 2 illustrated the challenges of bias in AI*. For example, in DALL-E,
    a builder produced images featuring only men, while the caption of a flight attendant
    generated only images of women. DALL-E 2 additionally inherits various biases
    from its training data, and its outputs sometimes reinforce societal stereotypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The United Kingdom’s **Information Commissioner’s Office** (**ICO**) disclosed
    on July 2022 that AI automation’s potential discrimination could have grave consequences
    for society. For example, it could result in unfair or biased job rejection, bank
    loans, or university acceptance. In addition, coinciding with the ICO is the *Guardian
    newspaper* article *UK data watchdog investigates whether AI systems show racial
    bias*, by Dan Milmo. The ICO’s goal is to create a fair and ethical AI system
    guideline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'Using generative AI and **Stable Diffusion** on *GitHub*, forked by Duc Haba
    on Python Notebook, Pluto wrote, “*A cute adorable baby hippo made of crystal
    ball with low poly eye surrounded by glowing aura highly detailed intricated concept
    art trending art station 8k eating broccoli in the city wearing pink polka dots.*”
    After running repeatedly with slightly altering wordings, his favorite generated
    images were created. These are the original images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Generative AI, Stable Diffusion forked](img/B17990_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Generative AI, Stable Diffusion forked
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.1* displays a hippo eating broccoli. On that fun note, we have
    concluded this section on computational biases. Pluto is a digital dog but can
    speak about human biases, which he’ll do in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Human biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human biases are even harder to calculate using Python code. There is no Python
    or other language library for computing a numeric score for human bias in a dataset.
    We rely on observation to spot such human biases. It is time-consuming to manually
    study a particular dataset before deriving possible human biases. We could argue
    that it is not a programmer’s or data scientist’s job because there is no programable
    method to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Human biases reflect systematic errors in human thought. In other words, when
    you develop an AI system, you are limited by the algorithm and data chosen by
    you. Thus, the prediction of a limited outcome could be biased by your selections.
    These prejudices are implicit in individuals, groups, institutions, businesses,
    education, and government.
  prefs: []
  type: TYPE_NORMAL
- en: There is a wide variety of human biases. Cognitive and perceptual biases show
    themselves in all domains and are not unique to human interactions with AI. There
    is an entire field of study centered around biases and heuristics in thinking,
    decision-making, and behavioral economics, such as anchoring bias and confirmation
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: As data scientists that are augmenting data, by simply being aware of the inherent
    human prejudices, we can call out the flaws in the data before developing and
    training the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples of human biases in real-world AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: The **People’s Republic of China** (**PRC**) implemented facial recognition
    AI in the province of Xinjiang to monitor ethnic minorities such as the Uyghurs.
    It is the first known example of a government using AI specifically for racial
    profiling. The system is flawed with discrimination as it identifies the poor
    and the old as ethnic minorities in false-positive predictions. Compounding the
    problem is when Myanmar bought the PRC system to crack down on political dissidents,
    as the **Council on Foreign Relations** reported in their *The Importance of International
    Norms in Artificial Intelligence Ethics* article in 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To stop advertisers from abusing the AI Facebook newsfeed, Meta limited the
    target algorithm from using health, race, ethnicity, political affiliation, religion,
    and sexual orientation. **NPR** reported in the article that Facebook had scrapped
    advertised targeting based on politics, race, and other “*sensitive*” topics.
    The changes took effect on January 10 across Meta’s apps, including Facebook,
    Instagram, Messenger, and the Audience Network. It is reported that advertisers
    microtargeted people with tailored messages. In other words, the advertisers excluded
    people based on protected characteristics and targeted advertisements using anti-Semitic
    phrases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *Racial Bias Found in a Major Health Care Risk Algorithm*, published
    by **Scientific American** on October 4, 2019, found many biases in the healthcare
    system. Black patients would pay more for interventions and emergency visits.
    In addition to incurring higher costs, black patients would receive lesser-quality
    care. AI scientists used race and wealth in historical data to train the healthcare
    system. Thus, the system displayed prejudice toward minority groups and affected
    200 million Americans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'This challenge is a thought experiment. How could you build an AI without biases,
    given a substantial budget and ample time? Hint: think about when we had world
    peace or no crime in our city. It can’t be an absolute answer. It has to be a
    level of acceptance.'
  prefs: []
  type: TYPE_NORMAL
- en: It may be challenging to see the differences between human and computational
    biases. Some biases are not one or the other. In other words, they are not mutually
    exclusive – you can have both human and computational biases in one AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Human biases are difficult to identify because they shape our perception of
    the world. However, systemic biases may be easier to address in theory, but may
    be challenging to put into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Systemic biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we cannot conceive a method to calculate computational and human biases,
    then it is impossible to devise an algorithm to compute systemic biases programmatically.
    We must rely on human judgment to spot the systemic bias in the dataset. Furthermore,
    it has to be specific to a particular dataset with a distinct AI prediction goal.
    There are no generalization rules and no fairness matrix to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Systemic biases in AI are the most notorious of all AI biases. Simply put, systemic
    discrimination is when a business, institution, or government limits access to
    AI benefits to a group and excludes other underserved groups. It is insidious
    because it hides behind society’s existing rules and norms. Institutional racism
    and sexism are the most common examples. Another AI accessibility issue in everyday
    occurrences is limiting or excluding admission to people with disabilities, such
    as the sight and hearing impaired.
  prefs: []
  type: TYPE_NORMAL
- en: The poor and the underserved have no representation in the process of developing
    the AI system, but they are forced to accept the AI’s prediction or forecast.
    These AIs make significant life decisions, including those for the poor and underserved,
    such as how much to pay for a car or health insurance, options for housing or
    a business bank loan, or whether they are eligible for medical treatments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few real-world examples of AI systemic biases:'
  prefs: []
  type: TYPE_NORMAL
- en: The article *Researchers use AI to predict crime, biased policing in major U.S.
    cities like L.A.*, published by The Los Angeles Times on July 4, 2022, found AI
    biases in policing crimes. The University of Chicago’s AI crime prediction system
    does not address law enforcement systemic biases. The forecast for possible crime
    locations, or hot spots, is based on flawed input and environmental factors associated
    with poor neighborhoods, rather than the actual locations where crimes are committed.
    The AI reflects the systemic bias in law enforcement practices and procedures.
    Thus, it forecasts a higher crime rate in poor neighborhoods because of the police’s
    prior systemic biases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The US Department of Justice reviewed the **Prisoner Assessment Tool Targeting
    Estimated Risk and Needs** (**PATTERN**) and found systemic bias in who can access
    PATTERN. This was discussed in *Addressing an Algorithmic PATTERN of Bias*, published
    by the Regulatory Review on May 10, 2020\. The report reinforces the Justice Department’s
    biased view that low-risk criminals should be the only ones eligible for early
    release. PATTERN classifies inmates as low, medium, or high risk, which forecasts
    if those individuals would engage in crime after release. Since PATTERN is limited
    to a particular group, it precludes other inmates from early release.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article *The Potential For Bias In Machine Learning And Opportunities For
    Health Insurers To Address It* reports growing concerns about how ML can reflect
    and perpetuate past and present systemic inequities and biases. This was published
    by Health Affairs in February 2022 published the article. Limited access to the
    likelihood of hospitalization, admission to pharmacies, and missing or incomplete
    data are a few systemic biases for racism and underrepresented populations. Thus,
    the AI predictions may reflect those systemic biases, and the policy decisions
    based on the forecast risk reinforcing and exacerbating existing inequities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'This challenge is a thought experiment. Which category of biases is easier
    to spot? Hint: think about company profit.'
  prefs: []
  type: TYPE_NORMAL
- en: Computation, human, and systemic biases have similarities and are not mutually
    exclusive. There is no algorithm or libraries to guide you in coding. It relies
    on your observation from studying the datasets. At this point, Pluto is ready
    to learn about fetching real-world datasets from the *Kaggle* website. Optionally,
    he will ask you to spot the biases in the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Python Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s coding lessons primarily focus on downloading real-world datasets
    from the *Kaggle* website. The later chapters rely on or reuse these fetching
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about this book’s general rules for development
    on the Python Notebook. The object-oriented class named **Pluto** contains the
    methods and attributes, and you add new methods to Pluto as you learn new concepts
    and techniques. Review [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016) if you
    are uncertain about the development philosophy.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, the term **P****ython Notebook** is used synonymously for **Jupyter
    Notebook**, **JupyterLab**, and **Google** **Colab Notebook**.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to change the object’s name from `pluto.draw_batch_image()`
    becomes `sandy.draw_batch_image()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with this chapter, the setup process for using the Python Notebook
    will be the same for every chapter. The goal of this chapter is to help you gain
    a deeper understanding of the datasets and not to write Python code for calculating
    the bias value for each dataset. The setup steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load Python Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clone GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create Kaggle ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download real-world datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start with loading the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Python Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to locate the `data_augmentation_with_python_chapter_2.ipynb`
    file. It is in this book’s GitHub repository at [https://github.com/PacktPublishing/Data-Augmentation-with-Python](https://github.com/PacktPublishing/Data-Augmentation-with-Python).
    Refer to [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016) if you forgot how to
    load the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to clone the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second step is locating the `~/Data-Augmentation-with-Python/pluto/pluto_chapter_1.py`
    file. It’s in the main GitHub repository for this book, under the `pluto` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto is using the Python Notebook on **Google Colab**. It starts with a new
    session every time – that is, no permanent storage is saved from the previous
    session. Thus, the faster and easier method to load all the required files is
    to clone the GitHub repository. It could be this book’s GitGub or the GitHub repository
    that you forked.
  prefs: []
  type: TYPE_NORMAL
- en: From this point onward, all commands, code, and references are from the `data_augmentation_with_python_chapter_2.ipynb`
    Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the`!git clone {url}` command to clone a GitHub repository, where
    `{url}` is the link for the GitHub repository. The code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the Python Notebook, any code cell that begins with an exclamation point
    (`!`) will tell the system to run as a system shell command line. For Google Colab,
    it is a `%`) are special commands. They are called **magic keywords** or **magic
    commands**.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebook’s built-in magic commands provide convenience functions to
    the underlying `%`). For example, `%ldir` is for listing the current directory
    files, `%cp` is for copying files in your local directory, `%debug` is for debugging,
    and so on. The helpful `%lsmagic` command is for listing all the available magic
    commands supported by your current Python Notebook environment. The exclamation
    character (`!`) is for running the underlying OS command-line function. For example,
    in a Linux system, `!ls -la` is for listing the files in the current directory,
    while `!pip` is for installing Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have downloaded the Pluto Python code, the next step is to instantiate
    Pluto.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Python Notebook’s magic command for instantiating Pluto is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The next-to-last step in the setup process is to verify that Pluto is running
    with the correct version.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For double-checking, Pluto runs the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The results should be similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since this code is from [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016), the
    Pluto version is **1.0**. Before Pluto can download the dataset from the *Kaggle*
    website, he needs Kaggle’s **key** and **access token**.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle ID
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pluto uses Kaggle datasets because he wants to learn how to retrieve real-world
    data for learning data augmentation. It is more impactful than using a small set
    of dummy data. Thus, the first two steps are installing the library to aid in
    downloading the Kaggle data and signing up with [Kaggle.com](https://Kaggle.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for installing and importing can be found in the open source **opendatasets**
    library by **Jovian**. The function code is in the Python Notebook; here is a
    code snippet from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After you create an account on [Kaggle.com](http://Kaggle.com), you will have
    a **Kaggle username** and receive a **Kaggle key**. Next, go to the **Account**
    page, scroll down to the **API** section, and click on the **Create New API Token**
    button to generate the **Kaggle key**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Kaggle Account page – new token](img/B17990_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Kaggle Account page – new token
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a `remember_kaggle_access_key()` wrapper method to store the
    attributes inside the object. The code uses the Python `self` keyword to store
    this information – for example, `self.kaggle_username`. The method’s definition
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Other methods will use these attributes automatically. Pluto runs the following
    method to remember your Kaggle username and key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `_write_kaggle_credit()` method writes your Kaggle username and key in two
    locations – `~/.kaggle/kaggle.json` and `./kaggle.json`. It also changes the file
    attribute to `0o600`. This function begins with an underscore; hence, it is a
    helper function used primarily by other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two methods for Pluto to fetch data from Kaggle: `fetch_kaggle_comp_data(competition_name)`,
    where `competition_name` is the title of the contest, and `fetch_kaggle_dataset(url)`,
    where `url` is the link to the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `fetch_kaggle_comp_data()` wrapper method, the primary code line that
    does most of the work is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `fetch_kaggle_dataset()` method, the primary code line that does most
    of the work is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: As of 2022, there are over 2,500 past and current competitions on the Kaggle
    website and more than 150,000 datasets. These datasets are diverse, from medical
    and financial to other industry-specific datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Image biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pluto has access to thousands of datasets, and downloading these datasets is
    as simple as replacing the **URL**. In particular, he will download the following
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: The *State Farm distracted drivers detection (**SFDDD)* dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Nike* *shoes* dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Grapevine* *leaves* dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the SFDDD dataset.
  prefs: []
  type: TYPE_NORMAL
- en: State Farm distracted drivers detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, Pluto will slow down and explain every step in downloading the real-world
    datasets, even though he will use a wrapper function, which seems deceptively
    simple. Pluto will not write any Python code for programmatically computing the
    bias fairness matrix values. He relies on your observation to spot the biases
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Give Pluto a command to fetch, and he will download and **unzip** or **untar**
    the data to your local disk space. For example, in retrieving data from a competition,
    ask Pluto to fetch it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this data is from a competition, you must join the State Farm competition
    before downloading the dataset. You should go to the **State Farm Distracted Driver
    Detection** competition and click the **Join** button. The description for the
    competition from the *Kaggle* website is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “State Farm hopes to improve these alarming statistics and better insure their
    customers by testing whether dashboard cameras can automatically detect drivers
    engaging in distracting behaviors. Given a dataset of 2D dashboard camera images,
    State Farm is challenging Kagglers to classify each driver’s behavior.”
  prefs: []
  type: TYPE_NORMAL
- en: '*State Farm* provided the dataset, announced in 2016\. The rules and usage
    licenses can be found at [https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/rules](https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/rules).'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: You have to join a Kaggle competition to download competition data, but you
    don’t need to enter a competition to download a Kaggle dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all the methods are in the Python Notebook’s `pluto`. For example, you
    can’t do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**# example of** **wrong syntax**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, using the `pluto` prefix is correct, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before Pluto displays the image in batches, he must write a few simple code
    lines to check if the downloads are correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – State Farm Distracted driver](img/B17990_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – State Farm Distracted driver
  prefs: []
  type: TYPE_NORMAL
- en: 'The SFDDD dataset consists of 22,423 images, and viewing one photo at a time,
    as shown in *Figure 2**.3*, will not help Pluto to see the biases. Pluto loves
    putting lists and tabular data into the Python pandas library. Luckily, the State
    Farm competition comes with a `fetch_df(self, csv)` method easier. The relevant
    line of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `fetch_df(self, csv)` wrapper function to download the data,
    and he uses Pandas to display the last three rows. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – State Farm data – last three rows](img/B17990_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – State Farm data – last three rows
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto likes the data in the original CSV file, shown in *Figure 2**.4*, but
    it does not have a column with a full path to an image file. Pandas makes creating
    a new column containing the full image path super easy. There are no complicated
    `build_sf_fname(self, df)`, where `df` is the original DataFrame. The code snippet
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. Pluto adds the
    full path name column and displays the first three rows with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – State Farm data – full path image name](img/B17990_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – State Farm data – full path image name
  prefs: []
  type: TYPE_NORMAL
- en: 'For double-checking, Pluto writes a few lines of simple code to display an
    image from the pandas `fname` column, as shown in *Figure 2**.5*, using the **PIL**
    library. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – State Farm data – the fname column](img/B17990_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – State Farm data – the fname column
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.6* shows a driver. Using the `fname` column, drawing a batch or
    collection of images is relatively easy. The `draw_batch()` wrapper function’s
    definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: = `df_filenames` is the list of file =names, and it is in a pandas DataFrame.
    `disp_max` defaults to 10, which is an increment of 5, as in five photos per row.
    `is_shuffle` defaults to `False`. If you can set it to `True`, each batch is randomly
    selected. Lastly, `figsize` is the size of the output from the `(16,8)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `draw_batch()` wrapper method, Pluto can draw any photo collection.
    For example, Pluto can draw 10 random images from the SFDDD competition with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.7 – State F\uFEFFarm data – draw_patch()](img/B17990_02_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – State Farm data – draw_patch()
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto runs the code repeatedly to see different images in the dataset, as shown
    in *Figure 2**.7*. For example, he can draw 20 random images at a time using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – State Farm data – 20 randomly selected images](img/B17990_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – State Farm data – 20 randomly selected images
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.8* displays 20 photos of drivers. Using the `fetch_kaggle_comp_data()`,
    `fetch_df()`, and `draw_batch()` wrapper functions, Pluto can retrieve any of
    the thousand real-world datasets from Kaggle.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is a thought experiment. Before reading Pluto’s answer, what
    biases do you see in the images? It is optional, and there is no algorithm or
    library that you can use to compute the bias fairness value. It relies on your
    observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto read the SFDDD’s goal and thought about computational, human, and systemic
    biases. The following bullet points are not errors to be fixed, but they could
    be biases. These biases are observations from *Figure 2**.7* of underrepresented
    groups. Pluto assumes the long-term goal of the SFDDD is for it to be deployed
    across the United States:'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto does not see any older adults as drivers in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The driver demographic distribution is limited. There are about a dozen drivers
    represented in the dataset, and the long-term goal is to deploy this AI system
    in the United States. Therefore, the AI system will be trained on a limited number
    of drivers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are few vehicle types represented in the dataset. They are primary sedans,
    compacts, or SUVs. A sports car or truck interior is different, which might affect
    the prediction of false positives or false negatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other distracting activities while driving that are not represented,
    such as eating ice cream, watching an event unfolding outside of the car, head
    or hair grooming, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All drivers in the dataset wear urban-style clothing. More elaborate or ethnic-centric
    clothing styles might cause the AI to predict false positives or false negatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to save lives. Thus, a systemic bias could be affordable access
    to everyone, not just the tech-savvy urban elites.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is a thought experiment. Can you find other biases? There are
    no absolute right or wrong answers. The biases listed here can’t be spotted programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: That was a detailed discussion of the SFDDD dataset. Pluto will fetch another
    dataset from the *Kaggle* website, the *Nike* *shoes* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Nike shoes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Nike shoes dataset was chosen because it will show different biases. Like
    the State Farm photos, there is no algorithm or library to compute the fairness
    matrix. We rely on Pluto and your observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Nike, Adidas, and Converse Shoes Images* (Nike) dataset contains images
    in folders; there is no **CSV** file. The Nike dataset’s description on the Kaggle
    website is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “*This dataset is ideal for performing multiclass classification with deep neural
    networks such as CNNs or simpler machine learning classification models. You can
    use TensorFlow, its high-level API Keras, sklearn, PyTorch, or other deep/machine*
    *learning libraries.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'The author is *Iron486*, and the license is **CC0: Public** **Domain**: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no CSV file for Pluto to import into pandas, Pluto has written
    the `build_df_fname(self,` `start_path)` method, where `start_path` is the directory
    where the data is stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key code line is the `os.walk()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto will perform the three familiar steps for reviewing the Nike dataset.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Nike data – 20 randomly selected images](img/B17990_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Nike data – 20 randomly selected images
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is Pluto’s list of data biases observations from *Figure 2**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: The shoes are too clean. Where are the muddy or dirty shoes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The photos are professionally taken. Thus, when the AI-powered app is deployed,
    people might find their app giving a wrong prediction because their pictures are
    taken haphazardly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lack of shoe images in urban, farming, or hiking settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s ask Pluto to grab one more image dataset before switching gears and digging
    into the text dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Grapevine leaves
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Grapevine leaves dataset is the third and last example of a real-world image
    dataset Pluto will fetch from the Kaggle website. The primary goal is for you
    to practice downloading datasets and importing the metadata into pandas. Incidentally,
    Pluto will use the Grapevine leaves dataset to name other types of data biases
    through observation. He does not rely on defining a fairness matrix through coding
    because it not yet feasible. Maybe the next level of generative AI will be able
    to process all the photos in a dataset and deduce the biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an excerpt from the Grapevine leaves dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: “The main product of grapevines is grapes that are consumed fresh or processed.
    In addition, grapevine leaves are harvested once a year as a by-product. The species
    of grapevine leaves are important in terms of price and taste.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors are *Koklu M., Unlersen M. F., Ozkan I. A., Aslan M. F., and Sabanci
    K.*, and the license is **CC0: Public** **Domain**: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The filenames in the Grapevine dataset contains a space character in the filename,
    which may confuse many Python libraries. Thus, Pluto runs a few simple Linux scripts
    to convert the space into an underscore. The code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After cleaning up the filenames, Pluto will perform the three familiar steps
    for fetching, importing, and displaying the Grapevine dataset. The images are
    in the same folder structure as the Nike photos. Thus, Pluto reuses the same `pluto.fetch_df()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Grapevine data – 20 randomly selected images](img/B17990_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Grapevine data – 20 randomly selected images
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is Pluto’s list of data biases from *Figure 2**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: The photos are too perfect, and undoubtedly, they are uncomplicated to augment
    and train, but how does the general public use the AI system? If the winemakers
    access the AI system through an iPhone, the grapevine leaf pictures they take
    are nothing like the flawless photos in the dataset. The resulting predictions
    could be false positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to the perfect photo bias, the leaf is flat, and the background is white,
    which is not common in real-world usage. The training cycle will achieve high
    accuracy, but it is unsuitable for real-world use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is trained as-is and deployed, then the resulting AI will have
    a systemic bias, only being available for lab technicians and not farmers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: There are thousands of image datasets on the *Kaggle* website. Pluto challenges
    you to select, download, display, and list the biases for three different image
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Other than Distracted Drivers, Nike shoes, and Grapevine Leaves, there are more
    examples in the Python Notebook. However, next, Pluto will move on from biases
    to text augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Text biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should recognize the patterns for fetching real-world image datasets
    and importing metadata into pandas. It is the same pattern for text datasets.
    Pluto will guide you through two sessions and use his power of observation to
    name the biases. He could employ the latest in generative AI such as OpenAI GPT3
    or GPT4 to list the biases in the text. Maybe he will do that later, but for now,
    he will use his noggin. Nevertheless, Pluto will attempt to write Python code
    to gain insight into the texts' structures, such as the word count and misspelled
    words. It is not the fairness matrix but a step in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto searches the Kaggle website for the **Natural Language Processing** (**NLP**)
    dataset, and the result consists of over 2,000 datasets. He chooses the *Netflix
    Shows* and the *Amazon Reviews* datasets. Retrieving and viewing the NLP dataset
    follows the same fetching, importing, and printing steps outlined in the image
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the Netflix data.
  prefs: []
  type: TYPE_NORMAL
- en: Netflix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto reuses the wrapper function to download the data. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The Netflix dataset’s description from the Kaggle website is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “The raw data is web scrapped through Selenium. It contains unlabelled text
    data of around 9,000 Netflix shows and movies, along with full details such as
    cast, release year, rating, description, and so on.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The author is *InFamousCoder*, and the license is **CC0: Public** **Domain**:
    [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to import the data into a pandas DataFrame. The Netflix
    data comes with a `fetch_df()` method to import the Netflix reviews into the DataFrame
    and displays the first three rows, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Netflix data, left columns](img/B17990_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Netflix data, left columns
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.11* displays the Netflix metadata. The first two steps do not require
    Pluto to write new code, but Pluto has to write code for the third step, which
    is to display the movie’s title and description. The goal is for Pluto to find
    any biases in the movie description.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas made writing the `display_batch_text()` wrapper method effortless. The
    method has no **loops**, **index counter**, **shuffle algorithm**, **or if-else**
    statements. There are just three lines of code, so Pluto displays the code in
    its entirety here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto displays the Netflix movies’ titles and descriptions in batch using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Netflix movie title and description](img/B17990_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Netflix movie title and description
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Every time Pluto runs the `print_batch_text()` wrapper function, movie titles
    and descriptions are displayed. It would be best to run the wrapper function repeatedly
    to gain more insight into the data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.12* displays a text batch. Pluto has read hundreds of movie descriptions
    and found no apparent bias. It is a job for a linguist. In general, the English
    language can have the following biases:'
  prefs: []
  type: TYPE_NORMAL
- en: Religious bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethnicity bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Racial bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mental health bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Former felon bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elitism bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LGBTQ bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disability bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluto is not a linguist, but there are other data attributes could contribute
    to language biases, such as word count and misspelled words. In other words, are
    the Netflix movie descriptions all relatively the same length? And are there many
    misspelled words?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an attempt to code a small fraction of the fairness matrix. When using
    a pandas DataFrame, the `count_words()` method has one line of code. It is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto counted the number of words in the Netflix movie and double-checked the
    result by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Movie description word count](img/B17990_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Movie description word count
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.13* displays the word count for each record. The next step is to
    plot the word count using the `draw_word_count()` function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. Pluto draws the
    BoxPlot and Histogram graphs with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Netflix movie description word count](img/B17990_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Netflix movie description word count
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 2**.14*, the BoxPlot and Histogram plots show that the distribution
    is even. There are a few outliers, the mean is 23.88, and the bulk of the Netflix
    movie descriptions are between 22 and 25 words. Thus, there is no bias here. Pluto
    investigates the misspelled words next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `pip` command to install the `spellchecker` class. The `check_spelling()`
    method takes the pandas DataFrame and the designated column as parameters. The
    function key code lines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto checks the Netflix movie descriptions'' spelling and uses the `print_batch_text()`
    function to display the result. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Netflix misspelled words](img/B17990_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Netflix misspelled words
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.15* displays the misspelled words. Pluto displays this data in
    graphs by reusing the same `draw_word_count()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Netflix misspelled words graph](img/B17990_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Netflix misspelled words graph
  prefs: []
  type: TYPE_NORMAL
- en: The misspelled words are mostly person or product names, as shown in *Figure
    2**.16*. The average is 0.92 per Netflix movie description and there are only
    a handful of outliners. Without a linguist’s help, Pluto can’t find any biases
    in the Netflix movie description. Let’s move on to the Amazon reviews and see
    if we can find any biases.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Amazon reviews dataset is the last real-world text dataset to download for
    this chapter. Pluto follows the same pattern, and you should now be comfortable
    with the code and ready to download any real-world datasets from the Kaggle website.
    In addition, as with the Netflix data, Pluto will use his powerful insight, as
    a digital dog, to find the biases in the text. He will use the same techniques
    and library to programmatically find the word count and misspelled words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto will not explain how the code is written for the Amazon reviews because
    he re-used the same functions in the Netflix data. The complete code can be found
    in the Python Notebook. The bare code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The data description of the Amazon reviews dataset on Kaggle is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “One of the most important problems in eCommerce is the correct calculation
    of the points given to after-sales products. The solution to this problem is to
    provide greater customer satisfaction for the eCommerce site, product prominence
    for sellers, and a seamless shopping experience for buyers. Another problem is
    the correct ordering of the comments given to the products. The prominence of
    misleading comments will cause both financial losses and customer losses.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The author is *Tarık kaan Koç*, and the license is **CC BY-NC-SA** **4.0**:
    [https://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto prints the batch using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Amazon reviews misspelled words](img/B17990_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Amazon reviews misspelled words
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto has chosen to display two data columns in the `print_batch` function,
    as shown in *Figure 2**.17*, but there are 12 data columns in the dataset. They
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reviewerName`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overall`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reviewText`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reviewTime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`day_diff`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helpful_yes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`helpful_no`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_vote`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score_pos_neg_diff`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score_average_rating`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wilson_lower_bound`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pluto draws the word counts and the misspelled words using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The result for the word counts is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Amazon reviews word count](img/B17990_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Amazon reviews word count
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the graph for the misspelled words:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.19 – Amazon reviews misspelled\uFEFF words graph](img/B17990_02_19.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Amazon reviews misspelled words graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto notices that the biases in the Amazon reviews, as shown in *Figures 2.17*,
    *2.18*, and *2.19*, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There are more grammatical errors in the Amazon reviews than in the Netflix
    movie description. Thus, there could be bias against well-written reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more technical product names and jargon in the reviews. Therefore,
    there could be bias against non-technical reviewers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many outlines. The mean is 50.46 words per review, with the bulk feedback
    between 20 and 180 words. It is worth digging deeper using other columns, such
    as `helpful_yes`, `total_vote`, and `score_pos_neg_diff`, to see if there is bias
    in the review length per category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Amazon reviews have more misspelled words than the Netflix movie description,
    reinforcing the well-written reviewer’s bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before jumping into the summary, here is a fun fact.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'Cathy O’Neil’s book, *Weapons of Math Destruction: How Big Data Increases Inequality
    and Threatens Democracy*, published in 2016, describes many biases in algorithms
    and AI, and it is a must-read for data scientists and college students. The two
    prominent examples are an accomplished teacher fired by a computer algorithm and
    a qualified college student rejected by the candidate screening software.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was not a typical one in this book because we discussed more theory
    than practical data augmentation techniques. At first, the link between data biases
    and data augmentation seems tenuous. Still, as you begin to learn about computational,
    human, and systemic biases, you see the strong connection because they all share
    the same goal of ensuring successful ethical AI system usage and acceptance.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, data augmentation increases the AI’s prediction accuracy while
    reducing the data biases in augmenting, ensuring the AI forecast has fewer false-negative
    and true-negative outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The computational, human, and systemic biases are similar but are not mutually
    exclusive. However, providing plenty of examples of real-world biases and observing
    three real-world image datasets and two real-world text datasets made these biases
    easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: The nature of data bias in augmenting makes it challenging to compute biases
    programmatically. However, you learned to write Python code for the fairness matrix
    in the text dataset using word counts and misspelled word techniques. You could
    use generative AI, such as Stable Diffusion or DALL-E, to automatically spot the
    biases in the photo and use OpenAI GPT3, GPT4, or Google Bard to compute the biases
    in text data. Unfortunately, generative AI is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, Pluto tended to go slow with step-by-step explanations, but as you
    learned, he shortened the justification and showed only the bare minimum code.
    The complete code can be found in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the Python code is devoted to teaching you how to download real-world
    datasets from the *Kaggle* website and importing the metadata into pandas. The
    later chapters will reuse these helper and wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, there were *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto looks forward to [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058), where
    he will play with image augmentation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Image Augmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B17990_03.xhtml#_idTextAnchor058), *Image Augmentation for Classification*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B17990_04.xhtml#_idTextAnchor082), *Image Augmentation for Segmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
