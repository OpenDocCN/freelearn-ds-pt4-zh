<html><head></head><body>
		<div id="_idContainer078">
			<h1 id="_idParaDest-58" class="chapter-number"><a id="_idTextAnchor058"/>3</h1>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Image Augmentation for Classification</h1>
			<p>Image augmentation<a id="_idIndexMarker195"/> in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) is a stable diet for increasing prediction accuracy, especially for the image classification domain. The causality logic is linear, meaning the more robust the data input, the higher the <span class="No-Break">forecast accuracy.</span></p>
			<p><strong class="bold">Deep learning</strong> (<strong class="bold">DL</strong>) is a <a id="_idIndexMarker196"/>subset of ML that uses artificial neural networks to learn patterns and forecast based on the input data. Unlike traditional ML algorithms, which depend on programmer coding and rules to analyze data, DL algorithms automatically learn, solve, and categorize the relationship between data and labels. Thus, expanding the datasets directly impacts DL predictions on new insights that the model has not seen in the <span class="No-Break">training data.</span></p>
			<p>DL algorithms are designed to mimic the human brain, with layers of neurons that process information and pass it on to the next layer. Each layer of neurons learns to extract increasingly complex features from the input data, allowing the network to identify patterns and make predictions with <span class="No-Break">increasing accuracy.</span></p>
			<p>DL for image classification has proven highly effective in various industries, ranging from healthcare, finance, transportation, and consumer products to social media. Some examples include <em class="italic">identifying 120 dog breeds</em>, <em class="italic">detecting cervical spine fractures</em>, <em class="italic">cataloging landmarks</em>, <em class="italic">classifying Nike shoes</em>, <em class="italic">spotting celebrity faces</em>, and <em class="italic">separating paper and plastic </em><span class="No-Break"><em class="italic">for recycling</em></span><span class="No-Break">.</span></p>
			<p>There is no standard formula to estimate how many images you need to achieve a designer prediction accuracy for image classification. Acquiring additional photos may not be a viable option because of cost and time. On the other hand, image data augmentation is a cost-effective technique that increases the number of photos for image <span class="No-Break">classification training.</span></p>
			<p>This chapter consists of two parts. First, you will learn the concepts and techniques of augmentation for image classification, followed by hands-on Python coding and a detailed explanation of the image <span class="No-Break">augmentation techniques.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The image dataset is typically broken into 75% training, 20% validation, and 5% testing in the image classification model. Typically, the images allotted for training are augmented but outside the validation and <span class="No-Break">testing set.</span></p>
			<p>The two primary approaches for image augmentation are pre-processing and dynamic. They share the same techniques but differ when augmentation is done. The pre-processing method creates and saves the augmented photos in disk storage before training, while the dynamic method expands the input images during the <span class="No-Break">training cycle.</span></p>
			<p>In <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, you learned about data biases, and it is worth remembering that image augmentation will increase the DL model’s accuracy and may also increase <span class="No-Break">the biases.</span></p>
			<p>In addition to biases, the other noteworthy concept is <em class="italic">safety</em>. It refers to the distortion magnitude that does not alter the original image label post-transformation. Different photo domains have different <em class="italic">safety</em> levels. For example, horizontally flipping a person’s portrait photo is an acceptable augmentation technique, but reversing the hand gesture images in sign language <span class="No-Break">is unsafe.</span></p>
			<p>By the end of this chapter, you will have learned the concepts and hands-on techniques in Python coding for classification image augmentation using real-world datasets. In addition, you will have examined several Python open source libraries for image augmentation. In particular, this chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Geometric transformations</span></li>
				<li><span class="No-Break">Photometric transformations</span></li>
				<li><span class="No-Break">Random erasing</span></li>
				<li><span class="No-Break">Combining</span></li>
				<li>Reinforcing your learning through <span class="No-Break">Python code</span></li>
			</ul>
			<p>Geometric transformations <a id="_idIndexMarker197"/>are the primary image augmentation technique used commonly across multiple image datasets. Thus, this is a good place to begin discussing <span class="No-Break">image augmentation.</span></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>Geometric transformations</h1>
			<p>Geometric transformation <a id="_idIndexMarker198"/>alters the photo’s geometry, which is done by flipping along the X-axis or Y-axis, cropping, padding, rotating, warping, and translation. Complex augmentation uses these base photo-altering techniques. While working with geometric transformations, the distortion magnitude has to be kept to a safe level, depending on the image topic. Thus, no general formula governing geometric transformation applies to all photos. In the second half of this chapter, the Python coding section, you and Pluto will download real-world image datasets to define the safe level for each <span class="No-Break">image set.</span></p>
			<p>The following techniques are not mutually exclusive. You can combine horizontal flipping with cropping, resizing, padding, rotating, or any combination thereof. The one constraint is the safe level <span class="No-Break">for distortion.</span></p>
			<p>In particular, you will learn the <span class="No-Break">following techniques:</span></p>
			<ul>
				<li><span class="No-Break">Flipping</span></li>
				<li><span class="No-Break">Cropping</span></li>
				<li><span class="No-Break">Resizing</span></li>
				<li><span class="No-Break">Padding</span></li>
				<li><span class="No-Break">Rotating</span></li>
				<li><span class="No-Break">Translation</span></li>
				<li><span class="No-Break">Noise injection</span></li>
			</ul>
			<p>Let’s start <span class="No-Break">with flipping.</span></p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Flipping</h2>
			<p>The two <a id="_idIndexMarker199"/>flipping types are the horizontal Y-axis and the vertical X-axis. Turning the photos along the Y-axis is like looking in a mirror. Therefore, it <a id="_idIndexMarker200"/>can be used for most types of pictures except for directional images such as street signs. There are many cases where rotating along the X-axis is not safe, such as landscape or cityscape images where the sky should be at the top of <span class="No-Break">the picture.</span></p>
			<p>It is not an either-or proposition, and the image can use horizontal and vertical flips, such as aerial photos from a plane. Therefore, flipping is generally safe to use. However, some pictures are <a id="_idIndexMarker201"/>not safe for either transformation, such as street signs, where any rotation changes the integrity of the original <span class="No-Break">label post-translation.</span></p>
			<p>Later in this <a id="_idIndexMarker202"/>chapter, you will learn how to flip images using Python code with an image augmentation library, but for now, here is a teaser demonstration. The function’s name is <strong class="source-inline">pluto.draw_image_teaser_flip()</strong>; the explanation will <span class="No-Break">come later.</span></p>
			<p>The image output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B17990_03_01.jpg" alt="Figure 3.1 – Image vertic﻿al flip"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Image vertical flip</p>
			<p>Flipping keeps all the image content intact. However, the following technique, known as cropping, <span class="No-Break">loses information.</span></p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>Cropping</h2>
			<p>Cropping<a id="_idIndexMarker203"/> an <a id="_idIndexMarker204"/>image involves removing the edges of the picture. Most <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) use a <a id="_idIndexMarker205"/>square image as input. Therefore, photos in portrait or landscape mode are regularly chopped to a square image. In most cases, the removal of the edges is based on the center of the picture, but there is no rule implying it has to be the center of <span class="No-Break">the image.</span></p>
			<p>The photo’s center <a id="_idIndexMarker206"/>point is 50% of the width and 50% of the height. However, in image augmentation, you can choose to move the cropping <a id="_idIndexMarker207"/>center to 45% of the width and 60% of the height. The cropping center can vary, depending on the photo’s subject. Once you have identified the safe range for moving the cropping center, you can try dynamically cropping the images per training epoch. Thus, every training epoch has a different set of photos. The effect is that the ML model will likely not overfit and gives higher accuracy from having <span class="No-Break">more images.</span></p>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_crop()</strong> function is another teaser demonstration. Moving forward, I will only display the teaser images for some augmentation methods since you will learn about all of them in more depth by using Python code later in <span class="No-Break">this chapter.</span></p>
			<p>The output <a id="_idIndexMarker208"/>image<a id="_idIndexMarker209"/> for <strong class="bold">center cropping</strong> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B17990_03_02.jpg" alt="Figure 3.2 – Image center crop"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Image center crop</p>
			<p>Cropping is not the same as resizing an image, which we will <span class="No-Break">discuss next.</span></p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>Resizing</h2>
			<p>Resizing <a id="_idIndexMarker210"/>can be done by keeping the aspect ratio the <a id="_idIndexMarker211"/>same <span class="No-Break">or not:</span></p>
			<ul>
				<li><strong class="bold">Zooming</strong> is the <a id="_idIndexMarker212"/>same<a id="_idIndexMarker213"/> as enlarging, cropping, and maintaining the same <span class="No-Break">aspect ratio.</span></li>
				<li><strong class="bold">Squishing</strong> is the<a id="_idIndexMarker214"/> same as <a id="_idIndexMarker215"/>enlarging or shrinking and changing the original aspect ratio. The safe level for zooming, squishing, or other resizing techniques depends on the <span class="No-Break">image category.</span></li>
			</ul>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_resize()</strong> function is a fun demonstration of <strong class="bold">resizing</strong> an<a id="_idIndexMarker216"/> image using the <strong class="bold">squishing</strong> mode. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B17990_03_03.jpg" alt="Figure 3.3 – Image resizing with squishing mode"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Image resizing with squishing mode</p>
			<p>When resizing a photo and not keeping the original aspect ratio, you need to pad the new image. There are different methods <span class="No-Break">for </span><span class="No-Break"><strong class="bold">padding</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Padding</h2>
			<p>Padding<a id="_idIndexMarker217"/> involves filling the outer edge of the canvas <a id="_idIndexMarker218"/>that is not an image. There are three popular methods <span class="No-Break">for padding:</span></p>
			<ul>
				<li><strong class="bold">Zero padding</strong> refers <a id="_idIndexMarker219"/>to padding the image with black, white, gray, or <span class="No-Break">Gaussian noise</span></li>
				<li><strong class="bold">Reflection padding</strong> mirrors<a id="_idIndexMarker220"/> the padding area with the <span class="No-Break">original image</span></li>
				<li><strong class="bold">Border padding</strong> involves <a id="_idIndexMarker221"/>repeating the borderline in the <span class="No-Break">padding section</span></li>
			</ul>
			<p>Padding<a id="_idIndexMarker222"/> is used in combination with cropping, resizing, translation, and rotating. Therefore, the safe proportion depends on cropping, resizing, <span class="No-Break">and rotating.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Rotating</h2>
			<p>Rotating <a id="_idIndexMarker223"/>an image involves turning the picture <a id="_idIndexMarker224"/>clockwise or counterclockwise. The measurement of turning is by a degree and clockwise direction. Therefore, turning 180 degrees is the same as flipping vertically, while rotating 360 degrees returns the photo to its <span class="No-Break">original position.</span></p>
			<p>General rotating <a id="_idIndexMarker225"/>operates on the X-Y plane, whereas turning in the Z plane is<a id="_idIndexMarker226"/> known as <strong class="bold">tilting</strong>. <strong class="bold">Skewing</strong> or <strong class="bold">shearing</strong> involves<a id="_idIndexMarker227"/> rotating <a id="_idIndexMarker228"/>on all <a id="_idIndexMarker229"/>three planes – that is, X, Y, and Z. As <a id="_idIndexMarker230"/>with most geometric transformations, rotating is a safe operation with a set limit for some image datasets and not <span class="No-Break">for others.</span></p>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_rotate()</strong> function is a fun demonstration of <strong class="bold">rotating</strong> an image <a id="_idIndexMarker231"/>with <strong class="bold">reflection padding</strong> mode. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B17990_03_04.jpg" alt="Figure 3.4 – Image rotating and reflection padding mode"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Image rotating and reflection padding mode</p>
			<p>Similar <a id="_idIndexMarker232"/>to rotating is <a id="_idIndexMarker233"/>shifting the images, which leads to the next technique, known <span class="No-Break">as translation.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Translation</h2>
			<p>The<a id="_idIndexMarker234"/> translation <a id="_idIndexMarker235"/>method shifts the image left or right along the X-axis or up or down along the Y-axis. It uses padding to backfill the negative space left by shifting the photo. Translation is beneficial for reducing center image biases, such as when people’s portraits are centered in the picture. The photo’s subject will dictate the safe parameters for how much to move <span class="No-Break">the images.</span></p>
			<p>The next geometric transformation is different from the ones we’ve talked about so far because noise injection reduces the <span class="No-Break">photo’s clarity.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Noise injection</h2>
			<p>Noise injection<a id="_idIndexMarker236"/> adds random black, white, or color<a id="_idIndexMarker237"/> pixels to a picture. It creates a grainy effect on the original image. Gaussian noise is a de facto standard for generating natural noises in a photo. It is based on the Gaussian distribution algorithm developed by mathematician Carl Friedrich Gauss in <span class="No-Break">the 1830s.</span></p>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_noise()</strong> function is a fun <a id="_idIndexMarker238"/>demonstration of <strong class="bold">noise injection</strong> using <strong class="bold">Gaussian</strong> mode. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B17990_03_05.jpg" alt="Figure 3.5 – Image noise injection using Gaussian mode"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Image noise injection using Gaussian mode</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: can you think of other geometric image transformations? Hint: use the Z-axis, not just the X-axis and <span class="No-Break">the Y-axis.</span></p>
			<p>In the <a id="_idIndexMarker239"/>second part of this chapter, Pluto and you will <a id="_idIndexMarker240"/>discover how to code geometric transformations, such as flipping, cropping, resizing, padding, rotation, and noise injection, but there are a few more image augmentations techniques to learn first. The next category is <span class="No-Break">photometric transformations.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Photometric transformations</h1>
			<p>Photometric transformations <a id="_idIndexMarker241"/>are also known as <span class="No-Break">lighting transformations.</span></p>
			<p>An image is represented in a three-dimensional array or a rank 3 tensor, and the first two dimensions are the picture’s width and height<a id="_idIndexMarker242"/> coordinates for each pixel position. The third dimension is a <strong class="bold">red, blue, and green</strong> (<strong class="bold">RGB</strong>) value ranging from zero to 255 or #0 to #FF in hexadecimal. The equivalent of RGB in printing is <strong class="bold">cyan, magenta, yellow, and key</strong> (<strong class="bold">CMYK</strong>). The other popular format is <strong class="bold">hue, saturation, and value</strong> (<strong class="bold">HSV</strong>). The salient point is that a photo is a matrix of an integer or float <span class="No-Break">when normalized.</span></p>
			<p>Visualizing<a id="_idIndexMarker243"/> the image as a matrix of numbers makes it easy to transform it. For example, in HSV format, changing the <strong class="bold">saturation</strong> value to zero in the matrix will convert an image from color <span class="No-Break">into grayscale.</span></p>
			<p>Dozens of filters alter the color space characteristics, from the basics to exotic ones. The basic methods are <strong class="bold">darkened</strong>, <strong class="bold">lightened</strong>, <strong class="bold">sharpened</strong>, <strong class="bold">blurring</strong>, <strong class="bold">contrast</strong>, and <strong class="bold">color casting</strong>. Aside from the basics, there are too many filter categories to list here, such as <strong class="bold">retro</strong>, <strong class="bold">groovy</strong>, <strong class="bold">steampunk</strong>, and many others. Furthermore, photo software, such as Adobe Photoshop, and online image editors create new image <span class="No-Break">filters frequently.</span></p>
			<p>In particular, this section will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Basic <span class="No-Break">and classic</span></li>
				<li>Advanced <span class="No-Break">and exotic</span></li>
			</ul>
			<p>Let’s begin with basic <span class="No-Break">and classic.</span></p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Basic and classic</h2>
			<p>Photometric transformations in image augmentation are a proven technique for increasing AI model accuracy. Most scholarly papers, such as <em class="italic">A comprehensive survey of recent trends in deep learning for digital images augmentation</em>, by Nour Eldeen Khalifa, Mohamed Loey, and Seyedali Mirjalili, published by <em class="italic">Artificial Intelligence Review</em> on September 4, 2021, use the classic filters exclusively because code execution is fast. There are <a id="_idIndexMarker244"/>many open source Python libraries for the classic filters, which Pluto and you will explore later in <span class="No-Break">this chapter.</span></p>
			<p>In particular, you will learn the following <span class="No-Break">classic techniques:</span></p>
			<ul>
				<li>Darken <span class="No-Break">and lighten</span></li>
				<li><span class="No-Break">Color saturation</span></li>
				<li><span class="No-Break">Hue shifting</span></li>
				<li><span class="No-Break">Color casting</span></li>
				<li><span class="No-Break">Contrast</span></li>
			</ul>
			<p>Let’s begin with the most common technique: the darken and <span class="No-Break">lighten filter.</span></p>
			<h3>Darken and lighten</h3>
			<p>Lightening<a id="_idIndexMarker245"/> an image means increasing the brightness level, while lowering the brightness value means darkening an image. In Python code, a photo is an integer or float values matrix, and once converted into HSV format, raising or lowering the <strong class="bold">value</strong> (<strong class="bold">V</strong>) in the HSV matrix increases or decreases the picture’s <span class="No-Break">brightness level.</span></p>
			<p>When it is time for you to write the functions for lightening or darkening the image for the Pluto object, you will use a Python image library to do the heavy lifting, but it is not hard to write the code from scratch. The safe range for the brightness value depends on the image subject and <span class="No-Break">label target.</span></p>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_brightness()</strong> function is a fun demonstration of <strong class="bold">darkening</strong> an image. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B17990_03_06.jpg" alt="Figure 3.6 – Image brightness, darken mode"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Image brightness, darken mode</p>
			<p>Similarly, color saturation is also easy to code <span class="No-Break">in Python.</span></p>
			<h3>Color saturation</h3>
			<p>Color saturation <a id="_idIndexMarker246"/>involves increasing or decreasing the intensity of the color in a photo. By reducing the saturation values close to zero, the image becomes a grayscale image. Inversely, the picture will show a more intense or vibrant color when raising the <span class="No-Break">saturation value.</span></p>
			<p>Similar to the brightness level coding, manipulating the picture’s <strong class="bold">saturation</strong> (<strong class="bold">S</strong>) value in the HSV matrix gives the desired effects. The safe range for color saturation depends on the image<a id="_idIndexMarker247"/> subject and <span class="No-Break">label target.</span></p>
			<p>So far, we’ve looked at the <em class="italic">S</em> and the <em class="italic">V</em> in <em class="italic">HSV</em>, but what does the <em class="italic">H</em> value do? It is for <span class="No-Break">hue shifting.</span></p>
			<h3>Hue shifting</h3>
			<p>Shifting<a id="_idIndexMarker248"/> the <strong class="bold">hue</strong> (<strong class="bold">H</strong>) value in the Python image matrix in HSV format alternates the photo’s color. Typically, a circle represents the hue values. Thus, the value starts at zero and ends at 360 degrees. Red is at the top of the rotation, beginning with zero, followed by yellow, green, cyan, blue, and magenta. Each color is separated by 60 degrees. Therefore, the last color, magenta, starts at 310 and ends at <span class="No-Break">360 degrees.</span></p>
			<p>Hue shifting is an excellent image editing filter, but for AI image augmentation, it is not helpful because it distorts the image beyond the intended label. For example, suppose you are developing an AI model to classify different species of chameleons. In that case, the hue-switching technique is sufficient for image augmentation. Still, if your project is to differentiate cats and fluffy furball toys, it might lead to false positives because you would get fluorescent <span class="No-Break">pink cats.</span></p>
			<p>The <strong class="source-inline">pluto.draw_image_teaser_hue()</strong> function is a fun demonstration of <strong class="bold">hue shifting</strong>. The<a id="_idIndexMarker249"/> output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B17990_03_07.jpg" alt="Figure 3.7 – Image hue shifting"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Image hue shifting</p>
			<p>Similar to hue shifting is <span class="No-Break">color casting.</span></p>
			<h3>Color casting</h3>
			<p>Color casting <a id="_idIndexMarker250"/>is also known <a id="_idIndexMarker251"/>as color tinting. It is<a id="_idIndexMarker252"/> when the white color is not balanced or is inaccurate. The tint colors are commonly <strong class="bold">red, green, or blue</strong> (<strong class="bold">RGB</strong>). In Python, tinting a photo is as easy as altering the RGB value in the image matrix. There is the same concern for the safe range as in the hue-shifting filter. In other words, color casting has limited use in AI <span class="No-Break">image augmentation.</span></p>
			<p>There is no formal definition of which filters are basic or classic and which are advanced and exotic. Hence, we have chosen to look at the contrast filter for our final example of classic <span class="No-Break">photometric transformations.</span></p>
			<h3>Contrast</h3>
			<p>Contrast <a id="_idIndexMarker253"/>is the difference in luminance or color that distinguishes objects in a picture from one another. For example, most photographers want a high contrast between a person in the foreground concerning the background. Usually, the foreground object is brighter and in sharper focus than the background. The safe range for the contrast value depends on the image subject and <span class="No-Break">label target.</span></p>
			<p>Pluto and you will explore the contrast filter and all other classic photometric transformations using Python code in the second half of this chapter. The following section will cover advanced and <span class="No-Break">exotic filters.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Advanced and exotic</h2>
			<p>The <a id="_idIndexMarker254"/>advanced or exotic techniques have<a id="_idIndexMarker255"/> no Python library for implementing these filters in data augmentation. Online photo editing websites and desktop software frequently create new exotic <span class="No-Break">filters monthly.</span></p>
			<p>If you review the filters section of <em class="italic">Adobe Photoshop</em> or many online photo editing websites, such as <a href="https://www.fotor.com">www.fotor.com</a>, you will find dozens or hundreds of filter options. Specialized filters for image subjects include people portraits, landscapes, cityscapes, still life, and many others. Filters are also categorized by styles, such as retro, vintage, steampunk, trendy, mellow, groovy, and <span class="No-Break">many others.</span></p>
			<p>The exotic filters are not featured in scholarly papers partly due to the lack of available Python libraries <a id="_idIndexMarker256"/>and the high CPU or GPU resource time to perform these operations during the training cycle. Nevertheless, in theory, exotic filters are excellent techniques for <span class="No-Break">image</span><span class="No-Break"><a id="_idIndexMarker257"/></span><span class="No-Break"> augmentation.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Let’s do a thought experiment. Can generative AI, like stable diffusion or DALL-E, create new images for augmentation? Generative AI can create hundreds or thousands of images from input text. For example, let’s say you’ve been tasked with developing an AI for identifying a unicorn, pegasus, or minotaur; is it more difficult to find images of those mythical creatures in print or real life? Generative AI can do this, but is it a practical technique? Hint: think about static versus dynamic augmentation disk space <span class="No-Break">and time.</span></p>
			<p>Photometric and geometric transformations manipulate photos, but random erasing adds new elements to <span class="No-Break">a picture.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>Random erasing</h1>
			<p>Random erasing <a id="_idIndexMarker258"/>selects a rectangle region in an image and replaces or overlays it with a gray, black, white, or Gaussian noise pixels rectangle. It is counterintuitive to why this technique increases the AI model’s <span class="No-Break">forecasting accuracy.</span></p>
			<p>The strength of any ML model, especially CNN, is in predicting or forecasting data that has not been seen in the training or validating stage. Thus, dropout, where randomly selected neurons are ignored during training, is a well-proven method to reduce overfitting and increase accuracy. Therefore, random erasing has the same effect as increasing the <span class="No-Break">dropout rate.</span></p>
			<p>A paper called <em class="italic">Random Erasing Data Augmentation</em>, which was published on November 16, 2017, by arXiv, shows how random erasing increases accuracy and reduces overfitting in a CNN-based model. The paper’s authors are Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang from the Cognitive Science Department, at Xiamen University, China, and the University of Technology <span class="No-Break">Sydney, Australia.</span></p>
			<p>Typically, the random erasing rectangle region, also known as the cutout, is filled with random pixels using <em class="italic">Gaussian randomization</em>. The safe range for random erasing depends on the image subject and <span class="No-Break">label target.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">There is one creative example of using random erasing in image augmentation that reduces biases. In a self-driving automobile system, one of the image classification models is to identify and classify street signs. The AI model was trained with clear and pristine street sign photos, so the AI model was biased against the real-world pictures of street signs in poor neighborhoods in the USA, where street signs are defaced with graffiti and abused. Randomly adding cutouts of graffiti, paint, dirt, and bullet holes increased the model’s accuracy and reduced overfitting and biases against poor neighborhood <span class="No-Break">street signs.</span></p>
			<p>Depending on the <a id="_idIndexMarker259"/>image dataset subject, random erasing, photometric, and geometric transformations can be mixed and matched. Let’s discuss this <span class="No-Break">in detail.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor072"/>Combining</h1>
			<p>The <a id="_idIndexMarker260"/>techniques or filters in geometric transformations can be readily combined with most image topics. For example, you can mix horizontal flip, cropping, resizing, padding, rotation, translation, and noise injection for many domains, such as people, landscapes, cityscapes, <span class="No-Break">and others.</span></p>
			<p>In addition, taking landscape as a topic, you can combine many filters in photometric transformations, such as darkening, lightening, color saturation, and contrast. Hue shifting and color casting may not apply to landscape photos. However, advanced photographic transformation filters, such as adding rain or snow to landscape images, <span class="No-Break">are acceptable.</span></p>
			<p>There’s more: you can add random erasing to landscape images. As a result, 1,000 landscape images may increase to 200,000 photos for training. That is the power of <span class="No-Break">image augmentation.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: should you augment the entire image dataset or only <span class="No-Break">a segment?</span></p>
			<p class="callout">Data augmentation can generate hundreds of thousands of new images for training, increasing AI prediction accuracy by decreasing the overfitting problem. But what if you also augmented the validation and testing dataset? Hint: think about real-world applications, DL generalization, and false negatives and <span class="No-Break">false positives.</span></p>
			<p>So far, we <a id="_idIndexMarker261"/>have discussed various image augmentation filters and techniques. The next step is for you and Pluto to write Python code to reinforce your understanding of <span class="No-Break">these concepts.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Reinforcing your learning through Python code</h1>
			<p>We will pursue the same approach as in <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. Start by loading the <strong class="source-inline">data_augmentation_with_python_chapter_3.ipynb</strong> file in Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this point onward, the code snippets will be from the Python Notebook, which contains all <span class="No-Break">the functions.</span></p>
			<p>This chapter’s coding lessons topics are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Pluto and the <span class="No-Break">Python Notebook</span></li>
				<li>Real-world <span class="No-Break">image dataset</span></li>
				<li>Image <span class="No-Break">augmentation library</span></li>
				<li><span class="No-Break">Geometric transformations</span></li>
				<li><span class="No-Break">Photometric transformations</span></li>
				<li><span class="No-Break">Random erasing</span></li>
				<li><span class="No-Break">Combining</span></li>
			</ul>
			<p>The next step is to download, set up, and verify that Pluto and the Python Notebook are <span class="No-Break">working adequately.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>Pluto and the Python Notebook</h2>
			<p>Before loading <a id="_idIndexMarker262"/>Pluto from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, we must retrieve him by cloning this book’s <strong class="bold">GitHub</strong> repository. Using<a id="_idIndexMarker263"/> the Python Notebook’s <strong class="source-inline">%run</strong> magic command, we can invoke Pluto. If you improved or hacked Pluto, load that file. You should review <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> if these steps are not familiar <span class="No-Break">to you.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The startup process for coding is the same for every chapter. Pluto only displays the essential code snippets in this book, and he relies on you to review the complete code in the <span class="No-Break">Python Notebook.</span></p>
			<p>Use the following code to<a id="_idIndexMarker264"/> clone the Python Notebook and <span class="No-Break">invoke Pluto:</span></p>
			<pre class="source-code">
<strong class="bold"># clone the book repo.</strong>
f = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'
!git clone {f}
<strong class="bold"># invoke Pluto</strong>
%run 'Data-Augmentation-with-Python/pluto/pluto_chapter_2.py'</pre>
			<p>The output will be similar to <span class="No-Break">the following:</span></p>
			<pre class="console">
---------------------------- : ---------------------------
            Hello from class : &lt;class '__main__.PacktDataAug'&gt; Class: PacktDataAug
                   Code name : Pluto
                   Author is : Duc Haba
---------------------------- : ---------------------------</pre>
			<p>Double-check that Pluto has loaded correctly by running the following code in the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># display system and libraries version</strong>
pluto.say_sys_info()</pre>
			<p>The output will be as follows or something similar, depending on <span class="No-Break">your system:</span></p>
			<pre class="console">
---------------------------- : ---------------------------
                 System time : 2022/09/18 06:07
                    Platform : linux
     Pluto Version (Chapter) : 2.0
       Python version (3.7+) : 3.7.13 (default, Apr 24 2022, 01:04:09) [GCC 7.5.0]
            PyTorch (1.11.0) : actual: 1.12.1+cu113
              Pandas (1.3.5) : actual: 1.3.5
                 PIL (9.0.0) : actual: 7.1.2
          Matplotlib (3.2.2) : actual: 3.2.2
                   CPU count : 2
                  CPU speed : NOT available
---------------------------- : ---------------------------</pre>
			<p>Pluto has verified that the<a id="_idIndexMarker265"/> Python Notebook is working correctly, so the next step is downloading real-world image datasets from the <span class="No-Break"><em class="italic">Kaggle</em></span><span class="No-Break"> website.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Real-world image datasets</h2>
			<p>In <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, Pluto learned how <a id="_idIndexMarker266"/>to download thousands of real-world datasets from the <em class="italic">Kaggle</em> website. For this chapter, he has selected six image datasets to illustrate different image augmentation techniques. Still, you can substitute or add new <em class="italic">Kaggle</em> image datasets by passing the new URLs to the code in the <span class="No-Break">Python Notebook.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Download two additional real-world datasets from the Kaggle website. Pluto likes to play fetch, so it is no problem for it to fetch new datasets. Hint: go to <a href="https://www.kaggle.com/datasets">https://www.kaggle.com/datasets</a> and search for <strong class="bold">image classification</strong>. Downloading additional real-world data will further reinforce your understanding of image <span class="No-Break">augmentation concepts.</span></p>
			<p>Pluto has chosen<a id="_idIndexMarker267"/> six image datasets based on the challenges each topic brings to bear on augmentation techniques. In other words, one concept may be acceptable for one subject but not for another. In particular, the six image datasets are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Covid-19 <span class="No-Break">image dataset</span></li>
				<li><span class="No-Break">Indian people</span></li>
				<li>Edible and <span class="No-Break">poisonous fungi</span></li>
				<li><span class="No-Break">Sea animals</span></li>
				<li><span class="No-Break">Vietnamese food</span></li>
				<li><span class="No-Break">Mall crowd</span></li>
			</ul>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The code for downloading these six real-world datasets from the <em class="italic">Kaggle</em> website looks repetitive. It is easy by design because Pluto worked hard to create reusable methods in <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. He wants it to be easy so that you can download any real-world dataset from the <span class="No-Break"><em class="italic">Kaggle</em></span><span class="No-Break"> website.</span></p>
			<p>Let’s start with the <span class="No-Break">Covid-19 data.</span></p>
			<h3>Covid-19 image dataset</h3>
			<p>Medical <a id="_idIndexMarker268"/>is a popular category for AI image predictive models. Therefore, Pluto selected the <em class="italic">Covid-19 Image Dataset</em>. He fetched the pictures and made the necessary pandas DataFrame using the methods shown in <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. Note that the complete code is in the <span class="No-Break">Python Notebook.</span></p>
			<p>The following commands fetch and load the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/pranavraikokte/covid19-image-dataset')
<strong class="bold"># import to Pandas data frame</strong>
f = 'kaggle/covid19-image-dataset/Covid19-dataset/train'
pluto.df_covid19 = pluto.make_dir_dataframe(f)</pre>
			<p>The first three records of the pandas DataFrame are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B17990_03_08.jpg" alt="Figure 3.8 – The first three rows of the pandas DataFrame"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – The first three rows of the pandas DataFrame</p>
			<p>On the <em class="italic">Kaggle</em> website, the <a id="_idIndexMarker269"/>data’s context is <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“<em class="italic">Helping Deep Learning and AI Enthusiasts like me to contribute to improving Covid-19 detection using just Chest X-rays. It contains around 137 cleaned images of Covid-19 and 317 containing Viral Pneumonia and Normal Chest X-Rays structured into the test and train directories.</em>”</p>
			<p>This citation is from the <em class="italic">University of Montreal</em>, and the collaborator listed is <strong class="bold">Pranav Raikote</strong> (owner), license: <strong class="bold">CC BY-SA </strong><span class="No-Break"><strong class="bold">4.0</strong></span><span class="No-Break">: </span><a href="https://choosealicense.com/licenses/cc-by-sa-4.0"><span class="No-Break">https://choosealicense.com/licenses/cc-by-sa-4.0</span></a><span class="No-Break">.</span></p>
			<p>Now that Pluto has downloaded the Covid-19 data, it will start working on the <span class="No-Break"><em class="italic">People</em></span><span class="No-Break"> dataset.</span></p>
			<h3>Indian People</h3>
			<p>The<a id="_idIndexMarker270"/> second typical category in image prediction or classification is people. Pluto has chosen the <em class="italic">Indian People</em> dataset. The following code snippet from the Python Notebook fetches and loads the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/sinhayush29/indian-people')
<strong class="bold"># import to Pandas DataFrame</strong>
f = 'kaggle/indian-people/Indian_Train_Set'
pluto.df_people = pluto.make_dir_dataframe(f)</pre>
			<p>On the <em class="italic">Kaggle</em> website, there is no description of the dataset. It’s not uncommon to get a dataset without an explanation or goals and be asked to augment it. The collaborator listed is <strong class="bold">Ayush Sinha</strong> (owner), license: <strong class="bold">None, Visible to </strong><span class="No-Break"><strong class="bold">the public</strong></span><span class="No-Break">.</span></p>
			<p>The typical usage for people data is to identify or classify age, sex, ethnicity, emotional sentiment, facial recognition, and <span class="No-Break">many more.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">There are controversial image classification AI systems, such as those for predicting people as criminals or not criminals, forecasting worthiness to society, identifying sexual orientation, and selecting immigrants or citizens. However, other creative uses include identifying a potential new whale – a super high casino spender from a casino installing cameras in the lobby and feeding them to <span class="No-Break">an AI.</span></p>
			<p>Now, let’s look at the <span class="No-Break">fungi data.</span></p>
			<h3>Edible and poisonous fungi</h3>
			<p>The third <a id="_idIndexMarker271"/>most often used topic for image classification is safety, such as distracted drivers, poisonous snakes, or cancerous tumors. Pluto found the real-word <em class="italic">Edible and Poisonous Fungi</em> dataset on the <em class="italic">Kaggle</em> website. The following code snippet from the Python Notebook fetches and loads the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/marcosvolpato/edible-and-poisonous-fungi')
<strong class="bold"># import into Pandas data frame</strong>
f = 'kaggle/edible-and-poisonous-fungi'
pluto.df_fungi = pluto.make_dir_dataframe(f)</pre>
			<p>On<a id="_idIndexMarker272"/> the <em class="italic">Kaggle</em> website, the description is <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“<em class="italic">We created this dataset as part of our school’s research project. As we didn’t find something similar when we started, we decided to publish it here so that future research with mushrooms and AI can benefit from it.</em>”</p>
			<p>The <a id="_idIndexMarker273"/>collaborator listed is  <strong class="bold">Marcos Volpato</strong> (owner), license: <strong class="bold">Open Data Commons Open Database License (</strong><span class="No-Break"><strong class="bold">ODbL)</strong></span><span class="No-Break">: </span><a href="https://opendatacommons.org/licenses/odbl/1-0/"><span class="No-Break">https://opendatacommons.org/licenses/odbl/1-0/</span></a><span class="No-Break">.</span></p>
			<p>Now, let’s look at the sea <span class="No-Break">animals data.</span></p>
			<h3>Sea animals</h3>
			<p>The <a id="_idIndexMarker274"/>fourth theme is nature. Pluto selected the <em class="italic">Sea Animals Image Dataset</em>. The following commands fetch and load the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/vencerlanz09/sea-animals-image-dataste')
<strong class="bold"># import to Pandas data frame</strong>
f = 'kaggle/sea-animals-image-dataste'
pluto.df_sea_animal = pluto.make_dir_dataframe(f)</pre>
			<p>The <em class="italic">Kaggle</em> website’s description for this dataset is <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“<em class="italic">Most life forms began their evolution in aquatic environments. The oceans provide about 90% of the world’s living space in terms of volume. Fish, which are only found in water, are the first known vertebrates. Some of these transformed into amphibians, which dwell on land and water for parts of the day.</em>”</p>
			<p>The collaborators listed are <strong class="bold">Vince Vence</strong> (owner), license: <strong class="bold">Other— Educational purposes and Free for Commercial Use (</strong><span class="No-Break"><strong class="bold">FFCU)</strong></span><span class="No-Break">: </span><a href="https://www.flickr.com/people/free_for_commercial_use/"><span class="No-Break">https://www.flickr.com/people/free_for_commercial_use/</span></a><span class="No-Break">.</span></p>
			<p>Next, we’ll look at<a id="_idIndexMarker275"/> <span class="No-Break">food data.</span></p>
			<h3>Vietnamese food</h3>
			<p>The fifth <a id="_idIndexMarker276"/>widespread subject for image classification is food. Pluto found the <em class="italic">30VNFoods – A Dataset for Vietnamese Food Images Recognition</em> dataset. The following commands fetch and load the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/quandang/vietnamese-foods')
<strong class="bold"># import to Pandas DataFrame</strong>
f = 'kaggle/vietnamese-foods/Images/Train'
pluto.df_food = pluto.make_dir_dataframe(f)</pre>
			<p>The <em class="italic">Kaggle</em> website’s description is <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“<em class="italic">This paper introduces a large dataset of 25,136 images of 30 popular Vietnamese foods. Several machine learning and deep learning image classification techniques have been applied to test the dataset, and the results were compared and reported.</em>”</p>
			<p>The collaborators listed are <strong class="bold">Quan Dang</strong> (owner), <strong class="bold">Anh Nguyen Duc Duy</strong> (editor), <strong class="bold">Hoang-Nhan Nguyen</strong> (viewer), <strong class="bold">Phuoc Pham Phu</strong> (viewer), and <strong class="bold">Tri Nguyen</strong> (viewer), license: <strong class="bold">CC BY-SA </strong><span class="No-Break"><strong class="bold">4.0</strong></span><span class="No-Break">: </span><a href="https://choosealicense.com/licenses/cc-by-sa-4.0"><span class="No-Break">https://choosealicense.com/licenses/cc-by-sa-4.0</span></a><span class="No-Break">.</span></p>
			<p>Now, let’s move on to mall <span class="No-Break">crowd data.</span></p>
			<h3>Mall crowd</h3>
			<p>Pluto chose the<a id="_idIndexMarker277"/> sixth and last dataset for the creative use of AI image classification – the <em class="italic">Mall - Crowd Estimation</em> dataset. The following code snippet from the Python Notebook fetches and loads the data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># fetch image data</strong>
pluto.fetch_kaggle_dataset('https://www.kaggle.com/datasets/ferasoughali/mall-crowd-estimation')
<strong class="bold"># import to Pandas DataFrame</strong>
f = 'kaggle/mall-crowd-estimation/mall_dataset/frames'
pluto.df_crowd = pluto.make_dir_dataframe(f)</pre>
			<p>The <em class="italic">Kaggle</em> website’s description is <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“The mall dataset was collected from a publicly accessible webcam for crowd counting and profiling research.”</p>
			<p>The<a id="_idIndexMarker278"/> collaborator listed is <strong class="bold">Feras</strong> (owner), license: <strong class="bold">None, Visible to </strong><span class="No-Break"><strong class="bold">the public</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Refactor the code provided and write one function that downloads all six datasets. Hint: put the six <em class="italic">Kaggle</em> data URLs into an array. Pluto does not write the uber-big method because he focuses on making the augmentation techniques easier to understand rather than writing compact code that might obfuscate <span class="No-Break">the meaning.</span></p>
			<p>After downloading all six datasets, Pluto must draw an <span class="No-Break">image batch.</span></p>
			<h3>Drawing an image batch</h3>
			<p>Let’s look <a id="_idIndexMarker279"/>at the pictures in the six datasets. Pluto will take samples from the pandas DataFrame and use the <strong class="source-inline">draw_batch()</strong> function defined in <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
			<p>The output for two Covid-19, two people, two fungi, two sea animals, one food, and one mall crowd picture are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B17990_03_09.jpg" alt="Figure 3.9 – Six image datasets"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Six image datasets</p>
			<p>Pluto has <a id="_idIndexMarker280"/>downloaded plenty of real-world pictures, so the next step is selecting an image <span class="No-Break">augmentation library.</span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Image augmentation library</h2>
			<p>There are many <a id="_idIndexMarker281"/>open source Python image augmentation and processing libraries. Most libraries have filters for geometric and photometric transformations. In addition, a few libraries have specialized functions for particular <span class="No-Break">image topics.</span></p>
			<p>Pluto will cover only some of these libraries. The most popular libraries are Albumentations, Fast.ai, <strong class="bold">Pillow</strong> (<strong class="bold">PIL</strong>), OpenCV, scikit-learn, Mahotas, <span class="No-Break">and pgmagick:</span></p>
			<ul>
				<li><strong class="bold">Albumentations</strong> is a fast and<a id="_idIndexMarker282"/> highly c<a id="_idIndexMarker283"/>ustomizable image augmentation Python library. It has become the de facto standard for research areas related to computer vision and DL. Albumentations efficiently implements over 70 varieties of image transform operations optimized for performance. Albumentations’ substantial benefit is broad integration with many DL frameworks. It was <a id="_idIndexMarker284"/>introduced in 2019. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/albumentations-team/albumentations"><span class="No-Break">https://github.com/albumentations-team/albumentations</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Fast.ai</strong> is a <a id="_idIndexMarker285"/>best-of-class for DL and AI library <a id="_idIndexMarker286"/>and framework. It was founded in 2016 by Jeremy Howard and Rachel Thomas to democratize DL. Fast.ai has extensive built-in functions for image augmentation. Furthermore, its image augmentation operations use GPU, so it is possible to perform dynamic image augmentation during the training cycle. In other words, because of the GPU, it is the best performance image augmentation library in the market. It <a id="_idIndexMarker287"/>can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/fastai"><span class="No-Break">https://github.com/fastai</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Pillow</strong> is a <a id="_idIndexMarker288"/>friendly modern fork of the <strong class="bold">Python Imaging Library</strong> (<strong class="bold">PIL</strong>) repository. PIL<a id="_idIndexMarker289"/> is a popular <a id="_idIndexMarker290"/>open source library for image processing and augmentation because it was first released in 1995. Many open source Python image processing, displaying, and augmenting libraries are built on top of PIL. It can be found on <a id="_idIndexMarker291"/>GitHub <span class="No-Break">at </span><a href="https://github.com/python-pillow/Pillow"><span class="No-Break">https://github.com/python-pillow/Pillow</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">AugLy</strong> is <a id="_idIndexMarker292"/>an <a id="_idIndexMarker293"/>open source Python project by Meta (Facebook) for data augmentation. The library provides over 100 audio, video, image, and text <a id="_idIndexMarker294"/>data augmentation methods. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/facebookresearch/AugLy"><span class="No-Break">https://github.com/facebookresearch/AugLy</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">OpenCV</strong> was <a id="_idIndexMarker295"/>developed by Intel in 2000 as an<a id="_idIndexMarker296"/> open source library. ML primarily uses OpenCV in computer vision tasks such as object classification and detection, face recognition, and image segmentation. In addition, OpenCV contains essential methods for ML. It can be found on <a id="_idIndexMarker297"/>GitHub <span class="No-Break">at </span><a href="https://github.com/opencv/opencv"><span class="No-Break">https://github.com/opencv/opencv</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">scikit-learn</strong> was one<a id="_idIndexMarker298"/> of the early open <a id="_idIndexMarker299"/>source libraries in 2009 for image augmentation. Part of scikit-learn is written in Cython, a programming language that is a superset of Python. One of its crucial benefits is high-performance speed, where a NumPy array is used as the<a id="_idIndexMarker300"/> image’s structure. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/scikit-image/scikit-image"><span class="No-Break">https://github.com/scikit-image/scikit-image</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Mahotas</strong> is an<a id="_idIndexMarker301"/> image processing and augmentation<a id="_idIndexMarker302"/> library specialized in bioimage informatics. Mahotas uses NumPy arrays and is written C++ with a Python interface. It was released in 2016. It can be found on<a id="_idIndexMarker303"/> GitHub <span class="No-Break">at </span><a href="https://github.com/luispedro/mahotas"><span class="No-Break">https://github.com/luispedro/mahotas</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">pgmagick</strong>: pgmagick <a id="_idIndexMarker304"/>is a GraphicsMagick <a id="_idIndexMarker305"/>binding for Python. GraphicsMagick is best known for supporting large images in a gigapixel-size range. It was initially derived from<a id="_idIndexMarker306"/> ImageMagick in 2002. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/hhatto/pgmagick"><span class="No-Break">https://github.com/hhatto/pgmagick</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>No library is better than another, and you can choose to use multiple libraries in a project. However, Pluto recommends picking two or three libraries and becoming proficient, maybe even an expert, <span class="No-Break">in them.</span></p>
			<p>Pluto will hide the library or libraries and create a wrapper function, such as <strong class="source-inline">draw_image_flip()</strong>, that uses other libraries to perform the transformation. The other reason for writing wrapper functions is to switch out the libraries and minimize the code changes. Pluto has chosen the <strong class="bold">Albumentations</strong>, <strong class="bold">Fast.ai</strong>, and <strong class="bold">PIL</strong> libraries for this chapter as the <span class="No-Break">under-the-hood engine.</span></p>
			<p>You have two options: creating image augmentation dynamically per batch or statically. When doing this statically, also known as pre-processing, you create and save the augmented pictures in your local or <span class="No-Break">cloud drive.</span></p>
			<p>For this chapter, Pluto has chosen to augment the image dynamically because, depending on the combinations of filters, you can generate over a million acceptable altered pictures. The only difference between the two methods is that the pre-processing method saves the augmented photos in local or cloud drives while the dynamic method <span class="No-Break">does not.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: should you select an augmentation library with more augmented methods over a library that runs on GPU? Hint: think about the goal of your project and its disk and <span class="No-Break">time resources.</span></p>
			<p>Let’s begin writing code for the geometric <span class="No-Break">transformation filters.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>Geometric transformation filters</h2>
			<p>Pluto can<a id="_idIndexMarker307"/> write Python code for many geometric transformation filters, and he will select two or three image datasets to illustrate each concept. In addition, by using multiple image subjects, he can discover the safe level. The range for the safe level is subjective, and you may need to consult a domain subject expert to know how far to distort the photo. When convenient, Pluto will write the same method using <span class="No-Break">different libraries.</span></p>
			<p>Let’s get started with <span class="No-Break">flipping images.</span></p>
			<h3>Flipping</h3>
			<p>Pluto will<a id="_idIndexMarker308"/> begin with the simplest filter: the horizontal flip. It mirrors the image, or in other words, it flips the photo along the Y-axis. The wrapper function is called <strong class="source-inline">draw_image_flip()</strong>. All image augmentation methods are prefixed with <strong class="source-inline">draw_image_</strong> as this makes it easy for Pluto to remember them. In addition, he can use the Python Notebook auto-complete typing feature. By typing <strong class="source-inline">pluto.draw_im</strong>, a popup menu containing all the filter functions will <span class="No-Break">be displayed.</span></p>
			<p>In the <strong class="source-inline">draw_image_flip_pil()</strong> function, when using the PIL library, the relevant code line is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use PIL to flip horizontal</strong>
mirror_img = PIL.ImageOps.mirror(img)</pre>
			<p>Thus, Pluto selects an image from the People dataset and flips it using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
<strong class="bold"># Select an image from Pandas and show the original and flip</strong>
pluto.draw_image_flip_pil(pluto.df_people.fname[100])</pre>
			<p>The result is as follows, with the original image at the top and the flip image at <span class="No-Break">the bottom:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B17990_03_10.jpg" alt="Figure 3.10 – Horizontal flip using the PIL library"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Horizontal flip using the PIL library</p>
			<p>Rather than<a id="_idIndexMarker309"/> viewing one image at a time, it is more advantageous to examine the entire dataset one batch at a time. This is because a filter may be applicable, or the <strong class="bold">safe</strong> range is acceptable for one image but not for another in the same dataset. The <strong class="bold">F</strong><strong class="bold">ast.ai</strong> library has the data-batch class that supports many ML functions, including accepting a transformation method and displaying a random collection of pictures, also known as displaying <span class="No-Break">a batch.</span></p>
			<p>Pluto will write two new methods: <strong class="source-inline">_make_data_loader()</strong>, which is a helper function for creating the <strong class="bold">Fast.ai</strong> data-loader object, and the <strong class="source-inline">draw_image_flip()</strong> function, which<a id="_idIndexMarker310"/> encodes the transformation for horizontal flip and displays the image batch using the data-loader <span class="No-Break"><strong class="source-inline">show_batch()</strong></span><span class="No-Break"> method.</span></p>
			<p><strong class="source-inline">show_batch()</strong> will select a random set of pictures to display, where <strong class="source-inline">max_n</strong> sets the number of images in a bunch. The Fast.ai transformation, by default, performs the modification at 75% probability. In other words, three out of four images in the dataset will <span class="No-Break">be transformed.</span></p>
			<p>The horizontal flip filter has no safe level, regardless of whether it applies to the image set. Pluto will use the <strong class="source-inline">draw_image_flip()</strong> method with the <strong class="bold">Fast.ai</strong> transformation. The coding for all wrapper functions is very similar. Only the augmentation function, the <strong class="source-inline">aug</strong> value, is different. The entirety of the flip wrapper code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use fast.ai to flip image in wrapper function</strong>
def draw_image_flip(self,df,bsize=15):
  aug = [fastai.vision.augment.Flip(p=0.75)]
  item_fms = fastai.vision.augment.Resize(480)
  dsl_org = self._make_data_loader(df, aug,item_fms)
  dsl_org.show_batch(max_n=bsize)
  return dsl_org</pre>
			<p>The definition of the <strong class="source-inline">aug</strong> variable differs from one wrapper function to another. Pluto needs to run a function on the Python Notebook for the People dataset with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
<strong class="bold"># Show random flip-image batch 15 in Pandas, wraper function</strong>
pluto.dls_people = pluto.draw_image_flip(pluto.df_people)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B17990_03_11.jpg" alt=" Figure 3.11 – Horizontal flip on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 3.11 – Horizontal flip on the People dataset</p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The complete fully functional object-oriented code can be found in the Python Notebook. You can hack it to show flip, rotate, tilt, and dozens of other <span class="No-Break">augmentation techniques.</span></p>
			<p>To ensure the<a id="_idIndexMarker311"/> horizontal flip is acceptable, you can repeatedly run the <strong class="source-inline">draw_image_flip()</strong> function in the Python Notebook to see a collection of varying image batches. Horizontal flip is a safe filter for fungi, sea animals, food, and mall crowd pictures. Common sense dictates that you wouldn’t expect otherwise. Here is the command for the <span class="No-Break">Fungi dataset:</span></p>
			<pre class="source-code">
<strong class="bold"># use flip wrapper function on Fungi data</strong>
pluto.dls_fungi = pluto.draw_image_flip(pluto.df_fungi)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B17990_03_12.jpg" alt="Figure 3.12 – Horizontal flip on the Fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Horizontal flip on the Fungi dataset</p>
			<p>For medical <a id="_idIndexMarker312"/>images, such as the Covid-19 photos, you need a domain expert to confirm that flipping horizontally does not change the image’s integrity. It does not make any difference to the layman, but it can be deceptively wrong and might create a <strong class="bold">false-positive</strong> or <strong class="bold">false-negative</strong> prediction. Here is the command <span class="No-Break">for it:</span></p>
			<pre class="source-code">
<strong class="bold"># use flip wrapper function on covid data</strong>
pluto.dls_covid19 = pluto.draw_image_flip(pluto.df_covid19)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B17990_03_13.jpg" alt="Figure 3.13 – Horizontal flip on the Covid-19 dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Horizontal flip on the Covid-19 dataset</p>
			<p>Notice <a id="_idIndexMarker313"/>that the Fast.ai transformation cropped the images center square. Therefore, in some pictures, content is lost – for example, a picture of a woman with most of her face missing. This is because Fast.ai is for ML, so the images need to be square. The default behavior is a center <span class="No-Break">square crop.</span></p>
			<p>Before Pluto can start cropping and padding, he must complete the flipping filter by combining horizontal flipping with vertical flipping. The people, Covid-19, fungi, and mall crowd pictures cannot be flipped vertically, but the sea animals and food <span class="No-Break">pictures can.</span></p>
			<p>For this, Pluto needs to create the <strong class="source-inline">draw_image_flip_both()</strong> method, with the transformation set to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
<strong class="bold"># using fast.ai for fliping</strong>
aug = fastai.vision.augment.Dihedral(p=0.8,pad_mode=pad_mode)</pre>
			<p>Now, Pluto must run the function on the People dataset with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
<strong class="bold"># use wrapper function on both flip on people images</strong>
pluto.dls_people = pluto.draw_image_flip_both(
  pluto.df_people)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B17990_03_14.jpg" alt="Figure 3.14 – Unsafe horizontal and vertical flips on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Unsafe horizontal and vertical flips on the People dataset</p>
			<p>He can apply<a id="_idIndexMarker314"/> the same function to the food pictures, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use flip wrapper function on food photos</strong>
pluto.dls_food = pluto.draw_image_flip_both(pluto.df_food)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17990_03_15.jpg" alt="Figure 3.15 – Safe horizontal and vertical flips on the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Safe horizontal and vertical flips on the food dataset</p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto loves to play the same game over and over again. You know, because he is a dog. :-) Thus, you can ask Pluto to run any wrapper functions repeatedly on the Python Notebook to see a different set of image batches from the data stored in pandas. Each real-world image dataset contains thousands of photos, and each batch displays 15 images; therefore, you must run the wrapper functions repeatedly to have a good mental picture of <span class="No-Break">the dataset.</span></p>
			<p>The next filters <a id="_idIndexMarker315"/>we will look at are for cropping <span class="No-Break">and padding.</span></p>
			<h3>Cropping and padding</h3>
			<p>Reusing the <a id="_idIndexMarker316"/>same process as when writing the flipping filter, Pluto can write the <strong class="source-inline">draw_image_crop()</strong> method. The one new code line uses a different <span class="No-Break">item transformation:</span></p>
			<pre class="source-code">
<strong class="bold"># use fast.ai to crop image in wrapper function</strong>
item_tfms=fastai.vision.augment.CropPad(480,
   pad_mode="zeros")</pre>
			<p>The padding<a id="_idIndexMarker317"/> mode can be <strong class="bold">zeros</strong>, which means the padding color is black, <strong class="bold">border</strong>, which means the padding repeats the border pixel, or <strong class="bold">reflection</strong>, which means padding is mirrored from <span class="No-Break">the picture.</span></p>
			<p>After <a id="_idIndexMarker318"/>much trial and error on the Python Notebook, Pluto found the safe range for cropping and padding for each of the six datasets. Before moving on, Pluto encourages you to use the Python Notebook to find the best <span class="No-Break">safe parameter.</span></p>
			<p>Pluto found that the safe setting for the people data is using a cropped image size of 640 and pad mode on <span class="No-Break">the border:</span></p>
			<pre class="source-code">
<strong class="bold"># use wrapper function to crop and pad people photo</strong>
pluto.dls_people = pluto.draw_image_crop(pluto.df_people,
  pad_mode="border",
  isize=640)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17990_03_16.jpg" alt="Figure 3.16 – Horizontal and vertical flip on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Horizontal and vertical flip on the People dataset</p>
			<p>In terms of the next dataset, Pluto found that the safe setting for the fungi images is a cropped<a id="_idIndexMarker319"/> image size of 240 and a pad mode <span class="No-Break">of zeros:</span></p>
			<pre class="source-code">
<strong class="bold"># use wrapper function to crop and pad Fungi image</strong>
pluto.dls_fungi = pluto.draw_image_crop(pluto.df_fungi,
  pad_mode="zeros",
  isize=240)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17990_03_17.jpg" alt="Figure 3.17 – Horizontal and vertical flip on the ﻿fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Horizontal and vertical flip on the fungi dataset</p>
			<p>For the food pictures, Pluto discovered that the safe parameters are a cropped image size of 640 and a pad mode <span class="No-Break">of reflection:</span></p>
			<pre class="source-code">
<strong class="bold"># use wrapper function to crop and pad food image</strong>
pluto.dls_food = pluto.draw_image_crop(pluto.df_food,
  pad_mode="reflection",
  isize=640)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17990_03_18.jpg" alt="Figure 3.18 – Horizontal and vertical flip on the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – Horizontal and vertical flip on the food dataset</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Find the safe cropping parameters for all six image datasets. You will get bonus points for applying these new functions to the set of images from your project or downloading them from the <span class="No-Break">Kaggle website.</span></p>
			<p>For the other image datasets, the results are in the Python Notebook. The safe parameter is 340 pixels with reflection padding for the sea animals pictures and 512 pixels with border padding for the mall crowd pictures. A cropping filter is not an option for the <span class="No-Break">Covid-19 pictures.</span></p>
			<p>Next, Pluto will rotate images, which is similar <span class="No-Break">to flipping.</span></p>
			<h3>Rotating</h3>
			<p>Rotation<a id="_idIndexMarker320"/> specifies how many degrees to turn the image clockwise or counter-clockwise. Since Pluto sets the max rotation value, the actual rotation is a random number between the minimum and the maximum value. The minimum default value is zero. Therefore, a higher maximum value will generate more augmentation images because every time the system fetches a new data batch, a different rotation value is chosen. In addition, randomness is the reason for selecting the dynamic augmentation option over saving the images to a local disk drive, as in the <span class="No-Break">static option.</span></p>
			<p>For this, Pluto has written the <strong class="source-inline">draw_image_rotate()</strong> method using the <strong class="source-inline">max_rotate = max_rotate</strong> transformation parameter, where the second <strong class="source-inline">max_rotate</strong> is the passed-in value. The key code line in the wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use fast.ai for rotating</strong>
aug = [fastai.vision.augment.Rotate(max_rotate,
  p=0.75,
  pad_mode=pad_mode)]</pre>
			<p>Once again, Pluto<a id="_idIndexMarker321"/> has arrived at the following safe parameter for rotating after much trial and error on the Python Notebook, but don’t take Pluto’s word for it. Pluto challenges you to find better safe parameters by experimenting with the <span class="No-Break">Python Notebook.</span></p>
			<p>For the sea animals data, Pluto has arrived at a safe parameter of <strong class="source-inline">180.0</strong> for the maximum rotation and reflection for padding. The command in the Python Notebook is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use wrapper function to rotate sea animal photo</strong>
pluto.dls_sea_animal = pluto.draw_image_rotate(
  pluto.df_sea_animal,
  max_rotate=180.0,
  pad_mode='reflection')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17990_03_19.jpg" alt="Figure 3.19 – Rotation on the sea animals dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Rotation on the sea animals dataset</p>
			<p>For the <a id="_idIndexMarker322"/>people pictures, Pluto has arrived at a safe parameter of <strong class="source-inline">25.0</strong> for the maximum rotation and border for padding. The command in the Python Notebook is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># user wrapper function to rotate people photo</strong>
pluto.dls_people = pluto.draw_image_rotate(pluto.df_people,
  max_rotate=25.0,
  pad_mode='border')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17990_03_20.jpg" alt="Figure 3.20 – Rotation on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 – Rotation on the People dataset</p>
			<p>For the <a id="_idIndexMarker323"/>other image datasets, the results are in the Notebook. The safe parameters are <strong class="source-inline">16.0</strong> maximum rotation with border padding for the mall crowd photos, <strong class="source-inline">45.0</strong> maximum rotation with border padding for the fungi pictures, <strong class="source-inline">90.0</strong> maximum rotation with reflection padding for the food images, and <strong class="source-inline">12.0</strong> maximum rotation with border zeros for the Covid-19 data. I encourage you to extend beyond Pluto’s safe range on the Notebook and see what happens to the pictures <span class="No-Break">for yourself.</span></p>
			<p>Now, let’s continue with the shifting image theme. The next filter we’ll look at is the <span class="No-Break">translation filter.</span></p>
			<h3>Translation</h3>
			<p>The <a id="_idIndexMarker324"/>translation filter shifts the image to the left, right, up, or down. It is not one of the commonly used filters. Pluto uses the <strong class="source-inline">ImageChops.offset()</strong> method in the <strong class="bold">PIL</strong> library to write the <strong class="source-inline">draw_image_shift()</strong> function. A negative horizontal shift value moves the image to the left, a positive value moves the image to the right, and the vertical shift parameter moves the image up or down. The relevant code line in the wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># using PIL for shifting image</strong>
shift_img = PIL.ImageChops.offset(img,x_axis,y_axis)</pre>
			<p>To test the <a id="_idIndexMarker325"/>function, Pluto selects a picture and shifts it left by 150 pixels and up by 50 pixels. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># select an image in Pandas</strong>
f = pluto.df_people.fname[30]
<strong class="bold"># user wrapper function to shift the image</strong>
pluto.draw_image_shift_pil(f, -150, -50)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17990_03_21.jpg" alt="Figure 3.21 – Translation using the PIL library; the top image is the original"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 – Translation using the PIL library; the top image is the original</p>
			<p>The <a id="_idIndexMarker326"/>translation filter is seldom used because it is easy to find the safe level for one picture but not for the <span class="No-Break">entire dataset.</span></p>
			<p>So far, Pluto has shown you the <strong class="bold">flipping</strong>, <strong class="bold">cropping</strong>, <strong class="bold">padding</strong>, <strong class="bold">resizing</strong>, <strong class="bold">rotating</strong>, and <strong class="bold">translation</strong> filters. However, there are many more geometric transformation filters, such as for <strong class="bold">warping</strong>, <strong class="bold">zooming</strong>, <strong class="bold">tilting</strong>, and <strong class="bold">scaling</strong>. Unfortunately, there are too many to cover, but the coding process is <span class="No-Break">the same.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Implement two more geometric transformation techniques, such as warping and tilting. Hint: copy and paste from Pluto’s wrapper functions and change the <strong class="source-inline">aug</strong> and <span class="No-Break"><strong class="source-inline">item_tfms</strong></span><span class="No-Break"> variables.</span></p>
			<p>Moving from <a id="_idIndexMarker327"/>geometric to photometric transformations follows the same coding process. First, Pluto writes the wrapper functions using the Albumentations library, then uses the real-world image dataset to <span class="No-Break">test them.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>Photographic transformations</h2>
			<p>Pluto chose the<a id="_idIndexMarker328"/> Albumentations library to power the photometric transformations. The primary reasons are that the <strong class="bold">Albumentations</strong> library has over 70 filters, and you can integrate it into the <strong class="bold">Fast.ai</strong> framework. Fast.ai has most of the basic photometric filters, such as hue shifting, contrast, and lighting, but only Albumentations has more exotic filters, such as those for adding rain, motion blur, and FancyPCA. Be careful when using fancy filters. Even though they are easy to implement, you should research AI scholarly published papers to see if the filter is beneficial for achieving a higher <span class="No-Break">accuracy rate.</span></p>
			<p>As with the geometric transformations coding process, Pluto creates the base method and writes the wrapper function for each photometric transformation. The <strong class="source-inline">_draw_image_album()</strong> method is used to select a sample set of images from the data, convert it into a <strong class="source-inline">numpy</strong> array, do the transformation, and display them in batch mode. The pertinent code for the <strong class="source-inline">_draw_image_album()</strong> function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
# <strong class="bold">select random images</strong>
samp = df.sample(int(ncol * nrow))
# <strong class="bold">convert to an array</strong>
img_numpy = numpy.array(PIL.Image.open(samp.fname[i]))
# <strong class="bold">perform the transformation using albumentations</strong>
img = aug_album(image=img_numpy)['image']
# <strong class="bold">display the image in batch modde</strong>
ax.imshow(img)</pre>
			<p>The wrapper function code is straightforward. For example, the code for the brightness filter is <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker329"/></span><span class="No-Break">follows:</span></p>
			<pre class="source-code">
def draw_image_brightness(self,df,
  brightness=0.2,
  bsize=5):
  aug_album = albumentations.ColorJitter(
    brightness = brightness,
    contrast=0.0,
    saturation=0.0,
    hue=0.0,
    always_apply=True,
    p=1.0)
  self._draw_image_album(df,aug_album,bsize)
  Return</pre>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">For any of the Albumentations functions, you can append a question mark (<strong class="source-inline">?</strong>) and run the code cell to see the documentation in the Python Notebook; for example, <strong class="source-inline">albumentations.ColorJitter?</strong>. Append two question marks (<strong class="source-inline">??</strong>) to see the function’s Python source code; for example, <strong class="source-inline">albumentations.ColorJitter??</strong>. A bonus fun fact is that Albumentations types are followed by a dot – for example, <strong class="source-inline">albumentations.</strong> – in the Python Notebook and wait for a second. A list of all the available functions appears in a drop-down list, where you can choose one. In other words, the Python Notebook has <span class="No-Break">auto-complete typing.</span></p>
			<p>The definition of the <strong class="source-inline">aug_albm</strong> variable differs from one wrapper function to another. Let’s test out the <span class="No-Break">brightness filter.</span></p>
			<h3>Brightness</h3>
			<p>It isn’t easy to view most of the photometric transformations in grayscale because you are reading them in a book. That is more reason for joining Pluto with coding in the Notebook<a id="_idIndexMarker330"/> as you can see color. Rather than showing you all optimal safe parameters for each filter, Pluto will show you the <em class="italic">unsafe</em> range for one dataset and a safe parameter for the other dataset. The key code line for brightness in the wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the Albumentations library function for brightness</strong>
aug_album = albumentations.ColorJitter(brightness=brightness,
  contrast=0.0,
  saturation=0.0,
  hue=0.0,
  always_apply=True,
  p=1.0)</pre>
			<p>You can see the mistake in this book by exaggerating the brightness – for example, if it’s too bright or too dark. Once again, you should look at the Notebook to see the brightness effects in color. For the people photos, the <em class="italic">unsafe</em> value is a brightness equal to <strong class="source-inline">1.7</strong>. Pluto runs the following command on the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># use the brightness wrapper function for people photo</strong>
pluto.draw_image_brightness(pluto.df_people, brightness=1.7)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17990_03_22.jpg" alt="Figure 3.22 – Unsafe brightness level for the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 – Unsafe brightness level for the People dataset</p>
			<p>The People dataset does not have any objective. Therefore, it is challenging to find safe parameters. If the goal is as simple as classifying people’s ages, the brightness level can be relatively high, but without knowing the intended use of the dataset, you don’t know how much to distort <span class="No-Break">the pictures.</span></p>
			<p>Pluto found<a id="_idIndexMarker331"/> that the safe brightness value for the food dataset is <strong class="source-inline">0.3</strong>, but it may not be easy to see the effects in this book. Here is the command that he used on the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># use the brightness wrapper function for food image</strong>
pluto.draw_image_brightness(pluto.df_food, brightness=0.3)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17990_03_23.jpg" alt="Figure 3.23 – Safe brightness level for the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Safe brightness level for the food dataset</p>
			<p>The brightness level for the other four datasets is similar. Pluto has left it up to you to experiment and find the safe level in the Python Notebook. The Covid-19 images are in grayscale, and the intent is to predict Covid-19 patients from their chest X-ray photos. A decrease or increase in the brightness level may result in a false-positive or false-negative prediction. You should consult with the domain experts to confirm the safe parameters for <span class="No-Break">Covid-19 images.</span></p>
			<p>The grayscale filter wasn’t discussed in the first half of this chapter, but it is similar to the <span class="No-Break">brightness filter.</span></p>
			<h3>Grayscale</h3>
			<p>A few<a id="_idIndexMarker332"/> scholarly papers describe the benefit of grayscale, such as <em class="italic">Data Augmentation Methods Applying Grayscale Images for Convolutional Neural Networks in Machine Vision</em>, by Jinyeong Wang and Sanghwan Lee in 2021 from the Department of <em class="italic">Mechanical Convergence Engineering, Hanyang University, Seoul 04763, Korea</em>. The paper explains the effective data augmentation method for grayscale images in CNN-based machine vision with <span class="No-Break">mono cameras.</span></p>
			<p>In the <strong class="source-inline">draw_image_grayscale()</strong> method, Pluto uses the Albumentations library function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use albumentations for grayscale</strong>
aug_album = albumentations.ToGray(p=1.0)</pre>
			<p>The fungi <a id="_idIndexMarker333"/>dataset aims to classify whether a mushroom is edible or poisonous, and the mushroom’s color significantly affects the classification. Therefore, converting into grayscale is not advisable. Nevertheless, Pluto illustrates the grayscale filter with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># use the grayscale wrapper function for fungi image</strong>
pluto.draw_image_grayscale(pluto.df_fungi)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17990_03_24.jpg" alt="Figure 3.24 – Unsafe use of grayscale on the fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24 – Unsafe use of grayscale on the fungi dataset</p>
			<p>The mall crowd dataset’s goal is to estimate the crowd size in a shopping mall. Thus, converting the photos into grayscale should not affect the prediction. Pluto runs the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># use the grayscale wrapper function for crowd photo</strong>
pluto.draw_image_grayscale(pluto.df_crowd)</pre>
			<p>The results are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17990_03_25.jpg" alt="Figure 3.25 – Safe use of grayscale on the mall crowd dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Safe use of grayscale on the mall crowd dataset</p>
			<p>Pluto left<a id="_idIndexMarker334"/> the other four datasets for you to experiment with to determine whether it is safe to use the grayscale filter. After you use the Python Notebook to explore these datasets, come back here, where we will examine the contrast, saturation, and <span class="No-Break">hue-shifting filters.</span></p>
			<h3>Contrast, saturation, and hue shifting</h3>
			<p>The <a id="_idIndexMarker335"/>contrast, saturation, and hue-shifting filters are beneficial. They are proven to aid in training AI models to achieve a higher accuracy rate, such as in the <em class="italic">Improving Deep Learning using Generic Data Augmentation</em> scholarly paper by Luke Taylor and Geoff Nitschke, published in 2017 by the <span class="No-Break"><em class="italic">Arxiv</em></span><span class="No-Break"> website.</span></p>
			<p>The code for the contrast, saturation, and<a id="_idIndexMarker336"/> hue-shifting wrapper functions is straightforward with the Albumentations<a id="_idIndexMarker337"/> library. Let’s take <span class="No-Break">a look:</span></p>
			<pre class="source-code">
<strong class="bold"># for contrast</strong>
aug_album = albumentations.ColorJitter(brightness=0.0,
  contrast=contrast, saturation=0.0,
  hue=0.0, always_apply=True, p=1.0)
<strong class="bold"># for saturation</strong>
aug_album = albumentations.ColorJitter(brightness=0.0,
  contrast=0.0, saturation=saturation,
  hue=0.0, always_apply=True, p=1.0)
<strong class="bold"># for hue shifting</strong>
aug_album = albumentations.ColorJitter(brightness=0.0,
  contrast=0.0, saturation=0.0,
  hue=hue, always_apply=True, p=1.0)</pre>
			<p>Pluto <a id="_idIndexMarker338"/>has exaggerated the <em class="italic">unsafe</em> value so <a id="_idIndexMarker339"/>that you can see <a id="_idIndexMarker340"/>the results in this book. The <em class="italic">unsafe</em> parameter for contrast in the sea animals dataset is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the contrast wrapper function on sea animal image</strong>
pluto.draw_image_contrast(pluto.df_sea_animal,
  contrast=8.5,
  bsize=2)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B17990_03_26.jpg" alt="Figure 3.26 – Unsafe use of contrast on the sea animals dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.26 – Unsafe use of contrast on the sea animals dataset</p>
			<p>The <em class="italic">unsafe</em> parameter for saturation in the food dataset is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the contrast wrapper function on food image</strong>
pluto.draw_image_saturation(pluto.df_food,
  saturation=10.5)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B17990_03_27.jpg" alt="Figure 3.27 – Unsafe use of saturation on the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.27 – Unsafe use of saturation on the food dataset</p>
			<p>The <em class="italic">unsafe</em> parameter<a id="_idIndexMarker341"/> for hue shifting in the <a id="_idIndexMarker342"/>People dataset is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the contrast wrapper function on people photo</strong>
pluto.draw_image_hue(pluto.df_people,hue=0.5)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B17990_03_28.jpg" alt="Figure 3.28 – Unsafe use of hue shifting on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.28 – Unsafe use of hue shifting on the People dataset</p>
			<p>Contrast, saturation, and hue shifting apply to five of the image datasets, and the key is to find the safe <a id="_idIndexMarker343"/>range for each dataset. The exception is the medical images – the Covid-19 photos. You need to consult a domain expert to see how much you can distort the images and retain <span class="No-Break">their integrity.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: can you think of an image category that would safely use hue shifting? In other words, in what photo subject can you shift the hue value and not compromise the image’s integrity? Hint: think of an animal that hunts by sonar or <span class="No-Break">heat source.</span></p>
			<p>The next filter we’ll cover is the noise injection filter, which can be easily recognized in this book’s <span class="No-Break">grayscale photos.</span></p>
			<h3>Noise injection</h3>
			<p>Noise injection <a id="_idIndexMarker344"/>is a strange filter because it is counterintuitive. Image augmentation distorts the original pictures within a safe limit, but injecting noise into a photo causes the images to <span class="No-Break">degrade deliberately.</span></p>
			<p>The scholarly paper <em class="italic">Data Augmentation in Training CNNs: Injecting Noise to Images</em>, by Murtaza Eren Akbiyik, published in 2019, and reviewed at the <em class="italic">ICLR 2020 Conference</em>, analyzes the effects of adding or applying different noise models of varying magnitudes to CNN architectures. It shows that noise injection provides a better understanding of optimal learning procedures for <span class="No-Break">image classification.</span></p>
			<p>For the <strong class="source-inline">draw_image_noise()</strong> wrapper method, Pluto uses the Albumentation’s Gaussian noise method, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use Albumentations for noise injection</strong>
aug_album = albumentations.GaussNoise(var_limit=var_limit,
  always_apply=True,
  p=1.0)</pre>
			<p>Pluto bumps the noise level to the extreme for the exaggerated <em class="italic">unsafe</em> case. The command in the Python Notebook is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use noise wrapper function on fungi photo</strong>
pluto.draw_image_noise(pluto.df_fungi,
  var_limit=(10000.0, 20000.0), bsize=2)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B17990_03_29.jpg" alt="Figure 3.29 – Unsafe use of noise injection on the fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.29 – Unsafe use of noise injection on the fungi dataset</p>
			<p>Since the <a id="_idIndexMarker345"/>goal of the mall crowd dataset is to estimate the crowd size, adding some noise to the image is acceptable. Pluto found that the <em class="italic">safe</em> noise level is from about <strong class="source-inline">200</strong> to <strong class="source-inline">400</strong>. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use noise wrapper function on crowd image</strong>
pluto.draw_image_noise(pluto.df_crowd,
  var_limit=(200.0, 400.0),
  bsize=2)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B17990_03_30.jpg" alt="Figure 3.30 – Safe use of noise injection on the mall crowd dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.30 – Safe use of noise injection on the mall crowd dataset</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is both a thought and a hands-on experiment. Can you define a set of ranges for each image augmentation that applies to a specific image topic, such as landscape, birds, or house appliances? If you think that is possible, can you write a Python function that uses Pluto’s <span class="No-Break">wrapper functions?</span></p>
			<p>This is <a id="_idIndexMarker346"/>when Pluto begins experimenting with more exotic filters, but he limits his choices to the image augmentation methods studied in published scholarly papers. The next two filters we will look at are the rain and sun <span class="No-Break">flare filters.</span></p>
			<h3>Rain and sun flare</h3>
			<p>In image augmentation, rain and sun flare effects are not widely used in AI. However, it is an acceptable option if the image domain is landscape or cityscape. The rain and sun flare implementations are simplistic because they are optimized for speed over a realistic depiction of rain or <span class="No-Break">sun flare.</span></p>
			<p>If you <a id="_idIndexMarker347"/>require a <a id="_idIndexMarker348"/>natural rain effect, then you can refer to a paper that presents a new approach to synthesizing realistic rainy scenes<a id="_idIndexMarker349"/> using a <strong class="bold">generative adversarial network</strong> (<strong class="bold">GAN</strong>): <em class="italic">Synthesized Rain Images for Deraining Algorithms</em>, by Jaewoong Choi, Dae Ha Kim, Sanghyuk Lee, Sang Hyuk Lee, and Byung Cheol Song, published in 2022 in <em class="italic">Neurocomputing </em><span class="No-Break"><em class="italic">Volume 492</em></span><span class="No-Break">.</span></p>
			<p>The realistic rendering will take some time. Therefore, you should use the pre-processing augmentation method, which generates the images and saves them to local or cloud disk storage, before training the <span class="No-Break">AI model.</span></p>
			<p>Pluto does not have <a id="_idIndexMarker350"/>access to the <a id="_idIndexMarker351"/>GAN method, so he uses the Albumentations library for dynamically generating the effects. The key code inside the <strong class="source-inline">draw_image_rain()</strong>, and <strong class="source-inline">draw_image_sunflare()</strong> wrapper functions is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># for rain</strong>
aug_album = albumentations.RandomRain(
  drop_length = drop_length,
  drop_width=drop_width,
  blur_value=blur_value,
  always_apply=True,
  p=1.0)
<strong class="bold"># for sun flare</strong>
aug_album = albumentations.RandomSunFlare(
  flare_roi = flare_roi,
  src_radius=src_radius,
  always_apply=True, p=1.0)</pre>
			<p>Pluto exaggerates the effects of the sun flare filter to an <em class="italic">unsafe</em> level. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use sunflare wrapper function with people photo</strong>
pluto.draw_image_sunflare(pluto.df_people,
  flare_roi=(0, 0, 1, 0.5),
  src_radius=400,
  bsize=2)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B17990_03_31.jpg" alt="Figure 3.31 – Unsafe use of the sun flare filter on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.31 – Unsafe use of the sun flare filter on the People dataset</p>
			<p>Pluto discovered that the <em class="italic">safe</em> level <a id="_idIndexMarker352"/>for the fungi dataset is a radius of <strong class="source-inline">120</strong>, with a <strong class="source-inline">flare-roi</strong> of <strong class="source-inline">(0, 0, 1)</strong>. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the sunflare wrapper function on fungi image</strong>
pluto.draw_image_sunflare(pluto.df_fungi, src_radius=120)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B17990_03_32.jpg" alt="Figure 3.32 – Safe use of the sun flare filter on the fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.32 – Safe use of the sun flare filter on the fungi dataset</p>
			<p>For the People dataset, Pluto found that the safe parameter is a <strong class="source-inline">drop_length</strong> equal to <strong class="source-inline">20</strong>, a <strong class="source-inline">drop_width</strong> equal to <strong class="source-inline">1</strong>, and a <strong class="source-inline">blur_value</strong> equal <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
<strong class="bold"># use the rain wrapper function on people photo</strong>
pluto.draw_image_rain(pluto.df_people, drop_length=20,
  drop_width=1, blur_value=1)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B17990_03_33.jpg" alt="Figure 3.33 – Safe use of the rain filter on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.33 – Safe use of the rain filter on the People dataset</p>
			<p>Many more photometric<a id="_idIndexMarker353"/> transformations are available; for example, Albumentations has over 70 image filters. However, for now, Pluto will present two more effects: the Sepia and <span class="No-Break">FancyPCA filters.</span></p>
			<h3>Sepia and FancyPCA</h3>
			<p>Sepia<a id="_idIndexMarker354"/> involves altering the color tone to a brownish color. This brown is the color of cuttlefish ink, and the result gives the effect of old or aged pictures. Fancy <strong class="bold">Principal Components Analysis</strong> (<strong class="bold">FancyPCA</strong>) color <a id="_idIndexMarker355"/>augmentation alters the RGB channels’ intensities along the images’ <span class="No-Break">natural variations.</span></p>
			<p>A scholarly research paper used the<a id="_idIndexMarker356"/> FancyPCA filter to improve DL prediction of rock properties in reservoir formations: <em class="italic">Predicting mineralogy using a deep neural network and fancy PCA</em> by Dokyeong Kim, Junhwan Choi, Dowan Kim, and Joongmoo Byun, in 2022, presented at the <em class="italic">SEG International Exposition and </em><span class="No-Break"><em class="italic">Annual Meeting</em></span><span class="No-Break">.</span></p>
			<p>For the <strong class="source-inline">draw_image_sepia()</strong> and <strong class="source-inline">draw_image_fancyPCA()</strong> wrapper functions, Pluto uses the <span class="No-Break">Albumentations library:</span></p>
			<pre class="source-code">
<strong class="bold"># for sepia use albumentations library</strong>
aug_album = albumentations.ToSepia(always_apply=True, p=1.0)
<strong class="bold"># for FancyPCA use albumentations library</strong>
aug_album = albumentations.FancyPCA(alpha=alpha,
  always_apply=True,
  p=1.0)</pre>
			<p>You can see the<a id="_idIndexMarker357"/> results in the Python Notebook’s color output images. Pluto has chosen the People dataset to experiment <a id="_idIndexMarker358"/>with the sepia and FancyPCA filters because it has no objective. Assuming the target is to classify people’s age ranges, both filters are applicable. For the sepia filter, the command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use the sepia wrapper function on people photo</strong>
pluto.draw_image_sepia(pluto.df_people)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B17990_03_34.jpg" alt="Figure 3.34 – Safe use of sepia on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.34 – Safe use of sepia on the People dataset</p>
			<p>Pluto overstates the FancyPCA filter to an <em class="italic">unsafe</em> level by setting the alpha value to <strong class="source-inline">5.0</strong>. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use fancyPCA wrapper function on people photo</strong>
pluto.draw_image_fancyPCA(pluto.df_people,alpha=5.0,bsize=2)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B17990_03_35.jpg" alt="Figure 3.35 – Unsafe use of FancyPCA on the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.35 – Unsafe use of FancyPCA on the People dataset</p>
			<p>So far, you and <a id="_idIndexMarker359"/>Pluto have covered <a id="_idIndexMarker360"/>and written many wrapper functions for photometric transformations, such as lighting, grayscale, contrast, saturation, hue shifting, noise injection, rain, sun flare, sepia, and FancyPCA. Still, there are far more image filters in the Albumentations library. Pluto follows the golden image augmentation rule for selecting a filter that improves prediction accuracy, as a published scholarly paper describes, such as <em class="italic">The Effectiveness of Data Augmentation in Image Classification using Deep Learning</em> paper by <strong class="bold">Luis Perez, Jason Wang</strong>, published by the <em class="italic">Cornell University Arxiv</em> in <span class="No-Break">December 2017.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: there are too many image augmentation techniques to count. So, how do you know which augmentation function is suitable to use? For example, is the Cinematic Anamorphic photo filter an effective image augmentation technique? Hint: think about the subject domain and the <span class="No-Break">processing speed.</span></p>
			<p>Moving away from photographic transformations, next, Pluto will dig into the random <span class="No-Break">erasing filter.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>Random erasing</h2>
			<p>Random <a id="_idIndexMarker361"/>erasing adds a block of noise, while noise injection adds one pixel at <span class="No-Break">a time.</span></p>
			<p>Two recently published papers show that random erasing filters and extended random erasing increase the prediction accuracy of the DL model. The first paper is called <em class="italic">Random Erasing Data Augmentation</em>, by Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang, in 2020, and was presented at the <em class="italic">AAAI Conference on Artificial Intelligence</em>. The second paper is called <em class="italic">Perlin Random Erasing for Data Augmentation</em>, by Murat Saran, Fatih Nar, and Ayşe Nurdan Saran, in 2021, and was presented at the 29th <em class="italic">Signal Processing and Communications Applications </em><span class="No-Break"><em class="italic">Conference (SIU)</em></span><span class="No-Break">.</span></p>
			<p>Pluto uses the Fast.ai library in the <strong class="source-inline">draw_image_erasing()</strong> wrapper function. The pertinent code is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use fastai library for random erasing</strong>
aug = [fastai.vision.augment.RandomErasing(p=1.0,
  max_count=max_count)]
itfms = fastai.vision.augment.Resize(480)</pre>
			<p>It is<a id="_idIndexMarker362"/> challenging to find a safe level for random erasing. It depends on the image subject, DL base model, and target label. Generally, Pluto selects a random erasing safe parameter and trains the AI model. If the DL model is overfitting, then he increases the random erasing effects, and if the model’s prediction accuracy is diverging, he decreases the random erasing parameters. Here is a safe starting point for the <span class="No-Break">food dataset:</span></p>
			<pre class="source-code">
<strong class="bold"># use random erasing wrapper function on food image</strong>
pluto.dls_food = pluto.draw_image_erasing(
  pluto.df_food,
  bsize=6,
  max_count=4)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B17990_03_36.jpg" alt="Figure 3.36 – Unsafe use of FancyPCA on the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.36 – Unsafe use of FancyPCA on the food dataset</p>
			<p>So far, Pluto <a id="_idIndexMarker363"/>uses one filter at a time. Next, he will combine multiple geographic and photographic transformations with <span class="No-Break">random erasing.</span></p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor080"/>Combining</h2>
			<p>The <a id="_idIndexMarker364"/>power of image augmentation is that Pluto can combine multiple image filters in one dataset. This increases the number of images for training by a <span class="No-Break">multiplication factor.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The horizontal flip filter’s default is set to 50% probability. The result is that the image size increases by half – that is, <strong class="source-inline">total_flip = image_size + (0.5 * image_size)</strong>. The image size will increase by a multiplication factor when the random cropping and padding are added together with a padding mode of 3 – that is, <strong class="source-inline">total_2_combine = total_fip + (3 * (image_size + (0.5 * image_size)) + (image_size * random_croping_factor))</strong>, where <strong class="source-inline">random_croping_factor</strong> is between zero and the safe cropping value, which is less <span class="No-Break">than 1.0.</span></p>
			<p>In this chapter, Pluto covered 15 image augmentation methods. Therefore, combining most or all of the filters into one dataset will increase its size substantially. Increasing the total number of images for training in DL is a proven method to reduce or eliminate the <span class="No-Break">overfitting problem.</span></p>
			<p>There are <a id="_idIndexMarker365"/>general rules for the applicable filters and safe parameters that should work with most image datasets. However, Pluto follows the golden rule of image augmentation. This golden rule selects which image filter to use and sets the safe parameters based on the photo subject and the predictive model’s goal. In other words, each project will have different image augmentation filters and <span class="No-Break">safe parameters.</span></p>
			<p>Before unveiling the table representing the safe parameters for each filter per six real-world image datasets, Pluto must review<a id="_idIndexMarker366"/> the image datasets’ topics <span class="No-Break">and goals:</span></p>
			<ul>
				<li>The <strong class="bold">Covid-19</strong> dataset consists of people’s chest X-ray images. The goal is to predict between Covid-19, viral pneumonia, <span class="No-Break">and normal.</span></li>
				<li>The <strong class="bold">People</strong> dataset consists of pictures of everyday people. No goal is stated, but Pluto assumes the usage could classify age, sex, ethnicity, emotional sentiment, and <span class="No-Break">facial recognition.</span></li>
				<li>The <strong class="bold">Fungi</strong> dataset consists of photos of fungi in a natural environment. The goal is to predict if the fungi are edible <span class="No-Break">or poisonous.</span></li>
				<li>The <strong class="bold">Sea Animal</strong> dataset consists of images of sea animals, mainly underwater. The goal is to classify the 19 sea <span class="No-Break">animals provided.</span></li>
				<li>The <strong class="bold">Food</strong> dataset consists of images of commonly served Vietnamese dishes. The goal is to classify the 31 types <span class="No-Break">of dishes.</span></li>
				<li>The <strong class="bold">Mall Crowd</strong> dataset consists of images of people in a typical shopping mall. The goal is to predict the size of <span class="No-Break">the crowd.</span></li>
			</ul>
			<p>To generate the filters and safe parameters table for each image dataset, Pluto has written a quick function, <strong class="source-inline">print_safe_parameters()</strong>, using pandas, because he thinks <a id="_idIndexMarker367"/>coding is fun. For readability, there are two parts to the table, <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17990_03_37.jpg" alt="Figure 3.37 – Safe parameter for each image dataset – part 1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.37 – Safe parameter for each image dataset – part 1</p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.37</em> shows the first half of the big table, and <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.38 </em>displays the <span class="No-Break">second half.</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17990_03_38.jpg" alt="Figure 3.38 – Safe parameter for each image dataset – part 2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.38 – Safe parameter for each image dataset – part 2</p>
			<p>The safe parameters<a id="_idIndexMarker368"/> are from Pluto’s exploration of the Python Notebook, but you may find more suitable values than Pluto. There are no rigid or fixed rules regarding image augmentation. Therefore, you should use the Python Notebook to explore the possibilities. If you read a scholarly paper about a new image augmentation technique, implement it using the Albumentations or other <span class="No-Break">image libraries.</span></p>
			<p>Pluto has written six wrapper functions for reinforcing learning through coding, one for each <span class="No-Break">image dataset.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">You can run the wrapper function repeatedly because it generates a different image set every time. In addition, it will randomly select other base images from the real-world dataset. Therefore, you can run it a thousand times and only see the same <span class="No-Break">output once.</span></p>
			<p>Each wrapper function defines a set of Fast.ai image augmentation filters; <span class="No-Break">for example:</span></p>
			<pre class="source-code">
<strong class="bold"># use fastai library for brightness and contrast</strong>
aug = [
  fastai.vision.augment.Brightness(max_lighting=0.3,p=0.5),
  fastai.vision.augment.Contrast(max_lighting=0.4, p=0.5)]
<strong class="bold"># use albumentation library</strong>
albumentations.Compose([
  albumentations.GaussNoise(var_limit=(100.0, 300.0),
    p=0.5)])</pre>
			<p>In addition, the <a id="_idIndexMarker369"/>wrapper function uses a helper method to fetch the <strong class="source-inline">Albumentations</strong> filters – for <span class="No-Break">example, </span><span class="No-Break"><strong class="source-inline">_fetch_album_covid19()</strong></span><span class="No-Break">.</span></p>
			<p>Pluto reviews the image augmentation for the Covid-19 dataset by using the following command in the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># use covid 19 wrapper function</strong>
pluto.draw_augment_covid19(pluto.df_covid19)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17990_03_39.jpg" alt="Figure 3.39 – Image augmentation for the Covid-19 dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.39 – Image augmentation for the Covid-19 dataset</p>
			<p>The<a id="_idIndexMarker370"/> relevant code lines for the combination filters for the People dataset are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use both fastai and albumentation library</strong>
aug = [
  fastai.vision.augment.Brightness(max_lighting=0.3,p=0.5),
  fastai.vision.augment.Contrast(max_lighting=0.4, p=0.5),
  AlbumentationsTransform(self._fetch_album_covid19())]
<strong class="bold"># use alpbumentation library in the _fetch_albm_covid()</strong>
albumentations.Compose([
  albumentations.ColorJitter(brightness=0.3,
    contrast=0.4, saturation=3.5,hue=0.0, p=0.5),
  albumentations.ToSepia(p=0.5),
  albumentations.FancyPCA(alpha=0.5, p=0.5),
  albumentations.GaussNoise(var_limit=(300.0, 500.0), p=0.5)
  ])</pre>
			<p>Pluto reviews <a id="_idIndexMarker371"/>the image augmentation for the People dataset by using the following command in the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># use people wrapper function</strong>
pluto.draw_augment_people(pluto.df_people)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17990_03_40.jpg" alt="Figure 3.40 – Image augmentation for the People dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.40 – Image augmentation for the People dataset</p>
			<p>The relevant code lines for the fungi combination filters are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use both fastai and albumentations libraries</strong>
aug = [
  fastai.vision.augment.Flip(p=0.5),
  fastai.vision.augment.Rotate(25.0,p=0.5,pad_mode='border'),
  fastai.vision.augment.Warp(magnitude=0.3,
    pad_mode='border',p=0.5),
  fastai.vision.augment.RandomErasing(p=0.5,max_count=2),
  AlbumentationsTransform(self._fetch_album_fungi())]
<strong class="bold"># use albumentation inside the _fetch_album_fungi()</strong>
albumentations.Compose([
  albumentations.ColorJitter(brightness=0.3,
    contrast=0.4, saturation=2.0,hue=0.0, p=0.5),
  albumentations.FancyPCA(alpha=0.5, p=0.5),
  albumentations.RandomSunFlare(flare_roi=(0, 0, 1, 0.5),
    src_radius=200, always_apply=True, p=0.5),
  albumentations.RandomRain(drop_length=20,
    drop_width=1.1,blur_value=1.1,always_apply=True, p=0.5),
  albumentations.GaussNoise(var_limit=(200.0, 400.0), p=0.5)
  ])</pre>
			<p>Pluto reviews the <a id="_idIndexMarker372"/>image augmentation for the fungi dataset by using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># use fungi wrapper function</strong>
pluto.draw_augment_fungi(pluto.df_fungi)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B17990_03_41.jpg" alt="Figure 3.41 – Image augmentation for the ﻿fungi dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.41 – Image augmentation for the fungi dataset</p>
			<p>The relevant <a id="_idIndexMarker373"/>code lines for the sea animal combination filters are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use both fastai and albumentations library</strong>
aug = [
  fastai.vision.augment.Dihedral(p=0.5,
    pad_mode='reflection'),
  fastai.vision.augment.Rotate(180.0,
    p=0.5,pad_mode='reflection'),
  fastai.vision.augment.Warp(magnitude=0.3,
    pad_mode='reflection',p=0.5),
  fastai.vision.augment.RandomErasing(p=0.5,max_count=2),
  AlbumentationsTransform(self._fetch_album_sea_animal())]
<strong class="bold"># use albumentations for _fetch_album_sea_animal()</strong>
albumentations.Compose([
  albumentations.ColorJitter(brightness=0.4,
    contrast=0.4, saturation=2.0,hue=1.5, p=0.5),
  albumentations.FancyPCA(alpha=0.5, p=0.5),
  albumentations.GaussNoise(var_limit=(200.0, 400.0),
    p=0.5)])</pre>
			<p>Pluto reviews <a id="_idIndexMarker374"/>the image augmentation for the sea animal dataset by using the following command in the <span class="No-Break">Python Notebook:</span></p>
			<pre class="source-code">
<strong class="bold"># use the sea animal wrapper function</strong>
pluto.draw_augment_sea_animal(pluto.df_sea_animal)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B17990_03_42.jpg" alt="Figure 3.42 – Image augmentation for the sea animals dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.42 – Image augmentation for the sea animals dataset</p>
			<p>The relevant code lines for the food combination filters are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use both fastai and albumentations libraries</strong>
aug = [
  fastai.vision.augment.Dihedral(p=0.5,
    pad_mode='reflection'),
  fastai.vision.augment.Rotate(180.0,
    p=0.5,pad_mode='reflection'),
  fastai.vision.augment.Warp(magnitude=0.3,
    pad_mode='reflection',p=0.5),
  fastai.vision.augment.RandomErasing(p=0.5,max_count=2),
  AlbumentationsTransform(self._fetch_album_food())]
<strong class="bold"># use albumentation library for _fetch_album_food()</strong>
albumentations.Compose([
  albumentations.ColorJitter(brightness=0.4,
    contrast=0.4, saturation=2.0,hue=1.5, p=0.5),
  albumentations.FancyPCA(alpha=0.5, p=0.5),
  albumentations.GaussNoise(var_limit=(200.0, 400.0),
    p=0.5)])</pre>
			<p>Pluto reviews the image augmentation for the food dataset by using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># use food wrapper function</strong>
pluto.draw_augment_food(pluto.df_food)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17990_03_43.jpg" alt="Figure 3.43 – Image augmentation for the food dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.43 – Image augmentation for the food dataset</p>
			<p>The relevant <a id="_idIndexMarker375"/>code lines for the mall crowd combination filters are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># use both fastai and albumentations libraries</strong>
aug = [
  fastai.vision.augment.Flip(p=0.5),
  fastai.vision.augment.Rotate(25.0,
    p=0.5,pad_mode='zeros'),
  fastai.vision.augment.Warp(magnitude=0.3,
    pad_mode='zeros',p=0.5),
  fastai.vision.augment.RandomErasing(p=0.5,max_count=2),
  AlbumentationsTransform(self._fetch_album_crowd())]
<strong class="bold"># use albumentation library for _fetch_album_crowd()</strong>
albumentations.Compose([
  albumentations.ColorJitter(brightness=0.3,
    contrast=0.4, saturation=3.5,hue=0.0, p=0.5),
  albumentations.ToSepia(p=0.5),
  albumentations.FancyPCA(alpha=0.5, p=0.5),
  albumentations.GaussNoise(var_limit=(300.0, 500.0),
    p=0.5)])</pre>
			<p>Pluto reviews the<a id="_idIndexMarker376"/> image augmentation for the mall crowd dataset by using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># use the crowd wrapper function</strong>
pluto.draw_augment_crowd(pluto.df_crowd)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B17990_03_44.jpg" alt="Figure 3.44 – Image augmentation for the mall crowd dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.44 – Image augmentation for the mall crowd dataset</p>
			<p>Every time Pluto runs any of the draw augmentation methods, there is an equal chance that one of the filters will be selected and that 50% of the filter will be applied per image in the batch. Pluto can override the default batch size of 15 using the <strong class="source-inline">bsize</strong> parameter. Since Pluto employs the safe range on all filters, you may not notice the difference. However, that is expected because the goal is to distort the images without compromising the<a id="_idIndexMarker377"/> target label <span class="No-Break">before pre-processing.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Write a new combination wrapper function for a real-world dataset. If you have not downloaded or imported a new real-world image dataset, do so now and write a combined wrapper function as <span class="No-Break">Pluto did.</span></p>
			<p>This was a challenging chapter. Together, you and Pluto learned about image augmentation and how to use Python code to gain a deeper insight. Now, let’s summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor081"/>Summary</h1>
			<p>In the first part of this chapter, you and Pluto learn about the image augmentation concepts for classification. Pluto grouped the filters into geometric transformations, photometric transformations, and random erasing to make the image filters <span class="No-Break">more manageable.</span></p>
			<p>When it came to geometric transformations, Pluto covered horizontal and vertical flipping, cropping and padding, rotating, warping, and translation. These filters are suitable for most image datasets, and there are other geometric transformations, such as tilting or skewing. Still, Pluto followed the golden image augmentation rule for selecting a filter that improves prediction accuracy described in a published <span class="No-Break">scholarly paper.</span></p>
			<p>This golden rule is more suitable for photometric transformations because there are about 70 image filters in the Albumentations library and hundreds more available in other image augmentation libraries. This chapter covered the most commonly used photometric transformations cited in published scholarly papers: lighting, grayscale, contrast, saturation, hue shifting, noise injection, rain, sepia, and FancyPCA. You are encouraged to explore more filters from the Albumations library in the Python <span class="No-Break">Notebook provided.</span></p>
			<p>The second part of this chapter consisted of Python code to reinforce your understanding of various image augmentation techniques. Pluto led you through the process of downloading the six real-world image datasets from the <em class="italic">Kaggle</em> website. Pluto wrote the fetching data code in <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. He reused the fetch functions to retrieve the Covid-19, people, fungi, sea animals, food, and mall crowd real-world <span class="No-Break">image datasets.</span></p>
			<p>Before digging into the code, Pluto reviewed seven popular image augmentation libraries: Albumentations, Fast.ai, PIL, OpenCV, scikit-learn, Mahotas, and GraphicsMagick. Pluto used the Albumentations, Fast.ai, and PIL libraries to code the <span class="No-Break">wrapper functions.</span></p>
			<p>The goal of these wrapper functions was to explain each image filter clearly. In all cases, the functions use the library augmentation methods under the hood. Many photometric transformations are more visible in the Python Notebook’s color output. Pluto showed the safe and <em class="italic">unsafe</em> parameters for each filter applied to the six image datasets. You are encouraged to run and even hack the Python Notebook’s code because there are no absolute right or wrong <span class="No-Break">safe levels.</span></p>
			<p>A lot of Python code was provided, but it consisted of simple wrapper functions that followed good OOP standards and there were no other methods. The goal was to give you insight into each image filter and make it easy for you to explore and hack the <span class="No-Break">code provided.</span></p>
			<p>At the end of this chapter, Pluto pulled together to create an image filter combination table customized for each of the six image datasets. He then wrote six combined augmentation methods for each <span class="No-Break">image dataset.</span></p>
			<p>Throughout this chapter, there were <em class="italic">fun facts</em> and <em class="italic">fun challenges</em>. Pluto hopes you will take advantage of these and expand your experience beyond the scope of <span class="No-Break">this chapter.</span></p>
			<p>In the next chapter, we will cover image segmentation, which reuses many of the image classification functions that were covered in <span class="No-Break">this chapter.</span></p>
		</div>
	</body></html>