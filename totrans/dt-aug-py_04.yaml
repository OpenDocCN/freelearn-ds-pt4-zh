- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Augmentation for Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image segmentation, like image classification, is the cornerstone in the computer
    vision domain. Image segmentation involves grouping parts of an image that belong
    to the same object, also known as pixel-level classification. Unlike image classification,
    which identifies and predicts the subject or label of a photo, image segmentation
    determines whether a pixel belongs to a list of objects – for example, an urban
    photograph has streets, street signs, cars, trucks, bicycles, buildings, trees,
    and pedestrians. Image segmentation’s job is to decide whether this image pixel
    belongs to a car, tree, or other objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**), an **artificial neural network** (**ANN**) algorithm,
    has made a tremendous breakthrough in image segmentation. For example, image segmentation
    in DL makes it possible for autonomous vehicles and **Advanced Driver Assistance
    Systems** (**ADASes**) to detect navigable surfaces or pedestrians. Many medical
    applications use segmentation for tumor boundary drawing or measuring tissue volumes,
    for example.'
  prefs: []
  type: TYPE_NORMAL
- en: The image augmentation methods for segmentation or classification are the same,
    except segmentation comes with an additional mask image or ground-truth image.
    Therefore, most of what we learned about augmenting images for classification
    in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058) applies to augmenting segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter aims to provide continuing geometric and photometric transformations
    for image segmentation. In particular, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric and photometric transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world segmentation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcing your learning through Python code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation or semantic segmentation is used in many self-driving car
    AI controllers. It is used to identify objects and people on a street. Worldwide
    competition wins or losses primarily due to image segmentation augmentation techniques,
    such as the *Udacity and Lyft Perception Challenge* winner of the *Kaggle* competition,
    use random resized crop, horizontal flip, and random color jitter in brightness,
    contrast, and saturation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the geometric and photometric transformations for segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric and photometric transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058), geometric
    transformations alter a picture’s geometry, such as by flipping, cropping, padding,
    rotating, or resizing it. For segmentation, when horizontally **flipping** an
    image, the same must be done for the mask. Pluto will show you how to flip an
    original and accompanying mask image; here is a sneak peek:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Image segmentation horizontal flip](img/B17990_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Image segmentation horizontal flip
  prefs: []
  type: TYPE_NORMAL
- en: Many of the **safe** values discussed in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058)
    stay mostly the same. For example, if the picture’s subject is people or an urban
    cityscape, the classification augmentation can’t flip vertically because the prediction
    of people’s age or the city’s name relies on the picture not being upside down.
    However, segmentation aims to group or draw an outline of the people or cars.
    Thus, vertical flipping is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: The safe range needs further investigation for many real-world applications.
    For example, for self-driving automobiles, what if you are in a car accident and
    your vehicle is upside down? Does the AI still need to classify its surroundings
    correctly?
  prefs: []
  type: TYPE_NORMAL
- en: Photometric transformations, such as brightness, saturation, contrast, hue shifting,
    and FancyPCA, are more problematic to apply to segmentation because the original
    image is distorted but not the mask image. The big question is, would augmenting
    the original but not the mask image increase the prediction’s accuracy?
  prefs: []
  type: TYPE_NORMAL
- en: '`Albumentations` library, 37 transformations are defined as safe for distorting
    both original and mask images.'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you can use photometric transformations for segmentation with Python
    code, but it is wise to research published scholarly papers for confirmation.
    The golden augmentation rule that we discussed in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058)
    is applied here as well – you select a filter that improves the prediction accuracy
    described in a published academic paper.
  prefs: []
  type: TYPE_NORMAL
- en: Learning by using Python code is another angle you can use to understand image
    segmentation. However, before we do that, let’s ask Pluto to download a few real-world
    segmentation datasets from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world segmentation datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Kaggle* website is an online community platform for data scientists and
    ML devotees. It contains thousands of real-world datasets, as mentioned in *Chapters
    1*, *2*, and *3*.
  prefs: []
  type: TYPE_NORMAL
- en: When searching for image segmentation datasets, Pluto found about 500 useable
    real-world segmentation datasets. The topics range from self-driving automobiles
    and medicine to micro-fossils. Pluto picked two segmentation datasets from popular
    market segments.
  prefs: []
  type: TYPE_NORMAL
- en: The other consideration is that the image type must be easy to work with in
    the Albumentations library. Pluto uses the **PIL** and **NumPy** libraries to
    read and convert the photos into a three-dimensional array. The original image’s
    **shape** is (width, height, and depth), where depth is usually equal to three.
    The mask image’s **shape** is (width, height), where the value is 0, 1, 2, and
    so on up to the number of labels.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: The PIL library can read image formats such as `.jpg`, `.gif`, `.tiff`, `.png`,
    and about 50 other image formats. Still, sometimes, the real-world segmentation
    datasets come with an image format that PIL can’t read. In those cases, Pluto
    relies on the Python **ImageIO** library, which can read over 100 image types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two selected segmentation datasets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Cambridge-Driving Labeled Video (CamVid)* database is the first real-world
    segmentation dataset. The context on the *Kaggle* website is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*The Cambridge-Driving Labeled Video Database (CamVid) provides ground truth
    labels that associate each pixel with one of 32 semantic classes. This dataset
    is often used in (real-time) semantic segmentation research.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'It was published in 2020 by the **University of Cambridge**, and the license
    is **CC BY-NC-SA 4.0**: [https://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second real-world dataset is called *Semantic segmentation of aerial imagery*.
    The description from the *Kaggle* website is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites
    and annotated with pixel-wise semantic segmentation in 6 classes. The total volume
    of the dataset is 72 images grouped into 6 larger tiles.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'It was published in 2020 by the **Roia Foundation in Syria**, and the license
    is **CC0: Public Domain**: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the two segmentation datasets, the following four steps should
    be familiar to you by now. Review *Chapters 2* and *3* if you need clarification.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the Python Notebook and Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download real-world data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the data images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Find and download two additional image segmentation datasets from the *Kaggle*
    website or other sources. *Kaggle* competitions and data consist of hundreds of
    image segmentation datasets. Thus, finding image segmentation datasets that are
    meaningful to you or your job shouldn’t be challenging. Hint: use Pluto’s `fetch_kaggle_dataset()`
    `or` `fetch_kaggle_comp_data()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with Pluto.
  prefs: []
  type: TYPE_NORMAL
- en: Python Notebook and Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading the `data_augmentation_with_python_chapter_4.ipynb` file into
    Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this
    point onward, the code snippets will be from the Python Notebook, which contains
    the complete functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you must clone the repository and use the `%run` command to start Pluto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows or similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Double-check that Pluto has loaded correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows or something similar, depending on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Pluto has verified that the Python Notebook is working correctly. The next step
    is downloading real-world image datasets from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following download function is from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038).
    Pluto has reused this here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Before viewing the downloaded photos, Pluto needs to load the information into
    a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A few cleanup tasks need to be done here, such as replacing a space character
    with an underscore character in the directories or filenames and separating original
    and mask images. After the cleanup, Pluto reuses the `make_dir_dataframe()` function
    to read the original image data into a pandas DataFrame. The command for the CamVid
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the first three records is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – CamVid pandas DataFrame, first three rows](img/B17990_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – CamVid pandas DataFrame, first three rows
  prefs: []
  type: TYPE_NORMAL
- en: The mask images are in a different folder, and the mask image’s name has `_L`
    appended to the filename.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary reason for Pluto using pandas is that adding a new column for the
    matching mask and original filename is a trivial task. There are only two key
    code lines. The first is in the helper function to generate the correct mask image
    path, while the second is to create a new column for applying the helper function.
    The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The command to complete the CamVid DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Complete CamVid DataFrame, first three rows](img/B17990_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Complete CamVid DataFrame, first three rows
  prefs: []
  type: TYPE_NORMAL
- en: Once Pluto has gathered all the information squared away in the DataFrame, the
    next step is to display the original and mask images.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing data images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pluto could reuse the `draw_batch()` function from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038)
    to display the original and mask images in separate batches, but the result does
    not reinforce the combination of original and mask images. Therefore, Pluto will
    hack the `draw_batch()` method and create a new `draw_batch_segmentation()` and
    a helper function called `_draw_batch_segmentation()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result shows the original image, then the mask image, and repeats this
    process. The command for displaying the CamVid segmentation photos is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – CamVid’s original and mask image batch](img/B17990_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – CamVid’s original and mask image batch
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation batch looks correct, so Pluto repeats the same process for
    the aerial segmentation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Clean the directory and filenames, then import them into pandas with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the first five records is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Aerial pandas DataFrame, first three rows](img/B17990_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Aerial pandas DataFrame, first three rows
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the mask’s filename using the new help function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the first three records is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Complete aerial DataFrame, first three rows](img/B17990_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Complete aerial DataFrame, first three rows
  prefs: []
  type: TYPE_NORMAL
- en: 'Display the segmentation image batch with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Aerial pandas DataFrame, first five rows](img/B17990_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Aerial pandas DataFrame, first five rows
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: given an image dataset, how do you create the
    mask for the photos? Hint: you could use fancy image software to auto-trace the
    objects or outlines, then label them. The other options are Mechanical Turk or
    crowd-sourced. You should think about cost versus time.'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has successfully downloaded and displayed the CamVid and aerial segmentation
    photos. Now, let’s do some image augmentation with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcing your learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The same concepts for classification image transformations apply to segmentation
    image transformations. Here, Pluto reuses or slightly hacks the wrapper functions
    in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058). In particular, Pluto hacks
    the following methods for segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random resizing and cropping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transpose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lighting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FancyPCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: You can’t complete or understand this chapter unless you have read [*Chapter
    3*](B17990_03.xhtml#_idTextAnchor058). This is because Pluto reuses or slightly
    modifies the existing image augmentation wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto chose these filters because the Albumentations library marked them as
    **safe** for segmentation. So, let’s start with horizontal flip.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal flip
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pluto demonstrated horizontal flip using the PIL library in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058)
    because the code is easy to understand. Thus, he will hack `draw_image_flip_pil()`
    into the `draw_image_flip_pil_segmen()` function. The transformation code is the
    same – that is, `PIL.ImageOps.mirror(img)`. The change is to display the images
    next to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for flipping an image in the CamVid dataset in the Python Notebook
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Flipping an image using the PIL library](img/B17990_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Flipping an image using the PIL library
  prefs: []
  type: TYPE_NORMAL
- en: Pluto uses the same function for the mask image and passes the `mask_image`
    column into the pandas DataFrame. It is that easy. Pluto has to transform the
    original and mask images with the same filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for flipping the mask image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Flipping the mask using the PIL library](img/B17990_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Flipping the mask using the PIL library
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto only shows relevant code snippets in this book, but the fully functional
    object-oriented methods can be found in the Python Notebook. The code for this
    chapter looks remarkably similar to the code for [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058).
    Pluto designed the software architecture using the principle layout provided in
    [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016). Thus, the code looks clean but
    contains high complexity under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, a color image is a three-dimensional array or a **Rank 3 tensor**.
    The image’s shape is (width, height, and depth), where depth is usually equal
    to three, while the mask image’s shape is (width, height), where the value is
    0, 1, 2, and so on up to the number of labels. Therefore, mirroring a **Rank 3
    tensor** follows the same operation as mirroring a **Rank** **1 tensor**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Albumentations library, the wrapper function for segmentation is as
    simple as the one provided in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058).
    The code for the `draw_image_flip_segmen()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It is the same as the `draw_image_flip()` function that we provided in [*Chapter
    3*](B17990_03.xhtml#_idTextAnchor058). The difference is that a different helper
    function is used. Instead of using the `_draw_image_album()` helper function,
    it uses the `_draw_image_album_segmentation()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for performing a horizontal flip on the CamVid segmentation data
    in the Python Notebook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Horizontal flip on the CamVid dataset](img/B17990_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Horizontal flip on the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for performing a horizontal flip on the aerial segmentation data
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Horizontal flip on the aerial dataset](img/B17990_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Horizontal flip on the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Like in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058), the wrapper functions
    in this chapter randomly select a new image batch every time.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: how can you use image segmentation to support
    environmental organizations such as a wildlife conservation group? Hint: consider
    how customs agents can spot people selling parts of an endangered species, such
    as elephant ivory or saga horn, in an open market using their iPhones or **Close-Circuit
    Television** (**CCTV**) monitoring system.'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto completes the flipping transformation with the vertical flip filter.
  prefs: []
  type: TYPE_NORMAL
- en: Vertical flip
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The vertical flip wrapper function is almost the same as the horizontal flip
    method. Pluto could write one uber function instead of each wrapper method individually.
    Still, the goal is to explain each transformation, not refactor it into more compact
    or efficient code. The key code line for the wrapper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for performing a vertical flip on the CamVid segmentation data
    in the Python Notebook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Vertical flip on the CamVid dataset](img/B17990_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Vertical flip on the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for performing a vertical flip on the aerial segmentation data
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Vertical flip on the aerial dataset](img/B17990_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Vertical flip on the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: That concludes flipping. Now, let’s look at rotating.
  prefs: []
  type: TYPE_NORMAL
- en: Rotating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rotating safe parameter can go 45 degrees clockwise or counter-clockwise
    in direction. The Albumentations method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for rotating the CamVid segmentation data in the Python Notebook
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Rotating the CamVid dataset](img/B17990_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Rotating the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for rotating the aerial segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Rotating the aerial dataset](img/B17990_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Rotating the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: The next filter is resizing and cropping.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing and cropping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The classification model aims to identify the subject, while the segmentation
    model groups object per pixel. Hence, cropping and resizing are acceptable transformations
    at relatively higher safe parameters. The key code line for the wrapper function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for randomly resizing and cropping the CamVid segmentation data
    in the Python Notebook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Resizing and cropping the CamVid dataset](img/B17990_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Resizing and cropping the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for randomly resizing and cropping the aerial segmentation data
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Resizing and cropping the aerial dataset](img/B17990_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Resizing and cropping the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll cover the transpose filter.
  prefs: []
  type: TYPE_NORMAL
- en: Transpose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto didn’t use a transpose filter in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058)
    for classification, but it is permissible for segmentation. Transposing involves
    switching the *x axis* with the *y axis*. The key code line for the wrapper function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for transposing the CamVid segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Transposing the CamVid dataset](img/B17990_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Transposing the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for transposing the aerial segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Transposing the aerial dataset](img/B17990_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Transposing the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement optical distortion in the Python Notebook. Hint: use a similar Pluto
    wrapper function to the Albumentations library function’s `albumentations.OpticalDistortion()`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Transpose is the last example Pluto uses for geometric transformations. Lighting,
    also known as brightness, belongs to the photometric transformations class.
  prefs: []
  type: TYPE_NORMAL
- en: Lighting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lighting or brightness is acceptable for segmentation in the Albumentations
    library, but it belongs to the photometric transformations class. The original
    image changes to a random brightness level up to a safe level, but the mask image
    will not change. For both datasets, the safe parameter is a brightness of 0.5\.
    The key code line in the wrapper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for using lighting on the CamVid segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Using lighting on the CamVid dataset](img/B17990_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Using lighting on the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for using lighting on the aerial segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – Using lightning on the aerial dataset](img/B17990_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – Using lightning on the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the lighting filter, FancyPCA belongs to the photometric transformations
    class.
  prefs: []
  type: TYPE_NORMAL
- en: FancyPCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FancyPCA is the last example Pluto demonstrates for photometric transformations.
    For both datasets, the safe parameter is an alpha value of 0.3\. Once again, FancyPCA
    will not alter the mask image. The key code line in the wrapper function is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for using FancyPCA on the CamVid segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Using FancyPCA on the CamVid dataset](img/B17990_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Using FancyPCA on the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for using FancyPCA on the aerial segmentation data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Using FancyPCA on the aerial dataset](img/B17990_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Using FancyPCA on the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment or maybe a practice one too: what can you do that
    appears acceptable in image augmentation but has a high probability of a **false-positive**
    or **false-negative** prediction in real-world deployment? Sorry, no hint.'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto finds that segmentation augmentation is not that different from classification
    augmentation. The wrapper functions are virtually the same, and only the helper
    methods display the images differently. Pluto has demonstrated segmentation for
    the flipping, resizing, cropping, rotating, transposing, lighting, and FancyPCA
    transformations. Similarly to [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058),
    next, Pluto will combine individual filters into an uber function.
  prefs: []
  type: TYPE_NORMAL
- en: Combining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before coding the uber combination methods in Python, Pluto needs to use pandas
    to summarize the filters in this chapter. Many more transformations are applicable
    for segmentation, so if you experiment with other filters in the Python Notebook,
    expand the pandas table with your new filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto displays the summary table using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Summary segmentation filters](img/B17990_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Summary segmentation filters
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the summary table, Pluto writes the wrapper function. The key code line
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto displays the combination segmentation transformations for the CamVid
    dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Combining the filters for the CamVid dataset](img/B17990_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Combining the filters for the CamVid dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for the aerial dataset in the Python Notebook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Combining filters for the aerial dataset](img/B17990_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Combining filters for the aerial dataset
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to refactor the **Pluto class** to make it faster and more
    compact. You are encouraged to create and upload your library to *GitHub and PyPI.org*.
    Furthermore, you don’t have to name the class **PacktDataAug**, but it would give
    Pluto and his human companion a great big smile if you cited or mentioned this
    book. The code goals were for ease of understanding, reusable patterns, and teaching
    on the **–Python Notebook**. Thus, refactoring the code as a Python library would
    be relatively painless and fun.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you’ve learned how to combine segmentation transformations. Next,
    we’ll summarize what was covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image segmentation consists of the original image and an accompanying mask image.
    The goal is to determine whether a pixel belongs to a list of objects. For example,
    an urban photograph consists of streets, street signs, cars, trucks, bicycles,
    buildings, trees, and pedestrians. Image segmentation’s job is to decide whether
    this pixel belongs to a car, tree, or other objects.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation and image classification share the same transformations.
    In other words, most geometric transformations, such as flipping, rotating, resizing,
    cropping, and transposing, work with the original image and mask image in image
    segmentation. Photometric transformations, such as brightness, contrast, and FancyPCA,
    can technically be done with Python, but the filter does not alter the mask image.
    On the other hand, filters such as noise injection and random erasing are unsuitable
    for segmentation because they add or replace pixels in the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, there have been *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Switching gear, the next chapter will cover text augmentation. Pluto can’t use
    any image augmentation functions, but he can reuse the wrapper functions for downloading
    datasets from the *Kaggle* website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Text Augmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), *Text Augmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17990_06.xhtml#_idTextAnchor116), *Text Augmentation with Machine
    Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
