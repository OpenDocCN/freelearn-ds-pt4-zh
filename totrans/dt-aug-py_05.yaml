- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text augmentation is a technique that is used in **Natural Language Processing**
    (**NLP**) to generate additional data by modifying or creating new text from existing
    text data. Text augmentation involves techniques such as character swapping, noise
    injection, synonym replacement, word deletion, word insertion, and word swapping.
    Image and text augmentation have the same goal. They strive to increase the size
    of the training dataset and improve AI prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Text augmentation is relatively more challenging to evaluate because it is not
    as intuitive as image augmentation. The intent of an image augmentation technique
    is clear, such as flipping a photo, but a character-swapping technique will be
    disorienting to the reader. Therefore, readers might perceive the benefits as
    subjective.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of text augmentation depends on the quality of the generated
    data and the specific NLP task being performed. It can be challenging to determine
    the appropriate *safe* level of text augmentation that is required for a given
    dataset, and it often requires experimentation and testing to achieve the desired
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Customer feedback or social media chatter is fair game for text augmentation
    because the writing is messy and, predominantly, contains grammatical errors.
    Conversely, legal documents or written medical communications, such as doctor’s
    prescriptions or reports, are off-limits because the message is precise. In other
    words, error injections, synonyms, or even AI-generated text might change the
    legal or medical meaning beyond the *safe* level.
  prefs: []
  type: TYPE_NORMAL
- en: The biases in text augmentation are equally difficult to discern. For example,
    adding noise by purposely misspelling words using the Keyboard augmentation method
    might introduce bias against real-world tweets, which typically contain misspelled
    words. There are no generalized rules to follow, and the answer only becomes evident
    after thoroughly studying the data and reviewing the AI forecasting objective.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: As generative AI becomes more widely available, you can use **OpenAI’s GPT-3**,
    **Google Bard**, or **Facebook’s Roberta** system to generate original articles
    for text augmentation. For example, you can ask generative AI to create positive
    or negative reviews about a company product, then use the AI-written articles
    to train predictive AI on sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), you will learn about text
    augmentation and how to code the methods in Python notebooks. In particular, we
    will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Character augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence and flow augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text augmentation libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world text datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcing learning through Python Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started with the simplest topic, character augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Character augmenting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Character augmentation substitutes or injects characters into the text. In other
    words, it creates typing errors. Therefore, the method seems counterintuitive.
    Still, just like noise injection in image augmentation, scholarly published papers
    illustrate the benefit of character augmentation in improving AI forecasting accuracy,
    such as *Effective Character-Augmented Word Embedding for Machine Reading Comprehension*
    by *Zhuosheng Zhang, Yafang Huang, Pengfei Zhu, and Hai Zhao*, from the 2018 *CCF
    International Conference on Natural* *Language Processing*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three standard methods for character augmentation are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Optical Character Recognition (OCR) augmenting** function substitutes
    frequent errors in OCR by converting images into text, such as the letter *o*
    into the number *0* or the capital letter *I* into the number *1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Keyboard augmenting** method replaces a character with other characters
    that are adjacent to it. For example, a typical typing error for character *b*
    is hitting key *v* or key *n* instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Random character** function randomly swaps, inserts, or deletes characters
    within the piece of text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer encoding text was very different from 1963 to 1970; for example, a
    computer would encode the letter *A* as an integer 64 or Hexidecimal 41\. This
    originated from the **American National Standards Institute** (**ANSI**) in 1964,
    and the **International Organization for Standardization** (**ISO**) adopted the
    standard around 1970\. In 1980, the **Unification Code** (**Unicode**) subsumed
    the ISO standard for all international languages. However, if you come across
    computer text from around 1964, it could be encoded in **Extended Binary Coded
    Decimal Interchange Code** (**EBCDIC**), which encodes the letter *A* as 193 or
    Hexidecimal C1\. As a programmer, you might have to answer this question: *does
    your website or mobile app* *support Unicode?*'
  prefs: []
  type: TYPE_NORMAL
- en: After character augmenting, the next category is word augmenting.
  prefs: []
  type: TYPE_NORMAL
- en: Word augmenting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word augmentations carry the same bias and *safe* level warning as character
    augmentations. Over half of these augmentation methods inject errors into the
    text, but other functions generate new text using synonyms or a pretrained AI
    model. The standard word augmentation functions are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Misspell augmentation** function uses a predefined dictionary to simulate
    spelling mistakes. It is based on the scholarly paper *Text Data Augmentation
    Made Simple By Leveraging NLP Cloud APIs* by **Claude Coulombe**, which was published
    in 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Split augmentation** function splits words into two tokens randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Random word** augmentation method applies random behavior to the text
    with four parameters: **substitute**, **swap**, **delete**, and **crop**. It is
    based on two scholarly papers: *Synthetic and Natural Noise Both Break Neural
    Machine Translation* by **Yonatan Belinkov and Yonatan Bisk**, published in 2018,
    and *Data Augmentation via Dependency Tree Morphing for Low-Resource Languages*
    by **Gozde Gul Sahin and** **Mark Steedman**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Synonym augmentation** function substitutes words with synonyms from a
    predefined database. The first option is to use **WordNet**. WordNet is an extensive
    lexical database of English from *Princeton University*. The database groups nouns,
    verbs, adjectives, and adverbs into sets of cognitive synonyms. The second option
    is to use a **Paraphrase Database** (**PPDB**). A PPDB is an automatically extracted
    database containing millions of paraphrases in 16 languages. A PPDB aims to improve
    language processing by making systems more robust to language variability and
    unseen words. The entire PPDB resource is freely available under the United States
    **Creative Commons Attribution** **3.0** license.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Antonym augmentation** function replaces words with antonyms. It is based
    on the scholarly paper, *Adversarial Over-Sensitivity and Over-Stability Strategies
    for Dialogue Models*, by **Tong Niu and Mohit Bansal**, which was published in
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Reserved Word augmentation** method swaps target words where you define
    a word list. It is the same as synonyms, except the terms are created manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: can you think of a new character or word augmentation
    technique? A hint is to think about how a dyslexic person reads.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at sentence augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentence augmenting uses generative AI to create new texts. Examples of AI models
    are BERT, Roberta, GPT-2, and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three sentence augmentation methods are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contextual Word Embeddings** uses GPT-2, Distilled-GPT-2, and XLNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abstractive Summarization** uses Facebook Robertaand T5-Large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-n Similar Word** uses LAMBADA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before Pluto explains the code in the Python Notebook, let’s review the text
    augmentation libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Text augmentation libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many more Python open source image augmentation libraries than text
    augmentation libraries. Some libraries are more adaptable to a particular category
    than others, but in general, it is a good idea to pick one or two and become proficient
    in them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The well-known libraries are **Nlpaug**, **Natural Language Toolkit** (**NLTK**),
    **Generate Similar** (**Gensim**), **TextBlob**, **TextAugment**, and **AugLy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nlpaug** is a library used for textual augmentation for DL. The goal is to
    improve DL model performance by generating textual data. The GitHub link is [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLTK** is a platform used for building Python programs to work with human
    language data. It provides interfaces to over 50 corpora and lexical resources,
    such as WordNet. NLTK contains text-processing libraries for classification, tokenization,
    stemming, tagging, parsing, and semantic reasoning. The GitHub link is [https://github.com/nltk/nltk](https://github.com/nltk/nltk).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gensim** is a popular open source NLP library used for unsupervised topic
    modeling. It uses academic models and modern statistical machine learning to perform
    word vectors, corpora, topic identification, document comparison, and analyzing
    plain-text documents. The GitHub link is [https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TextBlob** is a library that is used for processing textual data. It provides
    a simple API for diving into typical NLP tasks such as part-of-speech tagging,
    noun phrase extraction, sentiment analysis, classification, and translation. The
    GitHub link is [https://github.com/sloria/TextBlob](https://github.com/sloria/TextBlob).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TextAugment** is a library that is used for augmenting text in NLP applications.
    It uses and combines the NLTK, Gensim, and TextBlob libraries. The GitHub link
    is [https://github.com/dsfsi/textaugment](https://github.com/dsfsi/textaugment).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AugLy** is a data augmentation library from Facebook that supports audio,
    image, text, and video modules and over 100 augmentations. The augmentation of
    each modality is categorized into sub-libraries. The GitHub link is [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to image augmentation wrapper functions, Pluto will write wrapper
    functions that use the library under the hood. You can pick more than one library
    for a project, but Pluto will use the **Nlpaug** library to power the wrapper
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by downloading the real-world text datasets from the *Kaggle* website.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world text datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Kaggle* website is an online community platform for data scientists and
    machine learning enthusiasts. The Kaggle website has thousands of real-world datasets;
    Pluto found a little over 2,900 **NLP** datasets and has selected two NLP datasets
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038), Pluto uses the **Netflix**
    and **Amazon** datasets as examples with which to understand biases. Pluto keeps
    the **Netflix** NLP dataset because the movie reviews are curated . There are
    a few syntactical errors, but overall, the input texts are of high quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second **NLP** dataset is **Twitter Sentiment Analysis** (**TSA**). The
    29,530 real-world tweets contain many grammatical errors and misspelled words.
    The challenge is to classify the tweets into two categories: (1) normal or (2)
    racist and sexist.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset was published in 2021 by **Mayur Dalvi**, and the license is **CC0:
    Public** **Domain**, [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the two NLP datasets, you can use the same four steps to begin
    the process of practical learning through a Python Notebook. If you need clarification,
    review *Chapters 2* and *3*. The steps are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve Python Notebook and Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download real-world data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start with Pluto.
  prefs: []
  type: TYPE_NORMAL
- en: The Python Notebook and Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading the `data_augmentation_with_python_chapter_5.ipynb` file into
    **Google Colab** or your chosen Jupyter notebook or JupyterLab environment. From
    this point onward, the code snippets are from the Python Notebook, which contains
    the complete code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to clone the repository. Pluto will reuse the code from [*Chapter
    2*](B17990_02.xhtml#_idTextAnchor038) because it has the downloading Kaggle data
    methods and not the image augmentation functions. The `!git` and `%run` statements
    are used to start up Pluto. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We need one more check to ensure Pluto has been loaded satisfactorily. The
    following command asks Pluto to say his status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be as follows or similar, depending on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, Pluto reported that he is from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    which is also known as **version 2.0**. This is what we wanted because we don’t
    need any image augmentation functions from *Chapters 3* and *4*. The next step
    is to download the real-world **Netflix** and **Twitter** NLP datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world NLP datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There has yet to be any new code written for this chapter. Pluto reuses the
    `fetch_kaggle_dataset()` method to download the **Netflix** dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `dataset-netflix-shows.zip` file is 1.34 MB, and the function automatically
    unzips in the **kaggle** directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method for fetching the Twitter dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `twitter-sentiments-analysis-nlp.zip` file is 1.23 MB, and the function
    automatically unzips in the **kaggle** directory.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenge is to search and download two additional real-world NLP datasets
    from the Kaggle website. Hint: use the `pluto.fetch_kaggle_dataset()` method.
    Pluto is an imaginary digital Siberian Husky. Therefore, he will happily fetch
    data until your disk space is full.'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to load the data into pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`fetch_df()` method. Note that `df` is a typical shorthand for the pandas **DataFrame**
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the **Netflix** data, Pluto uses the following two commands for importing
    to pandas and printing out the data batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Netflix movie descriptions](img/B17990_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Netflix movie descriptions
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: The `fetch_df()` method randomly selects several records to display in the data
    batch. The number of records, or batch size, is the `bsize` parameter. The default
    is 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: The **Netflix** movie-reviewed data is curated; therefore, it is clean. Pluto
    doesn’t have to scrub the data. However, the **Twitter** data is another story.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands for cleaning, importing, and batch-displaying the **Twitter**
    data to pandas are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Twitter tweets](img/B17990_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Twitter tweets
  prefs: []
  type: TYPE_NORMAL
- en: Since the real-world tweets from **Twitter** have been written by the public,
    they contain misspelled words, bad words, and all sorts of shenanigans.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to predict regular versus racist or sexist tweets. Pluto focuses
    on learning text argumentation; therefore, he prefers to have tweets with printable
    characters, no HTML tags, and no words of profanity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto writes two simple helper methods to clean the text and remove the words
    of profanity. The `_clean_text()` function uses the `regex` library, and the one
    line of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_clean_bad_word()` helper function uses the `filter-profanity` library,
    and the one line of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `clean_text()` method uses the two helper functions with pandas’ powerful
    `apply` function. Using pandas’ built-in functions, Pluto writes the `clean_text()`
    function with two code lines instead of a dozen lines using standard `if-else`
    and `for`-loop construct. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The commands for the clean tweets and showing the data batch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Clean Twitter tweets](img/B17990_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Clean Twitter tweets
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Who would have known that a dog and a panda could work together well? The next
    *Kung Fu Panda* movie is about **Po** and **Pluto** teaming up to defend and augment
    the city wall against the storm of the century, which has been caused by global
    warming.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use pandas and some other libraries to visualize the NLP dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing NLP data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B17990_02.xhtml#_idTextAnchor038) uses the `draw_word_count()`
    method to display the average word per record and the shortest and longest movie
    reviews. The right-hand side of the graph shows the histogram of the movie review
    word counts. The pandas library generates beautiful word count charts. Pluto reuses
    the function to display the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Netflix word counts](img/B17990_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Netflix word counts
  prefs: []
  type: TYPE_NORMAL
- en: 'The Netflix movie description mean is 23.88 words, with a minimum of 10 words
    and a maximum of 48 words. Pluto does the same for the **Twitter** NLP data, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Twitter word counts](img/B17990_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Twitter word counts
  prefs: []
  type: TYPE_NORMAL
- en: The average word count of the Twitter tweets is 12.78 words, with a minimum
    of 1 word and a maximum of 33 words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto writes the `draw_text_null_data()` method to check whether there is any
    missing data, also known as a `Missingno` library generates the graph with the
    following key line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto draws the `null` data graph for the Netflix data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Netflix missing data](img/B17990_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Netflix missing data
  prefs: []
  type: TYPE_NORMAL
- en: There is missing data in the **director**, **cast**, and **country** categories
    for the **Netflix** data, but the **description** category, also known as the
    movie review, has no missing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Twitter missing data](img/B17990_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Twitter missing data
  prefs: []
  type: TYPE_NORMAL
- en: There is no missing data in the **Twitter** data.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Many multimillion-dollar AI systems have failed primarily because of a lack
    of control over the input data. For example, the *Amazon Recruiting* system in
    2020 failed because there was no diversity in the dataset, and the most egregious
    debacle was *Microsoft’s Chatbot Tay* in 2016\. It was corrupted by Twitter users
    inputting sexist and racist tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chart is the word cloud infographic diagram. This is an extraordinary
    method for visualizing the NLP text. The most commonly used words are displayed
    in a large font, while the least used terms are displayed in a smaller font. The
    **WordCloud** library generates the infographic chart, and the essential code
    snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `_draw_text_wordcloud()` helper function and the `draw_text_wordcloud()`
    method to display the infographic chart for real-world **Netflix** data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Netflix word cloud, with approximately 246,819 words](img/B17990_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Netflix word cloud, with approximately 246,819 words
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the real-world **Twitter** data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Twitter    word cloud, with approximately 464,992 words](img/B17990_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Twitter word cloud, with approximately 464,992 words
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Here is a fun fact about the history of word cloud graphs. The word cloud, also
    known as a tag cloud, Wordle, or weighted list, was first used in print by **Douglas
    Coupland** in the book *Microserfs*. It was published in 1995, but not until 2004
    did the word clouds exist in digital format on the *Flickr* website. Today, word
    cloud infographics are widespread on the web and in academic papers.
  prefs: []
  type: TYPE_NORMAL
- en: So far, Pluto has discussed character, word, and sentence augmentation theories,
    chosen the **Nlpaug** text augmentation library, and downloaded the real-world
    **Netflix** and **Twitter** NLP datasets. It is time for Pluto to reinforce his
    learning by performing text augmentation with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcing learning through Python Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pluto uses the Python Notebook to reinforce our understanding of text augmentation.
    He uses the batch function to display text in batches. This works similarly to
    the batch functions for images. In other words, it randomly selects new records
    and transforms them using the augmentation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto recommends running the batch functions repeatedly to gain a deeper insight
    into the text augmentation methods. There are thousands of text records in the
    **Twitter** and **Amazon** datasets. Each time you run the batch functions, it
    displays different records from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As with the image augmentation implementation, the wrapper functions use the
    **Nlpaug** library under the hood. The wrapper function allows you to focus on
    the text transformation concepts and not be distracted by the library implementation.
    You can use another text augmentation library, and the wrapper function input
    and output will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto could write one complex function that contains all the text transformation
    techniques, and it may be more efficient, but that is not the goal of this book.
    After reading this book, you can choose to rewrite or hack the Python Notebook
    to suit your style with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, Pluto uses an opening line from the book *A Tale of Two Cities*
    by **Charles Dickens** as the control text. Pluto paraphrases the text by substituting
    the commas between the phrases with periods because this makes it easier for the
    text augmentation process. The control text is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“It was the best of times. It was the worst of times. It was the age of wisdom.
    It was the age of foolishness. It was the epoch of belief. It was the epoch* *of
    incredulity.”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python Notebook covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Character augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the three character augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Character augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Character augmentation involves injecting errors into the text. The process
    is counterintuitive because it purposely adds errors to the data. In other words,
    it makes the text harder for humans to understand. In contrast, computers use
    deep learning algorithms to predict the outcome, particularly the **Convolutional
    Neural Network** (**CNN**) and the **Recurrent Neural Network** (**RNN**) algorithms.
    For example, sentiment classification for tweets does not affect by misspelled
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, Pluto will explain the following three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: OCR augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keyboard augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with OCR.
  prefs: []
  type: TYPE_NORMAL
- en: OCR augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The OCR process converts an image into a piece of text, with frequent errors
    such as mixing *0* and *o* during the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto writes the `_print_aug_batch()` helper function to randomly select sample
    records from the NLP data, apply the text augmenting method, and print it using
    pandas. The input or method definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, `df` is the pandas DataFrame, `aug_function` is the augmentation method
    from the wrapper function, `col_dest` is the chosen column destination, `bsize`
    is the number of samples in the batch with a default of three, and `title` is
    the optional title for the chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OCR wrapper function is elementary. The two lines of code are the `aug_func`)
    and the helper function. The entire code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_ocr()` method with the **Netflix** data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Netflix OCR augmenting](img/B17990_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Netflix OCR augmenting
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5**.10*, the first line is **Dickens’** control text, with the augmented
    text on the left-hand side and the original text on the right-hand side. The following
    three rows are randomly sampled from the **Netflix** NLP data. Pluto recommends
    that you read the left-hand augmented text first. Stop and try to decipher the
    meaning before reading the original text.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto recommends repeatedly running the `print_aug_ocr()` method to see other
    movie descriptions. You can increase `bsize` to see more than two records at a
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Twitter OCR augmenting](img/B17990_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Twitter OCR augmenting
  prefs: []
  type: TYPE_NORMAL
- en: Next, Pluto moves on from the OCR method to the keyboard technique.
  prefs: []
  type: TYPE_NORMAL
- en: Keyboard augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The keyboard augmenting method replaces a character with a close-distance key
    on a keyboard. For example, a typical typing error for character *b* is using
    key *v* or key *n*. The augmentation variable defines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_keyboard()` wrapper function with the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Netflix keyboard augmenting](img/B17990_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Netflix keyboard augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Twitter keyboard augmenting](img/B17990_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Twitter keyboard augmenting
  prefs: []
  type: TYPE_NORMAL
- en: The last of the three text augmentation methods is the random technique.
  prefs: []
  type: TYPE_NORMAL
- en: Random augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The random character function randomly swaps, inserts, or deletes characters
    in the text. The four modes for the random process are **inserting**, **deleting**,
    **substituting**, and **swapping**. The augmentation variable defines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `insert`
    in the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Netflix random insert augmenting](img/B17990_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Netflix random insert augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Twitter random insert augmenting](img/B17990_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Twitter random insert augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `delete`
    for the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Netflix random delete augmenting](img/B17990_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Netflix random delete augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the Twitter NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Twitter random delete augmenting](img/B17990_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Twitter random delete augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `print_aug_random()` wrapper function with `actio`n set to `substitute`
    for the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Netflix random substitute augmenting](img/B17990_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Netflix random substitute augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Twitter random substitute augmenting](img/B17990_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Twitter random substitute augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `print_aug_random()` wrapper function with `action` set to `swap`
    for the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Netflix random swap augmenting](img/B17990_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Netflix random swap augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Twitter random swap augmenting](img/B17990_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Twitter random swap augmenting
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: if the input text contains misspelled words and
    bad grammar, such as tweets, could correcting the spelling and grammar be a valid
    augmentation method?'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has covered the **OCR**, **Keyboard**, and four modes of **Random** character
    augmentation techniques. The next step is augmenting words.
  prefs: []
  type: TYPE_NORMAL
- en: Word augmenting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point in the book, Pluto might think text augmentation is effortless,
    and it is true. We built a solid foundation layer in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    with an object-oriented class and learned how to extend the object as we learned
    about new augmentation techniques. In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    Pluto added the functions for downloading any *Kaggle* real-world dataset, and
    *Chapters 3* and *4* gave us the wrapper function pattern. Therefore, at this
    point, Pluto reuses the methods and patterns to make the Python code concise and
    easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: The word augmentation process is similar to character augmentation. Pluto uses
    the same `_print_aug_batch()` helper method. In particular, Pluto will cover the
    **Misspell**, **Split**, **Random**, **Synonyms**, **Antonyms**, and **Reserved**
    word augmenting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the misspell augmentation technique.
  prefs: []
  type: TYPE_NORMAL
- en: Misspell augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The misspell augmentation function uses a predefined dictionary to simulate
    spelling mistakes. The augmentation variable defines this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_misspell()` wrapper function on the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Netflix misspell word augmenting](img/B17990_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – Netflix misspell word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Twitter misspell word augmenting](img/B17990_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – Twitter misspell word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: Similar to **Misspell** is the **Split** word augmentation technique.
  prefs: []
  type: TYPE_NORMAL
- en: Split augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The split augmentation function randomly splits words into two tokens. The
    augmentation variable defines this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_split()` wrapper function on the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Netflix split word augmenting](img/B17990_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – Netflix split word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Twitter split word augmenting](img/B17990_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – Twitter split word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: After the split word method, Pluto presents the random word augmenting method.
  prefs: []
  type: TYPE_NORMAL
- en: Random augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The random word augmentation method applies random behavior to the text with
    four parameters: **Swap**, **Crop**, **Substitute**, or **Delete**. The augmentation
    variable defines this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_random()` wrapper function for swapping mode
    on the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Netflix random swap word augmenting](img/B17990_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – Netflix random swap word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Twitter random swap word augmenting](img/B17990_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Twitter random swap word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `print_aug_word_random()` wrapper function for cropping mode
    on the **Netflix** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Netflix random crop word augmenting](img/B17990_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – Netflix random crop word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Twitter random crop word augmenting](img/B17990_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – Twitter random crop word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: So, Pluto has described the **Swap** and **Crop** word augmentation methods
    but not the **Substitute** and **Delete** ones. This is because they are similar
    to the character augmenting functions and are in the Python Notebook. Next on
    the block is synonym augmenting.
  prefs: []
  type: TYPE_NORMAL
- en: Synonym augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The synonym augmentation function substitutes words with synonyms from a predefined
    database. **WordNet** and **PPBD** are two optional databases. The augmentation
    variable defines this process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_synonym()` wrapper function on the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Netflix synonym word augmenting](img/B17990_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – Netflix synonym word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'It is interesting and funny that the synonym of *It* is **Information Technology**
    for the control text. Mr. Dickens, who wrote Tale of Two Cities in 1859, could
    never have known that IT is a popular acronym for information technology. Pluto
    does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – Twitter synonym word augmenting](img/B17990_05_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – Twitter synonym word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: When there are synonyms, you will also find antonyms.
  prefs: []
  type: TYPE_NORMAL
- en: Antonym augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The antonym augmentation function randomly replaces words with antonyms. The
    augmentation variable defines this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_antonym()` wrapper function on the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – Netflix antonym word augmenting](img/B17990_05_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 – Netflix antonym word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – Twitter antonym word augmenting](img/B17990_05_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.33 – Twitter antonym word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: After synonyms and antonyms, which are automated, reserved word augmentation
    requires a manual word list.
  prefs: []
  type: TYPE_NORMAL
- en: Reserved word augmenting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The reserved word augmentation method swaps target words where you define a
    word list. It is the same as synonyms, except the terms are created manually.
    Pluto uses the **Netflix** and **Twitter** word cloud diagrams, *Figures 5.8*
    and *5.9*, to select the top three reoccurring words in the NLP datasets. The
    augmentation variable defines this process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto uses the `print_aug_word_reserved()` wrapper function on the **Netflix**
    NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – Netflix reserved word augmenting](img/B17990_05_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 – Netflix reserved word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the words **wisdom** and **foolishness** are substituted with **Intelligence**
    and **idiocy**, **life** with **existance**, and **family** with **brood**. Pluto
    does the same for the **Twitter** NLP data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.35 – Twitter reserved word augmenting](img/B17990_05_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 – Twitter reserved word augmenting
  prefs: []
  type: TYPE_NORMAL
- en: Notice the words **wisdom** and **foolishness** are substituted with **sagacity**
    and **idiocy**, and **user** with **people** and **customer**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reserved Word** augmenting is the last word augmentation method of this chapter.
    Pluto has covered **Misspell**, **Split**, **Random**, **Synonym**, **Antonym**,
    and **Reserved Word** augmentation, but these are only some of the possible word
    augmentation techniques you can use.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is to use the Augly library or the NLTK, Gensim, or Textblob libraries
    to write a new wrapper function. It is relatively easy. The first step is to copy
    a wrapper function, such as the `print_aug_keyboard()` function. The second and
    last step is to replace `aug_func = nlpaug.augmenter.char.KeyboardAug()` with
    `aug_func = augly.text.functional.simulate_typos()`. There are more parameters
    in the Augly function. A hint is to use the `augly.text.functional.simulate_typos?`
    command to display the function documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The **Nlpaug** library and other text augmentation libraries, such as **NLTK**,
    **Gensim**, **Textblob**, and **Augly**, have additional text augmentation methods.
    In addition, newly published scholarly papers are an excellent source in which
    to discover new text augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first glance, text augmentation seems counterintuitive and problematic because
    the techniques inject errors into the text. Still, DL based on CNNs or RNNs recognizes
    patterns regardless of a few misspellings or synonym replacements. Furthermore,
    many published scholarly papers have described the benefits of text augmentation
    to increase prediction or forecast accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), you learned about three
    **Character** augmentation techniques, **OCR, Keyboard**, and **Random**. In addition,
    the six **Word** augmentation techniques are the **Misspell**, **Split**, **Random**,
    **Synonyms**, **Antonyms**, and **Reserved** words. There are more text augmentation
    methods in the Nlgaug, NLTK, Gensim, TextBlob, and Augly libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the text augmentation methods using a Python Notebook is deceptively
    simple. This is because Pluto built a solid foundation layer in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    with an object-oriented class and learned how to extend the object with **decorator**
    as he discovered new augmentation techniques. In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038),
    Pluto added the functions for downloading any *Kaggle* real-world dataset, and
    *Chapters 3* and *4* gave us the wrapper function pattern. Therefore, in this
    chapter, Pluto reused the methods and patterns to make the Python Notebook code
    concise and easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, there are *Fun facts* and *Fun challenges*. Pluto hopes
    you will take advantage of them and expand your experience beyond the scope of
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will delve deeper into text augmentation using machine learning
    methods. Ironically, the goal of text augmentation is to make machine learning
    and DL predict and forecast accurately, and we will use the same AI system to
    increase the efficiency of text augmentation. It is a circular logic or cyclical
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto is waiting for you in the next chapter, *Text Augmentation with* *Machine
    Learning*.
  prefs: []
  type: TYPE_NORMAL
