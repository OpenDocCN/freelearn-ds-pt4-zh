<html><head></head><body>
		<div id="_idContainer171">
			<h1 id="_idParaDest-116" class="chapter-number"><a id="_idTextAnchor116"/>6</h1>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>Text Augmentation with Machine Learning</h1>
			<p>Text augmentation with <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) is an advanced technique compared to the standard text augmenting methods we covered in the previous chapter. Ironically, text augmentation aims to improve ML model accuracy, but we used a pre-trained ML model to create additional training NLP data. It’s a circular process. ML coding is not in this book’s scope, but understanding the difference between using libraries and ML for text augmentation <span class="No-Break">is beneficial.</span></p>
			<p>Augmentation libraries, whether for image, text, or audio, follow the traditional programming methodologies with structure data, loops, and conditional statements in the algorithm. For example, as shown in <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, the pseudocode for implementing the <strong class="source-inline">_print_aug_reserved()</strong> method could be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define synonym words, pseudo-code</strong>
reserved = [['happy', 'joyful', 'cheerful'],
  ['sad', 'sorrowful', 'regretful']]
<strong class="bold"># substitute the word with its synonym, pseudo-code</strong>
for i, word in (input_text)
  for set_word in (reserved)
    for i, syn in set_word
      if (syn == word)
        input_text[i] = set_word[i+1]</pre>
			<p>The happy path code does not cover error checking, but the salient point is that the library’s function follows the standard sequential <span class="No-Break">coding method.</span></p>
			<p>On the other hand, ML is based on one of the 13 known ML algorithms, including <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) (or <strong class="bold">artificial neural networks</strong>), <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), <strong class="bold">linear regression</strong>, <strong class="bold">random forest</strong>, <strong class="bold">naive Bayes</strong>, and <strong class="bold">gradient boosting</strong>. The key to ML is that the system <em class="italic">learns</em> and not programs. DL uses the Universal Approximation theory, gradient descent, transfer learning, and hundreds of other techniques. ANNs have millions to billions of neural nodes – for example, OpenAI GPT3 has 96 layers and 175 billion nodes. The central point is that ML has no familiarity with the <strong class="source-inline">_print_aug_reserved()</strong> <span class="No-Break">pseudocode algorithm.</span></p>
			<p>The following is a representation of a DL architecture for image classification. It illustrates the difference between a procedural approach and the Neural Network algorithm. This figure was created from <strong class="bold">Latex</strong> and the <strong class="bold">Overleaf</strong> cloud system. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B17990_06_01.jpg" alt="Figure 6.1 – Representation of a DL model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Representation of a DL model</p>
			<p>The Overleaf project and its code are from Mr. Duc Haba’s public repository, and the URL is <a href="https://www.overleaf.com/project/6369a1eaba583e7cd423171b">https://www.overleaf.com/project/6369a1eaba583e7cd423171b</a>. You can clone and hack the code to display other <span class="No-Break">AI models.</span></p>
			<p>This chapter will cover text augmentation with ML, and in particular, the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Machine <span class="No-Break">learning models</span></li>
				<li><span class="No-Break">Word augmenting</span></li>
				<li><span class="No-Break">Sentence augmenting</span></li>
				<li>Real-world <span class="No-Break">NLP datasets</span></li>
				<li>Reinforcing your learning through the <span class="No-Break">Python Notebook</span></li>
			</ul>
			<p>Let’s briefly describe the ML models used in the Python wrapper <span class="No-Break">function code.</span></p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>Machine learning models</h1>
			<p>In this chapter, the text augmentation<a id="_idIndexMarker566"/> wrapper functions use ML to generate new text for training the ML model. Understanding how these models are built is not in scope, but a brief description of these ML models and their algorithms is necessary. The Python wrapper functions will use the following ML models under <span class="No-Break">the hood:</span></p>
			<ul>
				<li>Tomáš Mikolov published the NLP algorithm<a id="_idIndexMarker567"/> using a neural network named <strong class="bold">Word2Vec</strong> in 2013. The model can propose synonym words from the <span class="No-Break">input text.</span></li>
				<li>The <strong class="bold">Global Vectors for Word Representation</strong> (<strong class="bold">GloVe</strong>) algorithm was created by Jeffrey<a id="_idIndexMarker568"/> Pennington, Richard Socher, and Christopher D. Manning in 2014. It is an unsupervised learning NLP algorithm for representing words in vector format. The results are a linear algorithm that groups the closest <span class="No-Break">neighboring words.</span></li>
				<li><strong class="bold">Wiki-news-300d-1M</strong> is a pre-trained ML model that uses the <strong class="bold">fastText</strong> open source library. It was trained<a id="_idIndexMarker569"/> on 1 million words<a id="_idIndexMarker570"/> from Wikipedia 2017 articles, the UMBC WebBase corpus, which consists of over 3 billion words, and the Statmt.org news dataset, which consists of over 16 billion tokens. T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin introduced <strong class="bold">Wiki-news-300d-1M</strong> in their <em class="italic">Advances in Pre-Training Distributed Word Representations</em> paper. The license is the Creative Commons Attribution-Share-Alike <span class="No-Break">License 3.0.</span></li>
				<li><strong class="bold">GoogleNews-vectors-negative300</strong> is a pre-trained <strong class="bold">Word2Vec</strong> model that uses the Google News<a id="_idIndexMarker571"/> dataset, which contains<a id="_idIndexMarker572"/> about 100 billion words and <span class="No-Break">300 dimensions.</span></li>
				<li>Google introduced the <strong class="bold">transformer</strong> neural network algorithm<a id="_idIndexMarker573"/> in 2017. Recent cutting-edge breakthroughs in NLP and computer vision are from the <span class="No-Break">transformer model.</span></li>
				<li>The <strong class="bold">BERT</strong> model was introduced by Jacob Devlin, Ming-Wei Chang, Kenton<a id="_idIndexMarker574"/> Lee, and Kristina Toutanova in 2018. It is specialized in language inference <span class="No-Break">and prediction.</span></li>
				<li><strong class="bold">RoBERTa</strong> is an optimized algorithm for the self-supervised <a id="_idIndexMarker575"/>NLP model. It is a model built on top of BERT. It excels in performance on many NLP inferences. Meta AI published RoBERTa <span class="No-Break">in 2019.</span></li>
				<li>Facebook’s <strong class="bold">wmt19-en-de</strong> and <strong class="bold">wmt19-de-en</strong> are pre-trained NLP models from <em class="italic">HuggingFace</em> for translating<a id="_idIndexMarker576"/> from English<a id="_idIndexMarker577"/> to German (Deutsch) and back. It was made publicly available <span class="No-Break">in 2021.</span></li>
				<li>Facebook’s <strong class="bold">wmt19-en-ru</strong> and <strong class="bold">wmt19-ru-en</strong> are pre-trained NLP models from <em class="italic">HuggingFace</em> for translating<a id="_idIndexMarker578"/> from English<a id="_idIndexMarker579"/> to Russian (Русский) and back. It was made publicly available <span class="No-Break">in 2021.</span></li>
				<li><strong class="bold">XLNet</strong> is a transformer-XL pre-trained model<a id="_idIndexMarker580"/> that was made publicly available by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, and Quoc V. Le on <em class="italic">HuggingFace</em> in 2021. It was published in the scholarly paper <em class="italic">XLNet: Generalized Autoregressive Pretraining for </em><span class="No-Break"><em class="italic">Language Understanding</em></span><span class="No-Break">.</span></li>
				<li>The <strong class="bold">Generative Pre-trained Transformer 2</strong> (<strong class="bold">GPT-2</strong>) algorithm is an open source AI that was published<a id="_idIndexMarker581"/> by OpenAI in 2019. The model excels in writing feedback questions and answers<a id="_idIndexMarker582"/> and generating text summarization of an article. It is at the level of actual <span class="No-Break">human writing.</span></li>
				<li>The <strong class="bold">T5</strong> and <strong class="bold">T5X</strong> models use the text-to-text<a id="_idIndexMarker583"/> transformer<a id="_idIndexMarker584"/> algorithm. They were trained on a massive corpus. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu introduced T5 in their paper <em class="italic">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em> <span class="No-Break">in 2020.</span></li>
			</ul>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Generative AI, when using a transformer model, such as OpenAI’s GPT-3, GPT-4, or Google Bard, can write as well or better than a <span class="No-Break">human writer.</span></p>
			<p>Now that we know about some<a id="_idIndexMarker585"/> of the ML models, let’s see which augmenting function uses which <span class="No-Break">ML models.</span></p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>Word augmenting</h1>
			<p>In this chapter, the word augmenting<a id="_idIndexMarker586"/> techniques are similar to the methods from <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, which used the <strong class="bold">Nlpaug</strong> library. The difference is that rather than Python libraries, the wrapper functions<a id="_idIndexMarker587"/> use powerful ML models to achieve remarkable results. Sometimes, the output or rewritten text is akin to <span class="No-Break">human writers.</span></p>
			<p>In particular, you will learn four new techniques and two variants each. Let’s start <span class="No-Break">with Word2Vec:</span></p>
			<ul>
				<li>The <strong class="bold">Word2Vec</strong> method uses the neural network NLP Word2Vec algorithm<a id="_idIndexMarker588"/> and the GoogleNews-vectors-negative300 pre-trained model. Google trained it using a large corpus containing about 100 billion words and 300 dimensions. Substitute and insert are the two <span class="No-Break">mode variants.</span></li>
				<li>The <strong class="bold">BERT</strong> method uses Google’s transformer algorithm<a id="_idIndexMarker589"/> and BERT pre-trained model. Substitute and insert are the two <span class="No-Break">mode variants.</span></li>
				<li>The <strong class="bold">RoBERTa</strong> method is a variation of the BERT model. Substitute<a id="_idIndexMarker590"/> and insert are the two <span class="No-Break">mode variants.</span></li>
				<li>The last word augmenting<a id="_idIndexMarker591"/> technique that we’ll look at in this chapter is <strong class="bold">back translation</strong> using Facebook’s (Meta’s) pre-trained translation model. It translates the input English text into a different language and back to English. The two variants we’ll look at involve translating from English into German (Deutsch) and back to English using the <strong class="bold">facebook/wmt19-en-de</strong> and <strong class="bold">facebook/wmt19-de-en</strong> models, and from English to Russian (Русский) and back to English using the <strong class="bold">facebook/wmt19-en-ru</strong> and <span class="No-Break"><strong class="bold">facebook/wmt19-ru-en</strong></span><span class="No-Break"> models.</span></li>
			</ul>
			<p>It will be easier to understand this by reading the output from the word wrapper functions, but before we do, let’s describe <span class="No-Break">sentence augmenting.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Sentence augmenting</h1>
			<p>Augmenting at the sentence level<a id="_idIndexMarker592"/> is a powerful concept. It was not possible 5 years ago. You had to be working in an ML research company or a billionaire<a id="_idIndexMarker593"/> before accessing these acclaimed pre-trained models. Some transformer and <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) became available in 2019 and 2020 as open source, but they are generally for research. Convenient access to online AI servers via a GPU was not widely available at that time. The LLM and pre-trained models have recently become publicly accessible for incorporating them into your projects, such as the HuggingFace website. The salient point is that for independent researchers or students, LLM and pre-trained models only became accessible <span class="No-Break">in mid-2021.</span></p>
			<p>The sentence and word augmenting methods<a id="_idIndexMarker594"/> that use ML can’t be done dynamically as with methods using the <strong class="bold">Nlpaug</strong> library. In other words, you have to write and save the augmented text to your local or cloud disk space. The primary reason is that the augmentation step takes too long per training cycle. The upside is that you can increase the original text by 20 to 100 times <span class="No-Break">its size.</span></p>
			<p>In particular, we will cover the <span class="No-Break">following techniques:</span></p>
			<ul>
				<li>Summarizing<a id="_idIndexMarker595"/> text using the <strong class="bold">T5</strong> <span class="No-Break">NLP algorithm.</span></li>
				<li><strong class="bold">Sequence</strong> and <strong class="bold">Sometimes</strong> are two sentence<a id="_idIndexMarker596"/> flow<a id="_idIndexMarker597"/> methods. The flow<a id="_idIndexMarker598"/> methods<a id="_idIndexMarker599"/> use a combination of the <strong class="bold">GloVe</strong> and <strong class="bold">BERT</strong> <span class="No-Break">NLP algorithms.</span></li>
			</ul>
			<p>For the sentence augmentation techniques, they are easier to understand by reading the output of the wrapper functions using real-world NLP datasets as input text. Thus, the following section is about writing wrapper functions with Python code to gain insight into sentence augmenting, but first, let’s download the real-world <span class="No-Break">NLP datasets.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Real-world NLP datasets</h1>
			<p>This chapter will use the same Netflix<a id="_idIndexMarker600"/> and Twitter real-world NLP datasets from <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. In addition, both datasets have been vetted, cleaned, and stored in the <strong class="source-inline">pluto_data</strong> directory in this book’s GitHub repository. The startup sequence is similar to the previous chapters. It is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Clone the Python Notebook <span class="No-Break">and Pluto.</span></li>
				<li><span class="No-Break">Verify Pluto.</span></li>
				<li>Locate the <span class="No-Break">NLP data.</span></li>
				<li>Load the data <span class="No-Break">into pandas.</span></li>
				<li>View <span class="No-Break">the data.</span></li>
			</ol>
			<p>Let’s start with the Python Notebook <span class="No-Break">and Pluto.</span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor122"/>Python Notebook and Pluto</h2>
			<p>Start by loading<a id="_idIndexMarker601"/> the <strong class="source-inline">data_augmentation_with_python_chapter_6.ipynb</strong> file into<a id="_idIndexMarker602"/> Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this point onward, we will only display code snippets. The complete Python code can be found in the <span class="No-Break">Python Notebook.</span></p>
			<p>The next step is to clone the repository. We will reuse the code from <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. The <strong class="source-inline">!git</strong> and <strong class="source-inline">%run</strong> statements are used to <span class="No-Break">instantiate Pluto:</span></p>
			<pre class="source-code">
<strong class="bold"># clone Packt GitHub repo.</strong>
!git clone 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'
<strong class="bold"># Instantiate Pluto</strong>
%run 'Data-Augmentation-with-Python/pluto/pluto_chapter_5.py'</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<pre class="console">
---------------------------- : ----------------------------
                        Hello from class : &lt;class '__main__.PacktDataAug'&gt; Class: PacktDataAug
                                     Code name : Pluto
                                     Author is : Duc Haba
---------------------------- : ----------------------------</pre>
			<p>The following setup step<a id="_idIndexMarker603"/> is checking if Pluto<a id="_idIndexMarker604"/> <span class="No-Break">loaded correctly.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>Verify</h2>
			<p>The following command<a id="_idIndexMarker605"/> asks Pluto<a id="_idIndexMarker606"/> to display <span class="No-Break">his status:</span></p>
			<pre class="source-code">
<strong class="bold"># Am I alive?</strong>
pluto.say_sys_info()</pre>
			<p>The output will be as follows or similar, depending on <span class="No-Break">your system:</span></p>
			<pre class="console">
---------------------------- : ----------------------------
                 System time : 2022/11/09 05:31
                    Platform : linux
     Pluto Version (Chapter) : 5.0
             Python (3.7.10) : actual: 3.7.15 (default, Oct 12 2022, 19:14:55) [GCC 7.5.0]
            PyTorch (1.11.0) : actual: 1.12.1+cu113
              Pandas (1.3.5) : actual: 1.3.5
                 PIL (9.0.0) : actual: 7.1.2
          Matplotlib (3.2.2) : actual: 3.2.2
                   CPU count : 12
                  CPU speed : NOT available
---------------------------- : ----------------------------</pre>
			<p>Pluto showed that he is from <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> (version 5.0), which is correct. In addition, the cleaned NLP Twitter<a id="_idIndexMarker607"/> and Netflix datasets<a id="_idIndexMarker608"/> are in the <strong class="source-inline">~/</strong><span class="No-Break"><strong class="source-inline">Data-Augmentation-with-Python/pluto_data</strong></span><span class="No-Break"> directory.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>Real-world NLP data</h2>
			<p>Pluto is using the clean versions of the data without profanity from <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. They are the Netflix and Twitter NLP datasets<a id="_idIndexMarker609"/> from the Kaggle website. The clean datasets were saved in this book’s GitHub repository. Thus, Pluto does not need to download them again. Still, you can download them or other real-world datasets by using the <strong class="source-inline">fetch_kaggle_dataset()</strong> function. Pluto locates the cleaned NLP datasets with the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># check to see the files are there</strong>
f = 'Data-Augmentation-with-Python/pluto_data'
!ls -la {f}</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<pre class="console">
drwxr-xr-x 2 root root    4096 Nov 13 06:07 .
drwxr-xr-x 6 root root    4096 Nov 13 06:07 ..
-rw-r--r-- 1 root root 3423079 Nov 13 06:07 netflix_data.csv
-rw-r--r-- 1 root root 6072376 Nov 13 06:07 twitter_data.csv</pre>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto gets lazy, and instead of using a Python library and coding it in Python, he cheats by dropping down to the Linux Bash command-line code. The exclamation character (<strong class="source-inline">!</strong>) allows the Python Notebook to backdoor the kernel, such as via <strong class="source-inline">!ls -la</strong> on Linux or<strong class="source-inline">!dir</strong> on Windows. You can use any OS command-line code. Still, it is not portable code because the commands for Windows, iOS, Linux, Android, and other OSs that support web browsers such as Safari, Chrome, Edge, and Firefox <span class="No-Break">are different.</span></p>
			<p>The next step is to load the data into Pluto’s <span class="No-Break">buddy, pandas.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor125"/>Pandas</h2>
			<p>Pluto reuses the <strong class="source-inline">fetch_df()</strong> method from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> to load<a id="_idIndexMarker610"/> the data into<a id="_idIndexMarker611"/> pandas. The following commands import the real-world Netflix data <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># import to Pandas</strong>
f = 'Data-Augmentation-with-Python/pluto_data/netflix_data.csv'
pluto.df_netflix_data = pluto.fetch_df(f,sep='~')</pre>
			<p>Similarly, the commands for loading the real-world Twitter data are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># import to Pandas</strong>
f = 'Data-Augmentation-with-Python/pluto_data/twitter_data.csv'
pluto.df_twitter_data = pluto.fetch_df(f,sep='~')</pre>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to find and download two additional NLP data from the Kaggle website. Hint: use Pluto’s <strong class="source-inline">fetch_kaggle_dataset()</strong> function. Import it into pandas using the <span class="No-Break"><strong class="source-inline">fetch_df()</strong></span><span class="No-Break"> function.</span></p>
			<p>Now that Pluto has located and imported the data into pandas, the last step in loading the data sequence is to view and verify <span class="No-Break">the data.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>Viewing the text</h2>
			<p>The <strong class="source-inline">draw_word_count()</strong> and <strong class="source-inline">draw_null_data()</strong> methods help<a id="_idIndexMarker612"/> us understand the NLP data, and Pluto recommends revisiting <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> to view those Netflix and Twitter graphs. A more colorful and fun method is to use the <span class="No-Break"><strong class="source-inline">draw_word_cloud()</strong></span><span class="No-Break"> function.</span></p>
			<p>Pluto draws the Netflix word cloud infographic graph with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># draw infographic plot</strong>
pluto.draw_text_wordcloud(pluto.df_netflix_data.description,
  xignore_words=wordcloud.STOPWORDS,
  title='Word Cloud: Netflix Movie Review')</pre>
			<p>The output<a id="_idIndexMarker613"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17990_06_02.jpg" alt="Figure 6.2– Netflix word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2– Netflix word cloud</p>
			<p>Similarly, Pluto displays the Twitter word cloud using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># draw infographic plot</strong>
pluto.draw_text_wordcloud(pluto.df_twitter_data.clean_tweet,
  xignore_words=wordcloud.STOPWORDS,
  title='Word Cloud: Twitter Tweets')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17990_06_03.jpg" alt="Figure 6.3 – Twitter word cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Twitter word cloud</p>
			<p>Along with the real-world NLP data, Pluto uses the first few lines of the <em class="italic">Tale of Two Cities</em>, by Charles Dickens, as the control text. In this chapter, Pluto will extend the control text to the first page of Mr. Dickens’ book, the <em class="italic">Moby Dick</em> book, by Melville, and the <em class="italic">Alice in Wonderland</em> book, by Carroll. These books are in public domain, as defined in <span class="No-Break">Project Gutenberg.</span></p>
			<p>The varibles <a id="_idIndexMarker614"/>are <strong class="source-inline">pluto.orig_text</strong>, <strong class="source-inline">pluto.orig_dickens_page</strong>, <strong class="source-inline">pluto.orig_melville_page</strong>, and <span class="No-Break"><strong class="source-inline">pluto.orig_carroll_page</strong></span><span class="No-Break">, respectively.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">ML is good at altering text in typical human writing but modifying the masterworks is borderline criminal. Pluto seeks only to illustrate the augmentation concepts and never to bastardize the classics. It is in the name <span class="No-Break">of science.</span></p>
			<p>You have loaded the Python Notebook, instantiated Pluto, accessed the cleaned NLP real-world data, and verified it with the word cloud infographic. Now, it is time to write and hack Python code to gain a deeper insight into word and sentence augmentation <span class="No-Break">with ML.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor127"/>Reinforcing your learning through the Python Notebook</h1>
			<p>Even though NLP ML is highly complex, the implementation<a id="_idIndexMarker615"/> for the wrapper code is deceptively simple. This is because of Pluto’s structured object-oriented approach. First, we created a base class for Pluto in <a href="B17990_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and used the decorator to add a new method as we learned new augmentation concepts. In <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, Pluto learned to download any of the thousands of real-world datasets from the Kaggle website. <em class="italic">Chapters 3</em> and <em class="italic">4</em> introduced the wrapper functions process using powerful open source libraries under the hood. Finally, <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> put forward the text augmentation<a id="_idIndexMarker616"/> concepts and methods when using the <span class="No-Break"><strong class="bold">Nlpaug</strong></span><span class="No-Break"> library.</span></p>
			<p>Therefore, building upon our previous knowledge, the wrapper functions in this chapter use the powerful NLP ML pre-trained model to perform <span class="No-Break">the augmentations.</span></p>
			<p>In particular, this chapter will present wrapper functions and the augmenting results for the Netflix and Twitter real-world datasets using the <span class="No-Break">following techniques:</span></p>
			<ul>
				<li><strong class="bold">Word2Vec</strong> <span class="No-Break">word augmenting</span></li>
				<li><strong class="bold">BERT</strong> and <strong class="bold">Transformer</strong> <span class="No-Break">word augmenting</span></li>
				<li><span class="No-Break"><strong class="bold">RoBERTa</strong></span><span class="No-Break"> augmenting</span></li>
				<li><span class="No-Break"><strong class="bold">Back translation</strong></span></li>
				<li><span class="No-Break"><strong class="bold">T5</strong></span><span class="No-Break"> augmenting</span></li>
				<li><strong class="bold">Sequential</strong> and <span class="No-Break"><strong class="bold">Sometime</strong></span><span class="No-Break"> augmenting</span></li>
			</ul>
			<p>Let’s start <span class="No-Break">with Word2Vec.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor128"/>Word2Vec word augmenting</h2>
			<p>The <strong class="source-inline">print_aug_ai_word2vec()</strong> wrapper function’s key parameters<a id="_idIndexMarker617"/> are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for print_aug_ai_word2vec()</strong>
model_type = 'word2vec',
model_path = 'GoogleNews-vectors-negative300.bin'
action = 'insert'    # <strong class="bold">or 'substitute'</strong>
nlpaug.augmenter.word.WordEmbsAug(model_type,
  model_path,
  action)</pre>
			<p>The full functions<a id="_idIndexMarker618"/> can be found in the Python Notebook. Pluto uses the real-world NLP Netflix data to test the function, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using word2vec</strong>
pluto.print_aug_ai_word2vec(pluto.df_netflix_data,
  col_dest='description',
  action='insert',
  aug_name='Word2Vec-GoogleNews Word Embedding Augment')</pre>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">When you run a wrapper function, new data is randomly selected and processed. Thus, it would be best if you run the wrapper function repeatedly to see different movie reviews from the Netflix dataset or tweets from the <span class="No-Break">Twitter dataset.</span></p>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B17990_06_04.jpg" alt="Figure 6.4 – Word2Vec using insert mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Word2Vec using insert mode on the Netflix data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em>, the first row is the control<a id="_idIndexMarker619"/> input. It is a quote from the book <em class="italic">A Tale of Two Cities</em>. You will find that the augmented effects are easier to spot by comparing the control text with the text in the datasets. In addition, the control text is needed to compare the differences between <span class="No-Break">augmentation techniques.</span></p>
			<p>Pluto found the injection of names on <em class="italic">row #1</em>, such as <strong class="bold">Punta</strong> (a believable Spanish writer name) and <strong class="bold">Poydras</strong>, as actual names and plausible additions to this celebrity movie review context. It was not factual in the movie, but it is acceptable for text augmentation for movie <span class="No-Break">sentiment prediction.</span></p>
			<p>On <em class="italic">row #2</em>, the words <strong class="bold">blending</strong>, <strong class="bold">dangerous</strong>, and <strong class="bold">original 1960s</strong> add flare to the movie description without altering the intent of the spy <span class="No-Break">movie’s description.</span></p>
			<p>On <em class="italic">row #3</em>, the addition of names, such as <strong class="bold">Kent of Cabus</strong> (Kent from a village in English named Cabus), <strong class="bold">Rangjung</strong> (a village in Bhutan, served as a possible hero name), and <strong class="bold">Elizabeth</strong> (as the villain) in the comic Green Arrow movie description is 100% plausible plot for <span class="No-Break">our superhero.</span></p>
			<p>Overall, Pluto is flabbergasted by the <strong class="bold">Word2Vec</strong> ML model. The word and name injections are contexts<a id="_idIndexMarker620"/> that are appropriate as if a human writer were creating them. However, the control text from Dickens is funny to read, and it is not ML’s fault. The system does not know that the book was written in the 1800s and has only the first few lines of the text to go off. The movie review is a complete thought, while the control text is a tiny fragment of <span class="No-Break">the whole.</span></p>
			<p>Pluto runs a similar command on the real-world Twitter data, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using word2vec</strong>
pluto.print_aug_ai_word2vec(pluto.df_twitter_data,
  col_dest='clean_tweet',
  action='insert',
  aug_name='Word2Vec-GoogleNews Word Embedding Augment')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B17990_06_05.jpg" alt="Figure 6.5 – Word2Vec using insert mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Word2Vec using insert mode on the Twitter data</p>
			<p>Since tweets are like random thoughts<a id="_idIndexMarker621"/> written without forethoughts or editing, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em>, the <strong class="bold">Word2Vec</strong> injections are like a bored high school student doing homework while playing a computer game. Pluto can’t judge if the altered text is plausible or not. Would it increase or decrease the AI prediction accuracy for <span class="No-Break">sentiment analysis?</span></p>
			<p>For Dickens’ control text, Pluto flinched. It was dreadful, but he promised the AI would be better in the later model when using transformers and <span class="No-Break">generative AI.</span></p>
			<p>Now that we’ve looked at <strong class="bold">insert</strong> mode, let’s see how the <strong class="bold">Word2Vec</strong> model performs in <span class="No-Break"><strong class="bold">substitute</strong></span><span class="No-Break"> mode.</span></p>
			<h3>Substitute</h3>
			<p><strong class="bold">Substitute</strong> mode replaces words and then<a id="_idIndexMarker622"/> adds words to the sentence. Pluto applies the <strong class="bold">Word2Vec</strong> model using <strong class="bold">substitute</strong> mode to the Netflix data <span class="No-Break">like so:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using word2vec</strong>
pluto.print_aug_ai_word2vec(pluto.df_netflix_data,
  col_dest='description',
  action='substitute',
  aug_name='Word2Vec-GoogleNews Word Embedding Augment')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17990_06_06.jpg" alt="Figure 6.6 – Word2Vec using substitute mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Word2Vec using substitute mode on the Netflix data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em>, <em class="italic">row #0</em> is the control text, and on <em class="italic">row #1</em>, <strong class="bold">zany adventure</strong> is suitable for a kid adventure movie, but <strong class="bold">liquid viagra</strong> is definitely off <span class="No-Break">the mark.</span></p>
			<p>On <em class="italic">row #2</em>, replacing <strong class="bold">police</strong> with <strong class="bold">troopers</strong>, <strong class="bold">job</strong> with <strong class="bold">plum assignment</strong>, <strong class="bold">wrest</strong> with <strong class="bold">must take</strong>, <strong class="bold">figure</strong> with <strong class="bold">hand</strong>, and <strong class="bold">offenders</strong> with <strong class="bold">criminals</strong> are suitable<a id="_idIndexMarker623"/> in the police movie. Thus, the <strong class="bold">Word2Vec</strong> model did a proper <span class="No-Break">augmentation job.</span></p>
			<p>On <em class="italic">row #3</em>, replacing <strong class="bold">cinematic distillation</strong> with <strong class="bold">Scorcese decaffeination</strong> is an intriguing choice worthy of a human writer. Changing <strong class="bold">electrifying</strong> to <strong class="bold">sparkling</strong> is clever because electricity can spark. Substituting <strong class="bold">shadowy</strong> with <strong class="bold">clandestine</strong> is a good choice, but switching <strong class="bold">seven</strong> with <strong class="bold">five</strong> <span class="No-Break">is unnecessary.</span></p>
			<p>Once again, the <strong class="bold">Word2Vec</strong> model could have done better for the <span class="No-Break">control text.</span></p>
			<p>Pluto does the same to the Twitter data with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using word2vec</strong>
pluto.print_aug_ai_word2vec(pluto.df_twitter_data,
  col_dest='clean_tweet',
  action='substitute',
  aug_name='Word2Vec-GoogleNews Word Embedding Augment')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17990_06_07.jpg" alt="Figure 6.7 – Word2Vec using substitute mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Word2Vec using substitute mode on the Twitter data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.6</em>, the tweets are chaotic, and many are incomplete thoughts. The <strong class="bold">Word2Vec</strong> model does its best, and Pluto doesn’t think a human<a id="_idIndexMarker624"/> can <span class="No-Break">do better.</span></p>
			<p>The next technique we’ll look at is <strong class="bold">BERT</strong>, which uses the transformer model and <span class="No-Break">generative AI.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor129"/>BERT</h2>
			<p>BERT is a Google transformer model<a id="_idIndexMarker625"/> trained on a massive corpus. The result is a near-perfect human-quality output. BERT and many other transformer models were made available and easily accessible on <em class="italic">HuggingFace</em> to the public starting around <span class="No-Break">mid-August 2022.</span></p>
			<p>The key code lines for the <strong class="source-inline">print_aug_ai_bert()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for print_aug_id_bert()</strong>
model_path='bert-base-uncased'
aug_func = nlpaug.augmenter.word.ContextualWordEmbsAug(
  action=action,
  model_path=model_path)</pre>
			<p>The full function can be found in the Python Notebook. Pluto feeds<a id="_idIndexMarker626"/> in the NLP Netflix data using <strong class="source-inline">insert</strong> mode with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># Augmenting using BERT</strong>
pluto.print_aug_ai_bert(pluto.df_netflix_data,
  col_dest='description',
  action='insert',
  aug_name='BERT Embedding Insert Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B17990_06_08.jpg" alt="Figure 6.8 – BERT using insert mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – BERT using insert mode on the Netflix data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>, Pluto immediately<a id="_idIndexMarker627"/> recognizes the improvement over the <strong class="bold">Word2Vec</strong> model. In the control text, <em class="italic">row #0</em>, the injection of words is acceptable. It lacks the elegance of the prose, but if you must add words to the text, it could pass as a <span class="No-Break">human writer.</span></p>
			<p>In <em class="italic">row #1</em>, the added phrases are spot on, such as <strong class="bold">afterward</strong>, <strong class="bold">financial dubious</strong>, <strong class="bold">knee surgery</strong>, and <strong class="bold">to play a national </strong><span class="No-Break"><strong class="bold">film stage</strong></span><span class="No-Break">.</span></p>
			<p>In <em class="italic">row #2</em>, the augmented phrases are at human writer quality, such as <strong class="bold">whilst in hiding</strong>, <strong class="bold">deeply suspect</strong>, <strong class="bold">unknown maid</strong>, <strong class="bold">perhaps his only</strong>, <strong class="bold">outside Russian world</strong>, and <strong class="bold">maybe hiding </strong><span class="No-Break"><strong class="bold">quite something</strong></span><span class="No-Break">.</span></p>
			<p>In <em class="italic">row #3</em>, Pluto is impressed with the results, such as <strong class="bold">arriving in February</strong>, <strong class="bold">little Indian lad</strong>, <strong class="bold">despite sparse funding</strong>, <strong class="bold">funding mathematics and physics</strong>, and <strong class="bold">first </strong><span class="No-Break"><strong class="bold">functioning airplane</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Are you as amazed as Pluto regarding the BERT model’s output? It is like BERT is a real person, not <span class="No-Break">an ANN.</span></p>
			<p>Please rerun the wrapper function<a id="_idIndexMarker628"/> to see BERT’s augmentation on other movie reviews. The more you read, the more<a id="_idIndexMarker629"/> you will appreciate the advanced breakthrough in using the <strong class="bold">transformer</strong> model. It is the foundation of <span class="No-Break">generative AI.</span></p>
			<p>Next, Pluto feeds the Twitter data into the BERT model with insert mode with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using BERT</strong>
pluto.print_aug_ai_bert(pluto.df_twitter_data,
  col_dest='clean_tweet',
  action='insert',
  aug_name='BERT Embedding Insert Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17990_06_09.jpg" alt="Figure 6.9 – BERT using insert mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – BERT using insert mode on the Twitter data</p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em>, the BERT model gives another version of Dicken’s control text. There is a new rendition every time Pluto runs the wrapper function. The possibilities are endless. Pluto must have run the wrapper functions over 50 times. Not once did he notice the <span class="No-Break">same result.</span></p>
			<p>Pluto discovered that there is better NLP data to study than tweets, but they represent the real world, so it is worth continuing to use them. As Pluto repeatedly rerun the wrapper function, he preferred the BERT<a id="_idIndexMarker630"/> augmented version over the original tweets because inserting text made it easier <span class="No-Break">to read.</span></p>
			<p>When switching to <strong class="bold">substitute</strong> mode, the output from BERT is better than average <span class="No-Break">human writers.</span></p>
			<h3>Substitute</h3>
			<p>Next, Pluto feeds<a id="_idIndexMarker631"/> the Netflix data to BERT in <strong class="bold">substitute</strong> mode using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using BERT</strong>
pluto.print_aug_ai_bert(pluto.df_netflix_data,
  col_dest='description',
  action='substitute',
  aug_name='BERT Embedding Substitute Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B17990_06_10.jpg" alt="Figure 6.10 – BERT using substitute mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – BERT using substitute mode on the Netflix data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, for the control text, <em class="italic">row #0</em>, BERT replaced <strong class="bold">it was the age of foolishness</strong> with <strong class="bold">death was the age </strong><span class="No-Break"><strong class="bold">of love</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Full stop. Pluto’s mind is being blown. Even Pluto’s human companion is speechless. Pluto expects a transformer model such as BERT to be good, but philosophical thoughts or poetry are on another level. Now, are you impressed <span class="No-Break">with BERT?</span></p>
			<p>The rest of the movie review<a id="_idIndexMarker632"/> augmentation, shown are <em class="italic">rows #1</em>, <em class="italic">#2</em>, and <em class="italic">#3</em>, is flawless. The augmented words match the movie genre and context. It is like BERT understands the movie’s meaning, but this isn’t true. The BERT model is no more sentient than a toaster. However, BERT can mimic a human <span class="No-Break">writer well.</span></p>
			<p>One interesting note is that in <em class="italic">row #1</em>, in the movie description about a couple’s relationship, BERT uses the word <em class="italic">gay</em>, which was discussed in the previous chapter about data biases. This is because <em class="italic">gay</em> is a perfectly nice word for lighthearted and carefree, but in a modern context, <em class="italic">gay</em> is associated with a person’s homosexual orientation, especially of <span class="No-Break">a man.</span></p>
			<p>Once again, Pluto encourages you to rerun the wrapper function repeatedly on the Python Notebook. You will appreciate it beyond the technical achievement and think that BERT has <span class="No-Break">a personality.</span></p>
			<p>Pluto does the same for the Twitter data with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using BERT</strong>
pluto.print_aug_ai_bert(pluto.df_twitter_data,
  col_dest='clean_tweet',
  action='substitute',
  aug_name='BERT Embedding Substitute Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B17990_06_11.jpg" alt="Figure 6.11 – BERT using substitute mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – BERT using substitute mode on the Twitter data</p>
			<p>As Pluto repeatedly ran the wrapper<a id="_idIndexMarker633"/> function on the Python Notebook, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.10</em>, he found that the augmented tweets were more accessible to read than the <span class="No-Break">original text.</span></p>
			<p>For the control text, <em class="italic">row #0</em>, Pluto found that having the augmented text <strong class="bold">it was the age of youth</strong>, replace the original text of <strong class="bold">it was the epoch of belief</strong> profoundly appropriate. It fits into the context and style of Mr. <span class="No-Break">Dickens’s book.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">This challenge is a thought experiment. BERT is built on an ANN algorithm. It does not contain grammar rules, such as nouns and verbs for constructing sentences. With no grammar rules, how does it write English so well? Hint: think about patterns. BERT is trained on a massive corpus. The number of words and sentences is so large that it was impossible to conceive 5 years ago. A few, if any, know how neural network algorithms learn. It is not complex math. It is gradient descent and matrix multiplication nudging billions of nodes (or neurons), but how does that collection of nodes write <span class="No-Break">English convincingly?</span></p>
			<p>Pluto can spend days talking<a id="_idIndexMarker634"/> about BERT, but let’s move forward with <strong class="bold">RoBERTa</strong> (<strong class="bold">Roberta</strong>). It sounds like a female version <span class="No-Break">of BERT.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor130"/>RoBERTa</h2>
			<p>RoBERTa is an optimized algorithm<a id="_idIndexMarker635"/> for self-supervising BERT. While Google created BERT, Meta AI (or Facebook) <span class="No-Break">developed RoBERTa.</span></p>
			<p>Pluto feeds the Netflix data to RoBERTa in insert mode with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using Roberta</strong>
pluto.print_aug_ai_bert(pluto.df_netflix_data,
  col_dest='description',
  model_path='roberta-base',
  action='insert',
  aug_name='Roberta Embedding Insert Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17990_06_12.jpg" alt="Figure 6.12 – RoBERTa using insert mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – RoBERTa using insert mode on the Netflix data</p>
			<p>The output in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em> is similar to the output from <strong class="bold">BERT</strong>, which is impressive. The words are not randomly<a id="_idIndexMarker636"/> inserted in the sentence. They expressed a possible interpretation and gave the impression that <strong class="bold">RoBERTa</strong> understood the meaning of the words. This level of technical achievement was not feasible 1 year ago, and <strong class="bold">RoBERTa</strong> was only made available a few <span class="No-Break">months ago.</span></p>
			<p>Pluto ran the wrapper function repeatedly and never tired of reading the result. He does the same for the Twitter data with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using Roberta</strong>
pluto.print_aug_ai_bert(pluto.df_twitter_data,
  col_dest='clean_tweet',
  model_path='roberta-base',
  action='insert',
  aug_name='Roberta Embedding Insert Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17990_06_13.jpg" alt="Figure 6.13 – RoBERTa using insert mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – RoBERTa using insert mode on the Twitter data</p>
			<p>Pluto can’t turn lead into gold, and RoBERTa can’t turn tweets, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.12</em>, that contain misspellings<a id="_idIndexMarker637"/> and incomplete thoughts into coherent sentences. Nevertheless, RoBERTa is one of the best choices for augmenting <span class="No-Break">real-world tweets.</span></p>
			<p>Next, Pluto will try <strong class="bold">RoBERTa</strong> with <span class="No-Break"><strong class="bold">substitute</strong></span><span class="No-Break"> mode.</span></p>
			<h3>Substitute</h3>
			<p>In substitute mode, RoBERTa will replace words<a id="_idIndexMarker638"/> or phrases with uncanny accuracy matching the context and <span class="No-Break">writing style.</span></p>
			<p>Pluto drops the Netflix data into the RoBERTa model in substitute mode using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using Roberta</strong>
pluto.print_aug_ai_bert(pluto.df_netflix_data,
  col_dest='description',
  model_path='roberta-base',
  action='substitute',
  aug_name='Roberta Embedding Substitute Augment')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B17990_06_14.jpg" alt="Figure 6.14 – RoBERTa using substitute mode on the Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – RoBERTa using substitute mode on the Netflix data</p>
			<p>No matter how often Pluto executes<a id="_idIndexMarker639"/> the wrapper function, he continues to be astonished by the output RoBERTa provides in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.13</em>. For example, in <em class="italic">row #1</em>, she changed the phrase <strong class="bold">Alex discovers he has little in common with the local</strong> to <strong class="bold">Alex discovers Flix had special romantic chemistry with the local</strong>. RoBERTa has quite the imagination. Is that what humans do when <span class="No-Break">we write?</span></p>
			<p>Pluto does the same with the Twitter data using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pluto.print_aug_ai_bert(pluto.df_twitter_data,
  col_dest='clean_tweet',
  model_path='roberta-base',
  action='substitute',
  aug_name='Roberta Embedding Substitute Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17990_06_15.jpg" alt="Figure 6.15 – RoBERTa using substitute mode on the Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – RoBERTa using substitute mode on the Twitter data</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.14</em>, text augmentation<a id="_idIndexMarker640"/> does not have to be boring or clinical. Using transformer models such as BERT and RoBERTa, augmentations are fun and full of wonders. For example, in the control text, on <em class="italic">row #0</em>, RoBERTa wrote, <strong class="bold">It preached a curse at arrogance</strong>, replacing <strong class="bold">It was an epoch </strong><span class="No-Break"><strong class="bold">of belief</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto’s human companion has to ponder a long time to conclude that the augmented text does mean the same as the original text in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.14</em>, the control text. It is easy to be fooled that RoBERTa has a conscience. We pair intelligence with consciousness, meaning if you have intelligence, you must be self-aware or vice versa, but we know that is not true. For example, a career politician is self-aware. He talks about himself all the time, but is <span class="No-Break">he intelligent?</span></p>
			<p>Continuing to use<a id="_idIndexMarker641"/> the latest powerful ML models, Pluto will take a different path to text augmentation by using the <strong class="bold">back </strong><span class="No-Break"><strong class="bold">translation</strong></span><span class="No-Break"> technique.</span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>Back translation</h2>
			<p>Back translation<a id="_idIndexMarker642"/> is a new concept in text augmentation because it was not possible 2 years ago. ML NLP existed earlier, with Google Translate leading the charge. Still, only a few data scientists could access the large language model using a transformer and the powerful servers required for <span class="No-Break">language translation.</span></p>
			<p>The technique for text augmentation is to translate into another language and back to the original language. In doing so, the result will be an augmented version of the original. No language translation is perfect. Hence, the extended version will be slightly different from the original text. For example, the original text is in English. Using a powerful NLP model, we translated it into German and back to English again. The translated English text will be different from the original <span class="No-Break">English text.</span></p>
			<p>Compared to <strong class="bold">Word2Vec</strong>, <strong class="bold">BERT</strong>, and <strong class="bold">RoBERTa</strong>, the back translation method could be more robust. This is because translating back to the original text gives the same result the second or third time. In other words, other methods’ output results in thousands of variations, while back translations have two or three <span class="No-Break">augmented versions.</span></p>
			<p>Pluto found two NLP pre-trained translation models from Facebook, or Meta AI, that were made available on the <em class="italic">HuggingFace</em> site. They are for English to German (Deutsch) and English to Russian (Русский). There are dozens more, but two are sufficient to demonstrate the technique. Let’s start <span class="No-Break">with German.</span></p>
			<h3>German (Deutsch)</h3>
			<p>The <strong class="source-inline">print_aug_ai_back_translation()</strong> method follows<a id="_idIndexMarker643"/> the same structure as any other wrapper function. It looks deceptively simple with five lines of code, but it has truly complex theories and coding techniques. It reminds Pluto of a famous quote by Sir Isaac Newton: “<em class="italic">If I have seen further, it is by standing on the shoulders </em><span class="No-Break"><em class="italic">of giants.</em></span><span class="No-Break">”</span></p>
			<p>The key code lines are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for back translation</strong>
from_model_name='facebook/wmt19-en-de'
to_model_name='facebook/wmt19-de-en'
aug_func = nlpaug.augmenter.word.BackTranslationAug(
  from_model_name=from_model_name,
  to_model_name=to_model_name)</pre>
			<p>The full function can be found<a id="_idIndexMarker644"/> in the Python Notebook. Pluto feeds in the Netflix data using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using back translation</strong>
pluto.print_aug_ai_back_translation(pluto.df_netflix_data,
  col_dest='description',
  from_model_name='facebook/wmt19-en-de',
  to_model_name='facebook/wmt19-de-en',
  aug_name='FaceBook Back Translation: English &lt;-&gt; German Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17990_06_16.jpg" alt="Figure 6.16 – Back translation, German on Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Back translation, German on Netflix data</p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The output in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em>  is anticlimactic because it reads similarly to the original text, but the technical achievement is mind-blowing. First, you need an expert to translate from English to German. It is a challenging task for a human to learn. Second, you must translate back to English with no errors. The difference in choosing similar words is expressing the phrase. Maybe 5% of the world’s population can do this task. For a machine to do it 24 hours a day, 7 days a week, and maintain the same accuracy level is miraculous. No human can match <span class="No-Break">this level.</span></p>
			<p>The output in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.15</em> gives an almost<a id="_idIndexMarker645"/> perfect English to German and back translation. Pluto does the same with the Twitter data using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using back translation</strong>
pluto.print_aug_ai_back_translation(pluto.df_twitter_data,
  col_dest='clean_tweet',
  from_model_name='facebook/wmt19-en-de',
  to_model_name='facebook/wmt19-de-en',
  aug_name='FaceBook Back Translation: English &lt;-&gt; German Augment')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B17990_06_17.jpg" alt="Figure 6.17 – Back translation, German on Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – Back translation, German on Twitter data</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.16</em>, translating nonsensible tweets into German and back is harder for humans because our minds get tired more quickly and give up. Only a machine can do this work around the clock. The control text<a id="_idIndexMarker646"/> translations into German and back <span class="No-Break">are acceptable.</span></p>
			<p>Translation to Russian and back would yield similar results. Let’s take <span class="No-Break">a look.</span></p>
			<h3>Russian (Русский)</h3>
			<p>Pluto chose to repeat<a id="_idIndexMarker647"/> the same back translation technique with English to Russian and back because he is curious to see if choosing a non-Romance family language would affect the augmentation <span class="No-Break">results differently.</span></p>
			<p>Using the same <strong class="source-inline">print_aug_ai_back_translation()</strong> function, Pluto defines the Russian translation Facebook model <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for back translation to Russian</strong>
from_model_name='facebook/wmt19-en-ru'
to_model_name='facebook/wmt19-ru-en'</pre>
			<p>The full function code can be found in the Python Notebook. Pluto feeds the Netflix data to the wrapper function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using back translation</strong>
pluto.print_aug_ai_back_translation(pluto.df_netflix_data,
  col_dest='description',
  from_model_name='facebook/wmt19-en-ru',
  to_model_name='facebook/wmt19-ru-en',
  aug_name='FaceBook Back Translation: English &lt;-&gt; Russian Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B17990_06_18.jpg" alt="Figure 6.18 – Back translation, Russian on Netflix data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – Back translation, Russian on Netflix data</p>
			<p>Remarkably, in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.17</em>, the NLP <strong class="bold">T5</strong> model translates a Romance family <a id="_idIndexMarker648"/>language (English) into an East<a id="_idIndexMarker649"/> Slavic language (Russian) and back with almost perfect accuracy. The grammar rules, sentence structures, alphabets, histories, cultures, and languages are different, yet a machine can do the task <span class="No-Break">without awareness.</span></p>
			<p>Tweets are not perfect for testing, but not all projects are logical. Pluto had worked on real-world NLP projects that were ill-conceived. The command for feeding Twitter data to the wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augmenting using back translation</strong>
pluto.print_aug_ai_back_translation(pluto.df_twitter_data,
  col_dest='clean_tweet',
  from_model_name='facebook/wmt19-en-ru',
  to_model_name='facebook/wmt19-ru-en',
  aug_name='FaceBook Back Translation: English &lt;-&gt; Russian Augment')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B17990_06_19.jpg" alt="Figure 6.19 – Back translation, Russian on Twitter data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.19 – Back translation, Russian on Twitter data</p>
			<p>If Russians don’t understand tweets, then<a id="_idIndexMarker650"/> who else can? Reading the control text in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.18</em>, Pluto can tell the translations are correct. Since some tweets are short, the translations to Russian and back <span class="No-Break">are perfect.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">This challenge is a thought experiment. Can you use the same techniques to augment the German language? Or can you string several back translations together – for example, from English to German to Russian and back <span class="No-Break">to English?</span></p>
			<p>The <strong class="bold">back translation</strong>, <strong class="bold">RoBERTa</strong>, <strong class="bold">BERT</strong>, and <strong class="bold">Word2Vec</strong> NLP ML models are the state of the art for text augmentation. The next level is sentence augmentation using summarization and the Sequential and <span class="No-Break">Sometimes techniques.</span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>Sentence augmentation</h2>
			<p>The sentence <strong class="bold">flow</strong> level uses a combination of word augmentation<a id="_idIndexMarker651"/> methods. But before that, Pluto<a id="_idIndexMarker652"/> will use the <strong class="bold">T5</strong> NLP model to generate a text summary. The <strong class="bold">summarization</strong> technique is one of the novel concepts<a id="_idIndexMarker653"/> made possible recently. It takes a page, an article, or even a book and generates a summary to be used in the NLP text <span class="No-Break">augmentation model.</span></p>
			<h3>Summary technique</h3>
			<p>For text augmentation, the <strong class="bold">summary</strong> technique may bring a few different<a id="_idIndexMarker654"/> versions for training. However, suppose Pluto combines the <strong class="bold">flow</strong> and <strong class="bold">summary</strong> techniques, such as by feeding the synopsis text, instead of the original text, to the <strong class="bold">flow</strong> technique. In that case, it will yield many new<a id="_idIndexMarker655"/> original texts <span class="No-Break">for training.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto pioneered the <strong class="bold">summary-to-flow</strong> concept for text augmentation. He had<a id="_idIndexMarker656"/> done a preliminary search on the web and scholarly publications, but he needs help finding a reference to the summary-to-flow technique. If none are found, then Pluto is the first to implement the <span class="No-Break">summary-to-flow strategy.</span></p>
			<p>Pluto will not use the Netflix movie description or Twitter tweets for the summary method. This is because they are too short to showcase the power of the T5 NLP model. Instead, Pluto will use the first page of the following books mentioned in the <em class="italic">Real-world NLP </em><span class="No-Break"><em class="italic">data</em></span><span class="No-Break"> section:</span></p>
			<ul>
				<li><em class="italic">Tale of Two Cities</em> <span class="No-Break">by Dickens</span></li>
				<li><em class="italic">Moby Dick</em> <span class="No-Break">by Melville</span></li>
				<li><em class="italic">Alice in Wonderland</em> <span class="No-Break">by Carroll</span></li>
			</ul>
			<p>Once again, the books are in the public domain, as defined in <span class="No-Break">Project Gutenberg.</span></p>
			<p>In addition, Pluto will use the first page of this chapter because you have read this book, but you may not have read those three <span class="No-Break">classic books.</span></p>
			<p>The key code line for the <strong class="source-inline">print_aug_ai_t5()</strong> wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
aug_func = nlpaug.augmenter.sentence.AbstSummAug(
  model_path='t5-base')</pre>
			<p>Pluto is playing a guessing<a id="_idIndexMarker657"/> game with you. First, he will list the four command lines to generate the four summaries, but he will shuffle the output. Thus, you have to guess which summary belongs<a id="_idIndexMarker658"/> to which book. Once again, you will be amazed at the quality output of the <strong class="bold">T5</strong> NLP model. It is comparable to <span class="No-Break">human writers.</span></p>
			<p> The profound implication is that you or Pluto can auto-generate summaries of books, papers, documents, articles, and posts with a few lines of Python code. This task was deemed impossible a few <span class="No-Break">years ago.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment. Can you be an expert in German laws without speaking German? It was impossible a year ago because the ML breakthrough wasn’t available, but today, you can use the code in the Python Notebook as the base to translate all German <span class="No-Break">law books.</span></p>
			<p>The four commands to get a summary of the first page of the four books we’ll be looking at are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># Alice in Wonderland</strong>
pluto.df_t5_carroll = pluto.print_aug_ai_t5(
  pluto.orig_carroll_page,
  bsize=1)
<strong class="bold"># Tale of Two Cities</strong>
pluto.df_t5_dickens = pluto.print_aug_ai_t5(
  pluto.orig_dickens_page,
  bsize=1)
<strong class="bold"># Moby Dick</strong>
pluto.df_t5_melville = pluto.print_aug_ai_t5(
  pluto.orig_melville_page,
  bsize=1)
<strong class="bold"># This chapter first page</strong>
pluto.df_t5_self = pluto.print_aug_ai_t5(
  pluto.orig_self,
  bsize=1)</pre>
			<p>The shuffled results are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B17990_06_20.jpg" alt="Figure 6.20 – Summary T5 NLP engine – 1"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.20 – Summary T5 NLP engine – 1</p>
			<p>The second output<a id="_idIndexMarker659"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B17990_06_21.jpg" alt="Figure 6.21 – Summary T5 NLP engine – 2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.21 – Summary T5 NLP engine – 2</p>
			<p>The third output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B17990_06_22.jpg" alt="Figure 6.22 – Summary T5 NLP engine – 3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.22 – Summary T5 NLP engine – 3</p>
			<p>The fourth output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B17990_06_23.jpg" alt="Figure 6.23 – Summary T5 NLP engine – 4"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.23 – Summary T5 NLP engine – 4</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Can you match the summarized output with the book? The T5 engine is not a generative AI engine like OpenAI GPT3, GPT4, or Google Bard. Still, the summary is <span class="No-Break">very accurate.</span></p>
			<p>The <em class="italic">Tale of Two Cities</em> book, shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.19</em>, is a relatively hard book<a id="_idIndexMarker660"/> to read, and Pluto thinks that it is funny that <em class="italic">David Rothkopf</em>, a contemporary political commentator, is associated with Dickens’ book. The first page does talk about the <strong class="bold">congress of British subjects in America</strong>. Thus, the Mr. Rothkopf association is a good guess. Maybe Pluto should feed the first 10 pages of the chapter into the <strong class="bold">T5</strong> NLP engine and see <span class="No-Break">the summary.</span></p>
			<p>The <em class="italic">Moby Dick</em> first-page summary is spot on, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.20</em>. It could pass as a human writer, and the first word is <strong class="bold">Ishmael</strong>. Pluto wishes that the <strong class="bold">T5</strong> NLP model was<a id="_idIndexMarker661"/> available during Pluto’s early days <span class="No-Break">in school.</span></p>
			<p>Pluto’s human companion is delighted to admit that the summary of this chapter’s first page is more precise and easier to read, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.21</em>. Maybe the <strong class="bold">T5</strong> NLP engine should co-write this book with Pluto so that his companion can enjoy chasing squirrels on a <span class="No-Break">sunny afternoon.</span></p>
			<p>The <em class="italic">Alice in Wonderland</em> first-page summary is perfect, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.22</em>. The <strong class="bold">T5</strong> NLP engine captures the assent of the opening page flawlessly. As a bonus, Pluto only inputted the first five sentences. The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B17990_06_24.jpg" alt="Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland</p>
			<p>In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.23</em>, how does <strong class="bold">T5</strong> know that the white rabbit<a id="_idIndexMarker662"/> is essential to the story? The rabbit only appears in the last sentence in the input text, and referring to Alice as the daisy-chain maker <span class="No-Break">is delightful.</span></p>
			<p>The next step in sentence augmentation is to feed these summaries to the <span class="No-Break"><strong class="bold">flow</strong></span><span class="No-Break"> methods.</span></p>
			<h3>Summary-to-flow technique</h3>
			<p>The Sequential method in the flow technique<a id="_idIndexMarker663"/> applies a list of augmentation in successive order. Pluto creates two text augmentation methods, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using uloVe</strong>
pluto.ai_aug_glove = nlpaug.augmenter.word.WordEmbsAug(
    model_type='glove', model_path='glove.6B.300d.txt',
    action="substitute")
<strong class="bold"># augment using BERT</strong>
pluto.ai_aug_bert = nlpaug.augmenter.word.ContextualWordEmbsAug(
  model_path='bert-base-uncased',
  action='substitute',
  top_k=20)</pre>
			<p>The first uses the <strong class="bold">Word2Vec</strong> with the <strong class="bold">GloVe</strong> model, while the second employs the <strong class="bold">BERT</strong> NLP engine. The <strong class="source-inline">print_aug_ai_sequential()</strong> wrapper function<a id="_idIndexMarker664"/> uses the augmentation list with the key code lines, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using sequential</strong>
aug_func = nlpaug.flow.Sequential(
  [self.ai_aug_bert, self.ai_aug_glove])</pre>
			<p>Pluto feeds the four summaries to the flow method, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># Alice in Wonderland</strong>
pluto.print_aug_ai_sequential(pluto.df_t5_carroll)
<strong class="bold"># Tale of Two Cities</strong>
pluto.print_aug_ai_sequential(pluto.df_t5_dickens)
<strong class="bold"># Moby Dick</strong>
pluto.print_aug_ai_sequential(pluto.df_t5_melville)
<strong class="bold"># This chapter</strong>
pluto.print_aug_ai_sequential(pluto.df_t5_self)</pre>
			<p>Let’s take a look at <span class="No-Break">the results.</span></p>
			<p>The <em class="italic">Alice in Wonderland</em> augmented summary output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/B17990_06_25.jpg" alt="Figure 6.25 – Summary-to-flow method, Alice in Wonderland"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.25 – Summary-to-flow method, Alice in Wonderland</p>
			<p>The <em class="italic">Tale of Two Cities</em> augmented summary<a id="_idIndexMarker665"/> output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B17990_06_26.jpg" alt="Figure 6.26 – Summary-to-flow method, Tale of Two Cities"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.26 – Summary-to-flow method, Tale of Two Cities</p>
			<p>The <em class="italic">Moby Dick</em> augmented summary<a id="_idIndexMarker666"/> output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B17990_06_27.jpg" alt="Figure 6.27 – Summary-to-flow method, Moby Dick"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.27 – Summary-to-flow method, Moby Dick</p>
			<p>This chapter’s augmented summary<a id="_idIndexMarker667"/> output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B17990_06_28.jpg" alt="Figure 6.28 – Summary-to-flow method, this chapter"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.28 – Summary-to-flow method, this chapter</p>
			<p>Pluto enjoyed reading the augmented<a id="_idIndexMarker668"/> summaries. Some are clever, and some are exaggerated, but for augmentation text, they <span class="No-Break">are sufficient.</span></p>
			<p>The next <strong class="bold">flow</strong> method is the <strong class="bold">Sometimes</strong> method. It works the same<a id="_idIndexMarker669"/> as the <strong class="bold">Sequential</strong> method, except it randomly applies an<a id="_idIndexMarker670"/> augmentation<a id="_idIndexMarker671"/> method or not. Pluto wrote the <strong class="source-inline">print_aug_ai_sometime()</strong> wrapper function on the Python Notebook, but he does not think explaining the results in this chapter would add more insight. You can run the wrapper function in the Python Notebook and view <span class="No-Break">the results.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to refactor the <strong class="bold">Pluto class</strong> to make it faster and more compact. You should<a id="_idIndexMarker672"/> also include all the image wrapper and helper functions from previous chapters. Pluto encourages you to create and upload your library to <em class="italic">GitHub</em> and <em class="italic">PyPI.org</em>. Furthermore, you don’t have to name the class <strong class="bold">PacktDataAug</strong>, but it would give Pluto<a id="_idIndexMarker673"/> and his human companion a great big smile if you cited or mentioned this book. The code goals were for ease of understanding, reusable patterns, and teaching on the <strong class="bold">Python Notebook</strong>. Thus, refactoring the code as a Python library<a id="_idIndexMarker674"/> would be relatively painless <span class="No-Break">and fun.</span></p>
			<p>The <strong class="bold">summary-to-flow</strong> technique is the last method<a id="_idIndexMarker675"/> that will be covered in the chapter. Now, let’s summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor133"/>Summary</h1>
			<p>Text augmentation with machine learning (ML) is an advanced technique. We used a pre-trained ML model to create additional training <span class="No-Break">NLP data.</span></p>
			<p>After inputting the first three paragraphs, the <strong class="bold">T5</strong> NLP ML engine wrote the preceding summary for this chapter. It is perfect and illustrates the spirit of this chapter. Thus, Pluto has kept <span class="No-Break">it as-is.</span></p>
			<p>In addition, we discussed 14 NLP ML models and four word augmentation methods. They were <strong class="bold">Word2Vec</strong>, <strong class="bold">BERT</strong>, <strong class="bold">RoBERTa</strong>, and <span class="No-Break"><strong class="bold">back translation</strong></span><span class="No-Break">.</span></p>
			<p>Pluto demonstrated that BERT and RoBERTa are as good as human writers. The augmented text is not just merely appropriate but inspirational, such as replacing <em class="italic">it was the age of foolishness</em> with <em class="italic">death was the age of love</em> or <em class="italic">it was the epoch of belief</em> with <em class="italic">it was the age </em><span class="No-Break"><em class="italic">of youth</em></span><span class="No-Break">.</span></p>
			<p>For the <strong class="bold">back translation</strong> method, Pluto used the Facebook or Meta AI NLP model to translate to German and Russian and back <span class="No-Break">to English.</span></p>
			<p>For sentence augmentation, Pluto dazzled with the accuracy of the <strong class="bold">T5</strong> NLP ML engine to summarize the first page of three classic books. Furthermore, he pioneered the <strong class="bold">summary-to-flow</strong> concept for text augmentation. Pluto might be the first to implement the <span class="No-Break"><strong class="bold">summary-to-flow</strong></span><span class="No-Break"> strategy.</span></p>
			<p>Throughout this chapter, there were <em class="italic">fun facts</em> and <em class="italic">fun challenges</em>. Pluto hopes you will take advantage of these and expand your experience beyond the scope of <span class="No-Break">this chapter.</span></p>
			<p>The next chapter is about audio augmentation, which will pose different challenges, but Pluto is ready <span class="No-Break">for them.</span></p>
		</div>
	

		<div id="_idContainer172" class="Content">
			<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/>Part 4: Audio Data Augmentation</h1>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B17990_07.xhtml#_idTextAnchor135"><em class="italic">Chapter 7</em></a>, <em class="italic">Audio Data Augmentation</em></li>
				<li><a href="B17990_08.xhtml#_idTextAnchor167"><em class="italic">Chapter 8</em></a>, <em class="italic">Audio Data Augmentation with Spectogram</em></li>
			</ul>
		</div>
	</body></html>