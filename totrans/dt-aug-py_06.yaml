- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text Augmentation with Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text augmentation with **machine learning** (**ML**) is an advanced technique
    compared to the standard text augmenting methods we covered in the previous chapter.
    Ironically, text augmentation aims to improve ML model accuracy, but we used a
    pre-trained ML model to create additional training NLP data. It’s a circular process.
    ML coding is not in this book’s scope, but understanding the difference between
    using libraries and ML for text augmentation is beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmentation libraries, whether for image, text, or audio, follow the traditional
    programming methodologies with structure data, loops, and conditional statements
    in the algorithm. For example, as shown in [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101),
    the pseudocode for implementing the `_print_aug_reserved()` method could be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The happy path code does not cover error checking, but the salient point is
    that the library’s function follows the standard sequential coding method.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ML is based on one of the 13 known ML algorithms, including
    `_print_aug_reserved()` pseudocode algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a representation of a DL architecture for image classification.
    It illustrates the difference between a procedural approach and the Neural Network
    algorithm. This figure was created from **Latex** and the **Overleaf** cloud system.
    The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Representation of a DL model](img/B17990_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Representation of a DL model
  prefs: []
  type: TYPE_NORMAL
- en: The Overleaf project and its code are from Mr. Duc Haba’s public repository,
    and the URL is [https://www.overleaf.com/project/6369a1eaba583e7cd423171b](https://www.overleaf.com/project/6369a1eaba583e7cd423171b).
    You can clone and hack the code to display other AI models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover text augmentation with ML, and in particular, the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence augmenting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world NLP datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcing your learning through the Python Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s briefly describe the ML models used in the Python wrapper function code.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the text augmentation wrapper functions use ML to generate
    new text for training the ML model. Understanding how these models are built is
    not in scope, but a brief description of these ML models and their algorithms
    is necessary. The Python wrapper functions will use the following ML models under
    the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: Tomáš Mikolov published the NLP algorithm using a neural network named **Word2Vec**
    in 2013\. The model can propose synonym words from the input text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Global Vectors for Word Representation** (**GloVe**) algorithm was created
    by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in 2014\. It
    is an unsupervised learning NLP algorithm for representing words in vector format.
    The results are a linear algorithm that groups the closest neighboring words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wiki-news-300d-1M** is a pre-trained ML model that uses the **fastText**
    open source library. It was trained on 1 million words from Wikipedia 2017 articles,
    the UMBC WebBase corpus, which consists of over 3 billion words, and the Statmt.org
    news dataset, which consists of over 16 billion tokens. T. Mikolov, E. Grave,
    P. Bojanowski, C. Puhrsch, and A. Joulin introduced **Wiki-news-300d-1M** in their
    *Advances in Pre-Training Distributed Word Representations* paper. The license
    is the Creative Commons Attribution-Share-Alike License 3.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GoogleNews-vectors-negative300** is a pre-trained **Word2Vec** model that
    uses the Google News dataset, which contains about 100 billion words and 300 dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google introduced the **transformer** neural network algorithm in 2017\. Recent
    cutting-edge breakthroughs in NLP and computer vision are from the transformer
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **BERT** model was introduced by Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova in 2018\. It is specialized in language inference and prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RoBERTa** is an optimized algorithm for the self-supervised NLP model. It
    is a model built on top of BERT. It excels in performance on many NLP inferences.
    Meta AI published RoBERTa in 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook’s **wmt19-en-de** and **wmt19-de-en** are pre-trained NLP models from
    *HuggingFace* for translating from English to German (Deutsch) and back. It was
    made publicly available in 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook’s **wmt19-en-ru** and **wmt19-ru-en** are pre-trained NLP models from
    *HuggingFace* for translating from English to Russian (Русский) and back. It was
    made publicly available in 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XLNet** is a transformer-XL pre-trained model that was made publicly available
    by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov,
    and Quoc V. Le on *HuggingFace* in 2021\. It was published in the scholarly paper
    *XLNet: Generalized Autoregressive Pretraining for* *Language Understanding*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Generative Pre-trained Transformer 2** (**GPT-2**) algorithm is an open
    source AI that was published by OpenAI in 2019\. The model excels in writing feedback
    questions and answers and generating text summarization of an article. It is at
    the level of actual human writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **T5** and **T5X** models use the text-to-text transformer algorithm. They
    were trained on a massive corpus. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
    Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu introduced
    T5 in their paper *Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer* in 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI, when using a transformer model, such as OpenAI’s GPT-3, GPT-4,
    or Google Bard, can write as well or better than a human writer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know about some of the ML models, let’s see which augmenting function
    uses which ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Word augmenting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the word augmenting techniques are similar to the methods from
    [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101), which used the **Nlpaug** library.
    The difference is that rather than Python libraries, the wrapper functions use
    powerful ML models to achieve remarkable results. Sometimes, the output or rewritten
    text is akin to human writers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, you will learn four new techniques and two variants each. Let’s
    start with Word2Vec:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Word2Vec** method uses the neural network NLP Word2Vec algorithm and the
    GoogleNews-vectors-negative300 pre-trained model. Google trained it using a large
    corpus containing about 100 billion words and 300 dimensions. Substitute and insert
    are the two mode variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **BERT** method uses Google’s transformer algorithm and BERT pre-trained
    model. Substitute and insert are the two mode variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **RoBERTa** method is a variation of the BERT model. Substitute and insert
    are the two mode variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last word augmenting technique that we’ll look at in this chapter is **back
    translation** using Facebook’s (Meta’s) pre-trained translation model. It translates
    the input English text into a different language and back to English. The two
    variants we’ll look at involve translating from English into German (Deutsch)
    and back to English using the **facebook/wmt19-en-de** and **facebook/wmt19-de-en**
    models, and from English to Russian (Русский) and back to English using the **facebook/wmt19-en-ru**
    and **facebook/wmt19-ru-en** models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will be easier to understand this by reading the output from the word wrapper
    functions, but before we do, let’s describe sentence augmenting.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence augmenting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Augmenting at the sentence level is a powerful concept. It was not possible
    5 years ago. You had to be working in an ML research company or a billionaire
    before accessing these acclaimed pre-trained models. Some transformer and **large
    language models** (**LLMs**) became available in 2019 and 2020 as open source,
    but they are generally for research. Convenient access to online AI servers via
    a GPU was not widely available at that time. The LLM and pre-trained models have
    recently become publicly accessible for incorporating them into your projects,
    such as the HuggingFace website. The salient point is that for independent researchers
    or students, LLM and pre-trained models only became accessible in mid-2021.
  prefs: []
  type: TYPE_NORMAL
- en: The sentence and word augmenting methods that use ML can’t be done dynamically
    as with methods using the **Nlpaug** library. In other words, you have to write
    and save the augmented text to your local or cloud disk space. The primary reason
    is that the augmentation step takes too long per training cycle. The upside is
    that you can increase the original text by 20 to 100 times its size.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing text using the **T5** NLP algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence** and **Sometimes** are two sentence flow methods. The flow methods
    use a combination of the **GloVe** and **BERT** NLP algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sentence augmentation techniques, they are easier to understand by reading
    the output of the wrapper functions using real-world NLP datasets as input text.
    Thus, the following section is about writing wrapper functions with Python code
    to gain insight into sentence augmenting, but first, let’s download the real-world
    NLP datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world NLP datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will use the same Netflix and Twitter real-world NLP datasets
    from [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101). In addition, both datasets
    have been vetted, cleaned, and stored in the `pluto_data` directory in this book’s
    GitHub repository. The startup sequence is similar to the previous chapters. It
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Clone the Python Notebook and Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the NLP data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start with the Python Notebook and Pluto.
  prefs: []
  type: TYPE_NORMAL
- en: Python Notebook and Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading the `data_augmentation_with_python_chapter_6.ipynb` file into
    Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this
    point onward, we will only display code snippets. The complete Python code can
    be found in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to clone the repository. We will reuse the code from [*Chapter
    5*](B17990_05.xhtml#_idTextAnchor101). The `!git` and `%run` statements are used
    to instantiate Pluto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The following setup step is checking if Pluto loaded correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Verify
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following command asks Pluto to display his status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows or similar, depending on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pluto showed that he is from [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    (version 5.0), which is correct. In addition, the cleaned NLP Twitter and Netflix
    datasets are in the `~/``Data-Augmentation-with-Python/pluto_data` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world NLP data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto is using the clean versions of the data without profanity from [*Chapter
    5*](B17990_05.xhtml#_idTextAnchor101). They are the Netflix and Twitter NLP datasets
    from the Kaggle website. The clean datasets were saved in this book’s GitHub repository.
    Thus, Pluto does not need to download them again. Still, you can download them
    or other real-world datasets by using the `fetch_kaggle_dataset()` function. Pluto
    locates the cleaned NLP datasets with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto gets lazy, and instead of using a Python library and coding it in Python,
    he cheats by dropping down to the Linux Bash command-line code. The exclamation
    character (`!`) allows the Python Notebook to backdoor the kernel, such as via
    `!ls -la` on Linux or`!dir` on Windows. You can use any OS command-line code.
    Still, it is not portable code because the commands for Windows, iOS, Linux, Android,
    and other OSs that support web browsers such as Safari, Chrome, Edge, and Firefox
    are different.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to load the data into Pluto’s buddy, pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto reuses the `fetch_df()` method from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038)
    to load the data into pandas. The following commands import the real-world Netflix
    data into pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the commands for loading the real-world Twitter data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto challenges you to find and download two additional NLP data from the
    Kaggle website. Hint: use Pluto’s `fetch_kaggle_dataset()` function. Import it
    into pandas using the `fetch_df()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that Pluto has located and imported the data into pandas, the last step
    in loading the data sequence is to view and verify the data.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `draw_word_count()` and `draw_null_data()` methods help us understand the
    NLP data, and Pluto recommends revisiting [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    to view those Netflix and Twitter graphs. A more colorful and fun method is to
    use the `draw_word_cloud()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto draws the Netflix word cloud infographic graph with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2– Netflix word cloud](img/B17990_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2– Netflix word cloud
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, Pluto displays the Twitter word cloud using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Twitter word cloud](img/B17990_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Twitter word cloud
  prefs: []
  type: TYPE_NORMAL
- en: Along with the real-world NLP data, Pluto uses the first few lines of the *Tale
    of Two Cities*, by Charles Dickens, as the control text. In this chapter, Pluto
    will extend the control text to the first page of Mr. Dickens’ book, the *Moby
    Dick* book, by Melville, and the *Alice in Wonderland* book, by Carroll. These
    books are in public domain, as defined in Project Gutenberg.
  prefs: []
  type: TYPE_NORMAL
- en: The varibles are `pluto.orig_text`, `pluto.orig_dickens_page`, `pluto.orig_melville_page`,
    and `pluto.orig_carroll_page`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: ML is good at altering text in typical human writing but modifying the masterworks
    is borderline criminal. Pluto seeks only to illustrate the augmentation concepts
    and never to bastardize the classics. It is in the name of science.
  prefs: []
  type: TYPE_NORMAL
- en: You have loaded the Python Notebook, instantiated Pluto, accessed the cleaned
    NLP real-world data, and verified it with the word cloud infographic. Now, it
    is time to write and hack Python code to gain a deeper insight into word and sentence
    augmentation with ML.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcing your learning through the Python Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though NLP ML is highly complex, the implementation for the wrapper code
    is deceptively simple. This is because of Pluto’s structured object-oriented approach.
    First, we created a base class for Pluto in [*Chapter 1*](B17990_01.xhtml#_idTextAnchor016)
    and used the decorator to add a new method as we learned new augmentation concepts.
    In [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038), Pluto learned to download
    any of the thousands of real-world datasets from the Kaggle website. *Chapters
    3* and *4* introduced the wrapper functions process using powerful open source
    libraries under the hood. Finally, [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    put forward the text augmentation concepts and methods when using the **Nlpaug**
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, building upon our previous knowledge, the wrapper functions in this
    chapter use the powerful NLP ML pre-trained model to perform the augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, this chapter will present wrapper functions and the augmenting
    results for the Netflix and Twitter real-world datasets using the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2Vec** word augmenting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT** and **Transformer** word augmenting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RoBERTa** augmenting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Back translation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**T5** augmenting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential** and **Sometime** augmenting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with Word2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec word augmenting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `print_aug_ai_word2vec()` wrapper function’s key parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The full functions can be found in the Python Notebook. Pluto uses the real-world
    NLP Netflix data to test the function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: When you run a wrapper function, new data is randomly selected and processed.
    Thus, it would be best if you run the wrapper function repeatedly to see different
    movie reviews from the Netflix dataset or tweets from the Twitter dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Word2Vec using insert mode on the Netflix data](img/B17990_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Word2Vec using insert mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.3*, the first row is the control input. It is a quote from the
    book *A Tale of Two Cities*. You will find that the augmented effects are easier
    to spot by comparing the control text with the text in the datasets. In addition,
    the control text is needed to compare the differences between augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto found the injection of names on *row #1*, such as **Punta** (a believable
    Spanish writer name) and **Poydras**, as actual names and plausible additions
    to this celebrity movie review context. It was not factual in the movie, but it
    is acceptable for text augmentation for movie sentiment prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On *row #2*, the words **blending**, **dangerous**, and **original 1960s**
    add flare to the movie description without altering the intent of the spy movie’s
    description.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On *row #3*, the addition of names, such as **Kent of Cabus** (Kent from a
    village in English named Cabus), **Rangjung** (a village in Bhutan, served as
    a possible hero name), and **Elizabeth** (as the villain) in the comic Green Arrow
    movie description is 100% plausible plot for our superhero.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, Pluto is flabbergasted by the **Word2Vec** ML model. The word and name
    injections are contexts that are appropriate as if a human writer were creating
    them. However, the control text from Dickens is funny to read, and it is not ML’s
    fault. The system does not know that the book was written in the 1800s and has
    only the first few lines of the text to go off. The movie review is a complete
    thought, while the control text is a tiny fragment of the whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto runs a similar command on the real-world Twitter data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Word2Vec using insert mode on the Twitter data](img/B17990_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Word2Vec using insert mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: Since tweets are like random thoughts written without forethoughts or editing,
    in *Figure 6**.4*, the **Word2Vec** injections are like a bored high school student
    doing homework while playing a computer game. Pluto can’t judge if the altered
    text is plausible or not. Would it increase or decrease the AI prediction accuracy
    for sentiment analysis?
  prefs: []
  type: TYPE_NORMAL
- en: For Dickens’ control text, Pluto flinched. It was dreadful, but he promised
    the AI would be better in the later model when using transformers and generative
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at **insert** mode, let’s see how the **Word2Vec** model
    performs in **substitute** mode.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Substitute** mode replaces words and then adds words to the sentence. Pluto
    applies the **Word2Vec** model using **substitute** mode to the Netflix data like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Word2Vec using substitute mode on the Netflix data](img/B17990_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Word2Vec using substitute mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.5*, *row #0* is the control text, and on *row #1*, **zany adventure**
    is suitable for a kid adventure movie, but **liquid viagra** is definitely off
    the mark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On *row #2*, replacing **police** with **troopers**, **job** with **plum assignment**,
    **wrest** with **must take**, **figure** with **hand**, and **offenders** with
    **criminals** are suitable in the police movie. Thus, the **Word2Vec** model did
    a proper augmentation job.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On *row #3*, replacing **cinematic distillation** with **Scorcese decaffeination**
    is an intriguing choice worthy of a human writer. Changing **electrifying** to
    **sparkling** is clever because electricity can spark. Substituting **shadowy**
    with **clandestine** is a good choice, but switching **seven** with **five** is
    unnecessary.'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the **Word2Vec** model could have done better for the control text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same to the Twitter data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Word2Vec using substitute mode on the Twitter data](img/B17990_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Word2Vec using substitute mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.6*, the tweets are chaotic, and many are incomplete thoughts.
    The **Word2Vec** model does its best, and Pluto doesn’t think a human can do better.
  prefs: []
  type: TYPE_NORMAL
- en: The next technique we’ll look at is **BERT**, which uses the transformer model
    and generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BERT is a Google transformer model trained on a massive corpus. The result is
    a near-perfect human-quality output. BERT and many other transformer models were
    made available and easily accessible on *HuggingFace* to the public starting around
    mid-August 2022.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key code lines for the `print_aug_ai_bert()` function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function can be found in the Python Notebook. Pluto feeds in the NLP
    Netflix data using `insert` mode with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – BERT using insert mode on the Netflix data](img/B17990_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – BERT using insert mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.7*, Pluto immediately recognizes the improvement over the **Word2Vec**
    model. In the control text, *row #0*, the injection of words is acceptable. It
    lacks the elegance of the prose, but if you must add words to the text, it could
    pass as a human writer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *row #1*, the added phrases are spot on, such as **afterward**, **financial
    dubious**, **knee surgery**, and **to play a national** **film stage**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *row #2*, the augmented phrases are at human writer quality, such as **whilst
    in hiding**, **deeply suspect**, **unknown maid**, **perhaps his only**, **outside
    Russian world**, and **maybe hiding** **quite something**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *row #3*, Pluto is impressed with the results, such as **arriving in February**,
    **little Indian lad**, **despite sparse funding**, **funding mathematics and physics**,
    and **first** **functioning airplane**.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Are you as amazed as Pluto regarding the BERT model’s output? It is like BERT
    is a real person, not an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Please rerun the wrapper function to see BERT’s augmentation on other movie
    reviews. The more you read, the more you will appreciate the advanced breakthrough
    in using the **transformer** model. It is the foundation of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Pluto feeds the Twitter data into the BERT model with insert mode with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – BERT using insert mode on the Twitter data](img/B17990_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – BERT using insert mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.8*, the BERT model gives another version of Dicken’s control
    text. There is a new rendition every time Pluto runs the wrapper function. The
    possibilities are endless. Pluto must have run the wrapper functions over 50 times.
    Not once did he notice the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto discovered that there is better NLP data to study than tweets, but they
    represent the real world, so it is worth continuing to use them. As Pluto repeatedly
    rerun the wrapper function, he preferred the BERT augmented version over the original
    tweets because inserting text made it easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: When switching to **substitute** mode, the output from BERT is better than average
    human writers.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, Pluto feeds the Netflix data to BERT in **substitute** mode using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – BERT using substitute mode on the Netflix data](img/B17990_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – BERT using substitute mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.9*, for the control text, *row #0*, BERT replaced **it was the
    age of foolishness** with **death was the age** **of love**.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Full stop. Pluto’s mind is being blown. Even Pluto’s human companion is speechless.
    Pluto expects a transformer model such as BERT to be good, but philosophical thoughts
    or poetry are on another level. Now, are you impressed with BERT?
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the movie review augmentation, shown are *rows #1*, *#2*, and *#3*,
    is flawless. The augmented words match the movie genre and context. It is like
    BERT understands the movie’s meaning, but this isn’t true. The BERT model is no
    more sentient than a toaster. However, BERT can mimic a human writer well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One interesting note is that in *row #1*, in the movie description about a
    couple’s relationship, BERT uses the word *gay*, which was discussed in the previous
    chapter about data biases. This is because *gay* is a perfectly nice word for
    lighthearted and carefree, but in a modern context, *gay* is associated with a
    person’s homosexual orientation, especially of a man.'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, Pluto encourages you to rerun the wrapper function repeatedly on
    the Python Notebook. You will appreciate it beyond the technical achievement and
    think that BERT has a personality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the Twitter data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – BERT using substitute mode on the Twitter data](img/B17990_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – BERT using substitute mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: As Pluto repeatedly ran the wrapper function on the Python Notebook, in *Figure
    6**.10*, he found that the augmented tweets were more accessible to read than
    the original text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the control text, *row #0*, Pluto found that having the augmented text
    **it was the age of youth**, replace the original text of **it was the epoch of
    belief** profoundly appropriate. It fits into the context and style of Mr. Dickens’s
    book.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'This challenge is a thought experiment. BERT is built on an ANN algorithm.
    It does not contain grammar rules, such as nouns and verbs for constructing sentences.
    With no grammar rules, how does it write English so well? Hint: think about patterns.
    BERT is trained on a massive corpus. The number of words and sentences is so large
    that it was impossible to conceive 5 years ago. A few, if any, know how neural
    network algorithms learn. It is not complex math. It is gradient descent and matrix
    multiplication nudging billions of nodes (or neurons), but how does that collection
    of nodes write English convincingly?'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto can spend days talking about BERT, but let’s move forward with **RoBERTa**
    (**Roberta**). It sounds like a female version of BERT.
  prefs: []
  type: TYPE_NORMAL
- en: RoBERTa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RoBERTa is an optimized algorithm for self-supervising BERT. While Google created
    BERT, Meta AI (or Facebook) developed RoBERTa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto feeds the Netflix data to RoBERTa in insert mode with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – RoBERTa using insert mode on the Netflix data](img/B17990_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – RoBERTa using insert mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: The output in *Figure 6**.11* is similar to the output from **BERT**, which
    is impressive. The words are not randomly inserted in the sentence. They expressed
    a possible interpretation and gave the impression that **RoBERTa** understood
    the meaning of the words. This level of technical achievement was not feasible
    1 year ago, and **RoBERTa** was only made available a few months ago.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto ran the wrapper function repeatedly and never tired of reading the result.
    He does the same for the Twitter data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – RoBERTa using insert mode on the Twitter data](img/B17990_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – RoBERTa using insert mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: Pluto can’t turn lead into gold, and RoBERTa can’t turn tweets, as shown in
    *Figure 6**.12*, that contain misspellings and incomplete thoughts into coherent
    sentences. Nevertheless, RoBERTa is one of the best choices for augmenting real-world
    tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, Pluto will try **RoBERTa** with **substitute** mode.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In substitute mode, RoBERTa will replace words or phrases with uncanny accuracy
    matching the context and writing style.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto drops the Netflix data into the RoBERTa model in substitute mode using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – RoBERTa using substitute mode on the Netflix data](img/B17990_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – RoBERTa using substitute mode on the Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter how often Pluto executes the wrapper function, he continues to be
    astonished by the output RoBERTa provides in *Figure 6**.13*. For example, in
    *row #1*, she changed the phrase **Alex discovers he has little in common with
    the local** to **Alex discovers Flix had special romantic chemistry with the local**.
    RoBERTa has quite the imagination. Is that what humans do when we write?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same with the Twitter data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – RoBERTa using substitute mode on the Twitter data](img/B17990_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – RoBERTa using substitute mode on the Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 6**.14*, text augmentation does not have to be boring or
    clinical. Using transformer models such as BERT and RoBERTa, augmentations are
    fun and full of wonders. For example, in the control text, on *row #0*, RoBERTa
    wrote, **It preached a curse at arrogance**, replacing **It was an epoch** **of
    belief**.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto’s human companion has to ponder a long time to conclude that the augmented
    text does mean the same as the original text in *Figure 6**.14*, the control text.
    It is easy to be fooled that RoBERTa has a conscience. We pair intelligence with
    consciousness, meaning if you have intelligence, you must be self-aware or vice
    versa, but we know that is not true. For example, a career politician is self-aware.
    He talks about himself all the time, but is he intelligent?
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to use the latest powerful ML models, Pluto will take a different
    path to text augmentation by using the **back** **translation** technique.
  prefs: []
  type: TYPE_NORMAL
- en: Back translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back translation is a new concept in text augmentation because it was not possible
    2 years ago. ML NLP existed earlier, with Google Translate leading the charge.
    Still, only a few data scientists could access the large language model using
    a transformer and the powerful servers required for language translation.
  prefs: []
  type: TYPE_NORMAL
- en: The technique for text augmentation is to translate into another language and
    back to the original language. In doing so, the result will be an augmented version
    of the original. No language translation is perfect. Hence, the extended version
    will be slightly different from the original text. For example, the original text
    is in English. Using a powerful NLP model, we translated it into German and back
    to English again. The translated English text will be different from the original
    English text.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to **Word2Vec**, **BERT**, and **RoBERTa**, the back translation method
    could be more robust. This is because translating back to the original text gives
    the same result the second or third time. In other words, other methods’ output
    results in thousands of variations, while back translations have two or three
    augmented versions.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto found two NLP pre-trained translation models from Facebook, or Meta AI,
    that were made available on the *HuggingFace* site. They are for English to German
    (Deutsch) and English to Russian (Русский). There are dozens more, but two are
    sufficient to demonstrate the technique. Let’s start with German.
  prefs: []
  type: TYPE_NORMAL
- en: German (Deutsch)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `print_aug_ai_back_translation()` method follows the same structure as
    any other wrapper function. It looks deceptively simple with five lines of code,
    but it has truly complex theories and coding techniques. It reminds Pluto of a
    famous quote by Sir Isaac Newton: “*If I have seen further, it is by standing
    on the shoulders* *of giants.*”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key code lines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function can be found in the Python Notebook. Pluto feeds in the Netflix
    data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Back translation, German on Netflix data](img/B17990_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Back translation, German on Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: The output in *Figure 6**.15* is anticlimactic because it reads similarly to
    the original text, but the technical achievement is mind-blowing. First, you need
    an expert to translate from English to German. It is a challenging task for a
    human to learn. Second, you must translate back to English with no errors. The
    difference in choosing similar words is expressing the phrase. Maybe 5% of the
    world’s population can do this task. For a machine to do it 24 hours a day, 7
    days a week, and maintain the same accuracy level is miraculous. No human can
    match this level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output in *Figure 6**.15* gives an almost perfect English to German and
    back translation. Pluto does the same with the Twitter data using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Back translation, German on Twitter data](img/B17990_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Back translation, German on Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.16*, translating nonsensible tweets into German and back is harder
    for humans because our minds get tired more quickly and give up. Only a machine
    can do this work around the clock. The control text translations into German and
    back are acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Translation to Russian and back would yield similar results. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Russian (Русский)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pluto chose to repeat the same back translation technique with English to Russian
    and back because he is curious to see if choosing a non-Romance family language
    would affect the augmentation results differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same `print_aug_ai_back_translation()` function, Pluto defines the
    Russian translation Facebook model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. Pluto feeds the
    Netflix data to the wrapper function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Back translation, Russian on Netflix data](img/B17990_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Back translation, Russian on Netflix data
  prefs: []
  type: TYPE_NORMAL
- en: Remarkably, in *Figure 6**.17*, the NLP **T5** model translates a Romance family
    language (English) into an East Slavic language (Russian) and back with almost
    perfect accuracy. The grammar rules, sentence structures, alphabets, histories,
    cultures, and languages are different, yet a machine can do the task without awareness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweets are not perfect for testing, but not all projects are logical. Pluto
    had worked on real-world NLP projects that were ill-conceived. The command for
    feeding Twitter data to the wrapper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Back translation, Russian on Twitter data](img/B17990_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Back translation, Russian on Twitter data
  prefs: []
  type: TYPE_NORMAL
- en: If Russians don’t understand tweets, then who else can? Reading the control
    text in *Figure 6**.18*, Pluto can tell the translations are correct. Since some
    tweets are short, the translations to Russian and back are perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is a thought experiment. Can you use the same techniques to augment
    the German language? Or can you string several back translations together – for
    example, from English to German to Russian and back to English?
  prefs: []
  type: TYPE_NORMAL
- en: The **back translation**, **RoBERTa**, **BERT**, and **Word2Vec** NLP ML models
    are the state of the art for text augmentation. The next level is sentence augmentation
    using summarization and the Sequential and Sometimes techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sentence **flow** level uses a combination of word augmentation methods.
    But before that, Pluto will use the **T5** NLP model to generate a text summary.
    The **summarization** technique is one of the novel concepts made possible recently.
    It takes a page, an article, or even a book and generates a summary to be used
    in the NLP text augmentation model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary technique
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For text augmentation, the **summary** technique may bring a few different versions
    for training. However, suppose Pluto combines the **flow** and **summary** techniques,
    such as by feeding the synopsis text, instead of the original text, to the **flow**
    technique. In that case, it will yield many new original texts for training.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto pioneered the **summary-to-flow** concept for text augmentation. He had
    done a preliminary search on the web and scholarly publications, but he needs
    help finding a reference to the summary-to-flow technique. If none are found,
    then Pluto is the first to implement the summary-to-flow strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto will not use the Netflix movie description or Twitter tweets for the
    summary method. This is because they are too short to showcase the power of the
    T5 NLP model. Instead, Pluto will use the first page of the following books mentioned
    in the *Real-world NLP* *data* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tale of Two Cities* by Dickens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moby Dick* by Melville'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alice in Wonderland* by Carroll'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, the books are in the public domain, as defined in Project Gutenberg.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Pluto will use the first page of this chapter because you have
    read this book, but you may not have read those three classic books.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key code line for the `print_aug_ai_t5()` wrapper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Pluto is playing a guessing game with you. First, he will list the four command
    lines to generate the four summaries, but he will shuffle the output. Thus, you
    have to guess which summary belongs to which book. Once again, you will be amazed
    at the quality output of the **T5** NLP model. It is comparable to human writers.
  prefs: []
  type: TYPE_NORMAL
- en: The profound implication is that you or Pluto can auto-generate summaries of
    books, papers, documents, articles, and posts with a few lines of Python code.
    This task was deemed impossible a few years ago.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Here is a thought experiment. Can you be an expert in German laws without speaking
    German? It was impossible a year ago because the ML breakthrough wasn’t available,
    but today, you can use the code in the Python Notebook as the base to translate
    all German law books.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four commands to get a summary of the first page of the four books we’ll
    be looking at are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The shuffled results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Summary T5 NLP engine – 1](img/B17990_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Summary T5 NLP engine – 1
  prefs: []
  type: TYPE_NORMAL
- en: 'The second output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Summary T5 NLP engine – 2](img/B17990_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Summary T5 NLP engine – 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The third output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Summary T5 NLP engine – 3](img/B17990_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Summary T5 NLP engine – 3
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Summary T5 NLP engine – 4](img/B17990_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Summary T5 NLP engine – 4
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Can you match the summarized output with the book? The T5 engine is not a generative
    AI engine like OpenAI GPT3, GPT4, or Google Bard. Still, the summary is very accurate.
  prefs: []
  type: TYPE_NORMAL
- en: The *Tale of Two Cities* book, shown in *Figure 6**.19*, is a relatively hard
    book to read, and Pluto thinks that it is funny that *David Rothkopf*, a contemporary
    political commentator, is associated with Dickens’ book. The first page does talk
    about the **congress of British subjects in America**. Thus, the Mr. Rothkopf
    association is a good guess. Maybe Pluto should feed the first 10 pages of the
    chapter into the **T5** NLP engine and see the summary.
  prefs: []
  type: TYPE_NORMAL
- en: The *Moby Dick* first-page summary is spot on, as shown in *Figure 6**.20*.
    It could pass as a human writer, and the first word is **Ishmael**. Pluto wishes
    that the **T5** NLP model was available during Pluto’s early days in school.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto’s human companion is delighted to admit that the summary of this chapter’s
    first page is more precise and easier to read, as shown in *Figure 6**.21*. Maybe
    the **T5** NLP engine should co-write this book with Pluto so that his companion
    can enjoy chasing squirrels on a sunny afternoon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Alice in Wonderland* first-page summary is perfect, as shown in *Figure
    6**.22*. The **T5** NLP engine captures the assent of the opening page flawlessly.
    As a bonus, Pluto only inputted the first five sentences. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland](img/B17990_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Summary T5 NLP Engine – the first five lines of Alice in Wonderland
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.23*, how does **T5** know that the white rabbit is essential
    to the story? The rabbit only appears in the last sentence in the input text,
    and referring to Alice as the daisy-chain maker is delightful.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in sentence augmentation is to feed these summaries to the **flow**
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Summary-to-flow technique
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Sequential method in the flow technique applies a list of augmentation
    in successive order. Pluto creates two text augmentation methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The first uses the `print_aug_ai_sequential()` wrapper function uses the augmentation
    list with the key code lines, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto feeds the four summaries to the flow method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Alice in Wonderland* augmented summary output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Summary-to-flow method, Alice in Wonderland](img/B17990_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Summary-to-flow method, Alice in Wonderland
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Tale of Two Cities* augmented summary output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – Summary-to-flow method, Tale of Two Cities](img/B17990_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – Summary-to-flow method, Tale of Two Cities
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Moby Dick* augmented summary output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Summary-to-flow method, Moby Dick](img/B17990_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Summary-to-flow method, Moby Dick
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter’s augmented summary output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Summary-to-flow method, this chapter](img/B17990_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Summary-to-flow method, this chapter
  prefs: []
  type: TYPE_NORMAL
- en: Pluto enjoyed reading the augmented summaries. Some are clever, and some are
    exaggerated, but for augmentation text, they are sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: The next `print_aug_ai_sometime()` wrapper function on the Python Notebook,
    but he does not think explaining the results in this chapter would add more insight.
    You can run the wrapper function in the Python Notebook and view the results.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to refactor the **Pluto class** to make it faster and more
    compact. You should also include all the image wrapper and helper functions from
    previous chapters. Pluto encourages you to create and upload your library to *GitHub*
    and *PyPI.org*. Furthermore, you don’t have to name the class **PacktDataAug**,
    but it would give Pluto and his human companion a great big smile if you cited
    or mentioned this book. The code goals were for ease of understanding, reusable
    patterns, and teaching on the **Python Notebook**. Thus, refactoring the code
    as a Python library would be relatively painless and fun.
  prefs: []
  type: TYPE_NORMAL
- en: The **summary-to-flow** technique is the last method that will be covered in
    the chapter. Now, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text augmentation with machine learning (ML) is an advanced technique. We used
    a pre-trained ML model to create additional training NLP data.
  prefs: []
  type: TYPE_NORMAL
- en: After inputting the first three paragraphs, the **T5** NLP ML engine wrote the
    preceding summary for this chapter. It is perfect and illustrates the spirit of
    this chapter. Thus, Pluto has kept it as-is.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we discussed 14 NLP ML models and four word augmentation methods.
    They were **Word2Vec**, **BERT**, **RoBERTa**, and **back translation**.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto demonstrated that BERT and RoBERTa are as good as human writers. The augmented
    text is not just merely appropriate but inspirational, such as replacing *it was
    the age of foolishness* with *death was the age of love* or *it was the epoch
    of belief* with *it was the age* *of youth*.
  prefs: []
  type: TYPE_NORMAL
- en: For the **back translation** method, Pluto used the Facebook or Meta AI NLP
    model to translate to German and Russian and back to English.
  prefs: []
  type: TYPE_NORMAL
- en: For sentence augmentation, Pluto dazzled with the accuracy of the **T5** NLP
    ML engine to summarize the first page of three classic books. Furthermore, he
    pioneered the **summary-to-flow** concept for text augmentation. Pluto might be
    the first to implement the **summary-to-flow** strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, there were *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is about audio augmentation, which will pose different challenges,
    but Pluto is ready for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Audio Data Augmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17990_07.xhtml#_idTextAnchor135), *Audio Data Augmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17990_08.xhtml#_idTextAnchor167), *Audio Data Augmentation with
    Spectogram*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
