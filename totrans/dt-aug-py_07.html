<html><head></head><body>
		<div id="_idContainer226">
			<h1 id="_idParaDest-135" class="chapter-number"><a id="_idTextAnchor135"/>7</h1>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Audio Data Augmentation</h1>
			<p>Similar to image and text augmentation, the objective of audio data augmentation is to extend the dataset to gain a higher accuracy forecast or prediction in a generative AI system. Audio augmentation<a id="_idIndexMarker676"/> is cost-effective and is a viable option when acquiring additional audio files is expensive <span class="No-Break">or time-consuming.</span></p>
			<p>Writing about audio augmentation methods poses unique challenges. The first is that audio is not visual like images or text. If the format is audiobooks, web pages, or mobile apps, then we play the sound, but the medium is <a id="_idIndexMarker677"/>paper. Thus, we must transform the audio signal into a visual representation. The <strong class="bold">Waveform</strong> graph, also known as the <strong class="bold">time series graph</strong>, is a <a id="_idIndexMarker678"/>standard method for representing an audio signal. You can listen to the audio in the accompanying <span class="No-Break">Python Notebook.</span></p>
			<p>In this chapter, you will learn how to write Python code to read an audio file and draw a Waveform graph from scratch. Pluto has provided a preview here so that we can discuss the components of the Waveform graph. The function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># sneak peek at a waveform plot</strong>
pluto.draw_audio(pluto.df_audio_control_data)</pre>
			<p>The following is a Waveform graph of piano scales in <span class="No-Break">D major:</span></p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/B17990_07_01.jpg" alt="Figure 7.1 – Piano scales in D major"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Piano scales in D major</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em>, Pluto <a id="_idIndexMarker679"/>drew the positive amplitude in blue and the negative amplitude in yellow in the <strong class="bold">Waveform</strong> graph. This makes the chart easier to read and prettier. The <strong class="bold">amplitude</strong> is the <a id="_idIndexMarker680"/>value of the <em class="italic">Y</em>-axis. It measures the vibration or compression and decompression of the air molecules. The higher the amplitude, the greater the air displacement. In other words, the zero amplitude value is silent, and the greater the absolute distance from zero, the louder <span class="No-Break">the sound.</span></p>
			<p>The <strong class="bold">frequency</strong>, also<a id="_idIndexMarker681"/> known as the <strong class="bold">sampling rate</strong>, is the value of the <em class="italic">X</em>-axis. The sampling rate measures how many<a id="_idIndexMarker682"/> times you recorded the amplitude value in a second. The unit for sound frequency or the sampling rate is <strong class="bold">hertz</strong> (<strong class="bold">Hz</strong>). For <a id="_idIndexMarker683"/>example, a sampling rate of 1,000 Hz or 1 <strong class="bold">kilohertz</strong> (<strong class="bold">kHz</strong>) means you record a thousand amplitude values in 1 second. In other words, you register an amplitude <a id="_idIndexMarker684"/>value for every millisecond. Thus, the higher the frequency, the more accurate the sound, and therefore, a larger sound file size. This is because there is a higher recorded amplitude value. 1 kHz is equal to <span class="No-Break">1,000 Hz.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">A human’s range of hearing is between 20 Hz and 20 kHz. Younger children can hear sounds higher than 20 kHz, while older adults can’t listen to sounds greater than 17 kHz. Deep and low music bass sound is between 20 Hz and 120 Hz, while everyday human speech ranges from 600 Hz to 5 kHz. In contrast, a canine’s hearing frequency is approximately 40 Hz to 60 kHz, which is better than a human’s hearing frequency. That is why you can’t hear an above 20 kHz <span class="No-Break">dog whistle.</span></p>
			<p><strong class="bold">Pitch</strong> is the<a id="_idIndexMarker685"/> same as <strong class="bold">frequency</strong> but <a id="_idIndexMarker686"/>from a human point of view. It refers to the loudness of the <a id="_idIndexMarker687"/>sound and is measured in <strong class="bold">decibels</strong> (<strong class="bold">dB</strong>). Thus, high pitch means <span class="No-Break">high frequency.</span></p>
			<p>dB is the unit for the degree of loudness. A rocket sound is about 165 dB, busy traffic noise is about 85 dB, human speech is about 65 dB, rainfall is about 45 dB, and zero dB <span class="No-Break">means silence.</span></p>
			<p>The standard sampling rate for MP3 and other audio formats is 22.05 kHz. The frequency of high-quality sound, also <a id="_idIndexMarker688"/>known as <strong class="bold">Compact Disk</strong> (<strong class="bold">CD</strong>) sound, is <span class="No-Break">44.1 kHz.</span></p>
			<p>When storing an audio file on a computer, <strong class="bold">bit depth</strong> is the <a id="_idIndexMarker689"/>accuracy of the amplitude value. <strong class="bold">16 bits</strong> has 65,536 levels of detail, while <strong class="bold">24 bits</strong> has 16,777,216 levels of information. The higher the bit depth, the closer the digital recording is to the analog sound and the larger the audio <span class="No-Break">file size.</span></p>
			<p>The <strong class="bold">bit rate</strong> is <a id="_idIndexMarker690"/>similar to the sampling rate, where the bit rate measures the number of bits per second. In audio processing, the playback function uses the bit rate, while the record function uses the <span class="No-Break">sampling rate.</span></p>
			<p><strong class="bold">Mono sound</strong> has <a id="_idIndexMarker691"/>one channel (<strong class="bold">1-channel</strong>), while <strong class="bold">stereo sound</strong> has <a id="_idIndexMarker692"/>two channels (<strong class="bold">2-channel</strong>). Stereo sound has one channel for the right ear and another channel for the <span class="No-Break">left ear.</span></p>
			<p>The bottom graph in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows a zoom-in Waveform chart. It shows only 100 sampling rate points, starting at the midpoint of the top Waveform graph. Upon closer inspection, the Waveform is a simple time series plot. Many data types, such as text and images, can be represented as a time series chart because Python can represent the data as a one-dimensional array, regardless of the <span class="No-Break">data type.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout"><strong class="bold">Pitch correction</strong> involves<a id="_idIndexMarker693"/> tuning a vocal’s performance in a recording so <a id="_idIndexMarker694"/>that the singer sings on key. You can use software such as <strong class="bold">Antares Auto-Tune Pro</strong> or <strong class="bold">Waves Tune Real Time</strong> to<a id="_idIndexMarker695"/> correct the highness or lowness in singing pitch. It saves time and money in terms of re-recording. Pitch correction was relatively uncommon before 1977 when <strong class="bold">Antares Audio Technology’s Auto-Tune Pitch Correcting Plug-In</strong> was released. Today, about 90% of radio, television, website, or app songs have pitch correction. <strong class="bold">Autotune</strong> is used for <a id="_idIndexMarker696"/>vocal effects, while pitch correction is for <span class="No-Break">fixing vocals.</span></p>
			<p>Since most data can be used for Waveform graphs, Pluto can draw a time series graph for the phrase “<em class="italic">Mary had a little lamb, whose fleece was white as snow. And everywhere that Mary went, the lamb was sure to go.</em>” Pluto uses the <span class="No-Break">following function:</span></p>
			<pre class="source-code">
<strong class="bold"># fun use of waveform graph</strong>
pluto.draw_time_series_text(pluto.text_marry_lamb)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B17990_07_02.jpg" alt="Figure 7.2 – Text as a time series graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Text as a time series graph</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, blue is for alphanumeric characters, while yellow is for punctuation. The <em class="italic">Y</em>-axis consists of the ASCII value of <span class="No-Break">the character.</span></p>
			<p>The conversion is straightforward because each letter is encoded as <a id="_idIndexMarker697"/>an <strong class="bold">ASCII</strong> value, such as “A” as 65, “B” as 66, and so on. Similarly, an image composed of a three-dimensional array (width, height, and depth) has an RGB value. The result of collapsing the depth dimension by multiplying the RGB value is between zero and 16,581,375. Flatten the remaining two-dimensional array into a one-dimensional array and plot it as a time <span class="No-Break">series graph.</span></p>
			<p>This chapter will cover audio augmentation using <strong class="bold">Waveform</strong> transformation, and in particular, the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Standard audio <span class="No-Break">augmentation techniques</span></li>
				<li><span class="No-Break">Filters</span></li>
				<li>Audio <span class="No-Break">augmentation libraries</span></li>
				<li>Real-world <span class="No-Break">audio datasets</span></li>
				<li>Reinforcing <span class="No-Break">your learning</span></li>
			</ul>
			<p>Let’s begin by discussing common audio <span class="No-Break">augmentation methods.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Standard audio augmentation techniques</h1>
			<p>Similar<a id="_idIndexMarker698"/> to image augmentation in <a href="B17990_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, various audio libraries provide many more functions than are necessary for augmentation. Therefore, we will only cover some of the methods available in the chosen <span class="No-Break">audio library.</span></p>
			<p>In image augmentation, the term <strong class="bold">safe level</strong> is <a id="_idIndexMarker699"/>defined as not altering or distorting the original image beyond an acceptable level. There is no standard terminology for deforming the original audio signal beyond a permissible point. Thus, the term <strong class="bold">safe</strong> or <strong class="bold">true</strong> will be used interchangeably to denote a limit point for the <span class="No-Break">audio signal.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: all audio files are represented as numbers in time series format. Thus, can you create a statistically valid augmentation method that does not consider human hearing perception? In other words, use math to manipulate a statistically valid number array, but never listen to the before and after effects. After all, audio augmentation aims to have more data for enhancing the AI prediction, which does not comprehend human speech or good music from <span class="No-Break">bad music.</span></p>
			<p>The following functions are commonly used for <span class="No-Break">audio augmentation:</span></p>
			<ul>
				<li><span class="No-Break">Time stretching</span></li>
				<li><span class="No-Break">Time shifting</span></li>
				<li><span class="No-Break">Pitch scaling</span></li>
				<li><span class="No-Break">Noise injection</span></li>
				<li><span class="No-Break">Polarity inversion</span></li>
			</ul>
			<p>Let’s start with <span class="No-Break">time stretching.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>Time stretching</h2>
			<p><strong class="bold">Time stretching</strong> involves <a id="_idIndexMarker700"/>lengthening or shortening <a id="_idIndexMarker701"/>the duration of an audio signal. It is done without changing the pitch level. For example, in human speech, you would slow down and drag out your words or speed up and talk like a chipmunk <span class="No-Break">cartoon character.</span></p>
			<p>What is the <strong class="bold">safe</strong> level for <strong class="bold">time stretching</strong>? It depends on the type of audio and the goal of AI prediction. In general, you can speed up or slow down human speech and it can still be understood. But if the goal is to predict the speaker’s name, then there is a small time stretching range you can apply to the speech and stay <strong class="bold">true</strong> to the speaker’s <span class="No-Break">talking style.</span></p>
			<p>Music, on the other hand, is generally considered <strong class="bold">unsafe</strong> for time stretching. Changing the tempo beat of a music segment alters the music beyond the true <span class="No-Break">musician’s intention.</span></p>
			<p>Environmental or nature sounds are generally <strong class="bold">safe</strong> for time stretching within an acceptable <span class="No-Break">safe range.</span></p>
			<p>This augmentation technique, and all other methods, are covered in the Python Notebook; hence, it is easier to grasp the effect by listening to the original and the <span class="No-Break">augmented sound.</span></p>
			<p>Similar to time stretching is <span class="No-Break">time shifting.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>Time shifting</h2>
			<p><strong class="bold">Time shifting</strong> involves<a id="_idIndexMarker702"/> moving an audio segment <a id="_idIndexMarker703"/>forward or backward. For example, if you want a more dramatic pause between a speaker’s announcement and the audience’s applause, you can timeshift the applauses a few <span class="No-Break">seconds forward.</span></p>
			<p>Timeshift with rollover means the last sound will be added back to the beginning. Without rollover, the audio will have silence for the beginning or end, depending on whether you’re shifting forward <span class="No-Break">or backward.</span></p>
			<p>For example, suppose the goal of the AI prediction is to identify gunshots in a city to alert the police. In that case, timeshift with rollover is an acceptable <strong class="bold">safe</strong> augmentation technique. Another example of good use of timeshift with rollover is looped <span class="No-Break">background music.</span></p>
			<p>Human <a id="_idIndexMarker704"/>speech or music is typically <strong class="bold">unsafe</strong> for time shifting. This is because the sequential order is essential for it to stay <strong class="bold">true</strong> to the <span class="No-Break">original audio.</span></p>
			<p>Moving away from time, pitch shifting or pitch scaling is another <span class="No-Break">augmented parameter.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Pitch shifting</h2>
			<p><strong class="bold">Pitch shifting</strong> or <strong class="bold">pitch scaling</strong> changes <a id="_idIndexMarker705"/>the frequency of sound without affecting the speed or time shift. For example, a man has a <a id="_idIndexMarker706"/>lower voice pitch than a woman. Increasing the pitch level in a voice recording <a id="_idIndexMarker707"/>might make a man sound like <span class="No-Break">a woman.</span></p>
			<p>Pitch shifting should be used cautiously when augmenting human speech, music, environment, and nature audio files. The <strong class="bold">safe</strong> level can change drastically for the same dataset, depending on the AI <span class="No-Break">prediction’s objective.</span></p>
			<p>For example, the recordings of daily meadow sounds can be used to count how many birds visit the meadow a day, or an AI can predict what kinds of birds dwell in the field. The pitch-shifting <strong class="bold">safe</strong> range for counting birds is higher than for identifying birds. Applying pitch shifting to bird songs may inadvertently make one bird sound like <span class="No-Break">other birds.</span></p>
			<p>Another pitch alternation method is <span class="No-Break">polarity inversion.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>Polarity inversion</h2>
			<p><strong class="bold">Polarity inversion</strong> involves<a id="_idIndexMarker708"/> switching the amplitude<a id="_idIndexMarker709"/> value from positive to negative and vice versa. Mathematically, it multiplies the amplitude by a negative value. Graphically, it alters the color blue and makes it yellow and vice versa in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<p>To most humans, the playback after polarity inversion sounds the same as the original audio. It is most beneficial for ML when used with the phase-awareness model. There is no <strong class="bold">safe</strong> range because it is either used or <span class="No-Break">not used.</span></p>
			<p>The following augmentation is about adding noise to an <span class="No-Break">audio file.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>Noise injection</h2>
			<p><strong class="bold">Noise injection</strong> adds signal<a id="_idIndexMarker710"/> noise<a id="_idIndexMarker711"/> to an audio file. The effect of adding noise is that the augmented sound appears as though it consists of pops and crackles. The five types of <a id="_idIndexMarker712"/>noise<a id="_idIndexMarker713"/> typically used in <a id="_idIndexMarker714"/>audio <a id="_idIndexMarker715"/>augmentation<a id="_idIndexMarker716"/> are <strong class="bold">background noise</strong>, <strong class="bold">Gaussian</strong>, <strong class="bold">random</strong>, <strong class="bold">signal-to-noise ratio</strong> (<strong class="bold">SNR</strong>), and <strong class="bold">short </strong><span class="No-Break"><strong class="bold">burst noise</strong></span><span class="No-Break">.</span></p>
			<p>How much noise or the <strong class="bold">safe</strong> level depends on the AI project’s objective and the recording. Sometimes, you might have to employ a domain expert to attain a <span class="No-Break"><strong class="bold">safe</strong></span><span class="No-Break"> level.</span></p>
			<p>Many more techniques could be classified as commonly used audio augmentations, such as clip, gain transition, normalize, padding, or reverse, but let’s move on and look at filters <span class="No-Break">and masking.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Filters</h1>
			<p>Audio filters<a id="_idIndexMarker717"/> help eliminate<a id="_idIndexMarker718"/> unwanted interference or noise from an audio recording. The result is to improve the tone and playback of human speech, music, nature, and <span class="No-Break">environmental recordings.</span></p>
			<p>The audio filter changes frequency by <strong class="bold">increasing</strong>, <strong class="bold">boosting</strong>, or <strong class="bold">amplifying</strong> a range of frequencies. A filter could also <strong class="bold">decrease</strong>, <strong class="bold">delete</strong>, <strong class="bold">cut</strong>, <strong class="bold">attenuate</strong>, or <strong class="bold">pass</strong> a frequency range. For example, using a low-pass filter, you could remove the traffic noise from a recording of two people talking in <span class="No-Break">a city.</span></p>
			<p>In particular, we will cover the <span class="No-Break">following filters:</span></p>
			<ul>
				<li><span class="No-Break">Low-pass filter</span></li>
				<li><span class="No-Break">High-pass filter</span></li>
				<li><span class="No-Break">Band-pass filter</span></li>
				<li><span class="No-Break">Low-shelf filter</span></li>
				<li><span class="No-Break">High-shelf filter</span></li>
				<li><span class="No-Break">Band-stop filter</span></li>
				<li><span class="No-Break">Peak filter</span></li>
			</ul>
			<p>Let’s start with the low <span class="No-Break">pass filter.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>Low-pass filter</h2>
			<p>The <strong class="bold">low-pass filter</strong> cuts or <a id="_idIndexMarker719"/>deletes low-frequency sounds, such as traffic<a id="_idIndexMarker720"/> noise, machine engine rumbles, or <span class="No-Break">elephant calls.</span></p>
			<p>Typically, the minimum cut-off frequency is 150 Hz, the maximum cut-off is 7.5 kHz, the minimum roll-off is 12 dB, and the maximum roll-off is <span class="No-Break">24 dB.</span></p>
			<p>Here is a fun fact: elephant calls are lower than 20 Hz or into the infrasound range. The next filter we’ll cover is the high <span class="No-Break">pass filter.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>High-pass filter</h2>
			<p>Similar to the <a id="_idIndexMarker721"/>low-pass filter, the <strong class="bold">high-pass filter</strong> cuts high-frequency sound, such as whistling, babies crying, nail scratching, or <a id="_idIndexMarker722"/><span class="No-Break">bell ringing.</span></p>
			<p>Typically, the minimum and maximum cut-off frequencies are 20 Hz and 2.4 kHz, and the minimum and maximum roll-offs are 12 dB and 24 <span class="No-Break">dB, respectively.</span></p>
			<p>Fun fact: a human can whistle around 3 to 4 kHz. There is one more pass filter we need to look at: the ban <span class="No-Break">pass filter.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>Band-pass filter</h2>
			<p>The <strong class="bold">band-pass filter</strong> limits the<a id="_idIndexMarker723"/> sound wave to a range of frequencies. In other <a id="_idIndexMarker724"/>words, it combines the low and high-band filters. For example, a band-pass filter can make it clearer to listen to a recording of two friends’ conversations in a busy Paris outdoor restaurant. Similarly, it can be used to isolate bird song recordings in a noisy <span class="No-Break">Amazon jungle.</span></p>
			<p>Typically, the minimum and maximum center frequencies are 200 Hz and 4 kHz, the minimum and maximum bandwidth fractions are 0.5 and 1.99, and the minimum and maximum roll-offs are 12 dB and 24 <span class="No-Break">dB, respectively.</span></p>
			<p>Now, let’s move on from pass filters to <span class="No-Break">shelf filters.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>Low-shelf filter</h2>
			<p><strong class="bold">Shelf filtering</strong> is also known as <strong class="bold">shelf equalization</strong>. In particular, the <strong class="bold">low-shelf filter</strong> boosts or cuts<a id="_idIndexMarker725"/> the frequencies at the lower end of the spectrum. For example, you <a id="_idIndexMarker726"/>can use a low-shelf filter to reduce the bass in a heavy <span class="No-Break">metal </span><span class="No-Break"><a id="_idIndexMarker727"/></span><span class="No-Break">song.</span></p>
			<p>Usually, the minimum and maximum center frequencies are 50 Hz and 4 kHz, and the minimum and maximum gain are -18 dB to 18 <span class="No-Break">dB, respectively.</span></p>
			<p>The next technique is the <span class="No-Break">high-shelf filter.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor148"/>High-shelf filter</h2>
			<p>Similarly, a <strong class="bold">high-shelf filter</strong> increases or<a id="_idIndexMarker728"/> decreases the frequencies’ amplitude at the <a id="_idIndexMarker729"/>higher end of the spectrum. For example, you can use a high-shelf filter to brighten a <span class="No-Break">music recording.</span></p>
			<p>Commonly, the minimum and maximum center frequencies are 300 Hz and 7.5 kHz, and the minimum and maximum gains are -18 dB and 18 <span class="No-Break">dB, respectively.</span></p>
			<p>The band-stop filter is the next technique <span class="No-Break">we’ll cover.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>Band-stop filter</h2>
			<p>The <strong class="bold">band-stop filter</strong> is also <a id="_idIndexMarker730"/>known <a id="_idIndexMarker731"/>as a <strong class="bold">ban-reject filter</strong> or <strong class="bold">notch filter</strong>. It <a id="_idIndexMarker732"/>deletes <a id="_idIndexMarker733"/>frequencies between two cut-off points or on either side of the range. In addition, it uses low and high-pass filters under the hood. For example, a band-stop filter can remove unwanted spikes and noises from a backyard music <span class="No-Break">session jam.</span></p>
			<p>Typically, the<a id="_idIndexMarker734"/> minimum and maximum center frequencies are 200 Hz and 4 kHz, the minimum and maximum bandwidth fractions are 0.5 and 1.99, and the minimum and maximum roll-offs are 12 dB<a id="_idIndexMarker735"/> and 24 <span class="No-Break">dB, respectively.</span></p>
			<p>The peak filter is the last audio augmentation technique that will be covered in <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>Peak filter</h2>
			<p>The <strong class="bold">peak</strong> or <strong class="bold">bell filter</strong> is the <a id="_idIndexMarker736"/>opposite of the band-stop filter. In other words, it <a id="_idIndexMarker737"/>boosts shelf filters with a narrow band and higher gain signal or allows a boost or cut around a <span class="No-Break">center</span><span class="No-Break"><a id="_idIndexMarker738"/></span><span class="No-Break"> frequency.</span></p>
			<p>Typically, the minimum and maximum center frequencies are 50 Hz and 7.5 kHz, and the minimum and maximum gains are -24 dB and 24 <span class="No-Break">dB, respectively.</span></p>
			<p>Many methods are available in audio augmentation libraries. Thus, the next step is to select one or two audio augmentation libraries for Pluto’s <span class="No-Break">wrapper functions.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor151"/>Audio augmentation libraries</h1>
			<p>There are many commercia<a id="_idIndexMarker739"/>l and open source audio data augmentation libraries. In <a id="_idIndexMarker740"/>this chapter, we will focus on open source libraries available on <strong class="bold">GitHub</strong>. Some <a id="_idIndexMarker741"/>libraries are more robust than others, and some focus on a particular subject, such as human speech. Pluto will write wrapper functions using the libraries provided to do the heavy lifting; thus, you can select more than one library in your project. If a library is implemented in the <strong class="bold">CPU</strong>, it may not be suitable for dynamic data augmenting during the ML training cycle because it will slow down the process. Instead, choose a library that can run on the <strong class="bold">GPU</strong>. Choose a robust and easy-to-implement library to learn new audio augmentation techniques or output the augmented data on local or cloud <span class="No-Break">disk space.</span></p>
			<p>The well-known open source libraries for audio augmentation are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Librosa</strong> is an open <a id="_idIndexMarker742"/>source Python library for music and audio analysis. It was made available in 2015 and has long been a popular choice. Many other audio processing and<a id="_idIndexMarker743"/> augmentation libraries use <a id="_idIndexMarker744"/>Librosa’s functions as building blocks. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/librosa/librosa"><span class="No-Break">https://github.com/librosa/librosa</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Audiomentations</strong> is a <a id="_idIndexMarker745"/>Python library specifically for audio data augmentation. Its<a id="_idIndexMarker746"/> key benefit is its robustness and easy project integration. It is cited in many Kaggle <a id="_idIndexMarker747"/>competition winners. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/iver56/audiomentations"><span class="No-Break">https://github.com/iver56/audiomentations</span></a><span class="No-Break">.</span></li>
				<li>Facebook or Meta research <a id="_idIndexMarker748"/>published <strong class="bold">Augly</strong> as an open source Python library for image and audio augmentation. The<a id="_idIndexMarker749"/> goal is to provide specific data augmentations for real-life projects. It can be found on<a id="_idIndexMarker750"/> GitHub <span class="No-Break">at </span><a href="https://github.com/facebookresearch/AugLy/tree/main/augly/audio"><span class="No-Break">https://github.com/facebookresearch/AugLy/tree/main/augly/audio</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Keras</strong> is a Python library<a id="_idIndexMarker751"/> for audio and music signal preprocessing. It implements <a id="_idIndexMarker752"/>frequency conversions and data augmentation using <strong class="bold">GPU</strong> preprocessing. It can be<a id="_idIndexMarker753"/> found on GitHub <span class="No-Break">at </span><a href="https://github.com/keunwoochoi/kapre"><span class="No-Break">https://github.com/keunwoochoi/kapre</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Nlpaug</strong> is a Python<a id="_idIndexMarker754"/> library that’s versatile for both language and audio data augmentation. <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> used Nlpaug for text augmentation, but in this chapter, we <a id="_idIndexMarker755"/>will use the audio library. It can be found<a id="_idIndexMarker756"/> on GitHub <span class="No-Break">at </span><a href="https://github.com/makcedward/nlpaug"><span class="No-Break">https://github.com/makcedward/nlpaug</span></a><span class="No-Break">.</span></li>
				<li>Spotify’s Audio Intelligence <a id="_idIndexMarker757"/>Lab published<a id="_idIndexMarker758"/> the <strong class="bold">Pedalboard</strong> Python library. The goal is to enable studio-quality <a id="_idIndexMarker759"/>audio effects for ML. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/spotify/pedalboard"><span class="No-Break">https://github.com/spotify/pedalboard</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Pydiogment</strong> is a<a id="_idIndexMarker760"/> Python library that aims to simplify audio augmentation. It is easy<a id="_idIndexMarker761"/> to use but less robust than <a id="_idIndexMarker762"/>other audio augmentation libraries. It can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/SuperKogito/pydiogment"><span class="No-Break">https://github.com/SuperKogito/pydiogment</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Torch-augmentations</strong> is an<a id="_idIndexMarker763"/> implementation of the <strong class="bold">Audiomentations</strong> library<a id="_idIndexMarker764"/> for <strong class="bold">GPU</strong> processing. It <a id="_idIndexMarker765"/>can be found on GitHub <span class="No-Break">at </span><a href="https://github.com/asteroid-team/torch-audiomentations"><span class="No-Break">https://github.com/asteroid-team/torch-audiomentations</span></a><span class="No-Break">.</span></li>
			</ul>
			<p class="callout-heading">Fun fact</p>
			<p class="callout"><strong class="bold">Audiomentations</strong> library version 0.28.0 consists of 36 augmentation functions, <strong class="bold">Librosa</strong> library version 0.9.2 consists of over 400 methods, and<a id="_idIndexMarker766"/> the <strong class="bold">Pydiogment</strong> library’s latest update (July 2020) consists of 14 <span class="No-Break">augmentation methods.</span></p>
			<p>Pluto will primarily<a id="_idIndexMarker767"/> use the <strong class="bold">Audiomentations</strong> and <strong class="bold">Librosa</strong> libraries to demonstrate the concepts <a id="_idIndexMarker768"/>we’ve mentioned in Python code. But first, we will download Pluto and use him to download real-world audio datasets from the <span class="No-Break"><em class="italic">Kaggle</em></span><span class="No-Break"> website.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor152"/>Real-world audio datasets</h1>
			<p>By now, you should be<a id="_idIndexMarker769"/> familiar with downloading Pluto and real-world datasets from the <em class="italic">Kaggle</em> website. We chose to download Pluto from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> because the image augmentation functions shown in <em class="italic">Chapters 3</em> and <em class="italic">4</em>, and the text augmentation techniques shown in <em class="italic">Chapters 5</em> and <em class="italic">6</em>, are not beneficial for <span class="No-Break">audio augmentation.</span></p>
			<p>The three real-world audio datasets we will use are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <em class="italic">Musical Emotions Classification</em> (<em class="italic">MEC</em>) real-world audio dataset<a id="_idIndexMarker770"/> from Kaggle contains 2,126 songs separated into <strong class="bold">train</strong> and <strong class="bold">test</strong> folders. They are instrumental music, and <a id="_idIndexMarker771"/>the goal is to predict <strong class="bold">happy</strong> or <strong class="bold">sad</strong> music. Each piece is about 9 to 10 minutes in length and is in <strong class="bold">*.wav</strong> format. It was published in 2020 and is available to the public. Its license is <strong class="bold">Attribution-ShareAlike 4.0 International (CC BY-SA </strong><span class="No-Break"><strong class="bold">4.0)</strong></span><span class="No-Break">: </span><a href="https://creativecommons.org/licenses/by-sa/4.0/"><span class="No-Break">https://creativecommons.org/licenses/by-sa/4.0/</span></a><span class="No-Break">.</span></li>
				<li>The <em class="italic">Crowd Sourced Emotional Multimodal Actors Dataset</em> (<em class="italic">CREMA-D</em>) real-world audio <a id="_idIndexMarker772"/>dataset <a id="_idIndexMarker773"/>from Kaggle contains 7,442 original clips from 91 actors. The actors are 48 <strong class="bold">males</strong> and 43 <strong class="bold">females</strong> between 20 to 74 years old, and their ethnicities are <strong class="bold">African American</strong>, <strong class="bold">Asian</strong>, <strong class="bold">Caucasian</strong>, <strong class="bold">Hispanic</strong>, and <strong class="bold">Unspecified</strong>. In addition, the spoken phrases represent six different emotions. They are <strong class="bold">anger</strong>, <strong class="bold">disgust</strong>, <strong class="bold">fear</strong>, <strong class="bold">happy</strong>, <strong class="bold">neutral</strong>, and <strong class="bold">sad</strong>. There is no set goal for the datasets, but you can use them to predict age, sex, ethnicity, or emotions. It was published in 2019 and is available to the public. Its license is <strong class="bold">Open Data Commons Attribution License (ODC-By) </strong><span class="No-Break"><strong class="bold">v1.0</strong></span><span class="No-Break">: </span><a href="https://opendatacommons.org/licenses/by/1-0/index.html"><span class="No-Break">https://opendatacommons.org/licenses/by/1-0/index.html</span></a><span class="No-Break">.</span></li>
				<li>The <em class="italic">urban_sound_8k</em> (<em class="italic">US8K</em>) real-world dataset<a id="_idIndexMarker774"/> from Kaggle contains 8,732 labeled sound excerpts<a id="_idIndexMarker775"/> from an urban setting. Each clip is between 2 to 4 seconds, and the classification is <strong class="bold">Air conditioner, Car horn, Children playing, Dogs barking, Drilling, Engine idling, Gunshots, Jackhammers, Sirens, and Street music</strong>. It was published in 2021 and is available to the public. Its license is <strong class="bold">CC0 1.0 Universal (CC0 1.0) Public Domain </strong><span class="No-Break"><strong class="bold">Dedication</strong></span><span class="No-Break">: </span><a href="https://creativecommons.org/publicdomain/zero/1.0/"><span class="No-Break">https://creativecommons.org/publicdomain/zero/1.0/</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>The three audio datasets – music, human speech, and environmental sound – represent the typical sounds you <span class="No-Break">hear daily.</span></p>
			<p>The following four steps are the same in every chapter. Review <em class="italic">Chapters 2</em> and <em class="italic">3</em> if you need clarification. The steps are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Retrieve the Python Notebook <span class="No-Break">and Pluto.</span></li>
				<li>Download <span class="No-Break">real-world data.</span></li>
				<li>Load the data <span class="No-Break">into pandas.</span></li>
				<li>Listen to and view <span class="No-Break">the audio.</span></li>
			</ol>
			<p>Let’s begin by downloading Pluto in the <span class="No-Break">Python Notebook.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>Python Notebook and Pluto</h2>
			<p>Start by <a id="_idIndexMarker776"/>loading the <strong class="source-inline">data_augmentation_with_python_chapter_7.ipynb</strong> file into Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this point onward, the code snippets will be from the Python Notebook, which contains the <span class="No-Break">complete functions.</span></p>
			<p>The next step is to clone the repository. We will reuse the code from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. The <strong class="source-inline">!git</strong> and <strong class="source-inline">%run</strong> statements are used to start <span class="No-Break">up Pluto:</span></p>
			<pre class="source-code">
<strong class="bold"># clone the GitHub repo.</strong>
f='https://github.com/PacktPublishing/Data-Augmentation-with-Python'
!git clone {f}
<strong class="bold"># instantiate Pluto</strong>
%run 'Data-Augmentation-with-Python/pluto/pluto_chapter_2.py'</pre>
			<p>The output will be as follows <span class="No-Break">or similar:</span></p>
			<pre class="console">
---------------------------- : ----------------------------
            Hello from class : &lt;class '__main__.PacktDataAug'&gt; Class: PacktDataAug
                   Code name : Pluto
                   Author is : Duc Haba
---------------------------- : ----------------------------</pre>
			<p>We need to do one more check to ensure Pluto is loaded satisfactorily. The following command asks Pluto to state <span class="No-Break">his status:</span></p>
			<pre class="source-code">
<strong class="bold"># How are you doing Pluto?</strong>
pluto.say_sys_info()</pre>
			<p>The output <a id="_idIndexMarker777"/>will be as follows or similar, depending on <span class="No-Break">your system:</span></p>
			<pre class="console">
---------------------------- : ----------------------------
                 System time : 2022/12/30 19:17
                    Platform : linux
     Pluto Version (Chapter) : 2.0
             Python (3.7.10) : actual: 3.8.16 (default, Dec  7 2022, 01:12:13) [GCC 7.5.0]
            PyTorch (1.11.0) : actual: 1.13.0+cu116
              Pandas (1.3.5) : actual: 1.3.5
                 PIL (9.0.0) : actual: 7.1.2
          Matplotlib (3.2.2) : actual: 3.2.2
                   CPU count : 2
                  CPU speed : NOT available
---------------------------- : ----------------------------</pre>
			<p>Next, Pluto will download the <span class="No-Break">audio dataset.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>Real-world data and pandas</h2>
			<p>Pluto has<a id="_idIndexMarker778"/> downloaded the real-world music dataset, the MEC, using the <strong class="source-inline">fetch_kaggle_dataset(url)</strong> function from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. He found that the dataset consists of a comma-separated variable (CSV) header file. Thus, he used the <strong class="source-inline">fetch_df(fname)</strong> function to import it <span class="No-Break">into pandas:</span></p>
			<pre class="source-code">
<strong class="bold"># download from Kaggle</strong>
url = 'https://www.kaggle.com/datasets/kingofarmy/musical-emotions-classification'
pluto.fetch_kaggle_dataset(url)
<strong class="bold"># import to Pandas</strong>
f = 'kaggle/musical-emotions-classification/Train.csv'
pluto.df_music_data = pluto.fetch_df(f)
<strong class="bold"># out a few header record</strong>
Pluto.df_music_data.head(3)</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B17990_07_03.jpg" alt="Figure 7.3 – Music (MEC) top 3 records"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Music (MEC) top 3 records</p>
			<p>The <strong class="bold">ImageID</strong> in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> is not a full path name, so Pluto writes two quick Python functions to append the full path name. These methods are the <strong class="source-inline">_append_music_full_path()</strong> and <strong class="source-inline">fetch_music_full_path()</strong> helper functions. The<a id="_idIndexMarker779"/> key code lines are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># helper function snippet</strong>
y = re.findall('([a-zA-Z ]*)\d*.*', x)[0]
return (f'kaggle/musical-emotions-classification/Audio_Files/Audio_Files/Train/{y}/{x}')
<strong class="bold"># main function snippet</strong>
df['fname'] = df.ImageID.apply(self._append_music_full_path)</pre>
			<p>The function’s code can be found in the Python Notebook. The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B17990_07_04.jpg" alt="Figure 7.4 – Music (MEC) top 3 records revised"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Music (MEC) top 3 records revised</p>
			<p>The next<a id="_idIndexMarker780"/> real-world dataset from Kaggle is for human speech (<strong class="bold">CREMA-D</strong>). Pluto must download and import it into pandas using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># download the dataset</strong>
url = 'https://www.kaggle.com/datasets/ejlok1/cremad'
pluto.fetch_kaggle_dataset(url)
<strong class="bold"># import to Pandas and print out header record</strong>
f = 'kaggle/cremad/AudioWAV'
pluto.df_voice_data = pluto.make_dir_dataframe(f)
pluto.df_voice_data.head(3)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B17990_07_05.jpg" alt="Figure 7.5 – Voice (CREMA-D) top 3 records revised"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Voice (CREMA-D) top 3 records revised</p>
			<p>The third audio dataset from Kaggle is for urban sound (<strong class="bold">US8K</strong>). Incidentally, Kaggle consists of about 1,114 real-world audio datasets as of December 2022. Pluto must download and import it into pandas using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># download dataset from Kaggle website</strong>
url='https://www.kaggle.com/datasets/rupakroy/urban-sound-8k'
pluto.fetch_kaggle_dataset(url)
<strong class="bold"># import to Pandas and print header records</strong>
f = 'kaggle/urban-sound-8k/UrbanSound8K/UrbanSound8K/audio'
pluto.df_sound_data = pluto.make_dir_dataframe(f)
pluto.df_sound_data.head(3)</pre>
			<p>The <a id="_idIndexMarker781"/>output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B17990_07_06.jpg" alt="Figure 7.6 – Urban sound (US8K) top 3 records revised"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Urban sound (US8K) top 3 records revised</p>
			<p>Lastly, the <strong class="bold">audio control clip</strong> is a <a id="_idIndexMarker782"/>piano scale in D-Major. Pluto created it using a MIDI keyboard program. It plays from <em class="italic">D, E, F#, G, A, B, C#</em>, and scales down to <em class="italic">C#, B, A, G, F#, E, D</em>. When Pluto is not sure if the audio augmentation is working on the music, voice, or urban sound, he will use the control clip to verify the effect. The file can be found in the <strong class="source-inline">pluto_data</strong> directory; he stored the control clip in the <span class="No-Break"><strong class="source-inline">pluto.audio_control_dmajor</strong></span><span class="No-Break"> variable.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to search for and download an additional audio dataset from the <em class="italic">Kaggle</em> website or your project. It is more meaningful if you work with the data that matters to you. So long as you download and import it into pandas, all the augmentation wrapper functions will work the same for your audio files. Hint: use Pluto’s <strong class="source-inline">fetch_kaggle_dataset()</strong> and <span class="No-Break"><strong class="source-inline">fetch_df()</strong></span><span class="No-Break"> functions.</span></p>
			<p>With that, Pluto has downloaded the three real-world audio datasets. The next step is to play the audio and view the audio <span class="No-Break"><strong class="bold">Waveform</strong></span><span class="No-Break"> graph.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Listening and viewing</h2>
			<p>Pluto has <a id="_idIndexMarker783"/>written three new functions to play the audio and <a id="_idIndexMarker784"/>display the <strong class="bold">Waveform</strong> graph. The first is the <strong class="source-inline">_draw_area_with_neg()</strong> helper method, which displays the area graph for positive and negative numbers in the same dataset. Incidentally, the pandas and <strong class="bold">Matplotlib</strong> area graphs can only show positive values. The essential code line for this function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># draw area code snippet fill top/positive/blue section</strong>
pic.fill_between(
    i, xzero, ndata, where=(ndata &gt;= xzero),
    interpolate=True, color=tcolor, alpha=alpha,
    label="Positive"
)
<strong class="bold"># fill bottom/negative/yellow section</strong>
pic.fill_between(
    i, xzero, ndata, where=(ndata &lt; xzero),
    interpolate=True, color=bcolor, alpha=alpha,
    label="Negative"
)</pre>
			<p>The full function code can be found in the Python Notebook. The next helper function is <strong class="source-inline">_draw_audio()</strong>. Its main objectives are loading or reading the audio file using the <strong class="bold">Librosa</strong> library, drawing the two <strong class="bold">Waveform</strong> graphs, and displaying the play audio button. Pandas has the same filename that it had when fetching the audio datasets. The key code lines for the function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet, load/read and import to Pandas DataFrame</strong>
data_amp, sam_rate = librosa.load(samp.fname[0], mono=True)
<strong class="bold"># draw the Waveform graphs</strong>
self._draw_area_with_neg(data_amp,pic[0])
<strong class="bold"># draw the zoom in Waveform plot</strong>
self._draw_area_with_neg(data_amp[mid:end],pic[1])
<strong class="bold"># display the play-audio button</strong>
display(IPython.display.Audio(data_amp, rate=sam_rate))</pre>
			<p>The <a id="_idIndexMarker785"/>entirety of this function can be found in the Python Notebook. The <strong class="source-inline">draw_audio()</strong> method invokes the two helper functions. Additionally, it<a id="_idIndexMarker786"/> selects a random audio file from the pandas DataFrame. Thus, Pluto runs the command repeatedly to listen to and view a different audio file from the <span class="No-Break">real-world dataset.</span></p>
			<p>Pluto can display a music clip from the <strong class="bold">MEC</strong> dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># display the play button the waveform plot</strong>
pluto.draw_audio(pluto.df_music_data)</pre>
			<p>The audio play button is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B17990_07_07.jpg" alt="Figure 7.7 – Audio play button"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Audio play button</p>
			<p>The <strong class="bold">Waveform</strong> graphs are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B17990_07_08.jpg" alt="Figure 7.8 – Music waveform graph (Happy36521)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Music waveform graph (Happy36521)</p>
			<p>The <a id="_idIndexMarker787"/>audio play button in <em class="italic">Figures 7.7</em> and <em class="italic">7.8</em> (<strong class="source-inline">Happy36521.wav</strong>) will <a id="_idIndexMarker788"/>play the instrumental music with the flute, drum, <span class="No-Break">and guitar.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto names the function <strong class="source-inline">draw_audio()</strong> and not <strong class="source-inline">play_audio()</strong> because this book needs a Waveform graph, and to listen to the audio, you have to go to the Python Notebook and click on the play button shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.7</em>. Like all wrapper functions, you can repeatedly run the <strong class="source-inline">draw_audio()</strong> method to see and listen to different audio files from <span class="No-Break">the datasets.</span></p>
			<p>Pluto displays a human speech clip from the <strong class="bold">CREMA-D</strong> dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># display the play button the waveform plot</strong>
pluto.draw_audio(pluto.df_voice_data)</pre>
			<p>The audio play button’s output is not displayed here because it looks the same as in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.7</em>. The result of the <strong class="bold">Waveform</strong> graph is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B17990_07_09.jpg" alt="Figure 7.9 – Human speech waveform graph (1078_TIE_HAP_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Human speech waveform graph (1078_TIE_HAP_XX)</p>
			<p>The <a id="_idIndexMarker789"/>audio of <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.9</em> (<strong class="source-inline">1078_TIE_HAP_XX.wav</strong>) is a woman speaking the phrase: <em class="italic">that is exactly what happens</em>. She sounds happy <a id="_idIndexMarker790"/><span class="No-Break">and middle-aged.</span></p>
			<p>Pluto displays an urban sound clip from the <strong class="bold">US8K</strong> dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># display the play button the waveform plot</strong>
pluto.draw_audio(pluto.df_sound_data)</pre>
			<p>The result of the <strong class="bold">Waveform</strong> graph is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B17990_07_10.jpg" alt="Figure 7.10 – Urban sound waveform graph (119455-5-0-7)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Urban sound waveform graph (119455-5-0-7)</p>
			<p>The <a id="_idIndexMarker791"/>audio for <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.10</em> (<strong class="source-inline">119455-5-0-7.wav</strong>) is the sound <a id="_idIndexMarker792"/><span class="No-Break">of jackhammers.</span></p>
			<p>With that, we’ve discussed various audio augmentation concepts, selected audio libraries, downloaded Pluto, and asked him to fetch real-world datasets for music, human speech, and urban sounds. Pluto now also plays the audio and displays the Waveform graph for <span class="No-Break">each file.</span></p>
			<p>The next step is writing Python wrapper code from scratch to gain a deeper understanding of the audio augmentation techniques <span class="No-Break">we’ve covered.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor156"/>Reinforcing your learning</h1>
			<p>The key objectives of the <strong class="source-inline">_audio_transform()</strong> helper function are selecting a random clip, performing the augmentation using the Audiomentations library function, displaying the WaveForm graph using the <strong class="source-inline">_fetch_audio_data()</strong> and <strong class="source-inline">_draw_audio()</strong> helper functions, and showing the audio play button. The key code lines are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet, use Pandas to select a random/sample record</strong>
p = df.sample(dsize)
<strong class="bold"># fetch the audio data</strong>
data_amp, sam_rate, fname = self._fetch_audio_data(lname)
<strong class="bold"># do the transformation</strong>
xaug = xtransform(data_amp, sample_rate=sam_rate)
<strong class="bold"># display the Waveform graphs and the audio play button</strong>
self._draw_audio(xaug, sam_rate, title + ' Augmented: ' + fname)
display(IPython.display.Audio(xaug, rate=sam_rate))</pre>
			<p>The full function’s code can be found in the Python Notebook. Pluto will write the Python wrapper functions for audio augmentation in the same order as previously discussed. In particular, they are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Time shifting</span></li>
				<li><span class="No-Break">Time stretching</span></li>
				<li><span class="No-Break">Pitch scaling</span></li>
				<li><span class="No-Break">Noise injection</span></li>
				<li><span class="No-Break">Polarity inversion</span></li>
			</ul>
			<p>Let’s start with <span class="No-Break"><strong class="bold">time shifting</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>Time shifting</h2>
			<p>The definition <a id="_idIndexMarker793"/>and key code lines for the <strong class="source-inline">play_aug_time_shift()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_time_shift(self, df,
  min_fraction=-0.2,
  max_fraction=0.8,
  rollover=True,
  title='Time Shift'):
<strong class="bold"># code snippet for time shift</strong>
xtransform = audiomentations.Shift(
  min_fraction = min_fraction,
  max_fraction = max_fraction,
  rollover = rollover,
  p=1.0)</pre>
			<p>The full function’s code can be found in the Python Notebook. Pluto tests the time shift wrapper function with the audio control file <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using time shift</strong>
pluto.play_aug_time_shift(
  pluto.audio_control_dmajor,
  min_fraction=0.2)</pre>
			<p>The output for the <a id="_idIndexMarker794"/>time shift augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B17990_07_11.jpg" alt="Figure 7.11 – Time shift (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Time shift (control-d-major.mp3)</p>
			<p>The wrapper function displays the augmented audio clip, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em>, and the original audio clip, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.12</em>, for comparison. Sometimes, you must look at the bottom, zoom-in waveform graph to see the augmented effects. The other option to hear the augmented impact is to click the play button, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.13</em>, to listen to the before and after audio files in the <span class="No-Break">Python Notebook:</span></p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B17990_07_12..jpg" alt="Figure 7.12 – Original time shift (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Original time shift (control-d-major.mp3)</p>
			<p>Pluto plays the <a id="_idIndexMarker795"/>audio by clicking on the audio play button in the <span class="No-Break">Python Notebook:</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B17990_07_13..jpg" alt="Figure 7.13 – Audio play buttons, before and after"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Audio play buttons, before and after</p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Every time you run the wrapper function command, you will see and hear a new audio file with a random shift between the minimum and maximum range. It will select a different audio file from the <span class="No-Break">real-world dataset.</span></p>
			<p>The<a id="_idIndexMarker796"/> audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em> shows that the piano scale in D major is shifted almost at the midpoint. Thus, it plays from <strong class="bold">C#</strong> scale down to <strong class="bold">D</strong> and then from <strong class="bold">D</strong> scale up to <strong class="bold">C#</strong>. Therefore, there were better options for music with time order dependency than the time <span class="No-Break">shift technique.</span></p>
			<p>Moving on to the first of three datasets, Pluto runs the time shift function using default parameters on the music clip from the MEC dataset, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment audio using time shift</strong>
pluto.play_aug_time_shift(
  pluto.df_music_data)</pre>
			<p>The output augmented file is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B17990_07_14..jpg" alt="Figure 7.14 – Time shift, music clip (Sad17422.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Time shift, music clip (Sad17422.wav)</p>
			<p>The original file output for comparison is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B17990_07_15..jpg" alt="Figure 7.15 – Original music clip for the time shift (Sad17422.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Original music clip for the time shift (Sad17422.wav)</p>
			<p>It is <a id="_idIndexMarker797"/>hard to see the effect between <em class="italic">Figures 7.14</em> and <em class="italic">7.15</em> in the WaveForm graph, but if Pluto focuses on the lower zoom-in charts, he can see that it has shifted. When Pluto plays the audio, he cannot notice any difference between the before and <span class="No-Break">after excerpts.</span></p>
			<p>The music in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.14</em> sounds like an adventure cinematic orchestra clip for a Westen movie that is on a repeating loop, so shifting and looping back works perfectly. Pluto repeatedly ran the wrapper function to retrieve a different audio file and confirmed no adverse effects. Thus, it is <strong class="bold">safe</strong> to timeshift the music from the MEC dataset using the <span class="No-Break">default parameters.</span></p>
			<p>Moving on to the second real-world dataset, Pluto knows human speech is time sequence-dependent in the CREMA-D dataset. Thus, it is <strong class="bold">unsafe</strong> to timeshift. He has exaggerated the effects by increasing the <strong class="bold">minimum fraction</strong> to <strong class="source-inline">0.5</strong> so that you can see the damaging results. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment audio using time shift</strong>
pluto.play_aug_time_shift(pluto.df_voice_data,
  min_fraction=0.5)</pre>
			<p>The output for the augmented timeshift audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B17990_07_16..jpg" alt="Figure 7.16 – Time shift voice clip (1027_IEO_DIS_HI.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Time shift voice clip (1027_IEO_DIS_HI.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B17990_07_17..jpg" alt="Figure 7.17 – Original time shift voice clip (1027_IEO_DIS_HI.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Original time shift voice clip (1027_IEO_DIS_HI.wav)</p>
			<p>In the<a id="_idIndexMarker798"/> audio for <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>, a man’s voice said, <em class="italic">eleven o’clock [a pause] it is</em>, while in the audio for <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17</em>, it said, <em class="italic">It is eleven o’clock</em>. Once again, the timeshifting technique is not a safe option for the human speech (<span class="No-Break">CREMA-D) dataset.</span></p>
			<p>On the third dataset, Pluto repeated running the following command on the urban sound from the <span class="No-Break">US8K database:</span></p>
			<pre class="source-code">
<strong class="bold"># augment audio using time shift</strong>
pluto.play_aug_time_shift(pluto.df_sound_data,
  min_fraction=0.5)</pre>
			<p>The <a id="_idIndexMarker799"/>output for the augmented timeshift audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B17990_07_18..jpg" alt="Figure 7.18 – Time shift urban sound (135526-6-3-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Time shift urban sound (135526-6-3-0.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B17990_07_19..jpg" alt="Figure 7.19 – Original time shift urban sound (135526-6-3-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Original time shift urban sound (135526-6-3-0.wav)</p>
			<p><em class="italic">Figures 7.17</em> and <em class="italic">7.18</em> are audio of a gunshot with a high level of urban noise. The time shift moved the gunshot a bit later. After<a id="_idIndexMarker800"/> running the command repeatedly, Pluto found the time shift with a <strong class="bold">minimum fraction</strong> of 0.5 acceptable for the US8K <span class="No-Break">real-world dataset.</span></p>
			<p>The next audio augmentation technique we’ll cover is <span class="No-Break">time stretching.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor158"/>Time stretching</h2>
			<p>The definition and key<a id="_idIndexMarker801"/> code lines for the <strong class="source-inline">play_aug_time_stretch()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_time_stretch(self, df,
  min_rate=0.2,
  max_rate=6.8,
  leave_length_unchanged=True,
  title='Time Stretch'):
<strong class="bold"># code snippet for time stretch</strong>
xtransform = audiomentations.TimeStretch(
  min_rate = min_rate,
  max_rate = max_rate,
  leave_length_unchanged = leave_length_unchanged,
  p=1.0)</pre>
			<p>The fill function’s code can be found in the Python Notebook. Pluto tests the time stretch wrapper function with the audio control file and a <strong class="bold">maximum rate</strong> of <strong class="source-inline">5.4</strong>, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using time stretch</strong>
pluto.play_aug_time_stretch(pluto.audio_control_dmajor,
  max_rate=5.4)</pre>
			<p>The output for <a id="_idIndexMarker802"/>the time stretch augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B17990_07_20..jpg" alt="Figure 7.20 – Time stretch (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – Time stretch (control-d-major.mp3)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B17990_07_21..jpg" alt="Figure 7.21 – Original time stretch (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – Original time stretch (control-d-major.mp3)</p>
			<p>Pluto runs<a id="_idIndexMarker803"/> the wrapper function repeatedly, and the scaled audio is recognizable every time. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.20</em> audio plays the D major clip about three times faster, but the scales <span class="No-Break">are recognizable.</span></p>
			<p>The wrapper function works well on the control audio files, so Pluto applies to the music (MEC) dataset with a <strong class="bold">maximum rate</strong> of <strong class="source-inline">3.0</strong>, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using tim stretch</strong>
pluto.play_aug_time_stretch(pluto.df_music_data,
  max_rate=3.0)</pre>
			<p>The output for the time stretch augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B17990_07_22..jpg" alt=" Figure 7.22 – Time stretch music (Sad44404.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 7.22 – Time stretch music (Sad44404.wav)</p>
			<p>The wrapper <a id="_idIndexMarker804"/>function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B17990_07_23..jpg" alt="Figure 7.23 – Original time stretch music (Sad44404.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.23 – Original time stretch music (Sad44404.wav)</p>
			<p>The audio in <em class="italic">Figures 7.22</em> and <em class="italic">7.23</em> is of an afternoon lunch in a garden with a strong lead guitar and cinematic orchestra clip. With the time stretch filter at a <strong class="bold">maximum rate</strong> of <strong class="source-inline">3.0</strong>, the audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.22</em> plays a bit faster, but Pluto did not notice any degradation in the music’s mood. Pluto repeatedly ran the wrapper function on the MEC dataset and concluded that the time stretch technique is <strong class="bold">safe</strong> at a <strong class="bold">maximum rate</strong> <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">3.0</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Find a universal <strong class="bold">safe</strong> range for the time stretch technique for all types of music (MEC). You can use the Python Notebook to find a safe range for the MEC datasets and download other music datasets from the Kaggle website. On the other hand, is this an impossible task? Does a universal safe range exist for pop, classical, folklore, country, and <span class="No-Break">hip-hop music?</span></p>
			<p>Pluto does the same for the human speech (CREMA-D) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using time stretch</strong>
pluto.play_aug_time_stretch(pluto.df_voice_data,
  max_rate=3.5)</pre>
			<p>The output <a id="_idIndexMarker805"/>for the time stretch augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B17990_07_24..jpg" alt="Figure 7.24 – Time stretch voice clip (1073_WSI_SAD_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.24 – Time stretch voice clip (1073_WSI_SAD_XX.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B17990_07_25..jpg" alt="Figure 7.25 – Original time stretch voice clip (1073_WSI_SAD_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.25 – Original time stretch voice clip (1073_WSI_SAD_XX.wav)</p>
			<p>The <a id="_idIndexMarker806"/>audio in<em class="italic"> Figures 7.24</em> and <em class="italic">7.25</em> is of a woman’s voice saying, <em class="italic">Let’s stop for a couple of minutes</em>, while the audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.24</em> says it a bit faster but it’s recognizable. Pluto repeatedly runs the wrapper function on the CREMA-D dataset with a <strong class="bold">maximum rate</strong> of <strong class="source-inline">3.5</strong> and hears no deterioration in the recordings. Thus, he concluded that the CREMA-D dataset is <strong class="bold">safe</strong> for use with the time stretch technique set to a <strong class="bold">maximum rate</strong> <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">3.5</strong></span><span class="No-Break">.</span></p>
			<p>Pluto does the same for the urban sound (US8K) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using time stretch</strong>
pluto.play_aug_time_stretch(pluto.df_sound_data,
  max_rate=2.4)</pre>
			<p>The output for the time stretch augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B17990_07_26..jpg" alt="Figure 7.26 – Time stretch urban sound (76266-2-0-50.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.26 – Time stretch urban sound (76266-2-0-50.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B17990_07_27..jpg" alt="Figure 7.2﻿7 – Original ﻿time stretch urban sound (76266-2-0-50.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.27 – Original time stretch urban sound (76266-2-0-50.wav)</p>
			<p>The <a id="_idIndexMarker807"/>audio in <em class="italic">Figures 7.26</em> and <em class="italic">7.27</em> is of an urban clip of adults and children talking in a playground with high traffic or wind noises in the recording. The audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.26</em> plays a bit faster. Pluto repeatedly runs the wrapper function on the US8K dataset with a <strong class="bold">maximum rate</strong> of <strong class="source-inline">2.4</strong>, and he concluded that the US8K dataset<a id="_idIndexMarker808"/> is <strong class="bold">safe</strong> for the time <span class="No-Break">stretching technique.</span></p>
			<p>The next technique we’ll look at is <span class="No-Break"><strong class="bold">pitch scaling</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>Pitch scaling</h2>
			<p>The <a id="_idIndexMarker809"/>definition and key code lines for the <strong class="source-inline">play_aug_pitch_scaling()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_pitch_scaling(self, df,
  min_semitones = -6.0,
  max_semitones = 6.0,
  title='Pitch Scaling'):
<strong class="bold"># code snippet for pitch shift</strong>
xtransform = audiomentations.PitchShift(
  min_semitones = min_semitones,
  max_semitones = max_semitones,
  p=1.0)</pre>
			<p>Pluto tests the pitch scaling wrapper function with the audio control file using the default parameters, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using pitch scaling</strong>
pluto.play_aug_pitch_scaling(pluto.audio_control_dmajor)</pre>
			<p>The output augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B17990_07_28..jpg" alt="Figure 7.28 – Pitch scaling (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.28 – Pitch scaling (control-d-major.mp3)</p>
			<p>The wrapper function<a id="_idIndexMarker810"/> also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B17990_07_29..jpg" alt="Figure 7.29 – Original pitch scaling (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.29 – Original pitch scaling (control-d-major.mp3)</p>
			<p>Pluto can’t tell the difference from looking at the complete Waveform graphs in <em class="italic">Figures 7.28</em> and <em class="italic">7.29</em>, but if he focuses on the zoom-in chart, he can see the differences. Listening to the audio file is the best method. To do that, you must go to the Python Notebook and click on the audio play button. The audio in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.28</em> plays more like a harpsichord than the original piano scale in <span class="No-Break">D major.</span></p>
			<p>Next, Pluto applies the pitch scaling wrapper function to the music (MEC) dataset, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using pitch scaling</strong>
pluto.play_aug_pitch_scaling(pluto.df_music_data,
  min_semitones=-11.0,
  max_semitones=-9.0)</pre>
			<p>The output for the<a id="_idIndexMarker811"/> augmented audio clip is <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B17990_07_30..jpg" alt="Figure 7.30 – Pitch scaling music (Sad11601.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.30 – Pitch scaling music (Sad11601.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B17990_07_31..jpg" alt="Figure 7.31 – Original pitch scaling music (Sad11601.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.31 – Original pitch scaling music (Sad11601.wav)</p>
			<p>The <a id="_idIndexMarker812"/>audio in<em class="italic"> </em><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.30</em> plays warmer, is melodic, and accentuates the moodiness of a dramatic cinematic clip. It’s like a calm evening before a dramatic turn. Pluto purposefully exaggerated the pitch effects by setting the <strong class="bold">minimum semitones</strong> to <strong class="source-inline">-11.0</strong> and the <strong class="bold">maximum semitones</strong> to <strong class="source-inline">-9.0</strong>. The audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.31</em> plays the original clip. Using the default parameter value, Pluto found minimal pitch scaling effects on the MEC dataset. Thus, it is a <span class="No-Break"><strong class="bold">safe</strong></span><span class="No-Break"> technique.</span></p>
			<p>Using the default parameter values, Pluto does the same for the voice (CREMA-D) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using pitch scaling</strong>
pluto.play_aug_pitch_scaling(pluto.df_voice_data)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B17990_07_32..jpg" alt="Figure 7.32 – Pitch scaling voice (1031_IEO_ANG_LO.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.32 – Pitch scaling voice (1031_IEO_ANG_LO.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B17990_07_33..jpg" alt="Figure 7.33 - Original pitch scaling voice (1031_IEO_ANG_LO.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.33 - Original pitch scaling voice (1031_IEO_ANG_LO.wav)</p>
			<p>Pluto <a id="_idIndexMarker813"/>compares the zoom-in graphs in <em class="italic">Figures 7.32</em> and <em class="italic">7.33</em> to see the effects. When listening to the audio files, he heard the augmented version, from <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.32</em>, of a high-pitched kid saying, <em class="italic">It is eleven o’clock.</em> The original version is an adult man’s voice. After repeatedly running the wrapper command with safe <strong class="bold">minimum and maximum semitones</strong> set to -2.4 and 2.4, Pluto found it minimized the effects for the <span class="No-Break">CREMA-D dataset.</span></p>
			<p>The <a id="_idIndexMarker814"/>urban sound (US8K) dataset has a diverse frequency range. Machine noises are repetitive low-frequency sounds, while sirens are high-frequency sounds. Pluto could not find a safe range unless he limited the <strong class="bold">semitones’</strong> scope to -1.2 and 1.0. For fun, Pluto has moved the <strong class="bold">semitones</strong> range to <strong class="source-inline">4.0</strong> and <strong class="source-inline">14.0</strong>. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using pitch scaling</strong>
pluto.play_aug_pitch_scaling(pluto.df_sound_data,
  min_semitones=4.0,
  max_semitones=11.0)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B17990_07_34..jpg" alt="Figure 7.34 – Pitch scaling urban sound (93567-8-3-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.34 – Pitch scaling urban sound (93567-8-3-0.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/B17990_07_35..jpg" alt="Figure 7.35 – Original pitch scaling urban sound (93567-8-3-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.35 – Original pitch scaling urban sound (93567-8-3-0.wav)</p>
			<p>The<a id="_idIndexMarker815"/> audio in <em class="italic">Figures 7.34</em> and <em class="italic">7.35</em> play an urban clip of sirens in a busy urban street. The audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.34</em>  has the sirens sound clearer and with a bit less interference from the <span class="No-Break">traffic noise.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">This challenge is a thought experiment. Can you define rules for which audio augmentation methods are suitable for an audio category, such as human speech, music, bird songs, and so on? For example, can human speech be safely augmented using pitch shifting in a <span class="No-Break">small range?</span></p>
			<p>The next technique we’ll look at is <span class="No-Break"><strong class="bold">noise injection</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor160"/>Noise injection</h2>
			<p>The<a id="_idIndexMarker816"/> definition and key code lines for the <strong class="source-inline">play_aug_noise_injection()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_noise_injection(self, df,
  min_amplitude = 0.002,
  max_amplitude = 0.2,
  title='Gaussian noise injection'):
<strong class="bold"># code snippet for noise injection</strong>
xtransform = audiomentations.AddGaussianNoise(
  min_amplitude = min_amplitude,
  max_amplitude = max_amplitude,
  p=1.0)</pre>
			<p>The full <a id="_idIndexMarker817"/>function’s code can be found in the Python Notebook. Pluto will not explain the result here because they are similar to the previous three audio augmentation techniques. You should try them out on the Python Notebook to see and hear <span class="No-Break">the results.</span></p>
			<p>For the <strong class="bold">background noise injection</strong> method, the code <a id="_idIndexMarker818"/>snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for adding background noise</strong>
xtransform = audiomentations.AddBackgroundNoise(
  sounds_path="~/background_sound_files",
  min_snr_in_db=3.0,
  max_snr_in_db=30.0,
  noise_transform=PolarityInversion(),
  p=1.0)</pre>
			<p>For the <strong class="bold">short noise injection</strong> method, the <a id="_idIndexMarker819"/>code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for adding short noise</strong>
xtransform = audiomentations.AddShortNoises(
  sounds_path="~/background_sound_files",
  min_snr_in_db=3.0,
  max_snr_in_db=30.0,
  noise_rms="relative_to_whole_input",
  min_time_between_sounds=2.0,
  max_time_between_sounds=8.0,
  noise_transform=audiomentations.PolarityInversion(),
  p=1.0)</pre>
			<p>The full <a id="_idIndexMarker820"/>function code can be found in the Python Notebook. The next technique we’ll look at is <span class="No-Break"><strong class="bold">polarity inversion</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor161"/>Polarity inversion</h2>
			<p>The definition and key <a id="_idIndexMarker821"/>code lines for the <strong class="source-inline">play_aug_polar_inverse()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_polar_inverse(self, df,
  title='Polarity inversion'):
<strong class="bold"># code snippet for polarity inversion</strong>
xtransform = audiomentations.PolarityInversion(
  p=1.0)</pre>
			<p>Once again, Pluto will not explain the result here because they have similar outputs to what you saw previously. Try them out on the Python Notebook to see and hear the results. Pluto has written the Python code <span class="No-Break">for you.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">There is one fun fact about the polarity inversion technique: you will not hear any difference between the augmented and original recordings. You couldn’t even see the difference in the WaveForm graph, but you could see it in the zoom-in chart. The blue/positive and yellow/negative <span class="No-Break">are flipped.</span></p>
			<p>For example, Pluto applies the wrapper function to the voice (CREMA-D) dataset <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using polar inverse</strong>
pluto.play_aug_polar_inverse(pluto.df_voice_data)</pre>
			<p>The output for the <a id="_idIndexMarker822"/>augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B17990_07_36..jpg" alt="Figure 7.36 – Polar inversion voice (1081_WSI_HAP_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.36 – Polar inversion voice (1081_WSI_HAP_XX.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/B17990_07_37..jpg" alt="Figure 7.37 – Original polar inversion voice (1081_WSI_HAP_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.37 – Original polar inversion voice (1081_WSI_HAP_XX.wav)</p>
			<p>Another fun fact is <a id="_idIndexMarker823"/>that polar inversion is as simple as multiplying the <strong class="bold">amplitude</strong> array with a negative one, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
<strong class="bold"># implement using numpy</strong>
xaug = numpy.array(data_amp) * -1</pre>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: why does polarity inversion not affect the sound? After all, it is a drastic change in the data, as evidenced in the Waveform graph. Hint: think about the technical complexities of molecules’ vibration from compression and expansion relating to <span class="No-Break">absolute measurement.</span></p>
			<p>The next few techniques we’ll look at <span class="No-Break">use </span><span class="No-Break"><strong class="bold">filters</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor162"/>Low-pass filter</h2>
			<p>Before Pluto digs in and <a id="_idIndexMarker824"/>explains the filter’s audio techniques, he will only partially present all the filters in this book. This is because the process is repetitive, and you can gain much more insight from running the code in the Python Notebook. Pluto will thoroughly explain the code and the Waveform graph for the <strong class="bold">low-pass</strong> and <strong class="bold">band-pass</strong> filters; for the<a id="_idIndexMarker825"/> other filters, he will explain the code but not the output <span class="No-Break">Waveform graphs.</span></p>
			<p>The definition and key code lines for the <strong class="source-inline">play_aug_low_pass_filter()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_low_pass_filter(self, df,
  min_cutoff_freq=150, max_cutoff_freq=7500,
  min_rolloff=12, max_rolloff=24,
  title='Low pass filter'):
<strong class="bold"># code snippet for low pass filter</strong>
xtransform = audiomentations.LowPassFilter(
  min_cutoff_freq = min_cutoff_freq,
  max_cutoff_freq = max_cutoff_freq,
  min_rolloff = min_rolloff,
  max_rolloff = max_rolloff,
  p=1.0)</pre>
			<p>The full function’s <a id="_idIndexMarker826"/>code can be found in the Python Notebook. Pluto tests the low-pass filter wrapper function with the audio control file using default parameters, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using low pass filter</strong>
pluto.play_aug_low_pass_filter(pluto.audio_control_dmajor)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/B17990_07_38..jpg" alt="Figure 7.38 – Low-pass filter control (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.38 – Low-pass filter control (control-d-major.mp3)</p>
			<p>The wrapper<a id="_idIndexMarker827"/> function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/B17990_07_39..jpg" alt="Figure 7.39 – Original low-pass filter control (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.39 – Original low-pass filter control (control-d-major.mp3)</p>
			<p>Pluto does not detect any difference in listening to the augmented and original recordings shown in <em class="italic">Figures 7.38</em> and <em class="italic">7.39</em>. At first glance at the WaveForm graphs, Pluto did not see any differences until he inspected the zoom-in charts. There is a slight decrease in the positive (blue color) <strong class="bold">amplitude</strong> values and, inversely, a tiny increase in the negative (yellow color) values. In other words, the absolute differences between the before and after are slightly lower <span class="No-Break"><strong class="bold">amplitude</strong></span><span class="No-Break"> values.</span></p>
			<p>Next, Pluto <a id="_idIndexMarker828"/>applies the low-pass filter wrapper function to the music (MEC) dataset, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using low pass filter</strong>
pluto.play_aug_low_pass_filter(pluto.df_music_data)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="image/B17990_07_40..jpg" alt="Figure 7.40 – Low-pass filter music (Sad21828.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.40 – Low-pass filter music (Sad21828.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B17990_07_41..jpg" alt="Figure 7.41 – Original low-pass filter music (Sad21828.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.41 – Original low-pass filter music (Sad21828.wav)</p>
			<p>The audio in <em class="italic">Figures 7.40</em> and <em class="italic">7.41</em> is of a cinematic orchestra music clip with a driving drum beat. It could be the background music from an Indiana Jones movie before the giant boulder bars down the cave. In particular, the augmented file sounds, shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.40</em>, are smoother and the edges have been nipped off. Pluto repeatedly ran the wrapper function on the MEC dataset using the default parameter settings and found that the augmented<a id="_idIndexMarker829"/> audio file does not alter the happy or sad mood of the music. Hence, it is a <span class="No-Break"><strong class="bold">safe</strong></span><span class="No-Break"> technique.</span></p>
			<p>For the voice (CREMA-D) dataset, Pluto does <span class="No-Break">the same:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using low pass filter</strong>
pluto.play_aug_low_pass_filter(pluto.df_voice_data)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B17990_07_42..jpg" alt="Figure 7.42 – Low-pass filter voice (1067_IEO_HAP_LO.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.42 – Low-pass filter voice (1067_IEO_HAP_LO.wav)</p>
			<p>The wrapper <a id="_idIndexMarker830"/>function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/B17990_07_43..jpg" alt="Figure 7.43 – Original low-pass filter voice (1067_IEO_HAP_LO.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.43 – Original low-pass filter voice (1067_IEO_HAP_LO.wav)</p>
			<p>The audio in <em class="italic">Figures 7.42</em> and <em class="italic">7.43</em> both said <em class="italic">It is eleven o’clock</em>. Furthermore, the audio in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.42</em> has fewer snaps and crackles. Pluto has an unscientific thought that the zoom-in graph displays a smoother curve with fewer dips and dimples, which could translate to a cleaner voice in the augmented recording. After repeatedly applying the wrapper function to the CREMA-D dataset, Pluto thinks the low-pass filter <span class="No-Break">is </span><span class="No-Break"><strong class="bold">safe</strong></span><span class="No-Break">.</span></p>
			<p>The last of the three real-world datasets is the urban sound (US8K) dataset. Pluto applies the wrapper function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using low pass filter</strong>
pluto.play_aug_low_pass_filter(pluto.df_sound_data)</pre>
			<p>The output for the <a id="_idIndexMarker831"/>augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B17990_07_44..jpg" alt="Figure 7.44 – Low-pass filter urban sound (185373-9-0-6.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.44 – Low-pass filter urban sound (185373-9-0-6.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="image/B17990_07_45..jpg" alt="Figure 7.45 – Original low-pass filter urban sound (185373-9-0-6.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.45 – Original low-pass filter urban sound (185373-9-0-6.wav)</p>
			<p>The audio in <em class="italic">Figures 7.44</em> and <em class="italic">7.45</em> is of street music playing outdoors with traffic and urban noise. Repeatedly executing the wrapper functions gives mixed results for the US8K dataset. Pluto does not know which parameter values are <strong class="bold">safe</strong>. He needs to consult a domain <a id="_idIndexMarker832"/>expert – that is, a <span class="No-Break">sound engineer.</span></p>
			<p>The next technique we’ll look at is the <span class="No-Break"><strong class="bold">band-pass filter</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor163"/>Band-pass filter</h2>
			<p>The definition and key code lines<a id="_idIndexMarker833"/> for the <strong class="source-inline">play_aug_band_pass_filter()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
# <strong class="bold">function definition</strong>
def play_aug_band_pass_filter(self, df,
  min_center_freq=200, max_center_freq=4000,
  min_bandwidth_fraction=0.5, max_bandwidth_fraction=1.99,
  min_rolloff=12, max_rolloff=24,
  title='Band pass filter'):
# <strong class="bold">code snippet for band pass filter</strong>
xtransform = audiomentations.BandPassFilter(
  min_center_freq = min_center_freq,
  max_center_freq = max_center_freq,
  min_bandwidth_fraction = min_bandwidth_fraction,
  max_bandwidth_fraction = max_bandwidth_fraction,
  min_rolloff = min_rolloff,
  max_rolloff = max_rolloff,
  p=1.0)</pre>
			<p>The full function’s code can be found in the Python Notebook. Pluto tests the band-pass filter wrapper function with the audio control file using default parameters, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using band pass filter</strong>
pluto.play_aug_band_pass_filter(pluto.audio_control_dmajor)</pre>
			<p>The output for<a id="_idIndexMarker834"/> the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/B17990_07_46..jpg" alt="Figure 7.46 – Band-pass filter control (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.46 – Band-pass filter control (control-d-major.mp3)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/B17990_07_47..jpg" alt="Figure 7.47 – Original band-pass filter control (control-d-major.mp3)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.47 – Original band-pass filter control (control-d-major.mp3)</p>
			<p>From <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.46</em>, Pluto could guess that the sound has bee<a id="_idTextAnchor164"/>n slightly altered. When listening to the<a id="_idIndexMarker835"/> audio file, he confirms that the scale is the same, but it has a whom-whom sound <span class="No-Break">to it.</span></p>
			<p>Next, Pluto applies the band-pass filter function to the music (MEC) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using band pass filter</strong>
pluto.play_aug_band_pass_filter(pluto.df_music_data)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer220" class="IMG---Figure">
					<img src="image/B17990_07_48..jpg" alt="Figure 7.48 – Band-pass filter music (Happy15804.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.48 – Band-pass filter music (Happy15804.wav)</p>
			<p>The wrapper <a id="_idIndexMarker836"/>function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/B17990_07_49..jpg" alt="Figure 7.49 – Original band-pass filter music (Happy15804.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.49 – Original band-pass filter music (Happy15804.wav)</p>
			<p>The sound for this clip is a happy-go-lucky cinematic tune with a sprinkle of a drum beat. The augmented sound, shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.48</em>, is brighter, bunchier, and yet smoother. Pluto repeatedly executes the wrapper function against the <strong class="bold">MEC</strong> dataset, and it enhances the <strong class="bold">happier</strong> mood music and infuses a more substantial tone into the <strong class="bold">sadder</strong> clips. Thus, it is <strong class="bold">safe</strong> for the <span class="No-Break">MEC dataset.</span></p>
			<p>Pluto does the same for the voice (CREMA-D) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using band pass filter</strong>
pluto.play_aug_band_pass_filter(pluto.df_voice_data)</pre>
			<p>The output for<a id="_idIndexMarker837"/> the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/B17990_07_50..jpg" alt="Figure 7.50 – Band-pass filter voice (1071_IWL_NEU_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.50 – Band-pass filter voice (1071_IWL_NEU_XX.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B17990_07_51..jpg" alt="Figure 7.51 – Original band-pass filter voice (1071_IWL_NEU_XX.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.51 – Original band-pass filter voice (1071_IWL_NEU_XX.wav)</p>
			<p>The audio for <em class="italic">Figures 7.50</em> and <em class="italic">7.51</em> is of a woman saying, <em class="italic">I would like a new alarm clock</em>. The augmented audio file sounds cleaner with less noise interference than the original clip. The same results were found for most of the files in the CREMA-D dataset. Thus, the CREMA-D dataset is <strong class="bold">safe</strong> for use with the band-pass <span class="No-Break">filter technique.</span></p>
			<p>Pluto suspects <a id="_idIndexMarker838"/>the same improvement or at least a safe level for the urban sound (US8K) dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using band pass filter</strong>
pluto.play_aug_band_pass_filter(pluto.df_sound_data)</pre>
			<p>The output for the augmented audio clip is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B17990_07_52..jpg" alt="Figure 7.52 – Band-pass filter urban sound (95404-3-0-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.52 – Band-pass filter urban sound (95404-3-0-0.wav)</p>
			<p>The wrapper function also displays the original audio clip <span class="No-Break">for comparison:</span></p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B17990_07_53..jpg" alt="Figure 7.53 – Original band-pass filter urban sound (95404-3-0-0.wav)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.53 – Original band-pass filter urban sound (95404-3-0-0.wav)</p>
			<p>The audio file <a id="_idIndexMarker839"/>for this is the sound of a windy backyard with birds singing and fading dogs barking from far away. The augmented audio file, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.52</em>, sounds more distinct but with echoes in a tunnel effect. Pluto thinks the band-pass filter is <strong class="bold">safe</strong> for the <span class="No-Break">US8K dataset.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to implement the reversed audio technique. Can you think of a use case where reversed audio is a <strong class="bold">safe</strong> option? Hint: copy and rename the <strong class="source-inline">play_aug_time_shift()</strong> wrapper function. Change <strong class="source-inline">xtransform = audiomentations.Shift()</strong> to <strong class="source-inline">xtransform = </strong><span class="No-Break"><strong class="source-inline">audiomentations.Reverse()</strong></span><span class="No-Break">.</span></p>
			<p>The audio <a id="_idIndexMarker840"/>augmentation process becomes slightly repetitive, but the results are fascinating. Thus, Pluto has shared the code for the following audio filter techniques, but the resulting WaveForm graphs and audio play buttons are in the <span class="No-Break">Python Notebook.</span></p>
			<p>The next filter we’ll cover is the <span class="No-Break"><strong class="bold">high-pass filter</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/>High-pass and other filters</h2>
			<p>The definition and key<a id="_idIndexMarker841"/> code lines for the <strong class="source-inline">play_aug_high_pass_filter()</strong> function are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># function definition</strong>
def play_aug_high_pass_filter(self, df,
  min_cutoff_freq=20, max_cutoff_freq=2400,
  min_rolloff=12, max_rolloff=24,
  title='High pass filter'):
<strong class="bold"># code snippet for high pass filter</strong>
xtransform = audiomentations.HighPassFilter(
  min_cutoff_freq = min_cutoff_freq,
  max_cutoff_freq = max_cutoff_freq,
  min_rolloff = min_rolloff,
  max_rolloff = max_rolloff,
  p=1.0)</pre>
			<p>The results <a id="_idIndexMarker842"/>can be found in the <span class="No-Break">Python Notebook.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to implement other audio filters in the Audiomentations library, such as <strong class="source-inline">audiomentations.HighPassFilter</strong>, <strong class="source-inline">audiomentations.LowShelfFilter</strong>, <strong class="source-inline">audiomentations.HighShelfFilter</strong>, <strong class="source-inline">audiomentations.BandStopFilter</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">audiomentations.PeakingFilter</strong></span><span class="No-Break">.</span></p>
			<p>With that, we have covered the fundamentals of audio augmentations and practiced coding them. Next, we will summarize <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor166"/>Summary</h1>
			<p>As we saw from the beginning, audio augmentation is a challenging topic without hearing the audio recording in question, but we can visualize the techniques’ effect using <strong class="bold">Waveform</strong> graphs and zoom-in charts. Still, there is no substitution for listening to the before and after augmentation recordings. You have access to the Python Notebook with the complete code and audio button to play the augmented and <span class="No-Break">original recordings.</span></p>
			<p>First, we discussed the theories and concepts of an audio file. The three fundamental components of an audio file are <strong class="bold">amplitude</strong>, <strong class="bold">frequency</strong>, and <strong class="bold">sampling rate</strong>. The measurements of unit for frequency are <strong class="bold">Hertz</strong> (<strong class="bold">Hz</strong>) and <strong class="bold">Kilohertz</strong> (<strong class="bold">kHz</strong>). <strong class="bold">Pitch</strong> is similar to frequency, but the unit of measurement is the <strong class="bold">decibel</strong> (<strong class="bold">dB</strong>). Similarly, <strong class="bold">bit rate</strong> and <strong class="bold">bit depth</strong> are other forms expressing the <span class="No-Break">sampling rate.</span></p>
			<p>Next, we explained the standard audio augmentation techniques. The three essentials are <strong class="bold">time stretching</strong>, <strong class="bold">time shifting</strong>, and <strong class="bold">pitch scaling</strong>. The others are <strong class="bold">noise injection</strong> and <strong class="bold">polarity inversion</strong>. Even more methods are available in the augmentation libraries, such as clip, gain, normalize, and <strong class="bold">hyperbolic tangent</strong> (<span class="No-Break"><strong class="bold">tanh</strong></span><span class="No-Break">) distortion.</span></p>
			<p>Before downloading the real-world audio datasets, we discussed the top eight open source audio augmentation libraries. There are many robust audio augmentation libraries available. Pluto picked the <strong class="bold">Librosa</strong> library – after all, it’s the most established. Its second choice was the <strong class="bold">Audiomentations</strong> library because it is powerful and easy to integrate with other libraries. Facebook’s <strong class="bold">Augly</strong> libraries are strong contenders, and Pluto used them in other projects. Ultimately, because Pluto uses the wrapper function concept, he can choose any library or combination <span class="No-Break">of libraries.</span></p>
			<p>As with image and text augmentation, Pluto downloaded three real-world audio datasets from the <em class="italic">Kaggle</em> website. Each dataset represents an audio category in everyday experiences: music, human speech, and <span class="No-Break">urban sound.</span></p>
			<p>Writing code in the Python Notebook helps us reinforce our understanding of each audio augmentation technique. Pluto explains the code and the output <span class="No-Break">in detail.</span></p>
			<p>The output is fantastic, but the coding process seems repetitive. It is easy because Pluto follows the established pattern of creating a reusable class, adding new methods, downloading real-world data from the <em class="italic">Kaggle</em> website, importing it into pandas, leveraging best-of-class augmentation libraries, and writing new <span class="No-Break">wrapper functions.</span></p>
			<p>Throughout this chapter, there were <em class="italic">fun facts</em> and <em class="italic">fun challenges</em>. Pluto hopes you will take advantage of these and expand your experience beyond the scope of <span class="No-Break">this chapter.</span></p>
			<p>In the next chapter, Pluto will demystify audio <span class="No-Break">using </span><span class="No-Break"><strong class="bold">spectograms</strong></span><span class="No-Break">.</span></p>
		</div>
	</body></html>