- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to image and text augmentation, the objective of audio data augmentation
    is to extend the dataset to gain a higher accuracy forecast or prediction in a
    generative AI system. Audio augmentation is cost-effective and is a viable option
    when acquiring additional audio files is expensive or time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Writing about audio augmentation methods poses unique challenges. The first
    is that audio is not visual like images or text. If the format is audiobooks,
    web pages, or mobile apps, then we play the sound, but the medium is paper. Thus,
    we must transform the audio signal into a visual representation. The **Waveform**
    graph, also known as the **time series graph**, is a standard method for representing
    an audio signal. You can listen to the audio in the accompanying Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to write Python code to read an audio file
    and draw a Waveform graph from scratch. Pluto has provided a preview here so that
    we can discuss the components of the Waveform graph. The function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a Waveform graph of piano scales in D major:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Piano scales in D major](img/B17990_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Piano scales in D major
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.1*, Pluto drew the positive amplitude in blue and the negative
    amplitude in yellow in the **Waveform** graph. This makes the chart easier to
    read and prettier. The **amplitude** is the value of the *Y*-axis. It measures
    the vibration or compression and decompression of the air molecules. The higher
    the amplitude, the greater the air displacement. In other words, the zero amplitude
    value is silent, and the greater the absolute distance from zero, the louder the
    sound.
  prefs: []
  type: TYPE_NORMAL
- en: The **frequency**, also known as the **sampling rate**, is the value of the
    *X*-axis. The sampling rate measures how many times you recorded the amplitude
    value in a second. The unit for sound frequency or the sampling rate is **hertz**
    (**Hz**). For example, a sampling rate of 1,000 Hz or 1 **kilohertz** (**kHz**)
    means you record a thousand amplitude values in 1 second. In other words, you
    register an amplitude value for every millisecond. Thus, the higher the frequency,
    the more accurate the sound, and therefore, a larger sound file size. This is
    because there is a higher recorded amplitude value. 1 kHz is equal to 1,000 Hz.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: A human’s range of hearing is between 20 Hz and 20 kHz. Younger children can
    hear sounds higher than 20 kHz, while older adults can’t listen to sounds greater
    than 17 kHz. Deep and low music bass sound is between 20 Hz and 120 Hz, while
    everyday human speech ranges from 600 Hz to 5 kHz. In contrast, a canine’s hearing
    frequency is approximately 40 Hz to 60 kHz, which is better than a human’s hearing
    frequency. That is why you can’t hear an above 20 kHz dog whistle.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitch** is the same as **frequency** but from a human point of view. It refers
    to the loudness of the sound and is measured in **decibels** (**dB**). Thus, high
    pitch means high frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: dB is the unit for the degree of loudness. A rocket sound is about 165 dB, busy
    traffic noise is about 85 dB, human speech is about 65 dB, rainfall is about 45
    dB, and zero dB means silence.
  prefs: []
  type: TYPE_NORMAL
- en: The standard sampling rate for MP3 and other audio formats is 22.05 kHz. The
    frequency of high-quality sound, also known as **Compact Disk** (**CD**) sound,
    is 44.1 kHz.
  prefs: []
  type: TYPE_NORMAL
- en: When storing an audio file on a computer, **bit depth** is the accuracy of the
    amplitude value. **16 bits** has 65,536 levels of detail, while **24 bits** has
    16,777,216 levels of information. The higher the bit depth, the closer the digital
    recording is to the analog sound and the larger the audio file size.
  prefs: []
  type: TYPE_NORMAL
- en: The **bit rate** is similar to the sampling rate, where the bit rate measures
    the number of bits per second. In audio processing, the playback function uses
    the bit rate, while the record function uses the sampling rate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mono sound** has one channel (**1-channel**), while **stereo sound** has
    two channels (**2-channel**). Stereo sound has one channel for the right ear and
    another channel for the left ear.'
  prefs: []
  type: TYPE_NORMAL
- en: The bottom graph in *Figure 7**.1* shows a zoom-in Waveform chart. It shows
    only 100 sampling rate points, starting at the midpoint of the top Waveform graph.
    Upon closer inspection, the Waveform is a simple time series plot. Many data types,
    such as text and images, can be represented as a time series chart because Python
    can represent the data as a one-dimensional array, regardless of the data type.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: '**Pitch correction** involves tuning a vocal’s performance in a recording so
    that the singer sings on key. You can use software such as **Antares Auto-Tune
    Pro** or **Waves Tune Real Time** to correct the highness or lowness in singing
    pitch. It saves time and money in terms of re-recording. Pitch correction was
    relatively uncommon before 1977 when **Antares Audio Technology’s Auto-Tune Pitch
    Correcting Plug-In** was released. Today, about 90% of radio, television, website,
    or app songs have pitch correction. **Autotune** is used for vocal effects, while
    pitch correction is for fixing vocals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since most data can be used for Waveform graphs, Pluto can draw a time series
    graph for the phrase “*Mary had a little lamb, whose fleece was white as snow.
    And everywhere that Mary went, the lamb was sure to go.*” Pluto uses the following
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Text as a time series graph](img/B17990_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Text as a time series graph
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.2*, blue is for alphanumeric characters, while yellow is for
    punctuation. The *Y*-axis consists of the ASCII value of the character.
  prefs: []
  type: TYPE_NORMAL
- en: The conversion is straightforward because each letter is encoded as an **ASCII**
    value, such as “A” as 65, “B” as 66, and so on. Similarly, an image composed of
    a three-dimensional array (width, height, and depth) has an RGB value. The result
    of collapsing the depth dimension by multiplying the RGB value is between zero
    and 16,581,375\. Flatten the remaining two-dimensional array into a one-dimensional
    array and plot it as a time series graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover audio augmentation using **Waveform** transformation,
    and in particular, the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard audio augmentation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio augmentation libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world audio datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcing your learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by discussing common audio augmentation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Standard audio augmentation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to image augmentation in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058),
    various audio libraries provide many more functions than are necessary for augmentation.
    Therefore, we will only cover some of the methods available in the chosen audio
    library.
  prefs: []
  type: TYPE_NORMAL
- en: In image augmentation, the term **safe level** is defined as not altering or
    distorting the original image beyond an acceptable level. There is no standard
    terminology for deforming the original audio signal beyond a permissible point.
    Thus, the term **safe** or **true** will be used interchangeably to denote a limit
    point for the audio signal.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: all audio files are represented as numbers in
    time series format. Thus, can you create a statistically valid augmentation method
    that does not consider human hearing perception? In other words, use math to manipulate
    a statistically valid number array, but never listen to the before and after effects.
    After all, audio augmentation aims to have more data for enhancing the AI prediction,
    which does not comprehend human speech or good music from bad music.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following functions are commonly used for audio augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Time stretching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitch scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polarity inversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with time stretching.
  prefs: []
  type: TYPE_NORMAL
- en: Time stretching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Time stretching** involves lengthening or shortening the duration of an audio
    signal. It is done without changing the pitch level. For example, in human speech,
    you would slow down and drag out your words or speed up and talk like a chipmunk
    cartoon character.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the **safe** level for **time stretching**? It depends on the type of
    audio and the goal of AI prediction. In general, you can speed up or slow down
    human speech and it can still be understood. But if the goal is to predict the
    speaker’s name, then there is a small time stretching range you can apply to the
    speech and stay **true** to the speaker’s talking style.
  prefs: []
  type: TYPE_NORMAL
- en: Music, on the other hand, is generally considered **unsafe** for time stretching.
    Changing the tempo beat of a music segment alters the music beyond the true musician’s
    intention.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental or nature sounds are generally **safe** for time stretching within
    an acceptable safe range.
  prefs: []
  type: TYPE_NORMAL
- en: This augmentation technique, and all other methods, are covered in the Python
    Notebook; hence, it is easier to grasp the effect by listening to the original
    and the augmented sound.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to time stretching is time shifting.
  prefs: []
  type: TYPE_NORMAL
- en: Time shifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Time shifting** involves moving an audio segment forward or backward. For
    example, if you want a more dramatic pause between a speaker’s announcement and
    the audience’s applause, you can timeshift the applauses a few seconds forward.'
  prefs: []
  type: TYPE_NORMAL
- en: Timeshift with rollover means the last sound will be added back to the beginning.
    Without rollover, the audio will have silence for the beginning or end, depending
    on whether you’re shifting forward or backward.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose the goal of the AI prediction is to identify gunshots in
    a city to alert the police. In that case, timeshift with rollover is an acceptable
    **safe** augmentation technique. Another example of good use of timeshift with
    rollover is looped background music.
  prefs: []
  type: TYPE_NORMAL
- en: Human speech or music is typically **unsafe** for time shifting. This is because
    the sequential order is essential for it to stay **true** to the original audio.
  prefs: []
  type: TYPE_NORMAL
- en: Moving away from time, pitch shifting or pitch scaling is another augmented
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Pitch shifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pitch shifting** or **pitch scaling** changes the frequency of sound without
    affecting the speed or time shift. For example, a man has a lower voice pitch
    than a woman. Increasing the pitch level in a voice recording might make a man
    sound like a woman.'
  prefs: []
  type: TYPE_NORMAL
- en: Pitch shifting should be used cautiously when augmenting human speech, music,
    environment, and nature audio files. The **safe** level can change drastically
    for the same dataset, depending on the AI prediction’s objective.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the recordings of daily meadow sounds can be used to count how
    many birds visit the meadow a day, or an AI can predict what kinds of birds dwell
    in the field. The pitch-shifting **safe** range for counting birds is higher than
    for identifying birds. Applying pitch shifting to bird songs may inadvertently
    make one bird sound like other birds.
  prefs: []
  type: TYPE_NORMAL
- en: Another pitch alternation method is polarity inversion.
  prefs: []
  type: TYPE_NORMAL
- en: Polarity inversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Polarity inversion** involves switching the amplitude value from positive
    to negative and vice versa. Mathematically, it multiplies the amplitude by a negative
    value. Graphically, it alters the color blue and makes it yellow and vice versa
    in *Figure 7**.1*.'
  prefs: []
  type: TYPE_NORMAL
- en: To most humans, the playback after polarity inversion sounds the same as the
    original audio. It is most beneficial for ML when used with the phase-awareness
    model. There is no **safe** range because it is either used or not used.
  prefs: []
  type: TYPE_NORMAL
- en: The following augmentation is about adding noise to an audio file.
  prefs: []
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Noise injection** adds signal noise to an audio file. The effect of adding
    noise is that the augmented sound appears as though it consists of pops and crackles.
    The five types of noise typically used in audio augmentation are **background
    noise**, **Gaussian**, **random**, **signal-to-noise ratio** (**SNR**), and **short**
    **burst noise**.'
  prefs: []
  type: TYPE_NORMAL
- en: How much noise or the **safe** level depends on the AI project’s objective and
    the recording. Sometimes, you might have to employ a domain expert to attain a
    **safe** level.
  prefs: []
  type: TYPE_NORMAL
- en: Many more techniques could be classified as commonly used audio augmentations,
    such as clip, gain transition, normalize, padding, or reverse, but let’s move
    on and look at filters and masking.
  prefs: []
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio filters help eliminate unwanted interference or noise from an audio recording.
    The result is to improve the tone and playback of human speech, music, nature,
    and environmental recordings.
  prefs: []
  type: TYPE_NORMAL
- en: The audio filter changes frequency by **increasing**, **boosting**, or **amplifying**
    a range of frequencies. A filter could also **decrease**, **delete**, **cut**,
    **attenuate**, or **pass** a frequency range. For example, using a low-pass filter,
    you could remove the traffic noise from a recording of two people talking in a
    city.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following filters:'
  prefs: []
  type: TYPE_NORMAL
- en: Low-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Band-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-shelf filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-shelf filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Band-stop filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peak filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the low pass filter.
  prefs: []
  type: TYPE_NORMAL
- en: Low-pass filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **low-pass filter** cuts or deletes low-frequency sounds, such as traffic
    noise, machine engine rumbles, or elephant calls.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the minimum cut-off frequency is 150 Hz, the maximum cut-off is 7.5
    kHz, the minimum roll-off is 12 dB, and the maximum roll-off is 24 dB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a fun fact: elephant calls are lower than 20 Hz or into the infrasound
    range. The next filter we’ll cover is the high pass filter.'
  prefs: []
  type: TYPE_NORMAL
- en: High-pass filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the low-pass filter, the **high-pass filter** cuts high-frequency
    sound, such as whistling, babies crying, nail scratching, or bell ringing.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the minimum and maximum cut-off frequencies are 20 Hz and 2.4 kHz,
    and the minimum and maximum roll-offs are 12 dB and 24 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun fact: a human can whistle around 3 to 4 kHz. There is one more pass filter
    we need to look at: the ban pass filter.'
  prefs: []
  type: TYPE_NORMAL
- en: Band-pass filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **band-pass filter** limits the sound wave to a range of frequencies. In
    other words, it combines the low and high-band filters. For example, a band-pass
    filter can make it clearer to listen to a recording of two friends’ conversations
    in a busy Paris outdoor restaurant. Similarly, it can be used to isolate bird
    song recordings in a noisy Amazon jungle.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the minimum and maximum center frequencies are 200 Hz and 4 kHz,
    the minimum and maximum bandwidth fractions are 0.5 and 1.99, and the minimum
    and maximum roll-offs are 12 dB and 24 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on from pass filters to shelf filters.
  prefs: []
  type: TYPE_NORMAL
- en: Low-shelf filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Shelf filtering** is also known as **shelf equalization**. In particular,
    the **low-shelf filter** boosts or cuts the frequencies at the lower end of the
    spectrum. For example, you can use a low-shelf filter to reduce the bass in a
    heavy metal song.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the minimum and maximum center frequencies are 50 Hz and 4 kHz, and
    the minimum and maximum gain are -18 dB to 18 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The next technique is the high-shelf filter.
  prefs: []
  type: TYPE_NORMAL
- en: High-shelf filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similarly, a **high-shelf filter** increases or decreases the frequencies’ amplitude
    at the higher end of the spectrum. For example, you can use a high-shelf filter
    to brighten a music recording.
  prefs: []
  type: TYPE_NORMAL
- en: Commonly, the minimum and maximum center frequencies are 300 Hz and 7.5 kHz,
    and the minimum and maximum gains are -18 dB and 18 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The band-stop filter is the next technique we’ll cover.
  prefs: []
  type: TYPE_NORMAL
- en: Band-stop filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **band-stop filter** is also known as a **ban-reject filter** or **notch
    filter**. It deletes frequencies between two cut-off points or on either side
    of the range. In addition, it uses low and high-pass filters under the hood. For
    example, a band-stop filter can remove unwanted spikes and noises from a backyard
    music session jam.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the minimum and maximum center frequencies are 200 Hz and 4 kHz,
    the minimum and maximum bandwidth fractions are 0.5 and 1.99, and the minimum
    and maximum roll-offs are 12 dB and 24 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The peak filter is the last audio augmentation technique that will be covered
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Peak filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **peak** or **bell filter** is the opposite of the band-stop filter. In
    other words, it boosts shelf filters with a narrow band and higher gain signal
    or allows a boost or cut around a center frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the minimum and maximum center frequencies are 50 Hz and 7.5 kHz,
    and the minimum and maximum gains are -24 dB and 24 dB, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Many methods are available in audio augmentation libraries. Thus, the next step
    is to select one or two audio augmentation libraries for Pluto’s wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Audio augmentation libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many commercial and open source audio data augmentation libraries.
    In this chapter, we will focus on open source libraries available on **GitHub**.
    Some libraries are more robust than others, and some focus on a particular subject,
    such as human speech. Pluto will write wrapper functions using the libraries provided
    to do the heavy lifting; thus, you can select more than one library in your project.
    If a library is implemented in the **CPU**, it may not be suitable for dynamic
    data augmenting during the ML training cycle because it will slow down the process.
    Instead, choose a library that can run on the **GPU**. Choose a robust and easy-to-implement
    library to learn new audio augmentation techniques or output the augmented data
    on local or cloud disk space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The well-known open source libraries for audio augmentation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Librosa** is an open source Python library for music and audio analysis.
    It was made available in 2015 and has long been a popular choice. Many other audio
    processing and augmentation libraries use Librosa’s functions as building blocks.
    It can be found on GitHub at [https://github.com/librosa/librosa](https://github.com/librosa/librosa).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audiomentations** is a Python library specifically for audio data augmentation.
    Its key benefit is its robustness and easy project integration. It is cited in
    many Kaggle competition winners. It can be found on GitHub at [https://github.com/iver56/audiomentations](https://github.com/iver56/audiomentations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook or Meta research published **Augly** as an open source Python library
    for image and audio augmentation. The goal is to provide specific data augmentations
    for real-life projects. It can be found on GitHub at [https://github.com/facebookresearch/AugLy/tree/main/augly/audio](https://github.com/facebookresearch/AugLy/tree/main/augly/audio).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras** is a Python library for audio and music signal preprocessing. It
    implements frequency conversions and data augmentation using **GPU** preprocessing.
    It can be found on GitHub at [https://github.com/keunwoochoi/kapre](https://github.com/keunwoochoi/kapre).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nlpaug** is a Python library that’s versatile for both language and audio
    data augmentation. [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101) used Nlpaug
    for text augmentation, but in this chapter, we will use the audio library. It
    can be found on GitHub at [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotify’s Audio Intelligence Lab published the **Pedalboard** Python library.
    The goal is to enable studio-quality audio effects for ML. It can be found on
    GitHub at [https://github.com/spotify/pedalboard](https://github.com/spotify/pedalboard).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pydiogment** is a Python library that aims to simplify audio augmentation.
    It is easy to use but less robust than other audio augmentation libraries. It
    can be found on GitHub at [https://github.com/SuperKogito/pydiogment](https://github.com/SuperKogito/pydiogment).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Torch-augmentations** is an implementation of the **Audiomentations** library
    for **GPU** processing. It can be found on GitHub at [https://github.com/asteroid-team/torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: '**Audiomentations** library version 0.28.0 consists of 36 augmentation functions,
    **Librosa** library version 0.9.2 consists of over 400 methods, and the **Pydiogment**
    library’s latest update (July 2020) consists of 14 augmentation methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto will primarily use the **Audiomentations** and **Librosa** libraries to
    demonstrate the concepts we’ve mentioned in Python code. But first, we will download
    Pluto and use him to download real-world audio datasets from the *Kaggle* website.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world audio datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should be familiar with downloading Pluto and real-world datasets
    from the *Kaggle* website. We chose to download Pluto from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038)
    because the image augmentation functions shown in *Chapters 3* and *4*, and the
    text augmentation techniques shown in *Chapters 5* and *6*, are not beneficial
    for audio augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three real-world audio datasets we will use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Musical Emotions Classification* (*MEC*) real-world audio dataset from
    Kaggle contains 2,126 songs separated into **train** and **test** folders. They
    are instrumental music, and the goal is to predict **happy** or **sad** music.
    Each piece is about 9 to 10 minutes in length and is in ***.wav** format. It was
    published in 2020 and is available to the public. Its license is **Attribution-ShareAlike
    4.0 International (CC BY-SA** **4.0)**: [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Crowd Sourced Emotional Multimodal Actors Dataset* (*CREMA-D*) real-world
    audio dataset from Kaggle contains 7,442 original clips from 91 actors. The actors
    are 48 **males** and 43 **females** between 20 to 74 years old, and their ethnicities
    are **African American**, **Asian**, **Caucasian**, **Hispanic**, and **Unspecified**.
    In addition, the spoken phrases represent six different emotions. They are **anger**,
    **disgust**, **fear**, **happy**, **neutral**, and **sad**. There is no set goal
    for the datasets, but you can use them to predict age, sex, ethnicity, or emotions.
    It was published in 2019 and is available to the public. Its license is **Open
    Data Commons Attribution License (ODC-By)** **v1.0**: [https://opendatacommons.org/licenses/by/1-0/index.html](https://opendatacommons.org/licenses/by/1-0/index.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *urban_sound_8k* (*US8K*) real-world dataset from Kaggle contains 8,732
    labeled sound excerpts from an urban setting. Each clip is between 2 to 4 seconds,
    and the classification is **Air conditioner, Car horn, Children playing, Dogs
    barking, Drilling, Engine idling, Gunshots, Jackhammers, Sirens, and Street music**.
    It was published in 2021 and is available to the public. Its license is **CC0
    1.0 Universal (CC0 1.0) Public Domain** **Dedication**: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The three audio datasets – music, human speech, and environmental sound – represent
    the typical sounds you hear daily.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following four steps are the same in every chapter. Review *Chapters 2*
    and *3* if you need clarification. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve the Python Notebook and Pluto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download real-world data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data into pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listen to and view the audio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s begin by downloading Pluto in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Python Notebook and Pluto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by loading the `data_augmentation_with_python_chapter_7.ipynb` file into
    Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this
    point onward, the code snippets will be from the Python Notebook, which contains
    the complete functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to clone the repository. We will reuse the code from [*Chapter
    2*](B17990_02.xhtml#_idTextAnchor038). The `!git` and `%run` statements are used
    to start up Pluto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows or similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to do one more check to ensure Pluto is loaded satisfactorily. The
    following command asks Pluto to state his status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows or similar, depending on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, Pluto will download the audio dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world data and pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto has downloaded the real-world music dataset, the MEC, using the `fetch_kaggle_dataset(url)`
    function from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038). He found that the
    dataset consists of a comma-separated variable (CSV) header file. Thus, he used
    the `fetch_df(fname)` function to import it into pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Music (MEC) top 3 records](img/B17990_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Music (MEC) top 3 records
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_append_music_full_path()` and `fetch_music_full_path()` helper functions.
    The key code lines are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The function’s code can be found in the Python Notebook. The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Music (MEC) top 3 records revised](img/B17990_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Music (MEC) top 3 records revised
  prefs: []
  type: TYPE_NORMAL
- en: 'The next real-world dataset from Kaggle is for human speech (**CREMA-D**).
    Pluto must download and import it into pandas using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Voice (CREMA-D) top 3 records revised](img/B17990_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Voice (CREMA-D) top 3 records revised
  prefs: []
  type: TYPE_NORMAL
- en: 'The third audio dataset from Kaggle is for urban sound (**US8K**). Incidentally,
    Kaggle consists of about 1,114 real-world audio datasets as of December 2022\.
    Pluto must download and import it into pandas using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Urban sound (US8K) top 3 records revised](img/B17990_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Urban sound (US8K) top 3 records revised
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the `pluto_data` directory; he stored the control clip in the `pluto.audio_control_dmajor`
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto challenges you to search for and download an additional audio dataset
    from the *Kaggle* website or your project. It is more meaningful if you work with
    the data that matters to you. So long as you download and import it into pandas,
    all the augmentation wrapper functions will work the same for your audio files.
    Hint: use Pluto’s `fetch_kaggle_dataset()` and `fetch_df()` functions.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, Pluto has downloaded the three real-world audio datasets. The next
    step is to play the audio and view the audio **Waveform** graph.
  prefs: []
  type: TYPE_NORMAL
- en: Listening and viewing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto has written three new functions to play the audio and display the `_draw_area_with_neg()`
    helper method, which displays the area graph for positive and negative numbers
    in the same dataset. Incidentally, the pandas and **Matplotlib** area graphs can
    only show positive values. The essential code line for this function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. The next helper
    function is `_draw_audio()`. Its main objectives are loading or reading the audio
    file using the **Librosa** library, drawing the two **Waveform** graphs, and displaying
    the play audio button. Pandas has the same filename that it had when fetching
    the audio datasets. The key code lines for the function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The entirety of this function can be found in the Python Notebook. The `draw_audio()`
    method invokes the two helper functions. Additionally, it selects a random audio
    file from the pandas DataFrame. Thus, Pluto runs the command repeatedly to listen
    to and view a different audio file from the real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto can display a music clip from the **MEC** dataset using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The audio play button is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Audio play button](img/B17990_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Audio play button
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Waveform** graphs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Music waveform graph (Happy36521)](img/B17990_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Music waveform graph (Happy36521)
  prefs: []
  type: TYPE_NORMAL
- en: The audio play button in *Figures 7.7* and *7.8* (`Happy36521.wav`) will play
    the instrumental music with the flute, drum, and guitar.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto names the function `draw_audio()` and not `play_audio()` because this
    book needs a Waveform graph, and to listen to the audio, you have to go to the
    Python Notebook and click on the play button shown in *Figure 7**.7*. Like all
    wrapper functions, you can repeatedly run the `draw_audio()` method to see and
    listen to different audio files from the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto displays a human speech clip from the **CREMA-D** dataset using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The audio play button’s output is not displayed here because it looks the same
    as in *Figure 7**.7*. The result of the **Waveform** graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Human speech waveform graph (1078_TIE_HAP_XX)](img/B17990_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Human speech waveform graph (1078_TIE_HAP_XX)
  prefs: []
  type: TYPE_NORMAL
- en: 'The audio of *Figure 7**.9* (`1078_TIE_HAP_XX.wav`) is a woman speaking the
    phrase: *that is exactly what happens*. She sounds happy and middle-aged.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto displays an urban sound clip from the **US8K** dataset using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the **Waveform** graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Urban sound waveform graph (119455-5-0-7)](img/B17990_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Urban sound waveform graph (119455-5-0-7)
  prefs: []
  type: TYPE_NORMAL
- en: The audio for *Figure 7**.10* (`119455-5-0-7.wav`) is the sound of jackhammers.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve discussed various audio augmentation concepts, selected audio
    libraries, downloaded Pluto, and asked him to fetch real-world datasets for music,
    human speech, and urban sounds. Pluto now also plays the audio and displays the
    Waveform graph for each file.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is writing Python wrapper code from scratch to gain a deeper understanding
    of the audio augmentation techniques we’ve covered.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcing your learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key objectives of the `_audio_transform()` helper function are selecting
    a random clip, performing the augmentation using the Audiomentations library function,
    displaying the WaveForm graph using the `_fetch_audio_data()` and `_draw_audio()`
    helper functions, and showing the audio play button. The key code lines are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function’s code can be found in the Python Notebook. Pluto will write
    the Python wrapper functions for audio augmentation in the same order as previously
    discussed. In particular, they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Time shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time stretching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitch scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polarity inversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with **time shifting**.
  prefs: []
  type: TYPE_NORMAL
- en: Time shifting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_time_shift()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function’s code can be found in the Python Notebook. Pluto tests the
    time shift wrapper function with the audio control file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the time shift augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Time shift (control-d-major.mp3)](img/B17990_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Time shift (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function displays the augmented audio clip, *Figure 7**.11*, and
    the original audio clip, *Figure 7**.12*, for comparison. Sometimes, you must
    look at the bottom, zoom-in waveform graph to see the augmented effects. The other
    option to hear the augmented impact is to click the play button, as shown in *Figure
    7**.13*, to listen to the before and after audio files in the Python Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Original time shift (control-d-major.mp3)](img/B17990_07_12..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Original time shift (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto plays the audio by clicking on the audio play button in the Python Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Audio play buttons, before and after](img/B17990_07_13..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Audio play buttons, before and after
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Every time you run the wrapper function command, you will see and hear a new
    audio file with a random shift between the minimum and maximum range. It will
    select a different audio file from the real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figure 7**.11* shows that the piano scale in D major is shifted
    almost at the midpoint. Thus, it plays from **C#** scale down to **D** and then
    from **D** scale up to **C#**. Therefore, there were better options for music
    with time order dependency than the time shift technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to the first of three datasets, Pluto runs the time shift function
    using default parameters on the music clip from the MEC dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output augmented file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Time shift, music clip (Sad17422.wav)](img/B17990_07_14..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Time shift, music clip (Sad17422.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The original file output for comparison is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Original music clip for the time shift (Sad17422.wav)](img/B17990_07_15..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Original music clip for the time shift (Sad17422.wav)
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to see the effect between *Figures 7.14* and *7.15* in the WaveForm
    graph, but if Pluto focuses on the lower zoom-in charts, he can see that it has
    shifted. When Pluto plays the audio, he cannot notice any difference between the
    before and after excerpts.
  prefs: []
  type: TYPE_NORMAL
- en: The music in *Figure 7**.14* sounds like an adventure cinematic orchestra clip
    for a Westen movie that is on a repeating loop, so shifting and looping back works
    perfectly. Pluto repeatedly ran the wrapper function to retrieve a different audio
    file and confirmed no adverse effects. Thus, it is **safe** to timeshift the music
    from the MEC dataset using the default parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to the second real-world dataset, Pluto knows human speech is time
    sequence-dependent in the CREMA-D dataset. Thus, it is `0.5` so that you can see
    the damaging results. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented timeshift audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Time shift voice clip (1027_IEO_DIS_HI.wav)](img/B17990_07_16..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Time shift voice clip (1027_IEO_DIS_HI.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Original time shift voice clip (1027_IEO_DIS_HI.wav)](img/B17990_07_17..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Original time shift voice clip (1027_IEO_DIS_HI.wav)
  prefs: []
  type: TYPE_NORMAL
- en: In the audio for *Figure 7**.16*, a man’s voice said, *eleven o’clock [a pause]
    it is*, while in the audio for *Figure 7**.17*, it said, *It is eleven o’clock*.
    Once again, the timeshifting technique is not a safe option for the human speech
    (CREMA-D) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the third dataset, Pluto repeated running the following command on the urban
    sound from the US8K database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented timeshift audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Time shift urban sound (135526-6-3-0.wav)](img/B17990_07_18..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Time shift urban sound (135526-6-3-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Original time shift urban sound (135526-6-3-0.wav)](img/B17990_07_19..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Original time shift urban sound (135526-6-3-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figures 7.17* and *7.18* are audio of a gunshot with a high level of urban
    noise. The time shift moved the gunshot a bit later. After running the command
    repeatedly, Pluto found the time shift with a **minimum fraction** of 0.5 acceptable
    for the US8K real-world dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The next audio augmentation technique we’ll cover is time stretching.
  prefs: []
  type: TYPE_NORMAL
- en: Time stretching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_time_stretch()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The fill function’s code can be found in the Python Notebook. Pluto tests the
    time stretch wrapper function with the audio control file and a `5.4`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the time stretch augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Time stretch (control-d-major.mp3)](img/B17990_07_20..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Time stretch (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Original time stretch (control-d-major.mp3)](img/B17990_07_21..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Original time stretch (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: Pluto runs the wrapper function repeatedly, and the scaled audio is recognizable
    every time. *Figure 7**.20* audio plays the D major clip about three times faster,
    but the scales are recognizable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function works well on the control audio files, so Pluto applies
    to the music (MEC) dataset with a `3.0`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the time stretch augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 7.22 – Time stretch music (Sad44404.wav)](img/B17990_07_22..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – Time stretch music (Sad44404.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Original time stretch music (Sad44404.wav)](img/B17990_07_23..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Original time stretch music (Sad44404.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.22* and *7.23* is of an afternoon lunch in a garden
    with a strong lead guitar and cinematic orchestra clip. With the time stretch
    filter at a `3.0`, the audio in *Figure 7**.22* plays a bit faster, but Pluto
    did not notice any degradation in the music’s mood. Pluto repeatedly ran the wrapper
    function on the MEC dataset and concluded that the time stretch technique is `3.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Find a universal **safe** range for the time stretch technique for all types
    of music (MEC). You can use the Python Notebook to find a safe range for the MEC
    datasets and download other music datasets from the Kaggle website. On the other
    hand, is this an impossible task? Does a universal safe range exist for pop, classical,
    folklore, country, and hip-hop music?
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the human speech (CREMA-D) dataset. The command is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the time stretch augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Time stretch voice clip (1073_WSI_SAD_XX.wav)](img/B17990_07_24..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – Time stretch voice clip (1073_WSI_SAD_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Original time stretch voice clip (1073_WSI_SAD_XX.wav)](img/B17990_07_25..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Original time stretch voice clip (1073_WSI_SAD_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.24* and *7.25* is of a woman’s voice saying, *Let’s
    stop for a couple of minutes*, while the audio in *Figure 7**.24* says it a bit
    faster but it’s recognizable. Pluto repeatedly runs the wrapper function on the
    CREMA-D dataset with a `3.5` and hears no deterioration in the recordings. Thus,
    he concluded that the CREMA-D dataset is `3.5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the urban sound (US8K) dataset. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the time stretch augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Time stretch urban sound (76266-2-0-50.wav)](img/B17990_07_26..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Time stretch urban sound (76266-2-0-50.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.2\uFEFF7 – Original \uFEFFtime stretch urban sound (76266-2-0-50.wav)](img/B17990_07_27..jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – Original time stretch urban sound (76266-2-0-50.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.26* and *7.27* is of an urban clip of adults and children
    talking in a playground with high traffic or wind noises in the recording. The
    audio in *Figure 7**.26* plays a bit faster. Pluto repeatedly runs the wrapper
    function on the US8K dataset with a `2.4`, and he concluded that the US8K dataset
    is **safe** for the time stretching technique.
  prefs: []
  type: TYPE_NORMAL
- en: The next technique we’ll look at is **pitch scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: Pitch scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_pitch_scaling()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto tests the pitch scaling wrapper function with the audio control file
    using the default parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Pitch scaling (control-d-major.mp3)](img/B17990_07_28..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – Pitch scaling (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Original pitch scaling (control-d-major.mp3)](img/B17990_07_29..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – Original pitch scaling (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: Pluto can’t tell the difference from looking at the complete Waveform graphs
    in *Figures 7.28* and *7.29*, but if he focuses on the zoom-in chart, he can see
    the differences. Listening to the audio file is the best method. To do that, you
    must go to the Python Notebook and click on the audio play button. The audio in
    *Figure 2**.28* plays more like a harpsichord than the original piano scale in
    D major.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Pluto applies the pitch scaling wrapper function to the music (MEC) dataset,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – Pitch scaling music (Sad11601.wav)](img/B17990_07_30..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – Pitch scaling music (Sad11601.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Original pitch scaling music (Sad11601.wav)](img/B17990_07_31..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Original pitch scaling music (Sad11601.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in*Figure 7**.30* plays warmer, is melodic, and accentuates the moodiness
    of a dramatic cinematic clip. It’s like a calm evening before a dramatic turn.
    Pluto purposefully exaggerated the pitch effects by setting the `-11.0` and the
    `-9.0`. The audio in *Figure 7**.31* plays the original clip. Using the default
    parameter value, Pluto found minimal pitch scaling effects on the MEC dataset.
    Thus, it is a **safe** technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the default parameter values, Pluto does the same for the voice (CREMA-D)
    dataset. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – Pitch scaling voice (1031_IEO_ANG_LO.wav)](img/B17990_07_32..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – Pitch scaling voice (1031_IEO_ANG_LO.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 - Original pitch scaling voice (1031_IEO_ANG_LO.wav)](img/B17990_07_33..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 - Original pitch scaling voice (1031_IEO_ANG_LO.wav)
  prefs: []
  type: TYPE_NORMAL
- en: Pluto compares the zoom-in graphs in *Figures 7.32* and *7.33* to see the effects.
    When listening to the audio files, he heard the augmented version, from *Figure
    7**.32*, of a high-pitched kid saying, *It is eleven o’clock.* The original version
    is an adult man’s voice. After repeatedly running the wrapper command with safe
    **minimum and maximum semitones** set to -2.4 and 2.4, Pluto found it minimized
    the effects for the CREMA-D dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The urban sound (US8K) dataset has a diverse frequency range. Machine noises
    are repetitive low-frequency sounds, while sirens are high-frequency sounds. Pluto
    could not find a safe range unless he limited the `4.0` and `14.0`. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Pitch scaling urban sound (93567-8-3-0.wav)](img/B17990_07_34..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Pitch scaling urban sound (93567-8-3-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Original pitch scaling urban sound (93567-8-3-0.wav)](img/B17990_07_35..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Original pitch scaling urban sound (93567-8-3-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.34* and *7.35* play an urban clip of sirens in a busy
    urban street. The audio in *Figure 7**.34* has the sirens sound clearer and with
    a bit less interference from the traffic noise.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: This challenge is a thought experiment. Can you define rules for which audio
    augmentation methods are suitable for an audio category, such as human speech,
    music, bird songs, and so on? For example, can human speech be safely augmented
    using pitch shifting in a small range?
  prefs: []
  type: TYPE_NORMAL
- en: The next technique we’ll look at is **noise injection**.
  prefs: []
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_noise_injection()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The full function’s code can be found in the Python Notebook. Pluto will not
    explain the result here because they are similar to the previous three audio augmentation
    techniques. You should try them out on the Python Notebook to see and hear the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the **background noise injection** method, the code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For the **short noise injection** method, the code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The full function code can be found in the Python Notebook. The next technique
    we’ll look at is **polarity inversion**.
  prefs: []
  type: TYPE_NORMAL
- en: Polarity inversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_polar_inverse()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Once again, Pluto will not explain the result here because they have similar
    outputs to what you saw previously. Try them out on the Python Notebook to see
    and hear the results. Pluto has written the Python code for you.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one fun fact about the polarity inversion technique: you will not
    hear any difference between the augmented and original recordings. You couldn’t
    even see the difference in the WaveForm graph, but you could see it in the zoom-in
    chart. The blue/positive and yellow/negative are flipped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, Pluto applies the wrapper function to the voice (CREMA-D) dataset
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Polar inversion voice (1081_WSI_HAP_XX.wav)](img/B17990_07_36..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Polar inversion voice (1081_WSI_HAP_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Original polar inversion voice (1081_WSI_HAP_XX.wav)](img/B17990_07_37..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.37 – Original polar inversion voice (1081_WSI_HAP_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fun fact is that polar inversion is as simple as multiplying the **amplitude**
    array with a negative one, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: why does polarity inversion not affect the sound?
    After all, it is a drastic change in the data, as evidenced in the Waveform graph.
    Hint: think about the technical complexities of molecules’ vibration from compression
    and expansion relating to absolute measurement.'
  prefs: []
  type: TYPE_NORMAL
- en: The next few techniques we’ll look at use **filters**.
  prefs: []
  type: TYPE_NORMAL
- en: Low-pass filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before Pluto digs in and explains the filter’s audio techniques, he will only
    partially present all the filters in this book. This is because the process is
    repetitive, and you can gain much more insight from running the code in the Python
    Notebook. Pluto will thoroughly explain the code and the Waveform graph for the
    **low-pass** and **band-pass** filters; for the other filters, he will explain
    the code but not the output Waveform graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_low_pass_filter()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function’s code can be found in the Python Notebook. Pluto tests the
    low-pass filter wrapper function with the audio control file using default parameters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38 – Low-pass filter control (control-d-major.mp3)](img/B17990_07_38..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.38 – Low-pass filter control (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.39 – Original low-pass filter control (control-d-major.mp3)](img/B17990_07_39..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.39 – Original low-pass filter control (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: Pluto does not detect any difference in listening to the augmented and original
    recordings shown in *Figures 7.38* and *7.39*. At first glance at the WaveForm
    graphs, Pluto did not see any differences until he inspected the zoom-in charts.
    There is a slight decrease in the positive (blue color) **amplitude** values and,
    inversely, a tiny increase in the negative (yellow color) values. In other words,
    the absolute differences between the before and after are slightly lower **amplitude**
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Pluto applies the low-pass filter wrapper function to the music (MEC)
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.40 – Low-pass filter music (Sad21828.wav)](img/B17990_07_40..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.40 – Low-pass filter music (Sad21828.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.41 – Original low-pass filter music (Sad21828.wav)](img/B17990_07_41..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.41 – Original low-pass filter music (Sad21828.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.40* and *7.41* is of a cinematic orchestra music clip
    with a driving drum beat. It could be the background music from an Indiana Jones
    movie before the giant boulder bars down the cave. In particular, the augmented
    file sounds, shown in *Figure 7**.40*, are smoother and the edges have been nipped
    off. Pluto repeatedly ran the wrapper function on the MEC dataset using the default
    parameter settings and found that the augmented audio file does not alter the
    happy or sad mood of the music. Hence, it is a **safe** technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the voice (CREMA-D) dataset, Pluto does the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.42 – Low-pass filter voice (1067_IEO_HAP_LO.wav)](img/B17990_07_42..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.42 – Low-pass filter voice (1067_IEO_HAP_LO.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.43 – Original low-pass filter voice (1067_IEO_HAP_LO.wav)](img/B17990_07_43..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.43 – Original low-pass filter voice (1067_IEO_HAP_LO.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.42* and *7.43* both said *It is eleven o’clock*. Furthermore,
    the audio in *Figure 7**.42* has fewer snaps and crackles. Pluto has an unscientific
    thought that the zoom-in graph displays a smoother curve with fewer dips and dimples,
    which could translate to a cleaner voice in the augmented recording. After repeatedly
    applying the wrapper function to the CREMA-D dataset, Pluto thinks the low-pass
    filter is **safe**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last of the three real-world datasets is the urban sound (US8K) dataset.
    Pluto applies the wrapper function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.44 – Low-pass filter urban sound (185373-9-0-6.wav)](img/B17990_07_44..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.44 – Low-pass filter urban sound (185373-9-0-6.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.45 – Original low-pass filter urban sound (185373-9-0-6.wav)](img/B17990_07_45..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.45 – Original low-pass filter urban sound (185373-9-0-6.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio in *Figures 7.44* and *7.45* is of street music playing outdoors with
    traffic and urban noise. Repeatedly executing the wrapper functions gives mixed
    results for the US8K dataset. Pluto does not know which parameter values are **safe**.
    He needs to consult a domain expert – that is, a sound engineer.
  prefs: []
  type: TYPE_NORMAL
- en: The next technique we’ll look at is the **band-pass filter**.
  prefs: []
  type: TYPE_NORMAL
- en: Band-pass filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_band_pass_filter()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function’s code can be found in the Python Notebook. Pluto tests the
    band-pass filter wrapper function with the audio control file using default parameters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.46 – Band-pass filter control (control-d-major.mp3)](img/B17990_07_46..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.46 – Band-pass filter control (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.47 – Original band-pass filter control (control-d-major.mp3)](img/B17990_07_47..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.47 – Original band-pass filter control (control-d-major.mp3)
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 7**.46*, Pluto could guess that the sound has been slightly altered.
    When listening to the audio file, he confirms that the scale is the same, but
    it has a whom-whom sound to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Pluto applies the band-pass filter function to the music (MEC) dataset.
    The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.48 – Band-pass filter music (Happy15804.wav)](img/B17990_07_48..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.48 – Band-pass filter music (Happy15804.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.49 – Original band-pass filter music (Happy15804.wav)](img/B17990_07_49..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.49 – Original band-pass filter music (Happy15804.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The sound for this clip is a happy-go-lucky cinematic tune with a sprinkle of
    a drum beat. The augmented sound, shown in *Figure 7**.48*, is brighter, bunchier,
    and yet smoother. Pluto repeatedly executes the wrapper function against the **MEC**
    dataset, and it enhances the **happier** mood music and infuses a more substantial
    tone into the **sadder** clips. Thus, it is **safe** for the MEC dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the voice (CREMA-D) dataset. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.50 – Band-pass filter voice (1071_IWL_NEU_XX.wav)](img/B17990_07_50..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.50 – Band-pass filter voice (1071_IWL_NEU_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.51 – Original band-pass filter voice (1071_IWL_NEU_XX.wav)](img/B17990_07_51..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.51 – Original band-pass filter voice (1071_IWL_NEU_XX.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio for *Figures 7.50* and *7.51* is of a woman saying, *I would like
    a new alarm clock*. The augmented audio file sounds cleaner with less noise interference
    than the original clip. The same results were found for most of the files in the
    CREMA-D dataset. Thus, the CREMA-D dataset is **safe** for use with the band-pass
    filter technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto suspects the same improvement or at least a safe level for the urban
    sound (US8K) dataset. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the augmented audio clip is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.52 – Band-pass filter urban sound (95404-3-0-0.wav)](img/B17990_07_52..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.52 – Band-pass filter urban sound (95404-3-0-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapper function also displays the original audio clip for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.53 – Original band-pass filter urban sound (95404-3-0-0.wav)](img/B17990_07_53..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.53 – Original band-pass filter urban sound (95404-3-0-0.wav)
  prefs: []
  type: TYPE_NORMAL
- en: The audio file for this is the sound of a windy backyard with birds singing
    and fading dogs barking from far away. The augmented audio file, *Figure 7**.52*,
    sounds more distinct but with echoes in a tunnel effect. Pluto thinks the band-pass
    filter is **safe** for the US8K dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to implement the reversed audio technique. Can you think
    of a use case where reversed audio is a `play_aug_time_shift()` wrapper function.
    Change `xtransform = audiomentations.Shift()` to `xtransform =` `audiomentations.Reverse()`.
  prefs: []
  type: TYPE_NORMAL
- en: The audio augmentation process becomes slightly repetitive, but the results
    are fascinating. Thus, Pluto has shared the code for the following audio filter
    techniques, but the resulting WaveForm graphs and audio play buttons are in the
    Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The next filter we’ll cover is the **high-pass filter**.
  prefs: []
  type: TYPE_NORMAL
- en: High-pass and other filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The definition and key code lines for the `play_aug_high_pass_filter()` function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The results can be found in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to implement other audio filters in the Audiomentations
    library, such as `audiomentations.HighPassFilter`, `audiomentations.LowShelfFilter`,
    `audiomentations.HighShelfFilter`, `audiomentations.BandStopFilter`, and `audiomentations.PeakingFilter`.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have covered the fundamentals of audio augmentations and practiced
    coding them. Next, we will summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw from the beginning, audio augmentation is a challenging topic without
    hearing the audio recording in question, but we can visualize the techniques’
    effect using **Waveform** graphs and zoom-in charts. Still, there is no substitution
    for listening to the before and after augmentation recordings. You have access
    to the Python Notebook with the complete code and audio button to play the augmented
    and original recordings.
  prefs: []
  type: TYPE_NORMAL
- en: First, we discussed the theories and concepts of an audio file. The three fundamental
    components of an audio file are **amplitude**, **frequency**, and **sampling rate**.
    The measurements of unit for frequency are **Hertz** (**Hz**) and **Kilohertz**
    (**kHz**). **Pitch** is similar to frequency, but the unit of measurement is the
    **decibel** (**dB**). Similarly, **bit rate** and **bit depth** are other forms
    expressing the sampling rate.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explained the standard audio augmentation techniques. The three essentials
    are **time stretching**, **time shifting**, and **pitch scaling**. The others
    are **noise injection** and **polarity inversion**. Even more methods are available
    in the augmentation libraries, such as clip, gain, normalize, and **hyperbolic
    tangent** (**tanh**) distortion.
  prefs: []
  type: TYPE_NORMAL
- en: Before downloading the real-world audio datasets, we discussed the top eight
    open source audio augmentation libraries. There are many robust audio augmentation
    libraries available. Pluto picked the **Librosa** library – after all, it’s the
    most established. Its second choice was the **Audiomentations** library because
    it is powerful and easy to integrate with other libraries. Facebook’s **Augly**
    libraries are strong contenders, and Pluto used them in other projects. Ultimately,
    because Pluto uses the wrapper function concept, he can choose any library or
    combination of libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with image and text augmentation, Pluto downloaded three real-world audio
    datasets from the *Kaggle* website. Each dataset represents an audio category
    in everyday experiences: music, human speech, and urban sound.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing code in the Python Notebook helps us reinforce our understanding of
    each audio augmentation technique. Pluto explains the code and the output in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The output is fantastic, but the coding process seems repetitive. It is easy
    because Pluto follows the established pattern of creating a reusable class, adding
    new methods, downloading real-world data from the *Kaggle* website, importing
    it into pandas, leveraging best-of-class augmentation libraries, and writing new
    wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, there were *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond the scope
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, Pluto will demystify audio using **spectograms**.
  prefs: []
  type: TYPE_NORMAL
