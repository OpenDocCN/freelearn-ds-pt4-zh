<html><head></head><body>
		<div id="_idContainer254">
			<h1 id="_idParaDest-166" class="chapter-number"><a id="_idTextAnchor167"/>8</h1>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor168"/>Audio Data Augmentation with Spectrogram</h1>
			<p>In the previous chapter, we visualized the sound using the Waveform graph. An audio spectrogram is another visualizing method for seeing the audio components. The inputs to the Spectrogram are a one-dimensional array of <strong class="bold">amplitude</strong> values and the <strong class="bold">sampling rate</strong>. They are the same inputs as the <span class="No-Break">Waveform graph.</span></p>
			<p>An audio <strong class="bold">s</strong><strong class="bold">pectrogram</strong> is sometimes called a <strong class="bold">sonograph</strong>, <strong class="bold">sonogram</strong>, <strong class="bold">voiceprint</strong>, or <strong class="bold">voicegram</strong>. The Spectrogram is a more detailed representation<a id="_idIndexMarker843"/> of sound than the Waveform<a id="_idIndexMarker844"/> graph. It shows a correlation<a id="_idIndexMarker845"/> between frequency<a id="_idIndexMarker846"/> and amplitude (loudness) over time, which helps visualize the frequency content in a signal. Spectrograms make it easier to identify musical elements, detect melodic patterns, recognize frequency-based effects, and compare the results of different volume settings. Additionally, the Spectrogram can be more helpful in identifying non-musical aspects of a signal, such as noise and interference from <span class="No-Break">other frequencies.</span></p>
			<p>The typical usage is for music, human speech, and sonar. A short standard definition is a spectrum of frequency maps with time duration. In other words, the <em class="italic">y</em> axis is the frequency in <strong class="bold">Hz or kHz</strong>, and the <em class="italic">x</em> axis is the time duration in <strong class="bold">seconds or milliseconds</strong>. Sometimes, the graph comes with a color index for the <span class="No-Break">amplitude level.</span></p>
			<p>Pluto will explain the code in the Python Notebook later in the chapter, but here is a sneak peek of an audio Spectrogram. The command for drawing the <em class="italic">control piano scale in the D major</em> audio file is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># draw Spectrogram</strong>
pluto.draw_spectrogram(pluto.audio_control_dmajor)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/Image94335.jpg" alt="Figure 8.1 – An audio spectrogram of piano scale in D major"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – An audio spectrogram of piano scale in D major</p>
			<p>Before Pluto demystifies the audio Spectrogram, you should review <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> if the audio concepts and keywords sound alien to you. This chapter relies heavily on the knowledge and practices from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em>, Pluto uses the Matplotlib library to draw the audio spectrograph. The primary input is the amplitude array and the sampling rate. The library does all the heavy calculations, and other libraries, such as the Librosa or SciPy library, can perform the same task. In particular, Matplotlib can generate many types of audio spectrographs from the same input. Pluto will dig deeper into types of spectrographs a bit later, but first, let’s break down the steps of how the library constructs an audio spectrograph. The five high-level tasks are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Splitting the audio stream into overlapping<a id="_idIndexMarker847"/> segments, also known <span class="No-Break">as </span><span class="No-Break"><strong class="bold">windows</strong></span><span class="No-Break">.</span></li>
				<li>Calculating the <strong class="bold">Short-Time Fourier Transform</strong> (<strong class="bold">STFT</strong>) value on <span class="No-Break">each</span><span class="No-Break"><a id="_idIndexMarker848"/></span><span class="No-Break"> window.</span></li>
				<li>Converting<a id="_idIndexMarker849"/> the windows’ value into <span class="No-Break"><strong class="bold">decibels</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">dB</strong></span><span class="No-Break">).</span></li>
				<li>Linking the windows together as in the original <span class="No-Break">audio sequence.</span></li>
				<li>Displaying the result in a graph with the <em class="italic">y</em> axis as Hz, the <em class="italic">x</em> axis as seconds, and dB as a <span class="No-Break">color-coded value.</span></li>
			</ol>
			<p>The math for the previous five steps is complex, and the chapter’s goal is to use a Spectrogram to visualize the sound and augment the audio file. Thus, we rely on audio libraries to perform the <span class="No-Break">math calculation.</span></p>
			<p>As mentioned, the underlying data representing the Spectrogram is the same as the Waveform format. Therefore, the audio augmentation techniques are the same. Consequently, the resulting augmented audio file will sound the same. The visual representation is the only difference between the Spectrogram and the <span class="No-Break">Waveform graph.</span></p>
			<p>The majority of this chapter will cover the audio Spectrogram standard format, a variation of a Spectrogram, <strong class="bold">Mel-spectrogram</strong>, and <strong class="bold">Chroma</strong> STFT. The augmentation techniques represent a shorter section because you have learned the method in the previous chapter. We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Initializing <span class="No-Break">and downloading</span></li>
				<li><span class="No-Break">Audio Spectrogram</span></li>
				<li>Various <span class="No-Break">Spectrogram formats</span></li>
				<li>Mel-spectrogram and Chroma <span class="No-Break">STFT plots</span></li>
				<li><span class="No-Break">Spectrogram augmentation</span></li>
				<li><span class="No-Break">Spectrogram image</span></li>
			</ul>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">The <strong class="bold">Kay Electric Company</strong> introduced the first commercially available machine for audio spectrographic analysis in 1951. The black-and-white image was named a sonograph or sonogram for visualizing bird songs. In 1966, <strong class="bold">St. Martin’s Press</strong> used sonography for the book <em class="italic">Golden Field Guide to Birds of North America</em>. Spectrograms were favored over sonogram terminology around 1995 during the digital age. Spectrograms or sonograms were not limited to the study of birds in the early days. The US military used Spectrogram for encryption in the early 1940s and continues forward, as evidenced by the publication <em class="italic">Cryptologic Quarterly</em>, volume 38, published by the <strong class="bold">Center for Cryptologic History</strong> <span class="No-Break">in 2019.</span></p>
			<p>This chapter reuses the audio augmentation functions and the real-world audio datasets from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. Thus, we will start by initializing Pluto and downloading the <span class="No-Break">real-world datasets.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Initializing and downloading</h1>
			<p>Start with loading the <strong class="source-inline">data_augmentation_with_python_chapter_8.ipynb</strong> file on Google Colab or your<a id="_idIndexMarker850"/> chosen Python<a id="_idIndexMarker851"/> Notebook or JupyterLab environment. From this point onward, the code snippets are from the Python Notebook, which contains the <span class="No-Break">complete functions.</span></p>
			<p>The following initializing and downloading steps should be familiar to you because we have done them six times. The following code snippet is the same as from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
<strong class="bold"># Clone GitHub repo.</strong>
url = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'
!git clone {url}
<strong class="bold"># Intialize Pluto from Chapter 7</strong>
pluto_file = 'Data-Augmentation-with-Python/pluto/pluto_chapter_7.py'
%run {pluto_file}
<strong class="bold"># Verify Pluto</strong>
pluto.say_sys_info()
<strong class="bold"># Fetch Musical emotions classification</strong>
url = 'https://www.kaggle.com/datasets/kingofarmy/musical-emotions-classification'
pluto.fetch_kaggle_dataset(url)
f = 'kaggle/musical-emotions-classification/Train.csv'
pluto.df_music_data = pluto.fetch_df(f)
<strong class="bold"># Fetch human speaking</strong>
url = 'https://www.kaggle.com/datasets/ejlok1/cremad'
pluto.fetch_kaggle_dataset(url)
f = 'kaggle/cremad/AudioWAV'
pluto.df_voice_data = pluto.make_dir_dataframe(f)
<strong class="bold"># Fetch urban sound</strong>
url='https://www.kaggle.com/datasets/rupakroy/urban-sound-8k'
pluto.fetch_kaggle_dataset(url)
f = 'kaggle/urban-sound-8k/UrbanSound8K/UrbanSound8K/audio'
pluto.df_sound_data = pluto.make_dir_dataframe(f)</pre>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to search for and download an additional real-world audio dataset from the <em class="italic">Kaggle</em> website or your project. A hint is to use Pluto’s <strong class="source-inline">fetch_kaggle_data()</strong> and <strong class="source-inline">fetch_df()</strong> methods, and any of the audio augmentation <span class="No-Break">wrapper functions.</span></p>
			<p>A few under-the-hood<a id="_idIndexMarker852"/> methods<a id="_idIndexMarker853"/> make the process so easy to use. Pluto highly recommends that you review <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> before continuing with <span class="No-Break">the spectrogram.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>Audio Spectrogram</h1>
			<p>Before dissecting<a id="_idIndexMarker854"/> the Spectrogram, let’s review the fundamental differences between a Spectrogram and a Waveform plot. The Spectrogram graphs show the frequency components of a sound signal over time, focusing on frequency and intensity. In contrast, the Waveforms concentrate on the timing and amplitude of sounds. The difference is in the visual representation of the sound wave. The underlying data representation and the transformation methods are <span class="No-Break">the same.</span></p>
			<p>An audio Spectrogram<a id="_idIndexMarker855"/> is another visual representation of a sound wave, and you saw the Waveform graph in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. The <strong class="source-inline">_draw_spectrogram()</strong> helper method uses the Librosa library to import the audio file and convert it into an amplitude data one-dimensional array and a sampling rate in Hz. The next step is to use the Matplotlib library to draw the Spectrogram plot. Likewise, Pluto takes the output from the Librosa library function and uses the Matplotlib function to draw the fancy blue and yellow Waveform graph in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. The relevant code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># read audio file</strong>
data_amp, sam_rate = librosa.load(lname, mono=True)
<strong class="bold"># draw the spectrogram plot</strong>
spectrum, freq, ts, ax = pic.specgram(data_amp, Fs=sam_rate)</pre>
			<p>Here, the returned values are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">spectrum</strong> is a <strong class="source-inline">numpy.array</strong> type with <strong class="source-inline">shape(n,m)</strong>. For example, the result of plotting the Spectrogram of the c<em class="italic">ontrol piano scale in a D major</em> audio file <strong class="source-inline">shape()</strong> is equal to <strong class="source-inline">(129, 1057)</strong>. It represents the m-column of periodograms for each segment <span class="No-Break">or window.</span></li>
				<li><strong class="source-inline">freq</strong> is a <strong class="source-inline">numpy.array</strong> type with <strong class="source-inline">shape(n,)</strong>. Using the same example, <strong class="source-inline">freq shape</strong> is <strong class="source-inline">(129,)</strong>. It represents the frequencies corresponding to the elements (rows) in the <span class="No-Break"><strong class="source-inline">spectrum</strong></span><span class="No-Break"> array.</span></li>
				<li><strong class="source-inline">ts</strong> is a <strong class="source-inline">numpy.array</strong> type with <strong class="source-inline">shape(n,)</strong>. Using the same example as previously, <strong class="source-inline">ts shape</strong> is <strong class="source-inline">(1057,)</strong>. It represents the times corresponding to midpoints of <span class="No-Break"><strong class="source-inline">spectrum's</strong></span><span class="No-Break"> n-column.</span></li>
				<li><strong class="source-inline">ax</strong> is a <strong class="source-inline">matplotlib.image.AxesImage</strong> type. It is the image from the <span class="No-Break">Matplotlib library.</span></li>
			</ul>
			<p>Pluto draws the Spectrogram for the control piano scale in D major audio file using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the Spectrogram</strong>
pluto.draw_spectrogram(pluto.audio_control_dmajor)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/B17990_08_02.jpg" alt="Figure 8.2 – An audio Spectrogram of piano scale in D-major"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – An audio Spectrogram of piano scale in D-major</p>
			<p>Pluto displays the audio-play button<a id="_idIndexMarker856"/> in the Python Notebook, where you can listen to the audio. The button image looks like the following: </p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B17990_08_03.jpg" alt="Figure 8.3 – The audio play button"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The audio play button</p>
			<p>For comparison, the following is the Waveform graph from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> using the <span class="No-Break">helper function:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the Waveform</strong>
pluto._draw_audio(data_amp, sam_rate, 'Original: ' + fname)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B17990_08_04.jpg" alt="Figure 8.4 – Audio waveform of piano scale in D major"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Audio waveform of piano scale in D major</p>
			<p>The music sounds the same. Only the visual displays<a id="_idIndexMarker857"/> <span class="No-Break">are different.</span></p>
			<p>Sound engineers are trained to read Spectrogram plots to identify and remove unwanted noises, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Hum</strong>: This is usually the result of electrical<a id="_idIndexMarker858"/> noise in the recording. Its range is typically between 50 Hz and <span class="No-Break">60 Hz.</span></li>
				<li><strong class="bold">Buzz</strong>: This is the opposite of hum. It is the electrical noise<a id="_idIndexMarker859"/> of higher frequencies. Familiar sources are fluorescent light fixtures, on-camera microphones, and <span class="No-Break">high-pitched motors.</span></li>
				<li><strong class="bold">Hiss</strong>: This is a broadband noise, which is different<a id="_idIndexMarker860"/> from hum and buzz. It is typically concentrated<a id="_idIndexMarker861"/> at specific frequencies in both upper and lower spectrums. The usual suspects are <strong class="bold">heating, ventilation, and air conditioning</strong> (<strong class="bold">HVAC</strong>) systems or <span class="No-Break">motor fans.</span></li>
				<li><strong class="bold">Intermittent noises</strong>: These are commonly introduced<a id="_idIndexMarker862"/> by urban sounds such as thunders, birds, wind gusts, sirens, car horns, footsteps, knocking, coughs, or ringing <span class="No-Break">cell phones.</span></li>
				<li><strong class="bold">Digital clipping</strong>: This is when the audio is too loud to be recorded. It<a id="_idIndexMarker863"/> is the loss of the audio <span class="No-Break">signal’s peaks.</span></li>
				<li><strong class="bold">Gaps</strong>: Gaps or dropouts are silences due to missing cut-outs in the <span class="No-Break">audio recording.</span></li>
				<li><strong class="bold">Clicks and pops</strong>: These are noises in the recording caused<a id="_idIndexMarker864"/> by vinyl and other grooved media <span class="No-Break">recording devices.</span></li>
			</ul>
			<p>Pluto uses the <strong class="bold">Matplotlib</strong> library function, which has many parameters<a id="_idIndexMarker865"/> governing the display of the Spectrogram plots. Let’s use the three real-world audio datasets to illustrate other visual representations of <span class="No-Break">Spectrogram plots.</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor171"/>Various Spectrogram formats</h1>
			<p>There are many parameters<a id="_idIndexMarker866"/> Pluto can pass to the underlying <strong class="source-inline">specgram()</strong> method from the Matplotlib library. He will highlight only a <span class="No-Break">few parameters.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">You can print any function documentation by adding a question mark (<strong class="source-inline">?</strong>) at the end of the function in the <span class="No-Break">Python Notebook.</span></p>
			<p>For example, printing the documentation for the <strong class="source-inline">specgram()</strong> function is the following command: <strong class="source-inline">matplotlib.pyplot.specgram?</strong> The partial output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B17990_08_05.jpg" alt="Figure 8.5 – Partial print definition of specgram()"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Partial print definition of specgram()</p>
			<p>You can view the complete<a id="_idIndexMarker867"/> output of <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em> in the Python Notebook. Another example is printing Pluto’s <strong class="source-inline">draw_spectrogram()</strong> function documentation as <span class="No-Break">follows: </span><span class="No-Break"><strong class="source-inline">pluto.draw_spectrogram?</strong></span><span class="No-Break">.</span></p>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/B17990_08_06.jpg" alt="Figure 8.6 – The print definition of draw_spectrogram()"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – The print definition of draw_spectrogram()</p>
			<p>From <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em>, the simple one is changing the color map (<strong class="source-inline">cmap</strong>) variable. There are more than 60 color maps in the Matplotlib library. Thus, Pluto will choose a different <strong class="source-inline">cmap</strong> color for each audio dataset. Sound engineers may use different color maps to highlight specific frequency properties for spotting patterns or noises. Changing the visual representation does not affect the sound quality or the data. Thus, selecting the color map based solely on your preferences is acceptable. If vivid pink and blue are your favorite, choose the <strong class="source-inline">cool</strong> <span class="No-Break"><strong class="source-inline">cmap</strong></span><span class="No-Break"> value.</span></p>
			<p>The Spectrogram code for the music dataset is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the spectrogram in different color map</strong>
pluto.draw_spectrogram(pluto.df_music_data, cmap='plasma')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/B17990_08_07.jpg" alt="Figure 8.7 – Spectrogram of a music file (Sad39910)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Spectrogram of a music file (Sad39910)</p>
			<p>Every time Pluto runs the <strong class="source-inline">draw_spectrogram()</strong> wrapper function, a random<a id="_idIndexMarker868"/> audio file is selected from the dataset. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.7</em> is the audio of cinematic music with strong cello leads, and the <strong class="source-inline">plasma</strong> color map is a bright yellow transit to orange and <span class="No-Break">deep blue-purple.</span></p>
			<p>Likewise, for the human speech dataset, the command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the Spectrogram in different color map</strong>
pluto.draw_spectrogram(pluto.df_voice_data, cmap='cool')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="image/B17990_08_08.jpg" alt="Figure 8.8 – A spectrogram of human speech (1076_TAI_FEA_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – A spectrogram of human speech (1076_TAI_FEA_XX)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.8</em> is the audio of a woman saying “<em class="italic">The airplane is almost full</em>”. The <strong class="source-inline">cool</strong> color map is a fuchsia pink transit<a id="_idIndexMarker869"/> to <span class="No-Break">baby blue.</span></p>
			<p>Next, Pluto does the same for the urban sound dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the Spectrogram with different color map</strong>
pluto.draw_spectrogram(pluto.df_sound_data, cmap='brg')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/B17990_08_09.jpg" alt="Figure 8.9 – A spectrogram of urban sound (24347-8-0-88)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – A spectrogram of urban sound (24347-8-0-88)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em> sounds like a passing siren from an ambulance. The <strong class="source-inline">brg</strong> color map is blue, red, and green, making a striking and <span class="No-Break">dramatic graph.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">The challenge is a thought experiment. Is a particular color map with a multicolor such as a <strong class="source-inline">rainbow cmap</strong> or two colors such as <strong class="source-inline">ocean cmap</strong> more advantageous for different types of audio such as urban sound or music? In other words, is displaying the Spectrogram for a human singing an audio clip better in pink and magenta shades or multicolor <span class="No-Break">earth tones?</span></p>
			<p>In audio engineering, a <strong class="source-inline">window_hanning</strong> parameter uses weighted<a id="_idIndexMarker870"/> cosine to diminish the audio spectrum. Window-hanning is a technique used to reduce artifacts in the frequency domain of an audio signal. It uses a <strong class="source-inline">window</strong> function to gently taper off the signal’s amplitude near its edges, minimizing the effect of spectral leakage and reducing unwanted noise in the signal. Window-hanning also improves the time-domain resolution of the signal, making it easier to identify onsets and offsets with <span class="No-Break">greater precision.</span></p>
			<p>Pluto’s <strong class="source-inline">draw_spectrogram()</strong> method uses it as the default value. What if Pluto wants to see the raw signal without <strong class="source-inline">window_hanning</strong>? He can use <strong class="source-inline">window_none</strong> on the control and voice dataset, as per the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># control audio file</strong>
pluto.draw_spectrogram(pluto.audio_control_dmajor,
  window=matplotlib.mlab.window_none)
<strong class="bold"># Human speech</strong>
pluto.draw_spectrogram(pluto.df_voice_data,
  cmap='cool',
  window=matplotlib.mlab.window_none)</pre>
			<p>The output for the control piano scale in D major audio file is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B17990_08_10.jpg" alt="Figure 8.10 – Spectrogram with window_none, piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Spectrogram with window_none, piano scale (control-d-major)</p>
			<p>The output for the human speech dataset<a id="_idIndexMarker871"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B17990_08_11.jpg" alt="Figure 8.11 – A spectrogram with window_none, voice (1058_IEO_ANG_LO)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – A spectrogram with window_none, voice (1058_IEO_ANG_LO)</p>
			<p>The other values for window parameters are <strong class="source-inline">numpy.blackman</strong>, <strong class="source-inline">numpy.bartlett</strong>,<strong class="bold"> </strong><strong class="source-inline">scipy.signal</strong>, and <strong class="source-inline">scipy.signal.get_window</strong>, and the audio from <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.11</em> is a woman saying “<em class="italic">It is </em><span class="No-Break"><em class="italic">11 o’clock.</em></span><span class="No-Break">”</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment. Given the Spectrogram graph as an image, can you reverse-engineer and play the audio from the picture? A hint is to research the inverse Apectrogram software <span class="No-Break">and theories.</span></p>
			<p>Pluto continues plotting various Spectrograms<a id="_idIndexMarker872"/> and color maps because audio engineers may need to exaggerate or highlight a particular frequency or audio property. In addition, the augmentation technique is similar to the previous chapter. Thus, spending more time expanding your insight into the Spectrogram <span class="No-Break">is worthwhile.</span></p>
			<p>Pluto can use parameters individually or combine multiple parameters to produce a different desired outcome, such as using the <strong class="source-inline">sides</strong> parameter on the real-world music dataset and combining <strong class="source-inline">sides</strong> with the <strong class="source-inline">mode</strong> parameters on the control piano scale data. The commands are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># the control piano scale in D major</strong>
pluto.draw_spectrogram(pluto.df_music_data,
  cmap='plasma',
  sides='twosided')
<strong class="bold"># the music dataset</strong>
pluto.draw_spectrogram(pluto.audio_control_dmajor,
  window=matplotlib.mlab.window_none,
  sides='twoside<a id="_idTextAnchor172"/>d',
  mode='angle')</pre>
			<p>The output for the music with <strong class="source-inline">sides</strong> equal to <strong class="source-inline">twosided</strong> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B17990_08_12.jpg" alt="Figure 8.12 – A spectrogram with twosided﻿, music (Sad27307)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – A spectrogram with twosided<a id="_idTextAnchor173"/>, music (Sad27307)</p>
			<p>The output for the control piano scale<a id="_idIndexMarker873"/> audio with <strong class="source-inline">sides</strong> equal to <strong class="source-inline">twosided</strong> and <strong class="source-inline">mode</strong> equal to <strong class="source-inline">angle</strong> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B17990_08_13.jpg" alt="Figure 8.13 – Spectrogram with twosided and angle, music (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Spectrogram with twosided and angle, music (control-d-major)</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto has additional parameter combinations in the Python Notebook. Thus, it would be best if you modified or hacked the code. It will be fun to experience how different Spectrograms can look for different <span class="No-Break">real-world datasets.</span></p>
			<p>Next are the Mel-spectrogram and Chroma STFT plots. They are similar to <span class="No-Break">a Spectrogram.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor174"/>Mel-spectrogram and Chroma STFT plots</h1>
			<p>Pluto spends additional<a id="_idIndexMarker874"/> time plotting various<a id="_idIndexMarker875"/> Spectrograms because the augmentation technique is the same as in the Waveform graph in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. Pluto will write fewer new wrapper functions. He will reuse the methods from the previous chapter, but before that, let’s draw <span class="No-Break">more Spectrograms.</span></p>
			<p>The subjective unit of pitch, also known as the <strong class="bold">mel scale</strong>, is a pitch unit with equal distance<a id="_idIndexMarker876"/> between pitches. <em class="italic">S. S. Stevens, John Volkmann, and E. B. Newmann</em> proposed the mel scale in the scholarly paper titled, <em class="italic">A scale for the measurement of the psychological magnitude of pitch</em>, <span class="No-Break">in 1937.</span></p>
			<p>The math calculation for the mel scale is complex. Thus, Pluto relies on the <strong class="source-inline">melspectrogram()</strong> method from the Librosa library to perform the computation. The Pluto <strong class="source-inline">draw_melspectrogram()</strong> wrapper method uses the Librosa <strong class="source-inline">melspectrogram()</strong> function, and the code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippeet for the melspectrogram</strong>
mel = librosa.feature.melspectrogram(y=data_amp,
  sr=sam_rate,
  n_mels=128,
  fmax=8000)
mel_db = librosa.power_to_db(mel, ref=numpy.max)
self._draw_melspectrogram(mel_db, sam_rate, data_amp,
  cmap=cmap,
  fname=tname)</pre>
			<p>The entire function code is in the Python Notebook. Pluto draws the Mel-spectrogram for the control piano scale and the human speech datasets are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># Control piano scale</strong>
pluto.draw_melspectrogram(pluto.audio_control_dm<a id="_idTextAnchor175"/>ajor)
<strong class="bold"># Music dataset</strong>
pluto.draw_melspectrogram(pluto.df_voice_data, cmap='cool')</pre>
			<p>The output of the Mel-spectrogram<a id="_idIndexMarker877"/> for the control piano scale<a id="_idIndexMarker878"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B17990_08_14.jpg" alt="Figure 8.14 – Mel-spectrogram control piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Mel-spectrogram control piano scale (control-d-major)</p>
			<p>The output of the Mel-spectrogram for the human speech dataset is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B17990_08_15.jpg" alt="Figure 8.15 – Mel-spectrogram music (1016_MTI_FEA_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Mel-spectrogram music (1016_MTI_FEA_XX)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.15</em> audio is a man saying “<em class="italic">Maybe tomorrow, it will be cold.</em>” Every Mel-spectrogram has an audio-play button in the Python Notebook, where you can listen to the <span class="No-Break">audio file.</span></p>
			<p>The Chroma STFT is a signal’s sinusoidal frequency and local section<a id="_idIndexMarker879"/> phase content as it changes over time. <em class="italic">Dr. Dennis Gabor</em> introduced STFT, also known as the <strong class="bold">Gabor transform</strong>, in the scholarly paper, <em class="italic">Theory of Communication</em>, in 1944 and revised it <span class="No-Break">in 1945.</span></p>
			<p>Chroma STFT<a id="_idIndexMarker880"/> is a method of analyzing musical audio<a id="_idIndexMarker881"/> signals by decomposing them into their constituent frequencies and amplitudes with respect to time. It is used to characterize the instrument used in a given piece of music and identify unique features in short pieces of music. Chroma STFT is most often used to identify spectral characteristics of a music signal, allowing the components to be quantified and compared to other versions of the <span class="No-Break">same piece.</span></p>
			<p>Pluto adds slightly to the <strong class="source-inline">draw_melspectrogram()</strong> wrapper method to accommodate the Chroma STFT plot. The additional parameter is <strong class="source-inline">is_chroma</strong>, and the default value is <strong class="source-inline">False</strong>. The <strong class="source-inline">_draw_melspectrometer()</strong> helper function does not change. The code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for the chroma_stft</strong>
stft = librosa.feature.chroma_stft(data_amp,
  sr=sam_rate)
self._draw_melspectrogram(stft, sam_rate, data_amp,
  cmap=cmap,
  fname=tname,
  y_axis=yax,
  y_label=ylab)</pre>
			<p>The entire function code is on the Python Notebook. Pluto draws the Chroma STFT graphs for the control piano scale, the music, and the urban sound datasets <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># Control piano scale</strong>
pluto.draw_melspectrogram(pluto.audio_control_dmajor,
  is_chroma=True)
<strong class="bold"># Music dataset</strong>
pluto.draw_melspectrogram(pluto.df_music_data,
  is_chroma=True,
  cmap='plasma')
<strong class="bold"># Urban sound dataset</strong>
pluto.draw_melspectrogram(pluto.df_sound_data,
  is_chroma=True,
  cmap='brg')</pre>
			<p>The output for the Chroma STFT plot<a id="_idIndexMarker882"/> for the control piano scale<a id="_idIndexMarker883"/> in D major is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B17990_08_16.jpg" alt="Figure 8.16 – Chroma STFT, control piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Chroma STFT, control piano scale (control-d-major)</p>
			<p>The output for the music dataset is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B17990_08_17.jpg" alt="Figure 8.17 – Chrom﻿a STFT, music (Sad19513)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Chrom<a id="_idTextAnchor176"/>a STFT, music (Sad19513)</p>
			<p>The output for the urban<a id="_idIndexMarker884"/> sound dataset<a id="_idIndexMarker885"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/B17990_08_18.jpg" alt="Figure 8.18 – Chroma STFT, urban sound (192123-2-0-11)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Chroma STFT, urban sound (192123-2-0-11)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.17's</em> audio is cinematic music with a strong violin lead, and <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.18</em> sounds like noisy kids playing in an <span class="No-Break">outdoor playground.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">When generating new images or plots, Pluto automatically writes or exports the image files to the <strong class="source-inline">~/Data-Augmentation-with-Python/pluto_img</strong> directory. Thus, Pluto automatically saved the augmented images in <a href="B17990_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> and <a href="B17990_04.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> and the Waveform graph, audio Spectrogram, Mel-spectrogram, and Chroma STFT charts in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> and <a href="B17990_08.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. The helper function name is <strong class="source-inline">_drop_image()</strong> with the <strong class="source-inline">pluto[id].jpg</strong> file format, where <strong class="source-inline">id</strong> is an auto-increment integer from the <span class="No-Break"><strong class="source-inline">self.fname_id</strong></span><span class="No-Break"> variable.</span></p>
			<p>We have discussed in detail<a id="_idIndexMarker886"/> and written Python<a id="_idIndexMarker887"/> code for the audio Spectrogram, Mel-spectrogram, and Chroma STFT. Next, Pluto will describe how to perform audio augmentation with <span class="No-Break">a Spectrogram.</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor177"/>Spectrogram augmentation</h1>
			<p>Pluto will reuse most<a id="_idIndexMarker888"/> of the wrapper functions from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. You can reread the previous chapter if the following code seems challenging. Pluto will shorten his explanation of the wrapper functions because he assumes you are an expert at writing audio augmentation <span class="No-Break">wrapper functions.</span></p>
			<p>Audio Spectrogram, Mel-spectrogram, Chroma STFT, and Waveform charts take the returned amplitude data and sampling rate from the Librosa <strong class="source-inline">load()</strong> function reading an audio file. There is an additional transformation of the amplitude data, but they serve the same goal of visualizing the sound wave <span class="No-Break">and frequencies.</span></p>
			<p>After reviewing many scholarly published papers, Pluto concluded that the audio augmentation techniques in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> apply equally well to the audio Spectrogram, Mel-spectrogram, and Chroma STFT. In particular, he referred to the scholarly paper, <em class="italic">Audio Augmentation for Speech Recognition</em> by Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, published in 2015; <em class="italic">Data augmentation approaches for improving animal audio classification</em> by Loris Nannia, Gianluca Maguoloa, and Michelangelo Paci, published in 2020; and <em class="italic">Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification</em> by Justin Salamon and Juan Pablo Bello, published <span class="No-Break">in 2017.</span></p>
			<p>Intuitively, there shouldn’t be any difference from the technique in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> because the underlying amplitude data and sampling rate are the same. In other words, you can use <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> audio augmentation functions for the audio Spectrogram, Mel-spectrogram, and Chroma STFT, such as the <span class="No-Break">following techniques:</span></p>
			<ul>
				<li><span class="No-Break">Time-stretching</span></li>
				<li><span class="No-Break">Time-shifting</span></li>
				<li><span class="No-Break">Pitch-scaling</span></li>
				<li><span class="No-Break">Noise injection</span></li>
				<li><span class="No-Break">Polarity inversion</span></li>
				<li><span class="No-Break">Low-pass filter</span></li>
				<li><span class="No-Break">High-pass filter</span></li>
				<li><span class="No-Break">Ban-pass filter</span></li>
				<li><span class="No-Break">Low-shelf filter</span></li>
				<li><span class="No-Break">High-shelf filter</span></li>
				<li><span class="No-Break">Band-stop filter</span></li>
				<li><span class="No-Break">Peak filter</span></li>
			</ul>
			<p>There are others, such as <strong class="source-inline">Masking</strong> and <strong class="source-inline">Gaps</strong>. They are available<a id="_idIndexMarker889"/> from the <strong class="source-inline">audiomentation</strong> library. The <strong class="bold">safe</strong> level mentioned in the previous<a id="_idIndexMarker890"/> chapter applies equally to the audio Spectrogram, Mel-spectrogram, and <span class="No-Break">Chroma STFT.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">You can alter any Python functions by overriding them in the <strong class="source-inline">correct</strong> class. Pluto functions belong to the <strong class="source-inline">PacktDataAug</strong> class. Thus, you can hack and override any of Pluto’s methods by adding the <strong class="source-inline">@add_method(PacktDataAug)</strong> code line before the <span class="No-Break">function definition.</span></p>
			<p>Pluto needs to hack the <strong class="source-inline">_audio_transform()</strong> helper function and includes the new<strong class="bold"> </strong><strong class="source-inline">is_waveform</strong> parameter setting the default to <strong class="source-inline">True</strong> so it will not affect methods in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>. The definition of the new method is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># add is_waveform parameter</strong>
@add_method(PacktDataAug)
def _audio_transform(self, df, xtransform,
  Titl<a id="_idTextAnchor178"/>e = '',
  is_waveform = True):</pre>
			<p>The updated code snippet<a id="_idIndexMarker891"/> is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># keep the default to be same for Chapter 7, Waveform graph</strong>
if (is_waveform):
  <strong class="bold"># augmented waveform</strong>
  self._draw_audio(xaug, sam_rate,
    title + ' Augmented: ' + fname)
  display(IPython.display.Audio(xaug, rate=sam_rate))
  <strong class="bold"># original waveform</strong>
  self._draw_audio(data_amp, sam_rate, 'Original: ' + fname)
<strong class="bold"># update to use spectrogram, me-spectrogram, and Chroma</strong>
else:
  xdata = [xaug, sam_rate, lname, 'Pluto']
  self.draw_spectrogram(xdata)
  self.draw_melspectrogram(xdata)
  self.draw_melspectrogram(xdata, is_chroma=True)</pre>
			<p>Thus, the <strong class="source-inline">is_waveform</strong> parameter is to use the Waveform graphs in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> or the audio Spectrogram, Mel-spectrogram, and Chroma STFT charts. That’s it, and this is why we love coding with Pluto. He follows the best object-oriented coding practices, and his functions are in <span class="No-Break">one class.</span></p>
			<p>Pluto adds the new parameter to the <strong class="source-inline">play_aug_time_shift()</strong> wrapper function and tests it with the control data. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment the audio with time shift</strong>
pluto.play_aug_time_shift(pluto.audio_control_dmajor,
  min_fraction=0.8,
  is_waveform=False)</pre>
			<p>The output for the audio Spectrogram<a id="_idIndexMarker892"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B17990_08_19.jpg" alt="Figure 8.19 – Spectrogram, time shift, piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Spectrogram, time shift, piano scale (control-d-major)</p>
			<p>The output for the Mel-spectrogram is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B17990_08_20.jpg" alt="Figure 8.20 – Mel-spectrogram, time shift, piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – Mel-spectrogram, time shift, piano scale (control-d-major)</p>
			<p>The output for the Chroma STFT is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/B17990_08_21.jpg" alt="Figure 8.21 – Chroma STFT, time shift, piano scale (control-d-major)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Chroma STFT, time shift, piano scale (control-d-major)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.19</em>, <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.20</em>, and <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.21</em> play the piano scale<a id="_idIndexMarker893"/> in D major shift to the left by about 2 seconds. In other words, the audio started with the <strong class="bold">G note</strong>, looped around, and finished on the <strong class="bold">F# note</strong>. Pluto recommends listening to the before and after effects of the Python Notebook as the easiest method to <span class="No-Break">understand it.</span></p>
			<p>Pluto does the same for the human speech dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augment audio using time shift</strong>
pluto.play_aug_time_shift(pluto.df_voice_data,
  min_fraction=0.8,
  is_waveform=False)</pre>
			<p>The output for the audio Spectrogram is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/B17990_08_22.jpg" alt="Figure 8.22 – Spectrogram, time shift, human voice (1085_ITS_ANG_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Spectrogram, time shift, human voice (1085_ITS_ANG_XX)</p>
			<p>The output for the Mel-spectrogram<a id="_idIndexMarker894"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="image/B17990_08_23.jpg" alt="Figure 8.23 – Mel-spectrogram, time shift, human voice (1085_ITS_ANG_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Mel-spectrogram, time shift, human voice (1085_ITS_ANG_XX)</p>
			<p>The output for Chroma STFT is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="image/B17990_08_24.jpg" alt="Figure 8.24 – Chroma STFT, time shift, human voice (1085_ITS_ANG_XX)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Chroma STFT, time shift, human voice (1085_ITS_ANG_XX)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.22</em>, <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.23</em>, and <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.24's</em> original audio is a man’s voice saying “<em class="italic">We will stop in a couple of minutes.</em>” The augmented version is shifted to “<em class="italic">stop in a couple of minutes [silence] we will</em>.” Pluto can hear the difference in the before-and-after augmentation<a id="_idIndexMarker895"/> effect in the Python Notebook. The goal of audio augmentation is the same for Spectrogram and Waveform graphs, which is to increase the AI accuracy prediction by increasing the <span class="No-Break">input data.</span></p>
			<p>The results for the music and urban sound dataset are shifted similarly. Pluto has the time-shift code in the Python Notebook, where you can run it and see and hear the result. Furthermore, Pluto will skip describing the results for other audio augmentation functions in this chapter. It is because the results are the same as in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, and the wrapper functions code is in the Python Notebook. However, he will explain the <strong class="source-inline">play_aug_noise_injection()</strong> function because this function can extend to specific topics discussing how sound engineers <span class="No-Break">use Spectrograms.</span></p>
			<p>Sound engineers use standard audio Spectrograms and various other Spectrograms to spot and remove unwanted noises, such as hums, buzz, hiss, clips, gaps, clicks, and pops. Audio augmentation goals are the opposite. We add unwanted noises to the recording within a safe range. Thus, we increase the training datasets and improve the AI <span class="No-Break">prediction accuracy.</span></p>
			<p>Pluto adds white noise to the music dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># augment audio with noise injection</strong>
pluto.play_aug_noise_injection(pluto.df_music_data,
  min_amplitude=0.008,
  max_amplitude=0.05,
  is_waveform=False)</pre>
			<p>The output for the audio Spectrogram<a id="_idIndexMarker896"/> is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B17990_08_25.jpg" alt="Figure 8.25 – Spectrogram, noise injection, music (Happy41215)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Spectrogram, noise injection, music (Happy41215)</p>
			<p>The output for the Mel-spectrogram is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B17990_08_26.jpg" alt="Figure 8.26 – Mel-spectrogram, noise injection, music (Happy41215)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – Mel-spectrogram, noise injection, music (Happy41215)</p>
			<p>The output for the Chroma STFT is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="image/B17990_08_27.jpg" alt="Figure 8.27 – Chroma STFT, noise injection, music (Happy41215)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – Chroma STFT, noise injection, music (Happy41215)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.25</em>, <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.26</em>, and <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.27</em> play heavy drums, light electronic bells, and heavy electronic guitars<a id="_idIndexMarker897"/> with a medium level of <span class="No-Break">white noise.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment. You are part of a team developing a self-driving car system, and your goal is to recognize or identify car honking while driving. How would you augment the audio data? A hint is thinking about real-world driving conditions with traffic or <span class="No-Break">urban noises.</span></p>
			<p>If you have hums, buzz, or pops audio files, you can inject them into the recording by alternating the <strong class="source-inline">play_aug_noise_injection()</strong> wrapper function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># Original use white noise, code snippet</strong>
xtransform = audiomentations.AddGaussianNoise(
  min_amplitude=min_amplitude,
  max_amplitude=max_amplitude,
  p=1.0)
<strong class="bold"># Update to using unwanted noise file</strong>
xtransform = audiomentations.AddShortNoises(
  sounds_path="~/path_to_unwanted_noise_file",
  min_snr_in_db=3.0,
  max_snr_in_db=30.0,
  noise_rms="relative_to_whole_input",
  min_time_between_sounds=2.0,
  max_time_between_sounds=8.0,
  noise_transform=PolarityInversion(),
  p=1.0)</pre>
			<p>The preceding code snippet and complete documentation can be found in the <strong class="source-inline">audiomentations</strong> library<a id="_idIndexMarker898"/> <span class="No-Break">on GitHub.</span></p>
			<p>The next topic is a novel idea using a Spectrogram as an image input for deep learning <span class="No-Break">image classification.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor179"/>Spectrogram images</h1>
			<p>Fundamentally, audio data<a id="_idIndexMarker899"/> is time-series data. Thus AI uses<a id="_idIndexMarker900"/> a time-series algorithm, such as the <strong class="bold">autoregressive integrated moving average</strong> (<strong class="bold">ARIMA</strong>) or <strong class="bold">exponential smoothing</strong> (<strong class="bold">ES</strong>) algorithm for audio classification. However, there<a id="_idIndexMarker901"/> is a better method. You use the Spectrogram as an image representing the audio sound, not the time-series numerical array, for input. Using images as the input data, you can leverage the robust neural network algorithm to classify audio <span class="No-Break">more accurately.</span></p>
			<p>Strictly speaking, this topic does not directly pertain to new audio augmentation techniques. Still, it is an essential topic for data scientists to understand. However, Pluto will not write Python code for building a neural network model using Spectrograms <span class="No-Break">as input.</span></p>
			<p>Deep learning image classification, also known as the machine learning model that uses the artificial neural networks algorithm, achieved an unprecedented accuracy level that exceeds 98% accuracy recently. Many AI scientists apply deep learning techniques to audio datasets, such as <em class="italic">Audio Spectrogram Representations for Processing with Convolutional Neural Networks</em> by Lonce Wyse, published in 2017, and <em class="italic">Deep Learning Audio Spectrograms Processing to the Early COVID-19 Detection</em> by Ciro Rodriguez, Daniel Angeles, Renzo Chafloque, Freddy Kaseng, and Bishwajeet Pandey, published <span class="No-Break">in 2020.</span></p>
			<p>The technique takes an audio Spectrogram<a id="_idIndexMarker902"/> as the image input, not the audio amplitude, sampling rate, or Mel scale. For example, the music dataset (MEC) goal is to classify a piece of music clip as having a <strong class="source-inline">happy</strong> or <strong class="source-inline">sad</strong> mood. Pluto can generate all the audio files to audio Spectrograms and save them on the local drive. He will use the Fast.ai robust AI framework and libraries to create an image classification model. He can achieve 95% accuracy <span class="No-Break">or higher.</span></p>
			<p>The big question is can you use the image augmentation methods discussed in <a href="B17990_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> and <a href="B17990_04.xhtml#_idTextAnchor082"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> to apply <span class="No-Break">to Spectrogram?</span></p>
			<p>It depends on the safe level and the objective of the AI model. For example, using the image augmentation technique, vertically flipping a spectogram involves flipping high to low frequencies and vice versa. Pluto wonders how that would affect the music’s mood. It could be an <strong class="bold">unsafe</strong> technique. However, image noise injection methods with low noise values could be a safe technique with a Spectrogram. Pluto thinks it is more suitable to stay with the audio augmentation techniques in <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>Similar deep learning methods can be applied to the human speech (<strong class="source-inline">CREMA-D</strong>) dataset to classify the age, sex, or ethnicity of <span class="No-Break">the speaker.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">This is a thought experiment. Can you use speech-to-text software to convert the voice into text and use text augmentation functions in <a href="B17990_05.xhtml#_idTextAnchor101"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> and <a href="B17990_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>? A hint is to think about the scope of the project. For example, it could work if the AI aims to infer sentiment analysis but not if the goal is to identify male or <span class="No-Break">female voices.</span></p>
			<p>For the urban sound (US8K) dataset, Pluto could use the deep learning multilabel classification to identify different types of sound in an urban sound clip, such as a jackhammer, wind, kids playing, rain, dogs barking, <span class="No-Break">or gunshots.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to refactor the <strong class="source-inline">Pluto</strong> class to make it faster and more compact. You should also include all the image and text wrapper and helper functions from previous chapters. Pluto encourages you to create and upload your library to <em class="italic">GitHub and PyPI.org</em>. Furthermore, you don’t have to name the class <strong class="source-inline">PacktDataAug</strong>, but it would give Pluto and his human companion a great big smile if you cited or mentioned the book. The code goals were ease of understanding, reusable patterns, and teaching you about the Python Notebook. Thus, refactoring the code as a Python library would be relatively painless <span class="No-Break">and fun.</span></p>
			<p>We have covered<a id="_idIndexMarker903"/> audio Spectrogram, Mel-spectrogram, and Chroma STFT representation and augmentation, including the technique of using Spectrograms as image input to the deep learning image classification model. It is time for <span class="No-Break">a summary.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor180"/>Summary</h1>
			<p>Audio augmentation is challenging to explain in a book format. Still, we gain a deeper understanding of audio amplitude, frequency, and sampling rate with additional visualization techniques, such as the audio Spectrogram, Mel-spectrogram, and Chroma STFT. Furthermore, in the Python Notebook, you can listen to the before-and-after effects of the <span class="No-Break">audio augmentation.</span></p>
			<p>Compared to the previous chapter, Waveform graphs show the amplitude of a signal over time, giving an understanding of its shape and structure. Spectrogram graphs show a visual representation of the frequencies of a signal over time, providing a deeper insight into the harmonic content of <span class="No-Break">the sound.</span></p>
			<p>An Audio Spectrogram comes in many variations, whether <strong class="bold">color mapping</strong>, <strong class="bold">window filtering</strong>, <strong class="bold">spectrum sides</strong>, <strong class="bold">magnitude mode</strong>, or <strong class="bold">frequency scale</strong>, among many more in the underlying Matplotlib <strong class="source-inline">specgram()</strong> function. Pluto uses Python code in wrapper functions on a few Spectrogram types. The majority of Spectrogram variations are up to you to explore by expanding the <strong class="source-inline">Pluto</strong> object with additional wrapper functions. Using Pluto’s object-oriented best practices, the function wrapper concept, and the audiomentations library, it is easy to expand Pluto with additional <span class="No-Break">wrapper functions.</span></p>
			<p>For Spectrogram augmentation techniques, they are the same techniques as those from <a href="B17990_07.xhtml#_idTextAnchor135"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, such as time -shifting, time-stretching, pitch-scaling, noise injections, bandpass filters, and many others. Intuitively, there should be no difference because in the previous chapter, you choose to visualize the sound wave in Waveform graphs, and in this chapter, you drew them in the audio Spectrogram, Mel-spectrogram, and Chrom STFT plots. Thus, the underlying data is <span class="No-Break">the same.</span></p>
			<p>Pluto has to only modify the <strong class="source-inline">_audio_transform()</strong> helper method with an additional <strong class="source-inline">is_waveform</strong> parameter. The Python code becomes deceptively simple and repetitive afterward, but it hides the robust power of the audiomentations library and Pluto object-oriented <span class="No-Break">best practices.</span></p>
			<p>Throughout the chapter, there were <strong class="bold">fun facts</strong> and <strong class="bold">fun challenges</strong>. Pluto hopes you will take the advantages provided and expand the experience beyond the scope of <span class="No-Break">this chapter.</span></p>
			<p>The next chapter moves beyond the typical data types, such as image, text, and audio, to tubular <span class="No-Break">data augmentation.</span></p>
		</div>
	

		<div id="_idContainer255" class="Content">
			<h1 id="_idParaDest-175"><a id="_idTextAnchor181"/>Part 5: Tabular Data Augmentation</h1>
			<p>This part includes the <span class="No-Break">following chapter:</span></p>
			<ul>
				<li><a href="B17990_09.xhtml#_idTextAnchor182"><em class="italic">Chapter 9</em></a>, <em class="italic">Tabular Data Augmentation</em></li>
			</ul>
		</div>
	</body></html>