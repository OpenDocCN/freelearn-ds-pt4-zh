- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio Data Augmentation with Spectrogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we visualized the sound using the Waveform graph. An
    audio spectrogram is another visualizing method for seeing the audio components.
    The inputs to the Spectrogram are a one-dimensional array of **amplitude** values
    and the **sampling rate**. They are the same inputs as the Waveform graph.
  prefs: []
  type: TYPE_NORMAL
- en: An audio **s****pectrogram** is sometimes called a **sonograph**, **sonogram**,
    **voiceprint**, or **voicegram**. The Spectrogram is a more detailed representation
    of sound than the Waveform graph. It shows a correlation between frequency and
    amplitude (loudness) over time, which helps visualize the frequency content in
    a signal. Spectrograms make it easier to identify musical elements, detect melodic
    patterns, recognize frequency-based effects, and compare the results of different
    volume settings. Additionally, the Spectrogram can be more helpful in identifying
    non-musical aspects of a signal, such as noise and interference from other frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: The typical usage is for music, human speech, and sonar. A short standard definition
    is a spectrum of frequency maps with time duration. In other words, the *y* axis
    is the frequency in **Hz or kHz**, and the *x* axis is the time duration in **seconds
    or milliseconds**. Sometimes, the graph comes with a color index for the amplitude
    level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto will explain the code in the Python Notebook later in the chapter, but
    here is a sneak peek of an audio Spectrogram. The command for drawing the *control
    piano scale in the D major* audio file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – An audio spectrogram of piano scale in D major](img/Image94335.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – An audio spectrogram of piano scale in D major
  prefs: []
  type: TYPE_NORMAL
- en: Before Pluto demystifies the audio Spectrogram, you should review [*Chapter
    7*](B17990_07.xhtml#_idTextAnchor135) if the audio concepts and keywords sound
    alien to you. This chapter relies heavily on the knowledge and practices from
    [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8**.1*, Pluto uses the Matplotlib library to draw the audio spectrograph.
    The primary input is the amplitude array and the sampling rate. The library does
    all the heavy calculations, and other libraries, such as the Librosa or SciPy
    library, can perform the same task. In particular, Matplotlib can generate many
    types of audio spectrographs from the same input. Pluto will dig deeper into types
    of spectrographs a bit later, but first, let’s break down the steps of how the
    library constructs an audio spectrograph. The five high-level tasks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the audio stream into overlapping segments, also known as **windows**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating the **Short-Time Fourier Transform** (**STFT**) value on each window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting the windows’ value into **decibels** (**dB**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linking the windows together as in the original audio sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displaying the result in a graph with the *y* axis as Hz, the *x* axis as seconds,
    and dB as a color-coded value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The math for the previous five steps is complex, and the chapter’s goal is to
    use a Spectrogram to visualize the sound and augment the audio file. Thus, we
    rely on audio libraries to perform the math calculation.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the underlying data representing the Spectrogram is the same as
    the Waveform format. Therefore, the audio augmentation techniques are the same.
    Consequently, the resulting augmented audio file will sound the same. The visual
    representation is the only difference between the Spectrogram and the Waveform
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The majority of this chapter will cover the audio Spectrogram standard format,
    a variation of a Spectrogram, **Mel-spectrogram**, and **Chroma** STFT. The augmentation
    techniques represent a shorter section because you have learned the method in
    the previous chapter. We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing and downloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio Spectrogram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various Spectrogram formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mel-spectrogram and Chroma STFT plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectrogram augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectrogram image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: The **Kay Electric Company** introduced the first commercially available machine
    for audio spectrographic analysis in 1951\. The black-and-white image was named
    a sonograph or sonogram for visualizing bird songs. In 1966, **St. Martin’s Press**
    used sonography for the book *Golden Field Guide to Birds of North America*. Spectrograms
    were favored over sonogram terminology around 1995 during the digital age. Spectrograms
    or sonograms were not limited to the study of birds in the early days. The US
    military used Spectrogram for encryption in the early 1940s and continues forward,
    as evidenced by the publication *Cryptologic Quarterly*, volume 38, published
    by the **Center for Cryptologic History** in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter reuses the audio augmentation functions and the real-world audio
    datasets from [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135). Thus, we will start
    by initializing Pluto and downloading the real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing and downloading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start with loading the `data_augmentation_with_python_chapter_8.ipynb` file
    on Google Colab or your chosen Python Notebook or JupyterLab environment. From
    this point onward, the code snippets are from the Python Notebook, which contains
    the complete functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following initializing and downloading steps should be familiar to you
    because we have done them six times. The following code snippet is the same as
    from [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to search for and download an additional real-world audio
    dataset from the *Kaggle* website or your project. A hint is to use Pluto’s `fetch_kaggle_data()`
    and `fetch_df()` methods, and any of the audio augmentation wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: A few under-the-hood methods make the process so easy to use. Pluto highly recommends
    that you review [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135) before continuing
    with the spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Audio Spectrogram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before dissecting the Spectrogram, let’s review the fundamental differences
    between a Spectrogram and a Waveform plot. The Spectrogram graphs show the frequency
    components of a sound signal over time, focusing on frequency and intensity. In
    contrast, the Waveforms concentrate on the timing and amplitude of sounds. The
    difference is in the visual representation of the sound wave. The underlying data
    representation and the transformation methods are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'An audio Spectrogram is another visual representation of a sound wave, and
    you saw the Waveform graph in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135).
    The `_draw_spectrogram()` helper method uses the Librosa library to import the
    audio file and convert it into an amplitude data one-dimensional array and a sampling
    rate in Hz. The next step is to use the Matplotlib library to draw the Spectrogram
    plot. Likewise, Pluto takes the output from the Librosa library function and uses
    the Matplotlib function to draw the fancy blue and yellow Waveform graph in [*Chapter
    7*](B17990_07.xhtml#_idTextAnchor135). The relevant code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the returned values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spectrum` is a `numpy.array` type with `shape(n,m)`. For example, the result
    of plotting the Spectrogram of the c*ontrol piano scale in a D major* audio file
    `shape()` is equal to `(129, 1057)`. It represents the m-column of periodograms
    for each segment or window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freq` is a `numpy.array` type with `shape(n,)`. Using the same example, `freq
    shape` is `(129,)`. It represents the frequencies corresponding to the elements
    (rows) in the `spectrum` array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ts` is a `numpy.array` type with `shape(n,)`. Using the same example as previously,
    `ts shape` is `(1057,)`. It represents the times corresponding to midpoints of
    `spectrum''s` n-column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ax` is a `matplotlib.image.AxesImage` type. It is the image from the Matplotlib
    library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pluto draws the Spectrogram for the control piano scale in D major audio file
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – An audio Spectrogram of piano scale in D-major](img/B17990_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – An audio Spectrogram of piano scale in D-major
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto displays the audio-play button in the Python Notebook, where you can
    listen to the audio. The button image looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The audio play button](img/B17990_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The audio play button
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, the following is the Waveform graph from [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135)
    using the helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Audio waveform of piano scale in D major](img/B17990_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Audio waveform of piano scale in D major
  prefs: []
  type: TYPE_NORMAL
- en: The music sounds the same. Only the visual displays are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sound engineers are trained to read Spectrogram plots to identify and remove
    unwanted noises, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hum**: This is usually the result of electrical noise in the recording. Its
    range is typically between 50 Hz and 60 Hz.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Buzz**: This is the opposite of hum. It is the electrical noise of higher
    frequencies. Familiar sources are fluorescent light fixtures, on-camera microphones,
    and high-pitched motors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hiss**: This is a broadband noise, which is different from hum and buzz.
    It is typically concentrated at specific frequencies in both upper and lower spectrums.
    The usual suspects are **heating, ventilation, and air conditioning** (**HVAC**)
    systems or motor fans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermittent noises**: These are commonly introduced by urban sounds such
    as thunders, birds, wind gusts, sirens, car horns, footsteps, knocking, coughs,
    or ringing cell phones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Digital clipping**: This is when the audio is too loud to be recorded. It
    is the loss of the audio signal’s peaks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaps**: Gaps or dropouts are silences due to missing cut-outs in the audio
    recording.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clicks and pops**: These are noises in the recording caused by vinyl and
    other grooved media recording devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluto uses the **Matplotlib** library function, which has many parameters governing
    the display of the Spectrogram plots. Let’s use the three real-world audio datasets
    to illustrate other visual representations of Spectrogram plots.
  prefs: []
  type: TYPE_NORMAL
- en: Various Spectrogram formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many parameters Pluto can pass to the underlying `specgram()` method
    from the Matplotlib library. He will highlight only a few parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: You can print any function documentation by adding a question mark (`?`) at
    the end of the function in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, printing the documentation for the `specgram()` function is the
    following command: `matplotlib.pyplot.specgram?` The partial output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Partial print definition of specgram()](img/B17990_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Partial print definition of specgram()
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the complete output of *Figure 8**.5* in the Python Notebook.
    Another example is printing Pluto’s `draw_spectrogram()` function documentation
    as follows: `pluto.draw_spectrogram?`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The print definition of draw_spectrogram()](img/B17990_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – The print definition of draw_spectrogram()
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 8**.5*, the simple one is changing the color map (`cmap`) variable.
    There are more than 60 color maps in the Matplotlib library. Thus, Pluto will
    choose a different `cmap` color for each audio dataset. Sound engineers may use
    different color maps to highlight specific frequency properties for spotting patterns
    or noises. Changing the visual representation does not affect the sound quality
    or the data. Thus, selecting the color map based solely on your preferences is
    acceptable. If vivid pink and blue are your favorite, choose the `cool` `cmap`
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spectrogram code for the music dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Spectrogram of a music file (Sad39910)](img/B17990_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Spectrogram of a music file (Sad39910)
  prefs: []
  type: TYPE_NORMAL
- en: Every time Pluto runs the `draw_spectrogram()` wrapper function, a random audio
    file is selected from the dataset. *Figure 8**.7* is the audio of cinematic music
    with strong cello leads, and the `plasma` color map is a bright yellow transit
    to orange and deep blue-purple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, for the human speech dataset, the command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – A spectrogram of human speech (1076_TAI_FEA_XX)](img/B17990_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – A spectrogram of human speech (1076_TAI_FEA_XX)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.8* is the audio of a woman saying “*The airplane is almost full*”.
    The `cool` color map is a fuchsia pink transit to baby blue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Pluto does the same for the urban sound dataset using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – A spectrogram of urban sound (24347-8-0-88)](img/B17990_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – A spectrogram of urban sound (24347-8-0-88)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.9* sounds like a passing siren from an ambulance. The `brg` color
    map is blue, red, and green, making a striking and dramatic graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is a thought experiment. Is a particular color map with a multicolor
    such as a `rainbow cmap` or two colors such as `ocean cmap` more advantageous
    for different types of audio such as urban sound or music? In other words, is
    displaying the Spectrogram for a human singing an audio clip better in pink and
    magenta shades or multicolor earth tones?
  prefs: []
  type: TYPE_NORMAL
- en: In audio engineering, a `window_hanning` parameter uses weighted cosine to diminish
    the audio spectrum. Window-hanning is a technique used to reduce artifacts in
    the frequency domain of an audio signal. It uses a `window` function to gently
    taper off the signal’s amplitude near its edges, minimizing the effect of spectral
    leakage and reducing unwanted noise in the signal. Window-hanning also improves
    the time-domain resolution of the signal, making it easier to identify onsets
    and offsets with greater precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto’s `draw_spectrogram()` method uses it as the default value. What if Pluto
    wants to see the raw signal without `window_hanning`? He can use `window_none`
    on the control and voice dataset, as per the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the control piano scale in D major audio file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Spectrogram with window_none, piano scale (control-d-major)](img/B17990_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Spectrogram with window_none, piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the human speech dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – A spectrogram with window_none, voice (1058_IEO_ANG_LO)](img/B17990_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – A spectrogram with window_none, voice (1058_IEO_ANG_LO)
  prefs: []
  type: TYPE_NORMAL
- en: The other values for window parameters are `numpy.blackman`, `numpy.bartlett`,`scipy.signal`,
    and `scipy.signal.get_window`, and the audio from *Figure 8**.11* is a woman saying
    “*It is* *11 o’clock.*”
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Here is a thought experiment. Given the Spectrogram graph as an image, can you
    reverse-engineer and play the audio from the picture? A hint is to research the
    inverse Apectrogram software and theories.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto continues plotting various Spectrograms and color maps because audio engineers
    may need to exaggerate or highlight a particular frequency or audio property.
    In addition, the augmentation technique is similar to the previous chapter. Thus,
    spending more time expanding your insight into the Spectrogram is worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto can use parameters individually or combine multiple parameters to produce
    a different desired outcome, such as using the `sides` parameter on the real-world
    music dataset and combining `sides` with the `mode` parameters on the control
    piano scale data. The commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the music with `sides` equal to `twosided` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.12 – A spectrogram with twosided\uFEFF, music (Sad27307)](img/B17990_08_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – A spectrogram with twosided, music (Sad27307)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the control piano scale audio with `sides` equal to `twosided`
    and `mode` equal to `angle` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Spectrogram with twosided and angle, music (control-d-major)](img/B17990_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Spectrogram with twosided and angle, music (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has additional parameter combinations in the Python Notebook. Thus, it
    would be best if you modified or hacked the code. It will be fun to experience
    how different Spectrograms can look for different real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next are the Mel-spectrogram and Chroma STFT plots. They are similar to a Spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Mel-spectrogram and Chroma STFT plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pluto spends additional time plotting various Spectrograms because the augmentation
    technique is the same as in the Waveform graph in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135).
    Pluto will write fewer new wrapper functions. He will reuse the methods from the
    previous chapter, but before that, let’s draw more Spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: The subjective unit of pitch, also known as the **mel scale**, is a pitch unit
    with equal distance between pitches. *S. S. Stevens, John Volkmann, and E. B.
    Newmann* proposed the mel scale in the scholarly paper titled, *A scale for the
    measurement of the psychological magnitude of pitch*, in 1937.
  prefs: []
  type: TYPE_NORMAL
- en: 'The math calculation for the mel scale is complex. Thus, Pluto relies on the
    `melspectrogram()` method from the Librosa library to perform the computation.
    The Pluto `draw_melspectrogram()` wrapper method uses the Librosa `melspectrogram()`
    function, and the code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The entire function code is in the Python Notebook. Pluto draws the Mel-spectrogram
    for the control piano scale and the human speech datasets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the Mel-spectrogram for the control piano scale is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Mel-spectrogram control piano scale (control-d-major)](img/B17990_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Mel-spectrogram control piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the Mel-spectrogram for the human speech dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Mel-spectrogram music (1016_MTI_FEA_XX)](img/B17990_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Mel-spectrogram music (1016_MTI_FEA_XX)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.15* audio is a man saying “*Maybe tomorrow, it will be cold.*”
    Every Mel-spectrogram has an audio-play button in the Python Notebook, where you
    can listen to the audio file.'
  prefs: []
  type: TYPE_NORMAL
- en: The Chroma STFT is a signal’s sinusoidal frequency and local section phase content
    as it changes over time. *Dr. Dennis Gabor* introduced STFT, also known as the
    **Gabor transform**, in the scholarly paper, *Theory of Communication*, in 1944
    and revised it in 1945.
  prefs: []
  type: TYPE_NORMAL
- en: Chroma STFT is a method of analyzing musical audio signals by decomposing them
    into their constituent frequencies and amplitudes with respect to time. It is
    used to characterize the instrument used in a given piece of music and identify
    unique features in short pieces of music. Chroma STFT is most often used to identify
    spectral characteristics of a music signal, allowing the components to be quantified
    and compared to other versions of the same piece.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto adds slightly to the `draw_melspectrogram()` wrapper method to accommodate
    the Chroma STFT plot. The additional parameter is `is_chroma`, and the default
    value is `False`. The `_draw_melspectrometer()` helper function does not change.
    The code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The entire function code is on the Python Notebook. Pluto draws the Chroma
    STFT graphs for the control piano scale, the music, and the urban sound datasets
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the Chroma STFT plot for the control piano scale in D major
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Chroma STFT, control piano scale (control-d-major)](img/B17990_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Chroma STFT, control piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the music dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.17 – Chrom\uFEFFa STFT, music (Sad19513)](img/B17990_08_17.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Chroma STFT, music (Sad19513)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the urban sound dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Chroma STFT, urban sound (192123-2-0-11)](img/B17990_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Chroma STFT, urban sound (192123-2-0-11)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.17''s* audio is cinematic music with a strong violin lead, and
    *Figure 8**.18* sounds like noisy kids playing in an outdoor playground.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: When generating new images or plots, Pluto automatically writes or exports the
    image files to the `~/Data-Augmentation-with-Python/pluto_img` directory. Thus,
    Pluto automatically saved the augmented images in [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058)
    and [*Chapter 4*](B17990_04.xhtml#_idTextAnchor082) and the Waveform graph, audio
    Spectrogram, Mel-spectrogram, and Chroma STFT charts in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135)
    and [*Chapter 8*](B17990_08.xhtml#_idTextAnchor167). The helper function name
    is `_drop_image()` with the `pluto[id].jpg` file format, where `id` is an auto-increment
    integer from the `self.fname_id` variable.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed in detail and written Python code for the audio Spectrogram,
    Mel-spectrogram, and Chroma STFT. Next, Pluto will describe how to perform audio
    augmentation with a Spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Spectrogram augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pluto will reuse most of the wrapper functions from [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135).
    You can reread the previous chapter if the following code seems challenging. Pluto
    will shorten his explanation of the wrapper functions because he assumes you are
    an expert at writing audio augmentation wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Audio Spectrogram, Mel-spectrogram, Chroma STFT, and Waveform charts take the
    returned amplitude data and sampling rate from the Librosa `load()` function reading
    an audio file. There is an additional transformation of the amplitude data, but
    they serve the same goal of visualizing the sound wave and frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing many scholarly published papers, Pluto concluded that the audio
    augmentation techniques in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135) apply
    equally well to the audio Spectrogram, Mel-spectrogram, and Chroma STFT. In particular,
    he referred to the scholarly paper, *Audio Augmentation for Speech Recognition*
    by Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, published
    in 2015; *Data augmentation approaches for improving animal audio classification*
    by Loris Nannia, Gianluca Maguoloa, and Michelangelo Paci, published in 2020;
    and *Deep Convolutional Neural Networks and Data Augmentation for Environmental
    Sound Classification* by Justin Salamon and Juan Pablo Bello, published in 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, there shouldn’t be any difference from the technique in [*Chapter
    7*](B17990_07.xhtml#_idTextAnchor135) because the underlying amplitude data and
    sampling rate are the same. In other words, you can use [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135)
    audio augmentation functions for the audio Spectrogram, Mel-spectrogram, and Chroma
    STFT, such as the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Time-stretching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitch-scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noise injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polarity inversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ban-pass filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-shelf filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-shelf filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Band-stop filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peak filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are others, such as `Masking` and `Gaps`. They are available from the
    `audiomentation` library. The **safe** level mentioned in the previous chapter
    applies equally to the audio Spectrogram, Mel-spectrogram, and Chroma STFT.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: You can alter any Python functions by overriding them in the `correct` class.
    Pluto functions belong to the `PacktDataAug` class. Thus, you can hack and override
    any of Pluto’s methods by adding the `@add_method(PacktDataAug)` code line before
    the function definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto needs to hack the `_audio_transform()` helper function and includes the
    new`is_waveform` parameter setting the default to `True` so it will not affect
    methods in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135). The definition of
    the new method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the `is_waveform` parameter is to use the Waveform graphs in [*Chapter
    7*](B17990_07.xhtml#_idTextAnchor135) or the audio Spectrogram, Mel-spectrogram,
    and Chroma STFT charts. That’s it, and this is why we love coding with Pluto.
    He follows the best object-oriented coding practices, and his functions are in
    one class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto adds the new parameter to the `play_aug_time_shift()` wrapper function
    and tests it with the control data. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the audio Spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Spectrogram, time shift, piano scale (control-d-major)](img/B17990_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Spectrogram, time shift, piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the Mel-spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Mel-spectrogram, time shift, piano scale (control-d-major)](img/B17990_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Mel-spectrogram, time shift, piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the Chroma STFT is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Chroma STFT, time shift, piano scale (control-d-major)](img/B17990_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Chroma STFT, time shift, piano scale (control-d-major)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.19*, *Figure 8**.20*, and *Figure 8**.21* play the piano scale
    in D major shift to the left by about 2 seconds. In other words, the audio started
    with the **G note**, looped around, and finished on the **F# note**. Pluto recommends
    listening to the before and after effects of the Python Notebook as the easiest
    method to understand it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for the human speech dataset using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the audio Spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Spectrogram, time shift, human voice (1085_ITS_ANG_XX)](img/B17990_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Spectrogram, time shift, human voice (1085_ITS_ANG_XX)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the Mel-spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Mel-spectrogram, time shift, human voice (1085_ITS_ANG_XX)](img/B17990_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – Mel-spectrogram, time shift, human voice (1085_ITS_ANG_XX)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for Chroma STFT is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Chroma STFT, time shift, human voice (1085_ITS_ANG_XX)](img/B17990_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – Chroma STFT, time shift, human voice (1085_ITS_ANG_XX)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.22*, *Figure 8**.23*, and *Figure 8**.24''s* original audio is
    a man’s voice saying “*We will stop in a couple of minutes.*” The augmented version
    is shifted to “*stop in a couple of minutes [silence] we will*.” Pluto can hear
    the difference in the before-and-after augmentation effect in the Python Notebook.
    The goal of audio augmentation is the same for Spectrogram and Waveform graphs,
    which is to increase the AI accuracy prediction by increasing the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: The results for the music and urban sound dataset are shifted similarly. Pluto
    has the time-shift code in the Python Notebook, where you can run it and see and
    hear the result. Furthermore, Pluto will skip describing the results for other
    audio augmentation functions in this chapter. It is because the results are the
    same as in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135), and the wrapper functions
    code is in the Python Notebook. However, he will explain the `play_aug_noise_injection()`
    function because this function can extend to specific topics discussing how sound
    engineers use Spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: Sound engineers use standard audio Spectrograms and various other Spectrograms
    to spot and remove unwanted noises, such as hums, buzz, hiss, clips, gaps, clicks,
    and pops. Audio augmentation goals are the opposite. We add unwanted noises to
    the recording within a safe range. Thus, we increase the training datasets and
    improve the AI prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto adds white noise to the music dataset using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the audio Spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Spectrogram, noise injection, music (Happy41215)](img/B17990_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – Spectrogram, noise injection, music (Happy41215)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the Mel-spectrogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Mel-spectrogram, noise injection, music (Happy41215)](img/B17990_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – Mel-spectrogram, noise injection, music (Happy41215)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the Chroma STFT is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Chroma STFT, noise injection, music (Happy41215)](img/B17990_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – Chroma STFT, noise injection, music (Happy41215)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.25*, *Figure 8**.26*, and *Figure 8**.27* play heavy drums, light
    electronic bells, and heavy electronic guitars with a medium level of white noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Here is a thought experiment. You are part of a team developing a self-driving
    car system, and your goal is to recognize or identify car honking while driving.
    How would you augment the audio data? A hint is thinking about real-world driving
    conditions with traffic or urban noises.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have hums, buzz, or pops audio files, you can inject them into the recording
    by alternating the `play_aug_noise_injection()` wrapper function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet and complete documentation can be found in the `audiomentations`
    library on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic is a novel idea using a Spectrogram as an image input for deep
    learning image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Spectrogram images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentally, audio data is time-series data. Thus AI uses a time-series algorithm,
    such as the **autoregressive integrated moving average** (**ARIMA**) or **exponential
    smoothing** (**ES**) algorithm for audio classification. However, there is a better
    method. You use the Spectrogram as an image representing the audio sound, not
    the time-series numerical array, for input. Using images as the input data, you
    can leverage the robust neural network algorithm to classify audio more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Strictly speaking, this topic does not directly pertain to new audio augmentation
    techniques. Still, it is an essential topic for data scientists to understand.
    However, Pluto will not write Python code for building a neural network model
    using Spectrograms as input.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning image classification, also known as the machine learning model
    that uses the artificial neural networks algorithm, achieved an unprecedented
    accuracy level that exceeds 98% accuracy recently. Many AI scientists apply deep
    learning techniques to audio datasets, such as *Audio Spectrogram Representations
    for Processing with Convolutional Neural Networks* by Lonce Wyse, published in
    2017, and *Deep Learning Audio Spectrograms Processing to the Early COVID-19 Detection*
    by Ciro Rodriguez, Daniel Angeles, Renzo Chafloque, Freddy Kaseng, and Bishwajeet
    Pandey, published in 2020.
  prefs: []
  type: TYPE_NORMAL
- en: The technique takes an audio Spectrogram as the image input, not the audio amplitude,
    sampling rate, or Mel scale. For example, the music dataset (MEC) goal is to classify
    a piece of music clip as having a `happy` or `sad` mood. Pluto can generate all
    the audio files to audio Spectrograms and save them on the local drive. He will
    use the Fast.ai robust AI framework and libraries to create an image classification
    model. He can achieve 95% accuracy or higher.
  prefs: []
  type: TYPE_NORMAL
- en: The big question is can you use the image augmentation methods discussed in
    [*Chapter 3*](B17990_03.xhtml#_idTextAnchor058) and [*Chapter 4*](B17990_04.xhtml#_idTextAnchor082)
    to apply to Spectrogram?
  prefs: []
  type: TYPE_NORMAL
- en: It depends on the safe level and the objective of the AI model. For example,
    using the image augmentation technique, vertically flipping a spectogram involves
    flipping high to low frequencies and vice versa. Pluto wonders how that would
    affect the music’s mood. It could be an **unsafe** technique. However, image noise
    injection methods with low noise values could be a safe technique with a Spectrogram.
    Pluto thinks it is more suitable to stay with the audio augmentation techniques
    in [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135).
  prefs: []
  type: TYPE_NORMAL
- en: Similar deep learning methods can be applied to the human speech (`CREMA-D`)
    dataset to classify the age, sex, or ethnicity of the speaker.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: This is a thought experiment. Can you use speech-to-text software to convert
    the voice into text and use text augmentation functions in [*Chapter 5*](B17990_05.xhtml#_idTextAnchor101)
    and [*Chapter 6*](B17990_06.xhtml#_idTextAnchor116)? A hint is to think about
    the scope of the project. For example, it could work if the AI aims to infer sentiment
    analysis but not if the goal is to identify male or female voices.
  prefs: []
  type: TYPE_NORMAL
- en: For the urban sound (US8K) dataset, Pluto could use the deep learning multilabel
    classification to identify different types of sound in an urban sound clip, such
    as a jackhammer, wind, kids playing, rain, dogs barking, or gunshots.
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to refactor the `Pluto` class to make it faster and more
    compact. You should also include all the image and text wrapper and helper functions
    from previous chapters. Pluto encourages you to create and upload your library
    to *GitHub and PyPI.org*. Furthermore, you don’t have to name the class `PacktDataAug`,
    but it would give Pluto and his human companion a great big smile if you cited
    or mentioned the book. The code goals were ease of understanding, reusable patterns,
    and teaching you about the Python Notebook. Thus, refactoring the code as a Python
    library would be relatively painless and fun.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered audio Spectrogram, Mel-spectrogram, and Chroma STFT representation
    and augmentation, including the technique of using Spectrograms as image input
    to the deep learning image classification model. It is time for a summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Audio augmentation is challenging to explain in a book format. Still, we gain
    a deeper understanding of audio amplitude, frequency, and sampling rate with additional
    visualization techniques, such as the audio Spectrogram, Mel-spectrogram, and
    Chroma STFT. Furthermore, in the Python Notebook, you can listen to the before-and-after
    effects of the audio augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the previous chapter, Waveform graphs show the amplitude of a signal
    over time, giving an understanding of its shape and structure. Spectrogram graphs
    show a visual representation of the frequencies of a signal over time, providing
    a deeper insight into the harmonic content of the sound.
  prefs: []
  type: TYPE_NORMAL
- en: An Audio Spectrogram comes in many variations, whether `specgram()` function.
    Pluto uses Python code in wrapper functions on a few Spectrogram types. The majority
    of Spectrogram variations are up to you to explore by expanding the `Pluto` object
    with additional wrapper functions. Using Pluto’s object-oriented best practices,
    the function wrapper concept, and the audiomentations library, it is easy to expand
    Pluto with additional wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: For Spectrogram augmentation techniques, they are the same techniques as those
    from [*Chapter 7*](B17990_07.xhtml#_idTextAnchor135), such as time -shifting,
    time-stretching, pitch-scaling, noise injections, bandpass filters, and many others.
    Intuitively, there should be no difference because in the previous chapter, you
    choose to visualize the sound wave in Waveform graphs, and in this chapter, you
    drew them in the audio Spectrogram, Mel-spectrogram, and Chrom STFT plots. Thus,
    the underlying data is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has to only modify the `_audio_transform()` helper method with an additional
    `is_waveform` parameter. The Python code becomes deceptively simple and repetitive
    afterward, but it hides the robust power of the audiomentations library and Pluto
    object-oriented best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, there were **fun facts** and **fun challenges**. Pluto
    hopes you will take the advantages provided and expand the experience beyond the
    scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter moves beyond the typical data types, such as image, text, and
    audio, to tubular data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 5: Tabular Data Augmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part includes the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17990_09.xhtml#_idTextAnchor182), *Tabular Data Augmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
