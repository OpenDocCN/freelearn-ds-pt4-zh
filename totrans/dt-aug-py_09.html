<html><head></head><body>
		<div id="_idContainer277">
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor182"/>9</h1>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor183"/>Tabular Data Augmentation</h1>
			<p>Tabular augmentation supplements tabular data with additional information to make it more useful for predictive analytics. Database, spreadsheet, and table data are examples of tabular data. It involves transforming otherwise insufficient datasets into robust inputs for ML. Tabular augmentation can help turn unstructured data into structured data and can also assist in combining multiple data sources into a single dataset. It is an essential step in data pre-processing for increasing AI <span class="No-Break">predictive accuracy.</span></p>
			<p>The idea of tabular augmentation is to include additional information to a given dataset that can then be used to generate valuable insights. These datasets can come from various sources, such as customer feedback, social media posts, and IoT device logs. Tabular augmentation can add new information columns to the dataset by enriching the existing columns with more informative tags. It increases the completeness of the dataset and provides more <span class="No-Break">accurate insights.</span></p>
			<p>Tabular augmentation is an important method to consider when pre-processing and generating insights from <a id="_idIndexMarker904"/>data. It provides a way to work with incomplete and unstructured datasets by organizing and enriching them for improved accuracy and speed. By implementing tabular augmentation, you can better unlock the value of real-world datasets and make <span class="No-Break">better-informed decisions.</span></p>
			<p>Tabular augmentation is a young field for data scientists. It is contrary to using analytics for reporting, summarizing, or forecasting. In analytics, altering or adding data to skew the results to a preconceived desired outcome is unethical. In data augmentation, the purpose is to derive new data from an existing dataset. The two goals might be incongruent, but they are not. DL is an entirely different technique from traditional analytics. One is based on a neural network algorithm, while the other is based on statistical analysis and <span class="No-Break">data relationships.</span></p>
			<p>The salient point is that even though you might introduce synthetic data into the datasets, it is an acceptable practice. The <em class="italic">Synthesizing Tabular Data using Generative Adversarial Networks</em> paper, by Lei Xu and Kalyan Veeramachaneni, published in the <em class="italic">arXiv Forum</em> in November 2018, supports <span class="No-Break">this proposition.</span></p>
			<p>This chapter focuses on describing concepts. It has a few practical coding examples using the Python Notebook. One main reason for this is that there are only a few tabular augmentation open source libraries available. You will spend most of the coding time plotting various graphs to inspire further insight from <span class="No-Break">the datasets.</span></p>
			<p>Before continuing, let’s take a sneak peek at a real-world tabular dataset. Later, Pluto will explain in detail how to write Python code for <span class="No-Break">the following:</span></p>
			<pre class="source-code">
<strong class="bold"># print out the tabular data</strong>
pluto.df_bank_data[['fraud_bool',
  'proposed_credit_limit',
  'customer_age',
  'payment_type']].sample(5)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="image/B17990_09_01.jpg" alt="Figure 9.1 – Bank Account Fraud Dataset Suite (NeurIPS 2022)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Bank Account Fraud Dataset Suite (NeurIPS 2022)</p>
			<p>One challenge in augmenting tabular data is that no fixed methods work universally, such as flipping images, injecting <a id="_idIndexMarker905"/>misspelled words, or time-stretching audio files. You will learn that the dataset dictates which augmentation techniques are <strong class="bold">safe</strong> or in a <strong class="bold">safe range</strong>. It is essential to thoroughly review the tabular dataset before <span class="No-Break">augmenting it.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout"><strong class="bold">Deep neural networks</strong> (<strong class="bold">DNNs</strong>) excel at predicting future stock values and tabular data, based on the <a id="_idIndexMarker906"/>scholarly paper <em class="italic">Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies</em>, by Eunsuk Chong, Chulwoo Han, and Frank C. Park. It was published by Elsevier, <em class="italic">Expert Systems with Applications</em>, Volume 83, on 15 <span class="No-Break">October 2017.</span></p>
			<p>Tabular augmentation is an approach to augmenting a tabular dataset with synthetic data. It involves adding new columns to a tabular dataset with features from the derived calculation. You will spend the majority of the time in Python code visualizing the real-world tabular dataset with exotics plots. In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Tabular <span class="No-Break">augmentation libraries</span></li>
				<li><span class="No-Break">Augmentation categories</span></li>
				<li>Real-world <span class="No-Break">tabular datasets</span></li>
				<li>Exploring and visualizing <span class="No-Break">tabular data</span></li>
				<li><span class="No-Break">Transformation augmentation</span></li>
				<li><span class="No-Break">Extraction augmentation</span></li>
			</ul>
			<p>Let’s start with <span class="No-Break">augmentation libraries.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor184"/>Tabular augmentation libraries</h1>
			<p>Tabular augmentations are not established as image, text, or audio augmentations. Typically, data scientists develop <a id="_idIndexMarker907"/>tabular augmentation techniques specific to a project. There are a few open source projects on the GitHub website. Still, DL and generative AI will continue to advance in forecasting for time series and tabular data predictions, and so will tabular augmentations. The following open source libraries can be found on the <span class="No-Break">GitHub website:</span></p>
			<ul>
				<li><strong class="bold">DeltaPy</strong> is a tabular augmentation for generating and synthesizing data focusing on financial <a id="_idIndexMarker908"/>applications such as time <a id="_idIndexMarker909"/>series stock forecasting. It fundamentally applies to a broad range of datasets. The GitHub website link is <a href="https://github.com/firmai/deltapy.">https://github.com/firmai/deltapy.</a> The published scholarly <a id="_idIndexMarker910"/>paper is called <em class="italic">DeltaPy: A Framework for Tabular Data Augmentation in Python</em>, by Derek Snow, The Alan Turing Institute, <span class="No-Break">in 2020.</span></li>
				<li>The <strong class="bold">Synthetic Data Vault</strong> (<strong class="bold">SDV</strong>) is for augmenting tabular data by generating synthetic <a id="_idIndexMarker911"/>data from a <a id="_idIndexMarker912"/>single table, multi-table, and time series data. In 2020, Kalyan Veeramachaneni, Neha <a id="_idIndexMarker913"/>Patki, and Saman Amarsinghe developed a commercial version named <em class="italic">Datacebo</em>. The GitHub link <span class="No-Break">is </span><a href="https://github.com/sdv-dev/SDV"><span class="No-Break">https://github.com/sdv-dev/SDV</span></a><span class="No-Break">.</span></li>
				<li>The tabular <strong class="bold">Generative Adversarial Network</strong> (<strong class="bold">GAN</strong>) uses the successfully generating realistic <a id="_idIndexMarker914"/>image algorithm and applies it to tabular augmentation. The scholarly <a id="_idIndexMarker915"/>paper is <em class="italic">Tabular GANs for uneven distribution</em>, by Insaf Ashrapov, published <a id="_idIndexMarker916"/>by <em class="italic">Cornell University</em>, <em class="italic">Arxiv</em>, in 2020. The GitHub website link <span class="No-Break">is </span><a href="https://github.com/Diyago/GAN-for-tabular-data"><span class="No-Break">https://github.com/Diyago/GAN-for-tabular-data</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>Pluto has chosen the <strong class="bold">DeltaPy</strong> library to use as the engine under the hood for his tabular augmenting wrapper functions, but first, let’s look at the <span class="No-Break">augmentation categories.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor185"/>Augmentation categories</h1>
			<p>It is advantageous to group tabular augmentation into categories. The following concepts are new and particular to the DeltaPy library. The augmentation functions are grouped into the <span class="No-Break">following categories:</span></p>
			<ul>
				<li><strong class="bold">Transforming</strong> techniques can be applied for cross-section and time series data. Transforming techniques <a id="_idIndexMarker917"/>in tabular augmentation are used to modify existing rows or columns to create new, synthetic data representative of the original data. These methods can include <span class="No-Break">the following:</span><ul><li><strong class="bold">Scaling</strong>: Increasing or <a id="_idIndexMarker918"/>decreasing a column value to expand the diversity of values in <span class="No-Break">a dataset</span></li><li><strong class="bold">Binning</strong>: Combining two or more <a id="_idIndexMarker919"/>columns into a single bucket to create <span class="No-Break">new features</span></li><li><strong class="bold">Categorical encoding</strong>: Using a numerical <a id="_idIndexMarker920"/>representation of <span class="No-Break">categorical data</span></li><li><strong class="bold">Smoothing</strong>: Compensating for <a id="_idIndexMarker921"/>unusually high or low values in <span class="No-Break">a dataset</span></li><li><strong class="bold">Outlier detection and removal</strong>: Detecting <a id="_idIndexMarker922"/>and removing points farther from <span class="No-Break">the norm</span></li><li><strong class="bold">Correlation-based augmentation</strong>: Adding new features based on correlations <a id="_idIndexMarker923"/>between <span class="No-Break">existing features</span></li></ul></li>
				<li>The <strong class="bold">interacting</strong> function is a cross-sectional or time series tabular augmentation that <a id="_idIndexMarker924"/>includes normalizing, discretizing, and autoregression models. In tabular augmentation, these <a id="_idIndexMarker925"/>functions are used to specify interactions between two or more variables and help generate new features that represent combinations of the original variables. This type of augmentation is beneficial when modeling the relationships between multiple input features and the target variable, as it allows the model to consider interactions between the <span class="No-Break">different components.</span></li>
				<li>The <strong class="bold">mapping</strong> method, which <a id="_idIndexMarker926"/>uses <strong class="bold">eigendecomposition</strong> in tabular <a id="_idIndexMarker927"/>augmentation, is a method of unsupervised learning that uses data <a id="_idIndexMarker928"/>decomposition to transform data into lower-dimensional space using eigenvectors and eigenvalues. This type of feature transformation is useful for clustering, outlier detection, and dimensionality reduction. By projecting the data onto the eigenvectors, the data can be represented in a reduced space while still preserving the structure of <span class="No-Break">the data.</span></li>
				<li>The <strong class="bold">extraction</strong> method is a tabular augmentation technique that utilizes <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) to generate additional information from textual <a id="_idIndexMarker929"/>references in tabular <a id="_idIndexMarker930"/>datasets. It uses the <strong class="bold">TSflesh</strong> library, a collection <a id="_idIndexMarker931"/>of rules and heuristics, to extract additional data from text, such as names, dates, and locations. This approach is beneficial in <a id="_idIndexMarker932"/>augmenting structured <a id="_idIndexMarker933"/>datasets, where the output of <strong class="bold">sentence split</strong>, <strong class="bold">tokenization</strong>, and <strong class="bold">part-of-speech tagging</strong> is used to <a id="_idIndexMarker934"/>create features that can be used for <span class="No-Break">further processing.</span></li>
				<li><strong class="bold">Time series synthesis</strong> (<strong class="bold">TSS</strong>) is a method for tabular data augmentation where rows <a id="_idIndexMarker935"/>of data across multiple sources or temporal <a id="_idIndexMarker936"/>points in time are synthesized together. You can use it to increase a dataset’s size and create a more consistent set <span class="No-Break">of features.</span></li>
				<li><strong class="bold">Cross-sectional synthesis</strong> (<strong class="bold">CSS</strong>) is a method for tabular data augmentation <a id="_idIndexMarker937"/>where columns of data from multiple <a id="_idIndexMarker938"/>sources are combined. You can use it to increase a dataset’s features and create a more complete and holistic <span class="No-Break">data view.</span></li>
				<li>The <strong class="bold">combining</strong> technique uses <a id="_idIndexMarker939"/>the mix-and-match process from the <span class="No-Break">existing methods.</span></li>
			</ul>
			<p>There are functions associated with each category in the DeltaPy library. However, Pluto has to construct <a id="_idIndexMarker940"/>a neural network model, such as a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) or <strong class="bold">reoccurring neural network</strong> (<strong class="bold">RNN</strong>), to gauge the <a id="_idIndexMarker941"/>effectiveness of these methods. It is a complex process, and Pluto will not implement a CNN in this chapter. Nevertheless, Pluto will demonstrate the mechanics of using the DeltaPy library on the Python Notebook. He will not explain how <span class="No-Break">they work.</span></p>
			<p>Now, it is time to download the real-world datasets from the <span class="No-Break"><em class="italic">Kaggle</em></span><span class="No-Break"> website.</span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor186"/>Real-world tabular datasets</h1>
			<p>There are thousands <a id="_idIndexMarker942"/>of real-world tabular datasets on the <em class="italic">Kaggle</em> website. Pluto has chosen two tabular datasets for <span class="No-Break">this process.</span></p>
			<p>The <em class="italic">Bank Account Fraud Dataset Suite (NeurIPS 2022)</em> contains six synthetic bank account fraud tabular datasets. Each dataset contains 1 million records. They are based on real-world data for fraud detection. Each dataset focuses on a different type of bias. Sergio Jesus, Jose Pombal, and Pedro Saleiro published the dataset in 2022 under the <strong class="bold">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)</strong> license. The <em class="italic">Kaggle</em> link <span class="No-Break">is </span><a href="https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022"><span class="No-Break">https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022</span></a><span class="No-Break">.</span></p>
			<p>The <em class="italic">World Series Baseball Television Ratings</em> is a dataset for audiences watching the baseball World Series on television from 1969 to 2022. Matt OP published the dataset in 2022 under the <strong class="bold">CC0 1.0 Universal (CC0 1.0) Public Domain Dedication</strong> license. The <em class="italic">Kaggle</em> link <span class="No-Break">is </span><a href="https://www.kaggle.com/datasets/mattop/world-series-baseball-television-ratings"><span class="No-Break">https://www.kaggle.com/datasets/mattop/world-series-baseball-television-ratings</span></a><span class="No-Break">.</span></p>
			<p>The steps for instantiating Pluto and downloading real-world datasets from the <em class="italic">Kaggle</em> website are the same. It starts with loading the <strong class="source-inline">data_augmentation_with_python_chapter_9.ipynb</strong> file into Google Colab or your chosen Jupyter Notebook or JupyterLab environment. From this point onward, the code snippets are from the Python Notebook, which contains the <span class="No-Break">complete functions.</span></p>
			<p>You will be using the code from <a href="B17990_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> because you will need the wrapper functions for downloading the <em class="italic">Kaggle</em> dataset, not the wrapper functions for image, text, and audio augmentations. You should review <em class="italic">Chapters 2</em> and <em class="italic">3</em> if the steps <span class="No-Break">are unfamiliar:</span></p>
			<pre class="source-code">
<strong class="bold"># Clone GitHub repo.</strong>
url = 'https://github.com/PacktPublishing/Data-Augmentation-with-Python'
!git clone {url}
<strong class="bold"># Initialize Pluto from Chapter 2</strong>
pluto_file = 'Data-Augmentation-with-Python/pluto/pluto_chapter_2.py'
%run {pluto_file}
<strong class="bold"># Verify Pluto</strong>
pluto.say_sys_info()
<strong class="bold"># Fetch Bank Fraud dataset</strong>
url = 'https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022'
pluto.fetch_kaggle_dataset(url)
<strong class="bold"># Import to Pandas</strong>
f = 'kaggle/bank-account-fraud-dataset-neurips-2022/Base.csv'
pluto.df_bank_data = pluto.fetch_df(f)
<strong class="bold"># Fetch World Series Baseball dataset</strong>
url = 'https://www.kaggle.com/datasets/mattop/world-series-baseball-television-ratings'
pluto.fetch_kaggle_dataset(url)
<strong class="bold"># Import to Pandas</strong>
f = 'kaggle/world-series-baseball-television-ratings/world-series-ratings.csv'
pluto.df_world_data = pluto.make_dir_dataframe(f)</pre>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">At the end of <a href="B17990_08.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, Pluto challenged you to refactor the Pluto code for speed and compactness. The goal is to upload Pluto to <a href="https://Pypi.org">Pypi.org</a>. This challenge extends that concept and asks you to combine the setup code into one uber wrapper function, such as <strong class="source-inline">pluto.just_do_it()</strong>. Pluto does not use uber methods because this book aims to make the concepts and functions easier to learn and demystify <span class="No-Break">the process.</span></p>
			<p>The output for <a id="_idIndexMarker943"/>gathering Pluto’s system information is <span class="No-Break">as follows:</span></p>
			<pre class="console">
---------------------------- : ----------------------------
                                 System time : 2023/01/31 07:03
                                        Platform : linux
         Pluto Version (Chapter) : 2.0
                         Python (3.7.10) : actual: 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0]
                        PyTorch (1.11.0) : actual: 1.13.1+cu116
                            Pandas (1.3.5) : actual: 1.3.5
                                 PIL (9.0.0) : actual: 7.1.2
                    Matplotlib (3.2.2) : actual: 3.2.2
                                     CPU count : 2
                                    CPU speed : NOT available
---------------------------- : ----------------------------</pre>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to search for, download, and import two additional tabular datasets from the <em class="italic">Kaggle</em> website or your project <span class="No-Break">into pandas.</span></p>
			<p>With that, you have selected a tabular augmentation library, cloned the GitHub repository, instantiated Pluto, and downloaded the two real-world tabular datasets from the <em class="italic">Kaggle</em> website. Now, it is time for Pluto to explore and visualize <span class="No-Break">the data.</span></p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor187"/>Exploring and visualizing tabular data</h1>
			<p>Tabular augmentation is more <a id="_idIndexMarker944"/>challenging than image, text, and audio <a id="_idIndexMarker945"/>augmentation. The primary reason is that you need to build a CNN or RNN model to see the effect of the <span class="No-Break">synthetic data.</span></p>
			<p>Pluto will spend more time explaining his journey to investigate the real-world Bank Fraud and World Series datasets than implementing the tabular augmentation functions using the DeltaPy library. Once you understand the data visualization process, you can apply it to other <span class="No-Break">tabular datasets.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Typically, Pluto starts a chapter by writing code in the Python Notebook for that chapter. It consists of around 150 to 250 combined code and text cells. They are unorganized collections of research notes and try-and-error Python code cells. Once Pluto proves that the concepts and techniques are working correctly through coding, he starts writing the chapter. As part of the writing progress, he cleans and refactors the Python Notebook with wrapper functions and deletes the dead-end code. The clean version of the Python Notebook contains 20% to 30% of the original code and <span class="No-Break">text cells.</span></p>
			<p>In particular, while exploring tabular data, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Data structure</span></li>
				<li>First <span class="No-Break">graph view</span></li>
				<li><span class="No-Break">Checksum</span></li>
				<li><span class="No-Break">Specialized plots</span></li>
				<li>Exploring the World Series <span class="No-Break">baseball dataset</span></li>
			</ul>
			<p>Let’s start with <span class="No-Break">data structures.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor188"/>Data structure</h2>
			<p>Pluto starts by inspecting <a id="_idIndexMarker946"/>the data structure using pandas’ built-in function. He uses the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># display tabular data in Pandas</strong>
pluto.df_bank_data.info()</pre>
			<p>The result <a id="_idIndexMarker947"/>is <span class="No-Break">as follows:</span></p>
			<pre class="console">
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 32 columns):
#     Column                                                    Non-Null Count        Dtype
---    ------                                                 --------------        ----
0     fraud_bool                                            1000000 non-null    int64
1     income                                                    1000000 non-null    float64
2     name_email_similarity                     1000000 non-null    float64
3     prev_address_months_count             1000000 non-null    int64
4     current_address_months_count        1000000 non-null    int64
5     customer_age                                        1000000 non-null    int64
6     days_since_request                            1000000 non-null    float64
7     intended_balcon_amount                    1000000 non-null    float64
8     payment_type                                        1000000 non-null    object
9     zip_count_4w                                        1000000 non-null    int64
10    velocity_6h                                         1000000 non-null    float64
11    velocity_24h                                        1000000 non-null    float64
12    velocity_4w                                         1000000 non-null    float64
13    bank_branch_count_8w                        1000000 non-null    int64
14    date_of_birth_distinct_emails_4 1000000 non-null    int64
15    employment_status                             1000000 non-null    object
16    credit_risk_score                             1000000 non-null    int64
17    email_is_free                                     1000000 non-null    int64
18    housing_status                                    1000000 non-null    object
19    phone_home_valid                                1000000 non-null     int64
20    phone_mobile_valid                            1000000 non-null    int64
21    bank_months_count                             1000000 non-null    int64
22    has_other_cards                                 1000000 non-null     int64
23    proposed_credit_limit                     1000000 non-null    float64
24    foreign_request                                 1000000 non-null     int64
25    source                                                    1000000 non-null    object
26    session_length_in_minutes             1000000 non-null    float64
27    device_os                                             1000000 non-null    object
28    keep_alive_session                            1000000 non-null    int64
29    device_distinct_emails_8w             1000000 non-null    int64
30    device_fraud_count                            1000000 non-null    int64
31    month                                                     1000000 non-null    int64
dtypes: float64(9), int64(18), object(5)
memory usage: 244.1+ MB</pre>
			<p>The Bank Fraud dataset consists of 32 columns, 1 million records or rows, no null values, and five columns that are <a id="_idIndexMarker948"/>not numeric. Pluto wants to find out which columns are <strong class="bold">continuous</strong> or <strong class="bold">categorical</strong>. He does this by calculating the unique value in each column. He uses the following <span class="No-Break">pandas function:</span></p>
			<pre class="source-code">
<strong class="bold"># count uniqueness</strong>
pluto.df_bank_data.nunique()</pre>
			<p>The partial output is <span class="No-Break">as follows:</span></p>
			<pre class="console">
fraud_bool                                                             2
income                                                                     9
name_email_similarity                             998861
prev_address_months_count                            374
current_address_months_count                     423
customer_age                                                         9</pre>
			<p>The Python Notebook contains the complete result. There are 7 continuous columns and 25 categorical columns. Generally, continuous columns have many unique values, as in total records, while categorical columns have unique values between two and a <span class="No-Break">few hundred.</span></p>
			<p>Before using plots to <a id="_idIndexMarker949"/>display the data, Pluto will view sample data from the Bank Fraud dataset with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># display the tabular data using Pandas</strong>
pluto.df_bank_data[['fraud_bool',
    'proposed_credit_limit',
    'customer_age',
    'payment_type']].sample(5)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="image/B17990_09_02.jpg" alt="Figure 9.2 – Sample Bank Fraud data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Sample Bank Fraud data</p>
			<p>After repeatedly running the command and variation, Pluto finds no surprises in the data. It is clean. The Python Notebook contains additional inspecting functions, such as the pandas <span class="No-Break"><strong class="source-inline">describe()</strong></span><span class="No-Break"> method.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">For a tabular dataset, you will write custom code for inspecting, visualizing, and augmenting the data. In other words, there will be more reusable concepts and processes than reusable code being carried over to the <span class="No-Break">next project.</span></p>
			<p>The Bank Fraud dataset <a id="_idIndexMarker950"/>has 32 million elements, which is the typical size of data that data scientists work with. However, your Python Notebook would crash if you tried to plot 32 million points using pandas and Matplotlib with the default settings. Pluto created a simple graph, <strong class="source-inline">pluto.df_bank_data.plot()</strong>, and his Google Colab Pro-version Python Notebook crashed. It required <span class="No-Break">additional RAM.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor189"/>First graph view</h2>
			<p>The various plots are not directly aiding in the tabular augmentation process. The primary goal is for you <a id="_idIndexMarker951"/>to envision a sizeable tabular dataset. Reading millions of data points is less effective than seeing them plotted on a graph. You may skip the sections about plotting and go directly to the tabular augmentation techniques using the <span class="No-Break">DeltaPy library.</span></p>
			<p>For a large dataset, the solution is to select graphs with calculated or summarizing values. Hence, there will be fewer points to plot. For example, the <strong class="bold">histogram</strong> graph is a viable choice <a id="_idIndexMarker952"/>because it groups the frequency of ranges. Pluto uses a wrapper function to draw the <span class="No-Break">histogram plot:</span></p>
			<pre class="source-code">
<strong class="bold"># display histogram plot</strong>
pluto.draw_tabular_histogram(pluto.df_bank_data,
    title='Bank Fraud data with 32 million points')</pre>
			<p>The key code line for the wrapper function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet, use Pandas histogram function</strong>
df.plot.hist()</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/B17990_09_03.jpg" alt="Figure 9.3 – Bank Fraud histogram plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Bank Fraud histogram plot</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em> does not yield any <a id="_idIndexMarker953"/>beneficial insights. Thus, Pluto proceeds to summarize the data with a <span class="No-Break"><strong class="bold">checksum</strong></span><span class="No-Break"> concept.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor190"/>Checksum</h2>
			<p>Pluto spends weeks playing with <a id="_idIndexMarker954"/>different types of graphs and graphing packages such as <strong class="bold">Matplotlib</strong>, <strong class="bold">Seaborn</strong>, <strong class="bold">Joypi</strong>, and <strong class="bold">PyWaffle</strong>. He has fun, but most do not enhance <a id="_idIndexMarker955"/>the visualization <a id="_idIndexMarker956"/>of the Bank Fraud <a id="_idIndexMarker957"/>and World <a id="_idIndexMarker958"/><span class="No-Break">Series datasets.</span></p>
			<p>At this point, Pluto will get back to more plotting. In tabular data, displaying the string, non-numeric data is challenging. A clean solution is transforming the categorical string data into an integer token index. Pluto writes the <strong class="source-inline">_fetch_token_index()</strong> helper function to index value from a list. The key code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for token index</strong>
for i, x in enumerate(xarr):
  if (val == x):
    return i</pre>
			<p>The <strong class="source-inline">add_token_index()</strong> wrapped function uses the helper function and the pandas <strong class="source-inline">apply()</strong> function. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for tokenize</strong>
arrname = numpy.array(df[cname].unique())
df[tname] = df[cname].apply(
  self._fetch_token_index,
  args=(arrname,))</pre>
			<p>To put it all together, Pluto uses the following command to copy and create the tokenized columns <a id="_idIndexMarker959"/>for the Data <span class="No-Break">Fraud dataset:</span></p>
			<pre class="source-code">
<strong class="bold"># tokenize the data</strong>
pluto.df_bank_tokenize_data = pluto.df_bank_data.copy()
pluto.add_token_index(
  pluto.df_bank_tokenize_data,
  ['payment_type', 'employment_status',
  'housing_status', 'source', 'device_os'])</pre>
			<p>Pluto double-checked the tokenization by viewing sample values using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># print out first 6 row of the tabular data</strong>
pluto.df_bank_tokenize_data[['payment_type',
  'payment_type_tokenize']].head(6)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/B17990_09_04.jpg" alt="Figure 9.4 – Bank Fraud sample tokenized data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Bank Fraud sample tokenized data</p>
			<p>Pluto double-checked the other columns, and they are correct. You can view the code and the results by reading the <span class="No-Break">Python Notebook.</span></p>
			<p>For data analysis, it is practical to have a <strong class="bold">checksum</strong> column where a number represents each row. It <a id="_idIndexMarker960"/>could be a summation, average, or a complex formula of the elements’ relationship with weighted value. Pluto’s <strong class="source-inline">_fetch_checksum()</strong> helper function uses the pandas <strong class="source-inline">apply()</strong> method with <strong class="source-inline">lambda</strong>. The code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># code snippet for calculate the checksum</strong>
df['checksum'] = df.apply(
  lambda x: numpy.mean(tuple(x)), axis=1)</pre>
			<p>Pluto calculates the checksum for the Bank Fraud dataset using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># compute the checksum</strong>
pluto._fetch_checksum(pluto.df_bank_tokenize_data)</pre>
			<p>It took 27 seconds to <a id="_idIndexMarker961"/>compute the checksum for 32 million data points. Now, let’s explore a few specialized plots with the <span class="No-Break"><strong class="bold">checksum</strong></span><span class="No-Break"> concept.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor191"/>Specialized plots</h2>
			<p>Pluto wants to remind you that the following graphs and exercises do not directly pertain to tabular <a id="_idIndexMarker962"/>augmentation. The goal is to sharpen your skills in understanding and visualizing sizeable real-world datasets – for example, the Bank Fraud dataset consists of 1 million records in preparation for data augmentation. You can skip the plotting exercises and jump directly to the tabular augmentation lessons if <span class="No-Break">you wish.</span></p>
			<p>Pluto creates <strong class="source-inline">self.df_bank_half_data</strong> with a limited number of columns for ease of display. He uses <strong class="bold">Seaborn’s</strong> <strong class="source-inline">heatmap()</strong> function to draw the <strong class="bold">correlogram</strong> plot. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot correlogram</strong>
pluto.draw_tabular_correlogram(pluto.df_bank_half_data,
  title='Bank Fraud half Correlogram')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/B17990_09_05.jpg" alt="Figure 9.5 – Bank Fraud half correlogram"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Bank Fraud half correlogram</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em> shows a high relationship between <strong class="source-inline">credit_risk_score</strong> and <strong class="source-inline">proposed_credit_limit</strong> with 61%. <strong class="source-inline">fraud_bool</strong> has a low correlation with all <span class="No-Break">other parameters.</span></p>
			<p>When Pluto draws <a id="_idIndexMarker963"/>the correlogram plot with the entire dataset, it exposes a high correlation between the <strong class="bold">checksum</strong> and <strong class="bold">velocity_6h</strong>, <strong class="bold">velocity_24h</strong>, and <strong class="bold">velocity_4w</strong>. The code and the output can be found in the <span class="No-Break">Python Notebook.</span></p>
			<p>The <strong class="source-inline">draw_tabular_heatmap()</strong> wrapper function looks like a heatmap. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plotting heatmap</strong>
pluto.draw_tabular_heatmap(
  pluto.df_bank_tokenize_data,
  x='checksum',
  y='month')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/B17990_09_06.jpg" alt="Figure 9.6 – Bank Fraud checksum and month heatmap"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Bank Fraud checksum and month heatmap</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em> shows a <a id="_idIndexMarker964"/>pattern, but the relationship between the <strong class="bold">checksum</strong> and <strong class="bold">month</strong> <span class="No-Break">is unclear.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Pluto is not an expert in reading Bank Fraud data, and it is natural for you not to be an expert in every domain. Pluto consults friends in banking and consumer protection agencies for background research. Here are a few charts that he uses in <span class="No-Break">his work.</span></p>
			<p>The fraud data, <strong class="source-inline">fraud_bool == 1</strong>, is 1% of the total. Thus, Pluto might want to augment more fraud data. He creates a pandas DataFrame using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
<strong class="bold"># tokenize the text or categorical columns</strong>
pluto.df_bank_fraud_data = pluto.df_bank_tokenize_data[
  pluto.df_bank_tokenize_data.fraud_bool == 1]
pluto.df_bank_fraud_data.reset_index(
  drop=True,
  inplace=True)</pre>
			<p>The following two graphs suggested by Pluto’s banking expert friends are fun to create but may not benefit the <a id="_idIndexMarker965"/>Bank Fraud augmentation. The complete code is in the Python Notebook. Nevertheless, they are thought-provoking concepts over the standard line or <span class="No-Break">bar charts:</span></p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/B17990_09_07.jpg" alt="Figure 9.7 – Bank Fraud Seaborn heatmap with mask"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Bank Fraud Seaborn heatmap with mask</p>
			<p>The next graph is <span class="No-Break">the Swarmplot.</span></p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/B17990_09_08.jpg" alt="Figure 9.8 – Bank Fraud Seaborn swarm plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Bank Fraud Seaborn swarm plot</p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Can you make use of the <strong class="source-inline">tripcolor()</strong> 3D graph, shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.9</em>, using the Bank Fraud dataset? The <strong class="source-inline">tripcolor()</strong> code is in the <span class="No-Break">Python Notebook:</span></p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/B17990_09_09.jpg" alt="Figure 9.9 – Fun challenge – a tripcolor plot of random values"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Fun challenge – a tripcolor plot of random values</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor192"/>Exploring the World Series data</h2>
			<p>In this section, Pluto will <a id="_idIndexMarker966"/>spend much time plotting various graphs to understand and visualize the World Series data. He is not performing tabular augmentation. Even though comprehending the data is essential before deciding which augmentation functions are applicable, you can skip this exercise and directly go to the tabular augmentation <span class="No-Break">wrapper functions.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Anecdotally, Pluto, an imaginary Sybirian Huskey, loves to rush ahead and start writing augmenting code without taking the time to sniff out the content of the datasets. Consequently, his AI model diverged 9 out of 10 times, resulting in high levels of false negatives and false positives. Thus, spending 40% to 70% of the time studying the datasets seems non-productive, but it is not. It is an acceptable fact when working with <span class="No-Break">real-world datasets.</span></p>
			<p>Pluto follows a similar process for the World Series dataset. He runs the first <strong class="source-inline">info()</strong> method, followed by <strong class="source-inline">nunique(), describe()</strong>, and then <strong class="source-inline">sample()</strong>. The World Series dataset consists of 14 columns and 54 rows, representing 756 data points. There are 11 numeric columns and three label categories. Other factors are eight <strong class="bold">continuous</strong> and six <strong class="bold">categorical</strong> columns. The output of the <strong class="source-inline">pluto.df_world_data.info()</strong> command is <span class="No-Break">as follows:</span></p>
			<pre class="console">
<strong class="bold"># describe the tabular dataset</strong>
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 54 entries, 0 to 53
Data columns (total 14 columns):
#   Column              Non-Null Count  Dtype
---  ------              --------------  -----
0   year                54 non-null     int64
1   network             54 non-null     object
2   average_audience    54 non-null     int64
3   game_1_audience     53 non-null     float64
4   game_2_audience     52 non-null     float64
5   game_3_audience     53 non-null     float64
6   game_4_audience     53 non-null     float64
7   game_5_audience     44 non-null     float64
8   game_6_audience     31 non-null     float64
9   game_7_audience     18 non-null     float64
10  total_games_played  54 non-null     int64
11  winning_team        54 non-null     object
12  losing_team         54 non-null     object
13  losing_team_wins    54 non-null     int64
dtypes: float64(7), int64(4), object(3)
memory usage: 6.0+ KB</pre>
			<p>Other results can <a id="_idIndexMarker967"/>be found in the Python Notebook. The histogram plot is the practical first data visualization technique for the World Series dataset. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot histogram graph</strong>
pluto.draw_tabular_histogram(pluto.df_world_data,
  title='World Series Baseball',
  maxcolors=14)</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/B17990_09_10.jpg" alt="Figure 9.10 – World Series histogram"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – World Series histogram</p>
			<p>The histogram plot shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.10</em> does not highlight the comparison between the audience in the <a id="_idIndexMarker968"/>seven games. Pluto uses the <strong class="source-inline">joyplot()</strong> method from the <strong class="bold">joypy</strong> library to display the relationship between the viewing audience and the TV networks. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot joyplot graph</strong>
pluto.draw_tabular_joyplot(pluto.df_world_data,
  x=['game_1_audience', 'game_2_audience', 'game_3_audience',
     'game_4_audience', 'game_5_audience', 'game_6_audience',
     'game_7_audience'],
  y='network',
  t='World series baseball audience')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="image/B17990_09_11.jpg" alt="Figure 9.11 – World Series audience and TV networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – World Series audience and TV networks</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.11</em> is a beautiful and insightful visualization plot. NBC television network has the highest number of game viewers for game #7 but also has the lowest number for game #5. Fox TV has the least number of viewers, and ABC TV has the highest total viewers but only a little more than NBC TV. Game #3 has the lowest number of viewers, while game #7 has <span class="No-Break">the highest.</span></p>
			<p>Pluto prepares the World Series dataset for augmenting by converting the label categories into integer <a id="_idIndexMarker969"/>token indexes and calculates the checksum. The commands are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># copy tokenize data</strong>
pluto.df_world_tokenize_data = pluto.df_world_data.copy()
<strong class="bold"># eliminate the null value</strong>
pluto.df_world_tokenize_data=pluto.df_world_tokenize_data.fillna(0)
<strong class="bold"># tokenize the data</strong>
pluto.add_token_index(pluto.df_world_tokenize_data,
  ['network', 'winning_team', 'losing_team'])
pluto.df_world_tokenize_data =
  pluto.df_world_tokenize_data.drop(
  ['network', 'winning_team', 'losing_team'],
  axis=1)
<strong class="bold"># calculate the checksum</strong>
pluto._fetch_checksum(pluto.df_world_tokenize_data)</pre>
			<p>The code for double-checking and printing the results for the tokenization and checksum can be <a id="_idIndexMarker970"/>found in the Python Notebook. Pluto made a quick correlogram plot with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
<strong class="bold"># draw the correlogram graph</strong>
pluto.draw_tabular_correlogram(pluto.df_world_tokenize_data,
  title='World Series Baseball Correlogram')</pre>
			<p>The result is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/B17990_09_12.jpg" alt="Figure 9.12 – World Series correlogram plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – World Series correlogram plot</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.12</em> exposes many intriguing relationships between the data. For example, there is a 100% correlation between <strong class="bold">losing_team_wins</strong> and <strong class="bold">total_game_played</strong>, and strong relationships between <strong class="bold">average_audience, game_1_audience</strong>, <strong class="bold">game_2_audience</strong>, <strong class="bold">game_3_audience</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">game_4_audience</strong></span><span class="No-Break">.</span></p>
			<p>Pluto uses the <strong class="source-inline">joyplot()</strong> method to compare the checksum with the average viewers grouped by <a id="_idIndexMarker971"/>the TV networks. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># draw the joyplot graph</strong>
pluto.draw_tabular_joyplot(pluto.df_world_tokenize_data,
  x=['checksum', 'average_audience'],
  y='network_tokenize',
  t='World series baseball, checksum and average auidence',
  legloc='upper right')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B17990_09_13.jpg" alt="Figure 9.13 – World Series checksum, average audience grouped by TV networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – World Series checksum, average audience grouped by TV networks</p>
			<p>In <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.13</em>, Pluto uses the <strong class="source-inline">mean()</strong> function to calculate the checksum values. Thus, the comparison to the average viewers yields a similar shape. Compared to <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.11</em>, the relationship between average audience size and each game’s total is not immediately apparent because CBS TV has the highest average but seems to have lower <span class="No-Break">per-game viewers.</span></p>
			<p>At this stage, Pluto <a id="_idIndexMarker972"/>wonders if plotting more graphs would help him understand the dataset better. There is a good chance that you are thinking the <span class="No-Break">same thoughts.</span></p>
			<p>The justification for exploring additional charts is twofold. The real-world tabular data is diverse. Thus, knowing various graphs makes you better prepared to tackle your next project. Second, no criteria or algorithm lets you know you have learned about the datasets sufficiently. Therefore, if you know the data, skip to the tabular augmentation functions section or follow along with Pluto as he learns <span class="No-Break">new graphs.</span></p>
			<p>Pluto uses the <strong class="bold">waffle</strong> graph to visualize the winning and losing team count. The wrapper function <strong class="source-inline">draw_tabular_waffle()</strong> uses the <strong class="source-inline">Waffle</strong> class from the <strong class="bold">pywaffle</strong> library. The command for displaying the World Series winning teams is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># plot the waffle graph</strong>
pluto.draw_tabular_waffle(pluto.df_world_data,
  col='winning_team',
  title='World Series Baseball Winning Team')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="image/B17990_09_14.jpg" alt="Figure 9.14 – World Series winning team"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – World Series winning team</p>
			<p>Pluto does the <a id="_idIndexMarker973"/>same for displaying the losing teams. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># draw the waffle graph</strong>
pluto.draw_tabular_waffle(pluto.df_world_data,
  col='losing_team',
  title='World Series Baseball Losing Team')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B17990_09_15.jpg" alt="Figure 9.15 – World Series losing team"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – World Series losing team</p>
			<p><em class="italic">Figures 9.14</em> and <em class="italic">9.15</em> are beautifully colored waffle graphs. There is no dominant or underdog team. Pluto does <a id="_idIndexMarker974"/>the same for TV networks. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># draw the waffle graph</strong>
pluto.draw_tabular_waffle(pluto.df_world_data,
  col='network',
  title='World Series Baseball Network',
  anchor=(0.5, -0.2))</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/B17990_09_16.jpg" alt="Figure 9.16 – World Series TV networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.16 – World Series TV networks</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.16</em> yields a surprising hidden fact in the data: Fox TV aired the most games, but from <em class="italic">Figures 9.11</em> and <em class="italic">9.12</em>, it does not seem like the network with the <span class="No-Break">most viewers.</span></p>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Here is a thought experiment: can you visualize a <strong class="bold">four-dimensional</strong> (<strong class="bold">4D</strong>) graph? Hint: a 2D chart displays two measurements, such as the number of TV audiences per game, or one vector with an implied time series as the X-axis, such as the bank member’s income with the X-axis indicated as day or month. A 3D graph typically reveals the snow depth level on a mountain. Time could be the <span class="No-Break">fourth dimension.</span></p>
			<p>Pluto has explored <a id="_idIndexMarker975"/>and explained the real-world Bank Fraud and World Series datasets. He uses pandas functions to display statistical information and provides numerous graphs to visualize them. Understanding and visualizing the data is the first and most essential step before augmenting <span class="No-Break">tabular data.</span></p>
			<p class="callout-heading">Fun fact</p>
			<p class="callout">Data augmentation is a secret for DL and generative AI to achieve unprecedented accuracy and success. Many scholarly papers reinforced data augmentation’s significance, such as <em class="italic">Enhancing Performance of Deep Learning Models with Different Data Augmentation Techniques: A Survey</em>, by Cherry Khosla and Baljit Singh Saini, published by <em class="italic">IEEE 2020 Intelligent Engineering and Management (ICIEM), </em><span class="No-Break"><em class="italic">International Conference</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor193"/>Transforming augmentation</h1>
			<p>Before digging into the tabular augmentation methods, Pluto will reiterate that he will not build a neural network model to test if the augmentation benefits the particular dataset. In addition, the pattern for writing the wrapper functions follows the previous practice: using the chosen library to do the critical <span class="No-Break">augmentation step.</span></p>
			<p>As the Python Notebook notes, the DeltaPy library’s dependency is the <strong class="bold">fbprofet</strong> and <strong class="bold">pystan</strong> libraries. The three libraries are <a id="_idIndexMarker976"/>in beta and may be unstable. Pluto has repeatedly <a id="_idIndexMarker977"/>tested the Python code. Once the libraries have been loaded, the code <span class="No-Break">works flawlessly.</span></p>
			<p>Tabular transformation is a collection of techniques that take one variable and generate a new dataset based <a id="_idIndexMarker978"/>on the transformation method. It applies to both cross-section and time series data. The DeltaPy library defines 14 functions <span class="No-Break">for transformation.</span></p>
			<p>These transformation techniques include the <strong class="bold">operations</strong> functions used in present information, the <strong class="bold">smoothing</strong> method used with past data, and the <strong class="bold">select filters</strong> procedure used with lagging and <span class="No-Break">leading values.</span></p>
			<p>In image augmentation, Pluto can run the functions and see what changes in the photo. Here, the effects are apparent, such as <strong class="bold">cropped</strong>, <strong class="bold">enlarged</strong>, or <strong class="bold">altered</strong> hue values. Tabular augmentation <a id="_idIndexMarker979"/>requires knowledge of DL and time series data. In other words, the output effects are not obvious; therefore, selecting augmentation functions for a particular dataset can be intimidating. Pluto will demonstrate how to write Python code for tabular augmentation, but he will not thoroughly explain when to <span class="No-Break">use them.</span></p>
			<p>Time series forecasting is a mature and highly researched branch of AI. It could take several college courses to understand a time series and how to forecast or predict future outcomes. A compact definition of a time series is a data sequence that depends on time. Typical time series data is the <a id="_idIndexMarker980"/>market stock value. For example, Pluto uses the Microsoft stock data for the previous 10 years to predict the closing price of tomorrow, next week, or next month. Weather forecasting is another widespread use of time <span class="No-Break">series algorithms.</span></p>
			<p>The two key concepts in time series data are <strong class="bold">lag time</strong> and <strong class="bold">windows</strong>. The lag time is from the observer to a <a id="_idIndexMarker981"/>set point, while the window is the range of elements <a id="_idIndexMarker982"/>segmented. There are <a id="_idIndexMarker983"/>dozens of <a id="_idIndexMarker984"/>other key concepts in time series <a id="_idIndexMarker985"/>algorithms, from the <a id="_idIndexMarker986"/>earliest <strong class="bold">long short-term memory </strong>(<strong class="bold">LSTM</strong>) neural <a id="_idIndexMarker987"/>network to <strong class="bold">ARIMA</strong>, <strong class="bold">SARIMA</strong>, <strong class="bold">HWES</strong>, <strong class="bold">ResNet</strong>, <strong class="bold">InceptionTime</strong>, <strong class="bold">MiniRocket</strong>, and <span class="No-Break">many others.</span></p>
			<p>Most tabular data can be converted into time series data. The <strong class="bold">World Series</strong> data is a time series <a id="_idIndexMarker988"/>based on the year. The <strong class="bold">Bank Fraud</strong> data does not directly have a time vector. However, by <a id="_idIndexMarker989"/>adding time data, Pluto can predict at which hour of the day, be it early morning or late night, when most online bank fraud occurs, or he can forecast when most bank fraud happens seasonally, such as around Christmas or college <span class="No-Break">Spring Break.</span></p>
			<p>There are 14 transformation methods, and in particular, Pluto will cover the following <span class="No-Break">three functions:</span></p>
			<ul>
				<li><span class="No-Break">Robust scaler</span></li>
				<li><span class="No-Break">Standard scaler</span></li>
				<li><span class="No-Break">Capping</span></li>
			</ul>
			<p>Let’s start with the <span class="No-Break">robust scaler.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor194"/>Robust scaler</h2>
			<p>The <strong class="bold">K-means</strong> and <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) time series algorithms use Euclidean distance. Thus, scaling <a id="_idIndexMarker990"/>applies to the World Series dataset. When you’re <a id="_idIndexMarker991"/>unsure <a id="_idIndexMarker992"/>of the data <a id="_idIndexMarker993"/>distribution, the robust scaler, also known as <strong class="bold">normalization</strong>, is a viable technique. The <a id="_idIndexMarker994"/>algorithm forecasts <span class="No-Break">future outcomes.</span></p>
			<p>Pluto’s <strong class="source-inline">augment_tabular_robust_scaler()</strong> wrapped function uses the DeltaPy library function and the joy and waffle plots. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define robust scaler</strong>
def augment_tabular_robust_scaler(self, df):
  return deltapy.transform.robust_scaler(df.copy(),
    drop=["checksum"])</pre>
			<p>The full function code can be found in the Python Notebook. The command for the World Series data is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using robust scaler</strong>
df_out = pluto.augment_tabular_robust_scaler(
  pluto.df_world_tokenize_data)
<strong class="bold"># plot joy plot</strong>
pluto.draw_tabular_joyplot(df_out,
  x=['game_1_audience', 'game_2_audience', 'game_3_audience',
     'game_4_audience', 'game_5_audience', 'game_6_audience',
     'game_7_audience'],
  y='network_tokenize',
  t='World series baseball audience')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B17990_09_17.jpg" alt="Figure 9.17 – World Series and robust scaler joy plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – World Series and robust scaler joy plot</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.17</em> confirmed <a id="_idIndexMarker995"/>that Pluto successfully <a id="_idIndexMarker996"/>implemented the robust scaler augmenting technique. Whether it is practical in forecasting is another question entirely. It depends on the goal of the prediction and the base DL model or <span class="No-Break">algorithm used.</span></p>
			<p>The <strong class="bold">standard scaler</strong> is similar to the <span class="No-Break">robust scaler.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor195"/>Standard scaler</h2>
			<p>DL models that rely on <a id="_idIndexMarker997"/>Gaussian distributions or linear and logistic regressions <a id="_idIndexMarker998"/>will benefit from the standardization scaler augmentation method. Pluto’s <strong class="source-inline">augment_tabular_standard_scaler()</strong> wrapper function uses the DeltaPy library function and the joy and waffle plots. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define standard scaler</strong>
def augment_tabular_standard_scaler(self, df):
  return deltapy.transform.standard_scaler(df.copy(),
    drop=["checksum"])</pre>
			<p>The full function code can be found in the Python Notebook. The command is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using standard scaler</strong>
df_out = pluto.augment_tabular_standard_scaler(
  pluto.df_world_tokenize_data)
<strong class="bold"># draw using joy plot</strong>
pluto.draw_tabular_joyplot(df_out,
  x=['game_1_audience', 'game_2_audience', 'game_3_audience',
     'game_4_audience', 'game_5_audience', 'game_6_audience',
     'game_7_audience'],
  y='network_tokenize',
  t='World series baseball audience',
  legloc='upper right')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B17990_09_18.jpg" alt="Figure 9.18 – World Series and standard scaler joy plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – World Series and standard scaler joy plot</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.18</em> demonstrated that Pluto did the augmentation correctly. He did not build and train a DL model <a id="_idIndexMarker999"/>using the augmented data to confirm that it increased the forecast <a id="_idIndexMarker1000"/>accuracy. Many tabular augmentation methods require defining the goal for the DL project to verify if the augmentation is beneficial. For example, Pluto could build a DL model for predicting the audience size for the next <span class="No-Break">World Series.</span></p>
			<p>The next tabular transformation technique we’ll look at <span class="No-Break">is </span><span class="No-Break"><strong class="bold">capping</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor196"/>Capping</h2>
			<p>The capping technique limits the distribution value, such as average, maximum, minimum, or arbitrary <a id="_idIndexMarker1001"/>values. In particular, it restricts the values using <a id="_idIndexMarker1002"/>statistical analysis and replaces the outliers with specific <span class="No-Break">percentile values.</span></p>
			<p>Pluto’s <strong class="source-inline">augment_tabular_capping()</strong> wrapper function uses the DeltaPy library function and correlogram plots. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define capping</strong>
def augment_tabular_capping(self, df):
  x, y = deltapy.transform.outlier_detect(df, "checksum")
  return deltapy.transform.windsorization(df.copy(),
    "checksum",
    y,
    strategy='both')</pre>
			<p>The command <a id="_idIndexMarker1003"/>for the Bank <a id="_idIndexMarker1004"/>Fraud data is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using capping</strong>
df_out = pluto.augment_tabular_capping(
  pluto.df_bank_tokenize_data)
<strong class="bold"># draw correlogram plot</strong>
pluto.draw_tabular_correlogram(df_out,
  title='Bank Fraud Capping Transformation')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B17990_09_19.jpg" alt="Figure 9.19 – Bank Fraud capping correlogram plot, half data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – Bank Fraud capping correlogram plot, half data</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.19</em> indicates that Pluto <a id="_idIndexMarker1005"/>implemented the capping technique <a id="_idIndexMarker1006"/>correctly. Compared to <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em>, the original data, the values are similar, <span class="No-Break">as expected.</span></p>
			<p>The Python implementation of tabular transformation wrapper functions becomes repetitive. Thus, Pluto will provide a brief explanation of the other nine methods in the DeltaPy library. They are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Operations</strong> is a technique for using power, log, or <a id="_idIndexMarker1007"/>square root functions to replace elements in <span class="No-Break">the dataset</span></li>
				<li><strong class="bold">Smoothing</strong> is a technique that <a id="_idIndexMarker1008"/>uses the triple exponential smoothing or Holt-Winters exponential <span class="No-Break">smoothing function</span></li>
				<li><strong class="bold">Decomposing</strong> is a technique <a id="_idIndexMarker1009"/>that uses the naive decomposition function for seasonal vectors in time <span class="No-Break">series data</span></li>
				<li><strong class="bold">Filtering</strong> is a technique <a id="_idIndexMarker1010"/>that uses the Baxter-King bandpass filter to smooth time <span class="No-Break">series data</span></li>
				<li><strong class="bold">Spectral analysis</strong> is a technique <a id="_idIndexMarker1011"/>that uses the periodogram function to estimate the <span class="No-Break">spectral density</span></li>
				<li><strong class="bold">Waveforms</strong> is a technique <a id="_idIndexMarker1012"/>that uses the continuous harmonic wave radar function to augment <span class="No-Break">waveform data</span></li>
				<li><strong class="bold">Rolling</strong> is a technique that <a id="_idIndexMarker1013"/>uses mean or standard deviation to calculate the rolling average over a fixed window size in time <span class="No-Break">series data</span></li>
				<li><strong class="bold">Lagging</strong> is a technique <a id="_idIndexMarker1014"/>that calculates the lagged values in time <span class="No-Break">series data</span></li>
				<li><strong class="bold">Forecast model</strong> is a technique <a id="_idIndexMarker1015"/>that uses the prophet algorithm to forecast seasonal trends, such as weekly or yearly, in time <span class="No-Break">series data</span></li>
			</ul>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to implement three wrapper functions in the Python Notebook from the nine tabular transformation <span class="No-Break">techniques mentioned.</span></p>
			<p>Now that we’ve reviewed <a id="_idIndexMarker1016"/>various tabular transformation <a id="_idIndexMarker1017"/>techniques, let’s look at <span class="No-Break"><strong class="bold">interaction</strong></span><span class="No-Break"> techniques.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor197"/>Interaction augmentation</h1>
			<p>Interaction techniques are <a id="_idIndexMarker1018"/>used in ML and statistical modeling to capture the relationships between two or more features in a dataset for augmentation. The goal is to create new augmentation data that captures the interaction between existing components, which can help improve model performance and provide additional insights into the data. You can apply these techniques to cross-sectional or time-specific data, including normalizing, discretizing, and <span class="No-Break">autoregression models.</span></p>
			<p>Pluto has selected two <a id="_idIndexMarker1019"/>out of seven methods for a hands-on Python programming demonstration. As with the transformation augmentation methods, the coding is repetitive. Thus, Pluto will provide fun challenges for the other five interaction <span class="No-Break">augmentation techniques.</span></p>
			<p>Pluto will start with the <strong class="bold">regression</strong> method, then the <span class="No-Break"><strong class="bold">operator</strong></span><span class="No-Break"> method.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor198"/>Regression augmentation</h2>
			<p>The regression method uses the <strong class="bold">lowess smoother</strong> function to smooth the curve of the data by locally <a id="_idIndexMarker1020"/>weighting the observations near a <a id="_idIndexMarker1021"/>given point. It is a useful tabular augmentation technique for exploring relationships in scatterplots where the relationship between the dependent and independent variables needs to be well-described by a linear function. This method can suffer from forward-looking bias. Thus, Pluto recommends caution in using it for <span class="No-Break">predictive modeling.</span></p>
			<p>Pluto’s <strong class="source-inline">augment_tabular_regression()</strong> wrapper function uses the DeltaPy library function, a joy plot, and a correlogram graph. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define regression</strong>
def augment_tabular_regression(self, df):
  return deltapy.interact.lowess(
    df.copy(),
    ["winning_team_tokenize","losing_team_tokenize"],
    pluto.df_world_tokenize_data["checksum"],
    f=0.25, iter=3)</pre>
			<p>The command for the World Series data is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using tabular regression</strong>
df_out = pluto.augment_tabular_regression(
  pluto.df_world_tokenize_data)
<strong class="bold"># draw joy plot</strong>
pluto.draw_tabular_joyplot(df_out,
  x=['game_1_audience', 'game_2_audience', 'game_3_audience',
     'game_4_audience', 'game_5_audience', 'game_6_audience',
     'game_7_audience'],
  y='network_tokenize',
  t='World series baseball audience: Regression',
  legloc='upper right')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B17990_09_20.jpg" alt="Figure 9.20 – World Series regression augmentation, joy plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.20 – World Series regression augmentation, joy plot</p>
			<p><em class="italic">Figures 9.20</em> confirm that <a id="_idIndexMarker1022"/>Pluto implemented the <a id="_idIndexMarker1023"/>regression tabular augmentation correctly. The DeltaPy library does the actual calculation. Thus, if Pluto made a mistake, the result would be an error, or the dataset would contain random numbers and not display correctly. Pluto can only claim the effectiveness of the regression technique <a id="_idIndexMarker1024"/>to the World Series data. The <a id="_idIndexMarker1025"/>next tabular augmentation technique we’ll look at is the operator <span class="No-Break">augmenting method.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor199"/>Operator augmentation</h2>
			<p>The operator method <a id="_idIndexMarker1026"/>is a simple multiplication or division function <a id="_idIndexMarker1027"/>between two variables in <span class="No-Break">tabular data.</span></p>
			<p>Pluto’s <strong class="source-inline">augment_tabular_operator()</strong> wrapper function uses the DeltaPy library function and a correlogram graph. The essential code snippet is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># define tabular operator</strong>
def augment_tabular_operator(self, df):
  return deltapy.interact.muldiv(
    df.copy(),
    ["credit_risk_score","proposed_credit_limit"])</pre>
			<p>Pluto runs the command for the Bank Fraud data, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
<strong class="bold"># augment using tabular operator</strong>
df_out = pluto.augment_tabular_operator(
  pluto.df_bank_tokenize_data)
<strong class="bold"># draw the correlogram plot</strong>
pluto.draw_tabular_correlogram(df_out,
  title='Bank Fraud Operator Interaction')</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="image/B17990_09_21.jpg" alt="Figure 9.21 – Bank Fraud operator augmentation, correlogram plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.21 – Bank Fraud operator augmentation, correlogram plot</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.21</em> shows a strong <a id="_idIndexMarker1028"/>relationship between three new <a id="_idIndexMarker1029"/>vectors: <strong class="source-inline">credit_risk_score_X_proposed_credit_limit</strong> (multiply), <strong class="source-inline">proposed_credit_limit/credit_risk_score</strong> (divide), and <strong class="source-inline">proposed_credit_limit_X_credit_risk_score</strong> (multiply). Pluto implements the operator function correctly but still determines the benefit of the DL <span class="No-Break">prediction accuracy.</span></p>
			<p>The other five interaction tabular augmentation techniques are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Discretizing</strong> is a method <a id="_idIndexMarker1030"/>that uses <strong class="bold">decision trees</strong>, <strong class="bold">equal width binding</strong>, <strong class="bold">equal frequency binding</strong>, and <strong class="bold">K-means clustering</strong> to augment the tabular data. The discretization method depends on the AI model and the tabular data properties. Pluto recommends trying multiple approaches and evaluating <span class="No-Break">their performance.</span></li>
				<li>The <strong class="bold">quantile normalizing</strong> method <a id="_idIndexMarker1031"/>makes the distributions of the datasets comparable by transforming them so that they have the same cumulative <span class="No-Break">distribution value.</span></li>
				<li>The <strong class="bold">haversine distance</strong> calculates <a id="_idIndexMarker1032"/>the shortest distance between two angular points, such as the Earth’s surface. Tabular augmentation also uses the <strong class="bold">Euclidean</strong>, <strong class="bold">Mahalanobis</strong>, and <strong class="bold">Minkowski</strong> <span class="No-Break">distance algorithms.</span></li>
				<li>The <strong class="bold">technical</strong> indicator is one of the <strong class="bold">specialty</strong> methods in tabular augmentation. It uses <a id="_idIndexMarker1033"/>technical analysis to help predict future price movements of securities or financial instruments. They are based on mathematical calculations of price, volume, and <span class="No-Break">open interest.</span></li>
				<li>The <strong class="bold">genetic</strong> method, or <strong class="bold">genetic</strong> tabular augmentation, is a type of ML technique that uses <a id="_idIndexMarker1034"/>evolutionary algorithms to optimize the AI model. The concept is to create a <a id="_idIndexMarker1035"/>population of candidate solutions, or <strong class="bold">chromosomes</strong>, for a <a id="_idIndexMarker1036"/>problem, then evolve that population over time by applying genetic operations such as crossover, mutation, and selection. The goal is to find the best solution to the problem through <span class="No-Break">natural selection.</span></li>
			</ul>
			<p class="callout-heading">Fun challenge</p>
			<p class="callout">Pluto challenges you to implement two more interaction augmentations in the <span class="No-Break">Python Notebook.</span></p>
			<p>The next tabular augmentation class is <strong class="bold">mapping</strong> augmentation. Pluto will describe the mapping functions but not implement them in the <span class="No-Break">Python Notebook.</span></p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor200"/>Mapping augmentation</h1>
			<p>The mapping method <a id="_idIndexMarker1037"/>uses ML and data analysis to summarize and reduce the dimensionality of data for augmentation. It can be done via unsupervised or supervised learning. Some examples of mapping methods include <strong class="bold">eigendecomposition</strong> and PCA. PCA is a statistical procedure <a id="_idIndexMarker1038"/>that transforms a set of correlated variables into uncorrelated variables called <span class="No-Break">principal components.</span></p>
			<p>In the DeltaPy library, there are seven mapping methods for tabular augmentation. Pluto has done a few implementations in the Python Notebook, but he will not explain the coding here. The Python wrapper function is repetitive and can easily be applied to any mapping method. The functions are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Eigendecomposition</strong> (<strong class="bold">ED</strong>) is a form <a id="_idIndexMarker1039"/>of <strong class="bold">PCA</strong> for tabular <a id="_idIndexMarker1040"/>augmentation. In ED, the <strong class="bold">eigenvectors</strong> are the <a id="_idIndexMarker1041"/>covariance matrix of the data, <a id="_idIndexMarker1042"/>and the corresponding <strong class="bold">eigenvalues</strong> represent the amount <a id="_idIndexMarker1043"/>of variance by each component. ED includes <strong class="bold">linear discriminant analysis</strong> (<strong class="bold">LDA</strong>), <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>), and <span class="No-Break"><strong class="bold">Markov chains</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Cross-decomposition</strong> methods, including <strong class="bold">canonical correlation analysis</strong> (<strong class="bold">CCA</strong>), are used <a id="_idIndexMarker1044"/>to uncover linear relationships <a id="_idIndexMarker1045"/>between two pieces of multivariate tabular data. Various applications, such as dimensionality reduction, feature extraction, and feature selection, use the cross-decomposition method. The goal is to find a linear combination between tabular <span class="No-Break">data variables.</span></li>
				<li><strong class="bold">Kernel approximation</strong> methods are used in ML algorithms such as SVMs to transform the tabular <a id="_idIndexMarker1046"/>data into a higher dimensional <a id="_idIndexMarker1047"/>space where a linear boundary can be found to separate the classes. The <strong class="bold">additive Chi2 kernel</strong> is a specific <strong class="bold">kernel approximation</strong> method that measures the independence between two sets <span class="No-Break">of variables.</span></li>
				<li><strong class="bold">Autoencoders</strong> are used in various domains, such as image compression, anomaly detection, and <a id="_idIndexMarker1048"/>for generating new data for tabular augmentation. We use autoencoders in the pre-training step for supervised learning tasks to improve the subsequent <span class="No-Break">models’ performance.</span></li>
				<li><strong class="bold">Manifold learning</strong> is a class of techniques for non-linear dimensionality reduction, aiming to <a id="_idIndexMarker1049"/>preserve the tabular data’s underlying non-linear structure. <strong class="bold">Locally linear embedding</strong> (<strong class="bold">LLE</strong>) is one method in which <a id="_idIndexMarker1050"/>the idea is to approximate the local linear relationships between data points in the high-dimensional space. The goal is to find a lower-dimensional representation of high-dimensional data that still captures the essential patterns and relationships in the <span class="No-Break">tabular data.</span></li>
				<li><strong class="bold">Clustering</strong> is a popular <a id="_idIndexMarker1051"/>unsupervised ML technique for grouping similar tabular data points into clusters. Clustering methods help in identifying patterns and structure in <span class="No-Break">tabular data.</span></li>
				<li><strong class="bold">Neighbouring</strong> is the nearest neighbor method for supervised ML algorithms for classification <a id="_idIndexMarker1052"/>and regression problems. It also is used in tabular <a id="_idIndexMarker1053"/>augmentation. You can extend the nearest neighbor method to a <a id="_idIndexMarker1054"/>more sophisticated version called <strong class="bold">k-nearest neighbor</strong> (<span class="No-Break"><strong class="bold">k-NN</strong></span><span class="No-Break">) classification.</span></li>
			</ul>
			<p>The next classification of tabular augmentation we’ll look at is <span class="No-Break"><strong class="bold">extraction</strong></span><span class="No-Break"> augmentation.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor201"/>Extraction augmentation</h1>
			<p>The extraction method is a process in time series analysis where multiple constructed elements are used as <a id="_idIndexMarker1055"/>input, and a singular value is extracted from each time series to create new augmented data. This method uses a package called <strong class="bold">TSfresh</strong> and includes default and custom features. The output <a id="_idIndexMarker1056"/>of extraction methods differs from the output of transformation and interaction methods, as the latter outputs entirely new time series data. You can use this method when specific values need to be pulled from time <span class="No-Break">series data.</span></p>
			<p>The DeltaPy library contains 34 extraction methods. Writing the wrapper functions for extraction is similar to the wrapper transformation functions. The difficulty is how to discern the forecasting’s effectiveness from tabular augmentation. Furthermore, these methods are components and not complete functions for <span class="No-Break">tabular augmentation.</span></p>
			<p>Pluto will not explain each function, but here is a list of the extraction functions in the DeltaPy library: <strong class="source-inline">Amplitude</strong>, <strong class="source-inline">Averages</strong>, <strong class="source-inline">Autocorrelation</strong>, <strong class="source-inline">Count</strong>, <strong class="source-inline">Crossings</strong>, <strong class="source-inline">Density</strong>, <strong class="source-inline">Differencing</strong>, <strong class="source-inline">Derivative</strong>, <strong class="source-inline"><a id="_idIndexMarker1057"/></strong><strong class="source-inline">Distance</strong>, <strong class="source-inline">Distribution</strong>, <strong class="source-inline">Energy</strong>, <strong class="source-inline">Entropy</strong>, <strong class="source-inline">Exponent</strong>, <strong class="source-inline">Fixed Points</strong>, <strong class="source-inline">Fluctuation</strong>, <strong class="source-inline">Fractals</strong>, <strong class="source-inline">Information</strong>, <strong class="source-inline">Linearity</strong>, <strong class="source-inline">Location</strong>, <strong class="source-inline">Model</strong> <strong class="source-inline">Coefficients</strong>, <strong class="source-inline">Non-linearity</strong>, <strong class="source-inline">Occurrence</strong>, <strong class="source-inline">Peaks</strong>, <strong class="source-inline">Percentile</strong>, <strong class="source-inline">Probability</strong>, <strong class="source-inline">Quantile</strong>, <strong class="source-inline">Range</strong>, <strong class="source-inline">Shape</strong>, <strong class="source-inline">Size</strong>, <strong class="source-inline">Spectral</strong> <strong class="source-inline">Analysis</strong>, <strong class="source-inline">Stochasticity</strong>, <strong class="source-inline">Streaks</strong>, <strong class="source-inline">Structural</strong>, <span class="No-Break"><strong class="source-inline">and</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">Volatility</strong></span><span class="No-Break">.</span></p>
			<p>The extraction method is the last tabular augmentation category. Thus, it is time for <span class="No-Break">a summary.</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor202"/>Summary</h1>
			<p>Tabular augmentation is a technique that can improve the accuracy of ML models by increasing the amount of data used. It adds columns or rows to a dataset generated by existing features or data from other sources. It increases the available input data, allowing the model to make more accurate predictions. Tabular augmentation adds new information not currently included in the dataset, increasing the model’s utility. Tabular augmentation is beneficial when used with other ML techniques, such as DL, to improve the accuracy and performance of <span class="No-Break">predictive models.</span></p>
			<p>Pluto downloaded the real-world Bank Fraud and World Series datasets from the <em class="italic">Kaggle</em> website. He wrote most of the code in the Python Notebook for visualizing large datasets using various graphs, such as histograms, heatmaps, correlograms, and waffle and joy plots. He did this because understanding the datasets is essential before augmenting them. However, he didn’t write a CNN or RNN model to verify the augmentation methods because building a CNN model is a complex process worthy of a <span class="No-Break">separate book.</span></p>
			<p>The DeltaPy open source library contains dozens of methods for tabular augmentation, but it is a beta version and can be unstable to load. Still, Pluto demonstrated a few tabular augmentation techniques, such as the robust scaler, standard scaler, capping, regression, and <span class="No-Break">operator methods.</span></p>
			<p>Throughout this chapter, there were <em class="italic">fun facts</em> and <em class="italic">fun challenges</em>. Pluto hopes you will take advantage of these and expand your experience beyond this <span class="No-Break">book’s scope.</span></p>
			<p>This is the last chapter of the book. You and Pluto have covered augmentation techniques for image, text, audio, and tabular data. As AI and generative AI continue to expand and integrate into the fabric of our life, data will play an essential role. Data augmentation methods are the best practical option to extend your datasets without the high cost of gathering and purchasing additional data. Furthermore, generative AI transforms how we work and play, such as OpenAI's GPT3, GPT4, Google Bard, and Stability.ai's Stable Diffusion. What you discussed about AI in boardrooms or classrooms last month will be outdated, but the data augmentation concepts and techniques remain <span class="No-Break">the same.</span></p>
			<p>You and Pluto have learned to code the augmentation techniques using wrapper functions and download real-world datasets from the Kaggle website. As new, better, and faster augmentation libraries are available, you can add to your collection or switch the libraries under the hood. What you implement may change slightly, but what you have learned about data augmentation will <span class="No-Break">remain true.</span></p>
			<p>I hope you enjoyed reading the book and hacking the Python Notebook as much as I enjoyed writing it. <span class="No-Break">Thank you.</span></p>
		</div>
	</body></html>