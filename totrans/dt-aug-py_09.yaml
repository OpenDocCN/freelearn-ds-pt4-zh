- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tabular Data Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tabular augmentation supplements tabular data with additional information to
    make it more useful for predictive analytics. Database, spreadsheet, and table
    data are examples of tabular data. It involves transforming otherwise insufficient
    datasets into robust inputs for ML. Tabular augmentation can help turn unstructured
    data into structured data and can also assist in combining multiple data sources
    into a single dataset. It is an essential step in data pre-processing for increasing
    AI predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of tabular augmentation is to include additional information to a given
    dataset that can then be used to generate valuable insights. These datasets can
    come from various sources, such as customer feedback, social media posts, and
    IoT device logs. Tabular augmentation can add new information columns to the dataset
    by enriching the existing columns with more informative tags. It increases the
    completeness of the dataset and provides more accurate insights.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular augmentation is an important method to consider when pre-processing
    and generating insights from data. It provides a way to work with incomplete and
    unstructured datasets by organizing and enriching them for improved accuracy and
    speed. By implementing tabular augmentation, you can better unlock the value of
    real-world datasets and make better-informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular augmentation is a young field for data scientists. It is contrary to
    using analytics for reporting, summarizing, or forecasting. In analytics, altering
    or adding data to skew the results to a preconceived desired outcome is unethical.
    In data augmentation, the purpose is to derive new data from an existing dataset.
    The two goals might be incongruent, but they are not. DL is an entirely different
    technique from traditional analytics. One is based on a neural network algorithm,
    while the other is based on statistical analysis and data relationships.
  prefs: []
  type: TYPE_NORMAL
- en: The salient point is that even though you might introduce synthetic data into
    the datasets, it is an acceptable practice. The *Synthesizing Tabular Data using
    Generative Adversarial Networks* paper, by Lei Xu and Kalyan Veeramachaneni, published
    in the *arXiv Forum* in November 2018, supports this proposition.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on describing concepts. It has a few practical coding examples
    using the Python Notebook. One main reason for this is that there are only a few
    tabular augmentation open source libraries available. You will spend most of the
    coding time plotting various graphs to inspire further insight from the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing, let’s take a sneak peek at a real-world tabular dataset.
    Later, Pluto will explain in detail how to write Python code for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Bank Account Fraud Dataset Suite (NeurIPS 2022)](img/B17990_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Bank Account Fraud Dataset Suite (NeurIPS 2022)
  prefs: []
  type: TYPE_NORMAL
- en: One challenge in augmenting tabular data is that no fixed methods work universally,
    such as flipping images, injecting misspelled words, or time-stretching audio
    files. You will learn that the dataset dictates which augmentation techniques
    are **safe** or in a **safe range**. It is essential to thoroughly review the
    tabular dataset before augmenting it.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep neural networks** (**DNNs**) excel at predicting future stock values
    and tabular data, based on the scholarly paper *Deep learning networks for stock
    market analysis and prediction: Methodology, data representations, and case studies*,
    by Eunsuk Chong, Chulwoo Han, and Frank C. Park. It was published by Elsevier,
    *Expert Systems with Applications*, Volume 83, on 15 October 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tabular augmentation is an approach to augmenting a tabular dataset with synthetic
    data. It involves adding new columns to a tabular dataset with features from the
    derived calculation. You will spend the majority of the time in Python code visualizing
    the real-world tabular dataset with exotics plots. In this chapter, we will cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Tabular augmentation libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmentation categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world tabular datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and visualizing tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraction augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with augmentation libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular augmentation libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tabular augmentations are not established as image, text, or audio augmentations.
    Typically, data scientists develop tabular augmentation techniques specific to
    a project. There are a few open source projects on the GitHub website. Still,
    DL and generative AI will continue to advance in forecasting for time series and
    tabular data predictions, and so will tabular augmentations. The following open
    source libraries can be found on the GitHub website:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DeltaPy** is a tabular augmentation for generating and synthesizing data
    focusing on financial applications such as time series stock forecasting. It fundamentally
    applies to a broad range of datasets. The GitHub website link is [https://github.com/firmai/deltapy.](https://github.com/firmai/deltapy.)
    The published scholarly paper is called *DeltaPy: A Framework for Tabular Data
    Augmentation in Python*, by Derek Snow, The Alan Turing Institute, in 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Synthetic Data Vault** (**SDV**) is for augmenting tabular data by generating
    synthetic data from a single table, multi-table, and time series data. In 2020,
    Kalyan Veeramachaneni, Neha Patki, and Saman Amarsinghe developed a commercial
    version named *Datacebo*. The GitHub link is [https://github.com/sdv-dev/SDV](https://github.com/sdv-dev/SDV).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tabular **Generative Adversarial Network** (**GAN**) uses the successfully
    generating realistic image algorithm and applies it to tabular augmentation. The
    scholarly paper is *Tabular GANs for uneven distribution*, by Insaf Ashrapov,
    published by *Cornell University*, *Arxiv*, in 2020\. The GitHub website link
    is [https://github.com/Diyago/GAN-for-tabular-data](https://github.com/Diyago/GAN-for-tabular-data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluto has chosen the **DeltaPy** library to use as the engine under the hood
    for his tabular augmenting wrapper functions, but first, let’s look at the augmentation
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is advantageous to group tabular augmentation into categories. The following
    concepts are new and particular to the DeltaPy library. The augmentation functions
    are grouped into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transforming** techniques can be applied for cross-section and time series
    data. Transforming techniques in tabular augmentation are used to modify existing
    rows or columns to create new, synthetic data representative of the original data.
    These methods can include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: Increasing or decreasing a column value to expand the diversity
    of values in a dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binning**: Combining two or more columns into a single bucket to create new
    features'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical encoding**: Using a numerical representation of categorical data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing**: Compensating for unusually high or low values in a dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection and removal**: Detecting and removing points farther from
    the norm'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation-based augmentation**: Adding new features based on correlations
    between existing features'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **interacting** function is a cross-sectional or time series tabular augmentation
    that includes normalizing, discretizing, and autoregression models. In tabular
    augmentation, these functions are used to specify interactions between two or
    more variables and help generate new features that represent combinations of the
    original variables. This type of augmentation is beneficial when modeling the
    relationships between multiple input features and the target variable, as it allows
    the model to consider interactions between the different components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **mapping** method, which uses **eigendecomposition** in tabular augmentation,
    is a method of unsupervised learning that uses data decomposition to transform
    data into lower-dimensional space using eigenvectors and eigenvalues. This type
    of feature transformation is useful for clustering, outlier detection, and dimensionality
    reduction. By projecting the data onto the eigenvectors, the data can be represented
    in a reduced space while still preserving the structure of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **extraction** method is a tabular augmentation technique that utilizes
    **Natural Language Processing** (**NLP**) to generate additional information from
    textual references in tabular datasets. It uses the **TSflesh** library, a collection
    of rules and heuristics, to extract additional data from text, such as names,
    dates, and locations. This approach is beneficial in augmenting structured datasets,
    where the output of **sentence split**, **tokenization**, and **part-of-speech
    tagging** is used to create features that can be used for further processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series synthesis** (**TSS**) is a method for tabular data augmentation
    where rows of data across multiple sources or temporal points in time are synthesized
    together. You can use it to increase a dataset’s size and create a more consistent
    set of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-sectional synthesis** (**CSS**) is a method for tabular data augmentation
    where columns of data from multiple sources are combined. You can use it to increase
    a dataset’s features and create a more complete and holistic data view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **combining** technique uses the mix-and-match process from the existing
    methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are functions associated with each category in the DeltaPy library. However,
    Pluto has to construct a neural network model, such as a **convolutional neural
    network** (**CNN**) or **reoccurring neural network** (**RNN**), to gauge the
    effectiveness of these methods. It is a complex process, and Pluto will not implement
    a CNN in this chapter. Nevertheless, Pluto will demonstrate the mechanics of using
    the DeltaPy library on the Python Notebook. He will not explain how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to download the real-world datasets from the *Kaggle* website.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world tabular datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are thousands of real-world tabular datasets on the *Kaggle* website.
    Pluto has chosen two tabular datasets for this process.
  prefs: []
  type: TYPE_NORMAL
- en: The *Bank Account Fraud Dataset Suite (NeurIPS 2022)* contains six synthetic
    bank account fraud tabular datasets. Each dataset contains 1 million records.
    They are based on real-world data for fraud detection. Each dataset focuses on
    a different type of bias. Sergio Jesus, Jose Pombal, and Pedro Saleiro published
    the dataset in 2022 under the **Attribution-NonCommercial-ShareAlike 4.0 International
    (CC BY-NC-SA 4.0)** license. The *Kaggle* link is [https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022).
  prefs: []
  type: TYPE_NORMAL
- en: The *World Series Baseball Television Ratings* is a dataset for audiences watching
    the baseball World Series on television from 1969 to 2022\. Matt OP published
    the dataset in 2022 under the **CC0 1.0 Universal (CC0 1.0) Public Domain Dedication**
    license. The *Kaggle* link is [https://www.kaggle.com/datasets/mattop/world-series-baseball-television-ratings](https://www.kaggle.com/datasets/mattop/world-series-baseball-television-ratings).
  prefs: []
  type: TYPE_NORMAL
- en: The steps for instantiating Pluto and downloading real-world datasets from the
    *Kaggle* website are the same. It starts with loading the `data_augmentation_with_python_chapter_9.ipynb`
    file into Google Colab or your chosen Jupyter Notebook or JupyterLab environment.
    From this point onward, the code snippets are from the Python Notebook, which
    contains the complete functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be using the code from [*Chapter 2*](B17990_02.xhtml#_idTextAnchor038)
    because you will need the wrapper functions for downloading the *Kaggle* dataset,
    not the wrapper functions for image, text, and audio augmentations. You should
    review *Chapters 2* and *3* if the steps are unfamiliar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: At the end of [*Chapter 8*](B17990_08.xhtml#_idTextAnchor167), Pluto challenged
    you to refactor the Pluto code for speed and compactness. The goal is to upload
    Pluto to [Pypi.org](https://Pypi.org). This challenge extends that concept and
    asks you to combine the setup code into one uber wrapper function, such as `pluto.just_do_it()`.
    Pluto does not use uber methods because this book aims to make the concepts and
    functions easier to learn and demystify the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for gathering Pluto’s system information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to search for, download, and import two additional tabular
    datasets from the *Kaggle* website or your project into pandas.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you have selected a tabular augmentation library, cloned the GitHub
    repository, instantiated Pluto, and downloaded the two real-world tabular datasets
    from the *Kaggle* website. Now, it is time for Pluto to explore and visualize
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tabular augmentation is more challenging than image, text, and audio augmentation.
    The primary reason is that you need to build a CNN or RNN model to see the effect
    of the synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto will spend more time explaining his journey to investigate the real-world
    Bank Fraud and World Series datasets than implementing the tabular augmentation
    functions using the DeltaPy library. Once you understand the data visualization
    process, you can apply it to other tabular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Typically, Pluto starts a chapter by writing code in the Python Notebook for
    that chapter. It consists of around 150 to 250 combined code and text cells. They
    are unorganized collections of research notes and try-and-error Python code cells.
    Once Pluto proves that the concepts and techniques are working correctly through
    coding, he starts writing the chapter. As part of the writing progress, he cleans
    and refactors the Python Notebook with wrapper functions and deletes the dead-end
    code. The clean version of the Python Notebook contains 20% to 30% of the original
    code and text cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, while exploring tabular data, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First graph view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checksum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the World Series baseball dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Data structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pluto starts by inspecting the data structure using pandas’ built-in function.
    He uses the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Bank Fraud dataset consists of 32 columns, 1 million records or rows, no
    null values, and five columns that are not numeric. Pluto wants to find out which
    columns are **continuous** or **categorical**. He does this by calculating the
    unique value in each column. He uses the following pandas function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The partial output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Python Notebook contains the complete result. There are 7 continuous columns
    and 25 categorical columns. Generally, continuous columns have many unique values,
    as in total records, while categorical columns have unique values between two
    and a few hundred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using plots to display the data, Pluto will view sample data from the
    Bank Fraud dataset with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Sample Bank Fraud data](img/B17990_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Sample Bank Fraud data
  prefs: []
  type: TYPE_NORMAL
- en: After repeatedly running the command and variation, Pluto finds no surprises
    in the data. It is clean. The Python Notebook contains additional inspecting functions,
    such as the pandas `describe()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: For a tabular dataset, you will write custom code for inspecting, visualizing,
    and augmenting the data. In other words, there will be more reusable concepts
    and processes than reusable code being carried over to the next project.
  prefs: []
  type: TYPE_NORMAL
- en: The Bank Fraud dataset has 32 million elements, which is the typical size of
    data that data scientists work with. However, your Python Notebook would crash
    if you tried to plot 32 million points using pandas and Matplotlib with the default
    settings. Pluto created a simple graph, `pluto.df_bank_data.plot()`, and his Google
    Colab Pro-version Python Notebook crashed. It required additional RAM.
  prefs: []
  type: TYPE_NORMAL
- en: First graph view
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The various plots are not directly aiding in the tabular augmentation process.
    The primary goal is for you to envision a sizeable tabular dataset. Reading millions
    of data points is less effective than seeing them plotted on a graph. You may
    skip the sections about plotting and go directly to the tabular augmentation techniques
    using the DeltaPy library.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a large dataset, the solution is to select graphs with calculated or summarizing
    values. Hence, there will be fewer points to plot. For example, the **histogram**
    graph is a viable choice because it groups the frequency of ranges. Pluto uses
    a wrapper function to draw the histogram plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The key code line for the wrapper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Bank Fraud histogram plot](img/B17990_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Bank Fraud histogram plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.3* does not yield any beneficial insights. Thus, Pluto proceeds
    to summarize the data with a **checksum** concept.'
  prefs: []
  type: TYPE_NORMAL
- en: Checksum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pluto spends weeks playing with different types of graphs and graphing packages
    such as **Matplotlib**, **Seaborn**, **Joypi**, and **PyWaffle**. He has fun,
    but most do not enhance the visualization of the Bank Fraud and World Series datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, Pluto will get back to more plotting. In tabular data, displaying
    the string, non-numeric data is challenging. A clean solution is transforming
    the categorical string data into an integer token index. Pluto writes the `_fetch_token_index()`
    helper function to index value from a list. The key code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `add_token_index()` wrapped function uses the helper function and the pandas
    `apply()` function. The essential code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To put it all together, Pluto uses the following command to copy and create
    the tokenized columns for the Data Fraud dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto double-checked the tokenization by viewing sample values using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Bank Fraud sample tokenized data](img/B17990_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Bank Fraud sample tokenized data
  prefs: []
  type: TYPE_NORMAL
- en: Pluto double-checked the other columns, and they are correct. You can view the
    code and the results by reading the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'For data analysis, it is practical to have a `_fetch_checksum()` helper function
    uses the pandas `apply()` method with `lambda`. The code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto calculates the checksum for the Bank Fraud dataset using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It took 27 seconds to compute the checksum for 32 million data points. Now,
    let’s explore a few specialized plots with the **checksum** concept.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pluto wants to remind you that the following graphs and exercises do not directly
    pertain to tabular augmentation. The goal is to sharpen your skills in understanding
    and visualizing sizeable real-world datasets – for example, the Bank Fraud dataset
    consists of 1 million records in preparation for data augmentation. You can skip
    the plotting exercises and jump directly to the tabular augmentation lessons if
    you wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto creates `self.df_bank_half_data` with a limited number of columns for
    ease of display. He uses `heatmap()` function to draw the **correlogram** plot.
    The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Bank Fraud half correlogram](img/B17990_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Bank Fraud half correlogram
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.5* shows a high relationship between `credit_risk_score` and `proposed_credit_limit`
    with 61%. `fraud_bool` has a low correlation with all other parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: When Pluto draws the correlogram plot with the entire dataset, it exposes a
    high correlation between the **checksum** and **velocity_6h**, **velocity_24h**,
    and **velocity_4w**. The code and the output can be found in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `draw_tabular_heatmap()` wrapper function looks like a heatmap. The command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Bank Fraud checksum and month heatmap](img/B17990_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Bank Fraud checksum and month heatmap
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.6* shows a pattern, but the relationship between the **checksum**
    and **month** is unclear.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Pluto is not an expert in reading Bank Fraud data, and it is natural for you
    not to be an expert in every domain. Pluto consults friends in banking and consumer
    protection agencies for background research. Here are a few charts that he uses
    in his work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fraud data, `fraud_bool == 1`, is 1% of the total. Thus, Pluto might want
    to augment more fraud data. He creates a pandas DataFrame using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following two graphs suggested by Pluto’s banking expert friends are fun
    to create but may not benefit the Bank Fraud augmentation. The complete code is
    in the Python Notebook. Nevertheless, they are thought-provoking concepts over
    the standard line or bar charts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Bank Fraud Seaborn heatmap with mask](img/B17990_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Bank Fraud Seaborn heatmap with mask
  prefs: []
  type: TYPE_NORMAL
- en: The next graph is the Swarmplot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Bank Fraud Seaborn swarm plot](img/B17990_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Bank Fraud Seaborn swarm plot
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you make use of the `tripcolor()` 3D graph, shown in *Figure 9**.9*, using
    the Bank Fraud dataset? The `tripcolor()` code is in the Python Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Fun challenge – a tripcolor plot of random values](img/B17990_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Fun challenge – a tripcolor plot of random values
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the World Series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, Pluto will spend much time plotting various graphs to understand
    and visualize the World Series data. He is not performing tabular augmentation.
    Even though comprehending the data is essential before deciding which augmentation
    functions are applicable, you can skip this exercise and directly go to the tabular
    augmentation wrapper functions.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: Anecdotally, Pluto, an imaginary Sybirian Huskey, loves to rush ahead and start
    writing augmenting code without taking the time to sniff out the content of the
    datasets. Consequently, his AI model diverged 9 out of 10 times, resulting in
    high levels of false negatives and false positives. Thus, spending 40% to 70%
    of the time studying the datasets seems non-productive, but it is not. It is an
    acceptable fact when working with real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto follows a similar process for the World Series dataset. He runs the first
    `info()` method, followed by `nunique(), describe()`, and then `sample()`. The
    World Series dataset consists of 14 columns and 54 rows, representing 756 data
    points. There are 11 numeric columns and three label categories. Other factors
    are eight `pluto.df_world_data.info()` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Other results can be found in the Python Notebook. The histogram plot is the
    practical first data visualization technique for the World Series dataset. The
    command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – World Series histogram](img/B17990_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – World Series histogram
  prefs: []
  type: TYPE_NORMAL
- en: 'The histogram plot shown in *Figure 9**.10* does not highlight the comparison
    between the audience in the seven games. Pluto uses the `joyplot()` method from
    the **joypy** library to display the relationship between the viewing audience
    and the TV networks. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – World Series audience and TV networks](img/B17990_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – World Series audience and TV networks
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.11* is a beautiful and insightful visualization plot. NBC television
    network has the highest number of game viewers for game #7 but also has the lowest
    number for game #5\. Fox TV has the least number of viewers, and ABC TV has the
    highest total viewers but only a little more than NBC TV. Game #3 has the lowest
    number of viewers, while game #7 has the highest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto prepares the World Series dataset for augmenting by converting the label
    categories into integer token indexes and calculates the checksum. The commands
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for double-checking and printing the results for the tokenization
    and checksum can be found in the Python Notebook. Pluto made a quick correlogram
    plot with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – World Series correlogram plot](img/B17990_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – World Series correlogram plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.12* exposes many intriguing relationships between the data. For
    example, there is a 100% correlation between **losing_team_wins** and **total_game_played**,
    and strong relationships between **average_audience, game_1_audience**, **game_2_audience**,
    **game_3_audience**, and **game_4_audience**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `joyplot()` method to compare the checksum with the average
    viewers grouped by the TV networks. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – World Series checksum, average audience grouped by TV networks](img/B17990_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – World Series checksum, average audience grouped by TV networks
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9**.13*, Pluto uses the `mean()` function to calculate the checksum
    values. Thus, the comparison to the average viewers yields a similar shape. Compared
    to *Figure 9**.11*, the relationship between average audience size and each game’s
    total is not immediately apparent because CBS TV has the highest average but seems
    to have lower per-game viewers.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, Pluto wonders if plotting more graphs would help him understand
    the dataset better. There is a good chance that you are thinking the same thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: The justification for exploring additional charts is twofold. The real-world
    tabular data is diverse. Thus, knowing various graphs makes you better prepared
    to tackle your next project. Second, no criteria or algorithm lets you know you
    have learned about the datasets sufficiently. Therefore, if you know the data,
    skip to the tabular augmentation functions section or follow along with Pluto
    as he learns new graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto uses the `draw_tabular_waffle()` uses the `Waffle` class from the **pywaffle**
    library. The command for displaying the World Series winning teams is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – World Series winning team](img/B17990_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – World Series winning team
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto does the same for displaying the losing teams. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – World Series losing team](img/B17990_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – World Series losing team
  prefs: []
  type: TYPE_NORMAL
- en: '*Figures 9.14* and *9.15* are beautifully colored waffle graphs. There is no
    dominant or underdog team. Pluto does the same for TV networks. The command is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – World Series TV networks](img/B17990_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – World Series TV networks
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.16* yields a surprising hidden fact in the data: Fox TV aired the
    most games, but from *Figures 9.11* and *9.12*, it does not seem like the network
    with the most viewers.'
  prefs: []
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a thought experiment: can you visualize a **four-dimensional** (**4D**)
    graph? Hint: a 2D chart displays two measurements, such as the number of TV audiences
    per game, or one vector with an implied time series as the X-axis, such as the
    bank member’s income with the X-axis indicated as day or month. A 3D graph typically
    reveals the snow depth level on a mountain. Time could be the fourth dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has explored and explained the real-world Bank Fraud and World Series
    datasets. He uses pandas functions to display statistical information and provides
    numerous graphs to visualize them. Understanding and visualizing the data is the
    first and most essential step before augmenting tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Fun fact
  prefs: []
  type: TYPE_NORMAL
- en: 'Data augmentation is a secret for DL and generative AI to achieve unprecedented
    accuracy and success. Many scholarly papers reinforced data augmentation’s significance,
    such as *Enhancing Performance of Deep Learning Models with Different Data Augmentation
    Techniques: A Survey*, by Cherry Khosla and Baljit Singh Saini, published by *IEEE
    2020 Intelligent Engineering and Management (ICIEM),* *International Conference*.'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before digging into the tabular augmentation methods, Pluto will reiterate
    that he will not build a neural network model to test if the augmentation benefits
    the particular dataset. In addition, the pattern for writing the wrapper functions
    follows the previous practice: using the chosen library to do the critical augmentation
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: As the Python Notebook notes, the DeltaPy library’s dependency is the **fbprofet**
    and **pystan** libraries. The three libraries are in beta and may be unstable.
    Pluto has repeatedly tested the Python code. Once the libraries have been loaded,
    the code works flawlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular transformation is a collection of techniques that take one variable
    and generate a new dataset based on the transformation method. It applies to both
    cross-section and time series data. The DeltaPy library defines 14 functions for
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: These transformation techniques include the **operations** functions used in
    present information, the **smoothing** method used with past data, and the **select
    filters** procedure used with lagging and leading values.
  prefs: []
  type: TYPE_NORMAL
- en: In image augmentation, Pluto can run the functions and see what changes in the
    photo. Here, the effects are apparent, such as **cropped**, **enlarged**, or **altered**
    hue values. Tabular augmentation requires knowledge of DL and time series data.
    In other words, the output effects are not obvious; therefore, selecting augmentation
    functions for a particular dataset can be intimidating. Pluto will demonstrate
    how to write Python code for tabular augmentation, but he will not thoroughly
    explain when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Time series forecasting is a mature and highly researched branch of AI. It could
    take several college courses to understand a time series and how to forecast or
    predict future outcomes. A compact definition of a time series is a data sequence
    that depends on time. Typical time series data is the market stock value. For
    example, Pluto uses the Microsoft stock data for the previous 10 years to predict
    the closing price of tomorrow, next week, or next month. Weather forecasting is
    another widespread use of time series algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The two key concepts in time series data are **lag time** and **windows**. The
    lag time is from the observer to a set point, while the window is the range of
    elements segmented. There are dozens of other key concepts in time series algorithms,
    from the earliest **long short-term memory** (**LSTM**) neural network to **ARIMA**,
    **SARIMA**, **HWES**, **ResNet**, **InceptionTime**, **MiniRocket**, and many
    others.
  prefs: []
  type: TYPE_NORMAL
- en: Most tabular data can be converted into time series data. The **World Series**
    data is a time series based on the year. The **Bank Fraud** data does not directly
    have a time vector. However, by adding time data, Pluto can predict at which hour
    of the day, be it early morning or late night, when most online bank fraud occurs,
    or he can forecast when most bank fraud happens seasonally, such as around Christmas
    or college Spring Break.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 14 transformation methods, and in particular, Pluto will cover the
    following three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Robust scaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard scaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the robust scaler.
  prefs: []
  type: TYPE_NORMAL
- en: Robust scaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **K-means** and **principal component analysis** (**PCA**) time series algorithms
    use Euclidean distance. Thus, scaling applies to the World Series dataset. When
    you’re unsure of the data distribution, the robust scaler, also known as **normalization**,
    is a viable technique. The algorithm forecasts future outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto’s `augment_tabular_robust_scaler()` wrapped function uses the DeltaPy
    library function and the joy and waffle plots. The essential code snippet is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. The command for
    the World Series data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – World Series and robust scaler joy plot](img/B17990_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – World Series and robust scaler joy plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.17* confirmed that Pluto successfully implemented the robust scaler
    augmenting technique. Whether it is practical in forecasting is another question
    entirely. It depends on the goal of the prediction and the base DL model or algorithm
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: The **standard scaler** is similar to the robust scaler.
  prefs: []
  type: TYPE_NORMAL
- en: Standard scaler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DL models that rely on Gaussian distributions or linear and logistic regressions
    will benefit from the standardization scaler augmentation method. Pluto’s `augment_tabular_standard_scaler()`
    wrapper function uses the DeltaPy library function and the joy and waffle plots.
    The essential code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The full function code can be found in the Python Notebook. The command is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – World Series and standard scaler joy plot](img/B17990_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – World Series and standard scaler joy plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.18* demonstrated that Pluto did the augmentation correctly. He
    did not build and train a DL model using the augmented data to confirm that it
    increased the forecast accuracy. Many tabular augmentation methods require defining
    the goal for the DL project to verify if the augmentation is beneficial. For example,
    Pluto could build a DL model for predicting the audience size for the next World
    Series.'
  prefs: []
  type: TYPE_NORMAL
- en: The next tabular transformation technique we’ll look at is **capping**.
  prefs: []
  type: TYPE_NORMAL
- en: Capping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The capping technique limits the distribution value, such as average, maximum,
    minimum, or arbitrary values. In particular, it restricts the values using statistical
    analysis and replaces the outliers with specific percentile values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto’s `augment_tabular_capping()` wrapper function uses the DeltaPy library
    function and correlogram plots. The essential code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for the Bank Fraud data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Bank Fraud capping correlogram plot, half data](img/B17990_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Bank Fraud capping correlogram plot, half data
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.19* indicates that Pluto implemented the capping technique correctly.
    Compared to *Figure 9**.4*, the original data, the values are similar, as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python implementation of tabular transformation wrapper functions becomes
    repetitive. Thus, Pluto will provide a brief explanation of the other nine methods
    in the DeltaPy library. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operations** is a technique for using power, log, or square root functions
    to replace elements in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoothing** is a technique that uses the triple exponential smoothing or
    Holt-Winters exponential smoothing function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decomposing** is a technique that uses the naive decomposition function for
    seasonal vectors in time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering** is a technique that uses the Baxter-King bandpass filter to smooth
    time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral analysis** is a technique that uses the periodogram function to
    estimate the spectral density'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Waveforms** is a technique that uses the continuous harmonic wave radar function
    to augment waveform data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rolling** is a technique that uses mean or standard deviation to calculate
    the rolling average over a fixed window size in time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lagging** is a technique that calculates the lagged values in time series
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forecast model** is a technique that uses the prophet algorithm to forecast
    seasonal trends, such as weekly or yearly, in time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to implement three wrapper functions in the Python Notebook
    from the nine tabular transformation techniques mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve reviewed various tabular transformation techniques, let’s look
    at **interaction** techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interaction techniques are used in ML and statistical modeling to capture the
    relationships between two or more features in a dataset for augmentation. The
    goal is to create new augmentation data that captures the interaction between
    existing components, which can help improve model performance and provide additional
    insights into the data. You can apply these techniques to cross-sectional or time-specific
    data, including normalizing, discretizing, and autoregression models.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto has selected two out of seven methods for a hands-on Python programming
    demonstration. As with the transformation augmentation methods, the coding is
    repetitive. Thus, Pluto will provide fun challenges for the other five interaction
    augmentation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto will start with the **regression** method, then the **operator** method.
  prefs: []
  type: TYPE_NORMAL
- en: Regression augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The regression method uses the **lowess smoother** function to smooth the curve
    of the data by locally weighting the observations near a given point. It is a
    useful tabular augmentation technique for exploring relationships in scatterplots
    where the relationship between the dependent and independent variables needs to
    be well-described by a linear function. This method can suffer from forward-looking
    bias. Thus, Pluto recommends caution in using it for predictive modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto’s `augment_tabular_regression()` wrapper function uses the DeltaPy library
    function, a joy plot, and a correlogram graph. The essential code snippet is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for the World Series data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – World Series regression augmentation, joy plot](img/B17990_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – World Series regression augmentation, joy plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figures 9.20* confirm that Pluto implemented the regression tabular augmentation
    correctly. The DeltaPy library does the actual calculation. Thus, if Pluto made
    a mistake, the result would be an error, or the dataset would contain random numbers
    and not display correctly. Pluto can only claim the effectiveness of the regression
    technique to the World Series data. The next tabular augmentation technique we’ll
    look at is the operator augmenting method.'
  prefs: []
  type: TYPE_NORMAL
- en: Operator augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The operator method is a simple multiplication or division function between
    two variables in tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto’s `augment_tabular_operator()` wrapper function uses the DeltaPy library
    function and a correlogram graph. The essential code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Pluto runs the command for the Bank Fraud data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Bank Fraud operator augmentation, correlogram plot](img/B17990_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – Bank Fraud operator augmentation, correlogram plot
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.21* shows a strong relationship between three new vectors: `credit_risk_score_X_proposed_credit_limit`
    (multiply), `proposed_credit_limit/credit_risk_score` (divide), and `proposed_credit_limit_X_credit_risk_score`
    (multiply). Pluto implements the operator function correctly but still determines
    the benefit of the DL prediction accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other five interaction tabular augmentation techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discretizing** is a method that uses **decision trees**, **equal width binding**,
    **equal frequency binding**, and **K-means clustering** to augment the tabular
    data. The discretization method depends on the AI model and the tabular data properties.
    Pluto recommends trying multiple approaches and evaluating their performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **quantile normalizing** method makes the distributions of the datasets
    comparable by transforming them so that they have the same cumulative distribution
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **haversine distance** calculates the shortest distance between two angular
    points, such as the Earth’s surface. Tabular augmentation also uses the **Euclidean**,
    **Mahalanobis**, and **Minkowski** distance algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **technical** indicator is one of the **specialty** methods in tabular augmentation.
    It uses technical analysis to help predict future price movements of securities
    or financial instruments. They are based on mathematical calculations of price,
    volume, and open interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **genetic** method, or **genetic** tabular augmentation, is a type of ML
    technique that uses evolutionary algorithms to optimize the AI model. The concept
    is to create a population of candidate solutions, or **chromosomes**, for a problem,
    then evolve that population over time by applying genetic operations such as crossover,
    mutation, and selection. The goal is to find the best solution to the problem
    through natural selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fun challenge
  prefs: []
  type: TYPE_NORMAL
- en: Pluto challenges you to implement two more interaction augmentations in the
    Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The next tabular augmentation class is **mapping** augmentation. Pluto will
    describe the mapping functions but not implement them in the Python Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mapping method uses ML and data analysis to summarize and reduce the dimensionality
    of data for augmentation. It can be done via unsupervised or supervised learning.
    Some examples of mapping methods include **eigendecomposition** and PCA. PCA is
    a statistical procedure that transforms a set of correlated variables into uncorrelated
    variables called principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the DeltaPy library, there are seven mapping methods for tabular augmentation.
    Pluto has done a few implementations in the Python Notebook, but he will not explain
    the coding here. The Python wrapper function is repetitive and can easily be applied
    to any mapping method. The functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigendecomposition** (**ED**) is a form of **PCA** for tabular augmentation.
    In ED, the **eigenvectors** are the covariance matrix of the data, and the corresponding
    **eigenvalues** represent the amount of variance by each component. ED includes
    **linear discriminant analysis** (**LDA**), **singular value decomposition** (**SVD**),
    and **Markov chains**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-decomposition** methods, including **canonical correlation analysis**
    (**CCA**), are used to uncover linear relationships between two pieces of multivariate
    tabular data. Various applications, such as dimensionality reduction, feature
    extraction, and feature selection, use the cross-decomposition method. The goal
    is to find a linear combination between tabular data variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel approximation** methods are used in ML algorithms such as SVMs to
    transform the tabular data into a higher dimensional space where a linear boundary
    can be found to separate the classes. The **additive Chi2 kernel** is a specific
    **kernel approximation** method that measures the independence between two sets
    of variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders** are used in various domains, such as image compression, anomaly
    detection, and for generating new data for tabular augmentation. We use autoencoders
    in the pre-training step for supervised learning tasks to improve the subsequent
    models’ performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manifold learning** is a class of techniques for non-linear dimensionality
    reduction, aiming to preserve the tabular data’s underlying non-linear structure.
    **Locally linear embedding** (**LLE**) is one method in which the idea is to approximate
    the local linear relationships between data points in the high-dimensional space.
    The goal is to find a lower-dimensional representation of high-dimensional data
    that still captures the essential patterns and relationships in the tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering** is a popular unsupervised ML technique for grouping similar
    tabular data points into clusters. Clustering methods help in identifying patterns
    and structure in tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neighbouring** is the nearest neighbor method for supervised ML algorithms
    for classification and regression problems. It also is used in tabular augmentation.
    You can extend the nearest neighbor method to a more sophisticated version called
    **k-nearest neighbor** (**k-NN**) classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next classification of tabular augmentation we’ll look at is **extraction**
    augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Extraction augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The extraction method is a process in time series analysis where multiple constructed
    elements are used as input, and a singular value is extracted from each time series
    to create new augmented data. This method uses a package called **TSfresh** and
    includes default and custom features. The output of extraction methods differs
    from the output of transformation and interaction methods, as the latter outputs
    entirely new time series data. You can use this method when specific values need
    to be pulled from time series data.
  prefs: []
  type: TYPE_NORMAL
- en: The DeltaPy library contains 34 extraction methods. Writing the wrapper functions
    for extraction is similar to the wrapper transformation functions. The difficulty
    is how to discern the forecasting’s effectiveness from tabular augmentation. Furthermore,
    these methods are components and not complete functions for tabular augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pluto will not explain each function, but here is a list of the extraction
    functions in the DeltaPy library: `Amplitude`, `Averages`, `Autocorrelation`,
    `Count`, `Crossings`, `Density`, `Differencing`, `Derivative`, `Distance`, `Distribution`,
    `Energy`, `Entropy`, `Exponent`, `Fixed Points`, `Fluctuation`, `Fractals`, `Information`,
    `Linearity`, `Location`, `Model` `Coefficients`, `Non-linearity`, `Occurrence`,
    `Peaks`, `Percentile`, `Probability`, `Quantile`, `Range`, `Shape`, `Size`, `Spectral`
    `Analysis`, `Stochasticity`, `Streaks`, `Structural`, `and` `Volatility`.'
  prefs: []
  type: TYPE_NORMAL
- en: The extraction method is the last tabular augmentation category. Thus, it is
    time for a summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tabular augmentation is a technique that can improve the accuracy of ML models
    by increasing the amount of data used. It adds columns or rows to a dataset generated
    by existing features or data from other sources. It increases the available input
    data, allowing the model to make more accurate predictions. Tabular augmentation
    adds new information not currently included in the dataset, increasing the model’s
    utility. Tabular augmentation is beneficial when used with other ML techniques,
    such as DL, to improve the accuracy and performance of predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: Pluto downloaded the real-world Bank Fraud and World Series datasets from the
    *Kaggle* website. He wrote most of the code in the Python Notebook for visualizing
    large datasets using various graphs, such as histograms, heatmaps, correlograms,
    and waffle and joy plots. He did this because understanding the datasets is essential
    before augmenting them. However, he didn’t write a CNN or RNN model to verify
    the augmentation methods because building a CNN model is a complex process worthy
    of a separate book.
  prefs: []
  type: TYPE_NORMAL
- en: The DeltaPy open source library contains dozens of methods for tabular augmentation,
    but it is a beta version and can be unstable to load. Still, Pluto demonstrated
    a few tabular augmentation techniques, such as the robust scaler, standard scaler,
    capping, regression, and operator methods.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, there were *fun facts* and *fun challenges*. Pluto
    hopes you will take advantage of these and expand your experience beyond this
    book’s scope.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last chapter of the book. You and Pluto have covered augmentation
    techniques for image, text, audio, and tabular data. As AI and generative AI continue
    to expand and integrate into the fabric of our life, data will play an essential
    role. Data augmentation methods are the best practical option to extend your datasets
    without the high cost of gathering and purchasing additional data. Furthermore,
    generative AI transforms how we work and play, such as OpenAI's GPT3, GPT4, Google
    Bard, and Stability.ai's Stable Diffusion. What you discussed about AI in boardrooms
    or classrooms last month will be outdated, but the data augmentation concepts
    and techniques remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: You and Pluto have learned to code the augmentation techniques using wrapper
    functions and download real-world datasets from the Kaggle website. As new, better,
    and faster augmentation libraries are available, you can add to your collection
    or switch the libraries under the hood. What you implement may change slightly,
    but what you have learned about data augmentation will remain true.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading the book and hacking the Python Notebook as much
    as I enjoyed writing it. Thank you.
  prefs: []
  type: TYPE_NORMAL
