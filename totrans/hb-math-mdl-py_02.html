<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer054">
<h1 class="chapter-number" id="_idParaDest-27"><a id="_idTextAnchor026"/>2</h1>
<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Machine Learning vis-à-vis Mathematical Modeling</h1>
<p>Having learned about the main components of mathematical optimization, which are decision variables, objective functions, and constraints, in the previous chapter, it is time to throw light on <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, most of which can be cast as mathematical models. Humans make machines learn from huge amounts of historical data. ML models enhance the decision-making abilities of man and machine, exploiting the power of data and algorithms. There is almost always some optimization algorithm working in the background of most of <span class="No-Break">these models.</span></p>
<p>The term ML was first popularized by Arthur L. Samuel in the 1950s, who was a pioneer in computer science and gaming. Data volume has increased by leaps and bounds since then, particularly in the last couple of decades, and making sense of huge amounts of data is beyond the scope of the human mind. Hence, ML stepped in and found its application in almost all domains to assist humans with the <span class="No-Break">decision-making process.</span></p>
<p>Learning problems in data science can be broadly classified into regression, classification, and clustering depending on the business problem or use case. Regression and classification use supervised algorithms to predict a target, usually called the dependent variable, the independent variables being called predictors. Clustering makes use of unsupervised learning algorithms where the target is unknown. It is worth mentioning that learning in all ML algorithms is not all about optimization, an example of which is supervised <a id="_idIndexMarker035"/>learning in <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">kNN</strong>). ML is a predominantly predictive tool helping a business plan for the future, thereby being beneficial for its bottom line. Businesses also leverage ML in anomaly (or outlier) detection and recommendation systems. Strictly mathematical modeling, on the other hand, helps businesses make decisions in areas such as electricity distribution, employee scheduling, and <span class="No-Break">inventory management.</span></p>
<p>Some well-known algorithms used in ML models that employ constrained optimization are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Principal component </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PCA</strong></span><span class="No-Break">)</span></li>
<li>Clustering with an expectation maximization algorithm (a Gaussian mixture model, <span class="No-Break">for example)</span></li>
<li>Support vector machines using the method of <span class="No-Break">Lagrange multipliers</span></li>
</ul>
<p>Other ML algorithms that employ unconstrained optimization are <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) in neural <a id="_idIndexMarker036"/>networks and batch gradient descent in deep learning (neural networks with numerous hidden layers between the input and output). Apart from these, there are genetic algorithms in evolutionary learning, which encompass both constrained and unconstrained <span class="No-Break">optimization problems.</span></p>
<p>The main components of ML are representation, evaluation, and optimization. By representation, we essentially mean putting forth the knowledge and historical data statistically to find patterns, in other words, the formulation of a business problem to arrive at or estimate the solution. Next is the evaluation of the formulation, which we call the model, and fitting our data into and comparing it with known examples or data samples. Finally, the algorithm behind the model optimizes its weights and biases for a better fit with the data, and the optimization process iterates until a desired accuracy for the problem is attained. We will learn about PCA and gradient descent in the <span class="No-Break">following chapters.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>ML as a mathematical <span class="No-Break">optimization problem</span></li>
<li>ML as a <span class="No-Break">predictive tool</span></li>
<li>Mathematical modeling as a <span class="No-Break">prescriptive tool</span></li>
</ul>
<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>ML as mathematical optimization</h1>
<p>ML can be described as <a id="_idIndexMarker037"/>finding the unknown underlying (approximate) function that maps input examples to output examples. This<a id="_idIndexMarker038"/> is where the ML algorithm defines a parametrized mapping function and optimizes or minimizes the error in the function to find the values of its parameters. ML is function approximation along with function optimization. The function parameters are also called model coefficients. Each <a id="_idIndexMarker039"/>time we fit a model to a training dataset, we solve an <span class="No-Break">optimization problem.</span></p>
<p>Each ML algorithm makes different assumptions about the form of the mapping function, which in turn influences the type of optimization to be performed. ML is a function approximation method to optimally fit input data. It is particularly challenging when the data (the size or the number of examples) is limited. An ML algorithm must be chosen in a way that it most efficiently solves an optimization problem; for example, SGD is used for neural nets, while ordinary least squares and gradient descent are used for linear regression. When we deviate from the default algorithms, we need a good reason to do so. In mathematical optimization, a heuristic might sometimes be used to determine near-optimal solutions. This happens when the classical algorithms are too slow to even find <a id="_idIndexMarker040"/>an approximate solution or they fail to find an exact solution to the optimization problem. Examples of heuristics are a genetic algorithm and a simulated <span class="No-Break">annealing algorithm.</span></p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Example 1 – regression</h2>
<p>An ML problem is <a id="_idIndexMarker041"/>framed as the learning of a mapping function (f) given input data (X) and output data (Y) such that Y = f(X). Given new input data, we should be able to map each datum onto (or predict) the output with our learned function, f. A prediction error is expected in general with noise in observed data and<a id="_idIndexMarker042"/> with a choice of learning algorithm that approximates the mapping function. Finding the set of inputs that results in the minimum error, cost, or loss is essentially solving the optimization problem. The choice of mapping function dictates the level of difficulty of optimization. The more biased or constrained the choice, the easier <span class="No-Break">it is.</span></p>
<p>For example, linear regression is <a id="_idIndexMarker043"/>a constrained model. Using linear algebra, it can be solved analytically. The inputs to the mapping function in this case are the model coefficients. An optimization algorithm such as iterative local search can be used numerically but it is almost always less efficient than an analytical solution. A logistic regression (for a classification task) is a less constrained model, and an optimization algorithm is required in this case. The loss or error here is also called the logistic loss <a id="_idIndexMarker044"/>or cross-entropy. While<a id="_idIndexMarker045"/> a global search optimization algorithm can be used in both types of regression models, it is mostly less efficient than using either an analytical method or a local search method. An iterative global search (gradient descent, for example) is suitable when the search space or landscape is multimodal and nonlinear, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 2.1: 3D landscape of unconstrained optimization space, where A is the current state" height="648" src="image/Figure_02_01_B18943.jpg" width="877"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: 3D landscape of unconstrained optimization space, where A is the current state</p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Example 2 – neural network</h2>
<p>A neural network is <a id="_idIndexMarker046"/>a flexible model and imposes<a id="_idIndexMarker047"/> very few constraints. A network typically has an input layer, a hidden layer (can be more than one), and an output layer of nodes, and the inputs to the mapping function are weighted to the input layer, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em>. It is this mapping function that the supervised learning algorithm tries to <span class="No-Break">best approximate.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 2.2: The three essential, minimal layers in a network" height="710" src="image/Figure_02_02_B18943.jpg" width="1158"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: The three essential, minimal layers in a network</p>
<p>The deviation of predicted output from expected output is the error value, and this error or cost, shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>, is minimized while approximating the function during model training. A <a id="_idIndexMarker048"/>neural network requires an iterative global search algorithm. Gradient descent is the preferred method to<a id="_idIndexMarker049"/> optimize a neural network that has variants, namely, batch and<a id="_idIndexMarker050"/> mini-batch gradient descent and SGD. One of the most popular SGD algorithms is <strong class="bold">Adaptive Moment Estimation</strong> (<strong class="bold">Adam</strong>), which computes adaptive learning rates for each parameter of <span class="No-Break">the function.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 2.3: Minimization of cost function J(w) by gradient descent where w is the input (courtesy of Python Machine Learning by Sebastian Raschka)" height="398" src="image/Figure_02_03_B18943.jpg" width="735"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3: Minimization of cost function J(w) by gradient descent where w is the input (courtesy of Python Machine Learning by Sebastian Raschka)</p>
<p>A gradient is a vector of partial derivatives (slope/curvature) of the function with respect to input variable values. The gradient descent algorithm, as the name suggests, requires the calculation of this gradient. The negative of the gradient of each input is followed downhill as the gradient points uphill, to lead to new values of the input. A step size is used to scale the<a id="_idIndexMarker051"/> gradient and control the change of input with respect to the gradient. This step size or increment is the learning rate, a hyper-parameter of the algorithm, and is the<a id="_idIndexMarker052"/> proportion in which network weights are updated. The process is repeated until the minimum of the function is located. Gradient descent is adapted to minimize the loss function of a predictive model, such as regression or classification. This adaptation results in SGD, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 2.4: Gradient descent extension" height="706" src="image/Figure_02_04_B18943.jpg" width="1499"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4: Gradient descent extension</p>
<p>SGD is the extension of the gradient descent optimization algorithm, wherein the target function is considered to be the loss or error, such as mean squared error for regression and cross-entropy for classification. Since the gradients of the target function with respect to the inputs are noisy, and deterministic to the extent of probabilistic approximation only, the<a id="_idIndexMarker053"/> algorithm is referred to as “stochastic.” Due to the sparseness and noise in training data, the evaluated gradients have statistical noise. Generally speaking, SGD and its variants are still the most used optimization algorithms for ML as well as training deep learning (artificial neural network) models. The inputs to a neural network are the weights (model parameters) and the target function is the prediction error averaged over one batch, which is a subset of the <span class="No-Break">training dataset.</span></p>
<p>A popular extension to SGD for the improvement of process efficiency, such as finding out the same (or better) loss in fewer iterations, is Adam. The Adam optimization method is computationally efficient, requires little memory, and is well suited for problems that are large in terms of size and features. The configuration parameters of Adam are the<a id="_idIndexMarker054"/> learning rate (step size), exponential decay rate (denoted by beta 1) for the mean (first moment) estimates, exponential <a id="_idIndexMarker055"/>decay rate (denoted by beta 2) for variance (second moment) estimates, and epsilon (very small number) to prevent any division by zero in the implementation. Larger values of learning rate (denoted by alpha) result in faster initial learning before an update and lower values of learning rate mean slower learning during the entire training. These parameters typically require very little tuning as they have <span class="No-Break">intuitive interpretation.</span></p>
<p>A major challenge in using SGD to train a multi-layer neural network is the gradient calculation for nodes in the hidden layer(s) of the network. It can be tackled by utilizing a specific technique from calculus called the chain rule, and an efficient algorithm that implements this rule is called backpropagation, which<a id="_idIndexMarker056"/> calculates the gradient of a loss function concerning the model variables. The first-order derivative of a function for a specific input variable value is the rate of change of the function with that variable, and when there are multiple input variables, the (partial) derivatives form a vector. For each weight in the network, backpropagation calculates the gradient, which is then used by the SGD optimization algorithm to update the weights. Backpropagation works backward from the output toward the input of the network, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.5</em>. It propagates the error in the predicted output to compute the gradient for each input variable, basically a backward flow of information from the cost function through the network. Backpropagation involves the recursive application of the chain rule, which is the calculation of the derivative of a sub-function given the known derivative of the <span class="No-Break">parent function.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 2.5: Backpropagation in a neural network" height="890" src="image/Figure_02_05_B18943.jpg" width="1515"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5: Backpropagation in a neural network</p>
<p>A genetic algorithm does not utilize the structure of the model, meaning it does not require gradients. For <a id="_idIndexMarker057"/>problems in which we use neural network models, we need to optimize the model using gradients that are calculated with backpropagation. It is only fair to say that <a id="_idIndexMarker058"/>backpropagation is a part of the optimization process, the optimization algorithm <span class="No-Break">being SGD.</span></p>
<p>Now that we have explored ML tasks such as regression, classification, and neural nets in the form of mathematical optimization problems, we shall learn about ML as a predictive modeling tool and how it is utilized in a few <span class="No-Break">important domains.</span></p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>ML – a predictive tool</h1>
<p>Working through a <a id="_idIndexMarker059"/>predictive model involves optimization at multiple<a id="_idIndexMarker060"/> steps on top of optimally fitting the learning algorithm to the data. It involves transforming raw data into a form most appropriate for consumption in learning algorithms. An ML model has hyperparameters that can be configured to tailor it to a specific dataset. It is a standard practice to test a suite of hyper-parameters for a chosen ML algorithm, which is called hyper-parameter tuning or optimization. A <a id="_idIndexMarker061"/>grid search or random search algorithm is used for such tuning. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em> shows the two search algorithm types. Grid search is more suitable for a quick search of hyperparameters and is known to perform well in general. You can also use Bayesian optimization for hyper-parameter tuning in some problems. We will learn <a id="_idIndexMarker062"/>about these optimization techniques in detail in the last part of <span class="No-Break">the book.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="Figure 2.6: Grid search (L) versus random search (R)" height="231" src="image/Figure_02_06_B18943.jpg" width="491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6: Grid search (L) versus random search (R)</p>
<p>An ML practitioner often performs a manual process for predictive model selection involving tasks such as data preparation, evaluating models, tuning them, and finally, choosing the best model for a given dataset. This can be framed as an optimization problem that can be<a id="_idIndexMarker063"/> solved with <strong class="bold">automated machine learning</strong> (<strong class="bold">AutoML</strong>) with little user intervention. The automated optimization approach to ML is also offered as a cloud product service by companies such as Google <span class="No-Break">and Microsoft.</span></p>
<p>With or without a target variable in the input dataset, an ML algorithm becomes supervised or unsupervised learning, respectively. In reinforcement learning, certain behaviors are encouraged and others discouraged. The desired behavior is reinforced by rewards, which are gained through experiences from the environment. These three types of ML are shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="Figure 2.7: The three kinds of ML – supervised learning, unsupervised learning, and reinforcement learning" height="1000" src="image/Figure_02_07_B18943.jpg" width="1398"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7: The three kinds of ML – supervised learning, unsupervised learning, and reinforcement learning</p>
<p>We will now talk <a id="_idIndexMarker064"/>about a few major domains where the ML model has safely secured its place as a <span class="No-Break">predictive tool.</span></p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>E-commerce</h2>
<p>ML models help retailers <a id="_idIndexMarker065"/>understand the buying behavior of customers and their preferences. From historical purchase patterns of customers and click-through rates of products, e-commerce companies effectively recommend products and offer to maximize their sales. Personalized recommendations help retailers retain their customer base, thus creating loyalty. The following link outlines the particular ways ML can be utilized in the <span class="No-Break">e-commerce industry:</span></p>
<p><a href="https://blog.shift4shop.com/machine-learning-ecommerce-industry"><span class="No-Break">https://blog.shift4shop.com/machine-learning-ecommerce-industry</span></a></p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Sales and marketing</h2>
<p>ML models are <a id="_idIndexMarker066"/>used in B2B marketing as well. Identifying and acquiring prospects with features similar to existing businesses is one use case of customer segmentation. Prioritizing known prospects and generating new leads based on the likelihood of customers taking action is achieved using lead-scoring algorithms. Companies can streamline their sales and marketing activities by being data-driven as well as algorithm-driven. Here are some ways sales and marketing have<a id="_idIndexMarker067"/> improved when driven <span class="No-Break">by ML:</span></p>
<p><a href="https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and"><span class="No-Break">https://scinapse.ai/blog/11-ways-machine-learning-can-improve-marketing-and</span></a></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Cybersecurity</h2>
<p>Cyber-attacks may <a id="_idIndexMarker068"/>strike an organization at any time and cause serious harm; however, they can be predicted and prevented by ML algorithms. From processing both structured and unstructured data in a short time, real-time traffic can be analyzed to track unusual or anomalous patterns. Companies keep attacks at bay by analyzing these outlying points in the data. This also reduces the scope of human error stemming from the manual processing of massive volumes of data and enables humans to focus on strategizing the protection of the system from cyber-attacks. The following data-driven methods pointed out by Kaspersky are <span class="No-Break">worth studying:</span></p>
<p><a href="https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity"><span class="No-Break">https://www.kaspersky.com/enterprise-security/wiki-section/products/machine-learning-in-cybersecurity</span></a></p>
<p>Having explored how ML works as a predictive modeling tool in the industry, we will learn in the next section how mathematical modeling can be used as a prescriptive tool in <span class="No-Break">different sectors.</span></p>
<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Mathematical modeling – a prescriptive tool</h1>
<p>Businesses often make complex decisions about their course of action to achieve objectives with the help of mathematical modeling or heuristics. A mathematical model in this sense is a prescriptive analytical tool. Answering the “where” and “when” is as important as <a id="_idIndexMarker069"/>answering what happened in the past (descriptive analytics) and what could happen in the future (predictive analytics). If a business wants to drive decisions from data in addition to insights and future predictions, it has to use both predictive and prescriptive tools in an <span class="No-Break">integrated fashion.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 2.8: Mathematical optimization or mathematical modeling" height="482" src="image/Figure_02_08_B18943.jpg" width="1151"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8: Mathematical optimization or mathematical modeling</p>
<p>We will have a<a id="_idIndexMarker070"/> look at examples from industry verticals wherein these work in tandem, resulting in higher productivity <span class="No-Break">and profitability.</span></p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Finance</h2>
<p>Financial services, including banks, rely on ML models as well as mathematical models to determine the<a id="_idIndexMarker071"/> right allocation of their investment portfolios. An ML model in the form of time-series forecasting helps with the prediction of asset performance, which in turn is channelled into applications leveraging a mathematical model. Based on the market movements and forecasts, the mathematical optimization application determines the optimal allocation. The best portfolio allocation also takes individual investment objectives and preferences into account. These mitigate risks and maximize risk-adjusted returns <span class="No-Break">on investments.</span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Retail</h2>
<p>Leading retailers utilize ML models to forecast demand for products, especially high-selling ones in<a id="_idIndexMarker072"/> particular locations at given times. They feed these predictions into mathematical models to maximize profits and customer satisfaction. The mathematical optimization application, in this case, uses the forecast as input to generate optimal production, pricing, inventory and distribution planning, logistics, and warehousing, thereby making the best business decisions while minimizing operating costs. Supply chain management is a classic example of <span class="No-Break">mathematical optimization.</span></p>
<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Energy</h2>
<p>Governments and industry players are making high-stakes decisions on strategic investments in network infrastructure and resources as electric power is making a transition from being <a id="_idIndexMarker073"/>dependent on fossil fuels to renewables such as solar and wind. Organizations are utilizing ML models to predict future power demand and capacity needs. These forecasts are fed into mathematical models or mathematical optimization applications that generate optimal long-term investment planning and help in making decisions about <span class="No-Break">strategic investments.</span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Digital advertising</h2>
<p>Search engine giants such as Google leverage ML (and deep learning) models to predict the products and services individuals will be interested in looking up, based on their prior search history<a id="_idIndexMarker074"/> and a few other factors. In addition, they utilize mathematical models to figure out the online advertisements that can be shown to individual users at certain times. Search engine giants use this optimization model to charge advertisers and maximize <span class="No-Break">their revenue.</span></p>
<p>These domains have added mathematical modeling to their data science toolbox that handles complex, significant, and scalable business problems for greater value delivery. Other industries, such as telecommunications and cloud computing, also use both models to precisely assess long-term demand and capacity needs to make the best <span class="No-Break">business decisions.</span></p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Summary</h1>
<p>In this chapter, we introduced ML models as problems of mathematical optimization or mathematical programming. We found out that an end-to-end ML project is the sum of multiple small optimization problems. We also gained knowledge about how businesses can unlock the true value of data upon leveraging mathematical models (primarily driven by mathematical equations) in addition to ML (driven by data) models. We learned that an ML model is predominantly a predictive tool and a mathematical model is a <span class="No-Break">prescriptive one.</span></p>
<p>In the next chapter (which begins the next part of the book), we will take a meticulous look at a well-known algorithm called PCA, utilized in an unsupervised ML model fit to data with high dimensionality. It is a dimensionality reduction technique and one of the most tried and tested mathematical tools employing <span class="No-Break">constrained optimization.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer055">
<h1 id="_idParaDest-42" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor041"/>Part 2:Mathematical Tools</h1>
<p>In this part, you will learn some of the most tried and tested mathematical tools and algorithms. On the one hand, there are algorithms for data dimensionality reduction, optimization of machine learning models, and data classification, which are explored through Python code. On the other hand, there are algorithms that model the relationships between objects (data points) and estimate the current and future states of variables (unknown and immeasurable ones) of a dynamic system. There are also other algorithms that predict the next future state probabilistically from knowledge of the present state of a process, explained with simple examples and <span class="No-Break">Python code.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18943_03.xhtml#_idTextAnchor042"><em class="italic">Chapter 3</em></a>, <em class="italic">Principal Component Analysis</em></li>
<li><a href="B18943_04.xhtml#_idTextAnchor053"><em class="italic">Chapter 4</em></a>, <em class="italic">Gradient Descent</em></li>
<li><a href="B18943_05.xhtml#_idTextAnchor064"><em class="italic">Chapter 5</em></a>, <em class="italic">Support Vector Machine</em></li>
<li><a href="B18943_06.xhtml#_idTextAnchor070"><em class="italic">Chapter 6</em></a>, <em class="italic">Graph Theory</em></li>
<li><a href="B18943_07.xhtml#_idTextAnchor081"><em class="italic">Chapter 7</em></a>, <em class="italic">Kalman Filter</em></li>
<li><a href="B18943_08.xhtml#_idTextAnchor087"><em class="italic">Chapter 8</em></a>, <em class="italic">Markov Chain</em> </li>
</ul>
</div>
<div>
<div id="_idContainer056">
</div>
</div>
<div>
<div id="_idContainer057">
</div>
</div>
<div>
<div id="_idContainer058">
</div>
</div>
<div>
<div id="_idContainer059">
</div>
</div>
<div>
<div id="_idContainer060">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer061">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer062">
</div>
</div>
<div>
<div id="_idContainer063">
</div>
</div>
<div>
<div id="_idContainer064">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer065">
</div>
</div>
</div></body></html>