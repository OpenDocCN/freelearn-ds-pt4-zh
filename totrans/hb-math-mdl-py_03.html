<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer081">
<h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor042"/>3</h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Principal Component Analysis</h1>
<p>A well-known algorithm to extract features from high-dimensional data for consumption in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models is <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>). In mathematical terms, <em class="italic">dimension</em> is the<a id="_idIndexMarker075"/> minimum number of coordinates required to specify a vector in space. A lot of computational power is needed to find the distance between two vectors in high-dimensional space and in such cases, dimension is considered a curse. An increase in dimension will result in high performance of the algorithm only to a certain extent and will drop beyond that. This<a id="_idIndexMarker076"/> is the curse of dimensionality, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, which impedes the achievement of efficiency for most ML algorithms. The variable columns or features in data represent dimensions of space and the rows represent the coordinates in that space. With the increasing dimension of data, sparsity increases and there is an exponentially increasing computational effort required to calculate distance and density. Theoretically speaking, an increase in dimension practically increases noise and redundancy in large datasets. Arguably, PCA is the most popular technique to tackle this complexity of dimensionality in <span class="No-Break">high-dimensional problems.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Figure 3.1: Curse of dimensionality" height="329" src="image/Figure_03_01_B18943.jpg" width="679"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: Curse of dimensionality</p>
<p>PCA comes from the field of linear algebra and is essentially a data preparation method that projects the data in a subspace before fitting the ML model to the newly created low-dimensional dataset. PCA is a data projection technique useful in visualizing high-dimensional data and improving data classification. It was invented in the 1900s following the principal axis theorem. The main objectives of PCA are to find an orthonormal basis for the data, sort dimensions in the order of importance or variance, discard dimensions of low importance, and focus only on uncorrelated <span class="No-Break">Gaussian components.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>Linear algebra <span class="No-Break">for PCA</span></li>
<li>Linear discriminant analysis – the difference <span class="No-Break">from PCA</span></li>
<li>Applications <span class="No-Break">of PCA</span></li>
</ul>
<p>The following section talks about linear algebra, the subject of mathematics on which PCA <span class="No-Break">is based.</span></p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Linear algebra for PCA</h1>
<p>PCA is an <a id="_idIndexMarker077"/>unsupervised method used to reduce the number of features of a high-dimensional dataset. An unlabeled dataset is reduced into its constituent parts by matrix factorization (or decomposition) followed by<a id="_idIndexMarker078"/> ranking of these parts in accordance with variances. The projected data representative of the original data becomes the input to train <span class="No-Break">ML models.</span></p>
<p>PCA is defined as the orthogonal projection of data onto a lower dimensional linear space called the principal <a id="_idIndexMarker079"/>subspace, done by finding new axes (or basis vectors) that preserve the maximum variance of projected data; the new axes or vectors are known as principal components. PCA preserves the information by considering the variance of projection vectors: the highest variance lies on the first axis, the second highest on the second axis, and so forth. The working principle of the linear transformation called PCA is shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em>. It compresses the feature space by identifying a subspace that captures the essence of the complete <span class="No-Break">feature matrix.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<img alt="Figure 3.2: PCA working principle" height="507" src="image/Figure_03_02_B18943.jpg" width="538"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: PCA working principle</p>
<p>There are other <a id="_idIndexMarker080"/>approaches to reducing data dimensionality such as feature selection methods (wrapper and filter), non-linear methods such as manifold learning (t-SNE), and deep learning (autoencoders) networks; however, the<a id="_idIndexMarker081"/> widest and most popular exploratory approach is PCA. Typically, linear algebraic methods assume that all inputs have the same distribution, and hence, it is a good practice to (either normalize or standardize) scale data before using PCA if the input features have <span class="No-Break">different units.</span></p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Covariance matrix – eigenvalues and eigenvectors</h2>
<p>The constraint in PCA is all the principal axes should be mutually orthogonal. The covariance of data is a measure of how much any pair of features in the data vary from each other. A covariance matrix<a id="_idIndexMarker082"/> checks the correlations between features in data and the directions of these relationships are obtained depending on whether the covariance is less than, equal to, or greater than zero. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> displays the covariance matrix formula. Each element in the matrix represents the correlation between two features in the data where <em class="italic">j </em>and<em class="italic"> k </em>run over<em class="italic"> p</em> variates, <em class="italic">N</em> is the number of observations (rows), and the <img alt="" height="24" src="image/Formula_03_001.png" width="19"/> bar and <img alt="" height="21" src="image/Formula_03_002.png" width="23"/> bar in the formula denote the expected <span class="No-Break">values (averages).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 3.3: Covariance matrix" height="177" src="image/Formula_03_003.jpg" width="756"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: Covariance matrix</p>
<p>The objective of PCA is to explain <a id="_idIndexMarker083"/>most of the data variability with far fewer variables or features than that in the original dataset. Each of the <em class="italic">N</em> observations or records resides in <em class="italic">p</em>-dimensional space. Not all the dimensions are equally relevant. PCA seeks a smaller number of important dimensions, where importance is quantified by the amount of variation of the observations along each dimension. The dimensions figured out by PCA are a linear combination of the p features. We can take the linear combinations and reduce the number of plots required for visual analysis of the feature space while retaining the essence of the <span class="No-Break">original data.</span></p>
<p>The eigenvalues and <a id="_idIndexMarker084"/>eigenvectors of a covariance matrix computed by eigendecomposition determine the magnitude and direction of the new subspace, respectively. In linear <a id="_idIndexMarker085"/>algebra, an <strong class="bold">eigenvector</strong> (associated with a set of linear equations) of a linear transformation is a non-zero vector that changes by a magnitude (scalar) when the transformation is applied to it. The corresponding <strong class="bold">eigenvalue</strong> is the magnitude or factor by which the eigenvector is scaled, and <strong class="bold">eigendecomposition</strong> is the <a id="_idIndexMarker086"/>factorization of a matrix into its eigenvectors and eigenvalues. The principal components are the eigenvectors. The top eigenvalues of a covariance matrix after sorting in descending order yield the principal components of <span class="No-Break">the dataset.</span></p>
<p>The first <strong class="bold">principal component</strong> (<strong class="bold">PC</strong>) of data is the linear combination of the features that have the<a id="_idIndexMarker087"/> highest variance. The second PC is the linear combination of the features that have maximum variance out of all linear combinations uncorrelated with the first PC. The first two PCs are shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em>, and this computational process proceeds until all the PCs of the dataset are found. These PCs are essentially the eigenvectors, and linear algebraic techniques show that the eigenvector corresponding to the highest eigenvalue of the covariance matrix explains the greatest proportion of data variability. Each PC vector defines a direction in feature space and all of them are uncorrelated – that is, mutually orthogonal. The PCs form the basis of the <span class="No-Break">new space.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 3.4: Principal components in feature space (x = ￼, y = ￼)" height="283" src="image/Figure_03_03_B18943.jpg" width="328"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4: Principal components in feature space (x = <img alt="" height="17" src="image/Formula_03_004.png" width="22"/>, y = <img alt="" height="20" src="image/Formula_03_005.png" width="22"/>)</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Number of PCs – how to select for a dataset</h2>
<p>The question is how to determine how well a dataset is explained by a certain number of PCs. The answer lies in<a id="_idIndexMarker088"/> the percentage of variance retained by the number of PCs. We would ideally like to have the smallest number of PCs possible explaining most of the variability. There is no robust method to determine the number of usable components. As the number of observations and variables in the dataset vary, different levels of accuracy and amounts of reduction <span class="No-Break">are desirable.</span></p>
<p>The <strong class="bold">proportion of variance explained</strong> (<strong class="bold">PVE</strong>) by the <em class="italic">m</em>th PC is computed by considering the <em class="italic">m</em>th eigenvalue. PVE is<a id="_idIndexMarker089"/> the ratio of the mth eigenvalue represented by <img alt="" height="30" src="image/Formula_03_006.png" width="38"/> (of <em class="italic">j</em>th variate in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>) and the sum of the eigenvalues of all the PCs or eigenvectors. To put it simply, PVE is the variance explained by each PC divided by the total variance of all PCs in <span class="No-Break">the dataset.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 3.5: PVE calculation" height="167" src="image/Formula_03_007.jpg" width="581"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5: PVE calculation</p>
<p>In general, we look for the “elbow point” where the PVE significantly drops off to determine the number of <span class="No-Break">usable PCs.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<img alt="Figure 3.6a: PVE versus PC" height="387" src="image/Figure_03_04_B18943.jpg" width="501"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6a: PVE versus PC</p>
<p>The first PC in the <a id="_idIndexMarker090"/>example shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6a</em> explains 62.5% of data variability, and the second PC explains 25%. In a cumulative manner, the first two PCs explain 87.5% of the variability, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.6b</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 3.6b: Cumulative PVE (y axis) versus PC" height="393" src="image/Figure_03_05_B18943.jpg" width="507"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6b: Cumulative PVE (y axis) versus PC</p>
<p>Another widely used matrix decomposition method to identify the number of PCs is <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>). It is a<a id="_idIndexMarker091"/> reduced-rank approximation method that provides a simple means to do the same – that is, compute the principal components corresponding to the singular values. SVD is just another way to factorize a matrix and allows us to unravel similar information as eigendecomposition does. The singular values of the data matrix obtained via SVD are essentially the square roots of the eigenvalues of the covariance matrix in PCA. SVD is<a id="_idIndexMarker092"/> the same as PCA of a raw data matrix, which <span class="No-Break">is mean-centered.</span></p>
<p>A mathematical demonstration of SVD can be found on the Wolfram <span class="No-Break">pages: </span><a href="https://mathworld.wolfram.com/SingularValueDecomposition.xhtml"><span class="No-Break">https://mathworld.wolfram.com/SingularValueDecomposition.xhtml</span></a><span class="No-Break">.</span></p>
<p>SVD is an iterative numerical method. Every rectangular matrix has SVD, although, for a few complex problems, it may fail to decompose some matrices neatly. You can perform SVD using the linear algebra class of the Python library, <strong class="source-inline">scipy</strong>. The <strong class="source-inline">scikit-learn</strong> library provides functions for <span class="No-Break">SVD: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml</span></a><span class="No-Break">.</span></p>
<p>High-dimensional data can be reduced to a subset of dimensions (columns) that are most relevant to the problem being solved. The data matrix (rows by columns) results in a matrix with a lower rank that approximates the original matrix and best captures its <span class="No-Break">salient features.</span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Feature extraction methods</h2>
<p>There are two ways in which dimensionality reduction can be done: one is feature selection and the other is feature extraction. A subset of original features is selected in the former <a id="_idIndexMarker093"/>approach by filtering based on some criteria true to the particular use case and corresponding data. On the other hand, a set of new features is found in the feature <span class="No-Break">extraction approach.</span></p>
<p>Feature extraction is<a id="_idIndexMarker094"/> done using linear mapping from the original features, which no longer exist upon implementation of the method. In essence, new features constructed from available data do not have column names as in the original data. There are two feature extraction methods: PCA and LDA. A nonlinear mapping may also be used depending on the data but the method no longer remains PCA <span class="No-Break">or LDA.</span></p>
<p>Now that we have explored PCA for the reduction of features (and hence, the reduction of the high-dimensional space), we <a id="_idIndexMarker095"/>shall learn about a supervised method of linear feature extraction called <strong class="bold">linear discriminant </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LDA</strong></span><span class="No-Break">).</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>LDA – the difference from PCA</h1>
<p>LDA and PCA are linear <a id="_idIndexMarker096"/>transformation methods; the latter yields directions or PCs that maximize data variance and the former yields directions that maximize the separation between data classes. The way in which the PCA algorithm works disregards <span class="No-Break">class labels.</span></p>
<p>LDA is a supervised method to reduce dimensionality that projects the data onto a subspace in a way that maximizes the separability between (groups) classes; hence, it is used for pattern classification problems. LDA works well for data with multiple classes; however, it makes assumptions of normally distributed classes and equal class covariances. PCA tends to work well if the number of samples in each class is relatively small. In both cases, though, observations ought to be much higher relative to the dimensions for <span class="No-Break">meaningful results.</span></p>
<p>LDA seeks a projection that discriminates data in the best possible way, unlike PCA, which seeks a projection that preserves maximum information in the data. When regularization of the estimate of covariance is introduced to moderate the influence of different variables on<a id="_idIndexMarker097"/> LDA, it is called <strong class="bold">regularized </strong><span class="No-Break"><strong class="bold">discriminant analysis</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 3.7: Linear discriminants" height="297" src="image/Figure_03_06_B18943.jpg" width="508"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7: Linear discriminants</p>
<p>LDA involves developing a probabilistic model per class based on the distribution of each input variable (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em>). It may be considered as an application of the Bayes' theorem for classification and assumes that the input variables are uncorrelated; if they are correlated, the PCA transform may aid in removing the linear dependence. The scikit-learn library provides functions for LDA. Example code with a synthetic dataset is <span class="No-Break">given</span><span class="No-Break"><a id="_idIndexMarker098"/></span><span class="No-Break"> here:</span></p>
<pre class="source-code">
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
X, y = make_classification(n_samples = 1000, n_features = 8, n_informative = 8,
n_redundant = 0, random_state = 1) #train examples and labels
model = LinearDiscriminantAnalysis()
cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
grid = dict()
grid['solver'] = ['svd', 'lsqr', 'eigen'] #grid configuration
search = GridSearchCV(model, grid, scoring = 'accuracy', cv = cv, n_jobs = -1)
results = search.fit(X, y)
print('Mean Accuracy: %.4f' % results.best_score_) #model accuracy check
row = [0.1277, -3.6440, -2.2326, 1.8211, 1.7546, 0.1243, 1.0339, 2.3582] #new example
yhat = search.predict([row]) #predict on test data
print('Predicted Class: %d' % yhat) #class probability of new example</pre>
<p>In the preceding example, the hyperparameter (<strong class="source-inline">solver</strong>) in the grid search is set to <strong class="source-inline">'svd'</strong> (default) but other <strong class="source-inline">solver</strong> values can also be used. This example only introduces us to using LDA with scikit-learn; there is a whole lot of customization that can be done<a id="_idIndexMarker099"/> depending on the problem <span class="No-Break">being solved.</span></p>
<p>We have explored the linear algebraic methods for dimensionality reduction; we shall learn about the most important applications of PCA in the <span class="No-Break">next section.</span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Applications of PCA</h1>
<p>PCA is one fundamental algorithm and forms the foundation of ML. It finds use in diverse areas such as<a id="_idIndexMarker100"/> noise reduction in images, classification of data in general, anomaly detection, and other applications in medical data correlation. We will explore a couple of widely used applications of PCA in <span class="No-Break">this section.</span></p>
<p>The scikit-learn library in Python provides functions for PCA. The following example code shows how to leverage PCA for dimensionality reduction while developing a predictive model that uses a PCA projection as input. We will be using PCA on a synthetic dataset while fitting a logistic regression model <span class="No-Break">for classification:</span></p>
<pre class="source-code">
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from numpy import mean
from numpy import std
import matplotlib.pyplot as plt
def get_models():
     models = dict()
     for i in range(1, 11):
           steps = [('pca', PCA(n_components = i)), ('m', LogisticRegression())]
models[i] = Pipeline(steps = steps)
return models
def evaluate_model(model, X, y):
     cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
scores = cross_val_score(model, X,  y, scoring = 'accuracy', cv = cv, n_jobs = -1,  error_score = 'raise')
return scores
X, y = make_classification(n_samples = 1000, n_features = 10, n_informative = 8, n_redundant = 2, random_state = 7)
models = get_models()
results, names = list(), list()
for name, model in models.items():
     scores = evaluate_model(model, X, y)
     results.append(scores)
     names.append(name)
print('Mean Accuracy: %.4f (%.4f)' % (mean(results), std(results))) red_square = dict(markerfacecolor = 'r', marker = 's')</pre>
<p>We will plot the principal components using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
plt.boxplot(results, labels = names, showmeans = True, showfliers = True, flierprops = red_square)
plt.grid()
plt.xlabel('Principal Components')
plt.xticks(rotation = 45)
plt.show()
row = [0.1277, -3.6440, -2.2326, 1.8211, 1.7546, 0.1243, 1.0339, 2.3582, -2.8264,0.4491] #new example
steps = [('pca', PCA(n_components = 8)), ('m', LogisticRegression())]
model = Pipeline(steps = steps)
model.fit(X, y)
yhat = model.predict([row]) #predict on test data
print('Predicted Class: %d' % yhat) #predicted class of new example</pre>
<p>In the preceding <a id="_idIndexMarker101"/>example, we do not see improvement in the model accuracy beyond eight components (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em>). It is evident that the first eight components contain maximum information about the class and the remaining ones <span class="No-Break">are redundant.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 3.8: Classification accuracy versus the number of PCs for a synthetic dataset" height="533" src="image/Figure_03_07_B18943.jpg" width="745"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8: Classification accuracy versus the number of PCs for a synthetic dataset</p>
<p>The number of components after a PCA transform of features that results in the best average performance of the model is chosen and fed to the ML model for predictions. In the following<a id="_idIndexMarker102"/> subsection, we will learn about denoising and the detection of outliers <span class="No-Break">using PCA.</span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Noise reduction</h2>
<p>PCA finds use in the<a id="_idIndexMarker103"/> reduction of noise in data, especially images. PCA<a id="_idIndexMarker104"/> reconstruction of an image by denoising can be achieved with the decomposition method of the scikit-learn Python library. Details of the library function with examples can be found <span class="No-Break">here: </span><a href="https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml"><span class="No-Break">https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml</span></a></p>
<p>A good exercise would be to reconstruct images obtained from a video sequence exploring linear PCA as well as kernel PCA and check which one provides <span class="No-Break">smoother images.</span></p>
<p>Image compression (<span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.9</em>) is another important application <span class="No-Break">of PCA.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 3.9: Image compression with PCA" height="311" src="image/Figure_03_08_B18943.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9: Image compression with PCA</p>
<p>The percentage of variance expressed by the PCs determines how many features should be the input to deep learning models (neural networks) for image classification so that the computing performance is not affected while dealing with huge and <span class="No-Break">high-dimensional datasets.</span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Anomaly detection</h2>
<p>Detecting anomalies is <a id="_idIndexMarker105"/>common in fraud detection, fault detection, and system health monitoring in sensor networks. PCA makes use of the cluster <a id="_idIndexMarker106"/>method for detecting an outlier, typically collective and unordered outliers. Cluster-based anomaly detection assumes that the inlying (normal) data points belong to large and dense clusters, and outlying (anomalous) ones belong to small or sparse clusters or do not belong to any of them. Example code with sample telemetry data can be found in the following <span class="No-Break">repository: </span><a href="https://github.com/ranja-sarkar/mm"><span class="No-Break">https://github.com/ranja-sarkar/mm</span></a><span class="No-Break">.</span></p>
<p>The PCs apply distance metrics to identify anomalies. PCA, in this case, determines what constitutes a <a id="_idIndexMarker107"/>normal class. As an exercise, you can use unsupervised learning methods such as K-means and Isolation Forest to detect outliers in the same dataset for a comparison of the results and gain more <span class="No-Break">meaningful insights.</span></p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Summary</h1>
<p>In this chapter, we learned about two linear algebraic methods used to reduce the dimensionality of data: namely, principal component analysis and linear discriminant analysis. The focus was on PCA, which is an unsupervised method to reduce the feature space of high-dimensional data and to know why this reduction is necessary for solving business problems. We did a detailed study of the mathematics behind the algorithm and how it works in ML models. We also learned about a couple of important applications of PCA along with the <span class="No-Break">Python code.</span></p>
<p>In the next chapter, we will learn about an optimization method called Gradient Descent, which is arguably the most common (and popular) algorithm to optimize neural networks. It is a learning algorithm that works by minimizing a given cost function. As the name suggests, it uses a gradient (derivative) iteratively to minimize <span class="No-Break">the function.</span></p>
</div>
</div></body></html>