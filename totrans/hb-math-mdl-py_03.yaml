- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A well-known algorithm to extract features from high-dimensional data for consumption
    in **machine learning** (**ML**) models is **Principal Component Analysis** (**PCA**).
    In mathematical terms, *dimension* is the minimum number of coordinates required
    to specify a vector in space. A lot of computational power is needed to find the
    distance between two vectors in high-dimensional space and in such cases, dimension
    is considered a curse. An increase in dimension will result in high performance
    of the algorithm only to a certain extent and will drop beyond that. This is the
    curse of dimensionality, as shown in *Figure 3**.1*, which impedes the achievement
    of efficiency for most ML algorithms. The variable columns or features in data
    represent dimensions of space and the rows represent the coordinates in that space.
    With the increasing dimension of data, sparsity increases and there is an exponentially
    increasing computational effort required to calculate distance and density. Theoretically
    speaking, an increase in dimension practically increases noise and redundancy
    in large datasets. Arguably, PCA is the most popular technique to tackle this
    complexity of dimensionality in high-dimensional problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Curse of dimensionality](img/Figure_03_01_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Curse of dimensionality'
  prefs: []
  type: TYPE_NORMAL
- en: PCA comes from the field of linear algebra and is essentially a data preparation
    method that projects the data in a subspace before fitting the ML model to the
    newly created low-dimensional dataset. PCA is a data projection technique useful
    in visualizing high-dimensional data and improving data classification. It was
    invented in the 1900s following the principal axis theorem. The main objectives
    of PCA are to find an orthonormal basis for the data, sort dimensions in the order
    of importance or variance, discard dimensions of low importance, and focus only
    on uncorrelated Gaussian components.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra for PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear discriminant analysis – the difference from PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section talks about linear algebra, the subject of mathematics
    on which PCA is based.
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra for PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is an unsupervised method used to reduce the number of features of a high-dimensional
    dataset. An unlabeled dataset is reduced into its constituent parts by matrix
    factorization (or decomposition) followed by ranking of these parts in accordance
    with variances. The projected data representative of the original data becomes
    the input to train ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is defined as the orthogonal projection of data onto a lower dimensional
    linear space called the principal subspace, done by finding new axes (or basis
    vectors) that preserve the maximum variance of projected data; the new axes or
    vectors are known as principal components. PCA preserves the information by considering
    the variance of projection vectors: the highest variance lies on the first axis,
    the second highest on the second axis, and so forth. The working principle of
    the linear transformation called PCA is shown in *Figure 3**.2*. It compresses
    the feature space by identifying a subspace that captures the essence of the complete
    feature matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: PCA working principle](img/Figure_03_02_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: PCA working principle'
  prefs: []
  type: TYPE_NORMAL
- en: There are other approaches to reducing data dimensionality such as feature selection
    methods (wrapper and filter), non-linear methods such as manifold learning (t-SNE),
    and deep learning (autoencoders) networks; however, the widest and most popular
    exploratory approach is PCA. Typically, linear algebraic methods assume that all
    inputs have the same distribution, and hence, it is a good practice to (either
    normalize or standardize) scale data before using PCA if the input features have
    different units.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance matrix – eigenvalues and eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The constraint in PCA is all the principal axes should be mutually orthogonal.
    The covariance of data is a measure of how much any pair of features in the data
    vary from each other. A covariance matrix checks the correlations between features
    in data and the directions of these relationships are obtained depending on whether
    the covariance is less than, equal to, or greater than zero. *Figure 3**.3* displays
    the covariance matrix formula. Each element in the matrix represents the correlation
    between two features in the data where *j* and *k* run over *p* variates, *N*
    is the number of observations (rows), and the ![](img/Formula_03_001.png) bar
    and ![](img/Formula_03_002.png) bar in the formula denote the expected values
    (averages).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Covariance matrix](img/Formula_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Covariance matrix'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of PCA is to explain most of the data variability with far fewer
    variables or features than that in the original dataset. Each of the *N* observations
    or records resides in *p*-dimensional space. Not all the dimensions are equally
    relevant. PCA seeks a smaller number of important dimensions, where importance
    is quantified by the amount of variation of the observations along each dimension.
    The dimensions figured out by PCA are a linear combination of the p features.
    We can take the linear combinations and reduce the number of plots required for
    visual analysis of the feature space while retaining the essence of the original
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues and eigenvectors of a covariance matrix computed by eigendecomposition
    determine the magnitude and direction of the new subspace, respectively. In linear
    algebra, an **eigenvector** (associated with a set of linear equations) of a linear
    transformation is a non-zero vector that changes by a magnitude (scalar) when
    the transformation is applied to it. The corresponding **eigenvalue** is the magnitude
    or factor by which the eigenvector is scaled, and **eigendecomposition** is the
    factorization of a matrix into its eigenvectors and eigenvalues. The principal
    components are the eigenvectors. The top eigenvalues of a covariance matrix after
    sorting in descending order yield the principal components of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The first **principal component** (**PC**) of data is the linear combination
    of the features that have the highest variance. The second PC is the linear combination
    of the features that have maximum variance out of all linear combinations uncorrelated
    with the first PC. The first two PCs are shown in *Figure 3**.4*, and this computational
    process proceeds until all the PCs of the dataset are found. These PCs are essentially
    the eigenvectors, and linear algebraic techniques show that the eigenvector corresponding
    to the highest eigenvalue of the covariance matrix explains the greatest proportion
    of data variability. Each PC vector defines a direction in feature space and all
    of them are uncorrelated – that is, mutually orthogonal. The PCs form the basis
    of the new space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Principal components in feature space (x = ￼, y = ￼)](img/Figure_03_03_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Principal components in feature space (x = ![](img/Formula_03_004.png),
    y = ![](img/Formula_03_005.png))'
  prefs: []
  type: TYPE_NORMAL
- en: Number of PCs – how to select for a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The question is how to determine how well a dataset is explained by a certain
    number of PCs. The answer lies in the percentage of variance retained by the number
    of PCs. We would ideally like to have the smallest number of PCs possible explaining
    most of the variability. There is no robust method to determine the number of
    usable components. As the number of observations and variables in the dataset
    vary, different levels of accuracy and amounts of reduction are desirable.
  prefs: []
  type: TYPE_NORMAL
- en: The **proportion of variance explained** (**PVE**) by the *m*th PC is computed
    by considering the *m*th eigenvalue. PVE is the ratio of the mth eigenvalue represented
    by ![](img/Formula_03_006.png) (of *j*th variate in *Figure 3**.5*) and the sum
    of the eigenvalues of all the PCs or eigenvectors. To put it simply, PVE is the
    variance explained by each PC divided by the total variance of all PCs in the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: PVE calculation](img/Formula_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: PVE calculation'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we look for the “elbow point” where the PVE significantly drops
    off to determine the number of usable PCs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6a: PVE versus PC](img/Figure_03_04_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6a: PVE versus PC'
  prefs: []
  type: TYPE_NORMAL
- en: The first PC in the example shown in *Figure 3**.6a* explains 62.5% of data
    variability, and the second PC explains 25%. In a cumulative manner, the first
    two PCs explain 87.5% of the variability, as shown in *Figure 3**.6b*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6b: Cumulative PVE (y axis) versus PC](img/Figure_03_05_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6b: Cumulative PVE (y axis) versus PC'
  prefs: []
  type: TYPE_NORMAL
- en: Another widely used matrix decomposition method to identify the number of PCs
    is **singular value decomposition** (**SVD**). It is a reduced-rank approximation
    method that provides a simple means to do the same – that is, compute the principal
    components corresponding to the singular values. SVD is just another way to factorize
    a matrix and allows us to unravel similar information as eigendecomposition does.
    The singular values of the data matrix obtained via SVD are essentially the square
    roots of the eigenvalues of the covariance matrix in PCA. SVD is the same as PCA
    of a raw data matrix, which is mean-centered.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mathematical demonstration of SVD can be found on the Wolfram pages: [https://mathworld.wolfram.com/SingularValueDecomposition.xhtml](https://mathworld.wolfram.com/SingularValueDecomposition.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD is an iterative numerical method. Every rectangular matrix has SVD, although,
    for a few complex problems, it may fail to decompose some matrices neatly. You
    can perform SVD using the linear algebra class of the Python library, `scipy`.
    The `scikit-learn` library provides functions for SVD: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional data can be reduced to a subset of dimensions (columns) that
    are most relevant to the problem being solved. The data matrix (rows by columns)
    results in a matrix with a lower rank that approximates the original matrix and
    best captures its salient features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two ways in which dimensionality reduction can be done: one is feature
    selection and the other is feature extraction. A subset of original features is
    selected in the former approach by filtering based on some criteria true to the
    particular use case and corresponding data. On the other hand, a set of new features
    is found in the feature extraction approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature extraction is done using linear mapping from the original features,
    which no longer exist upon implementation of the method. In essence, new features
    constructed from available data do not have column names as in the original data.
    There are two feature extraction methods: PCA and LDA. A nonlinear mapping may
    also be used depending on the data but the method no longer remains PCA or LDA.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored PCA for the reduction of features (and hence, the
    reduction of the high-dimensional space), we shall learn about a supervised method
    of linear feature extraction called **linear discriminant** **analysis** (**LDA**).
  prefs: []
  type: TYPE_NORMAL
- en: LDA – the difference from PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA and PCA are linear transformation methods; the latter yields directions
    or PCs that maximize data variance and the former yields directions that maximize
    the separation between data classes. The way in which the PCA algorithm works
    disregards class labels.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a supervised method to reduce dimensionality that projects the data onto
    a subspace in a way that maximizes the separability between (groups) classes;
    hence, it is used for pattern classification problems. LDA works well for data
    with multiple classes; however, it makes assumptions of normally distributed classes
    and equal class covariances. PCA tends to work well if the number of samples in
    each class is relatively small. In both cases, though, observations ought to be
    much higher relative to the dimensions for meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: LDA seeks a projection that discriminates data in the best possible way, unlike
    PCA, which seeks a projection that preserves maximum information in the data.
    When regularization of the estimate of covariance is introduced to moderate the
    influence of different variables on LDA, it is called **regularized** **discriminant
    analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Linear discriminants](img/Figure_03_06_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Linear discriminants'
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA involves developing a probabilistic model per class based on the distribution
    of each input variable (*Figure 3**.7*). It may be considered as an application
    of the Bayes'' theorem for classification and assumes that the input variables
    are uncorrelated; if they are correlated, the PCA transform may aid in removing
    the linear dependence. The scikit-learn library provides functions for LDA. Example
    code with a synthetic dataset is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the hyperparameter (`solver`) in the grid search is
    set to `'svd'` (default) but other `solver` values can also be used. This example
    only introduces us to using LDA with scikit-learn; there is a whole lot of customization
    that can be done depending on the problem being solved.
  prefs: []
  type: TYPE_NORMAL
- en: We have explored the linear algebraic methods for dimensionality reduction;
    we shall learn about the most important applications of PCA in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is one fundamental algorithm and forms the foundation of ML. It finds use
    in diverse areas such as noise reduction in images, classification of data in
    general, anomaly detection, and other applications in medical data correlation.
    We will explore a couple of widely used applications of PCA in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn library in Python provides functions for PCA. The following
    example code shows how to leverage PCA for dimensionality reduction while developing
    a predictive model that uses a PCA projection as input. We will be using PCA on
    a synthetic dataset while fitting a logistic regression model for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will plot the principal components using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we do not see improvement in the model accuracy beyond
    eight components (*Figure 3**.8*). It is evident that the first eight components
    contain maximum information about the class and the remaining ones are redundant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Classification accuracy versus the number of PCs for a synthetic
    dataset](img/Figure_03_07_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Classification accuracy versus the number of PCs for a synthetic
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The number of components after a PCA transform of features that results in the
    best average performance of the model is chosen and fed to the ML model for predictions.
    In the following subsection, we will learn about denoising and the detection of
    outliers using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Noise reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PCA finds use in the reduction of noise in data, especially images. PCA reconstruction
    of an image by denoising can be achieved with the decomposition method of the
    scikit-learn Python library. Details of the library function with examples can
    be found here: [https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml](https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: A good exercise would be to reconstruct images obtained from a video sequence
    exploring linear PCA as well as kernel PCA and check which one provides smoother
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Image compression (*Figure 3**.9*) is another important application of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Image compression with PCA](img/Figure_03_08_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Image compression with PCA'
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of variance expressed by the PCs determines how many features
    should be the input to deep learning models (neural networks) for image classification
    so that the computing performance is not affected while dealing with huge and
    high-dimensional datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Detecting anomalies is common in fraud detection, fault detection, and system
    health monitoring in sensor networks. PCA makes use of the cluster method for
    detecting an outlier, typically collective and unordered outliers. Cluster-based
    anomaly detection assumes that the inlying (normal) data points belong to large
    and dense clusters, and outlying (anomalous) ones belong to small or sparse clusters
    or do not belong to any of them. Example code with sample telemetry data can be
    found in the following repository: [https://github.com/ranja-sarkar/mm](https://github.com/ranja-sarkar/mm).'
  prefs: []
  type: TYPE_NORMAL
- en: The PCs apply distance metrics to identify anomalies. PCA, in this case, determines
    what constitutes a normal class. As an exercise, you can use unsupervised learning
    methods such as K-means and Isolation Forest to detect outliers in the same dataset
    for a comparison of the results and gain more meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about two linear algebraic methods used to reduce
    the dimensionality of data: namely, principal component analysis and linear discriminant
    analysis. The focus was on PCA, which is an unsupervised method to reduce the
    feature space of high-dimensional data and to know why this reduction is necessary
    for solving business problems. We did a detailed study of the mathematics behind
    the algorithm and how it works in ML models. We also learned about a couple of
    important applications of PCA along with the Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about an optimization method called Gradient
    Descent, which is arguably the most common (and popular) algorithm to optimize
    neural networks. It is a learning algorithm that works by minimizing a given cost
    function. As the name suggests, it uses a gradient (derivative) iteratively to
    minimize the function.
  prefs: []
  type: TYPE_NORMAL
