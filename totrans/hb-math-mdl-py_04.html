<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer092">
<h1 class="chapter-number" id="_idParaDest-54"><a id="_idTextAnchor053"/>4</h1>
<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Gradient Descent</h1>
<p>One optimization algorithm that lays the foundation for machine learning models is <strong class="bold">gradient descent</strong> (<strong class="bold">GD</strong>). GD is a <a id="_idIndexMarker108"/>simple and effective tool useful to train such models. Gradient descent, as the name suggests, involves “going downhill.” We choose a direction across a landscape and take whichever step gets us downhill. The step size <a id="_idIndexMarker109"/>depends on the slope (gradient) of the hill. In <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, gradient descent estimates the error gradient, helping to minimize the cost function. Very few optimization methods are as computationally efficient as gradient descent. GD also lays the foundation for the optimization of deep <span class="No-Break">learning models.</span></p>
<p>In problems where the parameters cannot be calculated analytically by use of linear algebra and must be searched by optimization, GD finds its best use. The algorithm works iteratively by moving in the direction of the steepest descent. At each iteration, the model parameters, such as coefficients in linear regression and weights in neural networks, are updated. The model continues to update its parameters until the cost function converges or reaches its minimum value (the bottom of the slope in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.1a</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 4.1a: Gradient descent" height="238" src="image/Figure_04_01_B18943.jpg" width="444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1a: Gradient descent</p>
<p>The size of a step taken in each iteration is called the learning rate (a function derivative is scaled by the learning rate at each iteration). With a learning rate that is too low, the model may reach the maximum permissible number of iterations before reaching the bottom, whereas it may not converge or may diverge (the so-called exploding gradient problem) completely if the learning rate is too high. Selecting the most appropriate learning rate is crucial in achieving a model with the best possible accuracy, as seen in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.1b</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 4.1b: Learning rates in gradient descent" height="502" src="image/Figure_04_02_B18943.jpg" width="564"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1b: Learning rates in gradient descent</p>
<p>For GD to work, the objective or cost function must be differentiable (meaning the first derivative exists at each point in the domain of a univariate function) and convex (where two points on the function can be connected by a line segment without crossing). The second derivative of a convex function is always positive. Examples of convex and non-convex functions are shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>. GD is a first-order <span class="No-Break">optimization algorithm.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 4.2: Example of convex (L) and non-convex (R) function" height="315" src="image/Figure_04_03_B18943.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: Example of convex (L) and non-convex (R) function</p>
<p>In a multivariate function, the gradient is a vector of derivatives in each direction in the domain. Such functions have saddle points (quasi-convex or semi-convex) where the algorithm may get stuck and obtaining a minimum is not guaranteed. This is where second-order optimization algorithms are brought in to escape the saddle point and reach the global minimum. The GD algorithm finds its use in control as well as mechanical engineering, apart from ML and DL. The following sections compare the algorithm with other optimization algorithms used in ML and <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) models and specifically examines some commonly used gradient <span class="No-Break">descent optimizers.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>Gradient <span class="No-Break">descent variants</span></li>
<li>Gradient <span class="No-Break">descent optimizers</span></li>
</ul>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Gradient descent variants</h1>
<p>The workings <a id="_idIndexMarker110"/>of the gradient descent algorithm to optimize a simple linear regression model (<em class="italic">y = mx + c</em>) is elaborated with Python code in <span class="No-Break">this section.</span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Application of gradient descent</h2>
<p>Keeping <a id="_idIndexMarker111"/>the number of iterations the same, the algorithm is run for three different learning rates resulting in <a id="_idIndexMarker112"/>three models, hence three <strong class="bold">MSE</strong> (<strong class="bold">mean squared error</strong>) values. MSE is the calculated loss or cost function in <span class="No-Break">linear regression:</span></p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
#gradient descent method
class GDLinearRegression:
    def __init__(self, learning_rate, epoch):
        self.learning_rate, self.iterations = learning_rate, epoch
       #epoch is number of iterations
    def fit(self, X, y):
        c = 0
        m = 5
        n = X.shape[0]
        for _ in range(self.iterations):
            b_gradient = -2 * np.sum(y - m*X + c) / n
            m_gradient = -2 * np.sum(X*(y - (m*X + c))) / n
            c = c + (self.learning_rate * b_gradient)
            m = m - (self.learning_rate * m_gradient)
        self.m, self.c = m, c
    def predict(self, X):
        return self.m*X + self.c
#dataset
np.random.seed(42)
X = np.array(sorted(list(range(5))*20)) + np.random.normal(size = 100, scale = 0.5)
y = np.array(sorted(list(range(5))*20)) + np.random.normal(size = 100, scale = 0.3)
#model 1
Clf_1 = GDLinearRegression(learning_rate = 0.05, epoch = 1000)
Clf_1.fit(X, y)
y_pred = Clf_1.predict(X)
mse_1 = mean_squared_error(y, y_pred)
plt.style.use('fivethirtyeight')
plt.scatter(X, y, color='black')
plt.plot(X, y_pred)
plt.gca().set_title("Linear Regression Model 1")
print('Slope = ', round(Clf_1.m, 4))
print('Intercept = ', round(Clf_1.c, 4))
print('MSE = ', round(mse_1, 2))</pre>
<p>Two <a id="_idIndexMarker113"/>other models are trained with two different learning rates, one higher and another lower than model 1, as <span class="No-Break">seen here:</span></p>
<pre class="source-code">
#model 2
Clf_2 = GDLinearRegression(learning_rate = 0.2, epoch = 1000)
Clf_2.fit(X, y)
y_pred = Clf_2.predict(X)
mse_2 = mean_squared_error(y, y_pred)
plt.style.use('fivethirtyeight')
plt.scatter(X, y, color='black')
plt.plot(X, y_pred)
plt.gca().set_title("Linear Regression Model 2")
print('MSE = ', round(mse_2, 2))
#model 3
Clf_3 = GDLinearRegression(learning_rate = 0.0001, epoch = 1000)
Clf_3.fit(X, y)
y_pred = Clf_3.predict(X)
mse_3 = mean_squared_error(y, y_pred)
plt.style.use('fivethirtyeight')
plt.scatter(X, y, color='black')
plt.plot(X, y_pred)
plt.gca().set_title("Linear Regression Model 3")
print('MSE = ', round(mse_3, 2))</pre>
<p>Upon executing the code, the linear regression models obtained (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em>) show how carefully <a id="_idIndexMarker114"/>the parameter (learning rate) should be chosen to attain optimal performance or the best accuracy of the <span class="No-Break">ML model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 4.3: Gradient descent for a linear regression model" height="289" src="image/Figure_04_04_B18943.jpg" width="1203"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: Gradient descent for a linear regression model</p>
<p>There are GD variants (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em>) that differ in the data size used to compute the gradient of the objective function. A trade-off between the accuracy of the parameter (coefficient or weight) <a id="_idIndexMarker115"/>and the time taken to do it <a id="_idIndexMarker116"/>is made depending on the amount of data. The <a id="_idIndexMarker117"/>variants are <strong class="bold">batch gradient descent</strong> (<strong class="bold">BGD</strong>), <strong class="bold">mini-batch gradient descent</strong>, and <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>), which we will now discuss in the <span class="No-Break">following subsection.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Mini-batch gradient descent and stochastic gradient descent</h2>
<p>BGD, also known <a id="_idIndexMarker118"/>as vanilla gradient descent, is simply gradient <a id="_idIndexMarker119"/>descent and computes the gradient for the entire training <a id="_idIndexMarker120"/>data to perform one update (in one step) and thus can be very slow. Common examples of ML models that are optimized using BGD are linear regression and logistic regression for <span class="No-Break">smaller datasets.</span></p>
<p>For bigger datasets, we generally use mini-batch GD, which allows the splitting of training data into mini-batches that can be processed individually. After each mini-batch is processed, the parameters are updated and this continues until the entire dataset has iteratively been processed. One full cycle through the data is called an epoch. A number of steps are taken to reach the global minimum, which introduces some variance into the optimization process. This variant of GD is usually used for modeling problems where efficiency is as important <span class="No-Break">as accuracy.</span></p>
<p>SGD performs frequent parameter updates (for each training example) with a high level of variance <a id="_idIndexMarker121"/>that causes the cost function to fluctuate <a id="_idIndexMarker122"/>heavily. This enables it to jump to a new and potentially better local minimum. Upon slowly decreasing the learning rate, SGD shows convergence behavior similar to BGD. SGD is computationally faster than BGD as it considers one example at <span class="No-Break">a time.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 4.4: Gradient descent variants" height="424" src="image/Figure_04_05_B18943.jpg" width="1617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4: Gradient descent variants</p>
<p>SGD is typically the algorithm of choice for training a neural network. A key challenge with SGD while minimizing the highly non-convex error functions common in neural networks is avoiding getting trapped in their numerous suboptimal local minima. These saddle points make it very hard for SGD to escape, as the gradient is close to zero in all dimensions. In the next section, we outline some GD optimizers that deal with <span class="No-Break">such challenges.</span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Gradient descent optimizers</h1>
<p>The optimizers <a id="_idIndexMarker123"/>discussed here are widely used to train DL models depending on the degree of the non-convexity of the error or <span class="No-Break">cost function.</span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Momentum</h2>
<p>The momentum method uses a moving average gradient instead of a gradient at each time step and <a id="_idIndexMarker124"/>reduces the back-and-forth oscillations (fluctuations of the cost function) caused by SGD. This process focuses <a id="_idIndexMarker125"/>on the steepest descent path. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5a</em> shows movement with no momentum by creating oscillations in SGD while <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5b</em> shows movement in the relevant direction by accumulating velocity with damped oscillations and closer to <span class="No-Break">the optimum.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 4.5a: SGD with no momentum" height="285" src="image/Figure_04_06_B18943.jpg" width="681"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5a: SGD with no momentum</p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 4.5b: SGD with momentum" height="251" src="image/Figure_04_07_B18943.jpg" width="660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5b: SGD with momentum</p>
<p>The momentum term reduces updates for dimensions whose gradients change directions and as a result, faster convergence <span class="No-Break">is achieved.</span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>Adagrad</h2>
<p>The <strong class="source-inline">adagrad</strong> optimizer is used when dealing with sparse data as the algorithm performs small <a id="_idIndexMarker126"/>updates of parameters based on features that occur often. In <strong class="source-inline">adagrad</strong>, different or “adaptive” learning rates are used for every update at <a id="_idIndexMarker127"/>every time step (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.6</em>). The algorithm uses larger learning rates for infrequent features and smaller ones for more frequent features. The major advantage of using this optimizer is that the learning rate is not set manually. And when the learning rate shrinks to almost zero, the model gains no <span class="No-Break">new knowledge.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 4.6: Adagrad optimizer" height="366" src="image/Figure_04_08_B18943.jpg" width="469"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6: Adagrad optimizer</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/>RMSprop</h2>
<p>The RMSprop optimizer <a id="_idIndexMarker128"/>is similar to the <strong class="source-inline">adagrad</strong> optimizer <a id="_idIndexMarker129"/>and hence known as <strong class="bold">leaky adagrad</strong>, only it uses <a id="_idIndexMarker130"/>a different method for parameter updates. The RMSprop algorithm restricts oscillations in the vertical direction so that it can take larger steps in the horizontal direction (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.7</em>). The algorithm adaptively scales the learning rate in each dimension by using an exponentially weighted average of the gradient that allows it to focus on the most <span class="No-Break">recent gradients.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 4.7: RMSprop optimizer" height="351" src="image/Figure_04_09_B18943.jpg" width="507"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7: RMSprop optimizer</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor062"/>Adam</h2>
<p>The <strong class="bold">adaptive moment estimation</strong> (<strong class="bold">adam</strong>) optimizer inherits the advantages of both the momentum <a id="_idIndexMarker131"/>and RMSprop optimization algorithms (<span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.8</em>). It combines the ideas of a moving average gradient <a id="_idIndexMarker132"/>and an adaptive learning rate. These two respectively represent the estimates of the first moment (mean) and second moment (variance) of the gradient of the cost function, hence <span class="No-Break">the name.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 4.8: Adam optimizer" height="358" src="image/Figure_04_10_B18943.jpg" width="533"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8: Adam optimizer</p>
<p>It has been empirically observed that the adam optimizer is effective and works better than SGD in practice. It has become the default optimizer of choice to train DL models. For further <a id="_idIndexMarker133"/>reading, check out <a id="_idIndexMarker134"/>the following MachineLearningMastery <span class="No-Break">article: </span><a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"><span class="No-Break">https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Summary</h1>
<p>In this chapter, we learned about a foundational optimization algorithm and its variants used in training ML and DL models. An application of the optimization technique in Python to a linear regression problem was also elaborated on. Both the cost function and its gradient, and how to update the gradient to converge to the optimal point, are mathematical concepts every data scientist must understand thoroughly; optimizing a cost function is the basis of achieving an optimal model for a problem or predictions. Different ways can be used to estimate the gradients depending on the behavior of the <span class="No-Break">cost function.</span></p>
<p>In the following chapter, we will explore another fundamental algorithm, known as <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>). Although SVMs can be used for regression problems, they are more widely used for <span class="No-Break">classification tasks.</span></p>
</div>
</div></body></html>