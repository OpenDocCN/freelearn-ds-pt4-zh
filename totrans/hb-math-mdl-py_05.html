<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer106">
<h1 class="chapter-number" id="_idParaDest-65"><a id="_idTextAnchor064"/>5</h1>
<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Support Vector Machine</h1>
<p>This chapter explores a classic algorithm that one must keep in oneâ€™s machine learning arsenal called the <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>), which is mainly used for classification <a id="_idIndexMarker135"/><a id="_idIndexMarker136"/>problems rather than regression problems. Since its inception in the 1990s, it was commonly used to recognize patterns and outliers in data. Its popularity declined after the emergence of boosting<a id="_idIndexMarker137"/><a id="_idIndexMarker138"/> algorithms such as <strong class="bold">extreme gradient boost</strong> (<strong class="bold">XGB</strong>). However, it prevails as one of the most commonly used supervised <span class="No-Break">learning algorithms.</span></p>
<p>In the 1990s, efficient learning algorithms based on computational learning were developed for non-linear functions. Algorithms such as linear learning algorithms have well-defined theoretical properties. With this development, efficient separability (decision surfaces) of nonlinear regions that use kernel functions was established. Nonlinear SVMs are quite frequently used for the classification of real (<span class="No-Break">nonlinear) data.</span></p>
<p>SVM was initially known as a binary classifier that could <a id="_idIndexMarker139"/><a id="_idIndexMarker140"/>be used for one-class classification of skewed or imbalanced class distribution. This unsupervised algorithm could effectively learn from the majority or normal class in a dataset to classify new data points as either <em class="italic">normal</em> or <em class="italic">outlier</em>. The process of identifying the minority or rarity class generally referred to as outlier is called <strong class="bold">anomaly detection</strong>, as the outlier is an anomaly and the rest of the data is normal. Classification <a id="_idIndexMarker141"/><a id="_idIndexMarker142"/>involves fitting a model on the normal data (training examples) and predicting whether incoming new data is normal (inlier) or outlier. One-class SVM is most suited for a specific problem where the minority class does not have a consistent pattern or is a noisy instance, making it difficult for other classification algorithms to learn a decision boundary. The outliers in general are treated as deviations <span class="No-Break">from normal.</span></p>
<p>In general, SVMs are effective in problems where the number of variables is greater than the number of records, meaning, in high-dimensional spaces. The algorithm uses a subset of training examples in the decision function, hence it is memory-efficient. It turns out the algorithm is versatile, as different kernels can be specified for the <span class="No-Break">decision function.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>Support vectors <span class="No-Break">in SVM</span></li>
<li>Kernels <span class="No-Break">for SVM</span></li>
<li>Implementation <span class="No-Break">of SVM</span></li>
</ul>
<p>We will learn about support vectors and kernels in the <span class="No-Break">forthcoming sections.</span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Support vectors in SVM</h1>
<p>SVM is an algorithm that can produce<a id="_idIndexMarker143"/><a id="_idIndexMarker144"/> significantly accurate results with less computation power. It is widely used in data classification tasks. If a dataset has <em class="italic">n</em> number of features, SVM finds a hyperplane in the <em class="italic">n</em>-dimensional space, which<a id="_idIndexMarker145"/><a id="_idIndexMarker146"/> is also called the <strong class="bold">decision boundary</strong>, to classify the data points. An optimal decision boundary maximizes the distance <a id="_idIndexMarker147"/><a id="_idIndexMarker148"/>between the boundary and instances in both classes. The distance between data points in the classes (shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.1a</em>) is known as <span class="No-Break">the </span><span class="No-Break"><strong class="bold">margin</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 5.1a: Optimal hyperplane" height="363" src="image/Figure_05_01_B18943.jpg" width="370"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1a: Optimal hyperplane</p>
<p>An SVM algorithm finds the optimal line in two dimensions or the optimal hyperplane in more than two dimensions that separates the space into classes. The optimal hyperplane or optimal line maximizes the margin (the distance between the data points of the two classes). In 3D (or more), data points become vectors and those (very small subset of<a id="_idIndexMarker149"/><a id="_idIndexMarker150"/> training examples) that are closest to or on the hyperplanes (just outside the maximum margin) are called <strong class="bold">support vectors</strong> (see <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.1b</em></span><span class="No-Break">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 5.1b: Support vectors" height="212" src="image/Figure_05_02_B18943.jpg" width="426"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1b: Support vectors</p>
<p>If all support vectors are at the same distance from the optimal hyperplane, the margin is said to be good. The margin shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.1b</em> is bad, as support vectors in class +1 are very close to the optimal hyperplane, while those in class -1 are far away from it. Moving a support vector moves the decision boundary or hyperplane while moving other data points has <span class="No-Break">no effect.</span></p>
<p>If the number of features in an input <a id="_idIndexMarker151"/><a id="_idIndexMarker152"/>dataset is two, the hyperplane is just a line. If the number is three, then the hyperplane (shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>) is a two-dimensional plane. The dimension of the decision boundary depends on the number of features, and the data points on either side of it (the hyperplane) belong to <span class="No-Break">different classes:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 5.2: Hyperplane in 3D feature space" height="560" src="image/Figure_05_03_B18943.jpg" width="754"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: Hyperplane in 3D feature space</p>
<p>Support vectors influence the position and orientation of the hyperplane. The margin of the classifier is maximized using support vectors. The margin is hard if the data is linearly separable. For most practical problems, data is not linearly separable, and in such cases the margin is soft. This allows for data points within the marginal distance (shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.3</em>) between two data <span class="No-Break">class separators:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 5.3: Soft margin" height="403" src="image/Figure_05_04_B18943.jpg" width="541"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3: Soft margin</p>
<p>It is better to have a large margin that might allow for some margin violation to occur. The larger the margin, the lower the error of the classifier. Maximizing<a id="_idIndexMarker153"/><a id="_idIndexMarker154"/> the margin is equivalent to minimizing loss in machine learning algorithms. The function that helps maximize the margin is <strong class="bold">hinge loss</strong>. Hinge loss (error) is zero if data is classified correctly, meaning we have a hard <a id="_idIndexMarker155"/><a id="_idIndexMarker156"/>margin as the points are not close to the hyperplane. Hinge loss is one if most of the data points are classified incorrectly. In general, support vectors are within the margin boundaries (soft margin) when the problem is not <span class="No-Break">linearly separable.</span></p>
<p>In the next section, the kernel trick is introduced. Kernel is a technique used in SVMs to classify data points that are not linearly separable. Kernel functions enable operation in a high-dimensional feature space without computing data coordinates in that space, hence this operation is not <span class="No-Break">computationally expensive.</span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Kernels for SVM</h1>
<p>With a kernel trick, a 2D space is converted into a 3D <a id="_idIndexMarker157"/><a id="_idIndexMarker158"/>space using a mapping function such that the nonlinear data can be classified or separated in a higher dimension (see <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.4</em>). The transformation of original data for mapping into the new space is done via kernel. The kernel function defines inner products (measure of similarity) in the <span class="No-Break">transformed space.</span></p>
<p>The compute and storage requirements of SVMs increase with the number of training examples. The core of the algorithm is a quadratic programming problem separating support vectors from the training dataset. A linear kernel, which is just a dot product, is the fastest implementation of SVM. A few examples of linear and <a id="_idIndexMarker159"/><a id="_idIndexMarker160"/>nonlinear kernels <a id="_idIndexMarker161"/><a id="_idIndexMarker162"/>are shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5a</em>. The most common <a id="_idIndexMarker163"/><a id="_idIndexMarker164"/>nonlinear SVM kernels are <strong class="bold">radial basis function</strong> (<strong class="bold">RBF</strong>), <strong class="bold">sigmoid</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">polynomial</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively classified in higher dimension" height="654" src="image/Figure_05_05_B18943.jpg" width="1341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively classified in higher dimension</p>
<p>SVMs are very effective for small datasets that <a id="_idIndexMarker165"/><a id="_idIndexMarker166"/>are not linearly separable. Small data means that the number of features is more than the training size, due to which SVMs suffer from overfitting in some cases. The right kernel function and regularization (penalty function) come to the rescue in those cases. Each kernel has a different mathematical formulation, hence the set of parameters varies from one <span class="No-Break">to another.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M), and polynomial kernel (R) functions" height="195" src="image/Figure_05_06_B18943.jpg" width="753"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M), and polynomial kernel (R) functions</p>
<p>The parameter exponent (degree) in a polynomial kernel when set to 1 becomes a linear kernel and when set to 3 becomes a cubic kernel, an example of which is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.5a</em> (rightmost). The sigmoid kernel cumulative distribution function goes from 0 to 1 to classify data and is mostly used as an activation function or perceptron in neural networks. An example of data classification by SVM with sigmoid kernel function is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.5b</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)" height="425" src="image/Figure_05_07_B18943.jpg" width="1154"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)</p>
<p>All SVM kernels have a parameter that trades off the misclassification of the dataset against the simplicity of the separator. While training an SVM with the RBF kernel, which is an exponential (<img alt="" height="27" src="image/Formula_05_001.png" width="56"/>) function, the parameter <em class="italic">a</em> is greater than zero and defines<a id="_idIndexMarker167"/><a id="_idIndexMarker168"/> the influence of a training example on the separator. The selection of the parameters in respective kernel functions is critical to an <span class="No-Break">SVMâ€™s performance.</span></p>
<p>In imbalanced datasets, the parameters dedicated to providing weights on classes and samples become significant, as they might be required to give more importance to a certain sample or class in such cases. The effect of sample weighting on the class boundary is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em>, wherein the data point size is proportional to the <span class="No-Break">sample weights:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 5.6: Classification with constant sample weight (L), with modified weight (R)" height="441" src="image/Figure_05_08_B18943.jpg" width="986"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6: Classification with constant sample weight (L), with modified weight (R)</p>
<p>While various methods and algorithms can detect outliers in a dataset, the kernel method used by the one-class SVM algorithm has been demonstrated in this chapter. Other examples include the decision tree ensemble method in the Isolation Forest algorithm, the distance or density method in the local outlier factor algorithm, and so on. Anomaly types can be <em class="italic">point</em> or <em class="italic">collective</em>, and one selects the algorithm for detection based on the anomaly type in a dataset. <em class="italic">Figures 5.7a and 5.7b</em> show examples of these anomaly types. A point anomaly is a global behavior while a collective anomaly is a local abnormal (non-normal) behavior. There can also be datasets wherein an anomaly can be entirely contextual, which most of the time is visible in <span class="No-Break">time-series data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 5.7a: Examples of point anomalies" height="473" src="image/Figure_05_09_B18943.jpg" width="1455"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7a: Examples of point anomalies</p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt=" Figure 5.7b: Example of a collective (non-point) anomaly" height="295" src="image/Figure_05_10_B18943.jpg" width="870"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 5.7b: Example of a collective (non-point) anomaly</p>
<p>In the following section, we will implement a <a id="_idIndexMarker169"/><a id="_idIndexMarker170"/>one-class SVM solution using Python, as this solution in general has proven to be useful for problems where the (point) outliers forming the minority class lack structure and are predominantly noisy examples (i.e. severe deviations <span class="No-Break">from inliers).</span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Implementation of SVM</h1>
<p>The one-class SVM algorithm does <a id="_idIndexMarker171"/><a id="_idIndexMarker172"/>not use (ignores) the examples that are far from or deviated from the observations during training. Only the observations that are most concentrated or dense are leveraged for (unsupervised) learning and such an approach is effective in specific problems where very few deviations from normal <span class="No-Break">are expected.</span></p>
<p>A synthetic dataset is created to implement SVM. We will have about 2% of the synthetic data in the minority class (outliers) denoted by <strong class="source-inline">1</strong> and 98% in the majority class (inliers) denoted by <strong class="source-inline">0</strong>, and leverage the RBF kernel to map the data into a high-dimensional space. The Python code (with the scikit-learn library) runs <span class="No-Break">as follows:</span></p>
<pre class="source-code">
import pandas as pd, numpy as np
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
X, y = make_classification(n_samples = 10000, n_features = 2, n_informative = 2,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â n_redundant = 0, n_classes = 2,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â n_clusters_per_class = 1,
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â weights = [0.98, 0.02], class_sep = 0.5, random_state = 0)
#Dataset as pandas dataframe
df = pd.DataFrame({'feature1': X[:, 0], 'feature2': X[:, 1], 'target': y})
#Split dataset into train and test subsets in the ratio 4:1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
#Train SVM model with RBF
one_class_svm = OneClassSVM(nu = 0.01, kernel = 'rbf', gamma = 'auto').fit(X_train)
#nu (specifies number of outliers) = 1% , gamma is a parameter for nonlinear kernels
prediction = one_class_svm.predict(X_test)
prediction = [1 if i == -1 else 0 for i in prediction] #outliers denoted by 1, inliers by 0
print(classification_report(y_test, prediction))</pre>
<p>The report of the classifier (<span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.8</em>) clearly <a id="_idIndexMarker173"/><a id="_idIndexMarker174"/>shows that the one-class SVM model has a recall of 23%, which means the model captures 23% of outliers. The F1-score is the harmonic mean of the two measures, namely precision <span class="No-Break">and recall:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="Figure 5.8: Classification report of one-class SVM" height="165" src="image/Figure_05_11_B18943.jpg" width="618"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8: Classification report of one-class SVM</p>
<p>We will visualize the outliers <a id="_idIndexMarker175"/>using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
#Visualization of outliers
df_test = pd.DataFrame(X_test, columns = ['feature1', 'feature2'])
df_test['y_test'] = y_test
df_test['svm_predictions'] = prediction
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16, 8))
ax1.set_title('Original Data')
ax1.scatter(df_test['feature1'], df_test['feature2'], c = df_test['y_test'])
ax2.set_title('One-Class SVM Prediction')
ax2.scatter(df_test['feature1'], df_test['feature2'], c = df_test['svm_predictions'])</pre>
<p>The default threshold of the algorithm for identifying these 2% outliers can also be customized so that fewer or more data points are labeled as <a id="_idIndexMarker176"/><a id="_idIndexMarker177"/>outliers depending on the use case. What is evident from <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.9</em> is that most of the outliers (yellow) have been detected correctly by <span class="No-Break">the classifier:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="Figure 5.9: Classification by one-class SVM" height="589" src="image/Figure_05_12_B18943.jpg" width="1114"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9: Classification by one-class SVM</p>
<p>One-class SVM is<a id="_idIndexMarker178"/> particularly useful as an anomaly detector and finds wide usage in sensor data captured from machines in the <span class="No-Break">manufacturing industry.</span></p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Summary</h1>
<p>In this chapter, we explored SVM as a classifier. In addition to linear data, SVMs can efficiently classify non-linear data using kernel functions. The method used by the SVM algorithm can be extended to solve regression problems. SVM is utilized for novelty detection as well, wherein the training dataset is not polluted with outliers and the algorithm is exploited to detect a new observation as an anomaly, in which case the outlier is called <span class="No-Break">a novelty.</span></p>
<p>The next chapter is about graph theory, a tool that provides the necessary mathematics to quantify and simplify complex systems. Graph theory is the study of relations (connections or edges) between a set of nodes or individual entities in a dynamic system. It is an integral component of ML and DL because graphs provide a means to represent a business problem as a mathematical programming task in the form of nodes <span class="No-Break">and edges.</span></p>
</div>
</div></body></html>