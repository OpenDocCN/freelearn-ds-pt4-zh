- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support Vector Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explores a classic algorithm that one must keep in one’s machine
    learning arsenal called the **support vector machine** (**SVM**), which is mainly
    used for classification problems rather than regression problems. Since its inception
    in the 1990s, it was commonly used to recognize patterns and outliers in data.
    Its popularity declined after the emergence of boosting algorithms such as **extreme
    gradient boost** (**XGB**). However, it prevails as one of the most commonly used
    supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, efficient learning algorithms based on computational learning
    were developed for non-linear functions. Algorithms such as linear learning algorithms
    have well-defined theoretical properties. With this development, efficient separability
    (decision surfaces) of nonlinear regions that use kernel functions was established.
    Nonlinear SVMs are quite frequently used for the classification of real (nonlinear)
    data.
  prefs: []
  type: TYPE_NORMAL
- en: SVM was initially known as a binary classifier that could be used for one-class
    classification of skewed or imbalanced class distribution. This unsupervised algorithm
    could effectively learn from the majority or normal class in a dataset to classify
    new data points as either *normal* or *outlier*. The process of identifying the
    minority or rarity class generally referred to as outlier is called **anomaly
    detection**, as the outlier is an anomaly and the rest of the data is normal.
    Classification involves fitting a model on the normal data (training examples)
    and predicting whether incoming new data is normal (inlier) or outlier. One-class
    SVM is most suited for a specific problem where the minority class does not have
    a consistent pattern or is a noisy instance, making it difficult for other classification
    algorithms to learn a decision boundary. The outliers in general are treated as
    deviations from normal.
  prefs: []
  type: TYPE_NORMAL
- en: In general, SVMs are effective in problems where the number of variables is
    greater than the number of records, meaning, in high-dimensional spaces. The algorithm
    uses a subset of training examples in the decision function, hence it is memory-efficient.
    It turns out the algorithm is versatile, as different kernels can be specified
    for the decision function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Support vectors in SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernels for SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn about support vectors and kernels in the forthcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Support vectors in SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVM is an algorithm that can produce significantly accurate results with less
    computation power. It is widely used in data classification tasks. If a dataset
    has *n* number of features, SVM finds a hyperplane in the *n*-dimensional space,
    which is also called the **decision boundary**, to classify the data points. An
    optimal decision boundary maximizes the distance between the boundary and instances
    in both classes. The distance between data points in the classes (shown in *Figure
    5**.1a*) is known as the **margin**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1a: Optimal hyperplane](img/Figure_05_01_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1a: Optimal hyperplane'
  prefs: []
  type: TYPE_NORMAL
- en: 'An SVM algorithm finds the optimal line in two dimensions or the optimal hyperplane
    in more than two dimensions that separates the space into classes. The optimal
    hyperplane or optimal line maximizes the margin (the distance between the data
    points of the two classes). In 3D (or more), data points become vectors and those
    (very small subset of training examples) that are closest to or on the hyperplanes
    (just outside the maximum margin) are called **support vectors** (see *Figure
    5**.1b*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1b: Support vectors](img/Figure_05_02_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1b: Support vectors'
  prefs: []
  type: TYPE_NORMAL
- en: If all support vectors are at the same distance from the optimal hyperplane,
    the margin is said to be good. The margin shown in *Figure 5**.1b* is bad, as
    support vectors in class +1 are very close to the optimal hyperplane, while those
    in class -1 are far away from it. Moving a support vector moves the decision boundary
    or hyperplane while moving other data points has no effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the number of features in an input dataset is two, the hyperplane is just
    a line. If the number is three, then the hyperplane (shown in *Figure 5**.2*)
    is a two-dimensional plane. The dimension of the decision boundary depends on
    the number of features, and the data points on either side of it (the hyperplane)
    belong to different classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Hyperplane in 3D feature space](img/Figure_05_03_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Hyperplane in 3D feature space'
  prefs: []
  type: TYPE_NORMAL
- en: 'Support vectors influence the position and orientation of the hyperplane. The
    margin of the classifier is maximized using support vectors. The margin is hard
    if the data is linearly separable. For most practical problems, data is not linearly
    separable, and in such cases the margin is soft. This allows for data points within
    the marginal distance (shown in *Figure 5**.3*) between two data class separators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Soft margin](img/Figure_05_04_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Soft margin'
  prefs: []
  type: TYPE_NORMAL
- en: It is better to have a large margin that might allow for some margin violation
    to occur. The larger the margin, the lower the error of the classifier. Maximizing
    the margin is equivalent to minimizing loss in machine learning algorithms. The
    function that helps maximize the margin is **hinge loss**. Hinge loss (error)
    is zero if data is classified correctly, meaning we have a hard margin as the
    points are not close to the hyperplane. Hinge loss is one if most of the data
    points are classified incorrectly. In general, support vectors are within the
    margin boundaries (soft margin) when the problem is not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, the kernel trick is introduced. Kernel is a technique used
    in SVMs to classify data points that are not linearly separable. Kernel functions
    enable operation in a high-dimensional feature space without computing data coordinates
    in that space, hence this operation is not computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Kernels for SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a kernel trick, a 2D space is converted into a 3D space using a mapping
    function such that the nonlinear data can be classified or separated in a higher
    dimension (see *Figure 5**.4*). The transformation of original data for mapping
    into the new space is done via kernel. The kernel function defines inner products
    (measure of similarity) in the transformed space.
  prefs: []
  type: TYPE_NORMAL
- en: The compute and storage requirements of SVMs increase with the number of training
    examples. The core of the algorithm is a quadratic programming problem separating
    support vectors from the training dataset. A linear kernel, which is just a dot
    product, is the fastest implementation of SVM. A few examples of linear and nonlinear
    kernels are shown in *Figure 5**.5a*. The most common nonlinear SVM kernels are
    **radial basis function** (**RBF**), **sigmoid**, and **polynomial**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively
    classified in higher dimension](img/Figure_05_05_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: (a) Example of non-linear separator (L), and (b) Data effectively
    classified in higher dimension'
  prefs: []
  type: TYPE_NORMAL
- en: SVMs are very effective for small datasets that are not linearly separable.
    Small data means that the number of features is more than the training size, due
    to which SVMs suffer from overfitting in some cases. The right kernel function
    and regularization (penalty function) come to the rescue in those cases. Each
    kernel has a different mathematical formulation, hence the set of parameters varies
    from one to another.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M),
    and polynomial kernel (R) functions](img/Figure_05_06_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5a: Data classification using linear kernel (L), RBF kernel (M), and
    polynomial kernel (R) functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter exponent (degree) in a polynomial kernel when set to 1 becomes
    a linear kernel and when set to 3 becomes a cubic kernel, an example of which
    is shown in *Figure 5**.5a* (rightmost). The sigmoid kernel cumulative distribution
    function goes from 0 to 1 to classify data and is mostly used as an activation
    function or perceptron in neural networks. An example of data classification by
    SVM with sigmoid kernel function is shown in *Figure 5**.5b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)](img/Figure_05_07_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5b: Sigmoid kernel (L), data classified using sigmoid (R)'
  prefs: []
  type: TYPE_NORMAL
- en: All SVM kernels have a parameter that trades off the misclassification of the
    dataset against the simplicity of the separator. While training an SVM with the
    RBF kernel, which is an exponential (![](img/Formula_05_001.png)) function, the
    parameter *a* is greater than zero and defines the influence of a training example
    on the separator. The selection of the parameters in respective kernel functions
    is critical to an SVM’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In imbalanced datasets, the parameters dedicated to providing weights on classes
    and samples become significant, as they might be required to give more importance
    to a certain sample or class in such cases. The effect of sample weighting on
    the class boundary is shown in *Figure 5**.6*, wherein the data point size is
    proportional to the sample weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Classification with constant sample weight (L), with modified
    weight (R)](img/Figure_05_08_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Classification with constant sample weight (L), with modified weight
    (R)'
  prefs: []
  type: TYPE_NORMAL
- en: While various methods and algorithms can detect outliers in a dataset, the kernel
    method used by the one-class SVM algorithm has been demonstrated in this chapter.
    Other examples include the decision tree ensemble method in the Isolation Forest
    algorithm, the distance or density method in the local outlier factor algorithm,
    and so on. Anomaly types can be *point* or *collective*, and one selects the algorithm
    for detection based on the anomaly type in a dataset. *Figures 5.7a and 5.7b*
    show examples of these anomaly types. A point anomaly is a global behavior while
    a collective anomaly is a local abnormal (non-normal) behavior. There can also
    be datasets wherein an anomaly can be entirely contextual, which most of the time
    is visible in time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7a: Examples of point anomalies](img/Figure_05_09_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7a: Examples of point anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 5.7b: Example of a collective (non-point) anomaly](img/Figure_05_10_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7b: Example of a collective (non-point) anomaly'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will implement a one-class SVM solution using Python,
    as this solution in general has proven to be useful for problems where the (point)
    outliers forming the minority class lack structure and are predominantly noisy
    examples (i.e. severe deviations from inliers).
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The one-class SVM algorithm does not use (ignores) the examples that are far
    from or deviated from the observations during training. Only the observations
    that are most concentrated or dense are leveraged for (unsupervised) learning
    and such an approach is effective in specific problems where very few deviations
    from normal are expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'A synthetic dataset is created to implement SVM. We will have about 2% of the
    synthetic data in the minority class (outliers) denoted by `1` and 98% in the
    majority class (inliers) denoted by `0`, and leverage the RBF kernel to map the
    data into a high-dimensional space. The Python code (with the scikit-learn library)
    runs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The report of the classifier (*Figure 5**.8*) clearly shows that the one-class
    SVM model has a recall of 23%, which means the model captures 23% of outliers.
    The F1-score is the harmonic mean of the two measures, namely precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: Classification report of one-class SVM](img/Figure_05_11_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Classification report of one-class SVM'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will visualize the outliers using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The default threshold of the algorithm for identifying these 2% outliers can
    also be customized so that fewer or more data points are labeled as outliers depending
    on the use case. What is evident from *Figure 5**.9* is that most of the outliers
    (yellow) have been detected correctly by the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Classification by one-class SVM](img/Figure_05_12_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Classification by one-class SVM'
  prefs: []
  type: TYPE_NORMAL
- en: One-class SVM is particularly useful as an anomaly detector and finds wide usage
    in sensor data captured from machines in the manufacturing industry.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored SVM as a classifier. In addition to linear data,
    SVMs can efficiently classify non-linear data using kernel functions. The method
    used by the SVM algorithm can be extended to solve regression problems. SVM is
    utilized for novelty detection as well, wherein the training dataset is not polluted
    with outliers and the algorithm is exploited to detect a new observation as an
    anomaly, in which case the outlier is called a novelty.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is about graph theory, a tool that provides the necessary mathematics
    to quantify and simplify complex systems. Graph theory is the study of relations
    (connections or edges) between a set of nodes or individual entities in a dynamic
    system. It is an integral component of ML and DL because graphs provide a means
    to represent a business problem as a mathematical programming task in the form
    of nodes and edges.
  prefs: []
  type: TYPE_NORMAL
