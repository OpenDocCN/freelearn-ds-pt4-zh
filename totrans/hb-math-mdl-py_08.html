<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer155">
<h1 class="chapter-number" id="_idParaDest-88"><a id="_idTextAnchor087"/>8</h1>
<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Markov Chain</h1>
<p>The Markov chain is one of the most important stochastic processes and solves real-world problems with probabilities. A Markov chain is a model of random movement in a discrete set of possible locations (states), in other words, a model of transition from one location (state) to another with a certain probability. It is named after Andrey Markov, the Russian mathematician who is best known for his work on stochastic processes. It is a mathematical system describing a sequence of events in which the probability of each event depends only on the <span class="No-Break">previous event.</span></p>
<p class="author-quote">“The future depends only upon the present, not upon the past.”</p>
<p>The events or states can be written as {<img alt="" height="24" src="image/Formula_08_001.png" width="95"/>, where <img alt="" height="22" src="image/Formula_08_002.png" width="22"/> is the state at time t. The process {} has a property, which is <img alt="" height="22" src="image/Formula_08_003.png" width="37"/> , which depends only on <img alt="" height="23" src="image/Formula_08_004.png" width="22"/> and does not depend on {<img alt="" height="24" src="image/Formula_08_005.png" width="120"/>. Such a process is called a Markovian or Markov chain. It is a random walk to traverse a system of states. A two-state Markov chain is one in which a state can transition onto itself (that is, staying in the same state). It is shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> (which is a state diagram). An example of a Markov chain is the PageRank algorithm, which is used by Google to determine the order of results for <span class="No-Break">a search.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 8.1: Two-state (A and E) Markov chain" height="346" src="image/Figure_08_01_B18943.jpg" width="255"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: Two-state (A and E) Markov chain</p>
<p>Markov chains are quite powerful when it comes to including real-world phenomena in computer simulations. It is a class of probabilistic graphical models representing a dynamic process, the limitation being that it can only take on a finite number of states. Markov chains have no long-term memory (are memory-less, in short) and hence know no past states. Therefore, the only state determining the future state in a Markov chain is the present, and this is called a <span class="No-Break">Markov property.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>Discrete-time <span class="No-Break">Markov chain</span></li>
<li><strong class="bold">Markov Chain Monte </strong><span class="No-Break"><strong class="bold">Carlo</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MCMC</strong></span><span class="No-Break">)</span></li>
</ul>
<p>The following section discusses the very foundation of a Markov chain, which is a discrete-time <span class="No-Break">stochastic process.</span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Discrete-time Markov chain</h1>
<p>For a discrete-time Markov process, <img alt="" height="27" src="image/Formula_08_006.png" width="64"/> while in continuous time <img alt="" height="13" src="image/Formula_08_007.png" width="14"/> is replaced by <img alt="" height="15" src="image/Formula_08_008.png" width="9"/> where <img alt="" height="15" src="image/Formula_08_009.png" width="9"/> runs until infinity. Given the present state, past and future states are independent in a Markov chain, which in turn means that the future is only dependent on the present. In the following subsections, we will learn about the transition matrix and an application of the Markov chain in time-series data for <span class="No-Break">short-term forecasting.</span></p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Transition probability</h2>
<p>The transition probabilities between Markov states are captured in a state transition matrix. The dimension of the transition matrix is determined by the number of states in the state space. Every state is included as a row and a column, and each cell in the matrix gives the probability of transition from its row’s state to its column’s state, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em>. In order to forecast one step ahead, one must know the transition matrix and the current state. The transition probability (matrix element) is typically established from historical <span class="No-Break">sequential data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 8.2: Transition matrix for the two states" height="174" src="image/Figure_08_02_B18943.jpg" width="424"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: Transition matrix for the two states</p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Application of the Markov chain</h2>
<p>Markov chains model the behavior of a random process. They can be used for text prediction in order to autocomplete sentences or to model the evolution of time-series data, for example, modeling the behavior of <span class="No-Break">financial markets.</span></p>
<p>An example of modeling the price of stock using a Markov chain is depicted in the following Python code. A set of states (in the order <strong class="source-inline">increase</strong>, <strong class="source-inline">decrease</strong>, and <strong class="source-inline">stable</strong>) is defined for the time evolution of the stock price with the probability of transition between these states. The transition matrix is used to predict the probable future (next <span class="No-Break">state) price:</span></p>
<pre class="source-code">
import numpy as np
states = ["increase", "decrease", "stable"] #Markov states
transition_probs = np.array([[0.6, 0.3, 0.1], [0.4, 0.4, 0.2], [0.5, 0.3, 0.2]])
num_steps = 10                 #time-steps for simulation
def MC_states(current_state):
     future_states = []
for i in range(num_steps):
           probs = transition_probs[states.index(current_state)]
           new_state = np.random.choice(states, p = probs)
           future_states.append(new_state)
           current_state = new_state #Update current state
     return future_states
#output
MC_states("increase")</pre>
<p>The output is a sequence of future states, shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em>, given a current state. A different output is obtained if the current state is set to <strong class="source-inline">decrease</strong> or <strong class="source-inline">stable</strong> (initial state) while executing the function in the code. The sequence of states depicts the evolution of the stock price over time. Caution must be exercised when the system does not exhibit stationary behavior, that is, the transition probabilities between states change over time. In that case, a complex Markov model or a different model altogether may be used to capture the <span class="No-Break">system's behavior.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="Figure 8.3: Output of the example code in Python" height="208" src="image/Figure_08_03_B18943.jpg" width="126"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: Output of the example code in Python</p>
<p>If <img alt="" height="16" src="image/Formula_08_010.png" width="16"/> is the number of times the sequence is in state <img alt="" height="18" src="image/Formula_08_011.png" width="8"/> (state is observed) and <img alt="" height="16" src="image/Formula_08_012.png" width="19"/> is the number of times there is a transition from state <em class="italic">i</em> to state <em class="italic">j</em>, then the transition probability is defined <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="" height="78" src="image/Formula_08_013.jpg" width="130"/>
</div>
</div>
<p>In the next section, we will learn about a sampling method, MCMC, which is used for high-dimensional probability distributions wherein the next sample is dependent on the current sample drawn randomly from a population. In short, the samples drawn from the distribution are probabilistically dependent on each other. The volume of a sample space increases exponentially with the number of parameters or dimensions, and modeling such a space could easily be inaccurate with the usage of straightforward methods such as Monte Carlo sampling. The MCMC method is an attempt to harness the properties of a random problem and construct the corresponding Markov <span class="No-Break">process efficiently.</span></p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Markov Chain Monte Carlo</h1>
<p>MCMC is a method of random sampling from a target population/distribution defined by high-dimensional probability definition. It is a large-scale statistical method that draws samples randomly from a complex probabilistic space to approximate the distribution of attributes over a range of future states. It helps gauge the distribution of a future outcome and the sample averages help approximate expectations. A Markov chain is a <strong class="bold">graph</strong> of states over which a sampling algorithm takes a <span class="No-Break">random walk.</span></p>
<p>The most known MCMC algorithm is perhaps Gibbs sampling. The algorithms are nothing but different methodologies for constructing the Markov chain. The most general MCMC algorithm is Metropolis-Hastings and has flexibility in many ways. These two algorithms will be discussed in the <span class="No-Break">next subsections.</span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Gibbs sampling algorithm</h2>
<p>In Gibbs sampling, the probability of the next sample in the Markov chain is calculated as the conditional probability of the prior sample. Samples in the Markov chain are constructed by changing one random variable at a time (conditioned on other variables in the distribution), meaning subsequent samples in the search space are closer. Gibbs sampling is most appropriate with a discrete (not continuous) distribution, which has a parametric form that allows sampling and calculating the conditional probability. An example of sampling with Gibbs sampler is shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em>, which reproduces the <span class="No-Break">desired distribution.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 8.4: Gibbs sampler reproducing a desired Gaussian mixture" height="408" src="image/Figure_08_04_B18943.jpg" width="681"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: Gibbs sampler reproducing a desired Gaussian mixture</p>
<p>A Gibbs sampler is more efficient than a Metropolis-Hastings algorithm (discussed in the next subsection). It starts with a proposal distribution and a proposal is always accepted; that is, the acceptance probability is always 1. We will use an example of the bivariate Gaussian<a id="_idIndexMarker225"/> distribution to illustrate a Gibbs sampler with Python code in the <span class="No-Break">last subsection.</span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Metropolis-Hastings algorithm</h2>
<p>The Metropolis-Hastings<a id="_idIndexMarker226"/> algorithm is used for probabilistic models where Gibbs sampling cannot be used. It does not assume that the state of the next sample can be generated from a target distribution, which is the main assumption in Gibbs sampling. This algorithm involves using a surrogate probability <a id="_idIndexMarker227"/>distribution, also called the kernel, and an acceptance criterion that helps decide whether the new sample can be accepted into the Markov chain or has to be rejected. The proposed distribution (surrogate) is suggestive of an arbitrary<a id="_idIndexMarker228"/> next sample and the acceptance criterion ensures an appropriate limiting direction in getting closer to the true or desired the state of the next sample. The starting points of these algorithms are important and different proposal distributions can <span class="No-Break">be explored.</span></p>
<p>How does this <a id="_idIndexMarker229"/><span class="No-Break">algorithm work?</span></p>
<ol>
<li>We start with a <span class="No-Break">random state.</span></li>
<li>Based on the proposal probability, we randomly pick a <span class="No-Break">new state.</span></li>
<li>We calculate the acceptance probability of the proposed <span class="No-Break">new state.</span></li>
</ol>
<p>For example, say the probability of a flipped coin landing on heads is the acceptance probability. If it lands on heads, we accept the sample; otherwise, we <span class="No-Break">reject it.</span></p>
<ol>
<li value="4">We repeat the process for a <span class="No-Break">long time.</span></li>
</ol>
<p>We discard the initial few samples as the chain does not reach its stationary state. The period before the chain reaches its stationary<a id="_idIndexMarker230"/> state is called the burn-in period (see <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.5</em>). The accepted draws will converge to the stationary distribution after <span class="No-Break">some time.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 8.5: Markov chain" height="193" src="image/Figure_08_05_B18943.jpg" width="584"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: Markov chain</p>
<p>The stationary distribution shows the probability of being at any state X at any given time and is always<a id="_idIndexMarker231"/> reached if a very large number of samples is generated. This distribution is exactly the posterior distribution we’re looking for. A posterior distribution is proportional to the product of likelihood and prior distribution. The Metropolis-Hastings <a id="_idIndexMarker232"/>algorithm is analogous to a diffusion process wherein all states are communicating (by design) and hence the system eventually settles into an equilibrium state, which is<a id="_idIndexMarker233"/> the same as converging to a stationary state. This property is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">ergodicity</strong></span><span class="No-Break">.</span></p>
<p>In the next subsection, we illustrate the Metropolis-Hastings sampling algorithm, also with Python code, using the example of <span class="No-Break">bivariate distribution.</span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Illustration of MCMC algorithms</h2>
<p>The working of the<a id="_idIndexMarker234"/> Gibbs sampling algorithm is shown with a simple bivariate Gaussian distribution in the following code. We pass the two parameters (mu and sigma) for the conditional probability distribution and discard a part of initially sampled values for the algorithm to converge even if the starting (guess) value is way off. This part of the sample is known <span class="No-Break">as burn-in:</span></p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
np.random.seed(42)
def gibbs_sampler(mus, sigmas, n_iter = 10000):
    samples = []
    y = mus[1]
    for _ in range(n_iter):
        x = p_x_y(y, mus, sigmas)
        y = p_y_x(x, mus, sigmas)
        samples.append([x, y])
    return samples
def p_x_y(y, mus, sigmas):
    mu = mus[0] + sigmas[1, 0]/sigmas[0, 0] * (y - mus[1])
    sigma = sigmas[0, 0]-sigmas[1, 0]/sigmas[1, 1]*sigmas[1, 0]
    return np.random.normal(mu, sigma)
def p_y_x(x, mus, sigmas):
    mu = mus[1] + sigmas[0, 1] / sigmas[1, 1]*(x - mus[0])
    sigma = sigmas[1, 1] - sigmas[0, 1]/sigmas[0, 0]*sigmas[0, 1]
    return np.random.normal(mu, sigma)
mus = np.asarray([5, 5])
sigmas = np.asarray([[1, 0.9], [0.9, 1]])
samples = gibbs_sampler(mus, sigmas)
burnin = 200
x = list(zip(*samples[burnin:]))[0]
y = list(zip(*samples[burnin:]))[1]
sns.jointplot(samples[burnin:], x = x, y = y, kind = 'kde')
sns.jointplot(samples[burnin:], x = x, y = y, kind = 'reg')
plt.show()</pre>
<p>We run the code and the Gibbs sampler yields an output, shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6a</em>, in two forms, namely, a kernel <a id="_idIndexMarker235"/>distribution estimation plot and a linear regression fit. The output is the resulting (target) distribution based on sampled values using the Gibbs <span class="No-Break">sampling algorithm.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 8.6a: Target distribution from the Gibbs sampling algorithm" height="1448" src="image/Figure_08_06_B18943.jpg" width="767"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6a: Target distribution from the Gibbs sampling algorithm</p>
<p>We run a similar setup (bivariate distribution) for Metropolis-Hastings sampler. The Python code and output are given as follows. To begin with, we plot the true distribution and then use the multivariate normal distribution as the proposal. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6b</em> is the output (target distribution) based on sampling using <span class="No-Break">the algorithm:</span></p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm as tqdm
def density(z):
    z = np.reshape(z, [z.shape[0], 2])
    z1, z2 = z[:, 0], z[:, 1]
    norm = np.sqrt(z1 ** 2 + z2 ** 2)
    exp1 = np.exp(-0.5 * ((z1 - 2) / 0.6) ** 2)
    exp2 = np.exp(-0.5 * ((z1 + 2) / 0.6) ** 2)
    v = 0.5 * ((norm - 2) / 0.4) ** 2 – np.log(exp1 + exp2)
    return np.exp(-v)
r = np.linspace(-5, 5, 1000)
z = np.array(np.meshgrid(r, r)).transpose(1, 2, 0)
z = np.reshape(z, [z.shape[0] * z.shape[1], -1])
def metropolis_sampler(target_density, size = 100000):
    burnin = 5000
    size += burnin
    x0 = np.array([[0, 0]])
    xt = x0
    samples = []
    for i in tqdm(range(size)):
        xt_candidate = np.array([np.random.multivariate_normal(xt[0], np.eye(2))])
      accept_prob = (target_density(xt_candidate))/(target_density(xt))
      if np.random.uniform(0, 1) &lt; accept_prob:
         xt = xt_candidate
      samples.append(xt)
    samples = np.array(samples[burnin:])
     samples = np.reshape(samples, [samples.shape[0], 2])
    return samples
q = density(z) #true density
plt.hexbin(z[:,0], z[:,1], C = q.squeeze())
plt.gca().set_aspect('equal', adjustable ='box')
plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.show()
samples = metropolis_sampler(density)
plt.hexbin(samples[:,0], samples[:,1])
plt.gca().set_aspect('equal', adjustable = 'box')
plt.xlim([-3, 3])
plt.ylim([-3, 3])
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 8.6b: True distribution (L) and target distribution (R) from the Metropolis-Hastings sampling algorithm" height="465" src="image/Figure_08_07_B18943.jpg" width="937"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6b: True distribution (L) and target distribution (R) from the Metropolis-Hastings sampling algorithm</p>
<p>For finite (discrete as<a id="_idIndexMarker236"/> well as continuous) state spaces, the existence of a unique stationary state is guaranteed. We start from a prior probability distribution and end with a stationary distribution, which is the posterior or target distribution based on <span class="No-Break">sampled values.</span></p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Summary</h1>
<p>In this chapter, we learned about the Markov chain, which is utilized to model special types of stochastic processes, such as problems wherein one can assume the entire past is encoded in the present, which in turn can be leveraged to determine the next (future) state. An application of the Markov chain in modeling time-series data was illustrated. The most common MCMC algorithm (Metropolis-Hastings) for sampling was also covered with code to illustrate. If a system exhibits non-stationary behavior (transition probability changes with time), then a Markov chain is not the appropriate model and a more complex model may be required to capture the behavior of the <span class="No-Break">dynamic system.</span></p>
<p>With this chapter, we conclude the second part of the book. In the next chapter, we will explore fundamental optimization techniques, some of which are used in machine learning. We will touch upon evolutionary optimization, optimization in operations research, and that are leveraged in training <span class="No-Break">neural networks.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer156">
<h1 id="_idParaDest-98" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor097"/>Part 3:Mathematical Optimization</h1>
<p>In this part, you will have exposure to optimization techniques that lay the foundation for machine learning, deep learning, and other models used in operations research. Optimization techniques are extremely powerful for predictive and prescriptive analytics and find applications in several complex problems in heavy industry. Additionally, blending classical mathematical modeling with machine learning often allows for the extraction of more meaningful insights for specific sensitive <span class="No-Break">business problems.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18943_09.xhtml#_idTextAnchor098"><em class="italic">Chapter 9</em></a>, <em class="italic">Exploring Optimization Techniques</em></li>
<li><a href="B18943_10.xhtml#_idTextAnchor107"><em class="italic">Chapter 10</em></a>, <em class="italic">Optimization Techniques for Machine Learning</em> </li>
</ul>
</div>
<div>
<div id="_idContainer157">
</div>
</div>
<div>
<div id="_idContainer158">
</div>
</div>
<div>
<div id="_idContainer159">
</div>
</div>
<div>
<div id="_idContainer160">
</div>
</div>
<div>
<div id="_idContainer161">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer162">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer163">
</div>
</div>
<div>
<div id="_idContainer164">
</div>
</div>
<div>
<div id="_idContainer165">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer166">
</div>
</div>
</div></body></html>