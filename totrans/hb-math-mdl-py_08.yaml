- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Markov Chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Markov chain is one of the most important stochastic processes and solves
    real-world problems with probabilities. A Markov chain is a model of random movement
    in a discrete set of possible locations (states), in other words, a model of transition
    from one location (state) to another with a certain probability. It is named after
    Andrey Markov, the Russian mathematician who is best known for his work on stochastic
    processes. It is a mathematical system describing a sequence of events in which
    the probability of each event depends only on the previous event.
  prefs: []
  type: TYPE_NORMAL
- en: “The future depends only upon the present, not upon the past.”
  prefs: []
  type: TYPE_NORMAL
- en: The events or states can be written as {![](img/Formula_08_001.png), where ![](img/Formula_08_002.png)
    is the state at time t. The process {} has a property, which is ![](img/Formula_08_003.png)
    , which depends only on ![](img/Formula_08_004.png) and does not depend on {![](img/Formula_08_005.png).
    Such a process is called a Markovian or Markov chain. It is a random walk to traverse
    a system of states. A two-state Markov chain is one in which a state can transition
    onto itself (that is, staying in the same state). It is shown in *Figure 8**.1*
    (which is a state diagram). An example of a Markov chain is the PageRank algorithm,
    which is used by Google to determine the order of results for a search.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Two-state (A and E) Markov chain](img/Figure_08_01_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Two-state (A and E) Markov chain'
  prefs: []
  type: TYPE_NORMAL
- en: Markov chains are quite powerful when it comes to including real-world phenomena
    in computer simulations. It is a class of probabilistic graphical models representing
    a dynamic process, the limitation being that it can only take on a finite number
    of states. Markov chains have no long-term memory (are memory-less, in short)
    and hence know no past states. Therefore, the only state determining the future
    state in a Markov chain is the present, and this is called a Markov property.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete-time Markov chain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markov Chain Monte** **Carlo** (**MCMC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section discusses the very foundation of a Markov chain, which
    is a discrete-time stochastic process.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete-time Markov chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a discrete-time Markov process, ![](img/Formula_08_006.png) while in continuous
    time ![](img/Formula_08_007.png) is replaced by ![](img/Formula_08_008.png) where
    ![](img/Formula_08_009.png) runs until infinity. Given the present state, past
    and future states are independent in a Markov chain, which in turn means that
    the future is only dependent on the present. In the following subsections, we
    will learn about the transition matrix and an application of the Markov chain
    in time-series data for short-term forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Transition probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transition probabilities between Markov states are captured in a state transition
    matrix. The dimension of the transition matrix is determined by the number of
    states in the state space. Every state is included as a row and a column, and
    each cell in the matrix gives the probability of transition from its row’s state
    to its column’s state, as shown in *Figure 8**.2*. In order to forecast one step
    ahead, one must know the transition matrix and the current state. The transition
    probability (matrix element) is typically established from historical sequential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: Transition matrix for the two states](img/Figure_08_02_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Transition matrix for the two states'
  prefs: []
  type: TYPE_NORMAL
- en: Application of the Markov chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Markov chains model the behavior of a random process. They can be used for text
    prediction in order to autocomplete sentences or to model the evolution of time-series
    data, for example, modeling the behavior of financial markets.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of modeling the price of stock using a Markov chain is depicted
    in the following Python code. A set of states (in the order `increase`, `decrease`,
    and `stable`) is defined for the time evolution of the stock price with the probability
    of transition between these states. The transition matrix is used to predict the
    probable future (next state) price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output is a sequence of future states, shown in *Figure 8**.3*, given a
    current state. A different output is obtained if the current state is set to `decrease`
    or `stable` (initial state) while executing the function in the code. The sequence
    of states depicts the evolution of the stock price over time. Caution must be
    exercised when the system does not exhibit stationary behavior, that is, the transition
    probabilities between states change over time. In that case, a complex Markov
    model or a different model altogether may be used to capture the system's behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Output of the example code in Python](img/Figure_08_03_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Output of the example code in Python'
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![](img/Formula_08_010.png) is the number of times the sequence is in state
    ![](img/Formula_08_011.png) (state is observed) and ![](img/Formula_08_012.png)
    is the number of times there is a transition from state *i* to state *j*, then
    the transition probability is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_08_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will learn about a sampling method, MCMC, which is used
    for high-dimensional probability distributions wherein the next sample is dependent
    on the current sample drawn randomly from a population. In short, the samples
    drawn from the distribution are probabilistically dependent on each other. The
    volume of a sample space increases exponentially with the number of parameters
    or dimensions, and modeling such a space could easily be inaccurate with the usage
    of straightforward methods such as Monte Carlo sampling. The MCMC method is an
    attempt to harness the properties of a random problem and construct the corresponding
    Markov process efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Markov Chain Monte Carlo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MCMC is a method of random sampling from a target population/distribution defined
    by high-dimensional probability definition. It is a large-scale statistical method
    that draws samples randomly from a complex probabilistic space to approximate
    the distribution of attributes over a range of future states. It helps gauge the
    distribution of a future outcome and the sample averages help approximate expectations.
    A Markov chain is a **graph** of states over which a sampling algorithm takes
    a random walk.
  prefs: []
  type: TYPE_NORMAL
- en: The most known MCMC algorithm is perhaps Gibbs sampling. The algorithms are
    nothing but different methodologies for constructing the Markov chain. The most
    general MCMC algorithm is Metropolis-Hastings and has flexibility in many ways.
    These two algorithms will be discussed in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Gibbs sampling algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Gibbs sampling, the probability of the next sample in the Markov chain is
    calculated as the conditional probability of the prior sample. Samples in the
    Markov chain are constructed by changing one random variable at a time (conditioned
    on other variables in the distribution), meaning subsequent samples in the search
    space are closer. Gibbs sampling is most appropriate with a discrete (not continuous)
    distribution, which has a parametric form that allows sampling and calculating
    the conditional probability. An example of sampling with Gibbs sampler is shown
    in *Figure 8**.4*, which reproduces the desired distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Gibbs sampler reproducing a desired Gaussian mixture](img/Figure_08_04_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Gibbs sampler reproducing a desired Gaussian mixture'
  prefs: []
  type: TYPE_NORMAL
- en: A Gibbs sampler is more efficient than a Metropolis-Hastings algorithm (discussed
    in the next subsection). It starts with a proposal distribution and a proposal
    is always accepted; that is, the acceptance probability is always 1\. We will
    use an example of the bivariate Gaussian distribution to illustrate a Gibbs sampler
    with Python code in the last subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Metropolis-Hastings algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Metropolis-Hastings algorithm is used for probabilistic models where Gibbs
    sampling cannot be used. It does not assume that the state of the next sample
    can be generated from a target distribution, which is the main assumption in Gibbs
    sampling. This algorithm involves using a surrogate probability distribution,
    also called the kernel, and an acceptance criterion that helps decide whether
    the new sample can be accepted into the Markov chain or has to be rejected. The
    proposed distribution (surrogate) is suggestive of an arbitrary next sample and
    the acceptance criterion ensures an appropriate limiting direction in getting
    closer to the true or desired the state of the next sample. The starting points
    of these algorithms are important and different proposal distributions can be
    explored.
  prefs: []
  type: TYPE_NORMAL
- en: How does this algorithm work?
  prefs: []
  type: TYPE_NORMAL
- en: We start with a random state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the proposal probability, we randomly pick a new state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate the acceptance probability of the proposed new state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, say the probability of a flipped coin landing on heads is the acceptance
    probability. If it lands on heads, we accept the sample; otherwise, we reject
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the process for a long time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We discard the initial few samples as the chain does not reach its stationary
    state. The period before the chain reaches its stationary state is called the
    burn-in period (see *Figure 8**.5*). The accepted draws will converge to the stationary
    distribution after some time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Markov chain](img/Figure_08_05_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Markov chain'
  prefs: []
  type: TYPE_NORMAL
- en: The stationary distribution shows the probability of being at any state X at
    any given time and is always reached if a very large number of samples is generated.
    This distribution is exactly the posterior distribution we’re looking for. A posterior
    distribution is proportional to the product of likelihood and prior distribution.
    The Metropolis-Hastings algorithm is analogous to a diffusion process wherein
    all states are communicating (by design) and hence the system eventually settles
    into an equilibrium state, which is the same as converging to a stationary state.
    This property is called **ergodicity**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we illustrate the Metropolis-Hastings sampling algorithm,
    also with Python code, using the example of bivariate distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Illustration of MCMC algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The working of the Gibbs sampling algorithm is shown with a simple bivariate
    Gaussian distribution in the following code. We pass the two parameters (mu and
    sigma) for the conditional probability distribution and discard a part of initially
    sampled values for the algorithm to converge even if the starting (guess) value
    is way off. This part of the sample is known as burn-in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We run the code and the Gibbs sampler yields an output, shown in *Figure 8**.6a*,
    in two forms, namely, a kernel distribution estimation plot and a linear regression
    fit. The output is the resulting (target) distribution based on sampled values
    using the Gibbs sampling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6a: Target distribution from the Gibbs sampling algorithm](img/Figure_08_06_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6a: Target distribution from the Gibbs sampling algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'We run a similar setup (bivariate distribution) for Metropolis-Hastings sampler.
    The Python code and output are given as follows. To begin with, we plot the true
    distribution and then use the multivariate normal distribution as the proposal.
    *Figure 8**.6b* is the output (target distribution) based on sampling using the
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.6b: True distribution (L) and target distribution (R) from the Metropolis-Hastings
    sampling algorithm](img/Figure_08_07_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6b: True distribution (L) and target distribution (R) from the Metropolis-Hastings
    sampling algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: For finite (discrete as well as continuous) state spaces, the existence of a
    unique stationary state is guaranteed. We start from a prior probability distribution
    and end with a stationary distribution, which is the posterior or target distribution
    based on sampled values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the Markov chain, which is utilized to model
    special types of stochastic processes, such as problems wherein one can assume
    the entire past is encoded in the present, which in turn can be leveraged to determine
    the next (future) state. An application of the Markov chain in modeling time-series
    data was illustrated. The most common MCMC algorithm (Metropolis-Hastings) for
    sampling was also covered with code to illustrate. If a system exhibits non-stationary
    behavior (transition probability changes with time), then a Markov chain is not
    the appropriate model and a more complex model may be required to capture the
    behavior of the dynamic system.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, we conclude the second part of the book. In the next chapter,
    we will explore fundamental optimization techniques, some of which are used in
    machine learning. We will touch upon evolutionary optimization, optimization in
    operations research, and that are leveraged in training neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3:Mathematical Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will have exposure to optimization techniques that lay the
    foundation for machine learning, deep learning, and other models used in operations
    research. Optimization techniques are extremely powerful for predictive and prescriptive
    analytics and find applications in several complex problems in heavy industry.
    Additionally, blending classical mathematical modeling with machine learning often
    allows for the extraction of more meaningful insights for specific sensitive business
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18943_09.xhtml#_idTextAnchor098), *Exploring Optimization Techniques*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18943_10.xhtml#_idTextAnchor107), *Optimization Techniques
    for Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
