<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer176">
<h1 class="chapter-number" id="_idParaDest-99"><a id="_idTextAnchor098"/>9</h1>
<h1 id="_idParaDest-100"><a id="_idTextAnchor099"/> Exploring Optimization Techniques</h1>
<p>This chapter primarily aims to address the question, “Why is optimization necessary while solving problems?” Mathematical optimization, or mathematical programming, is a powerful decision-making tool that has been talked about in depth in the chapters of Part I. What is important is to recall the simple fact that optimization yields the best result to a problem by reducing errors that are, essentially, the gaps between predicted and real data. Optimization comes at a cost; almost all optimization problems are described in terms of costs such as money, time, and resources. This cost function is the error function. If a business problem has clear goals and constraints, such as in the airline and logistics industries, mathematical optimization is applied for <span class="No-Break">efficient decision-making.</span></p>
<p>In <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) problems, the cost is often called the <strong class="bold">loss function</strong>. ML models make predictions about trends or classify data wherein training a model is a process of optimization, as each iteration in this process aims to improve the accuracy of the model and lower the margin of error. Selecting the optimum value of hyperparameters is key to ensuring an accurately and efficiently performing model. Hyperparameters are the elements of an ML model (for example, learning rate, number of clusters, etc.) that are tuned to fit a specific dataset to the model. In short, they are parameters whose values control the learning process. Optimization is an iterative process, meaning that the ML model becomes more accurate with each iteration in most cases and becomes better at predicting an outcome or <span class="No-Break">classifying data.</span></p>
<p>The right blend of ML and mathematical optimization can prove to be useful for certain business problems. For example, the output of an ML model can determine the scope of an optimization model, especially in routing problems where one uses both predictive maintenance with ML as well as clustering, the results of which are fed into a mathematical model to create optimal routes. Similarly, an ML model may learn from a mathematical model. Initial values of decision variables obtained from a mathematical model can be used in an ML model that not only predicts optimal values of the decision variables, but also helps accelerate the performance of an <span class="No-Break">optimization algorithm.</span></p>
<p>ML optimization is performed using algorithms that exploit a range of techniques to refine an ML model. The optimization process searches for the most effective configuration or set of hyperparameters for the model to suit the specific use case (dataset) or <span class="No-Break">business problem.</span></p>
<p>To summarize, ML is data-driven and optimization is algorithm-driven. Every ML model operates on the principle of minimizing the cost function; hence, optimization is a superset at <span class="No-Break">its core.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>Optimizing machine <span class="No-Break">learning models</span></li>
<li>Optimization in <span class="No-Break">operations research</span></li>
<li><span class="No-Break">Evolutionary optimization</span></li>
</ul>
<p>The next section explores approaches and techniques used in optimizing ML models to arrive at the best set <span class="No-Break">of hyperparameters.</span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Optimizing machine learning models</h1>
<p>The concept of optimization is integral to an ML model. ML helps make clusters, detect anomalies, predict the future from historical data, and so forth. However, when it comes to minimizing costs in a business, finding optimal placement of business facilities, et cetera, what we need is a mathematical <span class="No-Break">optimization model.</span></p>
<p>We will talk about optimization in machine learning in this section. Optimization ensures that the structure and configuration of the ML model are as effective as possible to achieve the goal it has been built for. Optimization techniques automate the testing of different model configurations. The best configuration (set of hyperparameters) has the lowest margin of error, thereby yielding the most accurate model for a given dataset. Getting the hyperparameter optimization right for an ML model can be tedious, as both under-optimized (underfit) as well as over-optimized (overfit) models fail. Overfitting is when a model is trained too closely to training data, resulting in inaccurate yields with new data. Underfitting is when a model is poorly trained, making it ineffective with training data as well as new data. Hyperparameters can be sought manually, which is an exhaustive method using trial and error. Underfit, optimal, and overfit models are illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 9.1: Under-optimized (L) and over-optimized (R) model fits" height="292" src="image/Figure_09_01_B18943.jpg" width="742"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1: Under-optimized (L) and over-optimized (R) model fits</p>
<p>The main techniques of optimization include <strong class="bold">random search</strong>, <strong class="bold">grid search</strong> of hyperparameters, and <strong class="bold">Bayesian optimization</strong>, all of which are discussed in the <span class="No-Break">following subsections.</span></p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Random search</h2>
<p>The process of random sampling of the <a id="_idIndexMarker237"/><a id="_idIndexMarker238"/>search space and identifying the most effective configuration of a hyperparameter set is random search. A random search technique discovers new combinations of hyperparameters for an optimized ML model. The <a id="_idIndexMarker239"/><a id="_idIndexMarker240"/>number of iterations in the search process has to be set, which limits these new combinations, without which the process is a lot more time-consuming. It is an efficient process as it replaces an exhaustive search with randomness. A search space can be thought of as a volume in space, each dimension of which represents a hyperparameter, and each point or vector in the volume represents a model configuration. An optimization procedure involves defining the <span class="No-Break">search space.</span></p>
<p>The search space is a dictionary in the Python code, and the <strong class="source-inline">scikit-learn</strong> library provides functions to tune model hyperparameters. An example code of a random search for a classification model is <span class="No-Break">provided here:</span></p>
<pre class="source-code">
import pandas as pd
from scipy.stats import loguniform
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import RandomizedSearchCV
dataframe = pd.read_csv('sonar.csv')
data = dataframe.values
X, y = data[:, :-1], data[:, -1]
#Model
model = LogisticRegression()
cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
#Define search space
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']
space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
space['C'] = loguniform(1e-5, 100)
search = RandomizedSearchCV(model, space, n_iter = 500, scoring = 'accuracy',
                            n_jobs = -1, cv = cv, random_state = 1)
result = search.fit(X, y)
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)</pre>
<p>The dataset used is a set of 60 patterns obtained by bouncing sonar signals off a metal cylinder under various conditions. Each pattern is a set <a id="_idIndexMarker241"/><a id="_idIndexMarker242"/>of numbers lying in the range between 0.0 and 1.0, with each number representing the energy within a frequency band integrated over a period of time. The label associated with each record is either <em class="italic">R</em> if the object is a rock, or <em class="italic">M</em> if the object is a metal cylinder or mine. Data can be found in the GitHub repository <span class="No-Break">at </span><a href="https://github.com/ranja-sarkar/dataset"><span class="No-Break">https://github.com/ranja-sarkar/dataset</span></a><span class="No-Break">.</span></p>
<p>An example code of random search<a id="_idIndexMarker243"/><a id="_idIndexMarker244"/> for a linear regression model has also been provided. The insurance dataset with two variables, namely the number of claims and total payment (in Swedish Krona) for all claims in geographical Swedish zones, can be found in the GitHub repository <span class="No-Break">at </span><a href="https://github.com/ranja-sarkar/dataset"><span class="No-Break">https://github.com/ranja-sarkar/dataset</span></a><span class="No-Break">.</span></p>
<p>The difference between regression and classification tasks is in choosing the performance scoring protocol for the models. The hyperparameter optimization methods in the <strong class="source-inline">scikit-learn</strong> Python library assume good performance scores are negative values close to zero (for regression), with zero representing a perfect <span class="No-Break">regression model:</span></p>
<pre class="source-code">
import pandas as pd
from scipy.stats import loguniform
from sklearn.linear_model import Ridge
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import RandomizedSearchCV
df = pd.read_csv('auto-insurance.csv')
data = df.values
X, y = data[:, :-1], data[:, -1]
#Model
model = Ridge()
cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
#Define search space
space = dict()
space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']
space['alpha'] = loguniform(1e-5, 100)
space['fit_intercept'] = [True, False]
space['normalize'] = [True, False]
search = RandomizedSearchCV(model, space, n_iter = 500, scoring =      'neg_mean_absolute_error', n_jobs = -1, cv = cv, random_state = 1)
result = search.fit(X, y)
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)</pre>
<p>The runtime of the code depends on the size of the search space and the system processor speed. The <strong class="source-inline">result</strong> class in the code provides the <a id="_idIndexMarker245"/><a id="_idIndexMarker246"/>outcome, the<a id="_idIndexMarker247"/><a id="_idIndexMarker248"/> most important value being the best score for the best performance of the model and the hyperparameters that achieved this score. Once the best set of hyperparameters becomes known, one can define a new model, set the hyperparameters to the known values, and fit the model on available data. This model can then be used for predictions on new data. The number of random configurations in the parameter space look like <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, which shows that random search works best for <span class="No-Break">low-dimensional data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="Figure 9.2: Random search" height="403" src="image/Figure_09_02_B18943.jpg" width="399"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2: Random search</p>
<p>In the next subsection, we elaborate on grid search for optimization of classification and <span class="No-Break">regression models.</span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Grid search</h2>
<p>The process of assessing the<a id="_idIndexMarker249"/><a id="_idIndexMarker250"/> effectiveness of known hyperparameter values of an ML model is grid search. Each hyperparameter is represented as a dimension on a grid across the search space and each point in the grid is searched and evaluated. Grid search is great for <a id="_idIndexMarker251"/><a id="_idIndexMarker252"/>checking intuitive guesses and hyperparameter combinations that are known to perform well in general. As mentioned earlier, an optimization procedure involves defining a search space (a dictionary in Python), which can be thought of as a volume where each dimension represents a hyperparameter and each point (vector) represents a model configuration. A discrete grid has to be defined here. In other words, the grid search space takes discrete values (that can be on a log scale) instead of a log-uniform distribution used in a random <span class="No-Break">search space.</span></p>
<p>A sample code of grid search for a classification model using the same dataset (<strong class="source-inline">sonar.csv</strong>) explored for a random search algorithm is <span class="No-Break">given here:</span></p>
<pre class="source-code">
import pandas as pd
from scipy.stats import loguniform
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
dataframe = pd.read_csv('sonar.csv')
data = dataframe.values
X, y = data[:, :-1], data[:, -1]
#Model
model = LogisticRegression()
cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
#Define search space
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']
space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
search = GridSearchCV(model, space, scoring = 'accuracy', n_jobs = -1, cv = cv)
result = search.fit(X, y)
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)</pre>
<p>A sample code of a grid search for a linear regression model using the same dataset (<strong class="source-inline">auto-insurance.csv</strong>) explored for a random search algorithm is provided as follows. The best hyperparameters obtained using the random search and grid search <a id="_idIndexMarker253"/><a id="_idIndexMarker254"/>algorithms for this<a id="_idIndexMarker255"/><a id="_idIndexMarker256"/> dataset can be compared to get an estimate of which algorithm works better for <span class="No-Break">the dataset:</span></p>
<pre class="source-code">
import pandas as pd
from sklearn.linear_model import Ridge
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
df = pd.read_csv('auto-insurance.csv')
data = df.values
X, y = data[:, :-1], data[:, -1]
#Model
model = Ridge()
cv = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
#Define search space
space = dict()
space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']
space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
space['fit_intercept'] = [True, False]
space['normalize'] = [True, False]
search = GridSearchCV(model, space, scoring = 'neg_mean_absolute_error', n_jobs = -1, cv = cv)
result = search.fit(X, y)
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)</pre>
<p>The scores obtained for the datasets<a id="_idIndexMarker257"/><a id="_idIndexMarker258"/> with random search and grid search in classification and regression models are nearly identical. The selection of optimization technique for a given dataset depends on the use case. Though random search might in some cases result in better performance, it needs more time, while grid search is appropriate for quick searches of hyperparameters that perform well in general. The values of hyperparameters are placed like a matrix as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em>, similar to <span class="No-Break">a grid:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="Figure 9.3: Grid search" height="408" src="image/Figure_09_03_B18943.jpg" width="402"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3: Grid search</p>
<p>Another method, known as Bayesian <a id="_idIndexMarker259"/><a id="_idIndexMarker260"/>optimization, whose search<a id="_idIndexMarker261"/><a id="_idIndexMarker262"/> procedure is different from the preceding two, is discussed in the <span class="No-Break">following subsection.</span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>Bayesian optimization</h2>
<p>A directed and iterative approach to global <a id="_idIndexMarker263"/><a id="_idIndexMarker264"/>optimization using probability is Bayesian optimization. This is a Gaussian process that converges fast for continuous hyperparameters that is, in a continuous search space (<span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em>). In Bayesian optimization, a probabilistic model of the function is built, and maps hyperparameters to the objectives <a id="_idIndexMarker265"/><a id="_idIndexMarker266"/>evaluated on a validation dataset. This process evaluates a hyperparameter configuration based on the current model, then updates it until an optimal point is reached and it attempts to find the global optimum in a minimum number of steps. In most cases, it is more efficient and effective than optimization by way of random search. The optimization landscape (multiple local minima) with one global minimum is illustrated <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="Figure 9.4: Optimization landscape (response surface)" height="443" src="image/Figure_09_04_B18943.jpg" width="588"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4: Optimization landscape (response surface)</p>
<p>Bayesian optimization incorporates prior belief (marginal probability) about the objective function and updates the prior with samples drawn from the function to obtain a posterior belief (conditional probability) that better approximates the function, which is illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em>. This process repeats itself until the extremum of the objective <a id="_idIndexMarker267"/><a id="_idIndexMarker268"/>function is located or resources <span class="No-Break">are exhausted:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="Figure 9.5: Bayesian statistics" height="568" src="image/Figure_09_05_B18943.jpg" width="582"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5: Bayesian statistics</p>
<p>Bayesian search is typically beneficial when there<a id="_idIndexMarker269"/><a id="_idIndexMarker270"/> is a large amount of data, the learning is slow, and tuning time has to be minimized. The <strong class="source-inline">scikit-optimize</strong> library provides functions for Bayesian optimization of ML models. A sample code for hyperparameter tuning by the Bayesian method in a classification problem is provided <span class="No-Break">as follows:</span></p>
<pre class="source-code">
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from skopt.space import Integer
from skopt.utils import use_named_args
from skopt import gp_minimize
#Generate classification dataset
X, y = make_blobs(n_samples = 500, centers = 3, n_features = 2) ##3 class labels in data
#Model kNN
model = KNeighborsClassifier()
#Define search space
search_space = [Integer(1, 5, name = 'n_neighbors'), Integer(1, 2, name = 'p')]
@use_named_args(search_space)
def evaluate_model(**params):
    model.set_params(**params)
    result = cross_val_score(model, X, y, cv = 5, n_jobs = -1, scoring = 'accuracy')
    estimate = np.mean(result)
    return 1.0 – estimate
#Optimize
result = gp_minimize(evaluate_model, search_space)
print('Accuracy: %.3f' % (1.0 - result.fun))
print('Best Parameters: n_neighbors = %d, p = %d' % (result.x[0], result.x[1]))</pre>
<p>The model used for approximating the objective function is called the <strong class="bold">surrogate model</strong>, and the posterior probability is a surrogate objective <a id="_idIndexMarker271"/><a id="_idIndexMarker272"/>function that can <a id="_idIndexMarker273"/><a id="_idIndexMarker274"/>be used to estimate the cost of candidate samples. The posterior is used to select the next sample from the search space and the technique that does this is called the <strong class="bold">acquisition function</strong>. Bayesian <a id="_idIndexMarker275"/><a id="_idIndexMarker276"/>optimization is best when the function evaluation is expensive or the form of the objective function is complex (nonlinear, non-convex, highly multi-dimensional, or highly noisy) – for example, in deep <span class="No-Break">neural networks.</span></p>
<p>The process of optimization lowers errors or <a id="_idIndexMarker277"/><a id="_idIndexMarker278"/>loss from predictions in an ML model and improves the model’s accuracy. The very premise of ML relies on a form of function optimization so inputs can be almost accurately mapped to <span class="No-Break">expected outputs.</span></p>
<p>In the next section, we will learn about mathematical optimization in <span class="No-Break">operations research.</span></p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Optimization in operations research</h1>
<p>The term <strong class="bold">operations research</strong> was coined during <a id="_idIndexMarker279"/>World War I, when the British military brought together a group of scientists to allocate insufficient resources (food, medicines, etc.) in the most effective way possible to different military operations. Therefore, the term implies optimization, which is maximizing or minimizing an objective function subject to constraints, often in complex problems and in high dimensions. Operations problems typically include planning work shifts or creating a schedule for large organizations, designing facilities for customers at a large store, choosing investments for available funds, supply chain management, and inventory management, all of which can be posed or formulated as mathematical problems with a collection of variables and <span class="No-Break">their relationships.</span></p>
<p>In operations research, a business problem is mapped to a lower-level generic problem that is concise enough to be described in mathematical notations. These generic problems can in turn be expressed using higher-level languages; for example, resources and activities are used to describe a scheduling problem. The higher-level language is problem-specific, hence the generic problems can be described using modeling paradigms. A modeling paradigm is a set of rules and practices that allows for the representation of higher-level problems using lower-level data structures such as matrices. These data structures or matrices are passed to the last step of abstraction, which is algorithms. The most prominent modeling paradigms are linear programming, integer programming, and mixed-integer programming, all of which use linear equality constraints. There is a family of algorithms to solve these linear programming problems. Search algorithms, such as branch and bound, solve integer programming problems, while the simplex algorithm is used in a linear programming <span class="No-Break">modeling paradigm.</span></p>
<p>An example of how to solve a knapsack problem by optimization is illustrated with the following data (<em class="italic">Figures 9.6a </em><span class="No-Break"><em class="italic">and 9.6b</em></span><span class="No-Break">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<img alt="Figure 9.6a: Knapsack problem" height="271" src="image/Figure_09_06_B18943.jpg" width="1006"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6a: Knapsack problem</p>
<p>Let’s say the constraint is the ability to only carry a maximum of 2.9 kg in the camping sack, while the total weight of all items is 3.09 kg. The item’s value assists in choosing the optimum number of items. As the number of items increases, the problem becomes bigger, and<a id="_idIndexMarker280"/> solving it by trying all possible combinations of items takes a significant amount <span class="No-Break">of time:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<img alt="Figure 9.6b: Knapsack problem with another variable" height="184" src="image/Figure_09_07_B18943.jpg" width="919"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6b: Knapsack problem with another variable</p>
<p>The objective function is value, which must be maximized. The best of items has to be chosen to meet the constraint of 2.9 kg by total weight. A solver (pulp, in this case) is used to solve this linear programming problem, as shown in the following code. The decision variables (to be determined) are given by <img alt="" height="24" src="image/Formula_09_001.png" width="83"/>. The variable is 1 if the item is chosen and 0 if the item is <span class="No-Break">not chosen:</span></p>
<pre class="source-code">
from pulp import *
#value per weight
v = {'Sleeping bag': 4.17, 'Pillow': 5.13, 'Torch': 10.0, 'First Aid Kit': 8.0, 'Hand sanitiser': 2.0}
#weight
w = {'Sleeping bag': 1.2, 'Pillow': 0.39, 'Torch': 0.5, 'First Aid Kit': 0.5, 'Hand sanitiser': 0.5}
limit = 2.9
items = list(sorted(v.keys()))
# Model
m = LpProblem("Knapsack Problem", LpMaximize)
# Variables
x = LpVariable.dicts('x', items, lowBound = 0, upBound = 1, cat = LpInteger)
#Objective
m += sum(v[i]*x[i] for i in items)
#Constraint
m += sum(w[i]*x[i] for i in items) &lt;= limit
#Optimize
m.solve()
#decision variables
for i in items:
    print("%s = %f" % (x[i].name, x[i].varValue))</pre>
<p>This code when<a id="_idIndexMarker281"/> executed results in the <span class="No-Break">following output:</span></p>
<pre class="source-code">
x_First_Aid_Kit = 1.0
x_Hand_sanitizer = 0.0
x_Pillow = 1.0
x_Sleeping_bag = 1.0
x_Torch = 1.0</pre>
<p>Going by the result (optimal solution), a hand sanitizer must not be carried in the sack. This is a simple integer programming problem as decision variables are restricted to being integers. In a very similar manner, other practical business problems such as <a id="_idIndexMarker282"/><a id="_idIndexMarker283"/>production planning are solved by mathematical optimization wherein the right resources are chosen to maximize profit and so on. When operations research is combined with ML predictions, data science is effectively transformed into decision science, allowing organizations to make <span class="No-Break">actionable decisions.</span></p>
<p>In the next section, we will learn about <strong class="bold">evolutionary optimization</strong>, which is motivated by <a id="_idIndexMarker284"/>optimization processes observed in nature such as the migration of species, bird swarms, and <span class="No-Break">ant colonies.</span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor105"/>Evolutionary optimization</h1>
<p>Evolutionary optimization<a id="_idIndexMarker285"/> makes use of algorithms that mimic the selection process within the natural world. Examples of this are genetic algorithms that optimize via natural selection. Each iteration of a hyperparameter value is like a mutation in genetics that is assessed and altered. The process continues using recombined choices until the most effective configuration is reached. Hence, each generation improves with every iteration as it is optimized. Genetic algorithms are often used to train <span class="No-Break">neural networks.</span></p>
<p>An evolutionary algorithm typically consists of three steps: initialization, selection, and termination. Fitter generations survive and proliferate, like in natural selection. In general, an initial population of a wide range of solutions is randomly created within the constraints of the problem. The population contains an arbitrary number of possible solutions to the problem, or the solutions are roughly centered around what is believed to be an ideal solution. These solutions are then evaluated in the next step according to a fitness (or objective) function. A good fitness function is one that represents the data and calculates a numerical value for the viability of a solution to a specific problem. Once the fitness of all solutions is calculated, the top-scoring solutions are selected. There may be multiple fitness functions that result in more than one optimal solution, which is when a decider is used to narrow down a single problem-specific solution that is based on some key metrics. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em> depicts the steps of these algorithms <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 9.7: Steps of evolutionary algorithms" height="417" src="image/Figure_09_08_B18943.jpg" width="432"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7: Steps of evolutionary algorithms</p>
<p>The top solutions make the next generation in the algorithm. These solutions typically have a mixture of the characteristics of solutions from the previous generation. New genetic material is introduced into this new generation, which, mathematically speaking, means introducing new probability distribution. This step is mutation, without which optimal<a id="_idIndexMarker286"/> results are difficult to achieve. The last step is termination, when the algorithm reaches either some threshold of performance or some maximum number of iterations (runtime). A final solution is then selected <span class="No-Break">and returned.</span></p>
<p>An evolutionary algorithm is a heuristic-based approach to solving problems that would take too long to exhaustively process using deterministic methods. It is a stochastic search technique typically applied to combinatorial problems or in tandem with other methods to find an optimal <span class="No-Break">solution quickly.</span></p>
<h1 id="_idParaDest-107"><a id="_idTextAnchor106"/>Summary</h1>
<p>In this chapter, we learned about optimization techniques, especially the ones used in machine learning that aim to find the most effective hyperparameter configuration for an ML model fitted to a dataset. An optimized ML model has minimum errors, thereby improving the accuracy of predictions. There would be no learning or development of models <span class="No-Break">without optimization.</span></p>
<p>We touched upon optimization algorithms that are used in operations research, as well as evolutionary algorithms that find usage in the optimization of deep learning models and network modeling of more <span class="No-Break">complex problems.</span></p>
<p>In the final chapter of the book, we will learn about how standard techniques are selected to optimize ML models. Multiple optimal solutions may exist for a given problem and there may be multiple optimization techniques to arrive at them. Hence, it is essential to choose the technique carefully while building the model addressing the pertinent <span class="No-Break">business question.</span></p>
</div>
</div></body></html>