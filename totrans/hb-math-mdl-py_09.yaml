- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter primarily aims to address the question, “Why is optimization necessary
    while solving problems?” Mathematical optimization, or mathematical programming,
    is a powerful decision-making tool that has been talked about in depth in the
    chapters of Part I. What is important is to recall the simple fact that optimization
    yields the best result to a problem by reducing errors that are, essentially,
    the gaps between predicted and real data. Optimization comes at a cost; almost
    all optimization problems are described in terms of costs such as money, time,
    and resources. This cost function is the error function. If a business problem
    has clear goals and constraints, such as in the airline and logistics industries,
    mathematical optimization is applied for efficient decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: In **machine learning** (**ML**) problems, the cost is often called the **loss
    function**. ML models make predictions about trends or classify data wherein training
    a model is a process of optimization, as each iteration in this process aims to
    improve the accuracy of the model and lower the margin of error. Selecting the
    optimum value of hyperparameters is key to ensuring an accurately and efficiently
    performing model. Hyperparameters are the elements of an ML model (for example,
    learning rate, number of clusters, etc.) that are tuned to fit a specific dataset
    to the model. In short, they are parameters whose values control the learning
    process. Optimization is an iterative process, meaning that the ML model becomes
    more accurate with each iteration in most cases and becomes better at predicting
    an outcome or classifying data.
  prefs: []
  type: TYPE_NORMAL
- en: The right blend of ML and mathematical optimization can prove to be useful for
    certain business problems. For example, the output of an ML model can determine
    the scope of an optimization model, especially in routing problems where one uses
    both predictive maintenance with ML as well as clustering, the results of which
    are fed into a mathematical model to create optimal routes. Similarly, an ML model
    may learn from a mathematical model. Initial values of decision variables obtained
    from a mathematical model can be used in an ML model that not only predicts optimal
    values of the decision variables, but also helps accelerate the performance of
    an optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ML optimization is performed using algorithms that exploit a range of techniques
    to refine an ML model. The optimization process searches for the most effective
    configuration or set of hyperparameters for the model to suit the specific use
    case (dataset) or business problem.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, ML is data-driven and optimization is algorithm-driven. Every
    ML model operates on the principle of minimizing the cost function; hence, optimization
    is a superset at its core.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization in operations research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section explores approaches and techniques used in optimizing ML models
    to arrive at the best set of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of optimization is integral to an ML model. ML helps make clusters,
    detect anomalies, predict the future from historical data, and so forth. However,
    when it comes to minimizing costs in a business, finding optimal placement of
    business facilities, et cetera, what we need is a mathematical optimization model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will talk about optimization in machine learning in this section. Optimization
    ensures that the structure and configuration of the ML model are as effective
    as possible to achieve the goal it has been built for. Optimization techniques
    automate the testing of different model configurations. The best configuration
    (set of hyperparameters) has the lowest margin of error, thereby yielding the
    most accurate model for a given dataset. Getting the hyperparameter optimization
    right for an ML model can be tedious, as both under-optimized (underfit) as well
    as over-optimized (overfit) models fail. Overfitting is when a model is trained
    too closely to training data, resulting in inaccurate yields with new data. Underfitting
    is when a model is poorly trained, making it ineffective with training data as
    well as new data. Hyperparameters can be sought manually, which is an exhaustive
    method using trial and error. Underfit, optimal, and overfit models are illustrated
    in *Figure 9**.1* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Under-optimized (L) and over-optimized (R) model fits](img/Figure_09_01_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Under-optimized (L) and over-optimized (R) model fits'
  prefs: []
  type: TYPE_NORMAL
- en: The main techniques of optimization include **random search**, **grid search**
    of hyperparameters, and **Bayesian optimization**, all of which are discussed
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of random sampling of the search space and identifying the most
    effective configuration of a hyperparameter set is random search. A random search
    technique discovers new combinations of hyperparameters for an optimized ML model.
    The number of iterations in the search process has to be set, which limits these
    new combinations, without which the process is a lot more time-consuming. It is
    an efficient process as it replaces an exhaustive search with randomness. A search
    space can be thought of as a volume in space, each dimension of which represents
    a hyperparameter, and each point or vector in the volume represents a model configuration.
    An optimization procedure involves defining the search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The search space is a dictionary in the Python code, and the `scikit-learn`
    library provides functions to tune model hyperparameters. An example code of a
    random search for a classification model is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The dataset used is a set of 60 patterns obtained by bouncing sonar signals
    off a metal cylinder under various conditions. Each pattern is a set of numbers
    lying in the range between 0.0 and 1.0, with each number representing the energy
    within a frequency band integrated over a period of time. The label associated
    with each record is either *R* if the object is a rock, or *M* if the object is
    a metal cylinder or mine. Data can be found in the GitHub repository at [https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset).
  prefs: []
  type: TYPE_NORMAL
- en: An example code of random search for a linear regression model has also been
    provided. The insurance dataset with two variables, namely the number of claims
    and total payment (in Swedish Krona) for all claims in geographical Swedish zones,
    can be found in the GitHub repository at [https://github.com/ranja-sarkar/dataset](https://github.com/ranja-sarkar/dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between regression and classification tasks is in choosing the
    performance scoring protocol for the models. The hyperparameter optimization methods
    in the `scikit-learn` Python library assume good performance scores are negative
    values close to zero (for regression), with zero representing a perfect regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The runtime of the code depends on the size of the search space and the system
    processor speed. The `result` class in the code provides the outcome, the most
    important value being the best score for the best performance of the model and
    the hyperparameters that achieved this score. Once the best set of hyperparameters
    becomes known, one can define a new model, set the hyperparameters to the known
    values, and fit the model on available data. This model can then be used for predictions
    on new data. The number of random configurations in the parameter space look like
    *Figure 9**.2*, which shows that random search works best for low-dimensional
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Random search](img/Figure_09_02_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Random search'
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we elaborate on grid search for optimization of classification
    and regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of assessing the effectiveness of known hyperparameter values of
    an ML model is grid search. Each hyperparameter is represented as a dimension
    on a grid across the search space and each point in the grid is searched and evaluated.
    Grid search is great for checking intuitive guesses and hyperparameter combinations
    that are known to perform well in general. As mentioned earlier, an optimization
    procedure involves defining a search space (a dictionary in Python), which can
    be thought of as a volume where each dimension represents a hyperparameter and
    each point (vector) represents a model configuration. A discrete grid has to be
    defined here. In other words, the grid search space takes discrete values (that
    can be on a log scale) instead of a log-uniform distribution used in a random
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample code of grid search for a classification model using the same dataset
    (`sonar.csv`) explored for a random search algorithm is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample code of a grid search for a linear regression model using the same
    dataset (`auto-insurance.csv`) explored for a random search algorithm is provided
    as follows. The best hyperparameters obtained using the random search and grid
    search algorithms for this dataset can be compared to get an estimate of which
    algorithm works better for the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The scores obtained for the datasets with random search and grid search in
    classification and regression models are nearly identical. The selection of optimization
    technique for a given dataset depends on the use case. Though random search might
    in some cases result in better performance, it needs more time, while grid search
    is appropriate for quick searches of hyperparameters that perform well in general.
    The values of hyperparameters are placed like a matrix as shown in *Figure 9**.3*,
    similar to a grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: Grid search](img/Figure_09_03_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Grid search'
  prefs: []
  type: TYPE_NORMAL
- en: Another method, known as Bayesian optimization, whose search procedure is different
    from the preceding two, is discussed in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A directed and iterative approach to global optimization using probability
    is Bayesian optimization. This is a Gaussian process that converges fast for continuous
    hyperparameters that is, in a continuous search space (*Figure 9**.4*). In Bayesian
    optimization, a probabilistic model of the function is built, and maps hyperparameters
    to the objectives evaluated on a validation dataset. This process evaluates a
    hyperparameter configuration based on the current model, then updates it until
    an optimal point is reached and it attempts to find the global optimum in a minimum
    number of steps. In most cases, it is more efficient and effective than optimization
    by way of random search. The optimization landscape (multiple local minima) with
    one global minimum is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: Optimization landscape (response surface)](img/Figure_09_04_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Optimization landscape (response surface)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian optimization incorporates prior belief (marginal probability) about
    the objective function and updates the prior with samples drawn from the function
    to obtain a posterior belief (conditional probability) that better approximates
    the function, which is illustrated in *Figure 9**.5*. This process repeats itself
    until the extremum of the objective function is located or resources are exhausted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5: Bayesian statistics](img/Figure_09_05_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Bayesian statistics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian search is typically beneficial when there is a large amount of data,
    the learning is slow, and tuning time has to be minimized. The `scikit-optimize`
    library provides functions for Bayesian optimization of ML models. A sample code
    for hyperparameter tuning by the Bayesian method in a classification problem is
    provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The model used for approximating the objective function is called the **surrogate
    model**, and the posterior probability is a surrogate objective function that
    can be used to estimate the cost of candidate samples. The posterior is used to
    select the next sample from the search space and the technique that does this
    is called the **acquisition function**. Bayesian optimization is best when the
    function evaluation is expensive or the form of the objective function is complex
    (nonlinear, non-convex, highly multi-dimensional, or highly noisy) – for example,
    in deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The process of optimization lowers errors or loss from predictions in an ML
    model and improves the model’s accuracy. The very premise of ML relies on a form
    of function optimization so inputs can be almost accurately mapped to expected
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about mathematical optimization in operations
    research.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization in operations research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **operations research** was coined during World War I, when the British
    military brought together a group of scientists to allocate insufficient resources
    (food, medicines, etc.) in the most effective way possible to different military
    operations. Therefore, the term implies optimization, which is maximizing or minimizing
    an objective function subject to constraints, often in complex problems and in
    high dimensions. Operations problems typically include planning work shifts or
    creating a schedule for large organizations, designing facilities for customers
    at a large store, choosing investments for available funds, supply chain management,
    and inventory management, all of which can be posed or formulated as mathematical
    problems with a collection of variables and their relationships.
  prefs: []
  type: TYPE_NORMAL
- en: In operations research, a business problem is mapped to a lower-level generic
    problem that is concise enough to be described in mathematical notations. These
    generic problems can in turn be expressed using higher-level languages; for example,
    resources and activities are used to describe a scheduling problem. The higher-level
    language is problem-specific, hence the generic problems can be described using
    modeling paradigms. A modeling paradigm is a set of rules and practices that allows
    for the representation of higher-level problems using lower-level data structures
    such as matrices. These data structures or matrices are passed to the last step
    of abstraction, which is algorithms. The most prominent modeling paradigms are
    linear programming, integer programming, and mixed-integer programming, all of
    which use linear equality constraints. There is a family of algorithms to solve
    these linear programming problems. Search algorithms, such as branch and bound,
    solve integer programming problems, while the simplex algorithm is used in a linear
    programming modeling paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of how to solve a knapsack problem by optimization is illustrated
    with the following data (*Figures 9.6a* *and 9.6b*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6a: Knapsack problem](img/Figure_09_06_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6a: Knapsack problem'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say the constraint is the ability to only carry a maximum of 2.9 kg in
    the camping sack, while the total weight of all items is 3.09 kg. The item’s value
    assists in choosing the optimum number of items. As the number of items increases,
    the problem becomes bigger, and solving it by trying all possible combinations
    of items takes a significant amount of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6b: Knapsack problem with another variable](img/Figure_09_07_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6b: Knapsack problem with another variable'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function is value, which must be maximized. The best of items
    has to be chosen to meet the constraint of 2.9 kg by total weight. A solver (pulp,
    in this case) is used to solve this linear programming problem, as shown in the
    following code. The decision variables (to be determined) are given by ![](img/Formula_09_001.png).
    The variable is 1 if the item is chosen and 0 if the item is not chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code when executed results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Going by the result (optimal solution), a hand sanitizer must not be carried
    in the sack. This is a simple integer programming problem as decision variables
    are restricted to being integers. In a very similar manner, other practical business
    problems such as production planning are solved by mathematical optimization wherein
    the right resources are chosen to maximize profit and so on. When operations research
    is combined with ML predictions, data science is effectively transformed into
    decision science, allowing organizations to make actionable decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about **evolutionary optimization**, which
    is motivated by optimization processes observed in nature such as the migration
    of species, bird swarms, and ant colonies.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evolutionary optimization makes use of algorithms that mimic the selection process
    within the natural world. Examples of this are genetic algorithms that optimize
    via natural selection. Each iteration of a hyperparameter value is like a mutation
    in genetics that is assessed and altered. The process continues using recombined
    choices until the most effective configuration is reached. Hence, each generation
    improves with every iteration as it is optimized. Genetic algorithms are often
    used to train neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An evolutionary algorithm typically consists of three steps: initialization,
    selection, and termination. Fitter generations survive and proliferate, like in
    natural selection. In general, an initial population of a wide range of solutions
    is randomly created within the constraints of the problem. The population contains
    an arbitrary number of possible solutions to the problem, or the solutions are
    roughly centered around what is believed to be an ideal solution. These solutions
    are then evaluated in the next step according to a fitness (or objective) function.
    A good fitness function is one that represents the data and calculates a numerical
    value for the viability of a solution to a specific problem. Once the fitness
    of all solutions is calculated, the top-scoring solutions are selected. There
    may be multiple fitness functions that result in more than one optimal solution,
    which is when a decider is used to narrow down a single problem-specific solution
    that is based on some key metrics. *Figure 9**.7* depicts the steps of these algorithms
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: Steps of evolutionary algorithms](img/Figure_09_08_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Steps of evolutionary algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: The top solutions make the next generation in the algorithm. These solutions
    typically have a mixture of the characteristics of solutions from the previous
    generation. New genetic material is introduced into this new generation, which,
    mathematically speaking, means introducing new probability distribution. This
    step is mutation, without which optimal results are difficult to achieve. The
    last step is termination, when the algorithm reaches either some threshold of
    performance or some maximum number of iterations (runtime). A final solution is
    then selected and returned.
  prefs: []
  type: TYPE_NORMAL
- en: An evolutionary algorithm is a heuristic-based approach to solving problems
    that would take too long to exhaustively process using deterministic methods.
    It is a stochastic search technique typically applied to combinatorial problems
    or in tandem with other methods to find an optimal solution quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about optimization techniques, especially the ones
    used in machine learning that aim to find the most effective hyperparameter configuration
    for an ML model fitted to a dataset. An optimized ML model has minimum errors,
    thereby improving the accuracy of predictions. There would be no learning or development
    of models without optimization.
  prefs: []
  type: TYPE_NORMAL
- en: We touched upon optimization algorithms that are used in operations research,
    as well as evolutionary algorithms that find usage in the optimization of deep
    learning models and network modeling of more complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of the book, we will learn about how standard techniques
    are selected to optimize ML models. Multiple optimal solutions may exist for a
    given problem and there may be multiple optimization techniques to arrive at them.
    Hence, it is essential to choose the technique carefully while building the model
    addressing the pertinent business question.
  prefs: []
  type: TYPE_NORMAL
