<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer186">
<h1 class="chapter-number" id="_idParaDest-108"><a id="_idTextAnchor107"/>10</h1>
<h1 id="_idParaDest-109"><a id="_idTextAnchor108"/>Optimization Techniques for Machine Learning</h1>
<p>We discussed mathematical optimization techniques in the previous chapter and their necessity in business problems that require minimizing the cost (error) function and in predictive modeling, wherein the machine learns from historical data to predict the future. In <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>), the cost is a loss function or an energy function that is minimized. It can be challenging in most cases to know which optimization algorithm should be considered for a given ML model. Optimization is an iterative process to maximize or minimize an objective function and there is always a trade-off between the number of iteration steps taken and the computational hardship to get to the next step. In this chapter, hints of how to choose an optimization algorithm given a problem (hence, an objective) have been provided. The choice of optimization algorithm depends on different factors, including the specific problem to be solved, the size and complexity of the associated dataset, and the resources, such as computational power and <span class="No-Break">memory, available.</span></p>
<p>Direct search as well as stochastic search algorithms are designed for an objective function where the derivative of this function is not available. Strictly speaking, optimization algorithms can be grouped into those that use derivatives and those that do not use derivatives. Optimization algorithms that rely on gradient descent are fast and efficient; however, they require well-behaved objective functions to work well. We can fall back on an exhaustive search if the function has tricky characteristics, but it takes an extremely long time (<span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em>). There are optimization methods tougher than gradient descent, such as <strong class="bold">Genetic Algorithms</strong> (<strong class="bold">GAs</strong>) and simulated annealing. These take longer computational time and a greater number of steps than gradient descent, but they discover the optimal point even when it is very difficult <span class="No-Break">to find.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 10.1: Performance of optimization algorithms" height="281" src="image/Figure_10_01_B18943.jpg" width="798"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Performance of optimization algorithms</p>
<p>There can be derivative-free as well as gradient-based algorithms for optimization. Optimization algorithms used in ML models can in general be grouped into ones that use the first derivative (called the gradient) of the objective function and others that use the second derivative (called the Hessian) of the function in the <span class="No-Break">search space.</span></p>
<p>This chapter covers the <span class="No-Break">following topics:</span></p>
<ul>
<li>General <span class="No-Break">optimization algorithms</span></li>
<li>Complex <span class="No-Break">optimization algorithms</span></li>
</ul>
<p>Complex optimization algorithms encompass differentiable and non-differential functions. The next two sections cover examples of general and complex <span class="No-Break">optimization algorithms.</span></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor109"/>General optimization algorithms</h1>
<p>The most common optimization problem encountered in ML is continuous function optimization, wherein the function’s input arguments are (real) numeric values. In training ML models, optimization entails minimizing the loss function till it reaches or converges to a local <span class="No-Break">minimum (value).</span></p>
<p>An entire search domain is utilized in global optimization whereas only a neighborhood is explored in local optimization, which requires the knowledge of an initial approximation, as evident from <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2a</em>. If the objective function has local minima, then local search algorithms (gradient methods, for example) can also be stuck in one of the local minima. If the algorithm attains a local minimum, it is nearly impossible to reach the global minimum in the search space. In discrete search space, the neighborhood is a finite set that can be completely explored, while the objective function is differentiable (gradient methods, Newton-like methods) in continuous <span class="No-Break">search space.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 10.2a: Local minimum versus global minimum" height="516" src="image/Figure_10_02_B18943.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2a: Local minimum versus global minimum</p>
<p>Functions may be of a discrete nature, taking discrete variables, and are found in combinatorial optimization problems (an example is the <strong class="bold">Traveling Salesman Problem</strong> (<strong class="bold">TSP</strong>), depicted in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2b</em>) wherein the feasible solutions are also discrete. Generally speaking, it is more efficient searching through continuous space to find the optimum than searching through <span class="No-Break">discrete space.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 10.2b: TSP is a combinatorial optimization problem" height="412" src="image/Figure_10_03_B18943.jpg" width="487"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2b: TSP is a combinatorial optimization problem</p>
<p>Bracketing algorithms are optimization algorithms with one input variable where the optima is known to exist within a specific range. They assume that a single optimum (unimodal objective function) is present in the known range of search space. These algorithms may sometimes even be used when the derivative information is unavailable. The bisection method of optimization is one <span class="No-Break">such example.</span></p>
<p>Optimization algorithms with more than one input variable are local descent algorithms. The process in local descent involves choosing a direction for movement in the search space, then performing a bracketing search in a line or hyperplane in the chosen direction. Local descent is also called the line search algorithm; it is, however, computationally expensive to optimize each directional move in the search space. Gradient descent is a classic example of the line <span class="No-Break">search algorithm.</span></p>
<p>Algorithms that are grouped in accordance with whether they use gradient (first-order) or gradient of gradient (second-order) information to move in the search space to find the optimal point are discussed in the <span class="No-Break">following subsections.</span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>First-order algorithms</h2>
<p>The first derivative (gradient or slope) of the objective function is used in first-order optimization algorithms. First-order algorithms are generally referred to as gradient descent (or steepest descent). Unlike gradient descent, regularization algorithms use a predefined objective function. An ML model learns by minimizing an objective (cost function) and regularization is used on top of that when such a <span class="No-Break">model overfits.</span></p>
<p>The gradient in the search space is calculated using a step size, called the learning rate, which is a hyperparameter controlling the distance of movement in the space (<span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em>). Too small a step size leads to a long time to search for the optimum point, while too large a step size might lead to completely missing it. Optimizers have hyperparameters such as the learning rate, which can have a big impact on the performance of the <span class="No-Break">ML model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 10.3: Learning rates in gradient descent" height="505" src="image/Figure_10_04_B18943.jpg" width="1165"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3: Learning rates in gradient descent</p>
<p>Gradient descent <a id="_idIndexMarker287"/>variants are batch gradient descent, mini-batch gradient descent, and <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>). Batch gradient descent computes the gradient with respect to the entire training dataset (all training examples), whereas <a id="_idIndexMarker288"/>SGD computes that with respect to each training example. A mini-batch performs an update for every (mini-) subset of <a id="_idIndexMarker289"/>training examples and hence takes the best of both worlds. A batch gradient descent can be very slow, whereas mini-batch gradient descent is very efficient. Mini-batch gradient descent is a good choice for problems with huge data. SGD performs frequent updates and hence the objective function fluctuates heavily, but it brings better convergence to the optimum. SGD is used to train artificial <span class="No-Break">neural networks.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 10.4: First-order algorithm (gradient descent) example" height="266" src="image/Figure_10_05_B18943.jpg" width="917"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4: First-order algorithm (gradient descent) example</p>
<p>Minor extensions to the gradient descent procedure of optimization lead to several algorithms, such as <a id="_idIndexMarker290"/>momentum, <strong class="bold">Adaptive Gradient</strong> (<strong class="bold">AdaGrad</strong>), and <strong class="bold">Adaptive Moment Estimation</strong> (<strong class="bold">Adam</strong>). Momentum, for example, is a method that helps accelerate SGD in the relevant direction (<span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em>) for faster convergence. Methods such as adagrad and Adam compute adaptive learning rates for each parameter, helping the function converge quickly. However, Adam might be the best choice for <a id="_idIndexMarker291"/>sparse data. Adam uses both the gradient and second moment of the gradient. Adagrad is good for problems with very <a id="_idIndexMarker292"/>noisy data and ill-conditioned cost functions; that is; different dimensions of the cost function are not of the <span class="No-Break">same scale.</span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>Second-order algorithms</h2>
<p>The second derivative (Hessian) of the objective function is used in second-order optimization <a id="_idIndexMarker293"/>algorithms, provided the Hessian (curvature) matrix can be either calculated or approximated. These algorithms are used for univariate <a id="_idIndexMarker294"/>objective functions that have a single real variable, few of which show either the minimum or the maximum while optimizing but a saddle point in its domain (search space). Newton’s method is an example of a second-order optimization algorithm. A comparison of gradient descent (first-order) with Newton’s method (second-order) of optimization is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 10.5: Gradient descent (green) and Newton’s method (red) t,  to find routes from ￼to ￼ considering very small learning rates" height="680" src="image/Figure_10_06_B18943.jpg" width="600"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5: Gradient descent (green) and Newton’s method (red) t, to find routes from <img alt="" height="16" src="image/Formula_10_001.png" width="20"/>to <img alt="" height="12" src="image/Formula_10_002.png" width="13"/> considering very small learning rates</p>
<p>Such algorithms work better for neural networks; however, computation and storage become challenging with a huge number of dimensions or parameters. In order to successfully use second-order algorithms, one must simplify the matrix, which is typically done by approximating the Hessian matrix with a <span class="No-Break">simpler form.</span></p>
<p>The following section elaborates the differentiability of objective functions, which is what decides whether to select a general (discussed in this section) or complex optimization algorithm given <span class="No-Break">a problem.</span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Complex optimization algorithms</h1>
<p>The nature of the objective function helps select the algorithm to be considered for the optimization <a id="_idIndexMarker295"/>of a given business problem. The more information that is available about the function, the easier it is to optimize the function. Of most importance is the fact that the objective function can be differentiated at any point in the <span class="No-Break">search space.</span></p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor113"/>Differentiability of objective functions</h2>
<p>A differentiable objective function is one for which the derivative can be calculated at any <a id="_idIndexMarker296"/>given point in input space. The derivative (slope) is the rate of change of the function at that point. The Hessian is the rate at which the derivative of the function changes. Calculus helps optimize <a id="_idIndexMarker297"/>simple differentiable functions analytically. For differentiable objective functions, gradient-based optimization algorithms are used. However, there are objective functions for which the derivative cannot be computed, typically for very complex (noisy, multimodal, etc.) functions, which are called non-differentiable objective functions. There can be discontinuous objective functions as well, for which the derivatives can only be calculated in some regions of the search space. Stochastic and population algorithms handle such functions and are, hence, sometimes called <span class="No-Break">black-box algorithms.</span></p>
<p>When an analytical form of the objective function is not available, one generally uses simulation-based optimization methods. The next subsection talks briefly about the algorithms considered while finding a feasible solution is challenging using classical methods, and they either compute or build around assumptions about the derivatives of <span class="No-Break">objective functions.</span></p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Direct and stochastic algorithms</h2>
<p>Direct <a id="_idIndexMarker298"/>and stochastic <a id="_idIndexMarker299"/>optimization algorithms are used in problems where the derivative of the objective function is unknown or cannot be calculated, that is, the search space is discontinuous. The former algorithms are deterministic and assume the objective function is unimodal (it has a single global optimum). Direct search is often referred to as pattern search as it effectively navigates through the search space using geometric shapes. Gradient information is approximated from the objective function and used in initiating a line search in the search space, eventually (with repeated line searches) triangulating the region of optimum. Powell’s method is one example of a direct search algorithm. It is a gradient-free method because the function to be optimized with it <span class="No-Break">is non-differentiable.</span></p>
<p>On the other hand, stochastic algorithms make use of randomness in the global search, hence the name. These typically involve sampling the objective function and can handle problems <a id="_idIndexMarker300"/>with deceptive local optima. Simulated annealing (<span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.6</em>) is an example of a stochastic search algorithm, that is, of global optimization, which occasionally accepts poorer initial configurations. Simulated <a id="_idIndexMarker301"/>annealing is a probabilistic technique used to solve unconstrained and bound-constrained optimization problems. It is a metaheuristic to approximate global optimization in a large search space of a physical process wherein the system energy <span class="No-Break">is minimized.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 10.6: Simulated annealing is a stochastic optimization algorithm" height="845" src="image/Figure_10_07_B18943.jpg" width="1323"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6: Simulated annealing is a stochastic optimization algorithm</p>
<p>Population optimization algorithms such as GAs are also stochastic and typically used for multimodal objective functions with multiple global optima and not-so-smooth (highly noisy) functions. These algorithms maintain a population of candidate solutions that add robustness to the search, thereby increasing the likelihood of overcoming local optima. The efficiency of these is very sensitive to the variables used in describing the problem. As with other heuristic algorithms, evolutionary algorithms have many degrees of freedom and, therefore, are difficult to tune for good <span class="No-Break">model performance.</span></p>
<p>A GA pursues the evolution analogy, which proceeds by maintaining an even number of individuals <a id="_idIndexMarker302"/>in the population. These individuals make a generation, and a new generation is produced by randomly <a id="_idIndexMarker303"/>selecting a pair wherein the fitter individual is more likely to be chosen. GA is used to solve complex optimization problems by initialization (of the population), fitness assignment (to individuals in the population), and selection of the best (recombined) solution to the problem. A large community of researchers is working on GAs for utilization in most <span class="No-Break">practical problems.</span></p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor115"/>Summary</h1>
<p>In this chapter, we gained knowledge about which optimization algorithm must be considered to minimize (continuous) objective functions that are generally encountered in ML models. Such models have a real-valued evaluation of the input variables and involve local search. The differentiability of an objective function is perhaps the most important factor when considering the optimization algorithm type for a <span class="No-Break">given problem.</span></p>
<p>The chapter did not contain an exhaustive list of algorithms to optimize ML models but captured the essence of the main ones and their underlying behavior with examples. It also touched upon the concepts of deterministic optimization and stochastic optimization, the latter encompassing GAs, whose utility is evolving in <span class="No-Break">real-world problems.</span></p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor116"/>Epilogue</h1>
<p>This book was primarily targeted at data scientists early in their careers. It was assumed that readers of this book have knowledge of linear algebra and the basics of statistics, differential equations, fundamental numerical algorithms, data types, and data structures. Having said that, it must be realized that transforming a business problem into a mathematical formulation is <span class="No-Break">an art.</span></p>
<p>While exploring the world of data science, it is important to understand the relevance of classical mathematical models and how they can be utilized along with ML models to solve business problems, often complex ones. Hybrid (blended) models enable better decision-making and become particularly essential for high-stake decisions in sensitive domains. Mathematical optimization typically elevates an ML model for the best interpretation of the connection between decision variables and relevant data and business objectives and of the optimal solution to the business problem. Nevertheless, simpler (pure or unblended) models are more often explainable, and while building complex ones, we need to look at the aspects of efficiency <span class="No-Break">and cost.</span></p>
<p>I would like to wind this book up by acknowledging and sincerely thanking the following subject <span class="No-Break">matter experts:</span></p>
<ul>
<li>Brandon <span class="No-Break">Rohrer (</span><a href="https://github.com/brohrer"><span class="No-Break">https://github.com/brohrer</span></a><span class="No-Break">)</span></li>
<li>Sebastian <span class="No-Break">Raschka (</span><a href="https://github.com/rasbt"><span class="No-Break">https://github.com/rasbt</span></a><span class="No-Break">)</span></li>
<li>Jason <span class="No-Break">Brownlee (</span><a href="http://fileName.py"><span class="No-Break">https://www.linkedin.com/in/jasonbrownlee/</span></a><span class="No-Break">)</span></li>
</ul>
<p>Their online articles, books, courses, and tutorials/blogs have motivated me to learn, relearn, and deep dive into the world of data science and mathematical optimization. My learning and work experience eventually have taken shape in <span class="No-Break">this book.</span></p>
</div>
</div></body></html>