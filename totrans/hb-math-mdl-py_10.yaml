- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization Techniques for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed mathematical optimization techniques in the previous chapter and
    their necessity in business problems that require minimizing the cost (error)
    function and in predictive modeling, wherein the machine learns from historical
    data to predict the future. In **Machine Learning** (**ML**), the cost is a loss
    function or an energy function that is minimized. It can be challenging in most
    cases to know which optimization algorithm should be considered for a given ML
    model. Optimization is an iterative process to maximize or minimize an objective
    function and there is always a trade-off between the number of iteration steps
    taken and the computational hardship to get to the next step. In this chapter,
    hints of how to choose an optimization algorithm given a problem (hence, an objective)
    have been provided. The choice of optimization algorithm depends on different
    factors, including the specific problem to be solved, the size and complexity
    of the associated dataset, and the resources, such as computational power and
    memory, available.
  prefs: []
  type: TYPE_NORMAL
- en: Direct search as well as stochastic search algorithms are designed for an objective
    function where the derivative of this function is not available. Strictly speaking,
    optimization algorithms can be grouped into those that use derivatives and those
    that do not use derivatives. Optimization algorithms that rely on gradient descent
    are fast and efficient; however, they require well-behaved objective functions
    to work well. We can fall back on an exhaustive search if the function has tricky
    characteristics, but it takes an extremely long time (*Figure 10**.1*). There
    are optimization methods tougher than gradient descent, such as **Genetic Algorithms**
    (**GAs**) and simulated annealing. These take longer computational time and a
    greater number of steps than gradient descent, but they discover the optimal point
    even when it is very difficult to find.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Performance of optimization algorithms](img/Figure_10_01_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Performance of optimization algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: There can be derivative-free as well as gradient-based algorithms for optimization.
    Optimization algorithms used in ML models can in general be grouped into ones
    that use the first derivative (called the gradient) of the objective function
    and others that use the second derivative (called the Hessian) of the function
    in the search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: General optimization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex optimization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex optimization algorithms encompass differentiable and non-differential
    functions. The next two sections cover examples of general and complex optimization
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: General optimization algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common optimization problem encountered in ML is continuous function
    optimization, wherein the functionâ€™s input arguments are (real) numeric values.
    In training ML models, optimization entails minimizing the loss function till
    it reaches or converges to a local minimum (value).
  prefs: []
  type: TYPE_NORMAL
- en: An entire search domain is utilized in global optimization whereas only a neighborhood
    is explored in local optimization, which requires the knowledge of an initial
    approximation, as evident from *Figure 10**.2a*. If the objective function has
    local minima, then local search algorithms (gradient methods, for example) can
    also be stuck in one of the local minima. If the algorithm attains a local minimum,
    it is nearly impossible to reach the global minimum in the search space. In discrete
    search space, the neighborhood is a finite set that can be completely explored,
    while the objective function is differentiable (gradient methods, Newton-like
    methods) in continuous search space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2a: Local minimum versus global minimum](img/Figure_10_02_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2a: Local minimum versus global minimum'
  prefs: []
  type: TYPE_NORMAL
- en: Functions may be of a discrete nature, taking discrete variables, and are found
    in combinatorial optimization problems (an example is the **Traveling Salesman
    Problem** (**TSP**), depicted in *Figure 10**.2b*) wherein the feasible solutions
    are also discrete. Generally speaking, it is more efficient searching through
    continuous space to find the optimum than searching through discrete space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2b: TSP is a combinatorial optimization problem](img/Figure_10_03_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2b: TSP is a combinatorial optimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: Bracketing algorithms are optimization algorithms with one input variable where
    the optima is known to exist within a specific range. They assume that a single
    optimum (unimodal objective function) is present in the known range of search
    space. These algorithms may sometimes even be used when the derivative information
    is unavailable. The bisection method of optimization is one such example.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms with more than one input variable are local descent
    algorithms. The process in local descent involves choosing a direction for movement
    in the search space, then performing a bracketing search in a line or hyperplane
    in the chosen direction. Local descent is also called the line search algorithm;
    it is, however, computationally expensive to optimize each directional move in
    the search space. Gradient descent is a classic example of the line search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that are grouped in accordance with whether they use gradient (first-order)
    or gradient of gradient (second-order) information to move in the search space
    to find the optimal point are discussed in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: First-order algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first derivative (gradient or slope) of the objective function is used in
    first-order optimization algorithms. First-order algorithms are generally referred
    to as gradient descent (or steepest descent). Unlike gradient descent, regularization
    algorithms use a predefined objective function. An ML model learns by minimizing
    an objective (cost function) and regularization is used on top of that when such
    a model overfits.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient in the search space is calculated using a step size, called the
    learning rate, which is a hyperparameter controlling the distance of movement
    in the space (*Figure 10**.3*). Too small a step size leads to a long time to
    search for the optimum point, while too large a step size might lead to completely
    missing it. Optimizers have hyperparameters such as the learning rate, which can
    have a big impact on the performance of the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: Learning rates in gradient descent](img/Figure_10_04_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Learning rates in gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent variants are batch gradient descent, mini-batch gradient descent,
    and **Stochastic Gradient Descent** (**SGD**). Batch gradient descent computes
    the gradient with respect to the entire training dataset (all training examples),
    whereas SGD computes that with respect to each training example. A mini-batch
    performs an update for every (mini-) subset of training examples and hence takes
    the best of both worlds. A batch gradient descent can be very slow, whereas mini-batch
    gradient descent is very efficient. Mini-batch gradient descent is a good choice
    for problems with huge data. SGD performs frequent updates and hence the objective
    function fluctuates heavily, but it brings better convergence to the optimum.
    SGD is used to train artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4: First-order algorithm (gradient descent) example](img/Figure_10_05_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: First-order algorithm (gradient descent) example'
  prefs: []
  type: TYPE_NORMAL
- en: Minor extensions to the gradient descent procedure of optimization lead to several
    algorithms, such as momentum, **Adaptive Gradient** (**AdaGrad**), and **Adaptive
    Moment Estimation** (**Adam**). Momentum, for example, is a method that helps
    accelerate SGD in the relevant direction (*Figure 10**.4*) for faster convergence.
    Methods such as adagrad and Adam compute adaptive learning rates for each parameter,
    helping the function converge quickly. However, Adam might be the best choice
    for sparse data. Adam uses both the gradient and second moment of the gradient.
    Adagrad is good for problems with very noisy data and ill-conditioned cost functions;
    that is; different dimensions of the cost function are not of the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: Second-order algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second derivative (Hessian) of the objective function is used in second-order
    optimization algorithms, provided the Hessian (curvature) matrix can be either
    calculated or approximated. These algorithms are used for univariate objective
    functions that have a single real variable, few of which show either the minimum
    or the maximum while optimizing but a saddle point in its domain (search space).
    Newtonâ€™s method is an example of a second-order optimization algorithm. A comparison
    of gradient descent (first-order) with Newtonâ€™s method (second-order) of optimization
    is shown in *Figure 10**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5: Gradient descent (green) and Newtonâ€™s method (red) t,  to find
    routes from ï¿¼to ï¿¼ considering very small learning rates](img/Figure_10_06_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Gradient descent (green) and Newtonâ€™s method (red) t, to find
    routes from ![](img/Formula_10_001.png)to ![](img/Formula_10_002.png) considering
    very small learning rates'
  prefs: []
  type: TYPE_NORMAL
- en: Such algorithms work better for neural networks; however, computation and storage
    become challenging with a huge number of dimensions or parameters. In order to
    successfully use second-order algorithms, one must simplify the matrix, which
    is typically done by approximating the Hessian matrix with a simpler form.
  prefs: []
  type: TYPE_NORMAL
- en: The following section elaborates the differentiability of objective functions,
    which is what decides whether to select a general (discussed in this section)
    or complex optimization algorithm given a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Complex optimization algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The nature of the objective function helps select the algorithm to be considered
    for the optimization of a given business problem. The more information that is
    available about the function, the easier it is to optimize the function. Of most
    importance is the fact that the objective function can be differentiated at any
    point in the search space.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiability of objective functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A differentiable objective function is one for which the derivative can be calculated
    at any given point in input space. The derivative (slope) is the rate of change
    of the function at that point. The Hessian is the rate at which the derivative
    of the function changes. Calculus helps optimize simple differentiable functions
    analytically. For differentiable objective functions, gradient-based optimization
    algorithms are used. However, there are objective functions for which the derivative
    cannot be computed, typically for very complex (noisy, multimodal, etc.) functions,
    which are called non-differentiable objective functions. There can be discontinuous
    objective functions as well, for which the derivatives can only be calculated
    in some regions of the search space. Stochastic and population algorithms handle
    such functions and are, hence, sometimes called black-box algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: When an analytical form of the objective function is not available, one generally
    uses simulation-based optimization methods. The next subsection talks briefly
    about the algorithms considered while finding a feasible solution is challenging
    using classical methods, and they either compute or build around assumptions about
    the derivatives of objective functions.
  prefs: []
  type: TYPE_NORMAL
- en: Direct and stochastic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Direct and stochastic optimization algorithms are used in problems where the
    derivative of the objective function is unknown or cannot be calculated, that
    is, the search space is discontinuous. The former algorithms are deterministic
    and assume the objective function is unimodal (it has a single global optimum).
    Direct search is often referred to as pattern search as it effectively navigates
    through the search space using geometric shapes. Gradient information is approximated
    from the objective function and used in initiating a line search in the search
    space, eventually (with repeated line searches) triangulating the region of optimum.
    Powellâ€™s method is one example of a direct search algorithm. It is a gradient-free
    method because the function to be optimized with it is non-differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, stochastic algorithms make use of randomness in the global
    search, hence the name. These typically involve sampling the objective function
    and can handle problems with deceptive local optima. Simulated annealing (*Figure
    10**.6*) is an example of a stochastic search algorithm, that is, of global optimization,
    which occasionally accepts poorer initial configurations. Simulated annealing
    is a probabilistic technique used to solve unconstrained and bound-constrained
    optimization problems. It is a metaheuristic to approximate global optimization
    in a large search space of a physical process wherein the system energy is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: Simulated annealing is a stochastic optimization algorithm](img/Figure_10_07_B18943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Simulated annealing is a stochastic optimization algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Population optimization algorithms such as GAs are also stochastic and typically
    used for multimodal objective functions with multiple global optima and not-so-smooth
    (highly noisy) functions. These algorithms maintain a population of candidate
    solutions that add robustness to the search, thereby increasing the likelihood
    of overcoming local optima. The efficiency of these is very sensitive to the variables
    used in describing the problem. As with other heuristic algorithms, evolutionary
    algorithms have many degrees of freedom and, therefore, are difficult to tune
    for good model performance.
  prefs: []
  type: TYPE_NORMAL
- en: A GA pursues the evolution analogy, which proceeds by maintaining an even number
    of individuals in the population. These individuals make a generation, and a new
    generation is produced by randomly selecting a pair wherein the fitter individual
    is more likely to be chosen. GA is used to solve complex optimization problems
    by initialization (of the population), fitness assignment (to individuals in the
    population), and selection of the best (recombined) solution to the problem. A
    large community of researchers is working on GAs for utilization in most practical
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained knowledge about which optimization algorithm must
    be considered to minimize (continuous) objective functions that are generally
    encountered in ML models. Such models have a real-valued evaluation of the input
    variables and involve local search. The differentiability of an objective function
    is perhaps the most important factor when considering the optimization algorithm
    type for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter did not contain an exhaustive list of algorithms to optimize ML
    models but captured the essence of the main ones and their underlying behavior
    with examples. It also touched upon the concepts of deterministic optimization
    and stochastic optimization, the latter encompassing GAs, whose utility is evolving
    in real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Epilogue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book was primarily targeted at data scientists early in their careers.
    It was assumed that readers of this book have knowledge of linear algebra and
    the basics of statistics, differential equations, fundamental numerical algorithms,
    data types, and data structures. Having said that, it must be realized that transforming
    a business problem into a mathematical formulation is an art.
  prefs: []
  type: TYPE_NORMAL
- en: While exploring the world of data science, it is important to understand the
    relevance of classical mathematical models and how they can be utilized along
    with ML models to solve business problems, often complex ones. Hybrid (blended)
    models enable better decision-making and become particularly essential for high-stake
    decisions in sensitive domains. Mathematical optimization typically elevates an
    ML model for the best interpretation of the connection between decision variables
    and relevant data and business objectives and of the optimal solution to the business
    problem. Nevertheless, simpler (pure or unblended) models are more often explainable,
    and while building complex ones, we need to look at the aspects of efficiency
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to wind this book up by acknowledging and sincerely thanking the
    following subject matter experts:'
  prefs: []
  type: TYPE_NORMAL
- en: Brandon Rohrer ([https://github.com/brohrer](https://github.com/brohrer))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebastian Raschka ([https://github.com/rasbt](https://github.com/rasbt))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jason Brownlee ([https://www.linkedin.com/in/jasonbrownlee/](http://fileName.py))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their online articles, books, courses, and tutorials/blogs have motivated me
    to learn, relearn, and deep dive into the world of data science and mathematical
    optimization. My learning and work experience eventually have taken shape in this
    book.
  prefs: []
  type: TYPE_NORMAL
