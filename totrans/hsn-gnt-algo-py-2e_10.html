<html><head></head><body>
<div id="_idContainer117" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-170"><a id="_idTextAnchor221" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-171" class="calibre5"><a id="_idTextAnchor222" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Enhancing Machine Learning Models Using Feature Selection</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter describes how genetic algorithms can be used to improve the performance of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">supervised machine learning</span></strong><span class="kobospan" id="kobo.5.1"> models by selecting the best subset of features from the provided input data. </span><span class="kobospan" id="kobo.5.2">We will start with a brief introduction to machine learning and then describe the two main types of supervised machine learning tasks – </span><strong class="bold"><span class="kobospan" id="kobo.6.1">regression</span></strong><span class="kobospan" id="kobo.7.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.8.1">classification</span></strong><span class="kobospan" id="kobo.9.1">. </span><span class="kobospan" id="kobo.9.2">We will then discuss the potential benefits of </span><strong class="bold"><span class="kobospan" id="kobo.10.1">feature selection</span></strong><span class="kobospan" id="kobo.11.1"> when it comes to the performance of these models. </span><span class="kobospan" id="kobo.11.2">Next, we will demonstrate how genetic algorithms can be utilized to pinpoint the genuine features that are generated by the </span><strong class="bold"><span class="kobospan" id="kobo.12.1">Friedman-1 Test</span></strong><span class="kobospan" id="kobo.13.1"> regression problem. </span><span class="kobospan" id="kobo.13.2">Then, we will use the real-life </span><strong class="bold"><span class="kobospan" id="kobo.14.1">Zoo dataset</span></strong><span class="kobospan" id="kobo.15.1"> to create a classification model and improve its accuracy – again by applying genetic algorithms to isolate the best features for </span><span><span class="kobospan" id="kobo.16.1">the task.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.17.1">In this chapter, we will cover the </span><span><span class="kobospan" id="kobo.18.1">following topics:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.19.1">Understand the basic concepts of supervised machine learning, as well as regression and </span><span><span class="kobospan" id="kobo.20.1">classification tasks</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.21.1">Understand the benefits of feature selection on the performance of supervised </span><span><span class="kobospan" id="kobo.22.1">learning models</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.23.1">Enhance the performance of a regression model for the Friedman-1 Test regression problem, using feature selection carried out by a genetic algorithm coded with the </span><span><span class="kobospan" id="kobo.24.1">DEAP framework</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.25.1">Enhance the performance of a classification model for the Zoo dataset classification problem, using feature selection carried out by a genetic algorithm coded with the </span><span><span class="kobospan" id="kobo.26.1">DEAP framework</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.27.1">We will start this chapter with a quick review of supervised machine learning. </span><span class="kobospan" id="kobo.27.2">If you are a seasoned data scientist, feel free to skip the </span><span><span class="kobospan" id="kobo.28.1">introductory sections.</span></span></p>
<h1 id="_idParaDest-172" class="calibre5"><a id="_idTextAnchor223" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.29.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.30.1">In this chapter, we will be using Python 3 with the following </span><span><span class="kobospan" id="kobo.31.1">supporting libraries:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.32.1">deap</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.33.1">numpy</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.34.1">pandas</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.35.1">matplotlib</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.36.1">seaborn</span></strong></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.37.1">scikit-learn</span></strong><span class="kobospan" id="kobo.38.1"> – introduced in </span><span><span class="kobospan" id="kobo.39.1">this chapter</span></span></li>
</ul>
<p class="callout-heading"><span class="kobospan" id="kobo.40.1">Important Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.41.1">If you use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.42.1">requirements.txt</span></strong><span class="kobospan" id="kobo.43.1"> file we provide (see </span><a href="B20851_03.xhtml#_idTextAnchor091" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.44.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.45.1">), these libraries are already included in </span><span><span class="kobospan" id="kobo.46.1">your environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">In addition, we will be using the </span><em class="italic"><span class="kobospan" id="kobo.48.1">UCI Zoo </span></em><span><em class="italic"><span class="kobospan" id="kobo.49.1">Dataset</span></em></span><span><span class="kobospan" id="kobo.50.1"> (</span></span><a href="https://archive.ics.uci.edu/ml/datasets/zoo" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.51.1">https://archive.ics.uci.edu/ml/datasets/zoo</span></span></a><span><span class="kobospan" id="kobo.52.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.53.1">The programs that will be used in this chapter can be found in this book’s GitHub repository </span><span><span class="kobospan" id="kobo.54.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_07" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.55.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_07</span></span></a><span><span class="kobospan" id="kobo.56.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.57.1">Check out the following video to see the code in action: </span></p>
<p class="calibre3"><a href="https://packt.link/OEBOd" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.58.1">https://packt.link/OEBOd</span></span></a><span><span class="kobospan" id="kobo.59.1">.</span></span></p>
<h1 id="_idParaDest-173" class="calibre5"><a id="_idTextAnchor224" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.60.1">Supervised machine learning</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.61.1">The term </span><strong class="bold"><span class="kobospan" id="kobo.62.1">machine learning</span></strong><span class="kobospan" id="kobo.63.1"> typically refers to a computer program that receives input and produces output. </span><span class="kobospan" id="kobo.63.2">Our goal is to train this</span><a id="_idIndexMarker465" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.64.1"> program, also known as the </span><strong class="bold"><span class="kobospan" id="kobo.65.1">model</span></strong><span class="kobospan" id="kobo.66.1">, to produce the correct output</span><a id="_idIndexMarker466" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.67.1"> for the given input </span><em class="italic"><span class="kobospan" id="kobo.68.1">without explicitly </span></em><span><em class="italic"><span class="kobospan" id="kobo.69.1">programming it</span></em></span><span><span class="kobospan" id="kobo.70.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.71.1">During this training process, the model learns the mapping between the inputs and the outputs by adjusting its internal parameters. </span><span class="kobospan" id="kobo.71.2">One common way to train the model is by providing it with a set of inputs for which the correct output is known. </span><span class="kobospan" id="kobo.71.3">For each of these inputs, we tell the model what the correct output is so that it can adjust, or tune itself, aiming to eventually produce the desired output for each of the given inputs. </span><span class="kobospan" id="kobo.71.4">This tuning is at the heart of the </span><span><span class="kobospan" id="kobo.72.1">learning process.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Over the years, many types of machine learning models have been developed. </span><span class="kobospan" id="kobo.73.2">Each model has its own particular internal parameters that can affect the mapping between the input and the output, and</span><a id="_idIndexMarker467" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.74.1"> the values of these parameters can be tuned, as illustrated in the </span><span><span class="kobospan" id="kobo.75.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer111">
<span class="kobospan" id="kobo.76.1"><img alt="Figure 7.1: Parameter tuning of a machine learning model" src="image/B20851_07_001.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.77.1">Figure 7.1: Parameter tuning of a machine learning model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.78.1">For example, if the model was implementing a </span><em class="italic"><span class="kobospan" id="kobo.79.1">decision tree</span></em><span class="kobospan" id="kobo.80.1">, it could contain several </span><strong class="source-inline"><span class="kobospan" id="kobo.81.1">IF- THEN</span></strong><span class="kobospan" id="kobo.82.1"> statements, which can be formulated </span><span><span class="kobospan" id="kobo.83.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.84.1">
IF &lt;input value&gt; IS LESS THEN &lt;some threshold value&gt;
    THEN &lt;go to some target branch&gt;</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.85.1">In this case, both the threshold value and the identity of the target branch are parameters that can be adjusted, or tuned, during the </span><span><span class="kobospan" id="kobo.86.1">learning process.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.87.1">To tune the internal parameters, each type of model has an accompanying </span><em class="italic"><span class="kobospan" id="kobo.88.1">learning algorithm</span></em><span class="kobospan" id="kobo.89.1"> that iterates over the given input and output values and seeks to match the given output for each of the given inputs. </span><span class="kobospan" id="kobo.89.2">To accomplish this goal, a typical learning algorithm will measure the</span><a id="_idIndexMarker468" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.90.1"> difference (also called </span><em class="italic"><span class="kobospan" id="kobo.91.1">error</span></em><span class="kobospan" id="kobo.92.1">, or more generally </span><em class="italic"><span class="kobospan" id="kobo.93.1">loss</span></em><span class="kobospan" id="kobo.94.1">) between the actual output and the desired output; the algorithm will then attempt to minimize this error by adjusting the model’s </span><span><span class="kobospan" id="kobo.95.1">internal parameters.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.96.1">The two main types of supervised </span><a id="_idIndexMarker469" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.97.1">machine learning are </span><strong class="bold"><span class="kobospan" id="kobo.98.1">classification</span></strong><span class="kobospan" id="kobo.99.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.100.1">regression</span></strong><span class="kobospan" id="kobo.101.1">, and will be described in the </span><span><span class="kobospan" id="kobo.102.1">following subsections.</span></span></p>
<h2 id="_idParaDest-174" class="calibre7"><a id="_idTextAnchor225" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.103.1">Classification</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.104.1">When carrying out a</span><a id="_idIndexMarker470" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.105.1"> classification task, the model needs to decide which </span><em class="italic"><span class="kobospan" id="kobo.106.1">category</span></em><span class="kobospan" id="kobo.107.1"> a certain input belongs to. </span><span class="kobospan" id="kobo.107.2">Each category is</span><a id="_idIndexMarker471" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.108.1"> represented by a </span><a id="_idIndexMarker472" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.109.1">single output (called a </span><strong class="bold"><span class="kobospan" id="kobo.110.1">label</span></strong><span class="kobospan" id="kobo.111.1">), while the inputs are</span><a id="_idIndexMarker473" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.112.1">called </span></span><span><strong class="bold"><span class="kobospan" id="kobo.113.1">features</span></strong></span><span><span class="kobospan" id="kobo.114.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer112">
<span class="kobospan" id="kobo.115.1"><img alt="Figure 7.2: Machine learning classification model" src="image/B20851_07_002.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.116.1">Figure 7.2: Machine learning classification model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.117.1">For example, in the well-known </span><em class="italic"><span class="kobospan" id="kobo.118.1">Iris dataset</span></em><span class="kobospan" id="kobo.119.1"> (</span><a href="https://archive.ics.uci.edu/ml/datasets/Iris" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.120.1">https://archive.ics.uci.edu/ml/datasets/Iris</span></a><span class="kobospan" id="kobo.121.1">), there are four features: </span><strong class="bold"><span class="kobospan" id="kobo.122.1">Petal length</span></strong><span class="kobospan" id="kobo.123.1">, </span><strong class="bold"><span class="kobospan" id="kobo.124.1">Petal width</span></strong><span class="kobospan" id="kobo.125.1">, </span><strong class="bold"><span class="kobospan" id="kobo.126.1">Sepal length</span></strong><span class="kobospan" id="kobo.127.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.128.1">Sepal width</span></strong><span class="kobospan" id="kobo.129.1">. </span><span class="kobospan" id="kobo.129.2">These</span><a id="_idIndexMarker474" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.130.1"> represent the </span><a id="_idIndexMarker475" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.131.1">measurements that have been manually taken</span><a id="_idIndexMarker476" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.132.1"> of actual </span><span><span class="kobospan" id="kobo.133.1">Iris</span></span><span><a id="_idIndexMarker477" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.134.1"> flowers.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.135.1">In terms of the output, there </span><a id="_idIndexMarker478" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.136.1">are three labels: </span><strong class="bold"><span class="kobospan" id="kobo.137.1">Iris setosa</span></strong><span class="kobospan" id="kobo.138.1">, </span><strong class="bold"><span class="kobospan" id="kobo.139.1">Iris virginica</span></strong><span class="kobospan" id="kobo.140.1">, and </span><strong class="bold"><span class="kobospan" id="kobo.141.1">Iris versicolor</span></strong><span class="kobospan" id="kobo.142.1">. </span><span class="kobospan" id="kobo.142.2">These</span><a id="_idIndexMarker479" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.143.1"> represent the</span><a id="_idIndexMarker480" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.144.1"> three different types of Iris in </span><span><span class="kobospan" id="kobo.145.1">the dataset.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.146.1">When input values, which represent the measurements that were taken from a given Iris flower, are present we expect the output of the correct label to go high and the other two to </span><span><span class="kobospan" id="kobo.147.1">go low:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer113">
<span class="kobospan" id="kobo.148.1"><img alt="Figure 7.3: Iris Flower classifier illustrated" src="image/B20851_07_003.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.149.1">Figure 7.3: Iris Flower classifier illustrated</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.150.1">Classification tasks have a multitude of real-life applications, such as approval of bank loans and credit cards, email spam </span><a id="_idIndexMarker481" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.151.1">detection, handwritten digit recognition, and face recognition. </span><span class="kobospan" id="kobo.151.2">Later in this chapter, we will be demonstrating</span><a id="_idIndexMarker482" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.152.1"> the classification of animal types using the </span><span><em class="italic"><span class="kobospan" id="kobo.153.1">Zoo dataset</span></em></span><span><span class="kobospan" id="kobo.154.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.155.1">The second main type of supervised </span><a id="_idIndexMarker483" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.156.1">machine learning, </span><strong class="bold"><span class="kobospan" id="kobo.157.1">regression</span></strong><span class="kobospan" id="kobo.158.1">, will be described in the </span><span><span class="kobospan" id="kobo.159.1">next subsection.</span></span></p>
<h2 id="_idParaDest-175" class="calibre7"><a id="_idTextAnchor226" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.160.1">Regression</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.161.1">In contrast to classification tasks, models </span><a id="_idIndexMarker484" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.162.1">for regression tasks </span><a id="_idIndexMarker485" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.163.1">map the input values into a </span><strong class="bold"><span class="kobospan" id="kobo.164.1">single output</span></strong><span class="kobospan" id="kobo.165.1"> to provide a continuous value, as illustrated in the </span><span><span class="kobospan" id="kobo.166.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer114">
<span class="kobospan" id="kobo.167.1"><img alt="Figure 7.4: Machine learning regression model" src="image/B20851_07_004.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.168.1">Figure 7.4: Machine learning regression model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">Given the input values, the model is expected to predict the correct value of </span><span><span class="kobospan" id="kobo.170.1">the output.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.171.1">Real-life examples of regression include predicting the value of stocks, the quality of wine, or the market price of a </span><a id="_idIndexMarker486" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.172.1">house, as depicted in the </span><span><span class="kobospan" id="kobo.173.1">following</span></span><span><a id="_idIndexMarker487" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.174.1"> diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer115">
<span class="kobospan" id="kobo.175.1"><img alt="Figure 7.5: House pricing regressor" src="image/B20851_07_005.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.176.1">Figure 7.5: House pricing regressor</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.177.1">In the preceding image, the inputs are features that provide information that describes a given house, while the output is the predicted value of </span><span><span class="kobospan" id="kobo.178.1">the house.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.179.1">Many types of models exist for carrying out classification and regression tasks – some of them are described in the </span><span><span class="kobospan" id="kobo.180.1">following subsection.</span></span></p>
<h2 id="_idParaDest-176" class="calibre7"><a id="_idTextAnchor227" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.181.1">Supervised learning algorithms</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.182.1">As we mentioned </span><a id="_idIndexMarker488" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.183.1">previously, each supervised learning model consists of a set of internal tunable parameters and an algorithm that tunes these parameters in an attempt to achieve the </span><span><span class="kobospan" id="kobo.184.1">required result.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.185.1">Some common supervised learning models/algorithms are </span><span><span class="kobospan" id="kobo.186.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.187.1">Support Vector Machines</span></strong><span class="kobospan" id="kobo.188.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.189.1">SVMs</span></strong><span class="kobospan" id="kobo.190.1">): Algorithms that map the given inputs as points in space so that the inputs that belong </span><a id="_idIndexMarker489" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.191.1">to separate categories are divided by the largest </span><span><span class="kobospan" id="kobo.192.1">possible gap.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.193.1">Decision Trees</span></strong><span class="kobospan" id="kobo.194.1">: A family of algorithms that utilize a tree-like graph, where branching points represent </span><a id="_idIndexMarker490" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.195.1">decisions and the branches represent </span><span><span class="kobospan" id="kobo.196.1">their consequences.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.197.1">Random Forests</span></strong><span class="kobospan" id="kobo.198.1">: Algorithms that</span><a id="_idIndexMarker491" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.199.1"> create a large number of decision trees during the training phase and use a combination of </span><span><span class="kobospan" id="kobo.200.1">their outputs.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.201.1">Artificial Neural Networks</span></strong><span class="kobospan" id="kobo.202.1">: Models that</span><a id="_idIndexMarker492" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.203.1"> consist of multiple simple nodes, or neurons, which can be interconnected in various ways. </span><span class="kobospan" id="kobo.203.2">Each connection can have a weight that controls the level of the signal that’s carried from one neuron to </span><span><span class="kobospan" id="kobo.204.1">the next.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.205.1">There are certain techniques </span><a id="_idIndexMarker493" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.206.1">that can be used to improve and enhance the performance of such models. </span><span class="kobospan" id="kobo.206.2">One interesting technique – </span><strong class="bold"><span class="kobospan" id="kobo.207.1">feature selection</span></strong><span class="kobospan" id="kobo.208.1"> – will be discussed in the </span><span><span class="kobospan" id="kobo.209.1">next section.</span></span></p>
<h1 id="_idParaDest-177" class="calibre5"><a id="_idTextAnchor228" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.210.1">Feature selection in supervised learning</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.211.1">As we saw in the previous section, a </span><a id="_idIndexMarker494" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.212.1">supervised learning model receives a set of inputs, called </span><strong class="bold"><span class="kobospan" id="kobo.213.1">features</span></strong><span class="kobospan" id="kobo.214.1">, and maps them</span><a id="_idIndexMarker495" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.215.1"> to a set of outputs. </span><span class="kobospan" id="kobo.215.2">The assumption is that the information described by the features is useful for determining the value of the corresponding outputs. </span><span class="kobospan" id="kobo.215.3">At first glance, it may seem that the more information we can use as input, the better our </span><a id="_idIndexMarker496" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.216.1">chances of predicting the output(s) correctly. </span><span class="kobospan" id="kobo.216.2">However, in many cases, the opposite holds true; if some of the features we use are irrelevant or redundant, the consequence could be a (sometimes significant) decrease in the accuracy of </span><span><span class="kobospan" id="kobo.217.1">the models.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.218.1">Feature selection is the process of selecting the most beneficial and essential set of features out of the entire given set of features. </span><span class="kobospan" id="kobo.218.2">Besides increasing the accuracy of the model, a successful feature selection can provide the </span><span><span class="kobospan" id="kobo.219.1">following advantages:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.220.1">The training times of the models </span><span><span class="kobospan" id="kobo.221.1">are shorter.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.222.1">The resulting trained models are simpler and easier </span><span><span class="kobospan" id="kobo.223.1">to interpret.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.224.1">The resulting models are likely to provide better generalization, that is, they perform better with new input data that is dissimilar to the data that was used </span><span><span class="kobospan" id="kobo.225.1">for training.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.226.1">When looking at methods to carry out feature selection, genetic algorithms are a natural candidate. </span><span class="kobospan" id="kobo.226.2">We will </span><a id="_idIndexMarker497" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.227.1">demonstrate how they can be applied to find </span><a id="_idIndexMarker498" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.228.1">the best features out of an artificially generated dataset in the </span><span><span class="kobospan" id="kobo.229.1">next section.</span></span></p>
<h1 id="_idParaDest-178" class="calibre5"><a id="_idTextAnchor229" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.230.1">Selecting the features for the Friedman-1 regression problem</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.231.1">The </span><em class="italic"><span class="kobospan" id="kobo.232.1">Friedman-1 </span></em><span class="kobospan" id="kobo.233.1">regression problem, which</span><a id="_idIndexMarker499" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.234.1"> was created by Friedman and Breiman, describes a single output value, </span><span><span class="kobospan" id="kobo.235.1">y</span></span><span> </span><span class="kobospan" id="kobo.236.1">, which is a function of five input values, </span><span><span class="kobospan" id="kobo.237.1">x</span></span><span><span class="kobospan" id="kobo.238.1"> </span></span><span><span class="kobospan" id="kobo.239.1">0</span></span><span><span class="kobospan" id="kobo.240.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.241.1">x</span></span><span><span class="kobospan" id="kobo.242.1"> </span></span><span><span class="kobospan" id="kobo.243.1">1</span></span><span><span class="kobospan" id="kobo.244.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.245.1">x</span></span><span><span class="kobospan" id="kobo.246.1"> </span></span><span><span class="kobospan" id="kobo.247.1">2</span></span><span><span class="kobospan" id="kobo.248.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.249.1">x</span></span><span><span class="kobospan" id="kobo.250.1"> </span></span><span><span class="kobospan" id="kobo.251.1">3</span></span><span><span class="kobospan" id="kobo.252.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.253.1">x</span></span><span><span class="kobospan" id="kobo.254.1"> </span></span><span><span class="kobospan" id="kobo.255.1">4</span></span><span class="kobospan" id="kobo.256.1">, and randomly generated noise, according to the </span><span><span class="kobospan" id="kobo.257.1">following formula:</span></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.258.1">y</span></span><span><span class="kobospan" id="kobo.259.1">(</span></span><span><span class="kobospan" id="kobo.260.1">x</span></span><span><span class="kobospan" id="kobo.261.1"> </span></span><span><span class="kobospan" id="kobo.262.1">0</span></span><span><span class="kobospan" id="kobo.263.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.264.1">x</span></span><span><span class="kobospan" id="kobo.265.1"> </span></span><span><span class="kobospan" id="kobo.266.1">1</span></span><span><span class="kobospan" id="kobo.267.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.268.1">x</span></span><span><span class="kobospan" id="kobo.269.1"> </span></span><span><span class="kobospan" id="kobo.270.1">2</span></span><span><span class="kobospan" id="kobo.271.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.272.1">x</span></span><span><span class="kobospan" id="kobo.273.1"> </span></span><span><span class="kobospan" id="kobo.274.1">3</span></span><span><span class="kobospan" id="kobo.275.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.276.1">x</span></span><span><span class="kobospan" id="kobo.277.1"> </span></span><span><span class="kobospan" id="kobo.278.1">4</span></span><span><span class="kobospan" id="kobo.279.1">)</span></span><span> </span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.280.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.281.1">10</span></span><span> </span><span><span class="kobospan" id="kobo.282.1">∙</span></span><span> </span><span><span class="kobospan" id="kobo.283.1">sin</span></span><span><span class="kobospan" id="kobo.284.1">(</span></span><span><span class="kobospan" id="kobo.285.1">π</span></span><span> </span><span><span class="kobospan" id="kobo.286.1">∙</span></span><span> </span><span><span class="kobospan" id="kobo.287.1">x</span></span><span><span class="kobospan" id="kobo.288.1"> </span></span><span><span class="kobospan" id="kobo.289.1">0</span></span><span> </span><span><span class="kobospan" id="kobo.290.1">∙</span></span><span> </span><span><span class="kobospan" id="kobo.291.1">x</span></span><span><span class="kobospan" id="kobo.292.1"> </span></span><span><span class="kobospan" id="kobo.293.1">1</span></span><span><span class="kobospan" id="kobo.294.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.295.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.296.1">20</span></span><span> </span><span><span class="kobospan" id="kobo.297.1">(</span></span><span><span class="kobospan" id="kobo.298.1">x</span></span><span><span class="kobospan" id="kobo.299.1"> </span></span><span><span class="kobospan" id="kobo.300.1">2</span></span><span> </span><span><span class="kobospan" id="kobo.301.1">−</span></span><span> </span><span><span class="kobospan" id="kobo.302.1">0.5</span></span><span><span class="kobospan" id="kobo.303.1">)</span></span><span><span class="kobospan" id="kobo.304.1"> </span></span><span><span class="kobospan" id="kobo.305.1">2</span></span><span> </span><span><span class="kobospan" id="kobo.306.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.307.1">10</span></span><span> </span><span><span class="kobospan" id="kobo.308.1">x</span></span><span><span class="kobospan" id="kobo.309.1"> </span></span><span><span class="kobospan" id="kobo.310.1">3</span></span><span> </span><span><span class="kobospan" id="kobo.311.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.312.1">5</span></span><span> </span><span><span class="kobospan" id="kobo.313.1">x</span></span><span><span class="kobospan" id="kobo.314.1"> </span></span><span><span class="kobospan" id="kobo.315.1">4</span></span><span> </span><span><span class="kobospan" id="kobo.316.1">+</span></span><span> </span><span><span class="kobospan" id="kobo.317.1">n</span></span><span><span class="kobospan" id="kobo.318.1">o</span></span><span><span class="kobospan" id="kobo.319.1">i</span></span><span><span class="kobospan" id="kobo.320.1">s</span></span><span><span class="kobospan" id="kobo.321.1">e</span></span><span> </span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.322.1">∙</span></span><span> </span><span><span class="kobospan" id="kobo.323.1">N</span></span><span><span class="kobospan" id="kobo.324.1">(</span></span><span><span class="kobospan" id="kobo.325.1">0</span></span><span><span class="kobospan" id="kobo.326.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.327.1">1</span></span><span><span class="kobospan" id="kobo.328.1">)</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.329.1">The input variables, </span><span><span class="kobospan" id="kobo.330.1">x</span></span><span><span class="kobospan" id="kobo.331.1"> </span></span><span><span class="kobospan" id="kobo.332.1">0</span></span><span> </span><span><span class="kobospan" id="kobo.333.1">.</span></span><span> </span><span><span class="kobospan" id="kobo.334.1">.</span></span><span><span class="kobospan" id="kobo.335.1">x</span></span><span><span class="kobospan" id="kobo.336.1"> </span></span><span><span class="kobospan" id="kobo.337.1">4</span></span><span class="kobospan" id="kobo.338.1">, are independent, and uniformly distributed over the interval [0, 1]. </span><span class="kobospan" id="kobo.338.2">The last component in the formula is the randomly generated noise. </span><span class="kobospan" id="kobo.338.3">The noise is </span><strong class="bold"><span class="kobospan" id="kobo.339.1">normally distributed</span></strong><span class="kobospan" id="kobo.340.1"> and multiplied by the constant </span><em class="italic"><span class="kobospan" id="kobo.341.1">noise</span></em><span class="kobospan" id="kobo.342.1">, which determines </span><span><span class="kobospan" id="kobo.343.1">its level.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.344.1">In Python, the </span><strong class="source-inline"><span class="kobospan" id="kobo.345.1">scikit-learn</span></strong><span class="kobospan" id="kobo.346.1"> (</span><strong class="source-inline"><span class="kobospan" id="kobo.347.1">sklearn</span></strong><span class="kobospan" id="kobo.348.1">) library provides us with the </span><strong class="source-inline"><span class="kobospan" id="kobo.349.1">make_friedman1()</span></strong><span class="kobospan" id="kobo.350.1"> function, which can be used to generate a dataset containing the desired number of samples. </span><span class="kobospan" id="kobo.350.2">Each of the samples consists of randomly generated </span><span><span class="kobospan" id="kobo.351.1">x</span></span><span><span class="kobospan" id="kobo.352.1"> </span></span><span><span class="kobospan" id="kobo.353.1">0</span></span><span> </span><span><span class="kobospan" id="kobo.354.1">.</span></span><span> </span><span><span class="kobospan" id="kobo.355.1">.</span></span><span><span class="kobospan" id="kobo.356.1">x</span></span><span><span class="kobospan" id="kobo.357.1"> </span></span><span><span class="kobospan" id="kobo.358.1">4</span></span><span class="kobospan" id="kobo.359.1"> values and their corresponding calculated </span><span><span class="kobospan" id="kobo.360.1">y</span></span><span> </span><span class="kobospan" id="kobo.361.1">value. </span><span class="kobospan" id="kobo.361.2">The interesting part, however, is that we can tell the function to add an arbitrary number of irrelevant input variables to the five original ones by setting the </span><strong class="source-inline"><span class="kobospan" id="kobo.362.1">n_features</span></strong><span class="kobospan" id="kobo.363.1"> parameter of the function to a value larger than five. </span><span class="kobospan" id="kobo.363.2">If, for example, we set the value of </span><strong class="source-inline"><span class="kobospan" id="kobo.364.1">n_features</span></strong><span class="kobospan" id="kobo.365.1"> to 15, we will get a dataset containing the original five input variables (or features) that were used to generate the </span><em class="italic"><span class="kobospan" id="kobo.366.1">y</span></em><span class="kobospan" id="kobo.367.1"> values according to the preceding formula and an additional 10 features that are completely irrelevant to the output. </span><span class="kobospan" id="kobo.367.2">This can be used, for example, to test the resilience of various regression models to noise and the presence of irrelevant features in </span><span><span class="kobospan" id="kobo.368.1">the dataset.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">We can take advantage of this function to test the effectiveness of genetic algorithms as a feature selection mechanism. </span><span class="kobospan" id="kobo.369.2">In our test, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.370.1">make_friedman1()</span></strong><span class="kobospan" id="kobo.371.1"> function to create a dataset with 15 features and use the genetic algorithm to search for the subset of features that provides the best performance. </span><span class="kobospan" id="kobo.371.2">As a result, we expect the genetic algorithm to pick the first five features and drop the rest, assuming that the model’s accuracy is better </span><a id="_idIndexMarker500" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.372.1">when only the relevant features are used as input. </span><span class="kobospan" id="kobo.372.2">The fitness function of the genetic algorithm will utilize a regression model that, for each potential solution a subset of the original features will be trained using the dataset containing only the </span><span><span class="kobospan" id="kobo.373.1">selected features.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.374.1">As usual, we will start by choosing an appropriate representation for the solution, as described in the </span><span><span class="kobospan" id="kobo.375.1">next subsection.</span></span></p>
<h2 id="_idParaDest-179" class="calibre7"><a id="_idTextAnchor230" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.376.1">Solution representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.377.1">The objective of our algorithm</span><a id="_idIndexMarker501" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.378.1"> is to find a subset of features that yield the best performance. </span><span class="kobospan" id="kobo.378.2">Therefore, a solution needs to indicate which features are chosen and which are dropped. </span><span class="kobospan" id="kobo.378.3">One obvious way to go about this is to represent </span><a id="_idIndexMarker502" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.379.1">each individual using a </span><strong class="bold"><span class="kobospan" id="kobo.380.1">list of binary values</span></strong><span class="kobospan" id="kobo.381.1">. </span><span class="kobospan" id="kobo.381.2">Every entry in that list corresponds to one of the features in the dataset. </span><span class="kobospan" id="kobo.381.3">A value of 1 represents selecting the corresponding feature, while a value of 0 means that the feature has not been selected. </span><span class="kobospan" id="kobo.381.4">This is very similar to the approach we used</span><a id="_idIndexMarker503" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.382.1"> in the </span><strong class="bold"><span class="kobospan" id="kobo.383.1">knapsack 0-1 problem</span></strong><span class="kobospan" id="kobo.384.1"> we described in </span><a href="B20851_04.xhtml#_idTextAnchor155" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.385.1">Chapter 4</span></em></span></a><em class="italic"><span class="kobospan" id="kobo.386.1">, </span></em><span><em class="italic"><span class="kobospan" id="kobo.387.1">Combinatorial Optimization</span></em></span><span><span class="kobospan" id="kobo.388.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.389.1">The presence of each 0 in the solution will be translated into dropping the corresponding feature’s data column from the dataset, as we will see in the </span><span><span class="kobospan" id="kobo.390.1">next subsection.</span></span></p>
<h2 id="_idParaDest-180" class="calibre7"><a id="_idTextAnchor231" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.391.1">Python problem representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.392.1">To encapsulate the Friedman-1</span><a id="_idIndexMarker504" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.393.1"> feature selection problem, we’ve created a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.394.1">Friedman1Test</span></strong><span class="kobospan" id="kobo.395.1">. </span><span class="kobospan" id="kobo.395.2">This class can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.396.1">friedman.py</span></strong><span class="kobospan" id="kobo.397.1"> file, which is located </span><span><span class="kobospan" id="kobo.398.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/friedman.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.399.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/friedman.py</span></span></a><span><span class="kobospan" id="kobo.400.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.401.1">The main parts of this class are </span><span><span class="kobospan" id="kobo.402.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.403.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.404.1">__init__()</span></strong><span class="kobospan" id="kobo.405.1"> method of the class creates the dataset, </span><span><span class="kobospan" id="kobo.406.1">as follows:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.407.1">
self.X, self.y = </span><strong class="bold1"><span class="kobospan1" id="kobo.408.1">datasets.make_friedman1</span></strong><span class="kobospan1" id="kobo.409.1">(
    n_samples=self.numSamples,
    n_features=self.</span><strong class="bold1"><span class="kobospan1" id="kobo.410.1">numFeatures</span></strong><span class="kobospan1" id="kobo.411.1">,
    noise=self.NOISE,
    random_state=self.randomSeed)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.412.1">Then, it divides</span><a id="_idIndexMarker505" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.413.1"> the data into two subsets—a training set and a validation set—using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.414.1">scikit-learn</span></strong> <span><strong class="source-inline1"><span class="kobospan" id="kobo.415.1">model_selection.train_test_split()</span></strong></span><span><span class="kobospan" id="kobo.416.1"> method:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.417.1">
self.X_train,self.X_validation,self.y_train,self.y_validation = \
    model_selection.</span><strong class="bold1"><span class="kobospan1" id="kobo.418.1">train_test_split</span></strong><span class="kobospan1" id="kobo.419.1">(self.X, self.y,
        test_size=self.VALIDATION_SIZE,
        random_state=self.randomSeed)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.420.1">Dividing the data into</span><a id="_idIndexMarker506" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.421.1"> a </span><strong class="bold"><span class="kobospan" id="kobo.422.1">train set </span></strong><span class="kobospan" id="kobo.423.1">and a </span><strong class="bold"><span class="kobospan" id="kobo.424.1">validation set</span></strong><span class="kobospan" id="kobo.425.1"> allows us to train the regression model on the train set, where the </span><a id="_idIndexMarker507" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.426.1">correct prediction is given to the model for training purposes, and then test it with the separate validation set, where the correct predictions are not given to the model and are, instead, compared to the predictions it produces. </span><span class="kobospan" id="kobo.426.2">This way, we can test how well the model is able to generalize, rather than memorize, the </span><span><span class="kobospan" id="kobo.427.1">training data.</span></span></p></li> <li class="calibre11"><span class="kobospan" id="kobo.428.1">Next, we create the regression model, for which we chose the </span><strong class="bold"><span class="kobospan" id="kobo.429.1">Gradient Boosting Regressor</span></strong><span class="kobospan" id="kobo.430.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.431.1">GBR</span></strong><span class="kobospan" id="kobo.432.1">) type. </span><span class="kobospan" id="kobo.432.2">This </span><a id="_idIndexMarker508" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.433.1">model creates an </span><strong class="bold"><span class="kobospan" id="kobo.434.1">ensemble</span></strong><span class="kobospan" id="kobo.435.1"> (or aggregation) of decision trees during the </span><span><span class="kobospan" id="kobo.436.1">training phase:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.437.1">
self.regressor = </span><strong class="bold1"><span class="kobospan1" id="kobo.438.1">GradientBoostingRegressor</span></strong><span class="kobospan1" id="kobo.439.1">(\
    random_state=self.randomSeed)</span></pre></li> </ol>
<p class="callout-heading"><span class="kobospan" id="kobo.440.1">Important Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.441.1">In our example, we are passing the random seed along so that it can be used internally by the regressor. </span><span class="kobospan" id="kobo.441.2">This way, we can make sure the results that we obtain </span><span><span class="kobospan" id="kobo.442.1">are repeatable.</span></span></p>
<ol class="calibre15">
<li value="4" class="calibre11"><span class="kobospan" id="kobo.443.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.444.1">getMSE()</span></strong><span class="kobospan" id="kobo.445.1"> method of the class is used to determine the performance of our gradient-boosting regression model for a set of selected features. </span><span class="kobospan" id="kobo.445.2">It accepts a list of binary values corresponding to the features in the dataset—a value of 1 represents selecting the corresponding feature, while a value of 0 means that the feature is</span><a id="_idIndexMarker509" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.446.1"> dropped. </span><span class="kobospan" id="kobo.446.2">The method then deletes the columns in the training and validation sets that correspond to the </span><span><span class="kobospan" id="kobo.447.1">unselected features:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.448.1">zeroIndices</span></strong><span class="kobospan1" id="kobo.449.1"> = [i for i, n in enumerate(zeroOneList) if n == 0]
currentX_train = </span><strong class="bold1"><span class="kobospan1" id="kobo.450.1">np.delete</span></strong><span class="kobospan1" id="kobo.451.1">(self.X_train, zeroIndices, 1)
currentX_validation = </span><strong class="bold1"><span class="kobospan1" id="kobo.452.1">np.delete</span></strong><span class="kobospan1" id="kobo.453.1">(self.X_validation, 
    zeroIndices, 1)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.454.1">The modified train set—containing only the selected features—is then used to train the regressor, while the modified validation set is used to evaluate </span><span><span class="kobospan" id="kobo.455.1">its predictions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.456.1">
self.</span><strong class="bold1"><span class="kobospan1" id="kobo.457.1">regressor.fit</span></strong><span class="kobospan1" id="kobo.458.1">(currentX_train, self.</span><strong class="bold1"><span class="kobospan1" id="kobo.459.1">y_train</span></strong><span class="kobospan1" id="kobo.460.1">)
prediction = self.</span><strong class="bold1"><span class="kobospan1" id="kobo.461.1">regressor.predict</span></strong><span class="kobospan1" id="kobo.462.1">(currentX_validation)
return </span><strong class="bold1"><span class="kobospan1" id="kobo.463.1">mean_squared_error</span></strong><span class="kobospan1" id="kobo.464.1">(self.</span><strong class="bold1"><span class="kobospan1" id="kobo.465.1">y_validation</span></strong><span class="kobospan1" id="kobo.466.1">, prediction)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.467.1">The metric used here to evaluate the regressor is called the </span><strong class="bold"><span class="kobospan" id="kobo.468.1">mean square error</span></strong><span class="kobospan" id="kobo.469.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.470.1">MSE</span></strong><span class="kobospan" id="kobo.471.1">), which finds the average</span><a id="_idIndexMarker510" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.472.1"> squared difference between the model’s predicted values and the actual values. </span><span class="kobospan" id="kobo.472.2">A </span><em class="italic"><span class="kobospan" id="kobo.473.1">lower </span></em><span class="kobospan" id="kobo.474.1">value of this measurement indicates </span><em class="italic"><span class="kobospan" id="kobo.475.1">better</span></em><span class="kobospan" id="kobo.476.1"> performance of </span><span><span class="kobospan" id="kobo.477.1">the regressor.</span></span></p></li> <li class="calibre11"><span class="kobospan" id="kobo.478.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.479.1">main()</span></strong><span class="kobospan" id="kobo.480.1"> method of the class creates an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.481.1">Friedman1Test</span></strong><span class="kobospan" id="kobo.482.1"> class with 15 features. </span><span class="kobospan" id="kobo.482.2">Then, it repeatedly uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.483.1">getMSE()</span></strong><span class="kobospan" id="kobo.484.1"> method to evaluate the performance of the regressor with the first </span><em class="italic"><span class="kobospan" id="kobo.485.1">n</span></em><span class="kobospan" id="kobo.486.1"> features, while </span><em class="italic"><span class="kobospan" id="kobo.487.1">n</span></em><span class="kobospan" id="kobo.488.1"> is incremented from 1 </span><span><span class="kobospan" id="kobo.489.1">to 15:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.490.1">
for n in range(1, len(test) + 1):
    </span><strong class="bold1"><span class="kobospan1" id="kobo.491.1">nFirstFeatures </span></strong><span class="kobospan1" id="kobo.492.1">= [1] * n + [0] * (len(test) - n)
    score = test.</span><strong class="bold1"><span class="kobospan1" id="kobo.493.1">getMSE</span></strong><span class="kobospan1" id="kobo.494.1">(nFirstFeatures)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.495.1">When running the main method, the results show that, as we add the first five features one by one, the </span><a id="_idIndexMarker511" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.496.1">performance improves. </span><span class="kobospan" id="kobo.496.2">However, afterward, each additional feature degrades the performance of </span><span><span class="kobospan" id="kobo.497.1">the regressor:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.498.1">
1 first features: score = 47.553993
2 first features: score = 26.121143
3 first features: score = 18.509415
4 first features: score = 7.322589
5 first features: score = 6.702669
6 first features: score = 7.677197
7 first features: score = 11.614536
8 first features: score = 11.294010
9 first features: score = 10.858028
10 first features: score = 11.602919
11 first features: score = 15.017591
12 first features: score = 14.258221
13 first features: score = 15.274851
14 first features: score = 15.726690
15 first features: score = 17.187479</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.499.1">This is further illustrated by the generated plot, showing the minimum MSE value where the first five features </span><span><span class="kobospan" id="kobo.500.1">are used:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer116">
<span class="kobospan" id="kobo.501.1"><img alt="Figure 7.6: Plot of error values for the Friedman-1 regression problem" src="image/B20851_07_006.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.502.1">Figure 7.6: Plot of error values for the Friedman-1 regression problem</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.503.1">In the next subsection, we </span><a id="_idIndexMarker512" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.504.1">will find out if a genetic algorithm can successfully identify these first </span><span><span class="kobospan" id="kobo.505.1">five features.</span></span></p>
<h2 id="_idParaDest-181" class="calibre7"><a id="_idTextAnchor232" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.506.1">Genetic algorithms solution</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.507.1">To identify the best set of</span><a id="_idIndexMarker513" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.508.1"> features to be used for our regression test using a genetic algorithm, we’ve created the Python program, </span><strong class="source-inline"><span class="kobospan" id="kobo.509.1">01_solve_friedman.py</span></strong><span class="kobospan" id="kobo.510.1">, which is located </span><span><span class="kobospan" id="kobo.511.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/01_solve_friedman.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.512.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/01_solve_friedman.py</span></span></a><span><span class="kobospan" id="kobo.513.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.514.1">As a reminder, the chromosome representation that’s being used here is a list of integers with values of 0 or 1, denoting whether a feature should be used or dropped. </span><span class="kobospan" id="kobo.514.2">This makes our problem, from the point of view of the genetic algorithm, similar to the </span><em class="italic"><span class="kobospan" id="kobo.515.1">OneMax</span></em><span class="kobospan" id="kobo.516.1"> problem, or the </span><em class="italic"><span class="kobospan" id="kobo.517.1">knapsack 0-1</span></em><span class="kobospan" id="kobo.518.1"> problem we solved previously. </span><span class="kobospan" id="kobo.518.2">The difference is in the fitness function returning the regression model’s MSE, which is calculated within the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.519.1">Friedman1Test</span></strong></span><span><span class="kobospan" id="kobo.520.1"> class.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.521.1">The following steps describe the main parts of </span><span><span class="kobospan" id="kobo.522.1">our solution:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.523.1">First, we need to create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.524.1">Friedman1Test</span></strong><span class="kobospan" id="kobo.525.1"> class with the </span><span><span class="kobospan" id="kobo.526.1">desired parameters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.527.1">
friedman = friedman.</span><strong class="bold1"><span class="kobospan1" id="kobo.528.1">Friedman1Test</span></strong><span class="kobospan1" id="kobo.529.1">(NUM_OF_FEATURES, \
    NUM_OF_SAMPLES, RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.530.1">Since our goal is to minimize the MSE of the regression model, we define a single objective, minimizing the </span><span><span class="kobospan" id="kobo.531.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.532.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.533.1">FitnessMin</span></strong><span class="kobospan1" id="kobo.534.1">", base.Fitness, weights=(-1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.535.1">Since the solution is represented by a list of 0 or 1 integer values, we use the following toolbox</span><a id="_idIndexMarker514" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.536.1"> definitions to create the </span><span><span class="kobospan" id="kobo.537.1">initial population:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.538.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.539.1">zeroOrOne</span></strong><span class="kobospan1" id="kobo.540.1">", random.randint, 0, 1)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.541.1">individualCreator</span></strong><span class="kobospan1" id="kobo.542.1">",\
    tools.initRepeat, creator.Individual, \
    toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.543.1">zeroOrOne</span></strong><span class="kobospan1" id="kobo.544.1">, len(friedman))
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.545.1">populationCreator</span></strong><span class="kobospan1" id="kobo.546.1">", tools.initRepeat, \
    list, toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.547.1">individualCreator</span></strong><span class="kobospan1" id="kobo.548.1">)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.549.1">Then, we instruct the genetic algorithm to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.550.1">getMSE()</span></strong><span class="kobospan" id="kobo.551.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.552.1">Friedman1Test</span></strong><span class="kobospan" id="kobo.553.1"> instance for </span><span><span class="kobospan" id="kobo.554.1">fitness evaluation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.555.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.556.1">friedmanTestScore</span></strong><span class="kobospan1" id="kobo.557.1">(individual):
    return friedman.</span><strong class="bold1"><span class="kobospan1" id="kobo.558.1">getMSE</span></strong><span class="kobospan1" id="kobo.559.1">(individual),  # return a tuple
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.560.1">evaluate</span></strong><span class="kobospan1" id="kobo.561.1">", friedmanTestScore)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.562.1">As for the genetic operators, we use </span><em class="italic"><span class="kobospan" id="kobo.563.1">tournament selection</span></em><span class="kobospan" id="kobo.564.1"> with a tournament size of 2 and </span><em class="italic"><span class="kobospan" id="kobo.565.1">crossover</span></em><span class="kobospan" id="kobo.566.1"> and </span><em class="italic"><span class="kobospan" id="kobo.567.1">mutation</span></em><span class="kobospan" id="kobo.568.1"> operators that are specialized for binary </span><span><span class="kobospan" id="kobo.569.1">list chromosomes:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.570.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.571.1">select</span></strong><span class="kobospan1" id="kobo.572.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.573.1">mate</span></strong><span class="kobospan1" id="kobo.574.1">", tools.cxTwoPoint)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.575.1">mutate</span></strong><span class="kobospan1" id="kobo.576.1">", tools.mutFlipBit, \
    indpb=1.0/len(friedman))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.577.1">In addition, we continue to use the </span><em class="italic"><span class="kobospan" id="kobo.578.1">elitist approach</span></em><span class="kobospan" id="kobo.579.1">, where the </span><strong class="bold"><span class="kobospan" id="kobo.580.1">hall of fame</span></strong><span class="kobospan" id="kobo.581.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.582.1">HOF</span></strong><span class="kobospan" id="kobo.583.1">) members – the current </span><a id="_idIndexMarker515" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.584.1">best individuals – are always passed untouched to the </span><span><span class="kobospan" id="kobo.585.1">next</span></span><span><a id="_idIndexMarker516" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.586.1"> generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.587.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.588.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.589.1">(
    population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.590.1">By running the algorithm for 30 generations with a population size of 30, we get the </span><span><span class="kobospan" id="kobo.591.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.592.1">
-- Best Ever Individual = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
-- Best Ever Fitness = 6.702668910463287</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.593.1">This indicates that the first five features have been selected to provide the best MSE (about 6.7) for our test. </span><span class="kobospan" id="kobo.593.2">Note that the genetic algorithm makes no assumptions about the set of features that it was looking for, meaning it did not know that we were looking for a subset of the first </span><em class="italic"><span class="kobospan" id="kobo.594.1">n</span></em><span class="kobospan" id="kobo.595.1"> features. </span><span class="kobospan" id="kobo.595.2">It simply searched for the best possible subset </span><span><span class="kobospan" id="kobo.596.1">of features.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.597.1">In the next section, we</span><a id="_idIndexMarker517" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.598.1"> will advance from using artificially generated data to an actual dataset, and utilize the genetic algorithm to select the best features for a </span><span><span class="kobospan" id="kobo.599.1">classification problem.</span></span></p>
<h1 id="_idParaDest-182" class="calibre5"><a id="_idTextAnchor233" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.600.1">Selecting the features for classifying the Zoo dataset</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.601.1">The UCI Machine Learning Repository (</span><a href="https://archive.ics.uci.edu/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.602.1">https://archive.ics.uci.edu/</span></a><span class="kobospan" id="kobo.603.1">) maintains over 600 datasets as a service to the machine learning community. </span><span class="kobospan" id="kobo.603.2">These datasets can be used for experimentation with various models </span><a id="_idIndexMarker518" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.604.1">and algorithms. </span><span class="kobospan" id="kobo.604.2">A typical dataset contains a number of features (inputs) and the desired output, in theform </span><a id="_idIndexMarker519" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.605.1">of columns, with a description of </span><span><span class="kobospan" id="kobo.606.1">their meaning.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.607.1">In this section, we will use the UCI Zoo dataset  (</span><a href="https://archive.ics.uci.edu/dataset/111/zoo" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.608.1">https://archive.ics.uci.edu/dataset/111/zoo</span></a><span class="kobospan" id="kobo.609.1">). </span><span class="kobospan" id="kobo.609.2">This dataset describes 101 different animals using the following </span><span><span class="kobospan" id="kobo.610.1">18 features:</span></span></p>
<table class="no-table-style" id="table001-5">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.611.1">No.</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.612.1">Feature Name</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.613.1">Data Type</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.614.1">1</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.615.1">animal name</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.616.1">unique for </span><span><span class="kobospan" id="kobo.617.1">each instance</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.618.1">2</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.619.1">hair</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.620.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.621.1">3</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.622.1">feathers</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.623.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.624.1">4</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.625.1">eggs</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.626.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.627.1">5</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.628.1">milk</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.629.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.630.1">6</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.631.1">airborne</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.632.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.633.1">7</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.634.1">aquatic</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.635.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.636.1">8</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.637.1">predator</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.638.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.639.1">9</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.640.1">toothed</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.641.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.642.1">10</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.643.1">backbone</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.644.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.645.1">11</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.646.1">breathes</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.647.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.648.1">12</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.649.1">venomous</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.650.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.651.1">13</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.652.1">fins</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.653.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.654.1">14</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.655.1">legs</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.656.1">Numeric (set of </span><span><span class="kobospan" id="kobo.657.1">values {0,2,4,5,6,8})</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.658.1">15</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.659.1">tail</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.660.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.661.1">16</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.662.1">domestic</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.663.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.664.1">17</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.665.1">catsize</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.666.1">boolean</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.667.1">18</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.668.1">type</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.669.1">numeric (integer values in the </span><span><span class="kobospan" id="kobo.670.1">range [1..7])</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.671.1">Table 7.1: Feature list for the Zoo dataset</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.672.1">Most features are </span><strong class="source-inline"><span class="kobospan" id="kobo.673.1">Boolean</span></strong><span class="kobospan" id="kobo.674.1"> (value of 1 or 0), indicating the presence or absence of a certain attribute, such as </span><strong class="source-inline"><span class="kobospan" id="kobo.675.1">hair</span></strong><span class="kobospan" id="kobo.676.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.677.1">fins</span></strong><span class="kobospan" id="kobo.678.1">, and so on. </span><span class="kobospan" id="kobo.678.2">The first feature, </span><strong class="source-inline"><span class="kobospan" id="kobo.679.1">animal name</span></strong><span class="kobospan" id="kobo.680.1">, is just to provide us with </span><a id="_idIndexMarker520" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.681.1">added information and does not participate in the </span><span><span class="kobospan" id="kobo.682.1">learning process.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.683.1">This dataset is used for testing</span><a id="_idIndexMarker521" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.684.1"> classification tasks, where the input features need to be mapped into two or more categories/labels. </span><span class="kobospan" id="kobo.684.2">In this dataset, the last feature, called </span><strong class="source-inline"><span class="kobospan" id="kobo.685.1">type</span></strong><span class="kobospan" id="kobo.686.1">, represents the category and is used as the </span><strong class="bold"><span class="kobospan" id="kobo.687.1">output</span></strong><span class="kobospan" id="kobo.688.1"> value. </span><span class="kobospan" id="kobo.688.2">For this value, there are seven categories altogether. </span><span class="kobospan" id="kobo.688.3">A </span><strong class="source-inline"><span class="kobospan" id="kobo.689.1">type</span></strong><span class="kobospan" id="kobo.690.1"> value of </span><strong class="source-inline"><span class="kobospan" id="kobo.691.1">5</span></strong><span class="kobospan" id="kobo.692.1">, for instance, represents the animal category that includes frog, newt, </span><span><span class="kobospan" id="kobo.693.1">and toad.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.694.1">To sum this up, a classification model trained with this dataset will use features 2–17 (</span><strong class="source-inline"><span class="kobospan" id="kobo.695.1">hair</span></strong><span class="kobospan" id="kobo.696.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.697.1">feathers</span></strong><span class="kobospan" id="kobo.698.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.699.1">fins</span></strong><span class="kobospan" id="kobo.700.1">, and so on) to predict the value of feature 18 (</span><span><span class="kobospan" id="kobo.701.1">animal </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.702.1">type</span></strong></span><span><span class="kobospan" id="kobo.703.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.704.1">Once again, we want to use a genetic algorithm to select the features that will give us the best predictions. </span><span class="kobospan" id="kobo.704.2">Let’s start </span><a id="_idIndexMarker522" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.705.1">by creating a Python class that</span><a id="_idIndexMarker523" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.706.1"> represents a classifier that’s been trained with </span><span><span class="kobospan" id="kobo.707.1">this dataset.</span></span></p>
<h2 id="_idParaDest-183" class="calibre7"><a id="_idTextAnchor234" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.708.1">Python problem representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.709.1">To encapsulate the feature</span><a id="_idIndexMarker524" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.710.1"> selection process for the Zoo dataset classification task, we’ve created a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.711.1">Zoo</span></strong><span class="kobospan" id="kobo.712.1">. </span><span class="kobospan" id="kobo.712.2">This class is contained in the </span><strong class="source-inline"><span class="kobospan" id="kobo.713.1">zoo.py</span></strong><span class="kobospan" id="kobo.714.1"> file, which is </span><span><span class="kobospan" id="kobo.715.1">located at:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/zoo.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.716.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/zoo.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.717.1">The main parts of this class are highlighted </span><span><span class="kobospan" id="kobo.718.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.719.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.720.1">__init__()</span></strong><span class="kobospan" id="kobo.721.1"> method of the class loads the Zoo dataset from the web while skipping the first feature—</span><strong class="source-inline1"><span class="kobospan" id="kobo.722.1">animal </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.723.1">name</span></strong></span><span><span class="kobospan" id="kobo.724.1">—as follows:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.725.1">
self.data = read_csv(self.DATASET_URL, header=None, 
    usecols=range(1, 18))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.726.1">Then, it separates the data to input features (first remaining 16 columns) and the resulting category (</span><span><span class="kobospan" id="kobo.727.1">last column):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.728.1">
self.X = self.data.iloc[:, 0:16]
self.y = self.data.iloc[:, 16]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.729.1">Instead of just separating the data into a training set and a test set, like we did in the previous section, we’re using </span><strong class="bold"><span class="kobospan" id="kobo.730.1">k-fold cross-validation</span></strong><span class="kobospan" id="kobo.731.1">. </span><span class="kobospan" id="kobo.731.2">This means that the data is split into </span><em class="italic"><span class="kobospan" id="kobo.732.1">k</span></em><span class="kobospan" id="kobo.733.1"> equal parts </span><a id="_idIndexMarker525" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.734.1">and the model is evaluated </span><em class="italic"><span class="kobospan" id="kobo.735.1">k</span></em><span class="kobospan" id="kobo.736.1"> times, each time using </span><em class="italic"><span class="kobospan" id="kobo.737.1">(k-1)</span></em><span class="kobospan" id="kobo.738.1"> parts for training and the remaining part for testing (or </span><em class="italic"><span class="kobospan" id="kobo.739.1">validation</span></em><span class="kobospan" id="kobo.740.1">). </span><span class="kobospan" id="kobo.740.2">This is easy to do in Python using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.741.1">scikit-learn</span></strong><span class="kobospan" id="kobo.742.1"> library’s </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.743.1">model_selection.KFold()</span></strong></span><span><span class="kobospan" id="kobo.744.1"> method:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.745.1">
self.kfold = </span><strong class="bold1"><span class="kobospan1" id="kobo.746.1">model_selection.KFold</span></strong><span class="kobospan1" id="kobo.747.1">(
    n_splits=self.</span><strong class="bold1"><span class="kobospan1" id="kobo.748.1">NUM_FOLDS</span></strong><span class="kobospan1" id="kobo.749.1">,
    random_state=self.randomSeed)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.750.1">Next, we create a classification </span><a id="_idIndexMarker526" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.751.1">model based on a </span><strong class="bold"><span class="kobospan" id="kobo.752.1">decision tree</span></strong><span class="kobospan" id="kobo.753.1">. </span><span class="kobospan" id="kobo.753.2">This type of classifier creates a tree structure during the training phase that splits the dataset into smaller subsets, eventually resulting in </span><span><span class="kobospan" id="kobo.754.1">a prediction:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.755.1">
self.</span><strong class="bold1"><span class="kobospan1" id="kobo.756.1">classifier</span></strong><span class="kobospan1" id="kobo.757.1"> = </span><strong class="bold1"><span class="kobospan1" id="kobo.758.1">DecisionTreeClassifier</span></strong><span class="kobospan1" id="kobo.759.1">(
    random_state=self.randomSeed)</span></pre></li> </ol>
<p class="callout-heading"><span class="kobospan" id="kobo.760.1">Important Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.761.1">We are passing a random seed so that it can be used internally by the classifier. </span><span class="kobospan" id="kobo.761.2">This way, we can make sure the results that are obtained </span><span><span class="kobospan" id="kobo.762.1">are repeatable.</span></span></p>
<ol class="calibre15">
<li value="5" class="calibre11"><span class="kobospan" id="kobo.763.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.764.1">getMeanAccuracy()</span></strong><span class="kobospan" id="kobo.765.1"> method of the class is used to evaluate the performance of the classifier for a set of selected features. </span><span class="kobospan" id="kobo.765.2">Similar to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.766.1">getMSE()</span></strong><span class="kobospan" id="kobo.767.1"> method in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.768.1">Friedman1Test</span></strong><span class="kobospan" id="kobo.769.1"> class, this method accepts a list of binary values</span><a id="_idIndexMarker527" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.770.1"> corresponding to the features in the dataset—a value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.771.1">1</span></strong><span class="kobospan" id="kobo.772.1"> represents selecting the corresponding feature, while a value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.773.1">0</span></strong><span class="kobospan" id="kobo.774.1"> means that the feature is dropped. </span><span class="kobospan" id="kobo.774.2">The method then drops the columns in the dataset that correspond to the </span><span><span class="kobospan" id="kobo.775.1">unselected features:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.776.1">zeroIndices </span></strong><span class="kobospan1" id="kobo.777.1">= [i for i, n in enumerate(</span><strong class="bold1"><span class="kobospan1" id="kobo.778.1">zeroOneList</span></strong><span class="kobospan1" id="kobo.779.1">) if n == 0]
currentX = self.X.</span><strong class="bold1"><span class="kobospan1" id="kobo.780.1">drop</span></strong><span class="kobospan1" id="kobo.781.1">(self.X.columns[</span><strong class="bold1"><span class="kobospan1" id="kobo.782.1">zeroIndices</span></strong><span class="kobospan1" id="kobo.783.1">], axis=1)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.784.1">This modified dataset—containing only the selected features—is then used to perform the </span><strong class="bold"><span class="kobospan" id="kobo.785.1">k-fold cross-validation</span></strong><span class="kobospan" id="kobo.786.1"> process and determine the classifier’s performance over the data partitions. </span><span class="kobospan" id="kobo.786.2">The</span><a id="_idIndexMarker528" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.787.1"> value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.788.1">k</span></strong><span class="kobospan" id="kobo.789.1"> in our class is set to </span><strong class="source-inline1"><span class="kobospan" id="kobo.790.1">5</span></strong><span class="kobospan" id="kobo.791.1">, so five evaluations take place </span><span><span class="kobospan" id="kobo.792.1">each time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.793.1">
cv_results = model_selection.</span><strong class="bold1"><span class="kobospan1" id="kobo.794.1">cross_val_score</span></strong><span class="kobospan1" id="kobo.795.1">(
    self.classifier, currentX, self.y, cv=self.</span><strong class="bold1"><span class="kobospan1" id="kobo.796.1">kfold</span></strong><span class="kobospan1" id="kobo.797.1">,
    scoring=</span><strong class="bold1"><span class="kobospan1" id="kobo.798.1">'accuracy'</span></strong><span class="kobospan1" id="kobo.799.1">)
return cv_results.</span><strong class="bold1"><span class="kobospan1" id="kobo.800.1">mean</span></strong><span class="kobospan1" id="kobo.801.1">()</span></pre><p class="calibre3"><span class="kobospan" id="kobo.802.1">The metric that’s being used here to evaluate the classifier is accuracy—the portion of the cases that were classified correctly. </span><span class="kobospan" id="kobo.802.2">An accuracy of 0.85, for example, means that 85% of the cases were classified correctly. </span><span class="kobospan" id="kobo.802.3">Since, in our case, we train and evaluate the classifier </span><em class="italic"><span class="kobospan" id="kobo.803.1">k</span></em><span class="kobospan" id="kobo.804.1"> times, we use the average (mean) accuracy value that was obtained over </span><span><span class="kobospan" id="kobo.805.1">these evaluations.</span></span></p></li> <li class="calibre11"><span class="kobospan" id="kobo.806.1">The main method of the </span><a id="_idIndexMarker529" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.807.1">class creates an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.808.1">Zoo</span></strong><span class="kobospan" id="kobo.809.1"> class and evaluates the classifier with all 16 features that are present using the all-one </span><span><span class="kobospan" id="kobo.810.1">solution representation:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.811.1">allOnes </span></strong><span class="kobospan1" id="kobo.812.1">= [1] * len(zoo)
print("-- All features selected: ", allOnes, ", accuracy = ", 
    zoo.</span><strong class="bold1"><span class="kobospan1" id="kobo.813.1">getMeanAccuracy</span></strong><span class="kobospan1" id="kobo.814.1">(allOnes))</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.815.1">When running the main method of the class, the printout shows that, after testing our classifier with 5-fold cross-validation using all 16 features, the classification accuracy that’s achieved is </span><span><span class="kobospan" id="kobo.816.1">about 91%:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.817.1">
-- All features selected:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], accuracy =  0.9099999999999999</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.818.1">In the next subsection, we will attempt to improve the accuracy of the classifier by selecting a subset of features from the dataset, instead of using all the features. </span><span class="kobospan" id="kobo.818.2">We will use—you guessed it—a genetic algorithm to select these features </span><span><span class="kobospan" id="kobo.819.1">for us.</span></span></p>
<h2 id="_idParaDest-184" class="calibre7"><a id="_idTextAnchor235" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.820.1">Genetic algorithms solution</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.821.1">To identify the best set of features to </span><a id="_idIndexMarker530" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.822.1">be used for our Zoo classification task using a genetic algorithm, we’ve created the Python program </span><strong class="source-inline"><span class="kobospan" id="kobo.823.1">02_solve_zoo.py</span></strong><span class="kobospan" id="kobo.824.1">, which is located at </span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/02_solve_zoo.py" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.825.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/02_solve_zoo.py</span></a><span class="kobospan" id="kobo.826.1">. </span><span class="kobospan" id="kobo.826.2">As in the previous section, the chromosome representation that’s being used here is a list of integers with the values of </span><strong class="source-inline"><span class="kobospan" id="kobo.827.1">0</span></strong><span class="kobospan" id="kobo.828.1"> or </span><strong class="source-inline"><span class="kobospan" id="kobo.829.1">1</span></strong><span class="kobospan" id="kobo.830.1">, denoting whether a feature should be used </span><span><span class="kobospan" id="kobo.831.1">or dropped.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.832.1">The following steps highlight the main parts of </span><span><span class="kobospan" id="kobo.833.1">the program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.834.1">First, we need to create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.835.1">Zoo</span></strong><span class="kobospan" id="kobo.836.1"> class and pass our random seed along for the sake of producing </span><span><span class="kobospan" id="kobo.837.1">repeatable results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.838.1">
zoo = zoo.</span><strong class="bold1"><span class="kobospan1" id="kobo.839.1">Zoo</span></strong><span class="kobospan1" id="kobo.840.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.841.1">Since our goal is to maximize the accuracy of the classifier model, we define a single objective, maximizing</span><a id="_idIndexMarker531" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.842.1"> the </span><span><span class="kobospan" id="kobo.843.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.844.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.845.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.846.1">", base.Fitness, weights=(</span><strong class="bold1"><span class="kobospan1" id="kobo.847.1">1.0</span></strong><span class="kobospan1" id="kobo.848.1">,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.849.1">Just like in the previous section, we use the following toolbox definitions to create the initial population of individuals, each constructed as a list of </span><strong class="source-inline1"><span class="kobospan" id="kobo.850.1">0</span></strong><span class="kobospan" id="kobo.851.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.852.1">1</span></strong> <span><span class="kobospan" id="kobo.853.1">integer values:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.854.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.855.1">zeroOrOne</span></strong><span class="kobospan1" id="kobo.856.1">", random.randint, 0, 1)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.857.1">individualCreator</span></strong><span class="kobospan1" id="kobo.858.1">", tools.initRepeat, \
    creator.Individual, toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.859.1">zeroOrOne</span></strong><span class="kobospan1" id="kobo.860.1">, len(zoo))
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.861.1">populationCreator</span></strong><span class="kobospan1" id="kobo.862.1">", tools.initRepeat, \
    list, toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.863.1">individualCreator</span></strong><span class="kobospan1" id="kobo.864.1">)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.865.1">Then, we instruct the genetic algorithm to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.866.1">getMeanAccuracy()</span></strong><span class="kobospan" id="kobo.867.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.868.1">Zoo</span></strong><span class="kobospan" id="kobo.869.1"> instance for fitness evaluation. </span><span class="kobospan" id="kobo.869.2">To do this, we have to make </span><span><span class="kobospan" id="kobo.870.1">two modifications:</span></span><ul class="calibre16"><li class="calibre11"><span class="kobospan" id="kobo.871.1">We eliminate the possibility of no features being selected (all-zeros individual) since our classifier will throw an exception in such </span><span><span class="kobospan" id="kobo.872.1">a case.</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.873.1">We add a small </span><em class="italic"><span class="kobospan" id="kobo.874.1">penalty</span></em><span class="kobospan" id="kobo.875.1"> for each feature being used to encourage the selection of fewer features. </span><span class="kobospan" id="kobo.875.2">The penalty value is very small (0.001), so it only comes into play as a tie-breaker between two equally performing classifiers, leading the algorithm to prefer the one that uses </span><span><span class="kobospan" id="kobo.876.1">fewer features:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.877.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.878.1">zooClassificationAccuracy</span></strong><span class="kobospan1" id="kobo.879.1">(individual):
    numFeaturesUsed = sum(individual)
    if </span><strong class="bold1"><span class="kobospan1" id="kobo.880.1">numFeaturesUsed == 0</span></strong><span class="kobospan1" id="kobo.881.1">:
        return 0.0,
    else:
        accuracy = zoo.</span><strong class="bold1"><span class="kobospan1" id="kobo.882.1">getMeanAccuracy</span></strong><span class="kobospan1" id="kobo.883.1">(individual)
    return accuracy - FEATURE_PENALTY_FACTOR * 
        </span><strong class="bold1"><span class="kobospan1" id="kobo.884.1">numFeaturesUsed</span></strong><span class="kobospan1" id="kobo.885.1">,  # return a tuple
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.886.1">evaluate</span></strong><span class="kobospan1" id="kobo.887.1">", zooClassificationAccuracy)</span></pre></li></ul></li> <li class="calibre11"><span class="kobospan" id="kobo.888.1">For the genetic operators, we again use </span><strong class="bold"><span class="kobospan" id="kobo.889.1">tournament selection</span></strong><span class="kobospan" id="kobo.890.1"> with a tournament size of </span><strong class="source-inline1"><span class="kobospan" id="kobo.891.1">2</span></strong><span class="kobospan" id="kobo.892.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.893.1">crossover</span></strong><span class="kobospan" id="kobo.894.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.895.1">mutation</span></strong><span class="kobospan" id="kobo.896.1"> operators that are specialized for binary </span><span><span class="kobospan" id="kobo.897.1">list chromosomes:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.898.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.899.1">select</span></strong><span class="kobospan1" id="kobo.900.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.901.1">mate</span></strong><span class="kobospan1" id="kobo.902.1">", tools.cxTwoPoint)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.903.1">mutate</span></strong><span class="kobospan1" id="kobo.904.1">", tools.mutFlipBit, indpb=1.0/len(zoo))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.905.1">And once again, we continue to use the </span><strong class="bold"><span class="kobospan" id="kobo.906.1">elitist approach</span></strong><span class="kobospan" id="kobo.907.1">, where HOF members—the current best individuals—are always passed untouched to the </span><span><span class="kobospan" id="kobo.908.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.909.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.910.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.911.1">(population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.912.1">At the end of the run, we print out all the members of the HOF so that we can see the top results that were found by the algorithm. </span><span class="kobospan" id="kobo.912.2">We print both the fitness value, which includes the</span><a id="_idIndexMarker532" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.913.1"> penalty for the number of features, and the actual </span><span><span class="kobospan" id="kobo.914.1">accuracy value:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.915.1">
print("- Best solutions are:")
for i in range(HALL_OF_FAME_SIZE):
    print(
        i, ": ", hof.items[i],
        ", fitness = ", hof.items[i].fitness.values[0],
        ", accuracy = ", zoo.getMeanAccuracy(hof.items[i]),
        ", features = ", sum(hof.items[i])
    )</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.916.1">By running the algorithm for 50 generations with a population size of 50 and HOF size of 5, we get the </span><span><span class="kobospan" id="kobo.917.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.918.1">
- Best solutions are:
0 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0] , fitness = 0.964 , accuracy = 0.97 , features = 6
1 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1] , fitness = 0.963 , accuracy = 0.97 , features = 7
2 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7
3 : [1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7
4 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.919.1">These results indicate that all five top solutions achieved an accuracy value of 97%, using either six or seven features </span><a id="_idIndexMarker533" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.920.1">out of the available 16. </span><span class="kobospan" id="kobo.920.2">Thanks to the penalty factor on a number of features, the top solution is the set of six features, which are </span><span><span class="kobospan" id="kobo.921.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.922.1">feathers</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.923.1">milk</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.924.1">airborne</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.925.1">backbone</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.926.1">fins</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.927.1">tail</span></strong></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.928.1">In conclusion, by selecting these particular features out of the 16 given in the dataset, not only did we reduce the dimensionality of the problem, but we were also able to improve our model’s accuracy from 91% to 97%. </span><span class="kobospan" id="kobo.928.2">If this does not seem like a large enhancement at first glance, think of it as reducing the error rate from 9% to 3% – a very significant improvement in terms of </span><span><span class="kobospan" id="kobo.929.1">classification performance.</span></span></p>
<h1 id="_idParaDest-185" class="calibre5"><a id="_idTextAnchor236" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.930.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.931.1">In this chapter, you were introduced to machine learning and the two main types of supervised machine learning tasks – </span><em class="italic"><span class="kobospan" id="kobo.932.1">regression</span></em><span class="kobospan" id="kobo.933.1"> and </span><em class="italic"><span class="kobospan" id="kobo.934.1">classification</span></em><span class="kobospan" id="kobo.935.1">. </span><span class="kobospan" id="kobo.935.2">Then, you were presented with the potential benefits of </span><em class="italic"><span class="kobospan" id="kobo.936.1">feature selection</span></em><span class="kobospan" id="kobo.937.1"> on the performance of the models carrying out these tasks. </span><span class="kobospan" id="kobo.937.2">At the heart of this chapter were two demonstrations of how genetic algorithms can be utilized to enhance the performance of such models via feature selection. </span><span class="kobospan" id="kobo.937.3">In the first case, we pinpointed the genuine features that were generated by the </span><em class="italic"><span class="kobospan" id="kobo.938.1">Friedman-1 Test</span></em><span class="kobospan" id="kobo.939.1"> regression problem, while, in the other case, we selected the most beneficial features of the </span><em class="italic"><span class="kobospan" id="kobo.940.1">Zoo </span></em><span><em class="italic"><span class="kobospan" id="kobo.941.1">classification dataset</span></em></span><span><span class="kobospan" id="kobo.942.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.943.1">In the next chapter, we will look at another possible way of enhancing the performance of supervised machine learning models, namely </span><span><strong class="bold"><span class="kobospan" id="kobo.944.1">hyperparameter tuning</span></strong></span><span><span class="kobospan" id="kobo.945.1">.</span></span></p>
<h1 id="_idParaDest-186" class="calibre5"><a id="_idTextAnchor237" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.946.1">Further reading</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.947.1">For more information about the topics that were covered in this chapter, please refer to the </span><span><span class="kobospan" id="kobo.948.1">following resources:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.949.1">Applied Supervised Learning with Python</span></em><span class="kobospan" id="kobo.950.1">, Benjamin Johnston and Ishita Mathur, April </span><span><span class="kobospan" id="kobo.951.1">26, 2019</span></span></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.952.1">Feature Engineering Made Easy</span></em><span class="kobospan" id="kobo.953.1">, Sinan Ozdemir and Divya Susarla, January </span><span><span class="kobospan" id="kobo.954.1">22, 2018</span></span></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.955.1">Feature selection for classification</span></em><span class="kobospan" id="kobo.956.1">, M.Dash and H.Liu, </span><span><span class="kobospan" id="kobo.957.1">1997: </span></span><a href="https://www.sciencedirect.com/science/article/abs/pii/S1088467X97000085?via%3Dihub" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.958.1">https://doi.org/10.1016/S1088-467X(97)00008-5</span></span></a></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.959.1">UCI Machine Learning </span></em><span><em class="italic"><span class="kobospan" id="kobo.960.1">Repository</span></em></span><span><span class="kobospan" id="kobo.961.1">: </span></span><a href="https://archive.ics.uci.edu/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.962.1">https://archive.ics.uci.edu/</span></span></a></li>
</ul>
</div>
</body></html>