["```py\nIF <input value> IS LESS THEN <some threshold value>\n    THEN <go to some target branch>\n```", "```py\n    self.X, self.y = datasets.make_friedman1(\n        n_samples=self.numSamples,\n        n_features=self.numFeatures,\n        noise=self.NOISE,\n        random_state=self.randomSeed)\n    ```", "```py\n    self.X_train,self.X_validation,self.y_train,self.y_validation = \\\n        model_selection.train_test_split(self.X, self.y,\n            test_size=self.VALIDATION_SIZE,\n            random_state=self.randomSeed)\n    ```", "```py\n    self.regressor = GradientBoostingRegressor(\\\n        random_state=self.randomSeed)\n    ```", "```py\n    zeroIndices = [i for i, n in enumerate(zeroOneList) if n == 0]\n    currentX_train = np.delete(self.X_train, zeroIndices, 1)\n    currentX_validation = np.delete(self.X_validation, \n        zeroIndices, 1)\n    ```", "```py\n    self.regressor.fit(currentX_train, self.y_train)\n    prediction = self.regressor.predict(currentX_validation)\n    return mean_squared_error(self.y_validation, prediction)\n    ```", "```py\n    for n in range(1, len(test) + 1):\n        nFirstFeatures = [1] * n + [0] * (len(test) - n)\n        score = test.getMSE(nFirstFeatures)\n    ```", "```py\n1 first features: score = 47.553993\n2 first features: score = 26.121143\n3 first features: score = 18.509415\n4 first features: score = 7.322589\n5 first features: score = 6.702669\n6 first features: score = 7.677197\n7 first features: score = 11.614536\n8 first features: score = 11.294010\n9 first features: score = 10.858028\n10 first features: score = 11.602919\n11 first features: score = 15.017591\n12 first features: score = 14.258221\n13 first features: score = 15.274851\n14 first features: score = 15.726690\n15 first features: score = 17.187479\n```", "```py\n    friedman = friedman.Friedman1Test(NUM_OF_FEATURES, \\\n        NUM_OF_SAMPLES, RANDOM_SEED)\n    ```", "```py\n    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n    ```", "```py\n    toolbox.register(\"zeroOrOne\", random.randint, 0, 1)\n    toolbox.register(\"individualCreator\",\\\n        tools.initRepeat, creator.Individual, \\\n        toolbox.zeroOrOne, len(friedman))\n    toolbox.register(\"populationCreator\", tools.initRepeat, \\\n        list, toolbox.individualCreator)\n    ```", "```py\n    def friedmanTestScore(individual):\n        return friedman.getMSE(individual),  # return a tuple\n    toolbox.register(\"evaluate\", friedmanTestScore)\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\", tools.cxTwoPoint)\n    toolbox.register(\"mutate\", tools.mutFlipBit, \\\n        indpb=1.0/len(friedman))\n    ```", "```py\n    population, logbook = elitism.eaSimpleWithElitism(\n        population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\n-- Best Ever Individual = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n-- Best Ever Fitness = 6.702668910463287\n```", "```py\n    self.data = read_csv(self.DATASET_URL, header=None, \n        usecols=range(1, 18))\n    ```", "```py\n    self.X = self.data.iloc[:, 0:16]\n    self.y = self.data.iloc[:, 16]\n    ```", "```py\n    self.kfold = model_selection.KFold(\n        n_splits=self.NUM_FOLDS,\n        random_state=self.randomSeed)\n    ```", "```py\n    self.classifier = DecisionTreeClassifier(\n        random_state=self.randomSeed)\n    ```", "```py\n    zeroIndices = [i for i, n in enumerate(zeroOneList) if n == 0]\n    currentX = self.X.drop(self.X.columns[zeroIndices], axis=1)\n    ```", "```py\n    cv_results = model_selection.cross_val_score(\n        self.classifier, currentX, self.y, cv=self.kfold,\n        scoring='accuracy')\n    return cv_results.mean()\n    ```", "```py\n    allOnes = [1] * len(zoo)\n    print(\"-- All features selected: \", allOnes, \", accuracy = \", \n        zoo.getMeanAccuracy(allOnes))\n    ```", "```py\n-- All features selected:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], accuracy =  0.9099999999999999\n```", "```py\n    zoo = zoo.Zoo(RANDOM_SEED)\n    ```", "```py\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    ```", "```py\n    toolbox.register(\"zeroOrOne\", random.randint, 0, 1)\n    toolbox.register(\"individualCreator\", tools.initRepeat, \\\n        creator.Individual, toolbox.zeroOrOne, len(zoo))\n    toolbox.register(\"populationCreator\", tools.initRepeat, \\\n        list, toolbox.individualCreator)\n    ```", "```py\n        def zooClassificationAccuracy(individual):\n            numFeaturesUsed = sum(individual)\n            if numFeaturesUsed == 0:\n                return 0.0,\n            else:\n                accuracy = zoo.getMeanAccuracy(individual)\n            return accuracy - FEATURE_PENALTY_FACTOR * \n                numFeaturesUsed,  # return a tuple\n        toolbox.register(\"evaluate\", zooClassificationAccuracy)\n        ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\", tools.cxTwoPoint)\n    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=1.0/len(zoo))\n    ```", "```py\n    population, logbook = elitism.eaSimpleWithElitism(population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\n    print(\"- Best solutions are:\")\n    for i in range(HALL_OF_FAME_SIZE):\n        print(\n            i, \": \", hof.items[i],\n            \", fitness = \", hof.items[i].fitness.values[0],\n            \", accuracy = \", zoo.getMeanAccuracy(hof.items[i]),\n            \", features = \", sum(hof.items[i])\n        )\n    ```", "```py\n- Best solutions are:\n0 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0] , fitness = 0.964 , accuracy = 0.97 , features = 6\n1 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1] , fitness = 0.963 , accuracy = 0.97 , features = 7\n2 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7\n3 : [1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7\n4 : [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0] , fitness = 0.963 , accuracy = 0.97 , features = 7\n```"]