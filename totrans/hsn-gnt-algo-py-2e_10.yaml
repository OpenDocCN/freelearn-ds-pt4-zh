- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enhancing Machine Learning Models Using Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes how genetic algorithms can be used to improve the performance
    of **supervised machine learning** models by selecting the best subset of features
    from the provided input data. We will start with a brief introduction to machine
    learning and then describe the two main types of supervised machine learning tasks
    – **regression** and **classification**. We will then discuss the potential benefits
    of **feature selection** when it comes to the performance of these models. Next,
    we will demonstrate how genetic algorithms can be utilized to pinpoint the genuine
    features that are generated by the **Friedman-1 Test** regression problem. Then,
    we will use the real-life **Zoo dataset** to create a classification model and
    improve its accuracy – again by applying genetic algorithms to isolate the best
    features for the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the basic concepts of supervised machine learning, as well as regression
    and classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the benefits of feature selection on the performance of supervised
    learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhance the performance of a regression model for the Friedman-1 Test regression
    problem, using feature selection carried out by a genetic algorithm coded with
    the DEAP framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhance the performance of a classification model for the Zoo dataset classification
    problem, using feature selection carried out by a genetic algorithm coded with
    the DEAP framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start this chapter with a quick review of supervised machine learning.
    If you are a seasoned data scientist, feel free to skip the introductory sections.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**deap**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**numpy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**matplotlib**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seaborn**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn** – introduced in this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will be using the *UCI Zoo* *Dataset* ([https://archive.ics.uci.edu/ml/datasets/zoo](https://archive.ics.uci.edu/ml/datasets/zoo)).
  prefs: []
  type: TYPE_NORMAL
- en: The programs that will be used in this chapter can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_07](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_07).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/OEBOd](https://packt.link/OEBOd).'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **machine learning** typically refers to a computer program that receives
    input and produces output. Our goal is to train this program, also known as the
    **model**, to produce the correct output for the given input *without explicitly*
    *programming it*.
  prefs: []
  type: TYPE_NORMAL
- en: During this training process, the model learns the mapping between the inputs
    and the outputs by adjusting its internal parameters. One common way to train
    the model is by providing it with a set of inputs for which the correct output
    is known. For each of these inputs, we tell the model what the correct output
    is so that it can adjust, or tune itself, aiming to eventually produce the desired
    output for each of the given inputs. This tuning is at the heart of the learning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the years, many types of machine learning models have been developed.
    Each model has its own particular internal parameters that can affect the mapping
    between the input and the output, and the values of these parameters can be tuned,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: Parameter tuning of a machine learning model](img/B20851_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Parameter tuning of a machine learning model'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the model was implementing a *decision tree*, it could contain
    several `IF- THEN` statements, which can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this case, both the threshold value and the identity of the target branch
    are parameters that can be adjusted, or tuned, during the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: To tune the internal parameters, each type of model has an accompanying *learning
    algorithm* that iterates over the given input and output values and seeks to match
    the given output for each of the given inputs. To accomplish this goal, a typical
    learning algorithm will measure the difference (also called *error*, or more generally
    *loss*) between the actual output and the desired output; the algorithm will then
    attempt to minimize this error by adjusting the model’s internal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The two main types of supervised machine learning are **classification** and
    **regression**, and will be described in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When carrying out a classification task, the model needs to decide which *category*
    a certain input belongs to. Each category is represented by a single output (called
    a **label**), while the inputs are called **features**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Machine learning classification model](img/B20851_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Machine learning classification model'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the well-known *Iris dataset* ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)),
    there are four features: **Petal length**, **Petal width**, **Sepal length**,
    and **Sepal width**. These represent the measurements that have been manually
    taken of actual Iris flowers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the output, there are three labels: **Iris setosa**, **Iris virginica**,
    and **Iris versicolor**. These represent the three different types of Iris in
    the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When input values, which represent the measurements that were taken from a
    given Iris flower, are present we expect the output of the correct label to go
    high and the other two to go low:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Iris Flower classifier illustrated](img/B20851_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Iris Flower classifier illustrated'
  prefs: []
  type: TYPE_NORMAL
- en: Classification tasks have a multitude of real-life applications, such as approval
    of bank loans and credit cards, email spam detection, handwritten digit recognition,
    and face recognition. Later in this chapter, we will be demonstrating the classification
    of animal types using the *Zoo dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: The second main type of supervised machine learning, **regression**, will be
    described in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast to classification tasks, models for regression tasks map the input
    values into a **single output** to provide a continuous value, as illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: Machine learning regression model](img/B20851_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Machine learning regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Given the input values, the model is expected to predict the correct value of
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-life examples of regression include predicting the value of stocks, the
    quality of wine, or the market price of a house, as depicted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: House pricing regressor](img/B20851_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: House pricing regressor'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, the inputs are features that provide information that
    describes a given house, while the output is the predicted value of the house.
  prefs: []
  type: TYPE_NORMAL
- en: Many types of models exist for carrying out classification and regression tasks
    – some of them are described in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned previously, each supervised learning model consists of a set
    of internal tunable parameters and an algorithm that tunes these parameters in
    an attempt to achieve the required result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common supervised learning models/algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**): Algorithms that map the given inputs
    as points in space so that the inputs that belong to separate categories are divided
    by the largest possible gap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Trees**: A family of algorithms that utilize a tree-like graph,
    where branching points represent decisions and the branches represent their consequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Forests**: Algorithms that create a large number of decision trees
    during the training phase and use a combination of their outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial Neural Networks**: Models that consist of multiple simple nodes,
    or neurons, which can be interconnected in various ways. Each connection can have
    a weight that controls the level of the signal that’s carried from one neuron
    to the next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are certain techniques that can be used to improve and enhance the performance
    of such models. One interesting technique – **feature selection** – will be discussed
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection in supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous section, a supervised learning model receives a set
    of inputs, called **features**, and maps them to a set of outputs. The assumption
    is that the information described by the features is useful for determining the
    value of the corresponding outputs. At first glance, it may seem that the more
    information we can use as input, the better our chances of predicting the output(s)
    correctly. However, in many cases, the opposite holds true; if some of the features
    we use are irrelevant or redundant, the consequence could be a (sometimes significant)
    decrease in the accuracy of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature selection is the process of selecting the most beneficial and essential
    set of features out of the entire given set of features. Besides increasing the
    accuracy of the model, a successful feature selection can provide the following
    advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: The training times of the models are shorter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting trained models are simpler and easier to interpret.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting models are likely to provide better generalization, that is, they
    perform better with new input data that is dissimilar to the data that was used
    for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When looking at methods to carry out feature selection, genetic algorithms are
    a natural candidate. We will demonstrate how they can be applied to find the best
    features out of an artificially generated dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the features for the Friedman-1 regression problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *Friedman-1* regression problem, which was created by Friedman and Breiman,
    describes a single output value, y , which is a function of five input values,
    x 0, x 1, x 2, x 3, x 4, and randomly generated noise, according to the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: y(x 0, x 1, x 2, x 3, x 4)
  prefs: []
  type: TYPE_NORMAL
- en: = 10 ∙ sin(π ∙ x 0 ∙ x 1) + 20 (x 2 − 0.5) 2 + 10 x 3 + 5 x 4 + noise
  prefs: []
  type: TYPE_NORMAL
- en: ∙ N(0, 1)
  prefs: []
  type: TYPE_NORMAL
- en: The input variables, x 0 . .x 4, are independent, and uniformly distributed
    over the interval [0, 1]. The last component in the formula is the randomly generated
    noise. The noise is **normally distributed** and multiplied by the constant *noise*,
    which determines its level.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, the `scikit-learn` (`sklearn`) library provides us with the `make_friedman1()`
    function, which can be used to generate a dataset containing the desired number
    of samples. Each of the samples consists of randomly generated x 0 . .x 4 values
    and their corresponding calculated y value. The interesting part, however, is
    that we can tell the function to add an arbitrary number of irrelevant input variables
    to the five original ones by setting the `n_features` parameter of the function
    to a value larger than five. If, for example, we set the value of `n_features`
    to 15, we will get a dataset containing the original five input variables (or
    features) that were used to generate the *y* values according to the preceding
    formula and an additional 10 features that are completely irrelevant to the output.
    This can be used, for example, to test the resilience of various regression models
    to noise and the presence of irrelevant features in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We can take advantage of this function to test the effectiveness of genetic
    algorithms as a feature selection mechanism. In our test, we will use the `make_friedman1()`
    function to create a dataset with 15 features and use the genetic algorithm to
    search for the subset of features that provides the best performance. As a result,
    we expect the genetic algorithm to pick the first five features and drop the rest,
    assuming that the model’s accuracy is better when only the relevant features are
    used as input. The fitness function of the genetic algorithm will utilize a regression
    model that, for each potential solution a subset of the original features will
    be trained using the dataset containing only the selected features.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we will start by choosing an appropriate representation for the solution,
    as described in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of our algorithm is to find a subset of features that yield the
    best performance. Therefore, a solution needs to indicate which features are chosen
    and which are dropped. One obvious way to go about this is to represent each individual
    using a **list of binary values**. Every entry in that list corresponds to one
    of the features in the dataset. A value of 1 represents selecting the corresponding
    feature, while a value of 0 means that the feature has not been selected. This
    is very similar to the approach we used in the **knapsack 0-1 problem** we described
    in [*Chapter 4*](B20851_04.xhtml#_idTextAnchor155)*,* *Combinatorial Optimization*.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of each 0 in the solution will be translated into dropping the
    corresponding feature’s data column from the dataset, as we will see in the next
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Python problem representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To encapsulate the Friedman-1 feature selection problem, we’ve created a Python
    class called `Friedman1Test`. This class can be found in the `friedman.py` file,
    which is located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/friedman.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/friedman.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main parts of this class are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **__init__()** method of the class creates the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, it divides the data into two subsets—a training set and a validation
    set—using the **scikit-learn** **model_selection.train_test_split()** method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Dividing the data into a **train set** and a **validation set** allows us to
    train the regression model on the train set, where the correct prediction is given
    to the model for training purposes, and then test it with the separate validation
    set, where the correct predictions are not given to the model and are, instead,
    compared to the predictions it produces. This way, we can test how well the model
    is able to generalize, rather than memorize, the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we create the regression model, for which we chose the **Gradient Boosting
    Regressor** (**GBR**) type. This model creates an **ensemble** (or aggregation)
    of decision trees during the training phase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we are passing the random seed along so that it can be used
    internally by the regressor. This way, we can make sure the results that we obtain
    are repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **getMSE()** method of the class is used to determine the performance of
    our gradient-boosting regression model for a set of selected features. It accepts
    a list of binary values corresponding to the features in the dataset—a value of
    1 represents selecting the corresponding feature, while a value of 0 means that
    the feature is dropped. The method then deletes the columns in the training and
    validation sets that correspond to the unselected features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The modified train set—containing only the selected features—is then used to
    train the regressor, while the modified validation set is used to evaluate its
    predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The metric used here to evaluate the regressor is called the **mean square error**
    (**MSE**), which finds the average squared difference between the model’s predicted
    values and the actual values. A *lower* value of this measurement indicates *better*
    performance of the regressor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The **main()** method of the class creates an instance of the **Friedman1Test**
    class with 15 features. Then, it repeatedly uses the **getMSE()** method to evaluate
    the performance of the regressor with the first *n* features, while *n* is incremented
    from 1 to 15:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When running the main method, the results show that, as we add the first five
    features one by one, the performance improves. However, afterward, each additional
    feature degrades the performance of the regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is further illustrated by the generated plot, showing the minimum MSE
    value where the first five features are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Plot of error values for the Friedman-1 regression problem](img/B20851_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Plot of error values for the Friedman-1 regression problem'
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will find out if a genetic algorithm can successfully
    identify these first five features.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify the best set of features to be used for our regression test using
    a genetic algorithm, we’ve created the Python program, `01_solve_friedman.py`,
    which is located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/01_solve_friedman.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/01_solve_friedman.py).
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the chromosome representation that’s being used here is a list
    of integers with values of 0 or 1, denoting whether a feature should be used or
    dropped. This makes our problem, from the point of view of the genetic algorithm,
    similar to the *OneMax* problem, or the *knapsack 0-1* problem we solved previously.
    The difference is in the fitness function returning the regression model’s MSE,
    which is calculated within the `Friedman1Test` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe the main parts of our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create an instance of the **Friedman1Test** class with the
    desired parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since our goal is to minimize the MSE of the regression model, we define a
    single objective, minimizing the fitness strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the solution is represented by a list of 0 or 1 integer values, we use
    the following toolbox definitions to create the initial population:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we instruct the genetic algorithm to use the **getMSE()** method of the
    **Friedman1Test** instance for fitness evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As for the genetic operators, we use *tournament selection* with a tournament
    size of 2 and *crossover* and *mutation* operators that are specialized for binary
    list chromosomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition, we continue to use the *elitist approach*, where the **hall of
    fame** (**HOF**) members – the current best individuals – are always passed untouched
    to the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By running the algorithm for 30 generations with a population size of 30, we
    get the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that the first five features have been selected to provide the
    best MSE (about 6.7) for our test. Note that the genetic algorithm makes no assumptions
    about the set of features that it was looking for, meaning it did not know that
    we were looking for a subset of the first *n* features. It simply searched for
    the best possible subset of features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will advance from using artificially generated data
    to an actual dataset, and utilize the genetic algorithm to select the best features
    for a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the features for classifying the Zoo dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The UCI Machine Learning Repository ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/))
    maintains over 600 datasets as a service to the machine learning community. These
    datasets can be used for experimentation with various models and algorithms. A
    typical dataset contains a number of features (inputs) and the desired output,
    in theform of columns, with a description of their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the UCI Zoo dataset ([https://archive.ics.uci.edu/dataset/111/zoo](https://archive.ics.uci.edu/dataset/111/zoo)).
    This dataset describes 101 different animals using the following 18 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No.** | **Feature Name** | **Data Type** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | animal name | unique for each instance |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | hair | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | feathers | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | eggs | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | milk | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | airborne | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | aquatic | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | predator | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | toothed | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | backbone | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | breathes | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | venomous | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | fins | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | legs | Numeric (set of values {0,2,4,5,6,8}) |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | tail | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | domestic | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | catsize | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | type | numeric (integer values in the range [1..7]) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: Feature list for the Zoo dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Most features are `Boolean` (value of 1 or 0), indicating the presence or absence
    of a certain attribute, such as `hair`, `fins`, and so on. The first feature,
    `animal name`, is just to provide us with added information and does not participate
    in the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is used for testing classification tasks, where the input features
    need to be mapped into two or more categories/labels. In this dataset, the last
    feature, called `type`, represents the category and is used as the `type` value
    of `5`, for instance, represents the animal category that includes frog, newt,
    and toad.
  prefs: []
  type: TYPE_NORMAL
- en: To sum this up, a classification model trained with this dataset will use features
    2–17 (`hair`, `feathers`, `fins`, and so on) to predict the value of feature 18
    (animal `type`).
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we want to use a genetic algorithm to select the features that will
    give us the best predictions. Let’s start by creating a Python class that represents
    a classifier that’s been trained with this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Python problem representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To encapsulate the feature selection process for the Zoo dataset classification
    task, we’ve created a Python class called `Zoo`. This class is contained in the
    `zoo.py` file, which is located at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/zoo.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/zoo.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main parts of this class are highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **__init__()** method of the class loads the Zoo dataset from the web while
    skipping the first feature—**animal** **name**—as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, it separates the data to input features (first remaining 16 columns)
    and the resulting category (last column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instead of just separating the data into a training set and a test set, like
    we did in the previous section, we’re using **k-fold cross-validation**. This
    means that the data is split into *k* equal parts and the model is evaluated *k*
    times, each time using *(k-1)* parts for training and the remaining part for testing
    (or *validation*). This is easy to do in Python using the **scikit-learn** library’s
    **model_selection.KFold()** method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a classification model based on a **decision tree**. This type
    of classifier creates a tree structure during the training phase that splits the
    dataset into smaller subsets, eventually resulting in a prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We are passing a random seed so that it can be used internally by the classifier.
    This way, we can make sure the results that are obtained are repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **getMeanAccuracy()** method of the class is used to evaluate the performance
    of the classifier for a set of selected features. Similar to the **getMSE()**
    method in the **Friedman1Test** class, this method accepts a list of binary values
    corresponding to the features in the dataset—a value of **1** represents selecting
    the corresponding feature, while a value of **0** means that the feature is dropped.
    The method then drops the columns in the dataset that correspond to the unselected
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This modified dataset—containing only the selected features—is then used to
    perform the **k-fold cross-validation** process and determine the classifier’s
    performance over the data partitions. The value of **k** in our class is set to
    **5**, so five evaluations take place each time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The metric that’s being used here to evaluate the classifier is accuracy—the
    portion of the cases that were classified correctly. An accuracy of 0.85, for
    example, means that 85% of the cases were classified correctly. Since, in our
    case, we train and evaluate the classifier *k* times, we use the average (mean)
    accuracy value that was obtained over these evaluations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The main method of the class creates an instance of the **Zoo** class and evaluates
    the classifier with all 16 features that are present using the all-one solution
    representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When running the main method of the class, the printout shows that, after testing
    our classifier with 5-fold cross-validation using all 16 features, the classification
    accuracy that’s achieved is about 91%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the next subsection, we will attempt to improve the accuracy of the classifier
    by selecting a subset of features from the dataset, instead of using all the features.
    We will use—you guessed it—a genetic algorithm to select these features for us.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify the best set of features to be used for our Zoo classification task
    using a genetic algorithm, we’ve created the Python program `02_solve_zoo.py`,
    which is located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/02_solve_zoo.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_07/02_solve_zoo.py).
    As in the previous section, the chromosome representation that’s being used here
    is a list of integers with the values of `0` or `1`, denoting whether a feature
    should be used or dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps highlight the main parts of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create an instance of the **Zoo** class and pass our random
    seed along for the sake of producing repeatable results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since our goal is to maximize the accuracy of the classifier model, we define
    a single objective, maximizing the fitness strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Just like in the previous section, we use the following toolbox definitions
    to create the initial population of individuals, each constructed as a list of
    **0** or **1** integer values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we instruct the genetic algorithm to use the **getMeanAccuracy()** method
    of the **Zoo** instance for fitness evaluation. To do this, we have to make two
    modifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We eliminate the possibility of no features being selected (all-zeros individual)
    since our classifier will throw an exception in such a case.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We add a small *penalty* for each feature being used to encourage the selection
    of fewer features. The penalty value is very small (0.001), so it only comes into
    play as a tie-breaker between two equally performing classifiers, leading the
    algorithm to prefer the one that uses fewer features:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'For the genetic operators, we again use **tournament selection** with a tournament
    size of **2** and **crossover** and **mutation** operators that are specialized
    for binary list chromosomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And once again, we continue to use the **elitist approach**, where HOF members—the
    current best individuals—are always passed untouched to the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the end of the run, we print out all the members of the HOF so that we can
    see the top results that were found by the algorithm. We print both the fitness
    value, which includes the penalty for the number of features, and the actual accuracy
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By running the algorithm for 50 generations with a population size of 50 and
    HOF size of 5, we get the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'These results indicate that all five top solutions achieved an accuracy value
    of 97%, using either six or seven features out of the available 16\. Thanks to
    the penalty factor on a number of features, the top solution is the set of six
    features, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**feathers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**milk**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**airborne**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**backbone**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fins**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tail**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, by selecting these particular features out of the 16 given in
    the dataset, not only did we reduce the dimensionality of the problem, but we
    were also able to improve our model’s accuracy from 91% to 97%. If this does not
    seem like a large enhancement at first glance, think of it as reducing the error
    rate from 9% to 3% – a very significant improvement in terms of classification
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to machine learning and the two main types
    of supervised machine learning tasks – *regression* and *classification*. Then,
    you were presented with the potential benefits of *feature selection* on the performance
    of the models carrying out these tasks. At the heart of this chapter were two
    demonstrations of how genetic algorithms can be utilized to enhance the performance
    of such models via feature selection. In the first case, we pinpointed the genuine
    features that were generated by the *Friedman-1 Test* regression problem, while,
    in the other case, we selected the most beneficial features of the *Zoo* *classification
    dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at another possible way of enhancing the performance
    of supervised machine learning models, namely **hyperparameter tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about the topics that were covered in this chapter, please
    refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Applied Supervised Learning with Python*, Benjamin Johnston and Ishita Mathur,
    April 26, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature Engineering Made Easy*, Sinan Ozdemir and Divya Susarla, January 22,
    2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feature selection for classification*, M.Dash and H.Liu, 1997: [https://doi.org/10.1016/S1088-467X(97)00008-5](https://www.sciencedirect.com/science/article/abs/pii/S1088467X97000085?via%3Dihub)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*UCI Machine Learning* *Repository*: [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
