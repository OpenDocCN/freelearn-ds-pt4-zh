<html><head></head><body>
<div id="_idContainer119" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-187"><a id="_idTextAnchor238" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-188" class="calibre5"><a id="_idTextAnchor239" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Hyperparameter Tuning of Machine Learning Models</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter describes how genetic algorithms can be used to improve the performance of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">supervised machine learning</span></strong><span class="kobospan" id="kobo.5.1"> models</span><a id="_idIndexMarker534" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.6.1"> by tuning the hyperparameters of the models. </span><span class="kobospan" id="kobo.6.2">The chapter will start with a brief introduction to </span><strong class="bold"><span class="kobospan" id="kobo.7.1">hyperparameter tuning</span></strong><span class="kobospan" id="kobo.8.1"> in </span><a id="_idIndexMarker535" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.9.1">machine learning before describing the concept of a </span><strong class="bold"><span class="kobospan" id="kobo.10.1">grid search</span></strong><span class="kobospan" id="kobo.11.1">. </span><span class="kobospan" id="kobo.11.2">After introducing the Wine dataset and the adaptive boosting </span><a id="_idIndexMarker536" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.12.1">classifier, both of which will be used throughout this chapter, we will demonstrate hyperparameter tuning using both a conventional grid search and a genetic-algorithm-driven grid search. </span><span class="kobospan" id="kobo.12.2">Finally, we will attempt to enhance the results we get by using a direct genetic algorithm approach for </span><span><span class="kobospan" id="kobo.13.1">hyperparameter tuning.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.14.1">By the end of this chapter, you will be able to do </span><span><span class="kobospan" id="kobo.15.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.16.1">Demonstrate familiarity with the concept of hyperparameter tuning in </span><span><span class="kobospan" id="kobo.17.1">machine learning</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.18.1">Demonstrate familiarity with the Wine dataset and the adaptive </span><span><span class="kobospan" id="kobo.19.1">boosting classifier</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.20.1">Enhance the performance of a classifier using a hyperparameter </span><span><span class="kobospan" id="kobo.21.1">grid search</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.22.1">Enhance the performance of a classifier using a genetic-algorithm-driven hyperparameter </span><span><span class="kobospan" id="kobo.23.1">grid search</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.24.1">Enhance the performance of a classifier using a direct genetic algorithm approach for </span><span><span class="kobospan" id="kobo.25.1">hyperparameter tuning</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.26.1">We will start this chapter with a quick overview of hyperparameters in machine learning. </span><span class="kobospan" id="kobo.26.2">If you are a seasoned data scientist, feel free to skip the </span><span><span class="kobospan" id="kobo.27.1">introductory section.</span></span></p>
<h1 id="_idParaDest-189" class="calibre5"><a id="_idTextAnchor240" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.28.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.29.1">In this chapter, we will be using Python 3 with the following </span><span><span class="kobospan" id="kobo.30.1">supporting libraries:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.31.1">deap</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.32.1">numpy</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.33.1">pandas</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.34.1">matplotlib</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.35.1">seaborn</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.36.1">scikit-learn</span></strong></span></li>
</ul>
<p class="callout-heading"><span class="kobospan" id="kobo.37.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.38.1">If you use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.39.1">requirements.txt</span></strong><span class="kobospan" id="kobo.40.1"> file we provide (see </span><a href="B20851_03.xhtml#_idTextAnchor091" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.41.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.42.1">), these libraries are already included in </span><span><span class="kobospan" id="kobo.43.1">your environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.44.1">In addition, we will be using the UCI Wine </span><span><span class="kobospan" id="kobo.45.1">dataset: </span></span><a href="https://archive.ics.uci.edu/ml/datasets/Wine" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.46.1">https://archive.ics.uci.edu/ml/datasets/Wine</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">The programs that will be used in this chapter can be found in this book’s </span><span><span class="kobospan" id="kobo.48.1">GitHub repository:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.49.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_08</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.50.1">Check out the following video to see the code in </span><span><span class="kobospan" id="kobo.51.1">action: </span></span><a href="https://packt.link/OEBOd" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.52.1">https://packt.link/OEBOd</span></span></a></p>
<h1 id="_idParaDest-190" class="calibre5"><a id="_idTextAnchor241" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.53.1">Hyperparameters in machine learning</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">In </span><a href="B20851_07.xhtml#_idTextAnchor221" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.55.1">Chapter 7</span></em></span></a><span class="kobospan" id="kobo.56.1">, </span><em class="italic"><span class="kobospan" id="kobo.57.1">Enhancing Machine Learning Models Using Feature Selection</span></em><span class="kobospan" id="kobo.58.1">, we described </span><em class="italic"><span class="kobospan" id="kobo.59.1">supervised learning</span></em><span class="kobospan" id="kobo.60.1"> as the programmatic process of adjusting (or tuning) the internal parameters of a model to produce the desired outputs in response to given inputs. </span><span class="kobospan" id="kobo.60.2">To make this happen, each type of supervised learning model is accompanied by a learning algorithm that iteratively adjusts its internal parameters during the </span><em class="italic"><span class="kobospan" id="kobo.61.1">learning </span></em><span class="kobospan" id="kobo.62.1">(or </span><span><span class="kobospan" id="kobo.63.1">training) phase.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.64.1">However, most models have another set of parameters that are set </span><em class="italic"><span class="kobospan" id="kobo.65.1">before</span></em><span class="kobospan" id="kobo.66.1"> the learning takes place. </span><span class="kobospan" id="kobo.66.2">These are called </span><strong class="bold"><span class="kobospan" id="kobo.67.1">hyperparameters</span></strong><span class="kobospan" id="kobo.68.1"> and</span><a id="_idIndexMarker537" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.69.1"> affect the way the learning is done. </span><span class="kobospan" id="kobo.69.2">The following figure illustrates the two types </span><span><span class="kobospan" id="kobo.70.1">of parameters:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer118">
<span class="kobospan" id="kobo.71.1"><img alt="Figure 8.1: Hyperparameter tuning of a machine learning model" src="image/B20851_08_001.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.72.1">Figure 8.1: Hyperparameter tuning of a machine learning model</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Usually, the hyperparameters have default values that will take effect if we don’t specifically set them. </span><span class="kobospan" id="kobo.73.2">For example, if we look at the </span><strong class="source-inline"><span class="kobospan" id="kobo.74.1">scikit-learn</span></strong><span class="kobospan" id="kobo.75.1"> library implementation of </span><a id="_idIndexMarker538" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.76.1">the </span><strong class="bold"><span class="kobospan" id="kobo.77.1">decision tree classifier</span></strong><span class="kobospan" id="kobo.78.1"> (</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.79.1">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</span></a><span class="kobospan" id="kobo.80.1">), we will see several hyperparameters and their </span><span><span class="kobospan" id="kobo.81.1">default values.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.82.1">A few of these hyperparameters are described in the </span><span><span class="kobospan" id="kobo.83.1">following table:</span></span></p>
<table class="no-table-style" id="table001-6">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.84.1">Name</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.85.1">Type</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.86.1">Description</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.87.1">Default value</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.88.1">max_depth</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.89.1">int</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.90.1">The maximum depth of </span><span><span class="kobospan" id="kobo.91.1">the tree</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.92.1">None</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.93.1">splitter</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.94.1">enumerated</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.95.1">The strategy that’s used to choose the split at each </span><span><span class="kobospan" id="kobo.96.1">best node:</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.97.1">{'</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.98.1">best', 'random'}</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.99.1">'</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.100.1">best'</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.101.1">min_samples_split</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.102.1">int </span><span><span class="kobospan" id="kobo.103.1">or float</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.104.1">The minimum number of samples required to split an </span><span><span class="kobospan" id="kobo.105.1">internal node</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.106.1">2</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre3"><span class="kobospan" id="kobo.107.1">Table 8.1: Hyperparameters and their details</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.108.1">Each of these parameters affects the way the decision tree is constructed during the learning process, and their combined effect on the results of the learning process—and, consequently, on the performance of the model—can </span><span><span class="kobospan" id="kobo.109.1">be significant.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.110.1">Since the choice of hyperparameters has a considerable impact on the performance of machine learning models, data scientists often spend significant amounts of time looking for the best hyperparameter combinations, a process called </span><strong class="bold"><span class="kobospan" id="kobo.111.1">hyperparameter tuning</span></strong><span class="kobospan" id="kobo.112.1">. </span><span class="kobospan" id="kobo.112.2">Some of the</span><a id="_idIndexMarker539" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.113.1"> methods that are used for hyperparameter tuning will be described in the </span><span><span class="kobospan" id="kobo.114.1">next subsection.</span></span></p>
<h2 id="_idParaDest-191" class="calibre7"><a id="_idTextAnchor242" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.115.1">Hyperparameter tuning</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.116.1">A common way of </span><a id="_idIndexMarker540" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.117.1">searching for good combinations of hyperparameters is using a </span><strong class="bold"><span class="kobospan" id="kobo.118.1">grid search</span></strong><span class="kobospan" id="kobo.119.1">. </span><span class="kobospan" id="kobo.119.2">Using this method, we choose a subset of values for each hyperparameter that we want to tune. </span><span class="kobospan" id="kobo.119.3">As an example, given the Decision Tree classifier, we can choose the subset of values, such as </span><strong class="source-inline"><span class="kobospan" id="kobo.120.1">{2, 5, 10}</span></strong><span class="kobospan" id="kobo.121.1">, for the </span><strong class="source-inline"><span class="kobospan" id="kobo.122.1">max_depth</span></strong><span class="kobospan" id="kobo.123.1"> parameter, while, for the </span><strong class="source-inline"><span class="kobospan" id="kobo.124.1">splitter</span></strong><span class="kobospan" id="kobo.125.1"> parameter, we choose both possible values—</span><strong class="source-inline"><span class="kobospan" id="kobo.126.1">{"best", "random"}</span></strong><span class="kobospan" id="kobo.127.1">. </span><span class="kobospan" id="kobo.127.2">Then, we try out all six possible combinations of these values. </span><span class="kobospan" id="kobo.127.3">For each combination, the classifier is trained and evaluated for a certain performance criterion; for example, accuracy. </span><span class="kobospan" id="kobo.127.4">At the end of the process, we pick the</span><a id="_idIndexMarker541" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.128.1"> combination of hyperparameter values that yielded the </span><span><span class="kobospan" id="kobo.129.1">best performance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.130.1">The main drawback of the grid search is the exhaustive search it conducts over all the possible combinations, which can prove very lengthy. </span><span class="kobospan" id="kobo.130.2">One common way to produce good combinations in a shorter amount of time</span><a id="_idIndexMarker542" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.131.1"> is </span><strong class="bold"><span class="kobospan" id="kobo.132.1">random search</span></strong><span class="kobospan" id="kobo.133.1">, where random combinations of hyperparameters are chosen </span><span><span class="kobospan" id="kobo.134.1">and tested.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.135.1">A better option—of particular interest to us—when it comes to performing the grid search is harnessing a genetic algorithm to look for the best combination(s) of hyperparameters within the predefined grid. </span><span class="kobospan" id="kobo.135.2">This method offers the potential for finding the best grid combinations in a shorter amount of time than the original, exhaustive </span><span><span class="kobospan" id="kobo.136.1">grid search.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.137.1">While grid search and random search are supported by the </span><strong class="source-inline"><span class="kobospan" id="kobo.138.1">scikit-learn</span></strong><span class="kobospan" id="kobo.139.1"> library, a genetic algorithm-driven grid search option is offered by </span><strong class="source-inline"><span class="kobospan" id="kobo.140.1">sklearn-deap</span></strong><span class="kobospan" id="kobo.141.1">. </span><span class="kobospan" id="kobo.141.2">This small library builds upon the DEAP-based genetic algorithm’s capabilities, as well as the existing features of </span><strong class="source-inline"><span class="kobospan" id="kobo.142.1">scikit-learn</span></strong><span class="kobospan" id="kobo.143.1">. </span><span class="kobospan" id="kobo.143.2">At the time of writing this book, this library is not in sync with the latest version of </span><strong class="source-inline"><span class="kobospan" id="kobo.144.1">scikit-learn</span></strong><span class="kobospan" id="kobo.145.1">; therefore, we included a slightly modified version of it under the </span><strong class="source-inline"><span class="kobospan" id="kobo.146.1">sklearn_deap</span></strong><span class="kobospan" id="kobo.147.1"> folder as part of the files of </span><a href="B20851_08.xhtml#_idTextAnchor238" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.148.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.149.1">; we will make </span><a id="_idIndexMarker543" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.150.1">use of </span><span><span class="kobospan" id="kobo.151.1">that version.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.152.1">In the following sections, we will compare both approaches to the grid search—exhaustive and genetic-algorithm-driven. </span><span class="kobospan" id="kobo.152.2">But first, we’ll take a quick look at the dataset we are going to use for our experiment—the </span><strong class="bold"><span class="kobospan" id="kobo.153.1">UCI </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.154.1">Wine dataset</span></strong></span><span><span class="kobospan" id="kobo.155.1">.</span></span></p>
<h2 id="_idParaDest-192" class="calibre7"><a id="_idTextAnchor243" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.156.1">The Wine dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.157.1">A commonly </span><a id="_idIndexMarker544" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.158.1">used dataset from the </span><em class="italic"><span class="kobospan" id="kobo.159.1">UCI Machine Learning Repository</span></em><span class="kobospan" id="kobo.160.1"> (</span><a href="https://archive.ics.uci.edu/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.161.1">https://archive.ics.uci.edu/</span></a><span class="kobospan" id="kobo.162.1">), the </span><a id="_idIndexMarker545" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.163.1">Wine dataset (</span><a href="https://archive.ics.uci.edu/ml/datasets/Wine" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.164.1">https://archive.ics.uci.edu/ml/datasets/Wine</span></a><span class="kobospan" id="kobo.165.1">) contains the results of a chemical analysis that was conducted for 178 different wines that were grown in the same region in Italy. </span><span class="kobospan" id="kobo.165.2">These wines are categorized into one of </span><span><span class="kobospan" id="kobo.166.1">three types.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.167.1">The chemical analysis consists of 13 different measurements, representing the quantities of the following constituents that are found in </span><span><span class="kobospan" id="kobo.168.1">each wine:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><span class="kobospan" id="kobo.169.1">Alcohol</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.170.1">Malic acid</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.171.1">Ash</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.172.1">Alkalinity </span><span><span class="kobospan" id="kobo.173.1">of ash</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.174.1">Magnesium</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.175.1">Total phenols</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.176.1">Flavanoids</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.177.1">Non-flavanoid phenols</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.178.1">Proanthocyanins</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.179.1">Color intensity</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.180.1">Hue</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.181.1">OD280/OD315 of </span><span><span class="kobospan" id="kobo.182.1">diluted wines</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.183.1">Proline</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.184.1">Columns </span><strong class="source-inline"><span class="kobospan" id="kobo.185.1">2</span></strong><span class="kobospan" id="kobo.186.1">–</span><strong class="source-inline"><span class="kobospan" id="kobo.187.1">14</span></strong><span class="kobospan" id="kobo.188.1"> of the dataset </span><a id="_idIndexMarker546" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.189.1">contain the values for the preceding measurements, while the classification outcome—the wine type itself (</span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">1</span></strong><span class="kobospan" id="kobo.191.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.192.1">2</span></strong><span class="kobospan" id="kobo.193.1">, or </span><strong class="source-inline"><span class="kobospan" id="kobo.194.1">3</span></strong><span class="kobospan" id="kobo.195.1">)—is found in the </span><span><span class="kobospan" id="kobo.196.1">first column.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.197.1">Next, let’s look at the classifier we chose to classify </span><span><span class="kobospan" id="kobo.198.1">this dataset.</span></span></p>
<h2 id="_idParaDest-193" class="calibre7"><a id="_idTextAnchor244" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.199.1">The adaptive boosting classifier</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.200.1">The </span><strong class="bold"><span class="kobospan" id="kobo.201.1">adaptive boosting algorithm</span></strong><span class="kobospan" id="kobo.202.1">, or </span><strong class="bold"><span class="kobospan" id="kobo.203.1">AdaBoost</span></strong><span class="kobospan" id="kobo.204.1">, for</span><a id="_idIndexMarker547" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.205.1"> short, is a powerful machine </span><a id="_idIndexMarker548" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.206.1">learning model that combines the outputs of multiple instances of a simple learning algorithm (</span><strong class="bold"><span class="kobospan" id="kobo.207.1">weak learner</span></strong><span class="kobospan" id="kobo.208.1">) using a weighted sum. </span><span class="kobospan" id="kobo.208.2">AdaBoost adds instances of the weak learner during the learning process, each of which is adjusted to improve previously </span><span><span class="kobospan" id="kobo.209.1">misclassified inputs.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.210.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.211.1">scikit-learn</span></strong><span class="kobospan" id="kobo.212.1"> library’s implementation of this model, the Adaboost classifier (</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.213.1">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</span></a><span class="kobospan" id="kobo.214.1">), uses several hyperparameters, some of which are </span><span><span class="kobospan" id="kobo.215.1">as follows:</span></span></p>
<table class="no-table-style" id="table002-1">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.216.1">Name</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.217.1">Type</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.218.1">Description</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.219.1">Default value</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.220.1">n_estimators</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.221.1">int</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.222.1">The maximum number </span><span><span class="kobospan" id="kobo.223.1">of estimators</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.224.1">50</span></strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.225.1">learning_rate</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.226.1">float</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.227.1">Weight applied to each classifier at each boosting iteration; a higher learning rate increases the contribution of </span><span><span class="kobospan" id="kobo.228.1">each classifier</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.229.1">1.0</span></strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.230.1">algorithm</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.231.1">enumerated</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.232.1">The boosting algorithm to </span><span><span class="kobospan" id="kobo.233.1">be used:</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.234.1">{'SAMME' , 'SAMME.R'}</span></strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.235.1">'SAMME.R'</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.236.1">Table 8.1: Hyperparameters and their details</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.237.1">Interestingly, each</span><a id="_idIndexMarker549" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.238.1"> of these three hyperparameters is of a different type—an int, a float, and an enumerated (or categorical) type. </span><span class="kobospan" id="kobo.238.2">Later, we will find out how each tuning method handles these different types. </span><span class="kobospan" id="kobo.238.3">We will start with two forms of the grid search, both of which will be described in the </span><span><span class="kobospan" id="kobo.239.1">next section.</span></span></p>
<h1 id="_idParaDest-194" class="calibre5"><a id="_idTextAnchor245" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.240.1">Tuning the hyperparameters using conventional versus genetic grid search</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.241.1">To encapsulate the</span><a id="_idIndexMarker550" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.242.1"> hyperparameter tuning of the AdaBoost classifier for the Wine dataset using a grid search—both the conventional version and the genetic-algorithm-driven version—we created a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.243.1">HyperparameterTuningGrid</span></strong><span class="kobospan" id="kobo.244.1">. </span><span class="kobospan" id="kobo.244.2">This class can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.245.1">01_hyperparameter_tuning_grid.py</span></strong><span class="kobospan" id="kobo.246.1"> file, which is </span><span><span class="kobospan" id="kobo.247.1">located at</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.248.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/01_hyperparameter_tuning_grid.py</span></span></a><span><span class="kobospan" id="kobo.249.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.250.1">The main parts of this class are highlighted </span><span><span class="kobospan" id="kobo.251.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.252.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.253.1">__init__()</span></strong><span class="kobospan" id="kobo.254.1"> method of the class initializes the wine dataset, the AdaBoost classifier, the k-fold cross-validation metric, and the </span><span><span class="kobospan" id="kobo.255.1">grid parameters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.256.1">
self.initWineDataset()
self.initClassifier()
self.initKfold()
self.initGridParams()</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.257.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.258.1">initGridParams()</span></strong><span class="kobospan" id="kobo.259.1"> method initializes the grid search by setting the tested values of the three hyperparameters mentioned in the </span><span><span class="kobospan" id="kobo.260.1">previous section:</span></span><ul class="calibre16"><li class="calibre11"><span class="kobospan" id="kobo.261.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.262.1">n_estimators</span></strong><span class="kobospan" id="kobo.263.1"> parameter is tested across 10 values, linearly spaced between 10 </span><span><span class="kobospan" id="kobo.264.1">and 100.</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.265.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.266.1">learning_rate</span></strong><span class="kobospan" id="kobo.267.1"> parameter is tested across 100 values, logarithmically spaced between 0.1 (</span><span><span class="kobospan" id="kobo.268.1">10</span></span><span><span class="kobospan" id="kobo.269.1"> </span></span><span><span class="kobospan" id="kobo.270.1">−</span></span><span><span class="kobospan" id="kobo.271.1">2</span></span><span class="kobospan" id="kobo.272.1">) and 1 (</span><span><span><span class="kobospan" id="kobo.273.1">10</span></span></span><span><span><span class="kobospan" id="kobo.274.1"> </span></span></span><span><span><span class="kobospan" id="kobo.275.1">0</span></span></span><span><span class="kobospan" id="kobo.276.1">).</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.277.1">Both possible values of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.278.1">algorithm</span></strong><span class="kobospan" id="kobo.279.1"> parameter, </span><strong class="source-inline1"><span class="kobospan" id="kobo.280.1">'SAMME'</span></strong><span class="kobospan" id="kobo.281.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.282.1">'SAMME.R'</span></strong><span class="kobospan" id="kobo.283.1">, </span><span><span class="kobospan" id="kobo.284.1">are tested.</span></span></li></ul><p class="calibre3"><span class="kobospan" id="kobo.285.1">This setup covers </span><a id="_idIndexMarker551" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.286.1">a total of 200 (10×10×2) different combinations of the </span><span><span class="kobospan" id="kobo.287.1">grid parameters:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.288.1">
self.gridParams = {
    '</span><strong class="bold1"><span class="kobospan1" id="kobo.289.1">n_estimators</span></strong><span class="kobospan1" id="kobo.290.1">': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    '</span><strong class="bold1"><span class="kobospan1" id="kobo.291.1">learning_rate</span></strong><span class="kobospan1" id="kobo.292.1">': np.logspace(-2, 0, num=10, base=10),
    '</span><strong class="bold1"><span class="kobospan1" id="kobo.293.1">algorithm</span></strong><span class="kobospan1" id="kobo.294.1">': ['SAMME', 'SAMME.R'],
}</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.295.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.296.1">getDefaultAccuracy()</span></strong><span class="kobospan" id="kobo.297.1"> method evaluates the accuracy of the classifier with its default hyperparameter values using the mean value of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.298.1">'</span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.299.1">accuracy'</span></strong></span><span><span class="kobospan" id="kobo.300.1"> metric:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.301.1">
cv_results = model_selection.</span><strong class="bold1"><span class="kobospan1" id="kobo.302.1">cross_val_score</span></strong><span class="kobospan1" id="kobo.303.1">(
    self.classifier,
    self.X,
    self.y,
    cv=self.kfold,
    scoring='</span><strong class="bold1"><span class="kobospan1" id="kobo.304.1">accuracy</span></strong><span class="kobospan1" id="kobo.305.1">')
return cv_results.</span><strong class="bold1"><span class="kobospan1" id="kobo.306.1">mean</span></strong><span class="kobospan1" id="kobo.307.1">()</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.308.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.309.1">gridTest()</span></strong><span class="kobospan" id="kobo.310.1"> method performs a conventional grid search over the set of tested </span><a id="_idIndexMarker552" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.311.1">hyperparameter values we defined earlier. </span><span class="kobospan" id="kobo.311.2">The best combination of parameters is determined based on the k-fold cross-validation mean </span><strong class="source-inline1"><span class="kobospan" id="kobo.312.1">'</span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.313.1">accuracy'</span></strong></span><span><span class="kobospan" id="kobo.314.1"> metric:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.315.1">
gridSearch = </span><strong class="bold1"><span class="kobospan1" id="kobo.316.1">GridSearchCV</span></strong><span class="kobospan1" id="kobo.317.1">(
    estimator=self.classifier,
    param_grid=self.</span><strong class="bold1"><span class="kobospan1" id="kobo.318.1">gridParams</span></strong><span class="kobospan1" id="kobo.319.1">,
    cv=self.kfold,
    scoring='</span><strong class="bold1"><span class="kobospan1" id="kobo.320.1">accuracy</span></strong><span class="kobospan1" id="kobo.321.1">')
gridSearch.</span><strong class="bold1"><span class="kobospan1" id="kobo.322.1">fit</span></strong><span class="kobospan1" id="kobo.323.1">(self.X, self.y)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.324.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.325.1">geneticGridTest()</span></strong><span class="kobospan" id="kobo.326.1"> method performs a genetic-algorithm-driven grid search. </span><span class="kobospan" id="kobo.326.2">It utilizes the </span><strong class="source-inline1"><span class="kobospan" id="kobo.327.1">sklearn-deap</span></strong><span class="kobospan" id="kobo.328.1"> library’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.329.1">EvolutionaryAlgorithmSearchCV()</span></strong><span class="kobospan" id="kobo.330.1"> method, which was designed to be called in a very similar manner to that of the conventional grid search. </span><span class="kobospan" id="kobo.330.2">All we need to do is add a few genetic algorithm parameters—population size, mutation probability, tournament size, and the number </span><span><span class="kobospan" id="kobo.331.1">of generations:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.332.1">
gridSearch = </span><strong class="bold1"><span class="kobospan1" id="kobo.333.1">EvolutionaryAlgorithmSearchCV</span></strong><span class="kobospan1" id="kobo.334.1">(
    estimator=self.classifier,
    params=self.</span><strong class="bold1"><span class="kobospan1" id="kobo.335.1">gridParams</span></strong><span class="kobospan1" id="kobo.336.1">,
    cv=self.kfold,
    scoring='</span><strong class="bold1"><span class="kobospan1" id="kobo.337.1">accuracy</span></strong><span class="kobospan1" id="kobo.338.1">',
    verbose=True,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.339.1">population_size</span></strong><span class="kobospan1" id="kobo.340.1">=20,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.341.1">gene_mutation_prob</span></strong><span class="kobospan1" id="kobo.342.1">=0.50,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.343.1">tournament_size</span></strong><span class="kobospan1" id="kobo.344.1">=2,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.345.1">generations_number</span></strong><span class="kobospan1" id="kobo.346.1">=5)
gridSearch.fit(self.X, self.y)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.347.1">Finally, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.348.1">main()</span></strong><span class="kobospan" id="kobo.349.1"> method of the class starts by evaluating the performance of the classifier with its default hyperparameter values. </span><span class="kobospan" id="kobo.349.2">Then, it runs the conventional, exhaustive grid search, followed by the genetic-algorithm-driven grid</span><a id="_idIndexMarker553" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.350.1"> search, while timing </span><span><span class="kobospan" id="kobo.351.1">each search.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.352.1">The results of running the main method of this class are described in the </span><span><span class="kobospan" id="kobo.353.1">next subsection.</span></span></p>
<h2 id="_idParaDest-195" class="calibre7"><a id="_idTextAnchor246" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.354.1">Testing the classifier’s default performance</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.355.1">The results of the</span><a id="_idIndexMarker554" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.356.1"> run indicate that, with the default parameter values of </span><strong class="source-inline"><span class="kobospan" id="kobo.357.1">n_estimators = 50</span></strong><span class="kobospan" id="kobo.358.1">,  </span><strong class="source-inline"><span class="kobospan" id="kobo.359.1">learning_rate = 1.0</span></strong><span class="kobospan" id="kobo.360.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.361.1">algorithm = 'SAMME.R'</span></strong><span class="kobospan" id="kobo.362.1">, the classification accuracy of the classifier is </span><span><span class="kobospan" id="kobo.363.1">about 66.4%:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.364.1">
Default Classifier Hyperparameter values:
{'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}
score with default values =  0.6636507936507937</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.365.1">This is not a particularly good accuracy. </span><span class="kobospan" id="kobo.365.2">Hopefully, grid search can improve this by finding a better combination of </span><span><span class="kobospan" id="kobo.366.1">hyperparameter values.</span></span></p>
<h2 id="_idParaDest-196" class="calibre7"><a id="_idTextAnchor247" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.367.1">Running the conventional grid search</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.368.1">The</span><a id="_idIndexMarker555" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.369.1"> conventional, exhaustive grid search, covering all 200 possible combinations, is performed next. </span><span class="kobospan" id="kobo.369.2">The search results indicated that the best combination within this grid was </span><strong class="source-inline"><span class="kobospan" id="kobo.370.1">n_estimators = 50</span></strong><span class="kobospan" id="kobo.371.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.372.1">learning_rate ≈ 0.5995</span></strong><span class="kobospan" id="kobo.373.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.374.1">algorithm = '</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.375.1">SAMME.R'</span></strong></span><span><span class="kobospan" id="kobo.376.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.377.1">The classification accuracy that we achieved with these values is about 92.7%, which is a vast improvement over the original 66.4%. </span><span class="kobospan" id="kobo.377.2">The search runtime was about 131 seconds using a </span><a id="_idIndexMarker556" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.378.1">relatively </span><span><span class="kobospan" id="kobo.379.1">old computer:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.380.1">
performing grid search...
</span><span class="kobospan1" id="kobo.380.2">best parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.5994842503189409, 'n_estimators': 50}
best score:  0.9266666666666667
Time Elapsed =  131.01380705833435</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.381.1">Next comes the genetic-powered grid search. </span><span class="kobospan" id="kobo.381.2">Will it match these results? </span><span class="kobospan" id="kobo.381.3">Let’s </span><span><span class="kobospan" id="kobo.382.1">find out.</span></span></p>
<h2 id="_idParaDest-197" class="calibre7"><a id="_idTextAnchor248" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.383.1">Running the genetic-algorithm-driven grid search</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.384.1">The last portion of the run </span><a id="_idIndexMarker557" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.385.1">describes the genetic-algorithm-driven grid search, which is carried out with the same grid parameters. </span><span class="kobospan" id="kobo.385.2">The verbose output of the search starts with a somewhat </span><span><span class="kobospan" id="kobo.386.1">cryptic printout:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.387.1">
performing Genetic grid search...
</span><span class="kobospan1" id="kobo.387.2">Types [1, 2, 1] and maxint [9, 9, 1] detected</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.388.1">This printout describes the grid we are searching on—a list of 10 integers (</span><strong class="source-inline"><span class="kobospan" id="kobo.389.1">n_estimators</span></strong><span class="kobospan" id="kobo.390.1"> values), an ndarray of 10 elements (</span><strong class="source-inline"><span class="kobospan" id="kobo.391.1">learning_rate</span></strong><span class="kobospan" id="kobo.392.1"> values), and a list of two strings (</span><strong class="source-inline"><span class="kobospan" id="kobo.393.1">algorithm</span></strong> <span><span class="kobospan" id="kobo.394.1">values)—as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.395.1">Types [1, 2, 1]</span></strong><span class="kobospan" id="kobo.396.1"> refers to the grid types of </span><strong class="source-inline1"><span class="kobospan" id="kobo.397.1">[list, </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.398.1">ndarray, list]</span></strong></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.399.1">maxint [9, 9, 1]</span></strong><span class="kobospan" id="kobo.400.1"> corresponds to the list/array sizes of </span><strong class="source-inline1"><span class="kobospan" id="kobo.401.1">[10, </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.402.1">10, 2]</span></strong></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.403.1">The next printed line refers to the total amount of possible grid </span><span><span class="kobospan" id="kobo.404.1">combinations (10×10×2):</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.405.1">
--- Evolve in 200 possible combinations ---</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.406.1">The rest of the printout looks very familiar since it utilizes the same DEAP-based genetic algorithm tools that we have been using all along, detailing the process of evolving the generations and printing a statistics line for </span><span><span class="kobospan" id="kobo.407.1">each generation:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.408.1">
gen  nevals    avg        min       max        std</span><a id="_idTextAnchor249" class="calibre17 pcalibre pcalibre1"/><span class="kobospan1" id="kobo.409.1">
0     20    0.708146   0.117978   0.910112   0.265811
1     13    0.870787   0.662921   0.910112   0.0701235
2     10    0.857865   0.662921   0.91573    0.0735955
3     12    0.87809    0.679775   0.904494   0.0473746
4     12    0.878933   0.662921   0.910112   0.0524153
5     7     0.864045   0.162921   0.926966   0.161174</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.410.1">At the end of </span><a id="_idIndexMarker558" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.411.1">the process, the best combination is printed, along with the score value and the time </span><span><span class="kobospan" id="kobo.412.1">that elapsed:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.413.1">
Best individual is: {'n_estimators': 50, 'learning_rate': 0.5994842503189409, 'algorithm': 'SAMME.R'}
with fitness: 0.9269662921348315
Time Elapsed =  21.147947072982788</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.414.1">These results indicate that the genetic-algorithm-driven grid search was able to find the same best result that was found using the exhaustive search but in a shorter amount </span><span><span class="kobospan" id="kobo.415.1">of time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.416.1">Please note that this is a simple example that runs very quickly. </span><span class="kobospan" id="kobo.416.2">In real-life situations, we often encounter large datasets, as well as complex models and extensive hyperparameter grids. </span><span class="kobospan" id="kobo.416.3">In these circumstances, running an exhaustive grid search can be prohibitively lengthy, while the genetic-algorithm-driven grid search has the potential to yield good results within a reasonable amount </span><span><span class="kobospan" id="kobo.417.1">of time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.418.1">But still, all grid searches, genetic-driven or not, are limited to the subset of hyperparameter values that are defined by the grid. </span><span class="kobospan" id="kobo.418.2">What if we would like to search outside the grid without being limited to a subset of predefined values? </span><span class="kobospan" id="kobo.418.3">A possible solution is described in the </span><span><span class="kobospan" id="kobo.419.1">next section.</span></span></p>
<h1 id="_idParaDest-198" class="calibre5"><a id="_idTextAnchor250" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.420.1">Tuning the hyperparameters using a direct genetic approach</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.421.1">Besides offering an</span><a id="_idIndexMarker559" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.422.1"> efficient grid search option, genetic algorithms can be utilized to directly search the entire parameter space, just as we used them to search the input space for many types of problems throughout this book. </span><span class="kobospan" id="kobo.422.2">Each hyperparameter can be represented as a variable participating in the search, and the chromosome can be a combination of all </span><span><span class="kobospan" id="kobo.423.1">these variables.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.424.1">Since the hyperparameters can be of varying types—for example, the types float, int, and enumerated, which we have in our AdaBoost classifier—we may want to code each of them differently and then define the genetic operations as a combination of separate operators that are adapted to each of the types. </span><span class="kobospan" id="kobo.424.2">However, we can also use a lazy approach and code all of them as float parameters to simplify the implementation of the algorithm, as we will </span><span><span class="kobospan" id="kobo.425.1">see next.</span></span></p>
<h2 id="_idParaDest-199" class="calibre7"><a id="_idTextAnchor251" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.426.1">Hyperparameter representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.427.1">In </span><a href="B20851_06.xhtml#_idTextAnchor197" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.428.1">Chapter 6</span></em></span></a><span class="kobospan" id="kobo.429.1">, </span><em class="italic"><span class="kobospan" id="kobo.430.1">Optimizing Continuous Functions</span></em><span class="kobospan" id="kobo.431.1">, we used genetic algorithms to optimize the </span><a id="_idIndexMarker560" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.432.1">functions of real-valued parameters. </span><span class="kobospan" id="kobo.432.2">These parameters were represented as a list of float numbers: </span><em class="italic"><span class="kobospan" id="kobo.433.1">[1.23, </span></em><span><em class="italic"><span class="kobospan" id="kobo.434.1">7.2134, -25.309]</span></em></span><span><span class="kobospan" id="kobo.435.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.436.1">Consequently, the genetic operators we used were specialized for handling lists of </span><span><span class="kobospan" id="kobo.437.1">floating-point numbers.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.438.1">To adapt this approach so that it can tune the hyperparameters, we are going to represent each hyperparameter as a floating-point number, regardless of its actual type. </span><span class="kobospan" id="kobo.438.2">To make this work, we need to find a way to transform each parameter into a floating-point number and back from a floating-point number to its original representation. </span><span class="kobospan" id="kobo.438.3">We will implement the </span><span><span class="kobospan" id="kobo.439.1">following transformations:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.440.1">n_estimators</span></strong><span class="kobospan" id="kobo.441.1">, originally an integer, will be represented by a float value in a certain range; for example, </span><strong class="source-inline1"><span class="kobospan" id="kobo.442.1">[1, 100]</span></strong><span class="kobospan" id="kobo.443.1">. </span><span class="kobospan" id="kobo.443.2">To transform the float value back into an integer, we will use the Python </span><strong class="source-inline1"><span class="kobospan" id="kobo.444.1">round()</span></strong><span class="kobospan" id="kobo.445.1"> function, which will round it to the </span><span><span class="kobospan" id="kobo.446.1">nearest integer.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.447.1">learning_rate</span></strong><span class="kobospan" id="kobo.448.1"> is already a float, so no conversion is needed. </span><span class="kobospan" id="kobo.448.2">It will be bound to the range of </span><strong class="source-inline1"><span class="kobospan" id="kobo.449.1">[</span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.450.1">0.01, 1.0]</span></strong></span><span><span class="kobospan" id="kobo.451.1">.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.452.1">algorithm</span></strong><span class="kobospan" id="kobo.453.1"> can have one of two values, </span><strong class="source-inline1"><span class="kobospan" id="kobo.454.1">'SAMME'</span></strong><span class="kobospan" id="kobo.455.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.456.1">'SAMME.R'</span></strong><span class="kobospan" id="kobo.457.1">, and will be represented by a float number in the range of </span><strong class="source-inline1"><span class="kobospan" id="kobo.458.1">[0, 1]</span></strong><span class="kobospan" id="kobo.459.1">. </span><span class="kobospan" id="kobo.459.2">To transform the float value, we will round it to the nearest integer—</span><strong class="source-inline1"><span class="kobospan" id="kobo.460.1">0</span></strong><span class="kobospan" id="kobo.461.1"> or </span><strong class="source-inline1"><span class="kobospan" id="kobo.462.1">1</span></strong><span class="kobospan" id="kobo.463.1">. </span><span class="kobospan" id="kobo.463.2">Then, we will replace a value</span><a id="_idIndexMarker561" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.464.1"> of </span><strong class="source-inline1"><span class="kobospan" id="kobo.465.1">0</span></strong><span class="kobospan" id="kobo.466.1"> with </span><strong class="source-inline1"><span class="kobospan" id="kobo.467.1">'SAMME'</span></strong><span class="kobospan" id="kobo.468.1"> and a value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.469.1">1</span></strong> <span><span class="kobospan" id="kobo.470.1">with </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.471.1">'SAMME.R'</span></strong></span><span><span class="kobospan" id="kobo.472.1">.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.473.1">These conversions will be carried out by two Python files, both of which will be described in the </span><span><span class="kobospan" id="kobo.474.1">following subsections.</span></span></p>
<h2 id="_idParaDest-200" class="calibre7"><a id="_idTextAnchor252" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.475.1">Evaluating the classifier accuracy</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.476.1">We start with a</span><a id="_idIndexMarker562" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.477.1"> Python class encapsulating the classifier’s </span><em class="italic"><span class="kobospan" id="kobo.478.1">accuracy</span></em><span class="kobospan" id="kobo.479.1"> evaluation, called </span><strong class="source-inline"><span class="kobospan" id="kobo.480.1">HyperparameterTuningGenetic</span></strong><span class="kobospan" id="kobo.481.1">. </span><span class="kobospan" id="kobo.481.2">This class can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.482.1">hyperparameter_tuning_genetic_test.py</span></strong><span class="kobospan" id="kobo.483.1"> file, which is </span><span><span class="kobospan" id="kobo.484.1">located at</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.485.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/hyperparameter_tuning_genetic_test.py</span></span></a><span><span class="kobospan" id="kobo.486.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.487.1">The main functionality of this class is highlighted </span><span><span class="kobospan" id="kobo.488.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.489.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.490.1">convertParam()</span></strong><span class="kobospan" id="kobo.491.1"> method of the class takes a list called </span><strong class="source-inline1"><span class="kobospan" id="kobo.492.1">params</span></strong><span class="kobospan" id="kobo.493.1">, containing the float values representing the hyperparameters, and transforms them into their actual values (as discussed in the </span><span><span class="kobospan" id="kobo.494.1">previous subsection):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.495.1">
n_estimators = round(params[0])
learning_rate = params[1]
algorithm = ['SAMME', 'SAMME.R'][round(params[2])]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.496.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.497.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.498.1"> method takes a list of float numbers representing the hyperparameter values, uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.499.1">convertParam()</span></strong><span class="kobospan" id="kobo.500.1"> method to transform them into actual values, and initializes the AdaBoost classifier with </span><span><span class="kobospan" id="kobo.501.1">these values:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.502.1">
n_estimators, learning_rate, algorithm = \
    self.convertParams(params)
self.classifier =  AdaBoostClassifier(
    n_estimators=n_estimators,
    learning_rate=learning_rate,
    algorithm=algorithm)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.503.1">Then, it finds the accuracy of the classifier using the k-fold cross-validation code that we created for the </span><span><span class="kobospan" id="kobo.504.1">wine dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.505.1">
cv_results = model_selection.</span><strong class="bold1"><span class="kobospan1" id="kobo.506.1">cross_val_score</span></strong><span class="kobospan1" id="kobo.507.1">(
    self.classifier,
    self.X,
    self.y,
    cv=self.kfold,
    scoring='</span><strong class="bold1"><span class="kobospan1" id="kobo.508.1">accuracy</span></strong><span class="kobospan1" id="kobo.509.1">')
return cv_results.</span><strong class="bold1"><span class="kobospan1" id="kobo.510.1">mean</span></strong><span class="kobospan1" id="kobo.511.1">()</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.512.1">This class is</span><a id="_idIndexMarker563" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.513.1"> utilized by the program that implements the hyperparameter-tuning genetic algorithm, as will be described in the </span><span><span class="kobospan" id="kobo.514.1">next section.</span></span></p>
<h2 id="_idParaDest-201" class="calibre7"><a id="_idTextAnchor253" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.515.1">Tuning the hyperparameters using genetic algorithms</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.516.1">The </span><a id="_idIndexMarker564" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.517.1">genetic-algorithm-based search for the best hyperparameter values is implemented by the Python program, </span><strong class="source-inline"><span class="kobospan" id="kobo.518.1">02_hyperparameter_tuning_genetic.py</span></strong><span class="kobospan" id="kobo.519.1">, which is </span><span><span class="kobospan" id="kobo.520.1">located at</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.521.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_08/02_hyperparameter_tuning_genetic.py</span></span></a><span><span class="kobospan" id="kobo.522.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.523.1">The following steps describe the main parts of </span><span><span class="kobospan" id="kobo.524.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.525.1">We start by setting the lower and upper boundary for each of the float values representing a hyperparameter, as described in the previous subsection</span><strong class="source-inline1"><span class="kobospan" id="kobo.526.1">—[1, 100]</span></strong><span class="kobospan" id="kobo.527.1"> for </span><strong class="source-inline1"><span class="kobospan" id="kobo.528.1">n_estimators</span></strong><span class="kobospan" id="kobo.529.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.530.1">[0.01, 1]</span></strong><span class="kobospan" id="kobo.531.1"> for </span><strong class="source-inline1"><span class="kobospan" id="kobo.532.1">learning_rate</span></strong><span class="kobospan" id="kobo.533.1">, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.534.1">[0, 1]</span></strong> <span><span class="kobospan" id="kobo.535.1">for </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.536.1">algorithm</span></strong></span><span><span class="kobospan" id="kobo.537.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.538.1">
# [n_estimators, learning_rate, algorithm]:
BOUNDS_LOW =  [  1, 0.01, 0]
BOUNDS_HIGH = [100, 1.00, 1]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.539.1">Then, we </span><a id="_idIndexMarker565" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.540.1">create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.541.1">HyperparameterTuningGenetic</span></strong><span class="kobospan" id="kobo.542.1"> class that will allow us to test the various combinations of </span><span><span class="kobospan" id="kobo.543.1">the hyperparameters:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.544.1">
test = </span><strong class="bold1"><span class="kobospan1" id="kobo.545.1">HyperparameterTuningGenetic</span></strong><span class="kobospan1" id="kobo.546.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.547.1">Since our goal is to maximize the accuracy of the classifier, we define a single objective, maximizing </span><span><span class="kobospan" id="kobo.548.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.549.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.550.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.551.1">", base.Fitness, weights=(1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.552.1">Now comes a particularly interesting part—since the solution is represented by a list of float values, each of a different range, we use the following loop to iterate over all pairs of lower-bound and upper-bound values. </span><span class="kobospan" id="kobo.552.2">For each hyperparameter, we create a separate toolbox operator, which will be used to generate random float values in the </span><span><span class="kobospan" id="kobo.553.1">appropriate range:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.554.1">
for i in range(NUM_OF_PARAMS):
    # "hyperparameter_0", "hyperparameter_1", ...
</span><span class="kobospan1" id="kobo.554.2">    toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.555.1">hyperparameter_</span></strong><span class="kobospan1" id="kobo.556.1">" + </span><strong class="bold1"><span class="kobospan1" id="kobo.557.1">str(i)</span></strong><span class="kobospan1" id="kobo.558.1">,
                      random.uniform,
                      </span><strong class="bold1"><span class="kobospan1" id="kobo.559.1">BOUNDS_LOW[i]</span></strong><span class="kobospan1" id="kobo.560.1">,
                      </span><strong class="bold1"><span class="kobospan1" id="kobo.561.1">BOUNDS_HIGH[i]</span></strong><span class="kobospan1" id="kobo.562.1">)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.563.1">Then, we </span><a id="_idIndexMarker566" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.564.1">create the hyperparameter tuple, which contains the specific float number generators we just created for </span><span><span class="kobospan" id="kobo.565.1">each hyperparameter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.566.1">
hyperparameters = ()
for i in range(NUM_OF_PARAMS):
    hyperparameters = hyperparameters + \
        (toolbox.__getattribute__("</span><strong class="bold1"><span class="kobospan1" id="kobo.567.1">hyperparameter_</span></strong><span class="kobospan1" id="kobo.568.1">" + </span><strong class="bold1"><span class="kobospan1" id="kobo.569.1">str(i)</span></strong><span class="kobospan1" id="kobo.570.1">),)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.571.1">Now, we can use this </span><a id="_idIndexMarker567" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.572.1">hyperparameter tuple, in conjunction with DEAP’s built-in </span><strong class="source-inline1"><span class="kobospan" id="kobo.573.1">initCycle()</span></strong><span class="kobospan" id="kobo.574.1"> operator, to create a new </span><strong class="source-inline1"><span class="kobospan" id="kobo.575.1">individualCreator</span></strong><span class="kobospan" id="kobo.576.1"> operator that fills up an individual instance with a combination of randomly generated </span><span><span class="kobospan" id="kobo.577.1">hyperparameter values:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.578.1">
toolbox.register("individualCreator",
                  tools.</span><strong class="bold1"><span class="kobospan1" id="kobo.579.1">initCycle</span></strong><span class="kobospan1" id="kobo.580.1">,
                  creator.Individual,
                  </span><strong class="bold1"><span class="kobospan1" id="kobo.581.1">hyperparameters</span></strong><span class="kobospan1" id="kobo.582.1">,
                  n=1)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.583.1">Then, we instruct the genetic algorithm to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.584.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.585.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.586.1">HyperparameterTuningGenetic</span></strong><span class="kobospan" id="kobo.587.1"> instance for fitness evaluation. </span><span class="kobospan" id="kobo.587.2">As a reminder, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.588.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.589.1"> method, which we described in the previous subsection, converts the given individual—a list of three floats—back into the classifier hyperparameter values they represent, trains the classifier with these values, and evaluates its accuracy using </span><span><span class="kobospan" id="kobo.590.1">k-fold cross-validation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.591.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.592.1">classificationAccuracy</span></strong><span class="kobospan1" id="kobo.593.1">(individual):
    return test.</span><strong class="bold1"><span class="kobospan1" id="kobo.594.1">getAccuracy</span></strong><span class="kobospan1" id="kobo.595.1">(individual),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.596.1">evaluate</span></strong><span class="kobospan1" id="kobo.597.1">", classificationAccuracy)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.598.1">Now, we need to define the genetic operators. </span><span class="kobospan" id="kobo.598.2">While for the </span><strong class="source-inline1"><span class="kobospan" id="kobo.599.1">selection</span></strong><span class="kobospan" id="kobo.600.1"> operator, we use the usual tournament selection with a tournament size of </span><strong class="source-inline1"><span class="kobospan" id="kobo.601.1">2</span></strong><span class="kobospan" id="kobo.602.1">, we choose </span><strong class="source-inline1"><span class="kobospan" id="kobo.603.1">crossover</span></strong><span class="kobospan" id="kobo.604.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.605.1">mutation</span></strong><span class="kobospan" id="kobo.606.1"> operators that are specialized for bounded float-list chromosomes and provide them with the boundaries we defined for</span><a id="_idIndexMarker568" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.607.1">each hyperparameter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.608.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.609.1">select</span></strong><span class="kobospan1" id="kobo.610.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.611.1">mate</span></strong><span class="kobospan1" id="kobo.612.1">",
                 tools.cxSimulatedBinaryBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.613.1">mutate</span></strong><span class="kobospan1" id="kobo.614.1">",
                 tools.mutPolynomialBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR,
                 indpb=1.0 / NUM_OF_PARAMS)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.615.1">In addition, we continue to use the elitist approach, where the HOF members—the current best individuals—are always passed untouched to the </span><span><span class="kobospan" id="kobo.616.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.617.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.618.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.619.1">(
    population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.620.1">By running the </span><a id="_idIndexMarker569" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.621.1">algorithm for five generations with a population size of 30, we get the </span><span><span class="kobospan" id="kobo.622.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.623.1">
gen nevals max avg
0       30      0.927143        0.831439
1       22      0.93254         0.902741
2       23      0.93254         0.907847
3       25      0.943651        0.916566
4       24      0.943651        0.921106
5       24      0.943651        0.921751
- Best solution is:
params =  'n_estimators'= </span><strong class="bold1"><span class="kobospan1" id="kobo.624.1">30</span></strong><span class="kobospan1" id="kobo.625.1">, 'learning_rate'=</span><strong class="bold1"><span class="kobospan1" id="kobo.626.1">0.613</span></strong><span class="kobospan1" id="kobo.627.1">, 'algorithm'=</span><strong class="bold1"><span class="kobospan1" id="kobo.628.1">SAMME.R</span></strong><span class="kobospan1" id="kobo.629.1">
Accuracy = </span><strong class="bold1"><span class="kobospan1" id="kobo.630.1">0.94365</span></strong></pre> <p class="calibre3"><span class="kobospan" id="kobo.631.1">These results indicate that the best combination that was found was </span><strong class="source-inline"><span class="kobospan" id="kobo.632.1">n_estimators = 30</span></strong><span class="kobospan" id="kobo.633.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.634.1">learning_rate = 0.613</span></strong><span class="kobospan" id="kobo.635.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.636.1">algorithm = '</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.637.1">SAMME.R'</span></strong></span><span><span class="kobospan" id="kobo.638.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.639.1">The classification accuracy that we achieved with these values is about 94.4%—a worthy improvement over the accuracy we achieved with the grid search. </span><span class="kobospan" id="kobo.639.2">Interestingly, the best value that was found for </span><strong class="source-inline"><span class="kobospan" id="kobo.640.1">learning_rate</span></strong><span class="kobospan" id="kobo.641.1"> is just outside the grid values we </span><span><span class="kobospan" id="kobo.642.1">searched on.</span></span></p>
<h1 id="_idParaDest-202" class="calibre5"><a id="_idTextAnchor254" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.643.1">Dedicated libraries</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.644.1">In recent years, several genetic-algorithm-based libraries have been developed that are dedicated to optimizing machine learning model </span><a id="_idIndexMarker570" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.645.1">development. </span><span class="kobospan" id="kobo.645.2">One of them is </span><strong class="source-inline"><span class="kobospan" id="kobo.646.1">sklearn-genetic-opt</span></strong><span class="kobospan" id="kobo.647.1"> (</span><a href="https://sklearn-genetic-opt.readthedocs.io/en/stable/index.html" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.648.1">https://sklearn-genetic-opt.readthedocs.io/en/stable/index.html</span></a><span class="kobospan" id="kobo.649.1">); it supports both hyperparameters tuning and feature selection. </span><span class="kobospan" id="kobo.649.2">Another more elaborate library</span><a id="_idIndexMarker571" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.650.1"> is </span><strong class="source-inline"><span class="kobospan" id="kobo.651.1">TPOT</span></strong><em class="italic"> </em><span class="kobospan" id="kobo.652.1">(</span><a href="https://epistasislab.github.io/tpot/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.653.1">https://epistasislab.github.io/tpot/</span></a><span class="kobospan" id="kobo.654.1">); this library provides optimization for the end-to-end machine learning</span><a id="_idIndexMarker572" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.655.1"> development process, also called the </span><strong class="bold"><span class="kobospan" id="kobo.656.1">pipeline</span></strong><span class="kobospan" id="kobo.657.1">. </span><span class="kobospan" id="kobo.657.2">You are encouraged to try out these libraries in your </span><span><span class="kobospan" id="kobo.658.1">own projects.</span></span></p>
<h1 id="_idParaDest-203" class="calibre5"><a id="_idTextAnchor255" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.659.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.660.1">In this chapter, you were introduced to the concept of hyperparameter tuning in machine learning. </span><span class="kobospan" id="kobo.660.2">After getting acquainted with the Wine dataset and the AdaBoost classifier, both of which we used for testing throughout this chapter, you were presented with the hyperparameter tuning methods of an exhaustive grid search and its genetic-algorithm-driven counterpart. </span><span class="kobospan" id="kobo.660.3">These two methods were then compared using our test scenario. </span><span class="kobospan" id="kobo.660.4">Finally, we tried out a direct genetic algorithm approach, where all the hyperparameters were represented as float values. </span><span class="kobospan" id="kobo.660.5">This approach allowed us to improve the results of the </span><span><span class="kobospan" id="kobo.661.1">grid search.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.662.1">In the next chapter, we will look into the fascinating machine learning models of </span><strong class="bold"><span class="kobospan" id="kobo.663.1">neural networks</span></strong><span class="kobospan" id="kobo.664.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.665.1">deep learning</span></strong><span class="kobospan" id="kobo.666.1"> and apply genetic algorithms to improve </span><span><span class="kobospan" id="kobo.667.1">their performance.</span></span></p>
<h1 id="_idParaDest-204" class="calibre5"><a id="_idTextAnchor256" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.668.1">Further reading</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.669.1">For more information on the topics that were covered in this chapter, please refer to the </span><span><span class="kobospan" id="kobo.670.1">following resources:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.671.1">Cross-validation and Parameter Tuning, from the book </span><em class="italic"><span class="kobospan" id="kobo.672.1">Mastering Predictive Analytics with scikit-learn and TensorFlow</span></em><span class="kobospan" id="kobo.673.1">, Alan Fontaine, </span><span><span class="kobospan" id="kobo.674.1">September 2018:</span></span></li>
<li class="calibre11"><a href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789617740/2/ch02lvl1sec16/introduction-to-hyperparameter-tuning" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.675.1">https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789617740/2/ch02lvl1sec16/introduction-to-hyperparameter-tuning</span></span></a></li>
<li class="calibre11"> <em class="italic"><span class="kobospan" id="kobo.676.1">sklearn-deap </span></em><span class="kobospan" id="kobo.677.1">at </span><span><span class="kobospan" id="kobo.678.1">GitHub: </span></span><a href="https://github.com/rsteca/sklearn-deap" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.679.1">https://github.com/rsteca/sklearn-deap</span></span></a></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.680.1">Scikit-learn</span></em><span class="kobospan" id="kobo.681.1"> AdaBoost </span><span><span class="kobospan" id="kobo.682.1">Classifier: </span></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.683.1">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html</span></span></a></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.684.1">UCI Machine Learning </span></em><span><em class="italic"><span class="kobospan" id="kobo.685.1">Repository</span></em></span><span><span class="kobospan" id="kobo.686.1">: </span></span><a href="https://archive.ics.uci.edu/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.687.1">https://archive.ics.uci.edu/</span></span></a></li>
</ul>
</div>
</body></html>