["```py\n    self.initWineDataset()\n    self.initClassifier()\n    self.initKfold()\n    self.initGridParams()\n    ```", "```py\n    self.gridParams = {\n        'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n        'learning_rate': np.logspace(-2, 0, num=10, base=10),\n        'algorithm': ['SAMME', 'SAMME.R'],\n    }\n    ```", "```py\n    cv_results = model_selection.cross_val_score(\n        self.classifier,\n        self.X,\n        self.y,\n        cv=self.kfold,\n        scoring='accuracy')\n    return cv_results.mean()\n    ```", "```py\n    gridSearch = GridSearchCV(\n        estimator=self.classifier,\n        param_grid=self.gridParams,\n        cv=self.kfold,\n        scoring='accuracy')\n    gridSearch.fit(self.X, self.y)\n    ```", "```py\n    gridSearch = EvolutionaryAlgorithmSearchCV(\n        estimator=self.classifier,\n        params=self.gridParams,\n        cv=self.kfold,\n        scoring='accuracy',\n        verbose=True,\n        population_size=20,\n        gene_mutation_prob=0.50,\n        tournament_size=2,\n        generations_number=5)\n    gridSearch.fit(self.X, self.y)\n    ```", "```py\nDefault Classifier Hyperparameter values:\n{'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 42}\nscore with default values =  0.6636507936507937\n```", "```py\nperforming grid search...\nbest parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.5994842503189409, 'n_estimators': 50}\nbest score:  0.9266666666666667\nTime Elapsed =  131.01380705833435\n```", "```py\nperforming Genetic grid search...\nTypes [1, 2, 1] and maxint [9, 9, 1] detected\n```", "```py\n--- Evolve in 200 possible combinations ---\n```", "```py\ngen  nevals    avg        min       max        std\n0     20    0.708146   0.117978   0.910112   0.265811\n1     13    0.870787   0.662921   0.910112   0.0701235\n2     10    0.857865   0.662921   0.91573    0.0735955\n3     12    0.87809    0.679775   0.904494   0.0473746\n4     12    0.878933   0.662921   0.910112   0.0524153\n5     7     0.864045   0.162921   0.926966   0.161174\n```", "```py\nBest individual is: {'n_estimators': 50, 'learning_rate': 0.5994842503189409, 'algorithm': 'SAMME.R'}\nwith fitness: 0.9269662921348315\nTime Elapsed =  21.147947072982788\n```", "```py\n    n_estimators = round(params[0])\n    learning_rate = params[1]\n    algorithm = ['SAMME', 'SAMME.R'][round(params[2])]\n    ```", "```py\n    n_estimators, learning_rate, algorithm = \\\n        self.convertParams(params)\n    self.classifier =  AdaBoostClassifier(\n        n_estimators=n_estimators,\n        learning_rate=learning_rate,\n        algorithm=algorithm)\n    ```", "```py\n    cv_results = model_selection.cross_val_score(\n        self.classifier,\n        self.X,\n        self.y,\n        cv=self.kfold,\n        scoring='accuracy')\n    return cv_results.mean()\n    ```", "```py\n    # [n_estimators, learning_rate, algorithm]:\n    BOUNDS_LOW =  [  1, 0.01, 0]\n    BOUNDS_HIGH = [100, 1.00, 1]\n    ```", "```py\n    test = HyperparameterTuningGenetic(RANDOM_SEED)\n    ```", "```py\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    ```", "```py\n    for i in range(NUM_OF_PARAMS):\n        # \"hyperparameter_0\", \"hyperparameter_1\", ...\n        toolbox.register(\"hyperparameter_\" + str(i),\n                          random.uniform,\n                          BOUNDS_LOW[i],\n                          BOUNDS_HIGH[i])\n    ```", "```py\n    hyperparameters = ()\n    for i in range(NUM_OF_PARAMS):\n        hyperparameters = hyperparameters + \\\n            (toolbox.__getattribute__(\"hyperparameter_\" + str(i)),)\n    ```", "```py\n    toolbox.register(\"individualCreator\",\n                      tools.initCycle,\n                      creator.Individual,\n                      hyperparameters,\n                      n=1)\n    ```", "```py\n    def classificationAccuracy(individual):\n        return test.getAccuracy(individual),\n    toolbox.register(\"evaluate\", classificationAccuracy)\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\",\n                     tools.cxSimulatedBinaryBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR)\n    toolbox.register(\"mutate\",\n                     tools.mutPolynomialBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR,\n                     indpb=1.0 / NUM_OF_PARAMS)\n    ```", "```py\n    population, logbook = elitism.eaSimpleWithElitism(\n        population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\ngen nevals max avg\n0       30      0.927143        0.831439\n1       22      0.93254         0.902741\n2       23      0.93254         0.907847\n3       25      0.943651        0.916566\n4       24      0.943651        0.921106\n5       24      0.943651        0.921751\n- Best solution is:\nparams =  'n_estimators'= 30, 'learning_rate'=0.613, 'algorithm'=SAMME.R\nAccuracy = n_estimators = 30, learning_rate = 0.613, and algorithm = 'SAMME.R'.\nThe classification accuracy that we achieved with these values is about 94.4%—a worthy improvement over the accuracy we achieved with the grid search. Interestingly, the best value that was found for `learning_rate` is just outside the grid values we searched on.\nDedicated libraries\nIn recent years, several genetic-algorithm-based libraries have been developed that are dedicated to optimizing machine learning model development. One of them is `sklearn-genetic-opt` ([https://sklearn-genetic-opt.readthedocs.io/en/stable/index.html](https://sklearn-genetic-opt.readthedocs.io/en/stable/index.html)); it supports both hyperparameters tuning and feature selection. Another more elaborate library is `TPOT`([https://epistasislab.github.io/tpot/](https://epistasislab.github.io/tpot/)); this library provides optimization for the end-to-end machine learning development process, also called the **pipeline**. You are encouraged to try out these libraries in your own projects.\nSummary\nIn this chapter, you were introduced to the concept of hyperparameter tuning in machine learning. After getting acquainted with the Wine dataset and the AdaBoost classifier, both of which we used for testing throughout this chapter, you were presented with the hyperparameter tuning methods of an exhaustive grid search and its genetic-algorithm-driven counterpart. These two methods were then compared using our test scenario. Finally, we tried out a direct genetic algorithm approach, where all the hyperparameters were represented as float values. This approach allowed us to improve the results of the grid search.\nIn the next chapter, we will look into the fascinating machine learning models of **neural networks** and **deep learning** and apply genetic algorithms to improve their performance.\nFurther reading\nFor more information on the topics that were covered in this chapter, please refer to the following resources:\n\n*   Cross-validation and Parameter Tuning, from the book *Mastering Predictive Analytics with scikit-learn and TensorFlow*, Alan Fontaine, September 2018:\n*   [https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789617740/2/ch02lvl1sec16/introduction-to-hyperparameter-tuning](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789617740/2/ch02lvl1sec16/introduction-to-hyperparameter-tuning)\n*   *sklearn-deap* at GitHub: [https://github.com/rsteca/sklearn-deap](https://github.com/rsteca/sklearn-deap)\n*   *Scikit-learn* AdaBoost Classifier: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n*   *UCI Machine Learning* *Repository*: [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)\n\n```"]