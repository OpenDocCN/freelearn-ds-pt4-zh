<html><head></head><body>
<div id="_idContainer123" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-205"><a id="_idTextAnchor257" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-206" class="calibre5"><a id="_idTextAnchor258" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Architecture Optimization of Deep Learning Networks</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter describes how genetic</span><a id="_idIndexMarker573" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.4.1"> algorithms can be used to improve the performance of </span><strong class="bold"><span class="kobospan" id="kobo.5.1">artificial neural network</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.7.1">ANN</span></strong><span class="kobospan" id="kobo.8.1">)-based models </span><a id="_idIndexMarker574" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.9.1">by optimizing the </span><strong class="bold"><span class="kobospan" id="kobo.10.1">network architecture</span></strong><span class="kobospan" id="kobo.11.1"> of these models. </span><span class="kobospan" id="kobo.11.2">We will start</span><a id="_idIndexMarker575" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.12.1"> with a brief</span><a id="_idIndexMarker576" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.13.1"> introduction to </span><strong class="bold"><span class="kobospan" id="kobo.14.1">neural networks</span></strong><span class="kobospan" id="kobo.15.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.16.1">NNs</span></strong><span class="kobospan" id="kobo.17.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.18.1">deep learning</span></strong><span class="kobospan" id="kobo.19.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.20.1">DL</span></strong><span class="kobospan" id="kobo.21.1">). </span><span class="kobospan" id="kobo.21.2">After introducing </span><a id="_idIndexMarker577" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.22.1">the </span><em class="italic"><span class="kobospan" id="kobo.23.1">Iris dataset</span></em><span class="kobospan" id="kobo.24.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.25.1">Multilayer Perceptron</span></strong><span class="kobospan" id="kobo.26.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.27.1">MLP</span></strong><span class="kobospan" id="kobo.28.1">) classifiers, we will demonstrate </span><strong class="bold"><span class="kobospan" id="kobo.29.1">network architecture optimization</span></strong><span class="kobospan" id="kobo.30.1"> using a genetic algorithm-based</span><a id="_idIndexMarker578" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.31.1"> solution. </span><span class="kobospan" id="kobo.31.2">Then, we will extend this approach to combine network architecture</span><a id="_idIndexMarker579" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.32.1"> optimization with model </span><strong class="bold"><span class="kobospan" id="kobo.33.1">hyperparameter tuning</span></strong><span class="kobospan" id="kobo.34.1">, which will be jointly carried out by a genetic </span><span><span class="kobospan" id="kobo.35.1">algorithm-based solution.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.36.1">In this chapter, we will cover the </span><span><span class="kobospan" id="kobo.37.1">following topics:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.38.1">Understanding the basic concepts of ANNs </span><span><span class="kobospan" id="kobo.39.1">and DL</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.40.1">Enhancing the performance of a DL classifier using network </span><span><span class="kobospan" id="kobo.41.1">architecture optimization</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.42.1">Further enhancing the performance of the DL classifier by combining network architecture optimization with </span><span><span class="kobospan" id="kobo.43.1">hyperparameter tuning</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.44.1">We will start this chapter with an overview of ANNs. </span><span class="kobospan" id="kobo.44.2">If you are a seasoned data scientist, feel free to skip the </span><span><span class="kobospan" id="kobo.45.1">introductory sections.</span></span></p>
<h1 id="_idParaDest-207" class="calibre5"><a id="_idTextAnchor259" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.46.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">In this chapter, we will be using Python 3 with the following </span><span><span class="kobospan" id="kobo.48.1">supporting libraries:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.49.1">deap</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.50.1">numpy</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.51.1">scikit-learn</span></strong></span></li>
</ul>
<p class="callout-heading"><span class="kobospan" id="kobo.52.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.53.1">If you use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.54.1">requirements.txt</span></strong><span class="kobospan" id="kobo.55.1"> file we provide (see </span><a href="B20851_03.xhtml#_idTextAnchor091" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.56.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.57.1">), these libraries are already included in </span><span><span class="kobospan" id="kobo.58.1">your environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.59.1">In addition, we will be using the UCI Iris flower </span><span><span class="kobospan" id="kobo.60.1">dataset (</span></span><a href="https://archive.ics.uci.edu/ml/datasets/Iris" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.61.1">https://archive.ics.uci.edu/ml/datasets/Iris</span></span></a><span><span class="kobospan" id="kobo.62.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.63.1">The programs that will be used in this chapter can be found in this book’s GitHub repository at the </span><span><span class="kobospan" id="kobo.64.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.65.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.66.1">Check out the following video to see the code in </span><span><span class="kobospan" id="kobo.67.1">action: </span></span><a href="https://packt.link/OEBOd" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.68.1">https://packt.link/OEBOd</span></span></a></p>
<h1 id="_idParaDest-208" class="calibre5"><a id="_idTextAnchor260" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.69.1">ANNs and DL</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.70.1">Inspired by the structure</span><a id="_idIndexMarker580" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.71.1"> of the human</span><a id="_idIndexMarker581" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.72.1"> brain, NNs are among the most commonly used models in </span><strong class="bold"><span class="kobospan" id="kobo.73.1">machine learning</span></strong><span class="kobospan" id="kobo.74.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.75.1">ML</span></strong><span class="kobospan" id="kobo.76.1">). </span><span class="kobospan" id="kobo.76.2">The basic building blocks of these networks are nodes, or </span><strong class="bold"><span class="kobospan" id="kobo.77.1">neurons</span></strong><span class="kobospan" id="kobo.78.1">, which are based on the biological</span><a id="_idIndexMarker582" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.79.1"> neuron cell, as depicted in the </span><span><span class="kobospan" id="kobo.80.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer120">
<span class="kobospan" id="kobo.81.1"><img alt="Figure 9.1: Biological neuron model " src="image/B20851_09_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.82.1">Figure 9.1: Biological neuron model </span></p>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.83.1">Source: </span><a href="https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.84.1">https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg</span></a><span class="kobospan" id="kobo.85.1"> by Dhp1080</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.86.1">The neuron cell’s </span><strong class="bold"><span class="kobospan" id="kobo.87.1">dendrites</span></strong><span class="kobospan" id="kobo.88.1">, which surround the </span><strong class="bold"><span class="kobospan" id="kobo.89.1">cell body</span></strong><span class="kobospan" id="kobo.90.1"> on the left-hand</span><a id="_idIndexMarker583" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.91.1"> side of the preceding diagram, are used as inputs from multiple similar cells, while the long </span><strong class="bold"><span class="kobospan" id="kobo.92.1">axon</span></strong><span class="kobospan" id="kobo.93.1">, coming out of the </span><strong class="bold"><span class="kobospan" id="kobo.94.1">cell body</span></strong><span class="kobospan" id="kobo.95.1">, serves as output and can be connected to multiple other cells via </span><span><span class="kobospan" id="kobo.96.1">its </span></span><span><strong class="bold"><span class="kobospan" id="kobo.97.1">terminals</span></strong></span><span><span class="kobospan" id="kobo.98.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.99.1">This structure is mimicked</span><a id="_idIndexMarker584" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.100.1"> by an </span><a id="_idIndexMarker585" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.101.1">artificial model called a </span><strong class="bold"><span class="kobospan" id="kobo.102.1">perceptron</span></strong><span class="kobospan" id="kobo.103.1">, illustrated </span><span><span class="kobospan" id="kobo.104.1">as follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer121">
<span class="kobospan" id="kobo.105.1"><img alt="Figure 9.2: Artificial neuron model – the perceptron" src="image/B20851_09_2.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.106.1">Figure 9.2: Artificial neuron model – the perceptron</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.107.1">The perceptron calculates the output by multiplying each of the input values by a certain </span><strong class="bold"><span class="kobospan" id="kobo.108.1">weight</span></strong><span class="kobospan" id="kobo.109.1">; the results are accumulated, and a </span><strong class="bold"><span class="kobospan" id="kobo.110.1">bias</span></strong><span class="kobospan" id="kobo.111.1"> value is added to the sum. </span><span class="kobospan" id="kobo.111.2">A non-linear </span><strong class="bold"><span class="kobospan" id="kobo.112.1">activation function</span></strong><span class="kobospan" id="kobo.113.1"> then maps the result to the</span><a id="_idIndexMarker586" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.114.1"> output. </span><span class="kobospan" id="kobo.114.2">This functionality emulates the operation of the biological neuron, which fires (sends a series of pulses from its output) when the weighted sum of the inputs is above a </span><span><span class="kobospan" id="kobo.115.1">certain threshold.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.116.1">The perceptron model</span><a id="_idIndexMarker587" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.117.1"> can be used for simple classification and regression tasks if we adjust its weight and bias values so that they map certain inputs to the desired output levels. </span><span class="kobospan" id="kobo.117.2">However, a much more capable model can be constructed when connecting multiple perceptron units in a structure called an MLP, which will be described in the </span><span><span class="kobospan" id="kobo.118.1">next subsection.</span></span></p>
<h2 id="_idParaDest-209" class="calibre7"><a id="_idTextAnchor261" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.119.1">MLP</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.120.1">An MLP extends the idea</span><a id="_idIndexMarker588" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.121.1"> of the perceptron by using numerous nodes, each one implementing a perceptron. </span><span class="kobospan" id="kobo.121.2">The nodes in an MLP are arranged in </span><strong class="bold"><span class="kobospan" id="kobo.122.1">layers</span></strong><span class="kobospan" id="kobo.123.1">, and each layer is connected to the next. </span><span class="kobospan" id="kobo.123.2">The basic structure of an MLP is illustrated in the </span><span><span class="kobospan" id="kobo.124.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer122">
<span class="kobospan" id="kobo.125.1"><img alt="Figure 9.3: The basic structure of an MLP" src="image/B20851_09_3.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.126.1">Figure 9.3: The basic structure of an MLP</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.127.1">An MLP consists of three </span><span><span class="kobospan" id="kobo.128.1">main parts:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.129.1">Input layer</span></strong><span class="kobospan" id="kobo.130.1">: Receives the input values </span><a id="_idIndexMarker589" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.131.1">and connects each of them to every neuron in the </span><span><span class="kobospan" id="kobo.132.1">next layer.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.133.1">Output layer</span></strong><span class="kobospan" id="kobo.134.1">: Delivers the results calculated by </span><a id="_idIndexMarker590" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.135.1">the MLP. </span><span class="kobospan" id="kobo.135.2">When the MLP is used as a </span><strong class="bold"><span class="kobospan" id="kobo.136.1">classifier</span></strong><span class="kobospan" id="kobo.137.1">, each of the outputs represents</span><a id="_idIndexMarker591" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.138.1"> one of the classes. </span><span class="kobospan" id="kobo.138.2">When the MLP is used for </span><strong class="bold"><span class="kobospan" id="kobo.139.1">regression</span></strong><span class="kobospan" id="kobo.140.1">, there will be a single output</span><a id="_idIndexMarker592" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.141.1"> node, producing a </span><span><span class="kobospan" id="kobo.142.1">continuous value.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.143.1">Hidden layer(s)</span></strong><span class="kobospan" id="kobo.144.1">: Provide the true power and complexity</span><a id="_idIndexMarker593" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.145.1"> of this model. </span><span class="kobospan" id="kobo.145.2">While the preceding diagram shows only two hidden layers, there can be numerous hidden layers, each an arbitrary size, that are placed between the input and output layers. </span><span class="kobospan" id="kobo.145.3">As the number of hidden layers grows, the network becomes deeper and is capable of performing an increasingly more complex and non-linear mapping between the inputs and </span><span><span class="kobospan" id="kobo.146.1">the outputs.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.147.1">Training this model involves adjusting the weight and bias values for each of the nodes. </span><span class="kobospan" id="kobo.147.2">This is typically</span><a id="_idIndexMarker594" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.148.1"> done using a family of algorithms called </span><strong class="bold"><span class="kobospan" id="kobo.149.1">backpropagation</span></strong><span class="kobospan" id="kobo.150.1">. </span><span class="kobospan" id="kobo.150.2">The basic principle of backpropagation is to minimize the error between the actual outputs and the desired ones by propagating the output error through the layers of the MLP model, from the output layer inward. </span><span class="kobospan" id="kobo.150.3">The process begins by defining a cost (or “loss”) function, typically a measure of the difference between the predicted outputs and the actual target values. </span><span class="kobospan" id="kobo.150.4">The weights and biases of the various nodes are adjusted so that those that contributed the most to the error see the greatest adjustments. </span><span class="kobospan" id="kobo.150.5">By iteratively reducing the cost function, the algorithm refines the model parameters to </span><span><span class="kobospan" id="kobo.151.1">improve performance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.152.1">For many years, the computational</span><a id="_idIndexMarker595" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.153.1"> limitations of backpropagation algorithms restricted MLPs to no more than two or three hidden layers, until new developments changed matters dramatically. </span><span class="kobospan" id="kobo.153.2">These will be explained in the </span><span><span class="kobospan" id="kobo.154.1">next section.</span></span></p>
<h2 id="_idParaDest-210" class="calibre7"><a id="_idTextAnchor262" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.155.1">DL and convolutional NNs</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.156.1">In recent years, backpropagation</span><a id="_idIndexMarker596" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.157.1"> algorithms have made a leap forward, enabling the use of a large number of hidden layers in a single network. </span><span class="kobospan" id="kobo.157.2">In these </span><strong class="bold"><span class="kobospan" id="kobo.158.1">deep NNs</span></strong><span class="kobospan" id="kobo.159.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.160.1">DNNs</span></strong><span class="kobospan" id="kobo.161.1">), each layer can interpret a combination </span><a id="_idIndexMarker597" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.162.1">of several simpler abstract concepts that were learned by the nodes of the previous layer and produce higher-level concepts. </span><span class="kobospan" id="kobo.162.2">For example, when implementing a face recognition task, the first layer will process the pixels of an image and learn to detect edges in different orientations. </span><span class="kobospan" id="kobo.162.3">The next layer may assemble these into lines, corners, and so on, up to a layer that detects facial features such as nose and lips, and finally, one that combines these into the complete concept of </span><span><span class="kobospan" id="kobo.163.1">a face.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.164.1">Further advancements</span><a id="_idIndexMarker598" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.165.1"> have brought about the idea of </span><strong class="bold"><span class="kobospan" id="kobo.166.1">convolutional NNs</span></strong><span class="kobospan" id="kobo.167.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.168.1">CNNs</span></strong><span class="kobospan" id="kobo.169.1">). </span><span class="kobospan" id="kobo.169.2">These structures can reduce the count of nodes in DNNs that process two-dimensional information (such as images) by treating nearby inputs differently compared to inputs that are far apart. </span><span class="kobospan" id="kobo.169.3">As a result, these models have proved especially successful when it comes to image and video processing tasks. </span><span class="kobospan" id="kobo.169.4">Besides fully connected layers, similar to the hidden layers in the MLP, these networks utilize pooling (down-sampling) layers, which aggregate outputs of neurons from preceding layers, and convolutional layers, which are used for detecting specific features, such as edges in various orientations, by effectively sliding a filter over the </span><span><span class="kobospan" id="kobo.170.1">input image.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.171.1">Training such </span><strong class="bold"><span class="kobospan" id="kobo.172.1">DL models</span></strong><span class="kobospan" id="kobo.173.1"> can be computationally intensive</span><a id="_idIndexMarker599" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.174.1"> and is often done with the aid of </span><strong class="bold"><span class="kobospan" id="kobo.175.1">graphics processing units</span></strong><span class="kobospan" id="kobo.176.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.177.1">GPUs</span></strong><span class="kobospan" id="kobo.178.1">), which are more efficient than ordinary CPUs</span><a id="_idIndexMarker600" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.179.1"> in implementing the backpropagation algorithm. </span><span class="kobospan" id="kobo.179.2">Specialized DL libraries, such as Torch and  TensorFlow, are capable of utilizing GPU-based computing platforms. </span><span class="kobospan" id="kobo.179.3">In this chapter, however, for the sake of simplicity, we will be using the MLP implementation offered by the </span><strong class="source-inline"><span class="kobospan" id="kobo.180.1">scikit-learn</span></strong><span class="kobospan" id="kobo.181.1"> library and a simple dataset. </span><span class="kobospan" id="kobo.181.2">The principles that will be used, however, still apply to more complex networks </span><span><span class="kobospan" id="kobo.182.1">and datasets.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">In the next section, we will find out how the architecture of an MLP can be optimized using a </span><span><span class="kobospan" id="kobo.184.1">genetic algorithm.</span></span></p>
<h1 id="_idParaDest-211" class="calibre5"><a id="_idTextAnchor263" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.185.1">Optimizing the architecture of a DL classifier</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.186.1">When creating a NN model for a given ML task, one</span><a id="_idIndexMarker601" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.187.1"> crucial design decision that needs to be made</span><a id="_idIndexMarker602" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.188.1"> is the configuration of the </span><strong class="bold"><span class="kobospan" id="kobo.189.1">network architecture</span></strong><span class="kobospan" id="kobo.190.1">. </span><span class="kobospan" id="kobo.190.2">In the case of an MLP, the number of nodes in the input and output layers is determined by the characteristics of the problem at hand. </span><span class="kobospan" id="kobo.190.3">Therefore, the choices to be made are about the hidden layers—how many layers, and how many nodes are in each layer. </span><span class="kobospan" id="kobo.190.4">Some rules of thumb can be employed for making these decisions, but in many cases, identifying the best choices can turn into a cumbersome </span><span><span class="kobospan" id="kobo.191.1">trial-and-error process.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.192.1">One way to handle network architecture parameters is to consider them as hyperparameters of the model since they need to be determined before training is done and, consequently, affect the training’s results. </span><span class="kobospan" id="kobo.192.2">In this section, we are going to apply this approach and use a genetic algorithm to search for the best combination of hidden layers, in a similar manner to the way </span><a id="_idIndexMarker603" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.193.1">we went about choosing the best hyperparameter values in the previous chapter. </span><span class="kobospan" id="kobo.193.2">Let’s start with the task we want to tackle – the </span><strong class="bold"><span class="kobospan" id="kobo.194.1">Iris </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.195.1">flower classification</span></strong></span><span><span class="kobospan" id="kobo.196.1">.</span></span></p>
<h2 id="_idParaDest-212" class="calibre7"><a id="_idTextAnchor264" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.197.1">The Iris flower dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.198.1">Perhaps the most</span><a id="_idIndexMarker604" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.199.1"> well-studied dataset, the </span><em class="italic"><span class="kobospan" id="kobo.200.1">Iris flower dataset</span></em><span class="kobospan" id="kobo.201.1"> (</span><a href="https://archive.ics.uci.edu/ml/datasets/Iris" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.202.1">https://archive.ics.uci.edu/ml/datasets/Iris</span></a><span class="kobospan" id="kobo.203.1">) contains measurements of the </span><strong class="source-inline"><span class="kobospan" id="kobo.204.1">sepal</span></strong><em class="italic"> </em><span class="kobospan" id="kobo.205.1">and </span><strong class="source-inline"><span class="kobospan" id="kobo.206.1">petal</span></strong><span class="kobospan" id="kobo.207.1"> parts</span><a id="_idIndexMarker605" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.208.1"> of three Iris species (Iris setosa, Iris virginica, and Iris versicolor), as taken by biologists </span><span><span class="kobospan" id="kobo.209.1">in 1936.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.210.1">The dataset contains 50 samples from each of the three species, and consists of the following </span><span><span class="kobospan" id="kobo.211.1">four features:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.212.1">sepal_length (cm)</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.213.1">sepal_width (cm)</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.214.1">petal_length (cm)</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.215.1">petal_width (cm)</span></strong></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.216.1">This dataset is directly available via the </span><strong class="source-inline"><span class="kobospan" id="kobo.217.1">scikit-learn</span></strong><span class="kobospan" id="kobo.218.1"> library and can be initialized </span><span><span class="kobospan" id="kobo.219.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.220.1">
from sklearn import </span><strong class="bold1"><span class="kobospan1" id="kobo.221.1">datasets</span></strong><span class="kobospan1" id="kobo.222.1">
data = datasets.</span><strong class="bold1"><span class="kobospan1" id="kobo.223.1">load_iris()</span></strong><span class="kobospan1" id="kobo.224.1">
X = data['data']
y = data['target']</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.225.1">In our experiments, we will be using an MLP classifier in conjunction with this dataset and harness the power of genetic algorithms to find the network architecture—the number of hidden layers and the number of nodes in each layer—that will yield the best </span><span><span class="kobospan" id="kobo.226.1">classification accuracy.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.227.1">Since we are using the genetic algorithms approach, the first thing we need to do is find a way to represent this architecture </span><a id="_idIndexMarker606" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.228.1">using a chromosome, as described in the </span><span><span class="kobospan" id="kobo.229.1">next subsection.</span></span></p>
<h2 id="_idParaDest-213" class="calibre7"><a id="_idTextAnchor265" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.230.1">Representing the hidden layer configuration</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.231.1">Since the architecture of an MLP</span><a id="_idIndexMarker607" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.232.1"> is determined by the hidden layer configuration, let’s explore how this configuration can be represented in our solution. </span><span class="kobospan" id="kobo.232.2">The hidden layer</span><a id="_idIndexMarker608" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.233.1"> configuration of the </span><strong class="source-inline"><span class="kobospan" id="kobo.234.1">sklearn</span></strong><span class="kobospan" id="kobo.235.1"> MLP (</span><a href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.236.1">https://scikit-learn.org/stable/modules/neural_networks_supervised.html</span></a><span class="kobospan" id="kobo.237.1">) model is conveyed via the </span><strong class="source-inline"><span class="kobospan" id="kobo.238.1">hidden_layer_sizes</span></strong><span class="kobospan" id="kobo.239.1"> tuple, which is sent as a parameter to the model’s constructor. </span><span class="kobospan" id="kobo.239.2">By default, the value of this tuple is </span><strong class="source-inline"><span class="kobospan" id="kobo.240.1">(100,)</span></strong><span class="kobospan" id="kobo.241.1">, which means a single hidden layer of 100 nodes. </span><span class="kobospan" id="kobo.241.2">If we wanted, for example, to configure the MLP with three hidden layers of 20 nodes each, this parameter’s value would be </span><strong class="source-inline"><span class="kobospan" id="kobo.242.1">(20, 20, 20)</span></strong><span class="kobospan" id="kobo.243.1">. </span><span class="kobospan" id="kobo.243.2">Before we implement our genetic algorithm-based optimizer for the hidden layer configuration, we need to define a chromosome that can be translated into </span><span><span class="kobospan" id="kobo.244.1">this pattern.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.245.1">To accomplish this, we need to come up with a chromosome that can both express the number of layers and the number of nodes in each layer. </span><span class="kobospan" id="kobo.245.2">A variable-length chromosome that can be directly translated into the variable-length tuple that’s used as the model’s </span><strong class="source-inline"><span class="kobospan" id="kobo.246.1">hidden_layer_sizes</span></strong><span class="kobospan" id="kobo.247.1"> parameter is one option; however, this approach would require custom, possibly cumbersome, genetic operators. </span><span class="kobospan" id="kobo.247.2">To be able to use our standard genetic operators, we will use a fixed-length representation. </span><span class="kobospan" id="kobo.247.3">When using this approach, the maximum number of layers is decided in advance, and all the layers are always represented, but not necessarily expressed in the solution. </span><span class="kobospan" id="kobo.247.4">For example, if we decide to limit the network to four hidden layers, the chromosome will look </span><span><span class="kobospan" id="kobo.248.1">as follows:</span></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.249.1">[</span></span><span><span class="kobospan" id="kobo.250.1">n</span></span><span><span class="kobospan" id="kobo.251.1"> </span></span><span><span class="kobospan" id="kobo.252.1">1</span></span><span><span class="kobospan" id="kobo.253.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.254.1">n</span></span><span><span class="kobospan" id="kobo.255.1"> </span></span><span><span class="kobospan" id="kobo.256.1">2</span></span><span><span class="kobospan" id="kobo.257.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.258.1">n</span></span><span><span class="kobospan" id="kobo.259.1"> </span></span><span><span class="kobospan" id="kobo.260.1">3</span></span><span><span class="kobospan" id="kobo.261.1">,</span></span><span> </span><span><span><span class="kobospan" id="kobo.262.1">n</span></span></span><span><span><span class="kobospan" id="kobo.263.1"> </span></span></span><span><span><span class="kobospan" id="kobo.264.1">4</span></span></span><span><span><span class="kobospan" id="kobo.265.1">]</span></span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.266.1">Here, </span><span><span class="kobospan" id="kobo.267.1">n</span></span><span><span class="kobospan" id="kobo.268.1"> </span></span><span><span class="kobospan" id="kobo.269.1">i</span></span><span class="kobospan" id="kobo.270.1"> denotes the number of nodes in the layer </span><span><span class="kobospan" id="kobo.271.1">i</span></span><span> </span><span class="kobospan" id="kobo.272.1">. </span><span class="kobospan" id="kobo.272.2">However, to control the actual number of hidden layers in the network, some of these values may be zero, or negative. </span><span class="kobospan" id="kobo.272.3">Such a value means that no more layers will be added to the network. </span><span class="kobospan" id="kobo.272.4">The following examples illustrate </span><span><span class="kobospan" id="kobo.273.1">this method:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.274.1">The chromosome [10, 20, -5, 15] is translated into the tuple (10, 20) since -5 terminates the </span><span><span class="kobospan" id="kobo.275.1">layer count</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.276.1">The chromosome [10, 0, -5, 15] is translated into the tuple (10, ) since 0 terminates the </span><span><span class="kobospan" id="kobo.277.1">layer count</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.278.1">The chromosome [10, 20, 5, -15] is translated into the tuple (10, 20, 5) since -15 terminates the </span><span><span class="kobospan" id="kobo.279.1">layer count</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.280.1">The chromosome [10, 20, 5, 15] is translated into the tuple (10, 20, </span><span><span class="kobospan" id="kobo.281.1">5, 15)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">To guarantee that there is at least one hidden layer, we can make sure that the first parameter is always greater than zero. </span><span class="kobospan" id="kobo.282.2">The other parameters can have varying distributions around zero so that we can control their chances of being the </span><span><span class="kobospan" id="kobo.283.1">terminating parameters.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.284.1">In addition, even though this chromosome is made up of integers, we chose to utilize float numbers instead, just like we did in the previous chapter for various types of variables. </span><span class="kobospan" id="kobo.284.2">Using a list of float numbers is convenient as it allows us to use existing genetic operators while being able to easily extend the chromosome so that it includes other parameters of different types, which we will do later on. </span><span class="kobospan" id="kobo.284.3">The float numbers can be translated back into integers using the </span><strong class="source-inline"><span class="kobospan" id="kobo.285.1">round()</span></strong><span class="kobospan" id="kobo.286.1"> function. </span><span class="kobospan" id="kobo.286.2">A couple of examples of this generalized approach are </span><span><span class="kobospan" id="kobo.287.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.288.1">The chromosome [9.35, 10.71, -2.51, 17.99] is translated into the tuple (</span><span><span class="kobospan" id="kobo.289.1">9, 11)</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.290.1">The chromosome [9.35, 10.71, 2.51, -17.99] is translated into the tuple (9, </span><span><span class="kobospan" id="kobo.291.1">11, 3)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.292.1">To evaluate a given architecture-representing</span><a id="_idIndexMarker609" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.293.1"> chromosome, we will need to translate it back into the tuple of layers, create an MLP classifier implementing these layers, train it, and evaluate it. </span><span class="kobospan" id="kobo.293.2">We will learn how to do this in the </span><span><span class="kobospan" id="kobo.294.1">next subsection.</span></span></p>
<h2 id="_idParaDest-214" class="calibre7"><a id="_idTextAnchor266" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.295.1">Evaluating the classifier’s accuracy</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.296.1">Let’s start with a Python</span><a id="_idIndexMarker610" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.297.1"> class that encapsulates the MLP classifier’s accuracy evaluation for the Iris dataset. </span><span class="kobospan" id="kobo.297.2">The class is called </span><strong class="source-inline"><span class="kobospan" id="kobo.298.1">MlpLayersTest</span></strong><span class="kobospan" id="kobo.299.1"> and can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.300.1">mlp_layers_test.py</span></strong><span class="kobospan" id="kobo.301.1"> file, which is located at the </span><span><span class="kobospan" id="kobo.302.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.303.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.304.1">The main functionality of this class is highlighted </span><span><span class="kobospan" id="kobo.305.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.306.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.307.1">convertParam()</span></strong><span class="kobospan" id="kobo.308.1"> method of the class takes a list called </span><strong class="source-inline1"><span class="kobospan" id="kobo.309.1">params</span></strong><span class="kobospan" id="kobo.310.1">. </span><span class="kobospan" id="kobo.310.2">This is actually the chromosome that we described in the previous subsection and contains the float values that represent up to four hidden layers. </span><span class="kobospan" id="kobo.310.3">The method transforms this list of floats</span><a id="_idIndexMarker611" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.311.1"> into the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.312.1">hidden_layer_sizes</span></strong></span><span><span class="kobospan" id="kobo.313.1"> tuple:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.314.1">
if round(params[1]) &lt;= 0:
    </span><strong class="bold1"><span class="kobospan1" id="kobo.315.1">hiddenLayerSizes</span></strong><span class="kobospan1" id="kobo.316.1"> = round(params[0]),
elif round(params[2]) &lt;= 0:
    </span><strong class="bold1"><span class="kobospan1" id="kobo.317.1">hiddenLayerSizes</span></strong><span class="kobospan1" id="kobo.318.1"> = (round(params[0]),
                        round(params[1]))
elif round(params[3]) &lt;= 0:
    </span><strong class="bold1"><span class="kobospan1" id="kobo.319.1">hiddenLayerSizes</span></strong><span class="kobospan1" id="kobo.320.1"> = (round(params[0]),
                        round(params[1]),
                        round(params[2]))
else:
    </span><strong class="bold1"><span class="kobospan1" id="kobo.321.1">hiddenLayerSizes</span></strong><span class="kobospan1" id="kobo.322.1"> = (round(params[0]),
                        round(params[1]),
                        round(params[2]),
                        round(params[3]))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.323.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.324.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.325.1"> method takes the </span><strong class="source-inline1"><span class="kobospan" id="kobo.326.1">params</span></strong><span class="kobospan" id="kobo.327.1"> list representing the configuration of the hidden layers, uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.328.1">convertParam()</span></strong><span class="kobospan" id="kobo.329.1"> method to transform it into a </span><strong class="source-inline1"><span class="kobospan" id="kobo.330.1">hidden_layer_sizes</span></strong><span class="kobospan" id="kobo.331.1"> tuple, and initializes an MLP classifier with </span><span><span class="kobospan" id="kobo.332.1">this tuple:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.333.1">
hiddenLayerSizes = self.</span><strong class="bold1"><span class="kobospan1" id="kobo.334.1">convertParams</span></strong><span class="kobospan1" id="kobo.335.1">(params)
self.classifier = MLPClassifier(
    hidden_layer_sizes=</span><strong class="bold1"><span class="kobospan1" id="kobo.336.1">hiddenLayerSizes</span></strong><span class="kobospan1" id="kobo.337.1">)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.338.1">Then, it finds the accuracy of the classifier using the same </span><em class="italic"><span class="kobospan" id="kobo.339.1">k-fold cross-validation</span></em><span class="kobospan" id="kobo.340.1"> calculation that we created for the </span><em class="italic"><span class="kobospan" id="kobo.341.1">Wine dataset</span></em><span class="kobospan" id="kobo.342.1"> in </span><a href="B20851_08.xhtml#_idTextAnchor238" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.343.1">Chapter 8</span></em></span></a><span class="kobospan" id="kobo.344.1">, </span><em class="italic"><span class="kobospan" id="kobo.345.1">Hyperparameter Tuning of Machine </span></em><span><em class="italic"><span class="kobospan" id="kobo.346.1">Learning Models</span></em></span><span><span class="kobospan" id="kobo.347.1">:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.348.1">cv_results = model_selection.cross_val_score(self.classifier,
                                self.X,
                                self.y,
                                cv=self.kfold,
                                scoring='accuracy')
return cv_results.mean()</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.350.1">MlpLayersTest</span></strong><span class="kobospan" id="kobo.351.1"> class is utilized</span><a id="_idIndexMarker612" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.352.1"> by the genetic algorithm-based optimizer. </span><span class="kobospan" id="kobo.352.2">We will explain this part in the </span><span><span class="kobospan" id="kobo.353.1">next section.</span></span></p>
<h2 id="_idParaDest-215" class="calibre7"><a id="_idTextAnchor267" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.354.1">Optimizing the MLP architecture using genetic algorithms</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.355.1">Now that we have a way</span><a id="_idIndexMarker613" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.356.1"> to represent the architecture</span><a id="_idIndexMarker614" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.357.1"> configuration of the MLP that’s used to classify the Iris flower dataset and a way to determine the accuracy of the MLP for each configuration, we can move on and create a genetic algorithm-based optimizer to search for the configuration – the number of hidden layers (up to 4, in our case) and the number of nodes in each layer – that will yield the best accuracy. </span><span class="kobospan" id="kobo.357.2">This solution is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.358.1">01_optimize_mlp_layers.py</span></strong><span class="kobospan" id="kobo.359.1"> Python program, which is located at the </span><span><span class="kobospan" id="kobo.360.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.361.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.362.1">The following steps describe the main parts of </span><span><span class="kobospan" id="kobo.363.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.364.1">We start by setting the lower and upper boundary for each of the float values representing a hidden layer. </span><span class="kobospan" id="kobo.364.2">The first hidden layer is given the range [5, 15], while the rest of the layers start from increasingly larger negative values, which increases their chances of terminating the </span><span><span class="kobospan" id="kobo.365.1">layer count:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.366.1">
# [hidden_layer_layer_1_size, hidden_layer_2_size
#  hidden_layer_3_size, hidden_layer_4_size]
BOUNDS_LOW =  [ 5,  -5, -10, -20]
BOUNDS_HIGH = [15,  10,  10,  10]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.367.1">Then, we create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.368.1">MlpLayersTest</span></strong><span class="kobospan" id="kobo.369.1"> class, which will allow us to test the various combinations of the hidden </span><span><span class="kobospan" id="kobo.370.1">layers’ architecture:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.371.1">
test = mlp_layers_test.</span><strong class="bold1"><span class="kobospan1" id="kobo.372.1">MlpLayersTest</span></strong><span class="kobospan1" id="kobo.373.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.374.1">Since our goal is to maximize the accuracy of the classifier, we define a single objective, maximizing </span><span><span class="kobospan" id="kobo.375.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.376.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.377.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.378.1">", base.Fitness, weights=(1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.379.1">Now, we employ the same approach</span><a id="_idIndexMarker615" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.380.1"> we used in the previous</span><a id="_idIndexMarker616" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.381.1"> chapter—since the solution is represented by a list of float values, each of a different range, we use the following loop to iterate over all pairs of lower-bound, upper-bound values, and for each range, we create a separate </span><strong class="source-inline1"><span class="kobospan" id="kobo.382.1">toolbox</span></strong><span class="kobospan" id="kobo.383.1"> operator, </span><strong class="source-inline1"><span class="kobospan" id="kobo.384.1">layer_size_attribute</span></strong><span class="kobospan" id="kobo.385.1">, that will later be used to generate random float values in the </span><span><span class="kobospan" id="kobo.386.1">appropriate range:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.387.1">
for i in range(NUM_OF_PARAMS):
    toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.388.1">layer_size_attribute_</span></strong><span class="kobospan1" id="kobo.389.1">" + str(i),
                      random.uniform,
                      BOUNDS_LOW[i],
                      BOUNDS_HIGH[i])</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.390.1">Then, we create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.391.1">layer_size_attributes</span></strong><span class="kobospan" id="kobo.392.1"> tuple, which contains the separate float number generators we just created for each </span><span><span class="kobospan" id="kobo.393.1">hidden layer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.394.1">
layer_size_attributes = ()
for i in range(NUM_OF_PARAMS):
    </span><strong class="bold1"><span class="kobospan1" id="kobo.395.1">layer_size_attributes</span></strong><span class="kobospan1" id="kobo.396.1"> = layer_size_attributes + \
    (toolbox.__getattribute__("layer_size_attribute_" + \
        str(i)),)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.397.1">Now, we can use this </span><strong class="source-inline1"><span class="kobospan" id="kobo.398.1">layer_size_attributes</span></strong><span class="kobospan" id="kobo.399.1"> tuple in conjunction with DEAP’s built-in </span><strong class="source-inline1"><span class="kobospan" id="kobo.400.1">initCycle()</span></strong><span class="kobospan" id="kobo.401.1"> operator to create a new </span><strong class="source-inline1"><span class="kobospan" id="kobo.402.1">individualCreator</span></strong><span class="kobospan" id="kobo.403.1"> operator that fills up an individual instance with a combination of randomly generated hidden </span><span><span class="kobospan" id="kobo.404.1">layer-size values:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.405.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.406.1">individualCreator</span></strong><span class="kobospan1" id="kobo.407.1">",
                  tools.initCycle,
                  creator.Individual,
                  layer_size_attributes,
                  n=1)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.408.1">Then, we instruct the genetic algorithm</span><a id="_idIndexMarker617" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.409.1"> to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.410.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.411.1"> method</span><a id="_idIndexMarker618" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.412.1"> of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.413.1">MlpLayersTest</span></strong><span class="kobospan" id="kobo.414.1"> instance for fitness evaluation. </span><span class="kobospan" id="kobo.414.2">As a reminder, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.415.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.416.1"> method, which we described in the previous subsection, converts the given individual—a list of four floats—into a tuple of hidden layer sizes. </span><span class="kobospan" id="kobo.416.2">These are used to configure the MLP classifier. </span><span class="kobospan" id="kobo.416.3">Then, we train the classifier and evaluate its accuracy using </span><span><span class="kobospan" id="kobo.417.1">k-fold cross-validation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.418.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.419.1">classificationAccuracy</span></strong><span class="kobospan1" id="kobo.420.1">(individual):
    return test.getAccuracy(individual),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.421.1">evaluate</span></strong><span class="kobospan1" id="kobo.422.1">", classificationAccuracy)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.423.1">As for the genetic operators, we repeat the configuration from the previous chapter. </span><span class="kobospan" id="kobo.423.2">While for the </span><em class="italic"><span class="kobospan" id="kobo.424.1">selection</span></em><span class="kobospan" id="kobo.425.1"> operator, we use the usual tournament selection with a tournament size of 2, we choose </span><em class="italic"><span class="kobospan" id="kobo.426.1">crossover</span></em><span class="kobospan" id="kobo.427.1"> and </span><em class="italic"><span class="kobospan" id="kobo.428.1">mutation</span></em><span class="kobospan" id="kobo.429.1"> operators that are specialized for bounded float-list chromosomes and provide them with the boundaries we defined for each </span><span><span class="kobospan" id="kobo.430.1">hidden layer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.431.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.432.1">select</span></strong><span class="kobospan1" id="kobo.433.1">",
                  tools.selTournament,
                  tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.434.1">mate</span></strong><span class="kobospan1" id="kobo.435.1">",
                  tools.cxSimulatedBinaryBounded,
                  low=BOUNDS_LOW,
                  up=BOUNDS_HIGH,
                  eta=CROWDING_FACTOR)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.436.1">mutate</span></strong><span class="kobospan1" id="kobo.437.1">",
                  tools.mutPolynomialBounded,
                  low=BOUNDS_LOW,
                  up=BOUNDS_HIGH,
                  eta=CROWDING_FACTOR,
                  indpb=1.0 / NUM_OF_PARAMS)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.438.1">In addition, we continue</span><a id="_idIndexMarker619" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.439.1"> to use the </span><em class="italic"><span class="kobospan" id="kobo.440.1">elitist</span></em><span class="kobospan" id="kobo.441.1"> approach, where the </span><strong class="bold"><span class="kobospan" id="kobo.442.1">hall-of-fame</span></strong><span class="kobospan" id="kobo.443.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.444.1">HOF</span></strong><span class="kobospan" id="kobo.445.1">) members—the current best</span><a id="_idIndexMarker620" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.446.1"> individuals—are always</span><a id="_idIndexMarker621" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.447.1"> passed untouched to the </span><span><span class="kobospan" id="kobo.448.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.449.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.450.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.451.1">(population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.452.1">When running the algorithm for 10 generations with a population size of 20, we get the </span><span><span class="kobospan" id="kobo.453.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.454.1">
gen nevals max avg
0 20 0.666667 0.416333
1 17 0.693333 0.487
2 15 0.76 0.537333
3 14 0.76 0.550667
4 17 0.76 0.568333
5 17 0.76 0.653667
6 14 0.76 0.589333
7 15 0.76 0.618
8 16 0.866667 0.616667
9 16 0.866667 0.666333
10 16 0.866667 0.722667
- Best solution is: 'hidden_layer_sizes'=(15, 5, 8) , accuracy = 0.8666666666666666</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.455.1">The preceding results indicate that, within the ranges we defined, the best combination that was found was of three hidden layers of size 15, 5, and 8, respectively. </span><span class="kobospan" id="kobo.455.2">The classification accuracy that we achieved with these values is </span><span><span class="kobospan" id="kobo.456.1">about 86.7%.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.457.1">This accuracy seems to be a reasonable</span><a id="_idIndexMarker622" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.458.1"> result for the problem</span><a id="_idIndexMarker623" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.459.1"> at hand. </span><span class="kobospan" id="kobo.459.2">However, there’s more we can do to improve it </span><span><span class="kobospan" id="kobo.460.1">even further.</span></span></p>
<h1 id="_idParaDest-216" class="calibre5"><a id="_idTextAnchor268" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.461.1">Combining architecture optimization with hyperparameter tuning</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.462.1">While optimizing the network</span><a id="_idIndexMarker624" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.463.1"> architecture configuration—the hidden layer parameters—we have been using the default (hyper) parameters of the MLP classifier. </span><span class="kobospan" id="kobo.463.2">However, as we saw in the previous chapter, tuning the various hyperparameters has the potential to increase the classifier’s performance. </span><span class="kobospan" id="kobo.463.3">Can we incorporate hyperparameter tuning into our optimization? </span><span class="kobospan" id="kobo.463.4">As you may have guessed, the answer is yes. </span><span class="kobospan" id="kobo.463.5">But first, let’s take a look at the hyperparameters we would like </span><span><span class="kobospan" id="kobo.464.1">to optimize.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.465.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.466.1">scikit-learn</span></strong><span class="kobospan" id="kobo.467.1"> implementation of the MLP classifier contains numerous tunable hyperparameters. </span><span class="kobospan" id="kobo.467.2">For our demonstration, we will concentrate on the </span><span><span class="kobospan" id="kobo.468.1">following hyperparameters:</span></span></p>
<table class="no-table-style" id="table001-7">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.469.1">Name</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.470.1">Type</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.471.1">Description</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.472.1">Default Value</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.473.1">activation</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.474.1">enumerated</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.475.1">Activation function for the </span><span><span class="kobospan" id="kobo.476.1">hidden layers:</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.477.1">{'identity', 'logistic',  '</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.478.1">tanh', 'relu'}</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.479.1">'</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.480.1">relu'</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.481.1">solver</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.482.1">enumerated</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.483.1">The solver for </span><span><span class="kobospan" id="kobo.484.1">weight optimization:</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.485.1">{'lbfgs', '</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.486.1">sgd', 'adam'}</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.487.1">'</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.488.1">adam'</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.489.1">alpha</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.490.1">float</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.491.1">Strength of the L2 </span><span><span class="kobospan" id="kobo.492.1">regularization term</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.493.1">0.0001</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.494.1">learning_rate</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.495.1">enumerated</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.496.1">Learning rate schedule for </span><span><span class="kobospan" id="kobo.497.1">weight updates:</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.498.1">{‘</span><span><span class="kobospan" id="kobo.499.1">constant’, ‘invscaling’,’adaptive’}</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.500.1">'</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.501.1">constant'</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.502.1">Table 9.1: MLP hyperparameters</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.503.1">As we saw in the previous chapter, a floating point-based chromosome representation allows us to combine various types of hyperparameters into the genetic algorithm-based optimization process. </span><span class="kobospan" id="kobo.503.2">Since we already used a floating-point-based chromosome to represent the configuration of the hidden layers, we can now incorporate other hyperparameters into the optimization</span><a id="_idIndexMarker625" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.504.1"> process by augmenting the chromosome accordingly. </span><span class="kobospan" id="kobo.504.2">Let’s find out how we can </span><span><span class="kobospan" id="kobo.505.1">do this.</span></span></p>
<h2 id="_idParaDest-217" class="calibre7"><a id="_idTextAnchor269" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.506.1">Solution representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.507.1">To the existing four floats, representing</span><a id="_idIndexMarker626" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.508.1"> our network </span><span><span class="kobospan" id="kobo.509.1">architecture configuration—</span></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.510.1">[</span></span><span><span class="kobospan" id="kobo.511.1">n</span></span><span><span class="kobospan" id="kobo.512.1"> </span></span><span><span class="kobospan" id="kobo.513.1">1</span></span><span><span class="kobospan" id="kobo.514.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.515.1">n</span></span><span><span class="kobospan" id="kobo.516.1"> </span></span><span><span class="kobospan" id="kobo.517.1">2</span></span><span><span class="kobospan" id="kobo.518.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.519.1">n</span></span><span><span class="kobospan" id="kobo.520.1"> </span></span><span><span class="kobospan" id="kobo.521.1">3</span></span><span><span class="kobospan" id="kobo.522.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.523.1">n</span></span><span><span class="kobospan" id="kobo.524.1"> </span></span><span><span class="kobospan" id="kobo.525.1">4</span></span><span><span class="kobospan" id="kobo.526.1">]</span></span><span class="kobospan" id="kobo.527.1">—we can add the following </span><span><span class="kobospan" id="kobo.528.1">four hyperparameters:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.529.1">activation</span></strong><span class="kobospan" id="kobo.530.1"> can have one of four values: </span><strong class="source-inline1"><span class="kobospan" id="kobo.531.1">'tanh'</span></strong><span class="kobospan" id="kobo.532.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.533.1">'relu'</span></strong><span class="kobospan" id="kobo.534.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.535.1">'logistic'</span></strong><span class="kobospan" id="kobo.536.1">, or </span><strong class="source-inline1"><span class="kobospan" id="kobo.537.1">'identity'</span></strong><span class="kobospan" id="kobo.538.1">. </span><span class="kobospan" id="kobo.538.2">This can be achieved by representing it as a float number in the range of [0, 3.99]. </span><span class="kobospan" id="kobo.538.3">To transform the float value into one of the aforementioned values, we need to apply the </span><strong class="source-inline1"><span class="kobospan" id="kobo.539.1">floor()</span></strong><span class="kobospan" id="kobo.540.1"> function to it, which will yield either 0, 1, 2, or 3. </span><span class="kobospan" id="kobo.540.2">We then replace a value of 0 with </span><strong class="source-inline1"><span class="kobospan" id="kobo.541.1">'tanh'</span></strong><span class="kobospan" id="kobo.542.1">, a value of 1 with </span><strong class="source-inline1"><span class="kobospan" id="kobo.543.1">'relu'</span></strong><span class="kobospan" id="kobo.544.1">, a value of 2 with </span><strong class="source-inline1"><span class="kobospan" id="kobo.545.1">'logistic'</span></strong><span class="kobospan" id="kobo.546.1">, and a value of 3 </span><span><span class="kobospan" id="kobo.547.1">with </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.548.1">'identity'</span></strong></span><span><span class="kobospan" id="kobo.549.1">.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.550.1">solver</span></strong><span class="kobospan" id="kobo.551.1"> can have one of three values: </span><strong class="source-inline1"><span class="kobospan" id="kobo.552.1">'sgd'</span></strong><span class="kobospan" id="kobo.553.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.554.1">'adam'</span></strong><span class="kobospan" id="kobo.555.1">, or </span><strong class="source-inline1"><span class="kobospan" id="kobo.556.1">'lbfgs'</span></strong><span class="kobospan" id="kobo.557.1">. </span><span class="kobospan" id="kobo.557.2">Just as with the activation parameter, it can be represented using a float number in the range of [</span><span><span class="kobospan" id="kobo.558.1">0, 2.99].</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.559.1">alpha</span></strong><span class="kobospan" id="kobo.560.1"> is already a float, so no conversion is needed. </span><span class="kobospan" id="kobo.560.2">It will be bound to the range of [</span><span><span class="kobospan" id="kobo.561.1">0.0001, 2.0].</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.562.1">learning_rate</span></strong><span class="kobospan" id="kobo.563.1"> can have one of three values: </span><strong class="source-inline1"><span class="kobospan" id="kobo.564.1">'constant'</span></strong><span class="kobospan" id="kobo.565.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.566.1">'invscaling'</span></strong><span class="kobospan" id="kobo.567.1">, or </span><strong class="source-inline1"><span class="kobospan" id="kobo.568.1">'adaptive'</span></strong><span class="kobospan" id="kobo.569.1">. </span><span class="kobospan" id="kobo.569.2">Once again, we can use a float number in the range of [0, 2.99] to represent </span><span><span class="kobospan" id="kobo.570.1">its value.</span></span></li>
</ul>
<h2 id="_idParaDest-218" class="calibre7"><a id="_idTextAnchor270" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.571.1">Evaluating the classifier’s accuracy</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.572.1">The class that will be used to evaluate</span><a id="_idIndexMarker627" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.573.1"> the MLP classifier’s accuracy for the given combination of hidden layers and hyperparameters is called </span><strong class="source-inline"><span class="kobospan" id="kobo.574.1">MlpHyperparametersTest</span></strong><span class="kobospan" id="kobo.575.1"> and is contained in the  </span><strong class="source-inline"><span class="kobospan" id="kobo.576.1">mlp_hyperparameters_test.py</span></strong><span class="kobospan" id="kobo.577.1"> file, which is located at the </span><span><span class="kobospan" id="kobo.578.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.579.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.580.1">This class is based on the one we used to optimize the configuration of the hidden layers, </span><strong class="source-inline"><span class="kobospan" id="kobo.581.1">MlpLayersTest</span></strong><span class="kobospan" id="kobo.582.1">, but with a few modifications. </span><span class="kobospan" id="kobo.582.2">Let’s go </span><span><span class="kobospan" id="kobo.583.1">over these:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.584.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.585.1">convertParam()</span></strong><span class="kobospan" id="kobo.586.1"> method now handles a </span><strong class="source-inline1"><span class="kobospan" id="kobo.587.1">params</span></strong><span class="kobospan" id="kobo.588.1"> list, where the first four entries (</span><strong class="source-inline1"><span class="kobospan" id="kobo.589.1">params[0]</span></strong><span class="kobospan" id="kobo.590.1"> through </span><strong class="source-inline1"><span class="kobospan" id="kobo.591.1">params[3]</span></strong><span class="kobospan" id="kobo.592.1">) represent the sizes of the hidden layers, just as before, but in addition, </span><strong class="source-inline1"><span class="kobospan" id="kobo.593.1">params[4]</span></strong><span class="kobospan" id="kobo.594.1"> through </span><strong class="source-inline1"><span class="kobospan" id="kobo.595.1">params[7]</span></strong><span class="kobospan" id="kobo.596.1"> represent the four hyperparameters we added to the evaluation. </span><span class="kobospan" id="kobo.596.2">Consequently, the method has been augmented with the following lines of code, allowing it to transform the rest of the given parameters (</span><strong class="source-inline1"><span class="kobospan" id="kobo.597.1">params[4]</span></strong><span class="kobospan" id="kobo.598.1"> through </span><strong class="source-inline1"><span class="kobospan" id="kobo.599.1">params[7]</span></strong><span class="kobospan" id="kobo.600.1">) into their corresponding values, which can then be fed to the </span><span><span class="kobospan" id="kobo.601.1">MLP classifier:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.602.1">activation </span></strong><span class="kobospan1" id="kobo.603.1">= ['tanh', 'relu', 'logistic', 'identity'][floor(params[4])]
</span><strong class="bold1"><span class="kobospan1" id="kobo.604.1">solver </span></strong><span class="kobospan1" id="kobo.605.1">= ['sgd', 'adam', 'lbfgs'][floor(params[5])]
</span><strong class="bold1"><span class="kobospan1" id="kobo.606.1">alpha </span></strong><span class="kobospan1" id="kobo.607.1">= params[6]
</span><strong class="bold1"><span class="kobospan1" id="kobo.608.1">learning_rate</span></strong><span class="kobospan1" id="kobo.609.1"> = ['constant', 'invscaling',
    'adaptive'][floor(params[7])]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.610.1">Similarly, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.611.1">getAccuracy()</span></strong><span class="kobospan" id="kobo.612.1"> method now handles the augmented </span><strong class="source-inline1"><span class="kobospan" id="kobo.613.1">params </span></strong><span class="kobospan" id="kobo.614.1">list. </span><span class="kobospan" id="kobo.614.2">It configures the MLP classifier with the converted values of all these parameters rather than just the hidden </span><span><span class="kobospan" id="kobo.615.1">layer’s configuration:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.616.1">
hiddenLayerSizes, activation, solver, alpha, learning_rate = \
    self.convertParams(params)
self.classifier = MLPClassifier(
    random_state=self.randomSeed,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.617.1">hidden_layer_sizes</span></strong><span class="kobospan1" id="kobo.618.1">=hiddenLayerSizes,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.619.1">activation</span></strong><span class="kobospan1" id="kobo.620.1">=activation,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.621.1">solver</span></strong><span class="kobospan1" id="kobo.622.1">=solver,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.623.1">alpha</span></strong><span class="kobospan1" id="kobo.624.1">=alpha,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.625.1">learning_rate</span></strong><span class="kobospan1" id="kobo.626.1">=learning_rate)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.627.1">This </span><strong class="source-inline"><span class="kobospan" id="kobo.628.1">MlpHyperparametersTest</span></strong><span class="kobospan" id="kobo.629.1"> class is utilized</span><a id="_idIndexMarker628" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.630.1"> by the genetic algorithm-based optimizer. </span><span class="kobospan" id="kobo.630.2">We will look at this in the </span><span><span class="kobospan" id="kobo.631.1">next section.</span></span></p>
<h2 id="_idParaDest-219" class="calibre7"><a id="_idTextAnchor271" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.632.1">Optimizing the MLP’s combined configuration using genetic algorithms</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.633.1">The genetic algorithm-based search</span><a id="_idIndexMarker629" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.634.1"> for the best combination</span><a id="_idIndexMarker630" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.635.1"> of hidden layers and hyperparameters is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.636.1">02_ptimize_mlp_hyperparameters.py</span></strong><span class="kobospan" id="kobo.637.1"> Python program, which is located at the </span><span><span class="kobospan" id="kobo.638.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.639.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.640.1">Thanks to the unified floating number representation that’s used for all parameters, this program is almost identical to the one we used in the previous section to optimize the network architecture. </span><span class="kobospan" id="kobo.640.2">The main difference is in the definition of the </span><strong class="source-inline"><span class="kobospan" id="kobo.641.1">BOUNDS_LOW</span></strong><span class="kobospan" id="kobo.642.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.643.1">BOUNDS_HIGH</span></strong><span class="kobospan" id="kobo.644.1"> lists, which contain the ranges of the parameters. </span><span class="kobospan" id="kobo.644.2">To the four ranges we defined previously—one for each hidden layer—we now add another four, representing the additional hyperparameters that we discussed earlier in </span><span><span class="kobospan" id="kobo.645.1">this section:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.646.1">
# '</span><strong class="bold1"><span class="kobospan1" id="kobo.647.1">hidden_layer_sizes</span></strong><span class="kobospan1" id="kobo.648.1">': first four values
# '</span><strong class="bold1"><span class="kobospan1" id="kobo.649.1">activation</span></strong><span class="kobospan1" id="kobo.650.1">'        : 0..3.99
# '</span><strong class="bold1"><span class="kobospan1" id="kobo.651.1">solver</span></strong><span class="kobospan1" id="kobo.652.1">'            : 0..2.99
# '</span><strong class="bold1"><span class="kobospan1" id="kobo.653.1">alpha</span></strong><span class="kobospan1" id="kobo.654.1">'             : 0.0001..2.0
# '</span><strong class="bold1"><span class="kobospan1" id="kobo.655.1">learning_rate</span></strong><span class="kobospan1" id="kobo.656.1">'     : 0..2.99
BOUNDS_LOW =  [ 5,  -5, -10, -20, 0,     0,     0.0001, 0]
BOUNDS_HIGH = [15,  10,  10,  10, 3.999, 2.999, 2.0,    2.999]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.657.1">And that’s all it takes—the program</span><a id="_idIndexMarker631" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.658.1"> is able to handle</span><a id="_idIndexMarker632" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.659.1"> the added parameters without any </span><span><span class="kobospan" id="kobo.660.1">further changes.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.661.1">Running this program produces the </span><span><span class="kobospan" id="kobo.662.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.663.1">
gen     nevals  max     avg
0       20      0.94    0.605667
1       15      0.94    0.667
2       16      0.94    0.848667
3       17      0.94    0.935
4       17      0.94    0.908667
5       15      0.94    0.936
6       15      0.94    0.889667
7       16      0.94    0.938333
8       17      0.946667        0.938333
9       13      0.946667        0.938667
10      15      0.946667        0.940667
- Best solution is:
'hidden_layer_sizes'=(7, 4, 6)
'activation'='tanh'
'solver'='lbfgs'
'alpha'=1.2786182334834102
'learning_rate'='constant'
=&gt; accuracy =  0.9466666666666667</span></pre> <p class="callout-heading"><span class="kobospan" id="kobo.664.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.665.1">Please be aware that, due to variations between operating systems, the results that will be produced when you run this program on your system may be somewhat different from what’s being </span><span><span class="kobospan" id="kobo.666.1">shown here.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.667.1">The preceding results indicate that, within the ranges we defined, the best combination that we found for the hidden layer configuration and hyperparameters was </span><span><span class="kobospan" id="kobo.668.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.669.1">Three hidden layers of size 7, 4, and </span><span><span class="kobospan" id="kobo.670.1">6, respectively.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.671.1">An </span><strong class="source-inline1"><span class="kobospan" id="kobo.672.1">activation</span></strong><span class="kobospan" id="kobo.673.1"> parameter of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.674.1">'tanh'</span></strong><span class="kobospan" id="kobo.675.1"> type—instead of the </span><span><span class="kobospan" id="kobo.676.1">default </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.677.1">'relu'</span></strong></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.678.1">A </span><strong class="source-inline1"><span class="kobospan" id="kobo.679.1">solver</span></strong><span class="kobospan" id="kobo.680.1"> parameter of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.681.1">'lbfgs'</span></strong><span class="kobospan" id="kobo.682.1"> type—rather than the </span><span><span class="kobospan" id="kobo.683.1">default </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.684.1">'adam'</span></strong></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.685.1">An </span><strong class="source-inline1"><span class="kobospan" id="kobo.686.1">alpha</span></strong><span class="kobospan" id="kobo.687.1"> value of about </span><strong class="source-inline1"><span class="kobospan" id="kobo.688.1">1.279</span></strong><span class="kobospan" id="kobo.689.1"> – considerably larger than the default value </span><span><span class="kobospan" id="kobo.690.1">of 0.0001</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.691.1">A </span><strong class="source-inline1"><span class="kobospan" id="kobo.692.1">learning_rate</span></strong><span class="kobospan" id="kobo.693.1"> parameter of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.694.1">'constant'</span></strong><span class="kobospan" id="kobo.695.1"> type—the same as the </span><span><span class="kobospan" id="kobo.696.1">default value</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.697.1">This combined optimization</span><a id="_idIndexMarker633" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.698.1"> resulted in a classification </span><a id="_idIndexMarker634" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.699.1">accuracy of about 94.7%—a significant improvement over the previous results, all while using fewer nodes </span><span><span class="kobospan" id="kobo.700.1">than before.</span></span></p>
<h1 id="_idParaDest-220" class="calibre5"><a id="_idTextAnchor272" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.701.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.702.1">In this chapter, you were introduced to the basic concepts of ANNs and DL. </span><span class="kobospan" id="kobo.702.2">After getting acquainted with the Iris dataset and the MLP classifier, you were presented with the notion of network architecture optimization. </span><span class="kobospan" id="kobo.702.3">Next, we demonstrated a genetic algorithm-based optimization of network architecture for the MLP classifier. </span><span class="kobospan" id="kobo.702.4">Finally, we were able to combine network architecture optimization with model hyperparameter tuning using the same genetic algorithms approach, thereby enhancing the performance of the classifier </span><span><span class="kobospan" id="kobo.703.1">even further.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.704.1">So far, we have concentrated on </span><strong class="bold"><span class="kobospan" id="kobo.705.1">supervised learning</span></strong><span class="kobospan" id="kobo.706.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.707.1">SL</span></strong><span class="kobospan" id="kobo.708.1">). </span><span class="kobospan" id="kobo.708.2">In the next chapter, we will look into applying genetic algorithms to </span><strong class="bold"><span class="kobospan" id="kobo.709.1">reinforcement learning</span></strong><span class="kobospan" id="kobo.710.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.711.1">RL</span></strong><span class="kobospan" id="kobo.712.1">), an exciting and fast-developing branch </span><span><span class="kobospan" id="kobo.713.1">of ML.</span></span></p>
<h1 id="_idParaDest-221" class="calibre5"><a id="_idTextAnchor273" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.714.1">Further reading</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.715.1">For more information on the topics that we covered in this chapter, please refer to the </span><span><span class="kobospan" id="kobo.716.1">following resources:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.717.1">Python Deep Learning—Second Edition</span></em><span class="kobospan" id="kobo.718.1">, </span><em class="italic"><span class="kobospan" id="kobo.719.1">Gianmario Spacagna, Daniel Slater, et al.</span></em><span class="kobospan" id="kobo.720.1">, </span><em class="italic"><span class="kobospan" id="kobo.721.1">January </span></em><span><em class="italic"><span class="kobospan" id="kobo.722.1">16, 2019</span></em></span></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.723.1">Neural Network Projects with Python,</span></em> <em class="italic"><span class="kobospan" id="kobo.724.1">James Loy</span></em><span class="kobospan" id="kobo.725.1">, </span><em class="italic"><span class="kobospan" id="kobo.726.1">February </span></em><span><em class="italic"><span class="kobospan" id="kobo.727.1">28, 2019</span></em></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.728.1">scikit-learn</span></strong> <span><span class="kobospan" id="kobo.729.1">MLP classifier:</span></span></li>
<li class="calibre11"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.730.1">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</span></span></a></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.731.1">UCI Machine Learning </span></em><span><em class="italic"><span class="kobospan" id="kobo.732.1">Repository</span></em></span><span><span class="kobospan" id="kobo.733.1">: </span></span><a href="https://archive.ics.uci.edu/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.734.1">https://archive.ics.uci.edu/</span></span></a></li>
</ul>
</div>
</body></html>