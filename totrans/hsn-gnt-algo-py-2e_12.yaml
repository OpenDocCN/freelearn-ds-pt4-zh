- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecture Optimization of Deep Learning Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes how genetic algorithms can be used to improve the performance
    of **artificial neural network** (**ANN**)-based models by optimizing the **network
    architecture** of these models. We will start with a brief introduction to **neural
    networks** (**NNs**) and **deep learning** (**DL**). After introducing the *Iris
    dataset* and **Multilayer Perceptron** (**MLP**) classifiers, we will demonstrate
    **network architecture optimization** using a genetic algorithm-based solution.
    Then, we will extend this approach to combine network architecture optimization
    with model **hyperparameter tuning**, which will be jointly carried out by a genetic
    algorithm-based solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic concepts of ANNs and DL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing the performance of a DL classifier using network architecture optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further enhancing the performance of the DL classifier by combining network
    architecture optimization with hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start this chapter with an overview of ANNs. If you are a seasoned data
    scientist, feel free to skip the introductory sections.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**deap**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**numpy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will be using the UCI Iris flower dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The programs that will be used in this chapter can be found in this book’s
    GitHub repository at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_09)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action: [https://packt.link/OEBOd](https://packt.link/OEBOd)'
  prefs: []
  type: TYPE_NORMAL
- en: ANNs and DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inspired by the structure of the human brain, NNs are among the most commonly
    used models in **machine learning** (**ML**). The basic building blocks of these
    networks are nodes, or **neurons**, which are based on the biological neuron cell,
    as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Biological neuron model ](img/B20851_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Biological neuron model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg](https://simple.wikipedia.org/wiki/Neuron#/media/File:Neuron.svg)
    by Dhp1080'
  prefs: []
  type: TYPE_NORMAL
- en: The neuron cell’s **dendrites**, which surround the **cell body** on the left-hand
    side of the preceding diagram, are used as inputs from multiple similar cells,
    while the long **axon**, coming out of the **cell body**, serves as output and
    can be connected to multiple other cells via its **terminals**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This structure is mimicked by an artificial model called a **perceptron**,
    illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Artificial neuron model – the perceptron](img/B20851_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Artificial neuron model – the perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron calculates the output by multiplying each of the input values
    by a certain **weight**; the results are accumulated, and a **bias** value is
    added to the sum. A non-linear **activation function** then maps the result to
    the output. This functionality emulates the operation of the biological neuron,
    which fires (sends a series of pulses from its output) when the weighted sum of
    the inputs is above a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron model can be used for simple classification and regression tasks
    if we adjust its weight and bias values so that they map certain inputs to the
    desired output levels. However, a much more capable model can be constructed when
    connecting multiple perceptron units in a structure called an MLP, which will
    be described in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: MLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An MLP extends the idea of the perceptron by using numerous nodes, each one
    implementing a perceptron. The nodes in an MLP are arranged in **layers**, and
    each layer is connected to the next. The basic structure of an MLP is illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: The basic structure of an MLP](img/B20851_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: The basic structure of an MLP'
  prefs: []
  type: TYPE_NORMAL
- en: 'An MLP consists of three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: Receives the input values and connects each of them to every
    neuron in the next layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: Delivers the results calculated by the MLP. When the MLP
    is used as a **classifier**, each of the outputs represents one of the classes.
    When the MLP is used for **regression**, there will be a single output node, producing
    a continuous value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer(s)**: Provide the true power and complexity of this model. While
    the preceding diagram shows only two hidden layers, there can be numerous hidden
    layers, each an arbitrary size, that are placed between the input and output layers.
    As the number of hidden layers grows, the network becomes deeper and is capable
    of performing an increasingly more complex and non-linear mapping between the
    inputs and the outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training this model involves adjusting the weight and bias values for each of
    the nodes. This is typically done using a family of algorithms called **backpropagation**.
    The basic principle of backpropagation is to minimize the error between the actual
    outputs and the desired ones by propagating the output error through the layers
    of the MLP model, from the output layer inward. The process begins by defining
    a cost (or “loss”) function, typically a measure of the difference between the
    predicted outputs and the actual target values. The weights and biases of the
    various nodes are adjusted so that those that contributed the most to the error
    see the greatest adjustments. By iteratively reducing the cost function, the algorithm
    refines the model parameters to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: For many years, the computational limitations of backpropagation algorithms
    restricted MLPs to no more than two or three hidden layers, until new developments
    changed matters dramatically. These will be explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: DL and convolutional NNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, backpropagation algorithms have made a leap forward, enabling
    the use of a large number of hidden layers in a single network. In these **deep
    NNs** (**DNNs**), each layer can interpret a combination of several simpler abstract
    concepts that were learned by the nodes of the previous layer and produce higher-level
    concepts. For example, when implementing a face recognition task, the first layer
    will process the pixels of an image and learn to detect edges in different orientations.
    The next layer may assemble these into lines, corners, and so on, up to a layer
    that detects facial features such as nose and lips, and finally, one that combines
    these into the complete concept of a face.
  prefs: []
  type: TYPE_NORMAL
- en: Further advancements have brought about the idea of **convolutional NNs** (**CNNs**).
    These structures can reduce the count of nodes in DNNs that process two-dimensional
    information (such as images) by treating nearby inputs differently compared to
    inputs that are far apart. As a result, these models have proved especially successful
    when it comes to image and video processing tasks. Besides fully connected layers,
    similar to the hidden layers in the MLP, these networks utilize pooling (down-sampling)
    layers, which aggregate outputs of neurons from preceding layers, and convolutional
    layers, which are used for detecting specific features, such as edges in various
    orientations, by effectively sliding a filter over the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Training such `scikit-learn` library and a simple dataset. The principles that
    will be used, however, still apply to more complex networks and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will find out how the architecture of an MLP can be
    optimized using a genetic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the architecture of a DL classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating a NN model for a given ML task, one crucial design decision that
    needs to be made is the configuration of the **network architecture**. In the
    case of an MLP, the number of nodes in the input and output layers is determined
    by the characteristics of the problem at hand. Therefore, the choices to be made
    are about the hidden layers—how many layers, and how many nodes are in each layer.
    Some rules of thumb can be employed for making these decisions, but in many cases,
    identifying the best choices can turn into a cumbersome trial-and-error process.
  prefs: []
  type: TYPE_NORMAL
- en: One way to handle network architecture parameters is to consider them as hyperparameters
    of the model since they need to be determined before training is done and, consequently,
    affect the training’s results. In this section, we are going to apply this approach
    and use a genetic algorithm to search for the best combination of hidden layers,
    in a similar manner to the way we went about choosing the best hyperparameter
    values in the previous chapter. Let’s start with the task we want to tackle –
    the **Iris** **flower classification**.
  prefs: []
  type: TYPE_NORMAL
- en: The Iris flower dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most well-studied dataset, the *Iris flower dataset* ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris))
    contains measurements of the `sepal`and `petal` parts of three Iris species (Iris
    setosa, Iris virginica, and Iris versicolor), as taken by biologists in 1936.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset contains 50 samples from each of the three species, and consists
    of the following four features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sepal_length (cm)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sepal_width (cm)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**petal_length (cm)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**petal_width (cm)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This dataset is directly available via the `scikit-learn` library and can be
    initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In our experiments, we will be using an MLP classifier in conjunction with this
    dataset and harness the power of genetic algorithms to find the network architecture—the
    number of hidden layers and the number of nodes in each layer—that will yield
    the best classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using the genetic algorithms approach, the first thing we need
    to do is find a way to represent this architecture using a chromosome, as described
    in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Representing the hidden layer configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the architecture of an MLP is determined by the hidden layer configuration,
    let’s explore how this configuration can be represented in our solution. The hidden
    layer configuration of the `sklearn` MLP ([https://scikit-learn.org/stable/modules/neural_networks_supervised.html](https://scikit-learn.org/stable/modules/neural_networks_supervised.html))
    model is conveyed via the `hidden_layer_sizes` tuple, which is sent as a parameter
    to the model’s constructor. By default, the value of this tuple is `(100,)`, which
    means a single hidden layer of 100 nodes. If we wanted, for example, to configure
    the MLP with three hidden layers of 20 nodes each, this parameter’s value would
    be `(20, 20, 20)`. Before we implement our genetic algorithm-based optimizer for
    the hidden layer configuration, we need to define a chromosome that can be translated
    into this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, we need to come up with a chromosome that can both express
    the number of layers and the number of nodes in each layer. A variable-length
    chromosome that can be directly translated into the variable-length tuple that’s
    used as the model’s `hidden_layer_sizes` parameter is one option; however, this
    approach would require custom, possibly cumbersome, genetic operators. To be able
    to use our standard genetic operators, we will use a fixed-length representation.
    When using this approach, the maximum number of layers is decided in advance,
    and all the layers are always represented, but not necessarily expressed in the
    solution. For example, if we decide to limit the network to four hidden layers,
    the chromosome will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[n 1, n 2, n 3, n 4]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, n i denotes the number of nodes in the layer i . However, to control
    the actual number of hidden layers in the network, some of these values may be
    zero, or negative. Such a value means that no more layers will be added to the
    network. The following examples illustrate this method:'
  prefs: []
  type: TYPE_NORMAL
- en: The chromosome [10, 20, -5, 15] is translated into the tuple (10, 20) since
    -5 terminates the layer count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chromosome [10, 0, -5, 15] is translated into the tuple (10, ) since 0 terminates
    the layer count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chromosome [10, 20, 5, -15] is translated into the tuple (10, 20, 5) since
    -15 terminates the layer count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chromosome [10, 20, 5, 15] is translated into the tuple (10, 20, 5, 15)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To guarantee that there is at least one hidden layer, we can make sure that
    the first parameter is always greater than zero. The other parameters can have
    varying distributions around zero so that we can control their chances of being
    the terminating parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, even though this chromosome is made up of integers, we chose to
    utilize float numbers instead, just like we did in the previous chapter for various
    types of variables. Using a list of float numbers is convenient as it allows us
    to use existing genetic operators while being able to easily extend the chromosome
    so that it includes other parameters of different types, which we will do later
    on. The float numbers can be translated back into integers using the `round()`
    function. A couple of examples of this generalized approach are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The chromosome [9.35, 10.71, -2.51, 17.99] is translated into the tuple (9,
    11)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chromosome [9.35, 10.71, 2.51, -17.99] is translated into the tuple (9,
    11, 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To evaluate a given architecture-representing chromosome, we will need to translate
    it back into the tuple of layers, create an MLP classifier implementing these
    layers, train it, and evaluate it. We will learn how to do this in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the classifier’s accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with a Python class that encapsulates the MLP classifier’s accuracy
    evaluation for the Iris dataset. The class is called `MlpLayersTest` and can be
    found in the `mlp_layers_test.py` file, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_layers_test.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main functionality of this class is highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **convertParam()** method of the class takes a list called **params**.
    This is actually the chromosome that we described in the previous subsection and
    contains the float values that represent up to four hidden layers. The method
    transforms this list of floats into the **hidden_layer_sizes** tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **getAccuracy()** method takes the **params** list representing the configuration
    of the hidden layers, uses the **convertParam()** method to transform it into
    a **hidden_layer_sizes** tuple, and initializes an MLP classifier with this tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, it finds the accuracy of the classifier using the same *k-fold cross-validation*
    calculation that we created for the *Wine dataset* in [*Chapter 8*](B20851_08.xhtml#_idTextAnchor238),
    *Hyperparameter Tuning of Machine* *Learning Models*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `MlpLayersTest` class is utilized by the genetic algorithm-based optimizer.
    We will explain this part in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the MLP architecture using genetic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a way to represent the architecture configuration of the MLP
    that’s used to classify the Iris flower dataset and a way to determine the accuracy
    of the MLP for each configuration, we can move on and create a genetic algorithm-based
    optimizer to search for the configuration – the number of hidden layers (up to
    4, in our case) and the number of nodes in each layer – that will yield the best
    accuracy. This solution is implemented by the `01_optimize_mlp_layers.py` Python
    program, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/01_optimize_mlp_layers.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe the main parts of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by setting the lower and upper boundary for each of the float values
    representing a hidden layer. The first hidden layer is given the range [5, 15],
    while the rest of the layers start from increasingly larger negative values, which
    increases their chances of terminating the layer count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create an instance of the **MlpLayersTest** class, which will allow
    us to test the various combinations of the hidden layers’ architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since our goal is to maximize the accuracy of the classifier, we define a single
    objective, maximizing fitness strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we employ the same approach we used in the previous chapter—since the
    solution is represented by a list of float values, each of a different range,
    we use the following loop to iterate over all pairs of lower-bound, upper-bound
    values, and for each range, we create a separate **toolbox** operator, **layer_size_attribute**,
    that will later be used to generate random float values in the appropriate range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a **layer_size_attributes** tuple, which contains the separate
    float number generators we just created for each hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use this **layer_size_attributes** tuple in conjunction with DEAP’s
    built-in **initCycle()** operator to create a new **individualCreator** operator
    that fills up an individual instance with a combination of randomly generated
    hidden layer-size values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we instruct the genetic algorithm to use the **getAccuracy()** method
    of the **MlpLayersTest** instance for fitness evaluation. As a reminder, the **getAccuracy()**
    method, which we described in the previous subsection, converts the given individual—a
    list of four floats—into a tuple of hidden layer sizes. These are used to configure
    the MLP classifier. Then, we train the classifier and evaluate its accuracy using
    k-fold cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As for the genetic operators, we repeat the configuration from the previous
    chapter. While for the *selection* operator, we use the usual tournament selection
    with a tournament size of 2, we choose *crossover* and *mutation* operators that
    are specialized for bounded float-list chromosomes and provide them with the boundaries
    we defined for each hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition, we continue to use the *elitist* approach, where the **hall-of-fame**
    (**HOF**) members—the current best individuals—are always passed untouched to
    the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When running the algorithm for 10 generations with a population size of 20,
    we get the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding results indicate that, within the ranges we defined, the best
    combination that was found was of three hidden layers of size 15, 5, and 8, respectively.
    The classification accuracy that we achieved with these values is about 86.7%.
  prefs: []
  type: TYPE_NORMAL
- en: This accuracy seems to be a reasonable result for the problem at hand. However,
    there’s more we can do to improve it even further.
  prefs: []
  type: TYPE_NORMAL
- en: Combining architecture optimization with hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While optimizing the network architecture configuration—the hidden layer parameters—we
    have been using the default (hyper) parameters of the MLP classifier. However,
    as we saw in the previous chapter, tuning the various hyperparameters has the
    potential to increase the classifier’s performance. Can we incorporate hyperparameter
    tuning into our optimization? As you may have guessed, the answer is yes. But
    first, let’s take a look at the hyperparameters we would like to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` implementation of the MLP classifier contains numerous tunable
    hyperparameters. For our demonstration, we will concentrate on the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Type** | **Description** | **Default Value** |'
  prefs: []
  type: TYPE_TB
- en: '| activation | enumerated | Activation function for the hidden layers:`{''identity'',
    ''logistic'', ''``tanh'', ''relu''}` | `''``relu''` |'
  prefs: []
  type: TYPE_TB
- en: '| solver | enumerated | The solver for weight optimization:`{''lbfgs'', ''``sgd'',
    ''adam''}` | `''``adam''` |'
  prefs: []
  type: TYPE_TB
- en: '| alpha | float | Strength of the L2 regularization term | `0.0001` |'
  prefs: []
  type: TYPE_TB
- en: '| learning_rate | enumerated | Learning rate schedule for weight updates:{‘constant’,
    ‘invscaling’,’adaptive’} | `''``constant''` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.1: MLP hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, a floating point-based chromosome representation
    allows us to combine various types of hyperparameters into the genetic algorithm-based
    optimization process. Since we already used a floating-point-based chromosome
    to represent the configuration of the hidden layers, we can now incorporate other
    hyperparameters into the optimization process by augmenting the chromosome accordingly.
    Let’s find out how we can do this.
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To the existing four floats, representing our network architecture configuration—
  prefs: []
  type: TYPE_NORMAL
- en: '[n 1, n 2, n 3, n 4]—we can add the following four hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**activation** can have one of four values: **''tanh''**, **''relu''**, **''logistic''**,
    or **''identity''**. This can be achieved by representing it as a float number
    in the range of [0, 3.99]. To transform the float value into one of the aforementioned
    values, we need to apply the **floor()** function to it, which will yield either
    0, 1, 2, or 3\. We then replace a value of 0 with **''tanh''**, a value of 1 with
    **''relu''**, a value of 2 with **''logistic''**, and a value of 3 with **''identity''**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**solver** can have one of three values: **''sgd''**, **''adam''**, or **''lbfgs''**.
    Just as with the activation parameter, it can be represented using a float number
    in the range of [0, 2.99].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alpha** is already a float, so no conversion is needed. It will be bound
    to the range of [0.0001, 2.0].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**learning_rate** can have one of three values: **''constant''**, **''invscaling''**,
    or **''adaptive''**. Once again, we can use a float number in the range of [0,
    2.99] to represent its value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the classifier’s accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The class that will be used to evaluate the MLP classifier’s accuracy for the
    given combination of hidden layers and hyperparameters is called `MlpHyperparametersTest`
    and is contained in the `mlp_hyperparameters_test.py` file, which is located at
    the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/mlp_hyperparameters_test.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This class is based on the one we used to optimize the configuration of the
    hidden layers, `MlpLayersTest`, but with a few modifications. Let’s go over these:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **convertParam()** method now handles a **params** list, where the first
    four entries (**params[0]** through **params[3]**) represent the sizes of the
    hidden layers, just as before, but in addition, **params[4]** through **params[7]**
    represent the four hyperparameters we added to the evaluation. Consequently, the
    method has been augmented with the following lines of code, allowing it to transform
    the rest of the given parameters (**params[4]** through **params[7]**) into their
    corresponding values, which can then be fed to the MLP classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, the **getAccuracy()** method now handles the augmented **params**
    list. It configures the MLP classifier with the converted values of all these
    parameters rather than just the hidden layer’s configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This `MlpHyperparametersTest` class is utilized by the genetic algorithm-based
    optimizer. We will look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the MLP’s combined configuration using genetic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The genetic algorithm-based search for the best combination of hidden layers
    and hyperparameters is implemented by the `02_ptimize_mlp_hyperparameters.py`
    Python program, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_09/02_optimize_mlp_hyperparameters.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the unified floating number representation that’s used for all parameters,
    this program is almost identical to the one we used in the previous section to
    optimize the network architecture. The main difference is in the definition of
    the `BOUNDS_LOW` and `BOUNDS_HIGH` lists, which contain the ranges of the parameters.
    To the four ranges we defined previously—one for each hidden layer—we now add
    another four, representing the additional hyperparameters that we discussed earlier
    in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: And that’s all it takes—the program is able to handle the added parameters without
    any further changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this program produces the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that, due to variations between operating systems, the results
    that will be produced when you run this program on your system may be somewhat
    different from what’s being shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding results indicate that, within the ranges we defined, the best
    combination that we found for the hidden layer configuration and hyperparameters
    was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Three hidden layers of size 7, 4, and 6, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **activation** parameter of the **'tanh'** type—instead of the default **'relu'**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **solver** parameter of the **'lbfgs'** type—rather than the default **'adam'**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **alpha** value of about **1.279** – considerably larger than the default
    value of 0.0001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **learning_rate** parameter of the **'constant'** type—the same as the default
    value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This combined optimization resulted in a classification accuracy of about 94.7%—a
    significant improvement over the previous results, all while using fewer nodes
    than before.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to the basic concepts of ANNs and DL. After
    getting acquainted with the Iris dataset and the MLP classifier, you were presented
    with the notion of network architecture optimization. Next, we demonstrated a
    genetic algorithm-based optimization of network architecture for the MLP classifier.
    Finally, we were able to combine network architecture optimization with model
    hyperparameter tuning using the same genetic algorithms approach, thereby enhancing
    the performance of the classifier even further.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have concentrated on **supervised learning** (**SL**). In the next
    chapter, we will look into applying genetic algorithms to **reinforcement learning**
    (**RL**), an exciting and fast-developing branch of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that we covered in this chapter, please
    refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python Deep Learning—Second Edition*, *Gianmario Spacagna, Daniel Slater,
    et al.*, *January* *16, 2019*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Network Projects with Python,* *James Loy*, *February* *28, 2019*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn** MLP classifier:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*UCI Machine Learning* *Repository*: [https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
