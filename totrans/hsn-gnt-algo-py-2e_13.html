<html><head></head><body>
<div id="_idContainer131" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-222"><a id="_idTextAnchor274" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-223" class="calibre5"><a id="_idTextAnchor275" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Reinforcement Learning with Genetic Algorithms</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will demonstrate how genetic algorithms can be applied to </span><strong class="bold"><span class="kobospan" id="kobo.4.1">reinforcement learning</span></strong><span class="kobospan" id="kobo.5.1"> – a fast-developing branch of machine learning that is capable of </span><a id="_idIndexMarker635" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.6.1">tackling complex tasks. </span><span class="kobospan" id="kobo.6.2">We will do this by solving two benchmark environments from the </span><em class="italic"><span class="kobospan" id="kobo.7.1">Gymnasium</span></em><span class="kobospan" id="kobo.8.1"> (formerly </span><em class="italic"><span class="kobospan" id="kobo.9.1">OpenAI Gym</span></em><span class="kobospan" id="kobo.10.1">) toolkit. </span><span class="kobospan" id="kobo.10.2">We will start by providing an overview of reinforcement learning, followed by a brief introduction to </span><em class="italic"><span class="kobospan" id="kobo.11.1">Gymnasium</span></em><span class="kobospan" id="kobo.12.1">, a toolkit that can be used to compare and develop reinforcement learning algorithms, as well as a description of its Python-based interface. </span><span class="kobospan" id="kobo.12.2">Then, we will explore two Gymnasium environments, </span><em class="italic"><span class="kobospan" id="kobo.13.1">MountainCar</span></em><span class="kobospan" id="kobo.14.1"> and </span><em class="italic"><span class="kobospan" id="kobo.15.1">CartPole</span></em><span class="kobospan" id="kobo.16.1">, and develop genetic algorithm-based programs to solve the challenges </span><span><span class="kobospan" id="kobo.17.1">they present.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.18.1">In this chapter, we will cover the </span><span><span class="kobospan" id="kobo.19.1">following topics:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.20.1">Understanding the basic concepts of </span><span><span class="kobospan" id="kobo.21.1">reinforcement learning</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.22.1">Becoming familiar with the </span><em class="italic"><span class="kobospan" id="kobo.23.1">Gymnasium</span></em><span class="kobospan" id="kobo.24.1"> project and its </span><span><span class="kobospan" id="kobo.25.1">shared interface</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.26.1">Using genetic algorithms to solve the </span><em class="italic"><span class="kobospan" id="kobo.27.1">Gymnasium</span></em> <span><em class="italic"><span class="kobospan" id="kobo.28.1">MountainCar</span></em></span><span><span class="kobospan" id="kobo.29.1"> environment</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.30.1">Using genetic algorithms, in combination with a neural network, to solve the </span><em class="italic"><span class="kobospan" id="kobo.31.1">Gymnasium</span></em> <span><em class="italic"><span class="kobospan" id="kobo.32.1">CartPole</span></em></span><span><span class="kobospan" id="kobo.33.1"> environment</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.34.1">We will start this chapter by outlining the basic concepts of reinforcement learning. </span><span class="kobospan" id="kobo.34.2">If you are a seasoned data scientist, feel free to skip this </span><span><span class="kobospan" id="kobo.35.1">introductory section.</span></span></p>
<h1 id="_idParaDest-224" class="calibre5"><a id="_idTextAnchor276" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.36.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.37.1">In this chapter, we will use Python 3 with the following </span><span><span class="kobospan" id="kobo.38.1">supporting libraries:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.39.1">deap</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.40.1">numpy</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.41.1">scikit-learn</span></strong></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.42.1">gymnasium</span></strong><span class="kobospan" id="kobo.43.1"> – </span><em class="italic"><span class="kobospan" id="kobo.44.1">introduced in </span></em><span><em class="italic"><span class="kobospan" id="kobo.45.1">this chapter</span></em></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.46.1">pygame</span></strong><span class="kobospan" id="kobo.47.1"> – </span><em class="italic"><span class="kobospan" id="kobo.48.1">introduced in </span></em><span><em class="italic"><span class="kobospan" id="kobo.49.1">this chapter</span></em></span></li>
</ul>
<p class="callout-heading"><span class="kobospan" id="kobo.50.1">Important Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.51.1">If you use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.52.1">requirements.txt</span></strong><span class="kobospan" id="kobo.53.1"> file we provide (See </span><a href="B20851_03.xhtml#_idTextAnchor091" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.54.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.55.1">), these libraries are already included in </span><span><span class="kobospan" id="kobo.56.1">your environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.57.1">The </span><em class="italic"><span class="kobospan" id="kobo.58.1">Gymnasium</span></em><span class="kobospan" id="kobo.59.1"> environments that will be used in this chapter are </span><em class="italic"><span class="kobospan" id="kobo.60.1">MountainCar-v0</span></em><span class="kobospan" id="kobo.61.1"> (</span><a href="https://gymnasium.farama.org/environments/classic_control/mountain_car/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.62.1">https://gymnasium.farama.org/environments/classic_control/mountain_car/</span></a><span class="kobospan" id="kobo.63.1">) and </span><span><em class="italic"><span class="kobospan" id="kobo.64.1">CartPole-v1</span></em></span><span><span class="kobospan" id="kobo.65.1"> (</span></span><a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.66.1">https://gymnasium.farama.org/environments/classic_control/cart_pole/</span></span></a><span><span class="kobospan" id="kobo.67.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.68.1">The programs that will be used in this chapter can be found in this book’s GitHub repository </span><span><span class="kobospan" id="kobo.69.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.70.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10</span></span></a><span><span class="kobospan" id="kobo.71.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.72.1">Check out the following video to see the code in </span><span><span class="kobospan" id="kobo.73.1">action: </span></span><a href="https://packt.link/OEBOd" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.74.1">https://packt.link/OEBOd</span></span></a><span><span class="kobospan" id="kobo.75.1">.</span></span></p>
<h1 id="_idParaDest-225" class="calibre5"><a id="_idTextAnchor277" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.76.1">Reinforcement learning</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.77.1">In the previous chapters, we covered several topics related to machine learning and focused on </span><strong class="bold"><span class="kobospan" id="kobo.78.1">supervised learning</span></strong><span class="kobospan" id="kobo.79.1"> tasks. </span><span class="kobospan" id="kobo.79.2">While supervised learning is immensely important </span><a id="_idIndexMarker636" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.80.1">and has a lot of real-life applications, reinforcement learning currently seems to be the most exciting and promising branch of machine learning. </span><span class="kobospan" id="kobo.80.2">The </span><a id="_idIndexMarker637" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.81.1">reasons for this excitement include the complex, everyday-life-like tasks that reinforcement learning has the potential to handle. </span><span class="kobospan" id="kobo.81.2">In March 2016, </span><em class="italic"><span class="kobospan" id="kobo.82.1">AlphaGo</span></em><span class="kobospan" id="kobo.83.1">, a reinforcement learning-based system specializing in the highly complex game of </span><em class="italic"><span class="kobospan" id="kobo.84.1">Go</span></em><span class="kobospan" id="kobo.85.1">, was able to defeat the person considered to be the greatest Go player of the past decade in a competition that was widely covered by </span><span><span class="kobospan" id="kobo.86.1">the media.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.87.1">While supervised </span><a id="_idIndexMarker638" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.88.1">learning requires </span><strong class="bold"><span class="kobospan" id="kobo.89.1">labeled data</span></strong><span class="kobospan" id="kobo.90.1"> for training – in other words, pairs of inputs and matching outputs – reinforcement learning does not present immediate right/wrong feedback; instead, it provides an environment where a longer-term, cumulative </span><strong class="bold"><span class="kobospan" id="kobo.91.1">reward</span></strong><span class="kobospan" id="kobo.92.1"> is sought after. </span><span class="kobospan" id="kobo.92.2">This means that, sometimes, an algorithm will need to take a momentary step backward to eventually reach a longer-term goal, as we will demonstrate in the first example of </span><span><span class="kobospan" id="kobo.93.1">this chapter.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">The two main </span><a id="_idIndexMarker639" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.95.1">components of reinforcement learning task are the </span><strong class="bold"><span class="kobospan" id="kobo.96.1">environment</span></strong><span class="kobospan" id="kobo.97.1"> and the </span><strong class="bold"><span class="kobospan" id="kobo.98.1">agent</span></strong><span class="kobospan" id="kobo.99.1">, as illustrated in the </span><span><span class="kobospan" id="kobo.100.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer124">
<span class="kobospan" id="kobo.101.1"><img alt="Figure 10.1: Reinforcement learning represented as an interaction between the agent and the environment" src="image/B20851_10_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.102.1">Figure 10.1: Reinforcement learning represented as an interaction between the agent and the environment</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.103.1">The </span><em class="italic"><span class="kobospan" id="kobo.104.1">agent</span></em><span class="kobospan" id="kobo.105.1"> represents an algorithm that interacts with the </span><em class="italic"><span class="kobospan" id="kobo.106.1">environment</span></em><span class="kobospan" id="kobo.107.1"> and attempts to solve a given problem by maximizing the </span><span><span class="kobospan" id="kobo.108.1">cumulative reward.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.109.1">The exchange that takes place between the agent and the environment can be expressed as a series of </span><strong class="bold"><span class="kobospan" id="kobo.110.1">steps</span></strong><span class="kobospan" id="kobo.111.1">. </span><span class="kobospan" id="kobo.111.2">In each step, the environment presents the agent with a certain </span><strong class="bold"><span class="kobospan" id="kobo.112.1">state</span></strong><span class="kobospan" id="kobo.113.1"> (</span><em class="italic"><span class="kobospan" id="kobo.114.1">s</span></em><span class="kobospan" id="kobo.115.1">), also called an observation. </span><span class="kobospan" id="kobo.115.2">The agent, in turn, performs an </span><strong class="bold"><span class="kobospan" id="kobo.116.1">action</span></strong><span class="kobospan" id="kobo.117.1"> (</span><em class="italic"><span class="kobospan" id="kobo.118.1">a</span></em><span class="kobospan" id="kobo.119.1">). </span><span class="kobospan" id="kobo.119.2">The environment responds with a new state (</span><em class="italic"><span class="kobospan" id="kobo.120.1">s’</span></em><span class="kobospan" id="kobo.121.1">), as well as an intermediate reward value (</span><em class="italic"><span class="kobospan" id="kobo.122.1">R</span></em><span class="kobospan" id="kobo.123.1">). </span><span class="kobospan" id="kobo.123.2">This exchange repeats until a certain stopping condition is met. </span><span class="kobospan" id="kobo.123.3">The agent’s goal is to maximize the sum of the reward values that are collected along </span><span><span class="kobospan" id="kobo.124.1">the way.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.125.1">Despite the simplicity of this formulation, it can be used to describe extremely complex tasks and situations, which makes reinforcement learning applicable to a wide range of applications, such as game theory, healthcare, control systems, supply-chain automation, and </span><span><span class="kobospan" id="kobo.126.1">operations research.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.127.1">The versatility of genetic algorithms will be demonstrated once more in this chapter, since we will harness them to assist with reinforcement </span><span><span class="kobospan" id="kobo.128.1">learning tasks.</span></span></p>
<h2 id="_idParaDest-226" class="calibre7"><a id="_idTextAnchor278" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.129.1">Genetic algorithms and reinforcement learning</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.130.1">Various dedicated algorithms have been developed to carry out reinforcement learning tasks – </span><em class="italic"><span class="kobospan" id="kobo.131.1">Q-Learning</span></em><span class="kobospan" id="kobo.132.1">, </span><em class="italic"><span class="kobospan" id="kobo.133.1">SARSA</span></em><span class="kobospan" id="kobo.134.1">, and </span><em class="italic"><span class="kobospan" id="kobo.135.1">DQN</span></em><span class="kobospan" id="kobo.136.1">, to name a few. </span><span class="kobospan" id="kobo.136.2">However, since reinforcement learning tasks involve maximizing a long-term reward, we can think of them as optimization problems. </span><span class="kobospan" id="kobo.136.3">As we have seen throughout this book, genetic algorithms can be used to </span><a id="_idIndexMarker640" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.137.1">solve optimization problems of various types. </span><span class="kobospan" id="kobo.137.2">Therefore, genetic algorithms can be utilized for reinforcement learning as well, and in several different ways – two of them will be demonstrated in this chapter. </span><span class="kobospan" id="kobo.137.3">In the first case, our genetic algorithm-based solution will directly provide the agent’s optimal series of actions. </span><span class="kobospan" id="kobo.137.4">In the second case, it will supply the optimal parameters for the neural controller that provides </span><span><span class="kobospan" id="kobo.138.1">these actions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.139.1">Before we start applying genetic algorithms to reinforcement learning tasks, let’s get acquainted with the toolkit that will be used to conduct these tasks – </span><span><strong class="bold"><span class="kobospan" id="kobo.140.1">Gymnasium</span></strong></span><span><span class="kobospan" id="kobo.141.1">.</span></span></p>
<h1 id="_idParaDest-227" class="calibre5"><a id="_idTextAnchor279" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.142.1">Gymnasium</span></h1>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.143.1">Gymnasium</span></em><span class="kobospan" id="kobo.144.1"> (</span><a href="https://gymnasium.farama.org/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.145.1">https://gymnasium.farama.org/</span></a><span class="kobospan" id="kobo.146.1">) – a fork and the official successor of </span><em class="italic"><span class="kobospan" id="kobo.147.1">OpenAI Gym</span></em><span class="kobospan" id="kobo.148.1"> – is an open </span><a id="_idIndexMarker641" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.149.1">source library that was written to allow access to a </span><a id="_idIndexMarker642" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.150.1">standardized set of reinforcement learning tasks. </span><span class="kobospan" id="kobo.150.2">It provides a toolkit that can be used to compare and develop reinforcement </span><span><span class="kobospan" id="kobo.151.1">learning algorithms.</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.152.1">Gymnasium</span></em><span class="kobospan" id="kobo.153.1"> consists of a collection of environments, all presenting a common interface called </span><strong class="source-inline"><span class="kobospan" id="kobo.154.1">env</span></strong><span class="kobospan" id="kobo.155.1">. </span><span class="kobospan" id="kobo.155.2">This interface decouples the various environments from the agents, which can be implemented in any way we like – the only requirement from the agent is that it can interact with the environment(s) via the </span><strong class="source-inline"><span class="kobospan" id="kobo.156.1">env</span></strong><span class="kobospan" id="kobo.157.1"> interface. </span><span class="kobospan" id="kobo.157.2">This will be described in the </span><span><span class="kobospan" id="kobo.158.1">next subsection.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.159.1">The basic package, </span><strong class="source-inline"><span class="kobospan" id="kobo.160.1">gymnasium</span></strong><span class="kobospan" id="kobo.161.1">, provides access to several environments and can be installed </span><span><span class="kobospan" id="kobo.162.1">as follows:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.163.1">
pip install gymnasium</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.164.1">To allow us to render and animate our test environments, the </span><em class="italic"><span class="kobospan" id="kobo.165.1">PyGame</span></em><span class="kobospan" id="kobo.166.1"> library needs to be installed as well. </span><span class="kobospan" id="kobo.166.2">This can be done using the </span><span><span class="kobospan" id="kobo.167.1">following command:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.168.1">
pip install pygame</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.169.1">Several other packages are available, such as “Atari,” “Box2D,” and “MuJoCo,” that provide access to numerous and diverse additional environments. </span><span class="kobospan" id="kobo.169.2">Some of these packages have </span><a id="_idIndexMarker643" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.170.1">system dependencies and may only be available for certain operating systems. </span><span class="kobospan" id="kobo.170.2">More information is available </span><span><span class="kobospan" id="kobo.171.1">at </span></span><a href="https://github.com/Farama-Foundation/Gymnasium#installation" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.172.1">https://github.com/Farama-Foundation/Gymnasium#installation</span></span></a><span><span class="kobospan" id="kobo.173.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.174.1">The next subsection describes interaction with the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.175.1">env</span></strong></span><span><span class="kobospan" id="kobo.176.1"> interface.</span></span></p>
<h2 id="_idParaDest-228" class="calibre7"><a id="_idTextAnchor280" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.177.1">The env interface</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.178.1">To create </span><a id="_idIndexMarker644" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.179.1">an environment, we need to use the </span><strong class="source-inline"><span class="kobospan" id="kobo.180.1">make()</span></strong><span class="kobospan" id="kobo.181.1"> method and </span><a id="_idIndexMarker645" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.182.1">the name of the desired environment, </span><span><span class="kobospan" id="kobo.183.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.184.1">
import </span><strong class="bold1"><span class="kobospan1" id="kobo.185.1">gymnasium </span></strong><span class="kobospan1" id="kobo.186.1">as gym
env = gym.</span><strong class="bold1"><span class="kobospan1" id="kobo.187.1">make</span></strong><span class="kobospan1" id="kobo.188.1">('MountainCar-v0')</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.189.1">Once the environment has been created, it can be initialized using the </span><strong class="source-inline"><span class="kobospan" id="kobo.190.1">reset()</span></strong><span class="kobospan" id="kobo.191.1"> method, as shown in the following </span><span><span class="kobospan" id="kobo.192.1">code snippet:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.193.1">
observation, info = env.</span><strong class="bold1"><span class="kobospan1" id="kobo.194.1">reset</span></strong><span class="kobospan1" id="kobo.195.1">()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.196.1">This method returns an </span><strong class="source-inline"><span class="kobospan" id="kobo.197.1">observation</span></strong><span class="kobospan" id="kobo.198.1"> object, describing the initial state of the environment, and a dictionary, </span><strong class="source-inline"><span class="kobospan" id="kobo.199.1">info</span></strong><span class="kobospan" id="kobo.200.1">, that may contain auxiliary information complementing </span><strong class="source-inline"><span class="kobospan" id="kobo.201.1">observation</span></strong><span class="kobospan" id="kobo.202.1">. </span><span class="kobospan" id="kobo.202.2">The content of </span><strong class="source-inline"><span class="kobospan" id="kobo.203.1">observation</span></strong> <span><span class="kobospan" id="kobo.204.1">is environment-dependent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.205.1">Conforming with the reinforcement learning cycle that we described in the previous section, the ongoing interaction with the environment consists of sending it an </span><em class="italic"><span class="kobospan" id="kobo.206.1">action </span></em><span class="kobospan" id="kobo.207.1">and, in return, receiving an </span><em class="italic"><span class="kobospan" id="kobo.208.1">intermediate reward</span></em><span class="kobospan" id="kobo.209.1"> and a new </span><em class="italic"><span class="kobospan" id="kobo.210.1">state</span></em><span class="kobospan" id="kobo.211.1">. </span><span class="kobospan" id="kobo.211.2">This is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.212.1">step()</span></strong><span class="kobospan" id="kobo.213.1"> method, </span><span><span class="kobospan" id="kobo.214.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.215.1">
observation, reward, terminated, truncated, info = \
    env.</span><strong class="bold1"><span class="kobospan1" id="kobo.216.1">step</span></strong><span class="kobospan1" id="kobo.217.1">(action)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.218.1">In addition to the </span><strong class="source-inline"><span class="kobospan" id="kobo.219.1">observation</span></strong><span class="kobospan" id="kobo.220.1"> object, which describes the new state and the float </span><strong class="source-inline"><span class="kobospan" id="kobo.221.1">reward</span></strong><span class="kobospan" id="kobo.222.1"> value that represent the interim reward, this method returns the </span><span><span class="kobospan" id="kobo.223.1">following values:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.224.1">terminated</span></strong><span class="kobospan" id="kobo.225.1">: A Boolean that turns </span><strong class="source-inline1"><span class="kobospan" id="kobo.226.1">true</span></strong><span class="kobospan" id="kobo.227.1"> when the current run (also called </span><em class="italic"><span class="kobospan" id="kobo.228.1">episode</span></em><span class="kobospan" id="kobo.229.1">) reaches the terminal state – for example, the agent lost a life, or successfully completed </span><span><span class="kobospan" id="kobo.230.1">a task.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.231.1">truncated</span></strong><span class="kobospan" id="kobo.232.1">: A Boolean that can be used to end the episode prematurely before a terminal state is reached – for example, due to a time limit, or if the agent went out </span><span><span class="kobospan" id="kobo.233.1">of bounds.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.234.1">info</span></strong><span class="kobospan" id="kobo.235.1">: A dictionary containing optional, additional information that may be useful for debugging. </span><span class="kobospan" id="kobo.235.2">However, it should not be used by the agent </span><span><span class="kobospan" id="kobo.236.1">for learning.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.237.1">At any point </span><a id="_idIndexMarker646" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.238.1">in time, the environment can be rendered for visual presentation, </span><span><span class="kobospan" id="kobo.239.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.240.1">
env.</span><strong class="bold1"><span class="kobospan1" id="kobo.241.1">render</span></strong><span class="kobospan1" id="kobo.242.1">()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.243.1">The rendered </span><a id="_idIndexMarker647" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.244.1">presentation is environment-specific and is affected by the value of  </span><strong class="source-inline"><span class="kobospan" id="kobo.245.1">render_mode</span></strong><span class="kobospan" id="kobo.246.1">, which can be set when the environment is created. </span><span class="kobospan" id="kobo.246.2">A value of </span><strong class="source-inline"><span class="kobospan" id="kobo.247.1">"human"</span></strong><span class="kobospan" id="kobo.248.1">, for example, will result in the environment being continuously rendered in the current display or terminal, while the default value of </span><strong class="source-inline"><span class="kobospan" id="kobo.249.1">None</span></strong><span class="kobospan" id="kobo.250.1"> will result in </span><span><span class="kobospan" id="kobo.251.1">no rendering.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.252.1">Finally, an environment can be closed to invoke any necessary cleanup, </span><span><span class="kobospan" id="kobo.253.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.254.1">
env.</span><strong class="bold1"><span class="kobospan1" id="kobo.255.1">close</span></strong><span class="kobospan1" id="kobo.256.1">()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.257.1">If this method isn’t called, the environment will automatically close itself the next time Python runs its </span><em class="italic"><span class="kobospan" id="kobo.258.1">garbage collection</span></em><span class="kobospan" id="kobo.259.1"> process (the process of identifying and freeing memory that is no longer in use by the program), or when the </span><span><span class="kobospan" id="kobo.260.1">program exits.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.261.1">Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.262.1">Detailed </span><a id="_idIndexMarker648" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.263.1">information about the env interface is available at </span><a href="https://gymnasium.farama.org/api/env/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.264.1">https://gymnasium.farama.org/api/env/</span></span></a><span><span class="kobospan" id="kobo.265.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.266.1">The complete cycle of interaction with the environment will be demonstrated in the next section, where we’ll encounter our first gymnasium challenge – the </span><span><em class="italic"><span class="kobospan" id="kobo.267.1">MountainCar </span></em></span><span><span class="kobospan" id="kobo.268.1">environment.</span></span></p>
<h1 id="_idParaDest-229" class="calibre5"><a id="_idTextAnchor281" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.269.1">Solving the MountainCar environment</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.270.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.271.1">MountainCar-v0</span></strong><span class="kobospan" id="kobo.272.1"> environment simulates a car on a one-dimensional track, situated </span><a id="_idIndexMarker649" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.273.1">between two hills. </span><span class="kobospan" id="kobo.273.2">The simulation starts with the car placed between the hills, as shown in the following </span><span><span class="kobospan" id="kobo.274.1">rendered output:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer125">
<span class="kobospan" id="kobo.275.1"><img alt="Figure 10.2: The MountainCar simulation – the starting point" src="image/B20851_10_2.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.276.1">Figure 10.2: The MountainCar simulation – the starting point</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.277.1">The goal is to get the car to climb up the taller hill – the one on the right – and ultimately hit </span><span><span class="kobospan" id="kobo.278.1">the flag:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer126">
<span class="kobospan" id="kobo.279.1"><img alt="Figure 10.3: The MountainCar simulation – the car climbing the hill on the right" src="image/B20851_10_3.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.280.1">Figure 10.3: The MountainCar simulation – the car climbing the hill on the right</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.281.1">The simulation is set up with the car’s engine being too weak to directly climb the taller hill. </span><span class="kobospan" id="kobo.281.2">The only way to reach the goal is to drive the car back and forth until enough momentum </span><a id="_idIndexMarker650" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.282.1">is built for climbing. </span><span class="kobospan" id="kobo.282.2">Climbing the left hill can help to achieve this goal, as reaching the left peak will bounce the car back to the right, as shown in the </span><span><span class="kobospan" id="kobo.283.1">following screenshot:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer127">
<span class="kobospan" id="kobo.284.1"><img alt="Figure 10.4: The MountainCar simulation – the car bouncing off the hill on the left" src="image/B20851_10_4.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.285.1">Figure 10.4: The MountainCar simulation – the car bouncing off the hill on the left</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.286.1">This simulation is a great example where intermediate loss (moving left) can help to achieve the ultimate goal (going all the way to </span><span><span class="kobospan" id="kobo.287.1">the right).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.288.1">The expected </span><em class="italic"><span class="kobospan" id="kobo.289.1">action</span></em><span class="kobospan" id="kobo.290.1"> value in this simulation is an integer with of one of the three </span><span><span class="kobospan" id="kobo.291.1">following values:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.292.1">0: </span><span><span class="kobospan" id="kobo.293.1">Push left</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.294.1">1: </span><span><span class="kobospan" id="kobo.295.1">No push</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.296.1">2: </span><span><span class="kobospan" id="kobo.297.1">Push right</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.298.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.299.1">observation</span></strong><span class="kobospan" id="kobo.300.1"> object contains two floats that describe the position and the velocity of the car, such as </span><span><span class="kobospan" id="kobo.301.1">the following:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.302.1">
[-1.0260268, -0.03201975]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.303.1">Finally, the </span><strong class="source-inline"><span class="kobospan" id="kobo.304.1">reward</span></strong><span class="kobospan" id="kobo.305.1"> value is -1 for each time step, until the goal (located at position 0.5) is reached. </span><span class="kobospan" id="kobo.305.2">The simulation will stop after 200 steps if the goal is not </span><span><span class="kobospan" id="kobo.306.1">reached beforehand.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.307.1">The goal of this </span><a id="_idIndexMarker651" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.308.1">environment is to reach the flag placed on top of the right hill as quickly as possible, and therefore, the agent is penalised with a reward of -1 for each </span><span><span class="kobospan" id="kobo.309.1">timestep used.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.310.1">More information about the </span><em class="italic"><span class="kobospan" id="kobo.311.1">MountainCar-v0</span></em><span class="kobospan" id="kobo.312.1"> environment can be </span><span><span class="kobospan" id="kobo.313.1">found at</span></span></p>
<p class="calibre3"><a href="https://gymnasium.farama.org/environments/classic_control/mountain_car/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.314.1">https://gymnasium.farama.org/environments/classic_control/mountain_car/</span></span></a><span><span class="kobospan" id="kobo.315.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.316.1">In our implementation, we will attempt to hit the flag using the least amount of steps, as we apply a sequence of preselected actions from a fixed starting position. </span><span class="kobospan" id="kobo.316.2">To find a sequence of actions that will get the car to climb the tall hill and hit the flag, we will craft a genetic algorithm-based solution. </span><span class="kobospan" id="kobo.316.3">As usual, we will start by defining what a candidate solution for this challenge will </span><span><span class="kobospan" id="kobo.317.1">look like.</span></span></p>
<h2 id="_idParaDest-230" class="calibre7"><a id="_idTextAnchor282" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.318.1">Solution representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.319.1">Since </span><em class="italic"><span class="kobospan" id="kobo.320.1">MountainCar</span></em><span class="kobospan" id="kobo.321.1"> is controlled </span><a id="_idIndexMarker652" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.322.1">by a sequence of actions, each with a value of 0 (push left), 1 (no push), or 2 (push right), and there can be up to 200 actions in a single episode, one obvious way to represent a candidate solution is by using a list of length 200, containing values of 0, 1, or 2. </span><span class="kobospan" id="kobo.322.2">An example of this is </span><span><span class="kobospan" id="kobo.323.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.324.1">
[0, 1, 2, 0, 0, 1, 2, 2, 1, ... </span><span class="kobospan1" id="kobo.324.2">, 0, 2, 1, 1]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.325.1">The values in the list will be used as actions to control the car and, hopefully, drive it to the flag. </span><span class="kobospan" id="kobo.325.2">If the </span><a id="_idIndexMarker653" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.326.1">car made it to the flag in less than 200 steps, the last few items in the list will not </span><span><span class="kobospan" id="kobo.327.1">be used.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.328.1">Next, we need to determine how to evaluate a given solution of </span><span><span class="kobospan" id="kobo.329.1">this form.</span></span></p>
<h2 id="_idParaDest-231" class="calibre7"><a id="_idTextAnchor283" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.330.1">Evaluating the solution</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.331.1">While evaluating a given solution, or when comparing two solutions, it is apparent that the reward value alone may not provide us with sufficient information. </span><span class="kobospan" id="kobo.331.2">With the way the reward </span><a id="_idIndexMarker654" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.332.1">is defined, its value will always be -200 if we don’t hit the flag. </span><span class="kobospan" id="kobo.332.2">When we compare two candidate solutions that don’t hit the flag, we would still like to know which one got closer to it and consider it a better solution. </span><span class="kobospan" id="kobo.332.3">Therefore, we will use the final position of the car, in addition to the reward value, to determine the score of </span><span><span class="kobospan" id="kobo.333.1">the solution:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.334.1">If the car did not hit the flag, the score will be the distance from the flag. </span><span class="kobospan" id="kobo.334.2">Therefore, we will look for a solution that minimizes </span><span><span class="kobospan" id="kobo.335.1">the score.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.336.1">If the car hits the flag, the base score will be zero, and from that, we deduct an additional value based on how many unused steps were left, making the score negative. </span><span class="kobospan" id="kobo.336.2">Since we are looking for the lowest score possible, this arrangement will encourage solutions to hit the flag using the smallest possible amount </span><span><span class="kobospan" id="kobo.337.1">of actions.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.338.1">This score evaluation procedure is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.339.1">MountainCar</span></strong><span class="kobospan" id="kobo.340.1"> class, which is explored in the </span><span><span class="kobospan" id="kobo.341.1">following subsection.</span></span></p>
<h2 id="_idParaDest-232" class="calibre7"><a id="_idTextAnchor284" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.342.1">The Python problem representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.343.1">To </span><a id="_idIndexMarker655" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.344.1">encapsulate the MountainCar challenge, we’ve created a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.345.1">MountainCar</span></strong><span class="kobospan" id="kobo.346.1">. </span><span class="kobospan" id="kobo.346.2">This class is contained in the </span><strong class="source-inline"><span class="kobospan" id="kobo.347.1">mountain_car.py</span></strong><span class="kobospan" id="kobo.348.1"> file, which is located </span><span><span class="kobospan" id="kobo.349.1">at</span></span><span> </span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.350.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py</span></span></a><span><span class="kobospan" id="kobo.351.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.352.1">The class is initialized with a random seed and provides the </span><span><span class="kobospan" id="kobo.353.1">following methods:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.354.1">getScore(actions)</span></strong><span class="kobospan" id="kobo.355.1">: Calculates the score of a given solution, represented by the list of actions. </span><span class="kobospan" id="kobo.355.2">The score is calculated by initiating an episode of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.356.1">MountainCar</span></strong><span class="kobospan" id="kobo.357.1"> environment and running it with the provided actions, and this can be negative if we hit the target with fewer than 200 steps. </span><span class="kobospan" id="kobo.357.2">The lower the score is, </span><span><span class="kobospan" id="kobo.358.1">the better.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.359.1">saveActions(actions)</span></strong><span class="kobospan" id="kobo.360.1">: Saves a list of actions to a file using </span><strong class="source-inline1"><span class="kobospan" id="kobo.361.1">pickle</span></strong><span class="kobospan" id="kobo.362.1"> (Python’s object serialization and </span><span><span class="kobospan" id="kobo.363.1">deserialization module).</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.364.1">replaySavedActions()</span></strong><span class="kobospan" id="kobo.365.1">: Deserializes the last saved list of actions and replays it using the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.366.1">replay</span></strong></span><span><span class="kobospan" id="kobo.367.1"> method.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.368.1">replay(actions)</span></strong><span class="kobospan" id="kobo.369.1">: Renders the environment using the “human” </span><strong class="source-inline1"><span class="kobospan" id="kobo.370.1">render_mode</span></strong><span class="kobospan" id="kobo.371.1"> and replays the list of actions given to it, visualizing a </span><span><span class="kobospan" id="kobo.372.1">given solution.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.373.1">The main </span><a id="_idIndexMarker656" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.374.1">method of the class can be used after a solution has been found, serialized, and saved using the </span><strong class="source-inline"><span class="kobospan" id="kobo.375.1">saveActions()</span></strong><span class="kobospan" id="kobo.376.1"> method. </span><span class="kobospan" id="kobo.376.2">The main method will initialize the class and call </span><strong class="source-inline"><span class="kobospan" id="kobo.377.1">replaySavedActions()</span></strong><span class="kobospan" id="kobo.378.1"> to render and animate the last </span><span><span class="kobospan" id="kobo.379.1">saved solution.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.380.1">We typically use the main method to animate the best solution that’s found by the genetic algorithm-based program. </span><span class="kobospan" id="kobo.380.2">This will be explored in the </span><span><span class="kobospan" id="kobo.381.1">following subsection.</span></span></p>
<h2 id="_idParaDest-233" class="calibre7"><a id="_idTextAnchor285" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.382.1">Genetic algorithms solution</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.383.1">To tackle the </span><em class="italic"><span class="kobospan" id="kobo.384.1">MountainCar</span></em><span class="kobospan" id="kobo.385.1"> challenge using the genetic algorithms approach, we’ve created </span><a id="_idIndexMarker657" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.386.1">a Python program, </span><strong class="source-inline"><span class="kobospan" id="kobo.387.1">01_solve_mountain_car.py</span></strong><span class="kobospan" id="kobo.388.1">, which is located </span><span><span class="kobospan" id="kobo.389.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.390.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py</span></span></a><span><span class="kobospan" id="kobo.391.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.392.1">Since the solution representation we chose for this problem is a list containing the 0, 1, or 2 integer values, this program bears resemblance to the one we used to solve the knapsack 0-1 problem in </span><a href="B20851_04.xhtml#_idTextAnchor155" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.393.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.394.1">, </span><em class="italic"><span class="kobospan" id="kobo.395.1">Combinatorial Optimization</span></em><span class="kobospan" id="kobo.396.1">, where solutions were represented as lists with the values 0 </span><span><span class="kobospan" id="kobo.397.1">and 1.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.398.1">The following steps describe how to create the main parts of </span><span><span class="kobospan" id="kobo.399.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.400.1">We start by creating an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.401.1">MountainCar</span></strong><span class="kobospan" id="kobo.402.1"> class, which will allow us to score the various solutions for the </span><span><em class="italic"><span class="kobospan" id="kobo.403.1">MountainCar</span></em></span><span><span class="kobospan" id="kobo.404.1"> challenge:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.405.1">
car = mountain_car.</span><strong class="bold1"><span class="kobospan1" id="kobo.406.1">MountainCar</span></strong><span class="kobospan1" id="kobo.407.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.408.1">Since our goal is to minimize the score – in other words, hit the flag with the minimum </span><a id="_idIndexMarker658" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.409.1">step count, or get as close as possible to the flag – we define a single objective, minimizing the </span><span><span class="kobospan" id="kobo.410.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.411.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.412.1">FitnessMin</span></strong><span class="kobospan1" id="kobo.413.1">", base.Fitness, weights=(-1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.414.1">Now, we need to create a toolbox operator that can produce one of the three allowed action values – 0, 1, </span><span><span class="kobospan" id="kobo.415.1">or 2:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.416.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.417.1">zeroOneOrTwo</span></strong><span class="kobospan1" id="kobo.418.1">", random.randint, 0, 2)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.419.1">This is followed by an operator that fills up an individual instance with </span><span><span class="kobospan" id="kobo.420.1">these values:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.421.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.422.1">individualCreator</span></strong><span class="kobospan1" id="kobo.423.1">",
                 tools.initRepeat,
                 creator.Individual,
                 toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.424.1">zeroOneOrTwo</span></strong><span class="kobospan1" id="kobo.425.1">,
                 len(car))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.426.1">Then, we instruct the genetic algorithm to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.427.1">getScore()</span></strong><span class="kobospan" id="kobo.428.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.429.1">MountainCar</span></strong><span class="kobospan" id="kobo.430.1"> instance for </span><span><span class="kobospan" id="kobo.431.1">fitness evaluation.</span></span><p class="calibre3"><span class="kobospan" id="kobo.432.1">As a reminder, the </span><strong class="source-inline"><span class="kobospan" id="kobo.433.1">getScore()</span></strong><span class="kobospan" id="kobo.434.1"> method, which we described in the previous subsection, initiates an episode of the </span><em class="italic"><span class="kobospan" id="kobo.435.1">MountainCar</span></em><span class="kobospan" id="kobo.436.1"> environment and uses the given individual – a list of actions – as the inputs to the environment until the episode is done. </span><span class="kobospan" id="kobo.436.2">Then, it evaluates the score – the lower the better – according to the final location of the car. </span><span class="kobospan" id="kobo.436.3">If the car hit the flag, the score can even be negative, based on the number of unused </span><span><span class="kobospan" id="kobo.437.1">steps left:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.438.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.439.1">carScore</span></strong><span class="kobospan1" id="kobo.440.1">(individual):
    return car.getScore(individual),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.441.1">evaluate</span></strong><span class="kobospan1" id="kobo.442.1">", carScore)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.443.1">As for the genetic operators, we start with the usual </span><em class="italic"><span class="kobospan" id="kobo.444.1">tournament selection</span></em><span class="kobospan" id="kobo.445.1">, with a tournament size of 2. </span><span class="kobospan" id="kobo.445.2">Since our solution representation is a list of the 0, 1, or 2 integer values, we can use the </span><em class="italic"><span class="kobospan" id="kobo.446.1">two-point crossover</span></em><span class="kobospan" id="kobo.447.1"> operator, just like we did </span><a id="_idIndexMarker659" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.448.1">when the solution was represented by a list of 0 and </span><span><span class="kobospan" id="kobo.449.1">1 values.</span></span><p class="calibre3"><span class="kobospan" id="kobo.450.1">For </span><em class="italic"><span class="kobospan" id="kobo.451.1">mutation</span></em><span class="kobospan" id="kobo.452.1">, however, rather than the </span><em class="italic"><span class="kobospan" id="kobo.453.1">FlipBit</span></em><span class="kobospan" id="kobo.454.1"> operator, which is typically used for the binary case, we need to use the </span><em class="italic"><span class="kobospan" id="kobo.455.1">UniformInt</span></em><span class="kobospan" id="kobo.456.1"> operator, which is used for a range of integer values, and configure it for the range </span><span><span class="kobospan" id="kobo.457.1">of 0-2:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.458.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.459.1">select</span></strong><span class="kobospan1" id="kobo.460.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.461.1">mate</span></strong><span class="kobospan1" id="kobo.462.1">", tools.cxTwoPoint)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.463.1">mutate</span></strong><span class="kobospan1" id="kobo.464.1">", tools.mutUniformInt, low=0, up=2, 
    indpb=1.0/len(car))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.465.1">In addition, we </span><a id="_idIndexMarker660" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.466.1">continue to use the </span><em class="italic"><span class="kobospan" id="kobo.467.1">elitist approach</span></em><span class="kobospan" id="kobo.468.1">, where the </span><strong class="bold"><span class="kobospan" id="kobo.469.1">hall of fame</span></strong><span class="kobospan" id="kobo.470.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.471.1">HOF</span></strong><span class="kobospan" id="kobo.472.1">) members – the current best individuals – are always passed untouched to the </span><span><span class="kobospan" id="kobo.473.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.474.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.475.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.476.1">(population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.477.1">After the run, we print the best solution and save it so that we can later animate it, using the replay capability we built into the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.478.1">MountainCar</span></strong></span><span><span class="kobospan" id="kobo.479.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.480.1">
best = hof.items[0]
print("Best Solution = ", best)
print("Best Fitness = ", best.fitness.values[0])
car.</span><strong class="bold1"><span class="kobospan1" id="kobo.481.1">saveActions</span></strong><span class="kobospan1" id="kobo.482.1">(best)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.483.1">Running </span><a id="_idIndexMarker661" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.484.1">the algorithm for 80 generations and with a population size of 100, we get the </span><span><span class="kobospan" id="kobo.485.1">following outcome:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.486.1">gen nevals min avg
0       100     0.708709        1.03242
1       78      0.708709        0.975704
...
</span><span class="kobospan1" id="kobo.486.2">47      71      0.000170529     0.0300455
48      74      4.87566e-05     0.0207197
49      75      -0.005          0.0150622
50      77      -0.005          0.0121327
...
</span><span class="kobospan1" id="kobo.486.3">56      77      -0.02           -0.00321379
57      74      -0.025          -0.00564184
...
</span><span class="kobospan1" id="kobo.486.4">79      76      -0.035          -0.0342
80      76      -0.035          -0.03425
Best Solution =  [1, 0, 2, 1, 1, 2, 0, 2, 2, 2, 0, ... </span><span class="kobospan1" id="kobo.486.5">, 2, 0, 1, 1, 1, 1, 1, 0]
Best Fitness =  -0.035</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.487.1">From the preceding output, we can see that, after about 50 generations, the best solution(s) started hitting the flag, producing score values of zero or less. </span><span class="kobospan" id="kobo.487.2">From hereon, the best solutions hit the flag in fewer steps, thereby producing increasingly negative </span><span><span class="kobospan" id="kobo.488.1">score values.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.489.1">As we </span><a id="_idIndexMarker662" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.490.1">already mentioned, the best solution was saved at the end of the run, and we can now replay it by running the </span><strong class="source-inline"><span class="kobospan" id="kobo.491.1">mountain_car</span></strong><span class="kobospan" id="kobo.492.1"> program. </span><span class="kobospan" id="kobo.492.2">This replay illustrates how the actions of our solution drive the car back and forth between the two peaks, climbing higher each time, until the car is able to climb the lower hill on the left. </span><span class="kobospan" id="kobo.492.3">Then, it bounces back, which means we have enough momentum to continue climbing the higher hill on the right, ultimately hitting the flag, as shown in the </span><span><span class="kobospan" id="kobo.493.1">following screenshot:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer128">
<span class="kobospan" id="kobo.494.1"><img alt="Figure 10.5: The MountainCar simulation – the car reaching the goal" src="image/B20851_10_5.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.495.1">Figure 10.5: The MountainCar simulation – the car reaching the goal</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.496.1">While solving it was a lot of fun, the way this environment is set up did not require us to dynamically interact with it. </span><span class="kobospan" id="kobo.496.2">We were able to climb the hill using a sequence of actions that our </span><a id="_idIndexMarker663" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.497.1">algorithm put together, based on the initial location of the car. </span><span class="kobospan" id="kobo.497.2">In contrast, the next environment we are about to tackle – named </span><em class="italic"><span class="kobospan" id="kobo.498.1">CartPole</span></em><span class="kobospan" id="kobo.499.1"> – requires us to dynamically calculate our action at any time step, based upon the latest observation produced. </span><span class="kobospan" id="kobo.499.2">Read on to find out how we can make </span><span><span class="kobospan" id="kobo.500.1">this work.</span></span></p>
<h1 id="_idParaDest-234" class="calibre5"><a id="_idTextAnchor286" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.501.1">Solving the CartPole environment</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.502.1">The </span><em class="italic"><span class="kobospan" id="kobo.503.1">CartPole-v1</span></em><span class="kobospan" id="kobo.504.1"> environment simulates a balancing act of a pole, hinged at its bottom to a cart, which moves left and right along a track. </span><span class="kobospan" id="kobo.504.2">Balancing the pole upright is carried out by </span><a id="_idIndexMarker664" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.505.1">applying to the cart 1 unit of force – to the right or the left – at </span><span><span class="kobospan" id="kobo.506.1">a time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.507.1">The pole, acting as a pendulum in this environment, starts upright within a small random angle, as shown in the following </span><span><span class="kobospan" id="kobo.508.1">rendered output:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer129">
<span class="kobospan" id="kobo.509.1"><img alt="Figure 10.6: The CartPole simulation – the starting point" src="image/B20851_10_6.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.510.1">Figure 10.6: The CartPole simulation – the starting point</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.511.1">Our goal is to keep the pendulum from falling over to either side for as long as possible – that is, up to 500 time steps. </span><span class="kobospan" id="kobo.511.2">For every time step that the pole remains upright, we get a reward of +1, so the maximum total reward is 500. </span><span class="kobospan" id="kobo.511.3">The episode will end prematurely if one of the following occurs during </span><span><span class="kobospan" id="kobo.512.1">the run:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.513.1">The angle of the pole from the vertical position exceeds </span><span><span class="kobospan" id="kobo.514.1">15 degrees</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.515.1">The cart’s distance from the center exceeds </span><span><span class="kobospan" id="kobo.516.1">2.4 units</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.517.1">Consequently, the total </span><a id="_idIndexMarker665" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.518.1">reward in these cases will be smaller </span><span><span class="kobospan" id="kobo.519.1">than 500.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.520.1">The expected </span><strong class="source-inline"><span class="kobospan" id="kobo.521.1">action</span></strong><span class="kobospan" id="kobo.522.1"> value in this simulation is an integer of one of the two </span><span><span class="kobospan" id="kobo.523.1">following values:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.524.1">0: Push the cart to </span><span><span class="kobospan" id="kobo.525.1">the left</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.526.1">1: Push the cart to </span><span><span class="kobospan" id="kobo.527.1">the right</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.528.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.529.1">observation</span></strong><span class="kobospan" id="kobo.530.1"> object contains four floats that hold the </span><span><span class="kobospan" id="kobo.531.1">following information:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.532.1">Cart position</span></strong><span class="kobospan" id="kobo.533.1">, between -2.4 </span><span><span class="kobospan" id="kobo.534.1">and 2.4</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.535.1">Cart velocity</span></strong><span class="kobospan" id="kobo.536.1">, between -Inf </span><span><span class="kobospan" id="kobo.537.1">and Inf</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.538.1">Pole angle</span></strong><span class="kobospan" id="kobo.539.1">, between -0.418 rad (-24°) and 0.418 </span><span><span class="kobospan" id="kobo.540.1">rad (24°)</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.541.1">Pole angular velocity</span></strong><span class="kobospan" id="kobo.542.1">, between -Inf </span><span><span class="kobospan" id="kobo.543.1">and Inf</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.544.1">For example, we could have an </span><strong class="source-inline"><span class="kobospan" id="kobo.545.1">observation</span></strong><span class="kobospan" id="kobo.546.1"> of </span><strong class="source-inline"><span class="kobospan" id="kobo.547.1">[0.33676587, 0.3786464, -</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.548.1">0.00170739, -0.36586074]</span></strong></span><span><span class="kobospan" id="kobo.549.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.550.1">More information about the CartPole-v1 environment is available </span><span><span class="kobospan" id="kobo.551.1">at </span></span><a href="https://gymnasium.farama.org/environments/classic_control/cart_pole/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.552.1">https://gymnasium.farama.org/environments/classic_control/cart_pole/</span></span></a><span><span class="kobospan" id="kobo.553.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.554.1">In our proposed solution, we will use these values as inputs at every time step to determine which action to take. </span><span class="kobospan" id="kobo.554.2">We will do this with the aid of a neural network-based controller. </span><span class="kobospan" id="kobo.554.3">This is described in more detail in the </span><span><span class="kobospan" id="kobo.555.1">following subsection.</span></span></p>
<h2 id="_idParaDest-235" class="calibre7"><a id="_idTextAnchor287" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.556.1">Controlling the CartPole with a neural network</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.557.1">To successfully carry out the </span><em class="italic"><span class="kobospan" id="kobo.558.1">CartPole</span></em><span class="kobospan" id="kobo.559.1"> challenge, we would like to dynamically respond to the </span><a id="_idIndexMarker666" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.560.1">changes in the environment. </span><span class="kobospan" id="kobo.560.2">For example, when the pole starts leaning in one direction, we should probably move the cart in that direction but possibly stop pushing when it starts to stabilize. </span><span class="kobospan" id="kobo.560.3">So, the reinforcement learning task here can be thought of as teaching a controller to balance the pole by mapping the four available inputs – cart position, cart velocity, pole angle, and pole velocity – to the appropriate action at each time step. </span><span class="kobospan" id="kobo.560.4">How can we implement </span><span><span class="kobospan" id="kobo.561.1">such mapping?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.562.1">One good </span><a id="_idIndexMarker667" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.563.1">way to implement this mapping is by using a </span><strong class="bold"><span class="kobospan" id="kobo.564.1">neural network</span></strong><span class="kobospan" id="kobo.565.1">. </span><span class="kobospan" id="kobo.565.2">As we saw in </span><a href="B20851_09.xhtml#_idTextAnchor257" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.566.1">Chapter 9</span></em></span></a><span class="kobospan" id="kobo.567.1">, </span><em class="italic"><span class="kobospan" id="kobo.568.1">Architecture Optimization of Deep Learning Networks</span></em><span class="kobospan" id="kobo.569.1">, a neural </span><a id="_idIndexMarker668" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.570.1">network, such as a </span><strong class="bold"><span class="kobospan" id="kobo.571.1">multilayer perceptron</span></strong><span class="kobospan" id="kobo.572.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.573.1">MLP</span></strong><span class="kobospan" id="kobo.574.1">), can implement complex mappings between its inputs and outputs. </span><span class="kobospan" id="kobo.574.2">This mapping </span><a id="_idIndexMarker669" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.575.1">is done with the aid of the network parameters – namely, the </span><em class="italic"><span class="kobospan" id="kobo.576.1">weights and biases</span></em><span class="kobospan" id="kobo.577.1"> of the active nodes in the network, as well as the </span><em class="italic"><span class="kobospan" id="kobo.578.1">transfer functions</span></em><span class="kobospan" id="kobo.579.1"> that are implemented by these nodes. </span><span class="kobospan" id="kobo.579.2">In our case, we will use a network with a single </span><em class="italic"><span class="kobospan" id="kobo.580.1">hidden layer</span></em><span class="kobospan" id="kobo.581.1"> of four nodes. </span><span class="kobospan" id="kobo.581.2">In addition, the </span><em class="italic"><span class="kobospan" id="kobo.582.1">input layer</span></em><span class="kobospan" id="kobo.583.1"> consists of four nodes, one for each of the input values provided by the environment, while the </span><em class="italic"><span class="kobospan" id="kobo.584.1">output layer</span></em><span class="kobospan" id="kobo.585.1"> has a single node, since we only have one output value – the action to be taken. </span><span class="kobospan" id="kobo.585.2">This network structure can be illustrated with the </span><span><span class="kobospan" id="kobo.586.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer130">
<span class="kobospan" id="kobo.587.1"><img alt="Figure 10.7: The structure of the neural network that’s used to control the cart" src="image/B20851_10_7.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.588.1">Figure 10.7: The structure of the neural network that’s used to control the cart</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.589.1">As we have seen already, the values of the weights and biases of a neural network are typically set during a process in which the network is trained. </span><span class="kobospan" id="kobo.589.2">The interesting part is that, so far, we have only seen this kind of neural network being trained using the backpropagation algorithm while implementing </span><em class="italic"><span class="kobospan" id="kobo.590.1">supervised learning</span></em><span class="kobospan" id="kobo.591.1"> – that is, in each of the previous cases, we had a training set of inputs and matching outputs, and the network was trained to map each given input to its matching given output. </span><span class="kobospan" id="kobo.591.2">Here, however, as we practice </span><em class="italic"><span class="kobospan" id="kobo.592.1">reinforcement learning</span></em><span class="kobospan" id="kobo.593.1">, we don’t have such training information available. </span><span class="kobospan" id="kobo.593.2">Instead, we only know how well the network did at the end of the episode. </span><span class="kobospan" id="kobo.593.3">This means that instead of using the conventional training algorithms, we need a method that will allow us to find the best network parameters – weights and biases – based on </span><a id="_idIndexMarker670" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.594.1">the results that are obtained by running the environment’s episodes. </span><span class="kobospan" id="kobo.594.2">This is exactly the kind of optimization that genetic algorithms are good at – finding a set of parameters that will give us the best results, as long as you have a way to evaluate and compare them. </span><span class="kobospan" id="kobo.594.3">To do that, we need to figure out how to represent the network’s parameters, as well as how to evaluate a given set of those parameters. </span><span class="kobospan" id="kobo.594.4">Both of these topics will be covered in the </span><span><span class="kobospan" id="kobo.595.1">following subsection.</span></span></p>
<h2 id="_idParaDest-236" class="calibre7"><a id="_idTextAnchor288" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.596.1">Solution representation and evaluation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.597.1">Since we have decided to control the cart in the CartPole challenge using a neural network </span><a id="_idIndexMarker671" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.598.1">of the </span><em class="italic"><span class="kobospan" id="kobo.599.1">MLP</span></em><span class="kobospan" id="kobo.600.1"> type, the set of </span><a id="_idIndexMarker672" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.601.1">parameters that we will need to optimize are the network’s weights and biases, </span><span><span class="kobospan" id="kobo.602.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.603.1">Input layer</span></strong><span class="kobospan" id="kobo.604.1">: This layer does not participate in the network mapping; instead, it receives the input values and forwards them to every neuron in the next layer. </span><span class="kobospan" id="kobo.604.2">Therefore, no parameters are needed for </span><span><span class="kobospan" id="kobo.605.1">this layer.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.606.1">Hidden layer</span></strong><span class="kobospan" id="kobo.607.1">: Each node in this layer is fully connected to each of the four inputs and, therefore, requires four weights in addition to a single </span><span><span class="kobospan" id="kobo.608.1">bias value.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.609.1">Output layer</span></strong><span class="kobospan" id="kobo.610.1">: The single node in this layer is connected to each of the four nodes in the hidden layer and, therefore, requires four weights in addition to a single </span><span><span class="kobospan" id="kobo.611.1">bias value.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.612.1">In total, we have 20 weight values and 5 bias values we need to find, all of the </span><strong class="source-inline"><span class="kobospan" id="kobo.613.1">float</span></strong><span class="kobospan" id="kobo.614.1"> type. </span><span class="kobospan" id="kobo.614.2">Therefore, each potential solution can be represented as a list of 25 </span><strong class="source-inline"><span class="kobospan" id="kobo.615.1">float</span></strong><span class="kobospan" id="kobo.616.1"> values, </span><span><span class="kobospan" id="kobo.617.1">like so:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.618.1">
[0.9505049282421143, -0.8068797228337171, -0.45488246459260073, ... </span><span class="kobospan1" id="kobo.618.2">,0.6720551701599038]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.619.1">Evaluating a given solution means creating our MLP with the correct dimensions – four inputs, a four-node hidden layer, and a single output – and assigning the weight and bias values from our list of floats to the various nodes. </span><span class="kobospan" id="kobo.619.2">Then, we need to use this MLP as the controller </span><a id="_idIndexMarker673" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.620.1">for the cart pole during one episode. </span><span class="kobospan" id="kobo.620.2">The resulting total reward of the episode is used as the score value for this solution. </span><span class="kobospan" id="kobo.620.3">In </span><a id="_idIndexMarker674" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.621.1">contrast to the previous task, here, we aim to </span><em class="italic"><span class="kobospan" id="kobo.622.1">maximize</span></em><span class="kobospan" id="kobo.623.1"> the score that’s achieved. </span><span class="kobospan" id="kobo.623.2">This score evaluation procedure is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.624.1">CartPole</span></strong><span class="kobospan" id="kobo.625.1"> class, which will be explored in the </span><span><span class="kobospan" id="kobo.626.1">following subsection.</span></span></p>
<h2 id="_idParaDest-237" class="calibre7"><a id="_idTextAnchor289" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.627.1">The Python problem representation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.628.1">To encapsulate the </span><em class="italic"><span class="kobospan" id="kobo.629.1">CartPole</span></em><span class="kobospan" id="kobo.630.1"> challenge, we’ve created a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.631.1">CartPole</span></strong><span class="kobospan" id="kobo.632.1">. </span><span class="kobospan" id="kobo.632.2">This </span><a id="_idIndexMarker675" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.633.1">class is contained in the </span><strong class="source-inline"><span class="kobospan" id="kobo.634.1">cart_pole.py</span></strong><span class="kobospan" id="kobo.635.1"> file, which is located </span><span><span class="kobospan" id="kobo.636.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.637.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py</span></span></a><span><span class="kobospan" id="kobo.638.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.639.1">The class is initialized with an optional random seed and provides the </span><span><span class="kobospan" id="kobo.640.1">following methods:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.641.1">initMlp()</span></strong><span class="kobospan" id="kobo.642.1">: Initializes an MLP </span><em class="italic"><span class="kobospan" id="kobo.643.1">regressor</span></em><span class="kobospan" id="kobo.644.1"> with the desired network architecture (layers) and network parameters (weights and biases), which are derived from the list of floats that represent a </span><span><span class="kobospan" id="kobo.645.1">candidate solution.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.646.1">getScore()</span></strong><span class="kobospan" id="kobo.647.1">: Calculates the score of a given solution, represented by the list of float-valued network parameters. </span><span class="kobospan" id="kobo.647.2">This is done by creating a corresponding MLP regressor, initiating an episode of the </span><em class="italic"><span class="kobospan" id="kobo.648.1">CartPole</span></em><span class="kobospan" id="kobo.649.1"> environment, and running it with the MLP controlling the actions, all while using the observations as inputs. </span><span class="kobospan" id="kobo.649.2">The higher the score is, </span><span><span class="kobospan" id="kobo.650.1">the better.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.651.1">saveParams()</span></strong><span class="kobospan" id="kobo.652.1">: Serializes and saves a list of network parameters </span><span><span class="kobospan" id="kobo.653.1">using </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.654.1">pickle</span></strong></span><span><span class="kobospan" id="kobo.655.1">.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.656.1">replayWithSavedParams()</span></strong><span class="kobospan" id="kobo.657.1">: Deserializes the latest saved list of network parameters and uses it to replay an episode using the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.658.1">replay</span></strong></span><span><span class="kobospan" id="kobo.659.1"> method.</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.660.1">replay()</span></strong><span class="kobospan" id="kobo.661.1">: Renders the environment and uses the given network parameters to replay an episode, visualizing a </span><span><span class="kobospan" id="kobo.662.1">given solution.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.663.1">The main method of the class should be used after a solution has been serialized and saved, using the </span><strong class="source-inline"><span class="kobospan" id="kobo.664.1">saveParams()</span></strong><span class="kobospan" id="kobo.665.1"> method. </span><span class="kobospan" id="kobo.665.2">The main method will initialize the class and call </span><strong class="source-inline"><span class="kobospan" id="kobo.666.1">replayWithSavedParams()</span></strong><span class="kobospan" id="kobo.667.1"> to render and animate the </span><span><span class="kobospan" id="kobo.668.1">saved solution.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.669.1">We will </span><a id="_idIndexMarker676" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.670.1">typically use the main method to animate the best solution that’s found by our genetic algorithm-driven solution, as explored in the </span><span><span class="kobospan" id="kobo.671.1">following subsection.</span></span></p>
<h2 id="_idParaDest-238" class="calibre7"><a id="_idTextAnchor290" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.672.1">A genetic algorithm solution</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.673.1">To interact with the </span><em class="italic"><span class="kobospan" id="kobo.674.1">CartPole</span></em><span class="kobospan" id="kobo.675.1"> environment and solve it using a genetic algorithm, we’ve created </span><a id="_idIndexMarker677" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.676.1">a Python program, </span><strong class="source-inline"><span class="kobospan" id="kobo.677.1">02_solve_cart-pole.py</span></strong><span class="kobospan" id="kobo.678.1">, which is located </span><span><span class="kobospan" id="kobo.679.1">at </span></span><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.680.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py</span></span></a><span><span class="kobospan" id="kobo.681.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.682.1">Since we will use a list of floats to represent a solution – the network’s weights and biases – this program is very similar to the function optimization programs we looked at in </span><a href="B20851_06.xhtml#_idTextAnchor197" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.683.1">Chapter 6</span></em></span></a><span class="kobospan" id="kobo.684.1">, </span><em class="italic"><span class="kobospan" id="kobo.685.1">Optimizing Continuous Functions</span></em><span class="kobospan" id="kobo.686.1">, such as the one we used for the </span><em class="italic"><span class="kobospan" id="kobo.687.1">Eggholder </span></em><span><em class="italic"><span class="kobospan" id="kobo.688.1">function</span></em></span><span><span class="kobospan" id="kobo.689.1">’s optimization.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.690.1">The following steps describe how to create the main parts of </span><span><span class="kobospan" id="kobo.691.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.692.1">We start by creating an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.693.1">CartPole</span></strong><span class="kobospan" id="kobo.694.1"> class, which will allow us to test the various solutions for the </span><span><em class="italic"><span class="kobospan" id="kobo.695.1">CartPole</span></em></span><span><span class="kobospan" id="kobo.696.1"> challenge:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.697.1">
cartPole = cart_pole.</span><strong class="bold1"><span class="kobospan1" id="kobo.698.1">CartPole</span></strong><span class="kobospan1" id="kobo.699.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.700.1">Next, we set the upper and lower boundaries for the float values we will search for. </span><span class="kobospan" id="kobo.700.2">Since all of our values represent weights and biases within a neural network, this range should be between -1.0 and 1.0 in </span><span><span class="kobospan" id="kobo.701.1">every dimension:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.702.1">
BOUNDS_LOW, BOUNDS_HIGH = -1.0, 1.0</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.703.1">As you may recall, our goal in this challenge is to </span><em class="italic"><span class="kobospan" id="kobo.704.1">maximize</span></em><span class="kobospan" id="kobo.705.1"> the score – how long we can keep the pole balanced. </span><span class="kobospan" id="kobo.705.2">To do so, we define a single objective, maximizing the </span><span><span class="kobospan" id="kobo.706.1">fitness strategy:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.707.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.708.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.709.1">", base.Fitness, weights=(1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.710.1">Now, we need to create a helper function to create random real numbers that are uniformly distributed within a given range. </span><span class="kobospan" id="kobo.710.2">This function assumes that the range is the same for every dimension, as is the case in </span><span><span class="kobospan" id="kobo.711.1">our solution:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.712.1">
def randomFloat(low, up):
    return [random.</span><strong class="bold1"><span class="kobospan1" id="kobo.713.1">uniform</span></strong><span class="kobospan1" id="kobo.714.1">(l, u) for l, u in zip([</span><strong class="bold1"><span class="kobospan1" id="kobo.715.1">low</span></strong><span class="kobospan1" id="kobo.716.1">] * \ 
        NUM_OF_PARAMS, [</span><strong class="bold1"><span class="kobospan1" id="kobo.717.1">up</span></strong><span class="kobospan1" id="kobo.718.1">] * NUM_OF_PARAMS)]</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.719.1">Now, we use this function to create an operator that randomly returns a list of floats in </span><a id="_idIndexMarker678" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.720.1">the desired range that we </span><span><span class="kobospan" id="kobo.721.1">set earlier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.722.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.723.1">attrFloat</span></strong><span class="kobospan1" id="kobo.724.1">", randomFloat, BOUNDS_LOW, 
    BOUNDS_HIGH)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.725.1">This is followed by an operator that fills up an individual instance using the </span><span><span class="kobospan" id="kobo.726.1">preceding operator:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.727.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.728.1">individualCreator</span></strong><span class="kobospan1" id="kobo.729.1">",
                 tools.initIterate,
                 creator.Individual,
                 toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.730.1">attrFloat</span></strong><span class="kobospan1" id="kobo.731.1">)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.732.1">Then, we instruct the genetic algorithm to use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.733.1">getScore()</span></strong><span class="kobospan" id="kobo.734.1"> method of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.735.1">CartPole</span></strong><span class="kobospan" id="kobo.736.1"> instance for </span><span><span class="kobospan" id="kobo.737.1">fitness evaluation.</span></span><p class="calibre3"><span class="kobospan" id="kobo.738.1">As a reminder, the </span><strong class="source-inline"><span class="kobospan" id="kobo.739.1">getScore()</span></strong><span class="kobospan" id="kobo.740.1"> method, which we described in the previous subsection, initiates an episode of the </span><em class="italic"><span class="kobospan" id="kobo.741.1">CartPole</span></em><span class="kobospan" id="kobo.742.1"> environment. </span><span class="kobospan" id="kobo.742.2">During this episode, the cart is controlled by a single-hidden layer MLP. </span><span class="kobospan" id="kobo.742.3">The weight and bias values of this MLP are populated by the list of floats representing the current solution. </span><span class="kobospan" id="kobo.742.4">Throughout the episode, the MLP dynamically maps the observation values of the environment to an action of </span><em class="italic"><span class="kobospan" id="kobo.743.1">right</span></em><span class="kobospan" id="kobo.744.1"> or </span><em class="italic"><span class="kobospan" id="kobo.745.1">left</span></em><span class="kobospan" id="kobo.746.1">. </span><span class="kobospan" id="kobo.746.2">Once the episode is done, the score is set to the total reward, which equates to the number of time steps that the MLP was able to keep the pole balanced – the higher, </span><span><span class="kobospan" id="kobo.747.1">the better:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.748.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.749.1">score</span></strong><span class="kobospan1" id="kobo.750.1">(individual):
    return cartPole.</span><strong class="bold1"><span class="kobospan1" id="kobo.751.1">getScore</span></strong><span class="kobospan1" id="kobo.752.1">(individual),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.753.1">evaluate</span></strong><span class="kobospan1" id="kobo.754.1">", score)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.755.1">It’s time to choose our genetic operators. </span><span class="kobospan" id="kobo.755.2">Once again, we’ll use </span><em class="italic"><span class="kobospan" id="kobo.756.1">tournament selection</span></em><span class="kobospan" id="kobo.757.1"> with a tournament size of 2 as our </span><em class="italic"><span class="kobospan" id="kobo.758.1">selection</span></em><span class="kobospan" id="kobo.759.1"> operator. </span><span class="kobospan" id="kobo.759.2">Since our solution </span><a id="_idIndexMarker679" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.760.1">representation is a list of floats in a given range, we’ll use the specialized </span><em class="italic"><span class="kobospan" id="kobo.761.1">continuous bounded crossover</span></em><span class="kobospan" id="kobo.762.1"> and </span><em class="italic"><span class="kobospan" id="kobo.763.1">mutation</span></em><span class="kobospan" id="kobo.764.1"> operators provided by the DEAP framework – </span><strong class="source-inline1"><span class="kobospan" id="kobo.765.1">cxSimulatedBinaryBounded</span></strong><span class="kobospan" id="kobo.766.1"> and </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.767.1">mutPolynomialBounded</span></strong></span><span><span class="kobospan" id="kobo.768.1">, respectively:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.769.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.770.1">select</span></strong><span class="kobospan1" id="kobo.771.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.772.1">mate</span></strong><span class="kobospan1" id="kobo.773.1">",
                 tools.cxSimulatedBinaryBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.774.1">mutate</span></strong><span class="kobospan1" id="kobo.775.1">",
                 tools.mutPolynomialBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR,
                 indpb=1.0/NUM_OF_PARAMS)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.776.1">And, as usual, we use the </span><em class="italic"><span class="kobospan" id="kobo.777.1">elitist approach</span></em><span class="kobospan" id="kobo.778.1">, where the HOF members – the current best individuals – are always passed untouched to the </span><span><span class="kobospan" id="kobo.779.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.780.1">
population, logbook = elitism.</span><strong class="bold1"><span class="kobospan1" id="kobo.781.1">eaSimpleWithElitism</span></strong><span class="kobospan1" id="kobo.782.1">(
    population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.783.1">After the run, we print the best solution and save it so that we can animate it, using </span><a id="_idIndexMarker680" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.784.1">the replay capability we built into the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.785.1">MountainCar</span></strong></span><span><span class="kobospan" id="kobo.786.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.787.1">
best = hof.items[0]
print("Best Solution = ", best)
print("Best Score = ", best.fitness.values[0])
cartPole.</span><strong class="bold1"><span class="kobospan1" id="kobo.788.1">saveParams</span></strong><span class="kobospan1" id="kobo.789.1">(best)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.790.1">In addition, we will run 100 consecutive episodes using our best individual, randomly initiating the CartPole problem each time, so each episode starts from a slightly different starting condition and can potentially yield a different result. </span><span class="kobospan" id="kobo.790.2">We will then calculate the average of all </span><span><span class="kobospan" id="kobo.791.1">the results:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.792.1">
scores = []
for test in range(</span><strong class="bold1"><span class="kobospan1" id="kobo.793.1">100</span></strong><span class="kobospan1" id="kobo.794.1">):
    scores.append(cart_pole.</span><strong class="bold1"><span class="kobospan1" id="kobo.795.1">CartPole()</span></strong><span class="kobospan1" id="kobo.796.1">.getScore(best))
print("scores = ", scores)
print("Avg. </span><span class="kobospan1" id="kobo.796.2">score = ", sum(scores) / len(scores))</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.797.1">It is time to find out how well we did in this challenge. </span><span class="kobospan" id="kobo.797.2">By running the algorithm for 10 generations with a population size of 30, we get the </span><span><span class="kobospan" id="kobo.798.1">following outcome:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.799.1">
gen     nevals  max     avg
0       30      68      14.4333
1       26      77      21.7667
...
</span><span class="kobospan1" id="kobo.799.2">4       27      381     57.2667
5       26      500     105.733
...
</span><span class="kobospan1" id="kobo.799.3">9       22      500     207.133
10      26      500     293.267
Best Solution =  [-0.7441543221198176, 0.34598771744315737, -0.4221171254602347, ...
</span><span class="kobospan1" id="kobo.799.4">Best Score =  500.0</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.800.1">From the preceding output, we can see that, after just five generations, the best solution(s) reached the maximum score of 500, balancing the pole for the entire </span><span><span class="kobospan" id="kobo.801.1">episode’s time.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.802.1">Looking </span><a id="_idIndexMarker681" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.803.1">at the results of our additional test, it seems that all 100 tests ended with a perfect score </span><span><span class="kobospan" id="kobo.804.1">of 500:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.805.1">
Running 100 episodes using the best solution...
</span><span class="kobospan1" id="kobo.805.2">scores = [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, ... </span><span class="kobospan1" id="kobo.805.3">, 500.0]
Avg. </span><span class="kobospan1" id="kobo.805.4">score = 500.0</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.806.1">As we mentioned previously, each of these 100 runs is done with a slightly different random starting point. </span><span class="kobospan" id="kobo.806.2">However, the controller is powerful enough to balance the pole for the entire run, each and every time. </span><span class="kobospan" id="kobo.806.3">To watch the controller in action, we can play a CartPole episode – or several episodes – with the saved results by launching the </span><strong class="source-inline"><span class="kobospan" id="kobo.807.1">cart_pole</span></strong><span class="kobospan" id="kobo.808.1"> program. </span><span class="kobospan" id="kobo.808.2">The animation illustrates how the controller dynamically responds to the pole’s movement by applying actions that keep the pole balanced on the cart for the </span><span><span class="kobospan" id="kobo.809.1">entire episode.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.810.1">If you would like to contrast these results with less-than-perfect ones, you are encouraged </span><a id="_idIndexMarker682" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.811.1">to repeat this experiment with three (or even two) nodes in the hidden layer instead of four – just change the </span><strong class="source-inline"><span class="kobospan" id="kobo.812.1">HIDDEN_LAYER</span></strong><span class="kobospan" id="kobo.813.1"> constant value accordingly in the </span><strong class="source-inline"><span class="kobospan" id="kobo.814.1">CartPole</span></strong><span class="kobospan" id="kobo.815.1"> class. </span><span class="kobospan" id="kobo.815.2">Alternatively, you can reduce the number of generations and/or population size of the </span><span><span class="kobospan" id="kobo.816.1">genetic algorithm.</span></span></p>
<h1 id="_idParaDest-239" class="calibre5"><a id="_idTextAnchor291" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.817.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.818.1">In this chapter, you were introduced to the basic concepts of </span><strong class="bold"><span class="kobospan" id="kobo.819.1">reinforcement learning</span></strong><span class="kobospan" id="kobo.820.1">. </span><span class="kobospan" id="kobo.820.2">After getting acquainted with the </span><strong class="bold"><span class="kobospan" id="kobo.821.1">Gymnasium</span></strong><span class="kobospan" id="kobo.822.1"> toolkit, you were presented with the </span><em class="italic"><span class="kobospan" id="kobo.823.1">MountainCar</span></em><span class="kobospan" id="kobo.824.1"> challenge, where a car needs to be controlled in a way that will allow it to climb the taller of two mountains. </span><span class="kobospan" id="kobo.824.2">After solving this challenge using genetic algorithms, you were introduced to the next challenge, </span><em class="italic"><span class="kobospan" id="kobo.825.1">CartPole</span></em><span class="kobospan" id="kobo.826.1">, where a cart is to be precisely controlled to keep an upright pole balanced. </span><span class="kobospan" id="kobo.826.2">We were able to solve this challenge by combining the power of a neural network-based controller with genetic </span><span><span class="kobospan" id="kobo.827.1">algorithm-guided training.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.828.1">While we have primarily focused on problems involving structured numerical data thus far, the next chapter will shift its focus to applications of genetic algorithms in </span><strong class="bold"><span class="kobospan" id="kobo.829.1">Natural Language Processing</span></strong><span class="kobospan" id="kobo.830.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.831.1">NLP</span></strong><span class="kobospan" id="kobo.832.1">), a branch of machine learning that empowers computers to comprehend, interpret, and process </span><span><span class="kobospan" id="kobo.833.1">human language.</span></span></p>
<h1 id="_idParaDest-240" class="calibre5"><a id="_idTextAnchor292" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.834.1">Further reading</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.835.1">For more information, refer to the </span><span><span class="kobospan" id="kobo.836.1">following resources:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.837.1">Mastering Reinforcement Learning with Python</span></em><span class="kobospan" id="kobo.838.1">, Enes Bilgin, December </span><span><span class="kobospan" id="kobo.839.1">18 2020</span></span></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.840.1">Deep Reinforcement Learning Hands-On, 2nd Edition</span></em><span class="kobospan" id="kobo.841.1">, Maksim Lapan, January </span><span><span class="kobospan" id="kobo.842.1">21, 2020</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.843.1">Gymnasium documentation:</span></span></li>
<li class="calibre11"><a href="https://gymnasium.farama.org/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.844.1">https://gymnasium.farama.org/</span></span></a></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.845.1">OpenAI Gym</span></em><span class="kobospan" id="kobo.846.1"> (white paper), Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, </span><span><span class="kobospan" id="kobo.847.1">Wojciech Zaremba:</span></span></li>
<li class="calibre11"><a href="https://arxiv.org/abs/1606.01540" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.848.1">https://arxiv.org/abs/1606.01540</span></span></a></li>
</ul>
</div>
</body></html>