["```py\npip install gymnasium\n```", "```py\npip install pygame\n```", "```py\nimport gymnasium as gym\nenv = gym.reset() method, as shown in the following code snippet:\n\n```", "```py\nobservation, reward, terminated, truncated, info = \\\n    env.observation object, which describes the new state and the float reward value that represent the interim reward, this method returns the following values:\n\n*   **terminated**: A Boolean that turns **true** when the current run (also called *episode*) reaches the terminal state – for example, the agent lost a life, or successfully completed a task.\n*   **truncated**: A Boolean that can be used to end the episode prematurely before a terminal state is reached – for example, due to a time limit, or if the agent went out of bounds.\n*   **info**: A dictionary containing optional, additional information that may be useful for debugging. However, it should not be used by the agent for learning.\n\nAt any point in time, the environment can be rendered for visual presentation, as follows:\n\n```", "```py\nenv.close()\n```", "```py\n[-1.0260268, -0.03201975]\n```", "```py\n[0, 1, 2, 0, 0, 1, 2, 2, 1, ... , 0, 2, 1, 1]\n```", "```py\n    car = mountain_car.MountainCar(RANDOM_SEED)\n    ```", "```py\n    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n    ```", "```py\n    toolbox.register(\"zeroOneOrTwo\", random.randint, 0, 2)\n    ```", "```py\n    toolbox.register(\"individualCreator\",\n                     tools.initRepeat,\n                     creator.Individual,\n                     toolbox.zeroOneOrTwo,\n                     len(car))\n    ```", "```py\n    def carScore(individual):\n        return car.getScore(individual),\n    toolbox.register(\"evaluate\", carScore)\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\", tools.cxTwoPoint)\n    toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, \n        indpb=1.0/len(car))\n    ```", "```py\n    population, logbook = elitism.eaSimpleWithElitism(population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\n    best = hof.items[0]\n    print(\"Best Solution = \", best)\n    print(\"Best Fitness = \", best.fitness.values[0])\n    car.saveActions(best)\n    ```", "```py\n    gen nevals min avg\n    0       100     0.708709        1.03242\n    1       78      0.708709        0.975704\n    ...\n    47      71      0.000170529     0.0300455\n    48      74      4.87566e-05     0.0207197\n    49      75      -0.005          0.0150622\n    50      77      -0.005          0.0121327\n    ...\n    56      77      -0.02           -0.00321379\n    57      74      -0.025          -0.00564184\n    ...\n    79      76      -0.035          -0.0342\n    80      76      -0.035          -0.03425\n    Best Solution =  [1, 0, 2, 1, 1, 2, 0, 2, 2, 2, 0, ... , 2, 0, 1, 1, 1, 1, 1, 0]\n    Best Fitness =  -0.035\n    ```", "```py\n[0.9505049282421143, -0.8068797228337171, -0.45488246459260073, ... ,0.6720551701599038]\n```", "```py\n    cartPole = cart_pole.CartPole(RANDOM_SEED)\n    ```", "```py\n    BOUNDS_LOW, BOUNDS_HIGH = -1.0, 1.0\n    ```", "```py\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    ```", "```py\n    def randomFloat(low, up):\n        return [random.uniform(l, u) for l, u in zip([low] * \\ \n            NUM_OF_PARAMS, [up] * NUM_OF_PARAMS)]\n    ```", "```py\n    toolbox.register(\"attrFloat\", randomFloat, BOUNDS_LOW, \n        BOUNDS_HIGH)\n    ```", "```py\n    toolbox.register(\"individualCreator\",\n                     tools.initIterate,\n                     creator.Individual,\n                     toolbox.getScore() method, which we described in the previous subsection, initiates an episode of the *CartPole* environment. During this episode, the cart is controlled by a single-hidden layer MLP. The weight and bias values of this MLP are populated by the list of floats representing the current solution. Throughout the episode, the MLP dynamically maps the observation values of the environment to an action of *right* or *left*. Once the episode is done, the score is set to the total reward, which equates to the number of time steps that the MLP was able to keep the pole balanced – the higher, the better:\n\n    ```", "```py\n\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\",\n                     tools.cxSimulatedBinaryBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR)\n    toolbox.register(\"mutate\",\n                     tools.mutPolynomialBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR,\n                     indpb=1.0/NUM_OF_PARAMS)\n    ```", "```py\n    population, logbook = elitism.eaSimpleWithElitism(\n        population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\n    best = hof.items[0]\n    print(\"Best Solution = \", best)\n    print(\"Best Score = \", best.fitness.values[0])\n    cartPole.saveParams(best)\n    ```", "```py\n    scores = []\n    for test in range(100):\n        scores.append(cart_pole.CartPole().getScore(best))\n    print(\"scores = \", scores)\n    print(\"Avg. score = \", sum(scores) / len(scores))\n    ```", "```py\ngen     nevals  max     avg\n0       30      68      14.4333\n1       26      77      21.7667\n...\n4       27      381     57.2667\n5       26      500     105.733\n...\n9       22      500     207.133\n10      26      500     293.267\nBest Solution =  [-0.7441543221198176, 0.34598771744315737, -0.4221171254602347, ...\nBest Score =  500.0\n```", "```py\nRunning 100 episodes using the best solution...\nscores = [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, ... , 500.0]\nAvg. score = 500.0\n```", "```py\n\n```", "```py\n\n```"]