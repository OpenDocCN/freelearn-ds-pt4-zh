- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning with Genetic Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will demonstrate how genetic algorithms can be applied to
    **reinforcement learning** – a fast-developing branch of machine learning that
    is capable of tackling complex tasks. We will do this by solving two benchmark
    environments from the *Gymnasium* (formerly *OpenAI Gym*) toolkit. We will start
    by providing an overview of reinforcement learning, followed by a brief introduction
    to *Gymnasium*, a toolkit that can be used to compare and develop reinforcement
    learning algorithms, as well as a description of its Python-based interface. Then,
    we will explore two Gymnasium environments, *MountainCar* and *CartPole*, and
    develop genetic algorithm-based programs to solve the challenges they present.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic concepts of reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Becoming familiar with the *Gymnasium* project and its shared interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using genetic algorithms to solve the *Gymnasium* *MountainCar* environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using genetic algorithms, in combination with a neural network, to solve the
    *Gymnasium* *CartPole* environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start this chapter by outlining the basic concepts of reinforcement
    learning. If you are a seasoned data scientist, feel free to skip this introductory
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use Python 3 with the following supporting libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**deap**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**numpy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gymnasium** – *introduced in* *this chapter*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pygame** – *introduced in* *this chapter*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you use the **requirements.txt** file we provide (See [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: The *Gymnasium* environments that will be used in this chapter are *MountainCar-v0*
    ([https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/))
    and *CartPole-v1* ([https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)).
  prefs: []
  type: TYPE_NORMAL
- en: The programs that will be used in this chapter can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_10).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action: [https://packt.link/OEBOd](https://packt.link/OEBOd).'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered several topics related to machine learning
    and focused on **supervised learning** tasks. While supervised learning is immensely
    important and has a lot of real-life applications, reinforcement learning currently
    seems to be the most exciting and promising branch of machine learning. The reasons
    for this excitement include the complex, everyday-life-like tasks that reinforcement
    learning has the potential to handle. In March 2016, *AlphaGo*, a reinforcement
    learning-based system specializing in the highly complex game of *Go*, was able
    to defeat the person considered to be the greatest Go player of the past decade
    in a competition that was widely covered by the media.
  prefs: []
  type: TYPE_NORMAL
- en: While supervised learning requires **labeled data** for training – in other
    words, pairs of inputs and matching outputs – reinforcement learning does not
    present immediate right/wrong feedback; instead, it provides an environment where
    a longer-term, cumulative **reward** is sought after. This means that, sometimes,
    an algorithm will need to take a momentary step backward to eventually reach a
    longer-term goal, as we will demonstrate in the first example of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main components of reinforcement learning task are the **environment**
    and the **agent**, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Reinforcement learning represented as an interaction between
    the agent and the environment](img/B20851_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Reinforcement learning represented as an interaction between the
    agent and the environment'
  prefs: []
  type: TYPE_NORMAL
- en: The *agent* represents an algorithm that interacts with the *environment* and
    attempts to solve a given problem by maximizing the cumulative reward.
  prefs: []
  type: TYPE_NORMAL
- en: The exchange that takes place between the agent and the environment can be expressed
    as a series of **steps**. In each step, the environment presents the agent with
    a certain **state** (*s*), also called an observation. The agent, in turn, performs
    an **action** (*a*). The environment responds with a new state (*s’*), as well
    as an intermediate reward value (*R*). This exchange repeats until a certain stopping
    condition is met. The agent’s goal is to maximize the sum of the reward values
    that are collected along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the simplicity of this formulation, it can be used to describe extremely
    complex tasks and situations, which makes reinforcement learning applicable to
    a wide range of applications, such as game theory, healthcare, control systems,
    supply-chain automation, and operations research.
  prefs: []
  type: TYPE_NORMAL
- en: The versatility of genetic algorithms will be demonstrated once more in this
    chapter, since we will harness them to assist with reinforcement learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms and reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Various dedicated algorithms have been developed to carry out reinforcement
    learning tasks – *Q-Learning*, *SARSA*, and *DQN*, to name a few. However, since
    reinforcement learning tasks involve maximizing a long-term reward, we can think
    of them as optimization problems. As we have seen throughout this book, genetic
    algorithms can be used to solve optimization problems of various types. Therefore,
    genetic algorithms can be utilized for reinforcement learning as well, and in
    several different ways – two of them will be demonstrated in this chapter. In
    the first case, our genetic algorithm-based solution will directly provide the
    agent’s optimal series of actions. In the second case, it will supply the optimal
    parameters for the neural controller that provides these actions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start applying genetic algorithms to reinforcement learning tasks,
    let’s get acquainted with the toolkit that will be used to conduct these tasks
    – **Gymnasium**.
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Gymnasium* ([https://gymnasium.farama.org/](https://gymnasium.farama.org/))
    – a fork and the official successor of *OpenAI Gym* – is an open source library
    that was written to allow access to a standardized set of reinforcement learning
    tasks. It provides a toolkit that can be used to compare and develop reinforcement
    learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gymnasium* consists of a collection of environments, all presenting a common
    interface called `env`. This interface decouples the various environments from
    the agents, which can be implemented in any way we like – the only requirement
    from the agent is that it can interact with the environment(s) via the `env` interface.
    This will be described in the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic package, `gymnasium`, provides access to several environments and
    can be installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To allow us to render and animate our test environments, the *PyGame* library
    needs to be installed as well. This can be done using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Several other packages are available, such as “Atari,” “Box2D,” and “MuJoCo,”
    that provide access to numerous and diverse additional environments. Some of these
    packages have system dependencies and may only be available for certain operating
    systems. More information is available at [https://github.com/Farama-Foundation/Gymnasium#installation](https://github.com/Farama-Foundation/Gymnasium#installation).
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection describes interaction with the `env` interface.
  prefs: []
  type: TYPE_NORMAL
- en: The env interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create an environment, we need to use the `make()` method and the name of
    the desired environment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: observation, info = env.observation object, describing the initial state of
    the environment, and a dictionary, info, that may contain auxiliary information
    complementing observation. The content of observation is environment-dependent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conforming with the reinforcement learning cycle that we described in the previous
    section, the ongoing interaction with the environment consists of sending it an
    *action* and, in return, receiving an *intermediate reward* and a new *state*.
    This is implemented by the `step()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: env.render_mode, which can be set when the environment is created. A value of
    "human", for example, will result in the environment being continuously rendered
    in the current display or terminal, while the default value of None will result
    in no rendering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, an environment can be closed to invoke any necessary cleanup, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If this method isn’t called, the environment will automatically close itself
    the next time Python runs its *garbage collection* process (the process of identifying
    and freeing memory that is no longer in use by the program), or when the program
    exits.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Detailed information about the env interface is available at [https://gymnasium.farama.org/api/env/](https://gymnasium.farama.org/api/env/).
  prefs: []
  type: TYPE_NORMAL
- en: The complete cycle of interaction with the environment will be demonstrated
    in the next section, where we’ll encounter our first gymnasium challenge – the
    *MountainCar* environment.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the MountainCar environment
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MountainCar-v0` environment simulates a car on a one-dimensional track,
    situated between two hills. The simulation starts with the car placed between
    the hills, as shown in the following rendered output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: The MountainCar simulation – the starting point](img/B20851_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: The MountainCar simulation – the starting point'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to get the car to climb up the taller hill – the one on the right
    – and ultimately hit the flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: The MountainCar simulation – the car climbing the hill on the
    right](img/B20851_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: The MountainCar simulation – the car climbing the hill on the
    right'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simulation is set up with the car’s engine being too weak to directly climb
    the taller hill. The only way to reach the goal is to drive the car back and forth
    until enough momentum is built for climbing. Climbing the left hill can help to
    achieve this goal, as reaching the left peak will bounce the car back to the right,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4: The MountainCar simulation – the car bouncing off the hill on
    the left](img/B20851_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: The MountainCar simulation – the car bouncing off the hill on
    the left'
  prefs: []
  type: TYPE_NORMAL
- en: This simulation is a great example where intermediate loss (moving left) can
    help to achieve the ultimate goal (going all the way to the right).
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected *action* value in this simulation is an integer with of one of
    the three following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Push left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: No push'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: Push right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `observation` object contains two floats that describe the position and
    the velocity of the car, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `reward` value is -1 for each time step, until the goal (located
    at position 0.5) is reached. The simulation will stop after 200 steps if the goal
    is not reached beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this environment is to reach the flag placed on top of the right
    hill as quickly as possible, and therefore, the agent is penalised with a reward
    of -1 for each timestep used.
  prefs: []
  type: TYPE_NORMAL
- en: More information about the *MountainCar-v0* environment can be found at
  prefs: []
  type: TYPE_NORMAL
- en: '[https://gymnasium.farama.org/environments/classic_control/mountain_car/](https://gymnasium.farama.org/environments/classic_control/mountain_car/).'
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation, we will attempt to hit the flag using the least amount
    of steps, as we apply a sequence of preselected actions from a fixed starting
    position. To find a sequence of actions that will get the car to climb the tall
    hill and hit the flag, we will craft a genetic algorithm-based solution. As usual,
    we will start by defining what a candidate solution for this challenge will look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation
  prefs: []
  type: TYPE_NORMAL
- en: 'Since *MountainCar* is controlled by a sequence of actions, each with a value
    of 0 (push left), 1 (no push), or 2 (push right), and there can be up to 200 actions
    in a single episode, one obvious way to represent a candidate solution is by using
    a list of length 200, containing values of 0, 1, or 2\. An example of this is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The values in the list will be used as actions to control the car and, hopefully,
    drive it to the flag. If the car made it to the flag in less than 200 steps, the
    last few items in the list will not be used.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to determine how to evaluate a given solution of this form.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the solution
  prefs: []
  type: TYPE_NORMAL
- en: 'While evaluating a given solution, or when comparing two solutions, it is apparent
    that the reward value alone may not provide us with sufficient information. With
    the way the reward is defined, its value will always be -200 if we don’t hit the
    flag. When we compare two candidate solutions that don’t hit the flag, we would
    still like to know which one got closer to it and consider it a better solution.
    Therefore, we will use the final position of the car, in addition to the reward
    value, to determine the score of the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: If the car did not hit the flag, the score will be the distance from the flag.
    Therefore, we will look for a solution that minimizes the score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the car hits the flag, the base score will be zero, and from that, we deduct
    an additional value based on how many unused steps were left, making the score
    negative. Since we are looking for the lowest score possible, this arrangement
    will encourage solutions to hit the flag using the smallest possible amount of
    actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This score evaluation procedure is implemented by the `MountainCar` class, which
    is explored in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The Python problem representation
  prefs: []
  type: TYPE_NORMAL
- en: To encapsulate the MountainCar challenge, we’ve created a Python class called
    `MountainCar`. This class is contained in the `mountain_car.py` file, which is
    located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/mountain_car.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is initialized with a random seed and provides the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**getScore(actions)**: Calculates the score of a given solution, represented
    by the list of actions. The score is calculated by initiating an episode of the
    **MountainCar** environment and running it with the provided actions, and this
    can be negative if we hit the target with fewer than 200 steps. The lower the
    score is, the better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**saveActions(actions)**: Saves a list of actions to a file using **pickle**
    (Python’s object serialization and deserialization module).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replaySavedActions()**: Deserializes the last saved list of actions and replays
    it using the **replay** method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replay(actions)**: Renders the environment using the “human” **render_mode**
    and replays the list of actions given to it, visualizing a given solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main method of the class can be used after a solution has been found, serialized,
    and saved using the `saveActions()` method. The main method will initialize the
    class and call `replaySavedActions()` to render and animate the last saved solution.
  prefs: []
  type: TYPE_NORMAL
- en: We typically use the main method to animate the best solution that’s found by
    the genetic algorithm-based program. This will be explored in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms solution
  prefs: []
  type: TYPE_NORMAL
- en: To tackle the *MountainCar* challenge using the genetic algorithms approach,
    we’ve created a Python program, `01_solve_mountain_car.py`, which is located at
    [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/01_solve_mountain_car.py).
  prefs: []
  type: TYPE_NORMAL
- en: Since the solution representation we chose for this problem is a list containing
    the 0, 1, or 2 integer values, this program bears resemblance to the one we used
    to solve the knapsack 0-1 problem in [*Chapter 4*](B20851_04.xhtml#_idTextAnchor155),
    *Combinatorial Optimization*, where solutions were represented as lists with the
    values 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how to create the main parts of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating an instance of the **MountainCar** class, which will allow
    us to score the various solutions for the *MountainCar* challenge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '2.  Since our goal is to minimize the score – in other words, hit the flag
    with the minimum step count, or get as close as possible to the flag – we define
    a single objective, minimizing the fitness strategy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '3.  Now, we need to create a toolbox operator that can produce one of the three
    allowed action values – 0, 1, or 2:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '4.  This is followed by an operator that fills up an individual instance with
    these values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '5.  Then, we instruct the genetic algorithm to use the `getScore()` method,
    which we described in the previous subsection, initiates an episode of the *MountainCar*
    environment and uses the given individual – a list of actions – as the inputs
    to the environment until the episode is done. Then, it evaluates the score – the
    lower the better – according to the final location of the car. If the car hit
    the flag, the score can even be negative, based on the number of unused steps
    left:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 6.  As for the genetic operators, we start with the usual *tournament selection*,
    with a tournament size of 2\. Since our solution representation is a list of the
    0, 1, or 2 integer values, we can use the *two-point crossover* operator, just
    like we did when the solution was represented by a list of 0 and 1 values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For *mutation*, however, rather than the *FlipBit* operator, which is typically
    used for the binary case, we need to use the *UniformInt* operator, which is used
    for a range of integer values, and configure it for the range of 0-2:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '7.  In addition, we continue to use the *elitist approach*, where the **hall
    of fame** (**HOF**) members – the current best individuals – are always passed
    untouched to the next generation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '8.  After the run, we print the best solution and save it so that we can later
    animate it, using the replay capability we built into the **MountainCar** class:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the algorithm for 80 generations and with a population size of 100,
    we get the following outcome:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the preceding output, we can see that, after about 50 generations, the
    best solution(s) started hitting the flag, producing score values of zero or less.
    From hereon, the best solutions hit the flag in fewer steps, thereby producing
    increasingly negative score values.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we already mentioned, the best solution was saved at the end of the run,
    and we can now replay it by running the `mountain_car` program. This replay illustrates
    how the actions of our solution drive the car back and forth between the two peaks,
    climbing higher each time, until the car is able to climb the lower hill on the
    left. Then, it bounces back, which means we have enough momentum to continue climbing
    the higher hill on the right, ultimately hitting the flag, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5: The MountainCar simulation – the car reaching the goal](img/B20851_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: The MountainCar simulation – the car reaching the goal'
  prefs: []
  type: TYPE_NORMAL
- en: While solving it was a lot of fun, the way this environment is set up did not
    require us to dynamically interact with it. We were able to climb the hill using
    a sequence of actions that our algorithm put together, based on the initial location
    of the car. In contrast, the next environment we are about to tackle – named *CartPole*
    – requires us to dynamically calculate our action at any time step, based upon
    the latest observation produced. Read on to find out how we can make this work.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the CartPole environment
  prefs: []
  type: TYPE_NORMAL
- en: The *CartPole-v1* environment simulates a balancing act of a pole, hinged at
    its bottom to a cart, which moves left and right along a track. Balancing the
    pole upright is carried out by applying to the cart 1 unit of force – to the right
    or the left – at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pole, acting as a pendulum in this environment, starts upright within a
    small random angle, as shown in the following rendered output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: The CartPole simulation – the starting point](img/B20851_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The CartPole simulation – the starting point'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to keep the pendulum from falling over to either side for as long
    as possible – that is, up to 500 time steps. For every time step that the pole
    remains upright, we get a reward of +1, so the maximum total reward is 500\. The
    episode will end prematurely if one of the following occurs during the run:'
  prefs: []
  type: TYPE_NORMAL
- en: The angle of the pole from the vertical position exceeds 15 degrees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cart’s distance from the center exceeds 2.4 units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, the total reward in these cases will be smaller than 500.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected `action` value in this simulation is an integer of one of the
    two following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Push the cart to the left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Push the cart to the right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `observation` object contains four floats that hold the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cart position**, between -2.4 and 2.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cart velocity**, between -Inf and Inf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pole angle**, between -0.418 rad (-24°) and 0.418 rad (24°)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pole angular velocity**, between -Inf and Inf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, we could have an `observation` of `[0.33676587, 0.3786464, -``0.00170739,
    -0.36586074]`.
  prefs: []
  type: TYPE_NORMAL
- en: More information about the CartPole-v1 environment is available at [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/).
  prefs: []
  type: TYPE_NORMAL
- en: In our proposed solution, we will use these values as inputs at every time step
    to determine which action to take. We will do this with the aid of a neural network-based
    controller. This is described in more detail in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the CartPole with a neural network
  prefs: []
  type: TYPE_NORMAL
- en: To successfully carry out the *CartPole* challenge, we would like to dynamically
    respond to the changes in the environment. For example, when the pole starts leaning
    in one direction, we should probably move the cart in that direction but possibly
    stop pushing when it starts to stabilize. So, the reinforcement learning task
    here can be thought of as teaching a controller to balance the pole by mapping
    the four available inputs – cart position, cart velocity, pole angle, and pole
    velocity – to the appropriate action at each time step. How can we implement such
    mapping?
  prefs: []
  type: TYPE_NORMAL
- en: 'One good way to implement this mapping is by using a **neural network**. As
    we saw in [*Chapter 9*](B20851_09.xhtml#_idTextAnchor257), *Architecture Optimization
    of Deep Learning Networks*, a neural network, such as a **multilayer perceptron**
    (**MLP**), can implement complex mappings between its inputs and outputs. This
    mapping is done with the aid of the network parameters – namely, the *weights
    and biases* of the active nodes in the network, as well as the *transfer functions*
    that are implemented by these nodes. In our case, we will use a network with a
    single *hidden layer* of four nodes. In addition, the *input layer* consists of
    four nodes, one for each of the input values provided by the environment, while
    the *output layer* has a single node, since we only have one output value – the
    action to be taken. This network structure can be illustrated with the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7: The structure of the neural network that’s used to control the
    cart](img/B20851_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: The structure of the neural network that’s used to control the
    cart'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen already, the values of the weights and biases of a neural network
    are typically set during a process in which the network is trained. The interesting
    part is that, so far, we have only seen this kind of neural network being trained
    using the backpropagation algorithm while implementing *supervised learning* –
    that is, in each of the previous cases, we had a training set of inputs and matching
    outputs, and the network was trained to map each given input to its matching given
    output. Here, however, as we practice *reinforcement learning*, we don’t have
    such training information available. Instead, we only know how well the network
    did at the end of the episode. This means that instead of using the conventional
    training algorithms, we need a method that will allow us to find the best network
    parameters – weights and biases – based on the results that are obtained by running
    the environment’s episodes. This is exactly the kind of optimization that genetic
    algorithms are good at – finding a set of parameters that will give us the best
    results, as long as you have a way to evaluate and compare them. To do that, we
    need to figure out how to represent the network’s parameters, as well as how to
    evaluate a given set of those parameters. Both of these topics will be covered
    in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation and evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have decided to control the cart in the CartPole challenge using a
    neural network of the *MLP* type, the set of parameters that we will need to optimize
    are the network’s weights and biases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This layer does not participate in the network mapping; instead,
    it receives the input values and forwards them to every neuron in the next layer.
    Therefore, no parameters are needed for this layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: Each node in this layer is fully connected to each of the
    four inputs and, therefore, requires four weights in addition to a single bias
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The single node in this layer is connected to each of the
    four nodes in the hidden layer and, therefore, requires four weights in addition
    to a single bias value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In total, we have 20 weight values and 5 bias values we need to find, all of
    the `float` type. Therefore, each potential solution can be represented as a list
    of 25 `float` values, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating a given solution means creating our MLP with the correct dimensions
    – four inputs, a four-node hidden layer, and a single output – and assigning the
    weight and bias values from our list of floats to the various nodes. Then, we
    need to use this MLP as the controller for the cart pole during one episode. The
    resulting total reward of the episode is used as the score value for this solution.
    In contrast to the previous task, here, we aim to *maximize* the score that’s
    achieved. This score evaluation procedure is implemented by the `CartPole` class,
    which will be explored in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The Python problem representation
  prefs: []
  type: TYPE_NORMAL
- en: To encapsulate the *CartPole* challenge, we’ve created a Python class called
    `CartPole`. This class is contained in the `cart_pole.py` file, which is located
    at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/cart_pole.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is initialized with an optional random seed and provides the following
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**initMlp()**: Initializes an MLP *regressor* with the desired network architecture
    (layers) and network parameters (weights and biases), which are derived from the
    list of floats that represent a candidate solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**getScore()**: Calculates the score of a given solution, represented by the
    list of float-valued network parameters. This is done by creating a corresponding
    MLP regressor, initiating an episode of the *CartPole* environment, and running
    it with the MLP controlling the actions, all while using the observations as inputs.
    The higher the score is, the better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**saveParams()**: Serializes and saves a list of network parameters using **pickle**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replayWithSavedParams()**: Deserializes the latest saved list of network
    parameters and uses it to replay an episode using the **replay** method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**replay()**: Renders the environment and uses the given network parameters
    to replay an episode, visualizing a given solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main method of the class should be used after a solution has been serialized
    and saved, using the `saveParams()` method. The main method will initialize the
    class and call `replayWithSavedParams()` to render and animate the saved solution.
  prefs: []
  type: TYPE_NORMAL
- en: We will typically use the main method to animate the best solution that’s found
    by our genetic algorithm-driven solution, as explored in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: A genetic algorithm solution
  prefs: []
  type: TYPE_NORMAL
- en: To interact with the *CartPole* environment and solve it using a genetic algorithm,
    we’ve created a Python program, `02_solve_cart-pole.py`, which is located at [https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_10/02_solve_cart_pole.py).
  prefs: []
  type: TYPE_NORMAL
- en: Since we will use a list of floats to represent a solution – the network’s weights
    and biases – this program is very similar to the function optimization programs
    we looked at in [*Chapter 6*](B20851_06.xhtml#_idTextAnchor197), *Optimizing Continuous
    Functions*, such as the one we used for the *Eggholder* *function*’s optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe how to create the main parts of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating an instance of the **CartPole** class, which will allow
    us to test the various solutions for the *CartPole* challenge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '2.  Next, we set the upper and lower boundaries for the float values we will
    search for. Since all of our values represent weights and biases within a neural
    network, this range should be between -1.0 and 1.0 in every dimension:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '3.  As you may recall, our goal in this challenge is to *maximize* the score
    – how long we can keep the pole balanced. To do so, we define a single objective,
    maximizing the fitness strategy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '4.  Now, we need to create a helper function to create random real numbers
    that are uniformly distributed within a given range. This function assumes that
    the range is the same for every dimension, as is the case in our solution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '5.  Now, we use this function to create an operator that randomly returns a
    list of floats in the desired range that we set earlier:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '6.  This is followed by an operator that fills up an individual instance using
    the preceding operator:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'def score(individual):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return cartPole.getScore(individual),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: toolbox.register("evaluate", score)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '7.  It’s time to choose our genetic operators. Once again, we’ll use *tournament
    selection* with a tournament size of 2 as our *selection* operator. Since our
    solution representation is a list of floats in a given range, we’ll use the specialized
    *continuous bounded crossover* and *mutation* operators provided by the DEAP framework
    – **cxSimulatedBinaryBounded** and **mutPolynomialBounded**, respectively:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '8.  And, as usual, we use the *elitist approach*, where the HOF members – the
    current best individuals – are always passed untouched to the next generation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '9.  After the run, we print the best solution and save it so that we can animate
    it, using the replay capability we built into the **MountainCar** class:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '10.  In addition, we will run 100 consecutive episodes using our best individual,
    randomly initiating the CartPole problem each time, so each episode starts from
    a slightly different starting condition and can potentially yield a different
    result. We will then calculate the average of all the results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is time to find out how well we did in this challenge. By running the algorithm
    for 10 generations with a population size of 30, we get the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can see that, after just five generations, the
    best solution(s) reached the maximum score of 500, balancing the pole for the
    entire episode’s time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the results of our additional test, it seems that all 100 tests
    ended with a perfect score of 500:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned previously, each of these 100 runs is done with a slightly different
    random starting point. However, the controller is powerful enough to balance the
    pole for the entire run, each and every time. To watch the controller in action,
    we can play a CartPole episode – or several episodes – with the saved results
    by launching the `cart_pole` program. The animation illustrates how the controller
    dynamically responds to the pole’s movement by applying actions that keep the
    pole balanced on the cart for the entire episode.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to contrast these results with less-than-perfect ones, you
    are encouraged to repeat this experiment with three (or even two) nodes in the
    hidden layer instead of four – just change the `HIDDEN_LAYER` constant value accordingly
    in the `CartPole` class. Alternatively, you can reduce the number of generations
    and/or population size of the genetic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to the basic concepts of **reinforcement
    learning**. After getting acquainted with the **Gymnasium** toolkit, you were
    presented with the *MountainCar* challenge, where a car needs to be controlled
    in a way that will allow it to climb the taller of two mountains. After solving
    this challenge using genetic algorithms, you were introduced to the next challenge,
    *CartPole*, where a cart is to be precisely controlled to keep an upright pole
    balanced. We were able to solve this challenge by combining the power of a neural
    network-based controller with genetic algorithm-guided training.
  prefs: []
  type: TYPE_NORMAL
- en: While we have primarily focused on problems involving structured numerical data
    thus far, the next chapter will shift its focus to applications of genetic algorithms
    in **Natural Language Processing** (**NLP**), a branch of machine learning that
    empowers computers to comprehend, interpret, and process human language.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mastering Reinforcement Learning with Python*, Enes Bilgin, December 18 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Hands-On, 2nd Edition*, Maksim Lapan, January
    21, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gymnasium documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://gymnasium.farama.org/](https://gymnasium.farama.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI Gym* (white paper), Greg Brockman, Vicki Cheung, Ludwig Pettersson,
    Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1606.01540](https://arxiv.org/abs/1606.01540)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
