<html><head></head><body>
<div id="_idContainer135" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-241"><a id="_idTextAnchor293" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-242" class="calibre5"><a id="_idTextAnchor294" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Natural Language Processing</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter explores how genetic algorithms can enhance the performance of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">natural language processing</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">NLP</span></strong><span class="kobospan" id="kobo.7.1">) tasks while offering insights into their </span><span><span class="kobospan" id="kobo.8.1">underlying mechanisms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.9.1">The chapter begins by introducing the field of NLP and explaining the concept of </span><strong class="bold"><span class="kobospan" id="kobo.10.1">word embeddings</span></strong><span class="kobospan" id="kobo.11.1">. </span><span class="kobospan" id="kobo.11.2">We employ this technique to task a genetic algorithm with playing a </span><em class="italic"><span class="kobospan" id="kobo.12.1">Semantle</span></em><span class="kobospan" id="kobo.13.1">-like mystery-word game, challenging it to guess the </span><span><span class="kobospan" id="kobo.14.1">mystery word.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.15.1">Subsequently, we investigate </span><strong class="bold"><span class="kobospan" id="kobo.16.1">n-grams</span></strong><span class="kobospan" id="kobo.17.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.18.1">document classification</span></strong><span class="kobospan" id="kobo.19.1">. </span><span class="kobospan" id="kobo.19.2">We harness genetic algorithms to pinpoint a compact yet effective subset of features, shedding light on the </span><span><span class="kobospan" id="kobo.20.1">classifier’s operation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.21.1">By the end of this chapter, you will have achieved </span><span><span class="kobospan" id="kobo.22.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.23.1">Become familiar with the field of NLP and </span><span><span class="kobospan" id="kobo.24.1">its applications</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.25.1">Gained an understanding of the concept of word embeddings and </span><span><span class="kobospan" id="kobo.26.1">their importance</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.27.1">Implemented a mystery-word game using word embeddings and created a genetic algorithms-driven player to guess the </span><span><span class="kobospan" id="kobo.28.1">mystery word</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.29.1">Acquired knowledge about n-grams and their role in </span><span><span class="kobospan" id="kobo.30.1">document processing</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.31.1">Developed a process to significantly reduce the size of the feature set used for </span><span><span class="kobospan" id="kobo.32.1">message classification</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.33.1">Utilized a minimal feature set to gain insights into the </span><span><span class="kobospan" id="kobo.34.1">classifier’s operation</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.35.1">We will start this chapter with a quick overview of NLP. </span><span class="kobospan" id="kobo.35.2">If you are a seasoned data scientist, feel free to skip the </span><span><span class="kobospan" id="kobo.36.1">introductory section.</span></span></p>
<h1 id="_idParaDest-243" class="calibre5"><a id="_idTextAnchor295" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.37.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.38.1">In this chapter, we will be using Python 3 with the following </span><span><span class="kobospan" id="kobo.39.1">supporting libraries:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.40.1">deap</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.41.1">numpy</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.42.1">pandas</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.43.1">matplotlib</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.44.1">seaborn</span></strong></span></li>
<li class="calibre11"><span><strong class="source-inline1"><span class="kobospan" id="kobo.45.1">scikit-learn</span></strong></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.46.1">gensim</span></strong><span class="kobospan" id="kobo.47.1">—introduced in </span><span><span class="kobospan" id="kobo.48.1">this chapter</span></span></li>
</ul>
<p class="callout-heading"><span class="kobospan" id="kobo.49.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.50.1">If you use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.51.1">requirements.txt</span></strong><span class="kobospan" id="kobo.52.1"> file we provide (see </span><a href="B20851_03.xhtml#_idTextAnchor091" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.53.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.54.1">), these libraries are already included in </span><span><span class="kobospan" id="kobo.55.1">your environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.56.1">The code for this chapter can be </span><span><span class="kobospan" id="kobo.57.1">found here:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.58.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.59.1">Check out the following video to see the code </span><span><span class="kobospan" id="kobo.60.1">in action:</span></span></p>
<p class="calibre3"><a href="https://packt.link/OEBOd" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.61.1">https://packt.link/OEBOd</span></span></a></p>
<h1 id="_idParaDest-244" class="calibre5"><a id="_idTextAnchor296" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.62.1">Understanding NLP</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.63.1">NLP is a fascinating branch of </span><strong class="bold"><span class="kobospan" id="kobo.64.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.65.1"> that focuses on the interaction between computers and human language. </span><span class="kobospan" id="kobo.65.2">NLP</span><a id="_idIndexMarker683" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.66.1"> combines linguistics, computer science, and </span><strong class="bold"><span class="kobospan" id="kobo.67.1">machine learning</span></strong><span class="kobospan" id="kobo.68.1"> to enable machines to understand, interpret, and generate human language in a way that’s both meaningful and useful. </span><span class="kobospan" id="kobo.68.2">Over the last several years, NLP</span><a id="_idIndexMarker684" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.69.1"> has been progressively taking on an increasing role in our daily lives, with applications spanning numerous domains, from virtual assistants and chatbots to sentiment analysis, language translation, and </span><span><span class="kobospan" id="kobo.70.1">information retrieval.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.71.1">One of the primary goals of NLP is to bridge the communication gap between humans and machines; this is crucial as language is the principal medium through which people interact and communicate their thoughts, ideas, and desires. </span><span class="kobospan" id="kobo.71.2">This goal of bridging the communication gap between humans and machines has driven significant advancements in</span><a id="_idIndexMarker685" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.72.1"> the field of NLP. </span><span class="kobospan" id="kobo.72.2">A recent notable milestone in this journey is the development of </span><strong class="bold"><span class="kobospan" id="kobo.73.1">large language models</span></strong><span class="kobospan" id="kobo.74.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.75.1">LLMs</span></strong><span class="kobospan" id="kobo.76.1">), such as </span><span><span class="kobospan" id="kobo.77.1">OpenAI’s </span></span><span><strong class="bold"><span class="kobospan" id="kobo.78.1">ChatGPT</span></strong></span><span><span class="kobospan" id="kobo.79.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.80.1">To create a bridge for human-computer </span><a id="_idIndexMarker686" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.81.1">communication, there must be a method in place that can transform human language into numerical representations, allowing machines to understand </span><a id="_idIndexMarker687" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.82.1">and process text data more effectively. </span><span class="kobospan" id="kobo.82.2">One such technique is the use of </span><strong class="bold"><span class="kobospan" id="kobo.83.1">word embeddings</span></strong><span class="kobospan" id="kobo.84.1">, described in the </span><span><span class="kobospan" id="kobo.85.1">next section.</span></span></p>
<h2 id="_idParaDest-245" class="calibre7"><a id="_idTextAnchor297" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.86.1">Word embeddings</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.87.1">Word embeddings are numerical</span><a id="_idIndexMarker688" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.88.1"> representations of words in the English language (or other languages). </span><span class="kobospan" id="kobo.88.2">Each word is encoded using a fixed-length vector of real numbers. </span><span class="kobospan" id="kobo.88.3">These vectors effectively capture semantic and contextual information associated with the words </span><span><span class="kobospan" id="kobo.89.1">they represent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.90.1">Word embeddings are created by </span><a id="_idIndexMarker689" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.91.1">training </span><strong class="bold"><span class="kobospan" id="kobo.92.1">neural networks</span></strong><span class="kobospan" id="kobo.93.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.94.1">NNs</span></strong><span class="kobospan" id="kobo.95.1">) to create numerical </span><a id="_idIndexMarker690" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.96.1">representations for words from large collections of written or spoken texts, where words with similar contexts are mapped to nearby points in a continuous </span><span><span class="kobospan" id="kobo.97.1">vector space.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.98.1">Common techniques for creating word </span><a id="_idIndexMarker691" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.99.1">embeddings include </span><strong class="bold"><span class="kobospan" id="kobo.100.1">Word2Vec</span></strong><span class="kobospan" id="kobo.101.1">, </span><strong class="bold"><span class="kobospan" id="kobo.102.1">Global Vectors for Word Representation</span></strong><span class="kobospan" id="kobo.103.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.104.1">GloVe</span></strong><span class="kobospan" id="kobo.105.1">), </span><span><span class="kobospan" id="kobo.106.1">and </span></span><span><strong class="bold"><span class="kobospan" id="kobo.107.1">fastText</span></strong></span><span><span class="kobospan" id="kobo.108.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.109.1">The typical dimensionality of </span><a id="_idIndexMarker692" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.110.1">word embeddings can vary, but common choices are 50, 100, 200, or 300 dimensions. </span><span class="kobospan" id="kobo.110.2">Higher-dimensional embeddings can capture more nuanced relationships but may require more data and </span><span><span class="kobospan" id="kobo.111.1">computational resources.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.112.1">For example, the word “dog” in a 50-dimensional Word2Vec embedding space might look something like </span><span><span class="kobospan" id="kobo.113.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.114.1">
[0.11008 -0.38781 -0.57615 -0.27714 0.70521 0.53994 -1.0786 -0.40146 1.1504 -0.5678 0.0038977 0.52878 0.64561 0.47262  0.48549 -0.18407 0.1801 0.91397 -1.1979 -0.5778 -0.37985  0.33606 0.772 0.75555 0.45506 -1.7671 -1.0503 0.42566 0.41893 -0.68327 1.5673 0.27685 -0.61708 0.64638 -0.076996 0.37118 0.1308 -0.45137 0.25398 -0.74392 -0.086199 0.24068 -0.64819 0.83549 1.2502 -0.51379 0.04224 -0.88118 0.7158 0.38519]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.115.1">Each of these 50 values represents a different aspect of the word “dog” in the context of the training data. </span><span class="kobospan" id="kobo.115.2">Related words, such as “cat” or “pet,” would have word vectors that are closer to the “dog” vector in this space, indicating their semantic similarity. </span><span class="kobospan" id="kobo.115.3">These embeddings not only capture semantic information but also maintain relationships between words, enabling NLP models to understand word relationships, context, and even sentence- and </span><span><span class="kobospan" id="kobo.116.1">document-level semantics.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.117.1">The following figure is a 2-dimensional </span><a id="_idIndexMarker693" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.118.1">visualization of 50-dimensional </span><a id="_idIndexMarker694" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.119.1">vectors representing various English words. </span><span class="kobospan" id="kobo.119.2">This image was created using </span><strong class="bold"><span class="kobospan" id="kobo.120.1">t-Distributed Stochastic Neighbor Embedding</span></strong><span class="kobospan" id="kobo.121.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.122.1">t-SNE</span></strong><span class="kobospan" id="kobo.123.1">), a dimensionality reduction technique often used to visualize and explore word embeddings. </span><span class="kobospan" id="kobo.123.2">t-SNE projects word embeddings into a lower-dimensional space while preserving relationships and similarities between data points. </span><span class="kobospan" id="kobo.123.3">This figure demonstrates how certain groups of words, such as fruit or animals, are closer together. </span><span class="kobospan" id="kobo.123.4">Relations between words are</span><a id="_idIndexMarker695" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.124.1"> apparent as well—for example, the relation between “son” and “boy” resembles that between “daughter” </span><span><span class="kobospan" id="kobo.125.1">and “girl:”</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer132">
<span class="kobospan" id="kobo.126.1"><img alt="Figure 11.1: Two-dimensional t-SNE plot of word embeddings" src="image/B20851_11_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.127.1">Figure 11.1: Two-dimensional t-SNE plot of word embeddings</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.128.1">In addition to their</span><a id="_idIndexMarker696" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.129.1"> traditional role in NLP, word embeddings can find use in genetic algorithms, as we will </span><a id="_idIndexMarker697" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.130.1">see in the </span><span><span class="kobospan" id="kobo.131.1">next section.</span></span></p>
<h2 id="_idParaDest-246" class="calibre7"><a id="_idTextAnchor298" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.132.1">Word embeddings and genetic algorithms</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.133.1">In previous chapters of this book, we</span><a id="_idIndexMarker698" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.134.1"> implemented numerous examples of genetic algorithms using fixed-length real-valued vectors (or lists) as the chromosome representation of candidate solutions. </span><span class="kobospan" id="kobo.134.2">Given that word embeddings enable us to represent words (such as “dog”) using fixed-length vectors of real-valued numbers, these vectors can effectively serve as the genetic representation of words in genetic </span><span><span class="kobospan" id="kobo.135.1">algorithm-based applications.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.136.1">This means we can leverage genetic algorithms to solve problems in which candidate solutions are words in the English language, utilizing word embeddings as the translation mechanism between words and their </span><span><span class="kobospan" id="kobo.137.1">genetic representation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.138.1">To demonstrate this concept, we will embark on solving a fun word game using a genetic algorithm, as</span><a id="_idIndexMarker699" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.139.1"> described in the </span><span><span class="kobospan" id="kobo.140.1">next sections.</span></span></p>
<h1 id="_idParaDest-247" class="calibre5"><a id="_idTextAnchor299" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.141.1">Finding the mystery word using genetic algorithms</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.142.1">In recent years, online mystery-word games have gained significant popularity. </span><span class="kobospan" id="kobo.142.2">One standout among them is </span><em class="italic"><span class="kobospan" id="kobo.143.1">Semantle</span></em><span class="kobospan" id="kobo.144.1">, a game that challenges you to guess the word of the day based on </span><span><span class="kobospan" id="kobo.145.1">its meaning.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.146.1">This game provides feedback on how</span><a id="_idIndexMarker700" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.147.1"> semantically similar your guesses are to the target word and features a “Hot and Cold” meter that indicates the proximity of your guess to the </span><span><span class="kobospan" id="kobo.148.1">secret word.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.149.1">Behind the scenes, Semantle employs word embeddings, specifically Word2Vec, to represent both the mystery word and players’ guesses. </span><span class="kobospan" id="kobo.149.2">It calculates the semantic similarity between them by measuring the difference between their representations: the closer the vectors, the greater the resemblance between the words. </span><span class="kobospan" id="kobo.149.3">The similarity score returned by the game ranges from -100 (very different from the answer) to 100 (identical to </span><span><span class="kobospan" id="kobo.150.1">the answer).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.151.1">In the following subsections, we will create two Python programs. </span><span class="kobospan" id="kobo.151.2">The first serves as a simulation of the Semantle game, while the other embodies a player or solver driven by a genetic algorithm, attempting to uncover the mystery word by maximizing the game’s similarity score. </span><span class="kobospan" id="kobo.151.3">Both programs rely on word embedding models; however, to maintain a clear separation, mirroring a real-world scenario, each program employs its own, distinct model. </span><span class="kobospan" id="kobo.151.4">The interaction between the player and the game is limited to exchanging actual guessed words and their corresponding scores, and no embedding vectors are exchanged. </span><span class="kobospan" id="kobo.151.5">This is depicted in the </span><span><span class="kobospan" id="kobo.152.1">following diagram:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer133">
<span class="kobospan" id="kobo.153.1"><img alt="Figure 11.2: Component diagram of the Python modules and their interaction" src="image/B20851_11_2.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.154.1">Figure 11.2: Component diagram of the Python modules and their interaction</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.155.1">To add an extra layer of intrigue, we’ve decided to have each program utilize a completely different embedding model. </span><span class="kobospan" id="kobo.155.2">For that to work, we assume that both embedding models share</span><a id="_idIndexMarker701" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.156.1"> a substantial overlap in </span><span><span class="kobospan" id="kobo.157.1">their vocabularies.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.158.1">The following section provides a detailed account of the Python implementation of </span><span><span class="kobospan" id="kobo.159.1">these programs.</span></span></p>
<h2 id="_idParaDest-248" class="calibre7"><a id="_idTextAnchor300" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.160.1">Python implementation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.161.1">We will begin by creating the Python implementation of word embedding models using the </span><strong class="source-inline"><span class="kobospan" id="kobo.162.1">gensim</span></strong><span class="kobospan" id="kobo.163.1"> library, as detailed in the </span><span><span class="kobospan" id="kobo.164.1">following subsection.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.165.1">The gensim library</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.166.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.167.1">gensim</span></strong><span class="kobospan" id="kobo.168.1"> library is a versatile Python package primarily recognized for its role in NLP and text analysis tasks. </span><strong class="source-inline"><span class="kobospan" id="kobo.169.1">gensim</span></strong><span class="kobospan" id="kobo.170.1"> simplifies the</span><a id="_idIndexMarker702" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.171.1"> process of working with word vectors by offering a comprehensive </span><a id="_idIndexMarker703" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.172.1">suite of tools for creating, training, and using word embeddings efficiently. </span><span class="kobospan" id="kobo.172.2">One of its key features is its ability to serve as a provider of pre-trained word embedding models, of which we will take advantage in our first Python module, as </span><span><span class="kobospan" id="kobo.173.1">described next.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.174.1">The Embeddings class</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.175.1">We start with a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.176.1">Embeddings</span></strong><span class="kobospan" id="kobo.177.1">, encapsulating a </span><strong class="source-inline"><span class="kobospan" id="kobo.178.1">gensim</span></strong><span class="kobospan" id="kobo.179.1">-based pre-trained word embedding </span><a id="_idIndexMarker704" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.180.1">model. </span><span class="kobospan" id="kobo.180.2">This class can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.181.1">embeddings.py</span></strong><span class="kobospan" id="kobo.182.1"> file, which is located at the</span><a id="_idIndexMarker705" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.183.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.184.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.185.1">The main functionality of this class is highlighted </span><span><span class="kobospan" id="kobo.186.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.187.1">The class’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.188.1">__init__()</span></strong><span class="kobospan" id="kobo.189.1"> method initializes the random seed (if given), and then proceeds to initialize the chosen (or default) </span><strong class="source-inline1"><span class="kobospan" id="kobo.190.1">gensim</span></strong><span class="kobospan" id="kobo.191.1"> model using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.192.1">_init_model()</span></strong><span class="kobospan" id="kobo.193.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.194.1">_download_and_save_model()</span></strong><span class="kobospan" id="kobo.195.1"> private methods. </span><span class="kobospan" id="kobo.195.2">The former method uploads the model’s embedding information from a local file, if available. </span><span class="kobospan" id="kobo.195.3">Otherwise, the latter method downloads the model from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.196.1">gensim</span></strong><span class="kobospan" id="kobo.197.1"> repository, separates the essential part for embedding, </span><strong class="source-inline1"><span class="kobospan" id="kobo.198.1">KeyedVectors</span></strong><span class="kobospan" id="kobo.199.1">, and saves it locally to be used the </span><span><span class="kobospan" id="kobo.200.1">next time:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.201.1">
if not isfile(model_path):
    self.</span><strong class="bold1"><span class="kobospan1" id="kobo.202.1">_download_and_save_model</span></strong><span class="kobospan1" id="kobo.203.1">(model_path)
print(f"Loading model '{self.model_name}' from local file...")
self.model = KeyedVectors.</span><strong class="bold1"><span class="kobospan1" id="kobo.204.1">load_word2vec_format</span></strong><span class="kobospan1" id="kobo.205.1">(model_path, 
    binary=True)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.206.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.207.1">pick_random_embedding()</span></strong><span class="kobospan" id="kobo.208.1"> method can be used to pick a random word out of the </span><span><span class="kobospan" id="kobo.209.1">model’s vocabulary.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.210.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.211.1">get_similarity()</span></strong><span class="kobospan" id="kobo.212.1"> method is used to retrieve the similarity value of the model between two </span><span><span class="kobospan" id="kobo.213.1">specified words.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.214.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.215.1">vec2_nearest_word()</span></strong><span class="kobospan" id="kobo.216.1"> method utilizes the </span><strong class="source-inline1"><span class="kobospan" id="kobo.217.1">gensim</span></strong><span class="kobospan" id="kobo.218.1"> model’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.219.1">similar_by_vector()</span></strong><span class="kobospan" id="kobo.220.1"> method to retrieve the word that is closest to the specified embedding vector. </span><span class="kobospan" id="kobo.220.2">As we will see shortly, this enables the genetic algorithm to use arbitrary vectors (such as randomly generated ones) and have them represent an existing word in the </span><span><span class="kobospan" id="kobo.221.1">model’s vocabulary.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.222.1">Lastly, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.223.1">list_models()</span></strong><span class="kobospan" id="kobo.224.1"> method can be used to retrieve and display information about the available embedding models provided by the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.225.1">gensim</span></strong></span><span><span class="kobospan" id="kobo.226.1"> library.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.227.1">As mentioned earlier, this</span><a id="_idIndexMarker706" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.228.1"> class is used by both the </span><strong class="source-inline"><span class="kobospan" id="kobo.229.1">Player</span></strong><span class="kobospan" id="kobo.230.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.231.1">Game</span></strong><span class="kobospan" id="kobo.232.1"> components, discussed in the </span><span><span class="kobospan" id="kobo.233.1">following subsections.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.234.1">The MysteryWordGame class</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.235.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.236.1">MysteryWordGame</span></strong><span class="kobospan" id="kobo.237.1"> Python</span><a id="_idIndexMarker707" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.238.1"> class encapsulates the </span><strong class="source-inline"><span class="kobospan" id="kobo.239.1">Game</span></strong><span class="kobospan" id="kobo.240.1"> component. </span><span class="kobospan" id="kobo.240.2">It can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.241.1">mystery_word_game.py</span></strong><span class="kobospan" id="kobo.242.1"> file, which is</span><a id="_idIndexMarker708" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.243.1"> located at the </span><span><span class="kobospan" id="kobo.244.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.245.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.246.1">The main functionality of this class is highlighted </span><span><span class="kobospan" id="kobo.247.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.248.1">The class employs the </span><strong class="source-inline1"><span class="kobospan" id="kobo.249.1">glove-twitter-50</span></strong> <strong class="source-inline1"><span class="kobospan" id="kobo.250.1">gensim</span></strong><span class="kobospan" id="kobo.251.1"> pre-trained embedding model developed by Stanford University. </span><span class="kobospan" id="kobo.251.2">This model was specifically designed for Twitter text data and utilizes 50-dimensional </span><span><span class="kobospan" id="kobo.252.1">embedding vectors.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.253.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.254.1">__init__()</span></strong><span class="kobospan" id="kobo.255.1"> method of the class initializes the embedding model it will internally use, and then either selects a random mystery word or uses a word specified as an argument for the </span><span><span class="kobospan" id="kobo.256.1">mystery word:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.257.1">
self.embeddings = Embeddings(model_name=MODEL)
     self.mystery_word = </span><strong class="bold1"><span class="kobospan1" id="kobo.258.1">given_mystery_word</span></strong><span class="kobospan1" id="kobo.259.1"> if
          given_mystery_word else
          self.embeddings.</span><strong class="bold1"><span class="kobospan1" id="kobo.260.1">pick_random_embedding</span></strong><span class="kobospan1" id="kobo.261.1">()</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.262.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.263.1">score_guess()</span></strong><span class="kobospan" id="kobo.264.1"> method calculates the score returned by the game for a given guessed word. </span><span class="kobospan" id="kobo.264.2">If the word is not present in the model’s vocabulary, which can occur as the player module may use a potentially different model, the score is set to the minimum value of -100. </span><span class="kobospan" id="kobo.264.3">Otherwise, the calculated score value will be a number between -100 </span><span><span class="kobospan" id="kobo.265.1">and 100:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.266.1">
if self.embeddings.</span><strong class="bold1"><span class="kobospan1" id="kobo.267.1">has_word</span></strong><span class="kobospan1" id="kobo.268.1">(guess_word):
    score = 100 *
    self.embeddings.</span><strong class="bold1"><span class="kobospan1" id="kobo.269.1">get_similarity</span></strong><span class="kobospan1" id="kobo.270.1">(self.mystery_word,
    guess_word)
else:
    score = -100</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.271.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.272.1">main()</span></strong><span class="kobospan" id="kobo.273.1"> method</span><a id="_idIndexMarker709" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.274.1"> tests the class’s functionality by creating an instance of the game, selecting the word </span><strong class="source-inline1"><span class="kobospan" id="kobo.275.1">"dog"</span></strong><span class="kobospan" id="kobo.276.1">, and evaluating several guessed words related to it, such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.277.1">"canine"</span></strong><span class="kobospan" id="kobo.278.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.279.1">"hound"</span></strong><span class="kobospan" id="kobo.280.1">. </span><span class="kobospan" id="kobo.280.2">It also includes an unrelated word (</span><strong class="source-inline1"><span class="kobospan" id="kobo.281.1">"computer"</span></strong><span class="kobospan" id="kobo.282.1">) and a word that does not exist in the </span><span><span class="kobospan" id="kobo.283.1">vocabulary (</span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.284.1">"asdghf"</span></strong></span><span><span class="kobospan" id="kobo.285.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.286.1">
game = </span><strong class="bold1"><span class="kobospan1" id="kobo.287.1">MysteryWordGame</span></strong><span class="kobospan1" id="kobo.288.1">(given_mystery_word="dog")
print("-- Checking candidate guess words:")
for guess_word in ["computer", "asdghf", "canine", "hound", 
    "poodle", "puppy", "cat", "dog"]:
    score = game.</span><strong class="bold1"><span class="kobospan1" id="kobo.289.1">score_guess</span></strong><span class="kobospan1" id="kobo.290.1">(guess_word)
    print(f"- current guess: {guess_word.ljust(10)} =&gt; 
        score = {score:.2f}")</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.291.1">Executing the </span><strong class="source-inline"><span class="kobospan" id="kobo.292.1">main()</span></strong><span class="kobospan" id="kobo.293.1"> method of the class yields the </span><span><span class="kobospan" id="kobo.294.1">following output:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.295.1">
Loading model 'glove-twitter-50' from local file...
</span><span class="kobospan1" id="kobo.295.2">--- Mystery word is 'dog' — game on!
</span><span class="kobospan1" id="kobo.295.3">-- Checking candidate guess words:
- current guess: computer   =&gt; score = 54.05
- current guess: asdghf     =&gt; score = -100.00
- current guess: canine     =&gt; score = 47.07
- current guess: hound      =&gt; score = 64.93
- current guess: poodle     =&gt; score = 65.90
- current guess: puppy      =&gt; score = 87.90
- current guess: cat        =&gt; score = 94.30
- current guess: dog        =&gt; score = 100.00</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.296.1">We are now ready for</span><a id="_idIndexMarker710" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.297.1"> the interesting component—the program that attempts to solve </span><span><span class="kobospan" id="kobo.298.1">the game.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.299.1">The genetic algorithms-based player program</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.300.1">As mentioned earlier, this module uses a different embedding model from the one used by the game, although it has the </span><a id="_idIndexMarker711" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.301.1">option to use the same model. </span><span class="kobospan" id="kobo.301.2">In this case, we have selected the </span><strong class="source-inline"><span class="kobospan" id="kobo.302.1">glove-wiki-gigaword-50</span></strong> <strong class="source-inline"><span class="kobospan" id="kobo.303.1">gensim</span></strong><span class="kobospan" id="kobo.304.1"> pre-trained embedding model, which was trained on a </span><a id="_idIndexMarker712" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.305.1">substantial corpus of text from the English </span><em class="italic"><span class="kobospan" id="kobo.306.1">Wikipedia</span></em><span class="kobospan" id="kobo.307.1"> website and the </span><span><em class="italic"><span class="kobospan" id="kobo.308.1">Gigaword</span></em></span><span><span class="kobospan" id="kobo.309.1"> dataset.</span></span></p>
<h4 class="calibre19"><span class="kobospan" id="kobo.310.1">Solution representation</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.311.1">The solution representation in the genetic algorithm in this case is a real-valued vector (or a list) of the same dimension as</span><a id="_idIndexMarker713" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.312.1"> the embedding model. </span><span class="kobospan" id="kobo.312.2">This allows each solution to serve as an embedding vector, although not perfectly. </span><span class="kobospan" id="kobo.312.3">Initially, the algorithm employs randomly generated vectors, and through crossover and mutation operations, it’s likely that at least some of the vectors won’t directly correspond to existing words in the model’s vocabulary. </span><span class="kobospan" id="kobo.312.4">To address this issue, we utilize the </span><strong class="source-inline"><span class="kobospan" id="kobo.313.1">vec2_nearest_word()</span></strong><span class="kobospan" id="kobo.314.1"> method from the </span><strong class="source-inline"><span class="kobospan" id="kobo.315.1">Embedding</span></strong><span class="kobospan" id="kobo.316.1"> class, which</span><a id="_idIndexMarker714" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.317.1"> returns the closest word in the vocabulary. </span><span class="kobospan" id="kobo.317.2">This approach exemplifies the </span><strong class="bold"><span class="kobospan" id="kobo.318.1">genotype-to-phenotype mapping</span></strong><span class="kobospan" id="kobo.319.1"> concept, as discussed in </span><a href="B20851_04.xhtml#_idTextAnchor155" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.320.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.321.1">, </span><span><em class="italic"><span class="kobospan" id="kobo.322.1">Combinatorial Optimization</span></em></span><span><span class="kobospan" id="kobo.323.1">.</span></span></p>
<h4 class="calibre19"><span class="kobospan" id="kobo.324.1">Early convergence criteria</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.325.1">In most cases discussed so far, the solution does not possess the knowledge of the best possible score that can be achieved during the optimization process. </span><span class="kobospan" id="kobo.325.2">However, in this case, we know that the best possible score is 100. </span><span class="kobospan" id="kobo.325.3">Once it is achieved, the correct word has been found, and there is no point in continuing the evolutionary cycle. </span><span class="kobospan" id="kobo.325.4">Therefore, we modified the main loop </span><a id="_idIndexMarker715" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.326.1">of our genetic algorithm to break if the maximum score is reached. </span><span class="kobospan" id="kobo.326.2">The modified method is called </span><strong class="source-inline"><span class="kobospan" id="kobo.327.1">eaSimple_modified()</span></strong><span class="kobospan" id="kobo.328.1"> and can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.329.1">elitism_modified.py</span></strong><span class="kobospan" id="kobo.330.1"> module. </span><span class="kobospan" id="kobo.330.2">It accepts an optional parameter called </span><strong class="source-inline"><span class="kobospan" id="kobo.331.1">max_fitness</span></strong><span class="kobospan" id="kobo.332.1">. </span><span class="kobospan" id="kobo.332.2">When this parameter is provided with a value, the main loop breaks if the best fitness value found so far reaches or exceeds </span><span><span class="kobospan" id="kobo.333.1">this value:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.334.1">
if max_fitness and halloffame.items[0].fitness.values[0] &gt;= 
    max_fitness:
    break</span></pre> <h4 class="calibre19"><span class="kobospan" id="kobo.335.1">Printing out the current best-guessed word</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.336.1">Additionally, the </span><strong class="source-inline"><span class="kobospan" id="kobo.337.1">eaSimple_modified()</span></strong><span class="kobospan" id="kobo.338.1"> method includes the printing of the guessed word </span><a id="_idIndexMarker716" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.339.1">corresponding to the individual with the best fitness found so far, as part of the statistics summary generated for </span><span><span class="kobospan" id="kobo.340.1">every individual:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.341.1">
if verbose:
    print(f"{logbook.stream} =&gt; {embeddings.vec2_nearest_word(
        np.asarray(halloffame.items[0]))}")</span></pre> <h4 class="calibre19"><span class="kobospan" id="kobo.342.1">The genetic algorithm implementation</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.343.1">The genetic algorithm-based</span><a id="_idIndexMarker717" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.344.1"> player for the mystery-word game search for the best hyperparameter values is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.345.1">01_find_mystery_word.py</span></strong><span class="kobospan" id="kobo.346.1"> Python program, which is located at the </span><span><span class="kobospan" id="kobo.347.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.348.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">The following steps describe the main parts of </span><span><span class="kobospan" id="kobo.350.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.351.1">We begin by creating an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.352.1">Embeddings</span></strong><span class="kobospan" id="kobo.353.1"> class, which will serve as the word embeddings model for the </span><span><span class="kobospan" id="kobo.354.1">solver program:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.355.1">
embeddings = Embeddings(model_name='glove-wiki-gigaword-50', 
    randomSeed=RANDOM_SEED)
VECTOR_SIZE = embeddings.get_vector_size()</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.356.1">Next, we</span><a id="_idIndexMarker718" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.357.1"> create an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.358.1">MysteryWordGame</span></strong><span class="kobospan" id="kobo.359.1"> class, which represents the game we will be playing. </span><span class="kobospan" id="kobo.359.2">We instruct it to use the word “dog” for demonstration purposes. </span><span class="kobospan" id="kobo.359.3">This word can later be replaced with others, or we can allow the game to choose a random word if we omit the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.360.1">given_mystery_word</span></strong></span><span><span class="kobospan" id="kobo.361.1"> parameter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.362.1">
game = MysteryWordGame(given_mystery_word='dog')</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.363.1">Since our goal is to maximize the game’s score, we define a single-objective strategy for </span><span><span class="kobospan" id="kobo.364.1">maximizing fitness:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.365.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.366.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.367.1">", base.Fitness, weights=(1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.368.1">To create random individuals representing word embeddings, we create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.369.1">randomFloat()</span></strong><span class="kobospan" id="kobo.370.1"> function and register it with </span><span><span class="kobospan" id="kobo.371.1">the toolbox:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.372.1">
def randomFloat(low, up):
    return [</span><strong class="bold1"><span class="kobospan1" id="kobo.373.1">random.uniform</span></strong><span class="kobospan1" id="kobo.374.1">(l, u) for l, u in zip([low] * 
        VECTOR_SIZE, [up] * VECTOR_SIZE)]
toolbox.register("attrFloat", </span><strong class="bold1"><span class="kobospan1" id="kobo.375.1">randomFloat</span></strong><span class="kobospan1" id="kobo.376.1">, BOUNDS_LOW, 
    BOUNDS_HIGH)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.377.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.378.1">score()</span></strong><span class="kobospan" id="kobo.379.1"> function is used to evaluate the fitness of each solution, and this process consists of two steps: first, we employ the local </span><strong class="source-inline1"><span class="kobospan" id="kobo.380.1">embeddings</span></strong><span class="kobospan" id="kobo.381.1"> model to find the vocabulary word nearest to the evaluated vector (this is where the genotype-to-phenotype mapping takes place). </span><span class="kobospan" id="kobo.381.2">Next, we send this word to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.382.1">Game</span></strong><span class="kobospan" id="kobo.383.1"> component and request it to score it as a guessed word. </span><span class="kobospan" id="kobo.383.2">The score returned by the game, a value ranging from -100 to 100, is directly used as the </span><span><span class="kobospan" id="kobo.384.1">fitness value:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.385.1">
def score(individual):
    guess_word = embeddings.</span><strong class="bold1"><span class="kobospan1" id="kobo.386.1">vec2_nearest_word</span></strong><span class="kobospan1" id="kobo.387.1">(
        np.asarray(individual))
    return game.</span><strong class="bold1"><span class="kobospan1" id="kobo.388.1">score_guess</span></strong><span class="kobospan1" id="kobo.389.1">(guess_word),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.390.1">evaluate</span></strong><span class="kobospan1" id="kobo.391.1">", score)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.392.1">Now, we need to define genetic operators. </span><span class="kobospan" id="kobo.392.2">While for the </span><em class="italic"><span class="kobospan" id="kobo.393.1">selection</span></em><span class="kobospan" id="kobo.394.1"> operator, we use the usual </span><em class="italic"><span class="kobospan" id="kobo.395.1">tournament selection</span></em><span class="kobospan" id="kobo.396.1"> with a tournament size of 2, we choose </span><em class="italic"><span class="kobospan" id="kobo.397.1">crossover</span></em><span class="kobospan" id="kobo.398.1"> and </span><em class="italic"><span class="kobospan" id="kobo.399.1">mutation</span></em><span class="kobospan" id="kobo.400.1"> operators that are specialized for bounded float-list chromosomes and </span><a id="_idIndexMarker719" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.401.1">provide them with the boundaries we defined for </span><span><span class="kobospan" id="kobo.402.1">each hyperparameter:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.403.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.404.1">select</span></strong><span class="kobospan1" id="kobo.405.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.406.1">mate</span></strong><span class="kobospan1" id="kobo.407.1">",
                 tools.cxSimulatedBinaryBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.408.1">mutate</span></strong><span class="kobospan1" id="kobo.409.1">",
                 tools.mutPolynomialBounded,
                 low=BOUNDS_LOW,
                 up=BOUNDS_HIGH,
                 eta=CROWDING_FACTOR,
                 indpb=1.0 / NUM_OF_PARAMS)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.410.1">In addition, we continue to employ the elitist approach, where the </span><strong class="bold"><span class="kobospan" id="kobo.411.1">Hall of Fame</span></strong><span class="kobospan" id="kobo.412.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.413.1">HOF</span></strong><span class="kobospan" id="kobo.414.1">) members—the current best</span><a id="_idIndexMarker720" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.415.1"> individuals—are always passed untouched to the next</span><a id="_idIndexMarker721" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.416.1"> generation. </span><span class="kobospan" id="kobo.416.2">However, in this iteration, we use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.417.1">eaSimple_modified</span></strong><span class="kobospan" id="kobo.418.1"> algorithm, where—in addition—the main loop will terminate when the score reaches the maximum </span><span><span class="kobospan" id="kobo.419.1">known score</span></span><span><span class="kobospan" id="kobo.420.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.421.1">
population, logbook = </span><strong class="bold1"><span class="kobospan1" id="kobo.422.1">eaSimple_modified</span></strong><span class="kobospan1" id="kobo.423.1">(
    population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    </span><strong class="bold1"><span class="kobospan1" id="kobo.424.1">max_fitness=MAX_SCORE</span></strong><span class="kobospan1" id="kobo.425.1">,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.426.1">By running the algorithm with a population size of 30, we get the </span><span><span class="kobospan" id="kobo.427.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.428.1">
Loading model '</span><strong class="bold1"><span class="kobospan1" id="kobo.429.1">glove-wiki-gigaword-50</span></strong><span class="kobospan1" id="kobo.430.1">' from local file...
</span><span class="kobospan1" id="kobo.430.2">Loading model '</span><strong class="bold1"><span class="kobospan1" id="kobo.431.1">glove-twitter-50</span></strong><span class="kobospan1" id="kobo.432.1">' from local file...
</span><span class="kobospan1" id="kobo.432.2">--- </span><strong class="bold1"><span class="kobospan1" id="kobo.433.1">Mistery word is 'dog'</span></strong><span class="kobospan1" id="kobo.434.1"> — game on!
</span><span class="kobospan1" id="kobo.434.2">gen     nevals  max     avg
0       30      51.3262 -43.8478 =&gt; stories
1       25      51.3262 -17.5409 =&gt; stories
2       26      51.3262 -1.20704 =&gt; stories
3       26      51.3262 11.1749  =&gt; stories
4       26      64.7724 26.23    =&gt; bucket
5       25      64.7724 40.0518  =&gt; bucket
6       26      67.487  42.003   =&gt; toys
7       26      69.455  37.0863  =&gt; family
8       25      69.455  48.1514  =&gt; family
9       25      69.455  38.5332  =&gt; family
10      27      87.2265 47.9803  =&gt; pet
11      26      87.2265 46.3378  =&gt; pet
12      27      87.2265 40.0165  =&gt; pet
13      27      87.2265 52.6842  =&gt; pet
14      26      87.2265 59.186   =&gt; pet
15      27      87.2265 41.5553  =&gt; pet
16      27      87.2265 49.529   =&gt; pet
17      27      87.2265 50.9414  =&gt; pet
18      27      87.2265 44.9691  =&gt; pet
19      25      87.2265 30.8624  =&gt; pet
20      27      100     63.5354  =&gt; </span><strong class="bold1"><span class="kobospan1" id="kobo.435.1">dog</span></strong>
<strong class="bold1"><span class="kobospan1" id="kobo.436.1">Best Solution = dog</span></strong>
<strong class="bold1"><span class="kobospan1" id="kobo.437.1">Best Score = 100.00</span></strong></pre> <p class="calibre3"><span class="kobospan" id="kobo.438.1">From this printout, we can observe </span><span><span class="kobospan" id="kobo.439.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.440.1">Two distinct word</span><a id="_idIndexMarker722" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.441.1"> embedding models were loaded, one for the player and the other for the game, </span><span><span class="kobospan" id="kobo.442.1">as designed</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.443.1">The mystery word that was set to </span><strong class="source-inline1"><span class="kobospan" id="kobo.444.1">'dog'</span></strong><span class="kobospan" id="kobo.445.1"> was correctly guessed by the genetic algorithm-driven player after </span><span><span class="kobospan" id="kobo.446.1">20 generations</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.447.1">As soon as the word was found, the player quit playing, even though the maximum number of generations was set </span><span><span class="kobospan" id="kobo.448.1">to 1000</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.449.1">We can see how the current best-guessed word </span><span><span class="kobospan" id="kobo.450.1">has evolved:</span></span></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.451.1">stories</span></strong><span class="kobospan" id="kobo.452.1"> → </span><strong class="source-inline1"><span class="kobospan" id="kobo.453.1">bucket</span></strong><span class="kobospan" id="kobo.454.1"> → </span><strong class="source-inline1"><span class="kobospan" id="kobo.455.1">toys</span></strong><span class="kobospan" id="kobo.456.1"> → </span><strong class="source-inline1"><span class="kobospan" id="kobo.457.1">family</span></strong><span class="kobospan" id="kobo.458.1"> → </span><strong class="source-inline1"><span class="kobospan" id="kobo.459.1">pet</span></strong><span class="kobospan" id="kobo.460.1"> → </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.461.1">dog</span></strong></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.462.1">This looks great! </span><span class="kobospan" id="kobo.462.2">However, keep in </span><a id="_idIndexMarker723" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.463.1">mind that it’s just one example. </span><span class="kobospan" id="kobo.463.2">You are encouraged to try out other words, as well as different settings for the genetic algorithm; perhaps even change the embedding models. </span><span class="kobospan" id="kobo.463.3">Are there model pairs that are less compatible </span><span><span class="kobospan" id="kobo.464.1">than others?</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.465.1">In the next portion of this chapter, we will explore </span><span><strong class="bold"><span class="kobospan" id="kobo.466.1">document classification</span></strong></span><span><span class="kobospan" id="kobo.467.1">.</span></span></p>
<h1 id="_idParaDest-249" class="calibre5"><a id="_idTextAnchor301" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.468.1">Document classification</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.469.1">Document classification is a </span><a id="_idIndexMarker724" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.470.1">critical task in NLP, involving the categorization of textual documents into predefined classes or categories based on their content. </span><span class="kobospan" id="kobo.470.2">This process is essential for organizing, managing, and extracting meaningful information from large volumes of textual data. </span><span class="kobospan" id="kobo.470.3">Applications of document classification are vast and diverse, spanning various industries </span><span><span class="kobospan" id="kobo.471.1">and domains.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.472.1">In the field of information retrieval, document classification plays a crucial role in </span><strong class="bold"><span class="kobospan" id="kobo.473.1">search engines</span></strong><span class="kobospan" id="kobo.474.1">. </span><span class="kobospan" id="kobo.474.2">By categorizing web pages, articles, and documents into relevant topics or genres, search engines can deliver more accurate and targeted search results to users. </span><span class="kobospan" id="kobo.474.3">This enhances the overall user experience and ensures that individuals can quickly access the information </span><span><span class="kobospan" id="kobo.475.1">they seek.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.476.1">In customer service and support, document classification enables the </span><strong class="bold"><span class="kobospan" id="kobo.477.1">automatic routing</span></strong><span class="kobospan" id="kobo.478.1"> of customer inquiries and messages to the appropriate departments or teams. </span><span class="kobospan" id="kobo.478.2">For instance, emails received by a company can be classified into categories such as “Billing Inquiries,” “Technical Support,” or “General Inquiries,” ensuring that each message reaches the right team for prompt response </span><span><span class="kobospan" id="kobo.479.1">and resolution.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.480.1">In the legal domain, document classification is instrumental</span><a id="_idIndexMarker725" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.481.1"> for tasks such as </span><strong class="bold"><span class="kobospan" id="kobo.482.1">e-discovery</span></strong><span class="kobospan" id="kobo.483.1">, where large volumes of legal documents need to be analyzed for relevance to a case. </span><span class="kobospan" id="kobo.483.2">Classification helps identify documents that are potentially pertinent to a legal matter, streamlining the review process and reducing the time and resources required for </span><span><span class="kobospan" id="kobo.484.1">legal proceedings.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.485.1">Moreover, document classification is pivotal in </span><strong class="bold"><span class="kobospan" id="kobo.486.1">sentiment analysis</span></strong><span class="kobospan" id="kobo.487.1">, where it can be used to categorize social media</span><a id="_idIndexMarker726" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.488.1"> posts, reviews, and comments into positive, negative, or neutral sentiments. </span><span class="kobospan" id="kobo.488.2">This information is invaluable for businesses looking to gauge customer feedback, monitor brand reputation, and make data-driven decisions to improve their products </span><span><span class="kobospan" id="kobo.489.1">or services.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.490.1">One effective method for </span><a id="_idIndexMarker727" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.491.1">performing document classification is by leveraging n-grams, as elaborated in the </span><span><span class="kobospan" id="kobo.492.1">upcoming section.</span></span></p>
<h2 id="_idParaDest-250" class="calibre7"><a id="_idTextAnchor302" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.493.1">N-grams</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.494.1">An n-gram is a contiguous sequence of </span><em class="italic"><span class="kobospan" id="kobo.495.1">n</span></em><span class="kobospan" id="kobo.496.1"> items, which can be characters, words, or even phrases, extracted from </span><a id="_idIndexMarker728" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.497.1">a larger body of text. </span><span class="kobospan" id="kobo.497.2">By breaking down text into these smaller units, n-grams enable the extraction of valuable linguistic patterns, relationships, </span><span><span class="kobospan" id="kobo.498.1">and context.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.499.1">For example, in the case of </span><em class="italic"><span class="kobospan" id="kobo.500.1">character n-grams</span></em><span class="kobospan" id="kobo.501.1">, a 3-gram might break the word “apple” into “app,” “ppl,” </span><span><span class="kobospan" id="kobo.502.1">and “ple.”</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.503.1">Here are some examples of </span><span><em class="italic"><span class="kobospan" id="kobo.504.1">word n-grams</span></em></span><span><span class="kobospan" id="kobo.505.1">:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><strong class="bold"><span class="kobospan" id="kobo.506.1">Unigrams (1-grams)</span></strong></span><span><span class="kobospan" id="kobo.507.1">:</span></span><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.508.1">Text</span></em><span class="kobospan" id="kobo.509.1">: “I love </span><span><span class="kobospan" id="kobo.510.1">to</span></span><span><a id="_idIndexMarker729" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.511.1"> code.”</span></span></p><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.512.1">Unigrams</span></em><span class="kobospan" id="kobo.513.1">: [“I”, “love”, “</span><span><span class="kobospan" id="kobo.514.1">to”, “code”]</span></span></p></li>
<li class="calibre11"><span><strong class="bold"><span class="kobospan" id="kobo.515.1">Bigrams (2-grams)</span></strong></span><span><span class="kobospan" id="kobo.516.1">:</span></span><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.517.1">Text</span></em><span class="kobospan" id="kobo.518.1">: “Natural language </span><a id="_idIndexMarker730" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.519.1">processing </span><span><span class="kobospan" id="kobo.520.1">is fascinating.”</span></span></p><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.521.1">Bigrams</span></em><span class="kobospan" id="kobo.522.1">: [“Natural language”, “language processing”, “processing is”, “</span><span><span class="kobospan" id="kobo.523.1">is fascinating”]</span></span></p></li>
<li class="calibre11"><span><strong class="bold"><span class="kobospan" id="kobo.524.1">Trigrams (3-grams)</span></strong></span><span><span class="kobospan" id="kobo.525.1">:</span></span><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.526.1">Text</span></em><span class="kobospan" id="kobo.527.1">: “Machine learning </span><a id="_idIndexMarker731" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.528.1">models </span><span><span class="kobospan" id="kobo.529.1">can generalize.”</span></span></p><p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.530.1">Trigrams</span></em><span class="kobospan" id="kobo.531.1">: [“Machine learning models”, “learning models can”, “models </span><span><span class="kobospan" id="kobo.532.1">can generalize”]</span></span></p></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.533.1">N-grams provide insights into textual content by revealing the sequential arrangement of words or characters, identifying frequent patterns, and extracting features. </span><span class="kobospan" id="kobo.533.2">They help understand language</span><a id="_idIndexMarker732" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.534.1"> structure, context, and patterns, making them valuable for text analysis tasks such as </span><span><span class="kobospan" id="kobo.535.1">document classification.</span></span></p>
<h2 id="_idParaDest-251" class="calibre7"><a id="_idTextAnchor303" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.536.1">Selecting a subset of n-grams</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.537.1">In </span><a href="B20851_07.xhtml#_idTextAnchor221" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.538.1">Chapter 7</span></em></span></a><span class="kobospan" id="kobo.539.1">, </span><em class="italic"><span class="kobospan" id="kobo.540.1">Enhancing Machine Learning Models Using Feature Selection</span></em><span class="kobospan" id="kobo.541.1">, we demonstrated the importance of selecting a meaningful subset of features, known as “feature selection.” </span><span class="kobospan" id="kobo.541.2">This </span><a id="_idIndexMarker733" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.542.1">process is equally valuable in document classification, especially </span><a id="_idIndexMarker734" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.543.1">when dealing with a large number of extracted n-grams, a common occurrence in large documents. </span><span class="kobospan" id="kobo.543.2">The advantages of identifying a relevant subset of n-grams include </span><span><span class="kobospan" id="kobo.544.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.545.1">Dimensionality reduction</span></strong><span class="kobospan" id="kobo.546.1">: Reducing the number of n-grams makes computations more efficient and </span><span><span class="kobospan" id="kobo.547.1">prevents overfitting</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.548.1">Focus on key features</span></strong><span class="kobospan" id="kobo.549.1">: Selecting discriminative n-grams helps the model concentrate on </span><span><span class="kobospan" id="kobo.550.1">crucial features</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.551.1">Noise reduction</span></strong><span class="kobospan" id="kobo.552.1">: Filtering out uninformative n-grams minimizes noise in </span><span><span class="kobospan" id="kobo.553.1">the data</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.554.1">Enhanced generalization</span></strong><span class="kobospan" id="kobo.555.1">: A well-chosen subset improves the model’s ability to handle </span><span><span class="kobospan" id="kobo.556.1">new documents</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.557.1">Efficiency</span></strong><span class="kobospan" id="kobo.558.1">: Smaller feature sets speed up model training </span><span><span class="kobospan" id="kobo.559.1">and prediction</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.560.1">Furthermore, identifying a relevant subset of n-grams in document classification can be valuable for model interpretability. </span><span class="kobospan" id="kobo.560.2">By narrowing down the features to a manageable subset, it becomes easier to understand and interpret the factors influencing the </span><span><span class="kobospan" id="kobo.561.1">model’s predictions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.562.1">Similarly to what we did in </span><a href="B20851_07.xhtml#_idTextAnchor221" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.563.1">Chapter 7</span></em></span></a><span class="kobospan" id="kobo.564.1">, we will apply a genetic algorithms-based search here to identify a relevant subset of n-grams. </span><span class="kobospan" id="kobo.564.2">However, considering that the number of n-grams we anticipate is substantially larger than the number of features in the common datasets we’ve previously used, we won’t be searching for the overall best subset. </span><span class="kobospan" id="kobo.564.3">Instead, our goal will be to find a fixed-size subset of features, such as the best 1,000 or 100 n-grams </span><span><span class="kobospan" id="kobo.565.1">to use.</span></span></p>
<h2 id="_idParaDest-252" class="calibre7"><a id="_idTextAnchor304" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.566.1">Using genetic algorithms to search for a fixed-size subset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.567.1">As we need to identify a good, fixed-size subset of items within a very large group, let’s try to define the usual </span><a id="_idIndexMarker735" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.568.1">components needed for the genetic algorithm </span><span><span class="kobospan" id="kobo.569.1">to work:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.570.1">Solution representation</span></strong><span class="kobospan" id="kobo.571.1">: Since the subset size is much smaller than the full dataset, it’s more efficient to use a fixed-size list of integers representing the indices of the items within the large dataset. </span><span class="kobospan" id="kobo.571.2">For instance, if we aim to create a subset of size 3 from 100 items, a possible solution could be represented as a list, such as [5, 42, 88] or [73, </span><span><span class="kobospan" id="kobo.572.1">11, 42].</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.573.1">Crossover operation</span></strong><span class="kobospan" id="kobo.574.1">: To ensure valid offspring, we must prevent the same index from appearing more than once in each offspring. </span><span class="kobospan" id="kobo.574.2">In the previous example, the item “42” appears in both lists. </span><span class="kobospan" id="kobo.574.3">If we used a single-point crossover, for example, we could end up with the offspring [5, 42, 42], which in effect will have only two unique items rather than three. </span><span class="kobospan" id="kobo.574.4">One simple crossover method that overcomes this issue would be </span><span><span class="kobospan" id="kobo.575.1">as follows:</span></span><ol class="calibre20"><li class="upper-roman"><span class="kobospan" id="kobo.576.1">Create a set containing all unique items present in both parents. </span><span class="kobospan" id="kobo.576.2">In our example, this set would be {5, 11, 42, </span><span><span class="kobospan" id="kobo.577.1">73, 88}.</span></span></li><li class="upper-roman"><span class="kobospan" id="kobo.578.1">Generate offspring by randomly selecting from the set mentioned previously. </span><span class="kobospan" id="kobo.578.2">Each offspring should select three items (in this case). </span><span class="kobospan" id="kobo.578.3">A possible result could be [5, 11, 88] and [11, </span><span><span class="kobospan" id="kobo.579.1">42, 88].</span></span></li></ol></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.580.1">Mutation operation</span></strong><span class="kobospan" id="kobo.581.1">: A straightforward method to generate a valid mutated individual from an existing one is </span><span><span class="kobospan" id="kobo.582.1">as follows:</span></span><ul class="calibre16"><li class="calibre11"><span class="kobospan" id="kobo.583.1">For each item in the list, with a specified probability, the item will be replaced by one that does exist in the </span><span><span class="kobospan" id="kobo.584.1">current list.</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.585.1">For example, if we consider the list [11, 42, 88], there’s a possibility that the second item (42) could be replaced with, say, 27, resulting in the list [11, </span><span><span class="kobospan" id="kobo.586.1">27, 88].</span></span></li></ul></li>
</ul>
<h2 id="_idParaDest-253" class="calibre7"><a id="_idTextAnchor305" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.587.1">Python implementation</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.588.1">In the following </span><a id="_idIndexMarker736" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.589.1">sections, we will implement </span><span><span class="kobospan" id="kobo.590.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.591.1">A document classifier that will train on document data from two newsgroups and use n-grams to predict to which newsgroup each </span><span><span class="kobospan" id="kobo.592.1">document belongs</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.593.1">A genetic algorithms-driven optimizer that seeks to find the best subset of n-grams to use for this classification task, given the desired size of </span><span><span class="kobospan" id="kobo.594.1">the subset</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.595.1">We will start with the class implementing the classifier, as described in the </span><span><span class="kobospan" id="kobo.596.1">next subsection.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.597.1">Newsgroup document classifier</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.598.1">We start with a Python class called </span><strong class="source-inline"><span class="kobospan" id="kobo.599.1">NewsgroupClassifier</span></strong><span class="kobospan" id="kobo.600.1">, implementing a </span><strong class="source-inline"><span class="kobospan" id="kobo.601.1">scikit-learn</span></strong><span class="kobospan" id="kobo.602.1">-based document </span><a id="_idIndexMarker737" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.603.1">classifier that uses n-grams as features and learns to distinguish between posts from two different newsgroups. </span><span class="kobospan" id="kobo.603.2">This class can be found in the </span><strong class="source-inline"><span class="kobospan" id="kobo.604.1">newsgroup_classifier.py</span></strong><span class="kobospan" id="kobo.605.1"> file, which is located at the </span><span><span class="kobospan" id="kobo.606.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.607.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.608.1">The main functionality of this class is highlighted </span><span><span class="kobospan" id="kobo.609.1">as follows:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.610.1">The class’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.611.1">init_data()</span></strong><span class="kobospan" id="kobo.612.1"> method, called by </span><strong class="source-inline1"><span class="kobospan" id="kobo.613.1">__init__()</span></strong><span class="kobospan" id="kobo.614.1">, creates training and testing sets from </span><strong class="source-inline1"><span class="kobospan" id="kobo.615.1">scikit-learn</span></strong><span class="kobospan" id="kobo.616.1">’s built-in dataset of newsgroup posts. </span><span class="kobospan" id="kobo.616.2">It retrieves posts from two categories, </span><strong class="source-inline1"><span class="kobospan" id="kobo.617.1">'rec.autos'</span></strong><span class="kobospan" id="kobo.618.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.619.1">'rec.motorcycles'</span></strong><span class="kobospan" id="kobo.620.1">, and preprocesses them to remove headers, footers, </span><span><span class="kobospan" id="kobo.621.1">and quotes:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.622.1">
categories = ['rec.</span><strong class="bold1"><span class="kobospan1" id="kobo.623.1">autos'</span></strong><span class="kobospan1" id="kobo.624.1">, 'rec.</span><strong class="bold1"><span class="kobospan1" id="kobo.625.1">motorcycles'</span></strong><span class="kobospan1" id="kobo.626.1">]
</span><strong class="bold1"><span class="kobospan1" id="kobo.627.1">remove </span></strong><span class="kobospan1" id="kobo.628.1">= ('headers', 'footers', 'quotes')
</span><strong class="bold1"><span class="kobospan1" id="kobo.629.1">newsgroups_train</span></strong><span class="kobospan1" id="kobo.630.1"> = fetch_20newsgroups(subset=</span><strong class="bold1"><span class="kobospan1" id="kobo.631.1">'train'</span></strong><span class="kobospan1" id="kobo.632.1">, 
    categories=categories, remove=remove, shuffle=False)
</span><strong class="bold1"><span class="kobospan1" id="kobo.633.1">newsgroups_test</span></strong><span class="kobospan1" id="kobo.634.1"> = fetch_20newsgroups(subset=</span><strong class="bold1"><span class="kobospan1" id="kobo.635.1">'test'</span></strong><span class="kobospan1" id="kobo.636.1">, 
    categories=categories, remove=remove, shuffle=False)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.637.1">Next, we create two </span><strong class="source-inline1"><span class="kobospan" id="kobo.638.1">TfidfVectorizer</span></strong><span class="kobospan" id="kobo.639.1"> objects: one using word n-grams in the range of 1 to 3 words, and the other using character n-grams in the range of 1 to 10 characters. </span><span class="kobospan" id="kobo.639.2">These vectorizers convert text documents into numerical feature vectors based on the relative frequency of n-grams within each document compared to the entire set of documents. </span><span class="kobospan" id="kobo.639.3">These two vectorizers are then combined into a </span><a id="_idIndexMarker738" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.640.1">single </span><strong class="source-inline1"><span class="kobospan" id="kobo.641.1">vectorizer</span></strong><span class="kobospan" id="kobo.642.1"> instance to extract features from the provided </span><span><span class="kobospan" id="kobo.643.1">newsgroup messages:</span></span><pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.644.1">word_vectorizer </span></strong><span class="kobospan1" id="kobo.645.1">= TfidfVectorizer(analyzer=</span><strong class="bold1"><span class="kobospan1" id="kobo.646.1">'word'</span></strong><span class="kobospan1" id="kobo.647.1">, 
    sublinear_tf=True, max_df=0.5, min_df=5, 
    stop_words="english", </span><strong class="bold1"><span class="kobospan1" id="kobo.648.1">ngram_range=(1, 3)</span></strong><span class="kobospan1" id="kobo.649.1">)
</span><strong class="bold1"><span class="kobospan1" id="kobo.650.1">char_vectorizer</span></strong><span class="kobospan1" id="kobo.651.1"> = TfidfVectorizer(analyzer=</span><strong class="bold1"><span class="kobospan1" id="kobo.652.1">'char'</span></strong><span class="kobospan1" id="kobo.653.1">, 
    sublinear_tf=True, max_df=0.5, 
    min_df=5, </span><strong class="bold1"><span class="kobospan1" id="kobo.654.1">ngram_range=(1, 10)</span></strong><span class="kobospan1" id="kobo.655.1">)
vectorizer = </span><strong class="bold1"><span class="kobospan1" id="kobo.656.1">FeatureUnion</span></strong><span class="kobospan1" id="kobo.657.1">([('word_vectorizer', word_vectorizer), 
    ('char_vectorizer', char_vectorizer)])</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.658.1">We proceed by allowing the </span><strong class="source-inline1"><span class="kobospan" id="kobo.659.1">vectorizer</span></strong><span class="kobospan" id="kobo.660.1"> instance to “learn” the relevant n-gram information from the training data, and then convert both the training and test data into datasets of vectors containing their corresponding </span><span><span class="kobospan" id="kobo.661.1">n-gram-based features:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.662.1">
self.X_train = vectorizer.</span><strong class="bold1"><span class="kobospan1" id="kobo.663.1">fit_transform</span></strong><span class="kobospan1" id="kobo.664.1">(newsgroups_train.data)
self.y_train = newsgroups_train.target
self.X_test = vectorizer.</span><strong class="bold1"><span class="kobospan1" id="kobo.665.1">transform</span></strong><span class="kobospan1" id="kobo.666.1">(newsgroups_test.data)
self.y_test = newsgroups_test.target</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.667.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.668.1">get_predictions()</span></strong><span class="kobospan" id="kobo.669.1"> method generates “reduced” versions of both the training and testing sets, utilizing the subset of features provided via the </span><strong class="source-inline1"><span class="kobospan" id="kobo.670.1">features_indices</span></strong><span class="kobospan" id="kobo.671.1"> parameter. </span><span class="kobospan" id="kobo.671.2">It subsequently employs an instance of </span><strong class="source-inline1"><span class="kobospan" id="kobo.672.1">MultinomialNB</span></strong><span class="kobospan" id="kobo.673.1">, a classifier commonly used in the context of text classification, which trains </span><a id="_idIndexMarker739" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.674.1">on the reduced training set and generates predictions for the reduced </span><span><span class="kobospan" id="kobo.675.1">testing set:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.676.1">
reduced_X_train = self.X_train[:, </span><strong class="bold1"><span class="kobospan1" id="kobo.677.1">features_indices</span></strong><span class="kobospan1" id="kobo.678.1">]
reduced_X_test = self.X_test[:, </span><strong class="bold1"><span class="kobospan1" id="kobo.679.1">features_indices</span></strong><span class="kobospan1" id="kobo.680.1">]
classifier = </span><strong class="bold1"><span class="kobospan1" id="kobo.681.1">MultinomialNB</span></strong><span class="kobospan1" id="kobo.682.1">(alpha=.01)
classifier.</span><strong class="bold1"><span class="kobospan1" id="kobo.683.1">fit</span></strong><span class="kobospan1" id="kobo.684.1">(reduced_X_train, self.y_train)
return classifier.</span><strong class="bold1"><span class="kobospan1" id="kobo.685.1">predict</span></strong><span class="kobospan1" id="kobo.686.1">(reduced_X_test)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.687.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.688.1">get_accuracy()</span></strong><span class="kobospan" id="kobo.689.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.690.1">get_f1_score()</span></strong><span class="kobospan" id="kobo.691.1"> methods use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.692.1">get_predictions()</span></strong><span class="kobospan" id="kobo.693.1"> method to calculate and return the accuracy and the f-score of the </span><span><span class="kobospan" id="kobo.694.1">classifier, respectively.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.695.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.696.1">main()</span></strong><span class="kobospan" id="kobo.697.1"> method creates an instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.698.1">NewsgroupClassifier</span></strong><span class="kobospan" id="kobo.699.1"> class and then trains and tests it using the full set of features. </span><span class="kobospan" id="kobo.699.2">It then repeats the training and evaluation using a random subset of features of the </span><span><span class="kobospan" id="kobo.700.1">desired size.</span></span><p class="calibre3"><span class="kobospan" id="kobo.701.1">Running the </span><strong class="source-inline"><span class="kobospan" id="kobo.702.1">main()</span></strong><span class="kobospan" id="kobo.703.1"> method yields the </span><span><span class="kobospan" id="kobo.704.1">following output:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.705.1">
Initializing newsgroup data...
</span><span class="kobospan1" id="kobo.705.2">Number of features = 51280, train set size = 1192, test set size = 794
f1 score using all features: 0.8727376310606889
f1 score using random subset of 100 features: 0.589931144127823</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.706.1">We can see that using all 51,280 features, the classifier can achieve an f1-score of 0.87, while using a random subset of 100 features has brought the score down to 0.59. </span><span class="kobospan" id="kobo.706.2">Let’s find out if selecting the subset of features using a genetic algorithm will enable us to get closer to a </span><span><span class="kobospan" id="kobo.707.1">higher score.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.708.1">Finding the best feature subset using a genetic algorithm</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.709.1">The genetic algorithm-based </span><a id="_idIndexMarker740" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.710.1">search for the best subset of 100 features (out of the original 51,280) is implemented by the </span><strong class="source-inline"><span class="kobospan" id="kobo.711.1">02_solve_newsgroups.py</span></strong><span class="kobospan" id="kobo.712.1"> Python program, which is located at the </span><span><span class="kobospan" id="kobo.713.1">following link:</span></span></p>
<p class="calibre3"><a href="https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.714.1">https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.715.1">The following steps describe the main parts of </span><span><span class="kobospan" id="kobo.716.1">this program:</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.717.1">We start by creating an</span><a id="_idIndexMarker741" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.718.1"> instance of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.719.1">NewsgroupClassifier</span></strong><span class="kobospan" id="kobo.720.1"> class that will allow us to test the various fixed-size </span><span><span class="kobospan" id="kobo.721.1">feature subsets:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.722.1">
ngc = </span><strong class="bold1"><span class="kobospan1" id="kobo.723.1">NewsgroupClassifier</span></strong><span class="kobospan1" id="kobo.724.1">(RANDOM_SEED)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.725.1">We then define two specialized fixed-subset genetic operators, </span><strong class="source-inline1"><span class="kobospan" id="kobo.726.1">cxSubset()</span></strong><span class="kobospan" id="kobo.727.1">—implementing the crossover—and </span><strong class="source-inline1"><span class="kobospan" id="kobo.728.1">mutSubset()</span></strong><span class="kobospan" id="kobo.729.1">— implementing the mutation, as we </span><span><span class="kobospan" id="kobo.730.1">discussed earlier.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.731.1">Since our goal is to maximize the f1-score of the classifier, we define a single-objective strategy for </span><span><span class="kobospan" id="kobo.732.1">maximizing fitness:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.733.1">
creator.create("</span><strong class="bold1"><span class="kobospan1" id="kobo.734.1">FitnessMax</span></strong><span class="kobospan1" id="kobo.735.1">", base.Fitness, weights=(1.0,))</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.736.1">To create random individuals representing feature indices, we create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.737.1">randomOrder()</span></strong><span class="kobospan" id="kobo.738.1"> function, which utilizes </span><strong class="source-inline1"><span class="kobospan" id="kobo.739.1">random.sample()</span></strong><span class="kobospan" id="kobo.740.1"> to generate a random set of indices within the desired range of 51,280. </span><span class="kobospan" id="kobo.740.2">We can then use this function to </span><span><span class="kobospan" id="kobo.741.1">create individuals:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.742.1">
toolbox.register("randomOrder", </span><strong class="bold1"><span class="kobospan1" id="kobo.743.1">random.sample</span></strong><span class="kobospan1" id="kobo.744.1">, range(len(ngc)), 
    </span><strong class="bold1"><span class="kobospan1" id="kobo.745.1">SUBSET_SIZE</span></strong><span class="kobospan1" id="kobo.746.1">)
toolbox.register("individualCreator", tools.initIterate, 
    creator.Individual, toolbox.</span><strong class="bold1"><span class="kobospan1" id="kobo.747.1">randomOrder</span></strong><span class="kobospan1" id="kobo.748.1">)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.749.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.750.1">get_score()</span></strong><span class="kobospan" id="kobo.751.1"> function is used to evaluate the fitness of each solution (or subset of features) by</span><a id="_idIndexMarker742" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.752.1"> calling the </span><strong class="source-inline1"><span class="kobospan" id="kobo.753.1">get_f1_score()</span></strong><span class="kobospan" id="kobo.754.1"> method of the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.755.1">NewsgroupClassifier</span></strong></span><span><span class="kobospan" id="kobo.756.1"> instance:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.757.1">
def </span><strong class="bold1"><span class="kobospan1" id="kobo.758.1">get_score</span></strong><span class="kobospan1" id="kobo.759.1">(individual):
    return ngc.</span><strong class="bold1"><span class="kobospan1" id="kobo.760.1">get_f1_score</span></strong><span class="kobospan1" id="kobo.761.1">(individual),
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.762.1">evaluate</span></strong><span class="kobospan1" id="kobo.763.1">", get_score)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.764.1">Now, we need to define genetic operators. </span><span class="kobospan" id="kobo.764.2">While for the </span><em class="italic"><span class="kobospan" id="kobo.765.1">selection</span></em><span class="kobospan" id="kobo.766.1"> operator, we use the usual </span><em class="italic"><span class="kobospan" id="kobo.767.1">tournament selection</span></em><span class="kobospan" id="kobo.768.1"> with a tournament size of 2, we choose the specialized </span><em class="italic"><span class="kobospan" id="kobo.769.1">crossover</span></em><span class="kobospan" id="kobo.770.1"> and </span><em class="italic"><span class="kobospan" id="kobo.771.1">mutation</span></em><span class="kobospan" id="kobo.772.1"> functions that we </span><span><span class="kobospan" id="kobo.773.1">defined earlier:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.774.1">
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.775.1">select</span></strong><span class="kobospan1" id="kobo.776.1">", tools.selTournament, tournsize=2)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.777.1">mate</span></strong><span class="kobospan1" id="kobo.778.1">", cxSubset)
toolbox.register("</span><strong class="bold1"><span class="kobospan1" id="kobo.779.1">mutate</span></strong><span class="kobospan1" id="kobo.780.1">", mutSubset, indpb=1.0/SUBSET_SIZE)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.781.1">Finally, it is time to invoke the genetic algorithm flow, where we continue to employ the elitist approach, where the HOF members—the current best individuals—are always passed untouched to the </span><span><span class="kobospan" id="kobo.782.1">next generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.783.1">
population, logbook = </span><strong class="bold1"><span class="kobospan1" id="kobo.784.1">eaSimple</span></strong><span class="kobospan1" id="kobo.785.1">(
    population,
    toolbox,
    cxpb=P_CROSSOVER,
    mutpb=P_MUTATION,
    ngen=MAX_GENERATIONS,
    stats=stats,
    halloffame=hof,
    verbose=True)</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.786.1">By running the algorithm </span><a id="_idIndexMarker743" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.787.1">for 5 generations with a population size of 30, we get the </span><span><span class="kobospan" id="kobo.788.1">following outcome:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.789.1">
Initializing newsgroup data...
</span><span class="kobospan1" id="kobo.789.2">Number of features = 51280, train set size = 1192, test set size = 794
gen     nevals  max             avg
0       200     0.639922        0.526988
1       166     0.639922        0.544121
2       174     0.663326        0.557525
3       173     0.669138        0.574895
...
</span><span class="kobospan1" id="kobo.789.3">198     170     0.852034        0.788416
199     172     0.852034        0.786208
200     167     0.852034        0.788501
-- Best Ever Fitness =  0.8520343720882079
-- Features subset selected =
1:    5074 = char_vectorizer__ crit
2:    12016 = char_vectorizer__=oo
3:    18081 = char_vectorizer__d usi
...</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.790.1">The results demonstrate that we successfully identified a subset of 100 features with an f1-score of 85.2%, which is remarkably close to the 87.2% score achieved using all </span><span><span class="kobospan" id="kobo.791.1">51,280 features.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.792.1">When examining the plots displaying the maximum and average fitness over the generations, shown next, it suggests that further improvements might have been possible had we extended </span><a id="_idIndexMarker744" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.793.1">the </span><span><span class="kobospan" id="kobo.794.1">evolutionary process:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer134">
<span class="kobospan" id="kobo.795.1"><img alt="Figure 11.3: Stats of the program searching for the best feature subset" src="image/B20851_11_3.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.796.1">Figure 11.3: Stats of the program searching for the best feature subset</span></p>
<h4 class="calibre19"><span class="kobospan" id="kobo.797.1">Further reducing the subset size</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.798.1">What if we aim to further reduce the subset size to just 10 features? </span><span class="kobospan" id="kobo.798.2">The outcome may surprise you. </span><span class="kobospan" id="kobo.798.3">By adjusting the </span><strong class="source-inline"><span class="kobospan" id="kobo.799.1">SUBSET_SIZE</span></strong><span class="kobospan" id="kobo.800.1"> constant to 10, we still achieve a commendable f1-score of 76.1%. </span><span class="kobospan" id="kobo.800.2">Notably, when we examine the 10 selected features, they appear to be fragments </span><a id="_idIndexMarker745" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.801.1">of familiar words. </span><span class="kobospan" id="kobo.801.2">In the context of our classification task, which involves distinguishing between posts in a newsgroup dedicated to motorcycles and those related to cars, these features start to reveal </span><span><span class="kobospan" id="kobo.802.1">their relevance:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.803.1">
-- Features subset selected =
1:    16440 = char_vectorizer__car
2:    18813 = char_vectorizer__dod
3:    50905 = char_vectorizer__yamah
4:    18315 = char_vectorizer__dar
5:    10373 = char_vectorizer__. </span><span class="kobospan1" id="kobo.803.2">The
6:    6586 = char_vectorizer__ mu
7:    4747 = char_vectorizer__ bik
8:    4439 = char_vectorizer__ als
9:    15260 = char_vectorizer__ave
10:    40719 = char_vectorizer__rcy</span></pre> <h4 class="calibre19"><span class="kobospan" id="kobo.804.1">Removing the character n-grams</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.805.1">The preceding results raise the</span><a id="_idIndexMarker746" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.806.1"> question of whether we should exclusively utilize word n-grams and eliminate character n-grams. </span><span class="kobospan" id="kobo.806.2">We can implement this by employing a single vectorizer, </span><span><span class="kobospan" id="kobo.807.1">as follows:</span></span></p>
<pre class="source-code">
<strong class="bold1"><span class="kobospan1" id="kobo.808.1">vectorizer </span></strong><span class="kobospan1" id="kobo.809.1">= TfidfVectorizer(analyzer=</span><strong class="bold1"><span class="kobospan1" id="kobo.810.1">'word'</span></strong><span class="kobospan1" id="kobo.811.1">, sublinear_tf=True, max_df=0.5, min_df=5, stop_words="english", ngram_range=(1, 3))</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.812.1">The results of running the </span><strong class="source-inline"><span class="kobospan" id="kobo.813.1">newsgroup_classifier.py</span></strong><span class="kobospan" id="kobo.814.1"> program are </span><span><span class="kobospan" id="kobo.815.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.816.1">
Initializing newsgroup data...
</span><span class="kobospan1" id="kobo.816.2">Number of features = 2666, train set size = 1192, test set size = 794
f1 score using all features: 0.8551359241014413
f1 score using random subset of 100 features: 0.6333756056319708</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.817.1">These results suggest that exclusively using word n-grams can achieve comparable performance to the original approach while using a significantly smaller feature set (</span><span><span class="kobospan" id="kobo.818.1">2,666 features).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.819.1">If we now run the genetic algorithm again, the results are </span><span><span class="kobospan" id="kobo.820.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.821.1">
-- Best Ever Fitness =  0.750101164515984
-- Features subset selected =
1:    1669 = oil change
2:    472 = cars
3:    459 = car
4:    361 = bike
5:    725 = detector
6:    303 = autos
7:    296 = auto
8:    998 = ford
9:    2429 = toyota
10:    2510 = v6</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.822.1">This set of selected features </span><a id="_idIndexMarker747" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.823.1">makes a lot of sense within the context of our classification task and provides insights into how the </span><span><span class="kobospan" id="kobo.824.1">classifier operates.</span></span></p>
<h1 id="_idParaDest-254" class="calibre5"><a id="_idTextAnchor306" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.825.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.826.1">In this chapter, we delved into the rapidly evolving field of NLP. </span><span class="kobospan" id="kobo.826.2">We began by exploring word embeddings and their diverse applications. </span><span class="kobospan" id="kobo.826.3">Our journey led us to experiment with solving the mystery-word game using genetic algorithms, where word embedding vectors served as the genetic chromosome. </span><span class="kobospan" id="kobo.826.4">Following this, we ventured into n-grams and their role in document classification through a newsgroup message classifier. </span><span class="kobospan" id="kobo.826.5">In this context, we harnessed the power of genetic algorithms to identify a compact yet effective subset of n-gram features derived from </span><span><span class="kobospan" id="kobo.827.1">the dataset.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.828.1">Finally, we endeavored to minimize the feature subset, aiming to gain insights into the classifier’s operations and interpret the factors influencing its predictions. </span><span class="kobospan" id="kobo.828.2">In the next chapter, we will delve deeper into the realm of explainable and interpretable AI while applying </span><span><span class="kobospan" id="kobo.829.1">genetic algorithms.</span></span></p>
<h1 id="_idParaDest-255" class="calibre5"><a id="_idTextAnchor307" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.830.1">Further reading</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.831.1">For more information on the topics that were covered in this chapter, please refer to the </span><span><span class="kobospan" id="kobo.832.1">following resources:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.833.1">Hands-On Python Natural Language Processing</span></em><span class="kobospan" id="kobo.834.1"> by </span><em class="italic"><span class="kobospan" id="kobo.835.1">Aman Kedia</span></em><span class="kobospan" id="kobo.836.1"> and</span><em class="italic"><span class="kobospan" id="kobo.837.1"> Mayank Rasu</span></em><span class="kobospan" id="kobo.838.1">, </span><em class="italic"><span class="kobospan" id="kobo.839.1">June </span></em><span><em class="italic"><span class="kobospan" id="kobo.840.1">26, 2020</span></em></span></li>
<li class="calibre11"><em class="italic"><span class="kobospan" id="kobo.841.1">Semantle</span></em><span class="kobospan" id="kobo.842.1"> word </span><span><span class="kobospan" id="kobo.843.1">game: </span></span><a href="https://semantle.com/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.844.1">https://semantle.com/</span></span></a></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.845.1">scikit-learn</span></strong><span class="kobospan" id="kobo.846.1"> 20 newsgroups </span><span><span class="kobospan" id="kobo.847.1">dataset: </span></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.848.1">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html</span></span></a></li>
<li class="calibre11"><strong class="source-inline1"><span class="kobospan" id="kobo.849.1">scikit-learn</span></strong> <span><strong class="source-inline1"><span class="kobospan" id="kobo.850.1">TfidfVectorizer</span></strong></span><span><span class="kobospan" id="kobo.851.1">: </span></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.852.1">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</span></span></a></li>
</ul>
</div>
</body></html>