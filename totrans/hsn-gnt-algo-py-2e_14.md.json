["```py\n[0.11008 -0.38781 -0.57615 -0.27714 0.70521 0.53994 -1.0786 -0.40146 1.1504 -0.5678 0.0038977 0.52878 0.64561 0.47262  0.48549 -0.18407 0.1801 0.91397 -1.1979 -0.5778 -0.37985  0.33606 0.772 0.75555 0.45506 -1.7671 -1.0503 0.42566 0.41893 -0.68327 1.5673 0.27685 -0.61708 0.64638 -0.076996 0.37118 0.1308 -0.45137 0.25398 -0.74392 -0.086199 0.24068 -0.64819 0.83549 1.2502 -0.51379 0.04224 -0.88118 0.7158 0.38519]\n```", "```py\n    if not isfile(model_path):\n        self._download_and_save_model(model_path)\n    print(f\"Loading model '{self.model_name}' from local file...\")\n    self.model = KeyedVectors.load_word2vec_format(model_path, \n        binary=True)\n    ```", "```py\n    self.embeddings = Embeddings(model_name=MODEL)\n         self.mystery_word = given_mystery_word if\n              given_mystery_word else\n              self.embeddings.pick_random_embedding()\n    ```", "```py\n    if self.embeddings.has_word(guess_word):\n        score = 100 *\n        self.embeddings.get_similarity(self.mystery_word,\n        guess_word)\n    else:\n        score = -100\n    ```", "```py\n    game = MysteryWordGame(given_mystery_word=\"dog\")\n    print(\"-- Checking candidate guess words:\")\n    for guess_word in [\"computer\", \"asdghf\", \"canine\", \"hound\", \n        \"poodle\", \"puppy\", \"cat\", \"dog\"]:\n        score = game.score_guess(guess_word)\n        print(f\"- current guess: {guess_word.ljust(10)} => \n            score = {score:.2f}\")\n    ```", "```py\nLoading model 'glove-twitter-50' from local file...\n--- Mystery word is 'dog' — game on!\n-- Checking candidate guess words:\n- current guess: computer   => score = 54.05\n- current guess: asdghf     => score = -100.00\n- current guess: canine     => score = 47.07\n- current guess: hound      => score = 64.93\n- current guess: poodle     => score = 65.90\n- current guess: puppy      => score = 87.90\n- current guess: cat        => score = 94.30\n- current guess: dog        => score = 100.00\n```", "```py\nif max_fitness and halloffame.items[0].fitness.values[0] >= \n    max_fitness:\n    break\n```", "```py\nif verbose:\n    print(f\"{logbook.stream} => {embeddings.vec2_nearest_word(\n        np.asarray(halloffame.items[0]))}\")\n```", "```py\n    embeddings = Embeddings(model_name='glove-wiki-gigaword-50', \n        randomSeed=RANDOM_SEED)\n    VECTOR_SIZE = embeddings.get_vector_size()\n    ```", "```py\n    game = MysteryWordGame(given_mystery_word='dog')\n    ```", "```py\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    ```", "```py\n    def randomFloat(low, up):\n        return [random.uniform(l, u) for l, u in zip([low] * \n            VECTOR_SIZE, [up] * VECTOR_SIZE)]\n    toolbox.register(\"attrFloat\", randomFloat, BOUNDS_LOW, \n        BOUNDS_HIGH)\n    ```", "```py\n    def score(individual):\n        guess_word = embeddings.vec2_nearest_word(\n            np.asarray(individual))\n        return game.score_guess(guess_word),\n    toolbox.register(\"evaluate\", score)\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\",\n                     tools.cxSimulatedBinaryBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR)\n    toolbox.register(\"mutate\",\n                     tools.mutPolynomialBounded,\n                     low=BOUNDS_LOW,\n                     up=BOUNDS_HIGH,\n                     eta=CROWDING_FACTOR,\n                     indpb=1.0 / NUM_OF_PARAMS)\n    ```", "```py\n    population, logbook = eaSimple_modified(\n        population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        max_fitness=MAX_SCORE,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\nLoading model 'glove-wiki-gigaword-50' from local file...\nLoading model 'glove-twitter-50' from local file...\n--- Mistery word is 'dog' — game on!\ngen     nevals  max     avg\n0       30      51.3262 -43.8478 => stories\n1       25      51.3262 -17.5409 => stories\n2       26      51.3262 -1.20704 => stories\n3       26      51.3262 11.1749  => stories\n4       26      64.7724 26.23    => bucket\n5       25      64.7724 40.0518  => bucket\n6       26      67.487  42.003   => toys\n7       26      69.455  37.0863  => family\n8       25      69.455  48.1514  => family\n9       25      69.455  38.5332  => family\n10      27      87.2265 47.9803  => pet\n11      26      87.2265 46.3378  => pet\n12      27      87.2265 40.0165  => pet\n13      27      87.2265 52.6842  => pet\n14      26      87.2265 59.186   => pet\n15      27      87.2265 41.5553  => pet\n16      27      87.2265 49.529   => pet\n17      27      87.2265 50.9414  => pet\n18      27      87.2265 44.9691  => pet\n19      25      87.2265 30.8624  => pet\n20      27      100     63.5354  => dog\nBest Solution = dog\nBest Score = 100.00\n```", "```py\n    categories = ['rec.autos', 'rec.motorcycles']\n    remove = ('headers', 'footers', 'quotes')\n    newsgroups_train = fetch_20newsgroups(subset='train', \n        categories=categories, remove=remove, shuffle=False)\n    newsgroups_test = fetch_20newsgroups(subset='test', \n        categories=categories, remove=remove, shuffle=False)\n    ```", "```py\n    word_vectorizer = TfidfVectorizer(analyzer='word', \n        sublinear_tf=True, max_df=0.5, min_df=5, \n        stop_words=\"english\", ngram_range=(1, 3))\n    char_vectorizer = TfidfVectorizer(analyzer='char', \n        sublinear_tf=True, max_df=0.5, \n        min_df=5, ngram_range=(1, 10))\n    vectorizer = FeatureUnion([('word_vectorizer', word_vectorizer), \n        ('char_vectorizer', char_vectorizer)])\n    ```", "```py\n    self.X_train = vectorizer.fit_transform(newsgroups_train.data)\n    self.y_train = newsgroups_train.target\n    self.X_test = vectorizer.transform(newsgroups_test.data)\n    self.y_test = newsgroups_test.target\n    ```", "```py\n    reduced_X_train = self.X_train[:, features_indices]\n    reduced_X_test = self.X_test[:, features_indices]\n    classifier = MultinomialNB(alpha=.01)\n    classifier.fit(reduced_X_train, self.y_train)\n    return classifier.predict(reduced_X_test)\n    ```", "```py\n    Initializing newsgroup data...\n    Number of features = 51280, train set size = 1192, test set size = 794\n    f1 score using all features: 0.8727376310606889\n    f1 score using random subset of 100 features: 0.589931144127823\n    ```", "```py\n    ngc = NewsgroupClassifier(RANDOM_SEED)\n    ```", "```py\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n    ```", "```py\n    toolbox.register(\"randomOrder\", random.sample, range(len(ngc)), \n        SUBSET_SIZE)\n    toolbox.register(\"individualCreator\", tools.initIterate, \n        creator.Individual, toolbox.randomOrder)\n    ```", "```py\n    def get_score(individual):\n        return ngc.get_f1_score(individual),\n    toolbox.register(\"evaluate\", get_score)\n    ```", "```py\n    toolbox.register(\"select\", tools.selTournament, tournsize=2)\n    toolbox.register(\"mate\", cxSubset)\n    toolbox.register(\"mutate\", mutSubset, indpb=1.0/SUBSET_SIZE)\n    ```", "```py\n    population, logbook = eaSimple(\n        population,\n        toolbox,\n        cxpb=P_CROSSOVER,\n        mutpb=P_MUTATION,\n        ngen=MAX_GENERATIONS,\n        stats=stats,\n        halloffame=hof,\n        verbose=True)\n    ```", "```py\nInitializing newsgroup data...\nNumber of features = 51280, train set size = 1192, test set size = 794\ngen     nevals  max             avg\n0       200     0.639922        0.526988\n1       166     0.639922        0.544121\n2       174     0.663326        0.557525\n3       173     0.669138        0.574895\n...\n198     170     0.852034        0.788416\n199     172     0.852034        0.786208\n200     167     0.852034        0.788501\n-- Best Ever Fitness =  0.8520343720882079\n-- Features subset selected =\n1:    5074 = char_vectorizer__ crit\n2:    12016 = char_vectorizer__=oo\n3:    18081 = char_vectorizer__d usi\n...\n```", "```py\n-- Features subset selected =\n1:    16440 = char_vectorizer__car\n2:    18813 = char_vectorizer__dod\n3:    50905 = char_vectorizer__yamah\n4:    18315 = char_vectorizer__dar\n5:    10373 = char_vectorizer__. The\n6:    6586 = char_vectorizer__ mu\n7:    4747 = char_vectorizer__ bik\n8:    4439 = char_vectorizer__ als\n9:    15260 = char_vectorizer__ave\n10:    40719 = char_vectorizer__rcy\n```", "```py\nnewsgroup_classifier.py program are as follows:\n\n```", "```py\n\n These results suggest that exclusively using word n-grams can achieve comparable performance to the original approach while using a significantly smaller feature set (2,666 features).\nIf we now run the genetic algorithm again, the results are the following:\n\n```", "```py\n\n This set of selected features makes a lot of sense within the context of our classification task and provides insights into how the classifier operates.\nSummary\nIn this chapter, we delved into the rapidly evolving field of NLP. We began by exploring word embeddings and their diverse applications. Our journey led us to experiment with solving the mystery-word game using genetic algorithms, where word embedding vectors served as the genetic chromosome. Following this, we ventured into n-grams and their role in document classification through a newsgroup message classifier. In this context, we harnessed the power of genetic algorithms to identify a compact yet effective subset of n-gram features derived from the dataset.\nFinally, we endeavored to minimize the feature subset, aiming to gain insights into the classifier’s operations and interpret the factors influencing its predictions. In the next chapter, we will delve deeper into the realm of explainable and interpretable AI while applying genetic algorithms.\nFurther reading\nFor more information on the topics that were covered in this chapter, please refer to the following resources:\n\n*   *Hands-On Python Natural Language Processing* by *Aman Kedia* and *Mayank Rasu*, *June* *26, 2020*\n*   *Semantle* word game: [https://semantle.com/](https://semantle.com/)\n*   **scikit-learn** 20 newsgroups dataset: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)\n*   **scikit-learn** **TfidfVectorizer**: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n\n```"]