- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explores how genetic algorithms can enhance the performance of
    **natural language processing** (**NLP**) tasks while offering insights into their
    underlying mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter begins by introducing the field of NLP and explaining the concept
    of **word embeddings**. We employ this technique to task a genetic algorithm with
    playing a *Semantle*-like mystery-word game, challenging it to guess the mystery
    word.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we investigate **n-grams** and **document classification**. We
    harness genetic algorithms to pinpoint a compact yet effective subset of features,
    shedding light on the classifier’s operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have achieved the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Become familiar with the field of NLP and its applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gained an understanding of the concept of word embeddings and their importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implemented a mystery-word game using word embeddings and created a genetic
    algorithms-driven player to guess the mystery word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquired knowledge about n-grams and their role in document processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developed a process to significantly reduce the size of the feature set used
    for message classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilized a minimal feature set to gain insights into the classifier’s operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start this chapter with a quick overview of NLP. If you are a seasoned
    data scientist, feel free to skip the introductory section.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using Python 3 with the following supporting libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**deap**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**numpy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pandas**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**matplotlib**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**seaborn**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gensim**—introduced in this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you use the **requirements.txt** file we provide (see [*Chapter 3*](B20851_03.xhtml#_idTextAnchor091)),
    these libraries are already included in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/tree/main/chapter_11)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/OEBOd](https://packt.link/OEBOd)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is a fascinating branch of **artificial intelligence** that focuses on the
    interaction between computers and human language. NLP combines linguistics, computer
    science, and **machine learning** to enable machines to understand, interpret,
    and generate human language in a way that’s both meaningful and useful. Over the
    last several years, NLP has been progressively taking on an increasing role in
    our daily lives, with applications spanning numerous domains, from virtual assistants
    and chatbots to sentiment analysis, language translation, and information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary goals of NLP is to bridge the communication gap between humans
    and machines; this is crucial as language is the principal medium through which
    people interact and communicate their thoughts, ideas, and desires. This goal
    of bridging the communication gap between humans and machines has driven significant
    advancements in the field of NLP. A recent notable milestone in this journey is
    the development of **large language models** (**LLMs**), such as OpenAI’s **ChatGPT**.
  prefs: []
  type: TYPE_NORMAL
- en: To create a bridge for human-computer communication, there must be a method
    in place that can transform human language into numerical representations, allowing
    machines to understand and process text data more effectively. One such technique
    is the use of **word embeddings**, described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embeddings are numerical representations of words in the English language
    (or other languages). Each word is encoded using a fixed-length vector of real
    numbers. These vectors effectively capture semantic and contextual information
    associated with the words they represent.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are created by training **neural networks** (**NNs**) to create
    numerical representations for words from large collections of written or spoken
    texts, where words with similar contexts are mapped to nearby points in a continuous
    vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Common techniques for creating word embeddings include **Word2Vec**, **Global
    Vectors for Word Representation** (**GloVe**), and **fastText**.
  prefs: []
  type: TYPE_NORMAL
- en: The typical dimensionality of word embeddings can vary, but common choices are
    50, 100, 200, or 300 dimensions. Higher-dimensional embeddings can capture more
    nuanced relationships but may require more data and computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the word “dog” in a 50-dimensional Word2Vec embedding space might
    look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each of these 50 values represents a different aspect of the word “dog” in the
    context of the training data. Related words, such as “cat” or “pet,” would have
    word vectors that are closer to the “dog” vector in this space, indicating their
    semantic similarity. These embeddings not only capture semantic information but
    also maintain relationships between words, enabling NLP models to understand word
    relationships, context, and even sentence- and document-level semantics.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure is a 2-dimensional visualization of 50-dimensional vectors
    representing various English words. This image was created using **t-Distributed
    Stochastic Neighbor Embedding** (**t-SNE**), a dimensionality reduction technique
    often used to visualize and explore word embeddings. t-SNE projects word embeddings
    into a lower-dimensional space while preserving relationships and similarities
    between data points. This figure demonstrates how certain groups of words, such
    as fruit or animals, are closer together. Relations between words are apparent
    as well—for example, the relation between “son” and “boy” resembles that between
    “daughter” and “girl:”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: Two-dimensional t-SNE plot of word embeddings](img/B20851_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Two-dimensional t-SNE plot of word embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to their traditional role in NLP, word embeddings can find use in
    genetic algorithms, as we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings and genetic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters of this book, we implemented numerous examples of genetic
    algorithms using fixed-length real-valued vectors (or lists) as the chromosome
    representation of candidate solutions. Given that word embeddings enable us to
    represent words (such as “dog”) using fixed-length vectors of real-valued numbers,
    these vectors can effectively serve as the genetic representation of words in
    genetic algorithm-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: This means we can leverage genetic algorithms to solve problems in which candidate
    solutions are words in the English language, utilizing word embeddings as the
    translation mechanism between words and their genetic representation.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this concept, we will embark on solving a fun word game using
    a genetic algorithm, as described in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the mystery word using genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, online mystery-word games have gained significant popularity.
    One standout among them is *Semantle*, a game that challenges you to guess the
    word of the day based on its meaning.
  prefs: []
  type: TYPE_NORMAL
- en: This game provides feedback on how semantically similar your guesses are to
    the target word and features a “Hot and Cold” meter that indicates the proximity
    of your guess to the secret word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Behind the scenes, Semantle employs word embeddings, specifically Word2Vec,
    to represent both the mystery word and players’ guesses. It calculates the semantic
    similarity between them by measuring the difference between their representations:
    the closer the vectors, the greater the resemblance between the words. The similarity
    score returned by the game ranges from -100 (very different from the answer) to
    100 (identical to the answer).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsections, we will create two Python programs. The first
    serves as a simulation of the Semantle game, while the other embodies a player
    or solver driven by a genetic algorithm, attempting to uncover the mystery word
    by maximizing the game’s similarity score. Both programs rely on word embedding
    models; however, to maintain a clear separation, mirroring a real-world scenario,
    each program employs its own, distinct model. The interaction between the player
    and the game is limited to exchanging actual guessed words and their corresponding
    scores, and no embedding vectors are exchanged. This is depicted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: Component diagram of the Python modules and their interaction](img/B20851_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Component diagram of the Python modules and their interaction'
  prefs: []
  type: TYPE_NORMAL
- en: To add an extra layer of intrigue, we’ve decided to have each program utilize
    a completely different embedding model. For that to work, we assume that both
    embedding models share a substantial overlap in their vocabularies.
  prefs: []
  type: TYPE_NORMAL
- en: The following section provides a detailed account of the Python implementation
    of these programs.
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will begin by creating the Python implementation of word embedding models
    using the `gensim` library, as detailed in the following subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The gensim library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `gensim` library is a versatile Python package primarily recognized for
    its role in NLP and text analysis tasks. `gensim` simplifies the process of working
    with word vectors by offering a comprehensive suite of tools for creating, training,
    and using word embeddings efficiently. One of its key features is its ability
    to serve as a provider of pre-trained word embedding models, of which we will
    take advantage in our first Python module, as described next.
  prefs: []
  type: TYPE_NORMAL
- en: The Embeddings class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start with a Python class called `Embeddings`, encapsulating a `gensim`-based
    pre-trained word embedding model. This class can be found in the `embeddings.py`
    file, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/embeddings.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main functionality of this class is highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class’s **__init__()** method initializes the random seed (if given), and
    then proceeds to initialize the chosen (or default) **gensim** model using the
    **_init_model()** and **_download_and_save_model()** private methods. The former
    method uploads the model’s embedding information from a local file, if available.
    Otherwise, the latter method downloads the model from the **gensim** repository,
    separates the essential part for embedding, **KeyedVectors**, and saves it locally
    to be used the next time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The **pick_random_embedding()** method can be used to pick a random word out
    of the model’s vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **get_similarity()** method is used to retrieve the similarity value of
    the model between two specified words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **vec2_nearest_word()** method utilizes the **gensim** model’s **similar_by_vector()**
    method to retrieve the word that is closest to the specified embedding vector.
    As we will see shortly, this enables the genetic algorithm to use arbitrary vectors
    (such as randomly generated ones) and have them represent an existing word in
    the model’s vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, the **list_models()** method can be used to retrieve and display information
    about the available embedding models provided by the **gensim** library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As mentioned earlier, this class is used by both the `Player` and `Game` components,
    discussed in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The MysteryWordGame class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `MysteryWordGame` Python class encapsulates the `Game` component. It can
    be found in the `mystery_word_game.py` file, which is located at the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/mystery_word_game.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main functionality of this class is highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The class employs the **glove-twitter-50** **gensim** pre-trained embedding
    model developed by Stanford University. This model was specifically designed for
    Twitter text data and utilizes 50-dimensional embedding vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **__init__()** method of the class initializes the embedding model it will
    internally use, and then either selects a random mystery word or uses a word specified
    as an argument for the mystery word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **score_guess()** method calculates the score returned by the game for
    a given guessed word. If the word is not present in the model’s vocabulary, which
    can occur as the player module may use a potentially different model, the score
    is set to the minimum value of -100\. Otherwise, the calculated score value will
    be a number between -100 and 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **main()** method tests the class’s functionality by creating an instance
    of the game, selecting the word **"dog"**, and evaluating several guessed words
    related to it, such as **"canine"** and **"hound"**. It also includes an unrelated
    word (**"computer"**) and a word that does not exist in the vocabulary (**"asdghf"**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Executing the `main()` method of the class yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready for the interesting component—the program that attempts to
    solve the game.
  prefs: []
  type: TYPE_NORMAL
- en: The genetic algorithms-based player program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, this module uses a different embedding model from the
    one used by the game, although it has the option to use the same model. In this
    case, we have selected the `glove-wiki-gigaword-50` `gensim` pre-trained embedding
    model, which was trained on a substantial corpus of text from the English *Wikipedia*
    website and the *Gigaword* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution representation in the genetic algorithm in this case is a real-valued
    vector (or a list) of the same dimension as the embedding model. This allows each
    solution to serve as an embedding vector, although not perfectly. Initially, the
    algorithm employs randomly generated vectors, and through crossover and mutation
    operations, it’s likely that at least some of the vectors won’t directly correspond
    to existing words in the model’s vocabulary. To address this issue, we utilize
    the `vec2_nearest_word()` method from the `Embedding` class, which returns the
    closest word in the vocabulary. This approach exemplifies the **genotype-to-phenotype
    mapping** concept, as discussed in [*Chapter 4*](B20851_04.xhtml#_idTextAnchor155),
    *Combinatorial Optimization*.
  prefs: []
  type: TYPE_NORMAL
- en: Early convergence criteria
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In most cases discussed so far, the solution does not possess the knowledge
    of the best possible score that can be achieved during the optimization process.
    However, in this case, we know that the best possible score is 100\. Once it is
    achieved, the correct word has been found, and there is no point in continuing
    the evolutionary cycle. Therefore, we modified the main loop of our genetic algorithm
    to break if the maximum score is reached. The modified method is called `eaSimple_modified()`
    and can be found in the `elitism_modified.py` module. It accepts an optional parameter
    called `max_fitness`. When this parameter is provided with a value, the main loop
    breaks if the best fitness value found so far reaches or exceeds this value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Printing out the current best-guessed word
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Additionally, the `eaSimple_modified()` method includes the printing of the
    guessed word corresponding to the individual with the best fitness found so far,
    as part of the statistics summary generated for every individual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The genetic algorithm implementation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The genetic algorithm-based player for the mystery-word game search for the
    best hyperparameter values is implemented by the `01_find_mystery_word.py` Python
    program, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/01_find_mystery_word.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe the main parts of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by creating an instance of the **Embeddings** class, which will serve
    as the word embeddings model for the solver program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create an instance of the **MysteryWordGame** class, which represents
    the game we will be playing. We instruct it to use the word “dog” for demonstration
    purposes. This word can later be replaced with others, or we can allow the game
    to choose a random word if we omit the **given_mystery_word** parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since our goal is to maximize the game’s score, we define a single-objective
    strategy for maximizing fitness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create random individuals representing word embeddings, we create a **randomFloat()**
    function and register it with the toolbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **score()** function is used to evaluate the fitness of each solution,
    and this process consists of two steps: first, we employ the local **embeddings**
    model to find the vocabulary word nearest to the evaluated vector (this is where
    the genotype-to-phenotype mapping takes place). Next, we send this word to the
    **Game** component and request it to score it as a guessed word. The score returned
    by the game, a value ranging from -100 to 100, is directly used as the fitness
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to define genetic operators. While for the *selection* operator,
    we use the usual *tournament selection* with a tournament size of 2, we choose
    *crossover* and *mutation* operators that are specialized for bounded float-list
    chromosomes and provide them with the boundaries we defined for each hyperparameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition, we continue to employ the elitist approach, where the **Hall of
    Fame** (**HOF**) members—the current best individuals—are always passed untouched
    to the next generation. However, in this iteration, we use the **eaSimple_modified**
    algorithm, where—in addition—the main loop will terminate when the score reaches
    the maximum known score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By running the algorithm with a population size of 30, we get the following
    outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'From this printout, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two distinct word embedding models were loaded, one for the player and the other
    for the game, as designed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mystery word that was set to **'dog'** was correctly guessed by the genetic
    algorithm-driven player after 20 generations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As soon as the word was found, the player quit playing, even though the maximum
    number of generations was set to 1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see how the current best-guessed word has evolved:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stories** → **bucket** → **toys** → **family** → **pet** → **dog**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This looks great! However, keep in mind that it’s just one example. You are
    encouraged to try out other words, as well as different settings for the genetic
    algorithm; perhaps even change the embedding models. Are there model pairs that
    are less compatible than others?
  prefs: []
  type: TYPE_NORMAL
- en: In the next portion of this chapter, we will explore **document classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Document classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document classification is a critical task in NLP, involving the categorization
    of textual documents into predefined classes or categories based on their content.
    This process is essential for organizing, managing, and extracting meaningful
    information from large volumes of textual data. Applications of document classification
    are vast and diverse, spanning various industries and domains.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of information retrieval, document classification plays a crucial
    role in **search engines**. By categorizing web pages, articles, and documents
    into relevant topics or genres, search engines can deliver more accurate and targeted
    search results to users. This enhances the overall user experience and ensures
    that individuals can quickly access the information they seek.
  prefs: []
  type: TYPE_NORMAL
- en: In customer service and support, document classification enables the **automatic
    routing** of customer inquiries and messages to the appropriate departments or
    teams. For instance, emails received by a company can be classified into categories
    such as “Billing Inquiries,” “Technical Support,” or “General Inquiries,” ensuring
    that each message reaches the right team for prompt response and resolution.
  prefs: []
  type: TYPE_NORMAL
- en: In the legal domain, document classification is instrumental for tasks such
    as **e-discovery**, where large volumes of legal documents need to be analyzed
    for relevance to a case. Classification helps identify documents that are potentially
    pertinent to a legal matter, streamlining the review process and reducing the
    time and resources required for legal proceedings.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, document classification is pivotal in **sentiment analysis**, where
    it can be used to categorize social media posts, reviews, and comments into positive,
    negative, or neutral sentiments. This information is invaluable for businesses
    looking to gauge customer feedback, monitor brand reputation, and make data-driven
    decisions to improve their products or services.
  prefs: []
  type: TYPE_NORMAL
- en: One effective method for performing document classification is by leveraging
    n-grams, as elaborated in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An n-gram is a contiguous sequence of *n* items, which can be characters, words,
    or even phrases, extracted from a larger body of text. By breaking down text into
    these smaller units, n-grams enable the extraction of valuable linguistic patterns,
    relationships, and context.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the case of *character n-grams*, a 3-gram might break the word
    “apple” into “app,” “ppl,” and “ple.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of *word n-grams*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unigrams (1-grams)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text*: “I love to code.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Unigrams*: [“I”, “love”, “to”, “code”]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Bigrams (2-grams)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text*: “Natural language processing is fascinating.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Bigrams*: [“Natural language”, “language processing”, “processing is”, “is
    fascinating”]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Trigrams (3-grams)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text*: “Machine learning models can generalize.”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Trigrams*: [“Machine learning models”, “learning models can”, “models can
    generalize”]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: N-grams provide insights into textual content by revealing the sequential arrangement
    of words or characters, identifying frequent patterns, and extracting features.
    They help understand language structure, context, and patterns, making them valuable
    for text analysis tasks such as document classification.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a subset of n-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B20851_07.xhtml#_idTextAnchor221), *Enhancing Machine Learning
    Models Using Feature Selection*, we demonstrated the importance of selecting a
    meaningful subset of features, known as “feature selection.” This process is equally
    valuable in document classification, especially when dealing with a large number
    of extracted n-grams, a common occurrence in large documents. The advantages of
    identifying a relevant subset of n-grams include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Reducing the number of n-grams makes computations
    more efficient and prevents overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Focus on key features**: Selecting discriminative n-grams helps the model
    concentrate on crucial features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise reduction**: Filtering out uninformative n-grams minimizes noise in
    the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced generalization**: A well-chosen subset improves the model’s ability
    to handle new documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: Smaller feature sets speed up model training and prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, identifying a relevant subset of n-grams in document classification
    can be valuable for model interpretability. By narrowing down the features to
    a manageable subset, it becomes easier to understand and interpret the factors
    influencing the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to what we did in [*Chapter 7*](B20851_07.xhtml#_idTextAnchor221),
    we will apply a genetic algorithms-based search here to identify a relevant subset
    of n-grams. However, considering that the number of n-grams we anticipate is substantially
    larger than the number of features in the common datasets we’ve previously used,
    we won’t be searching for the overall best subset. Instead, our goal will be to
    find a fixed-size subset of features, such as the best 1,000 or 100 n-grams to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Using genetic algorithms to search for a fixed-size subset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we need to identify a good, fixed-size subset of items within a very large
    group, let’s try to define the usual components needed for the genetic algorithm
    to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution representation**: Since the subset size is much smaller than the
    full dataset, it’s more efficient to use a fixed-size list of integers representing
    the indices of the items within the large dataset. For instance, if we aim to
    create a subset of size 3 from 100 items, a possible solution could be represented
    as a list, such as [5, 42, 88] or [73, 11, 42].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crossover operation**: To ensure valid offspring, we must prevent the same
    index from appearing more than once in each offspring. In the previous example,
    the item “42” appears in both lists. If we used a single-point crossover, for
    example, we could end up with the offspring [5, 42, 42], which in effect will
    have only two unique items rather than three. One simple crossover method that
    overcomes this issue would be as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a set containing all unique items present in both parents. In our example,
    this set would be {5, 11, 42, 73, 88}.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate offspring by randomly selecting from the set mentioned previously.
    Each offspring should select three items (in this case). A possible result could
    be [5, 11, 88] and [11, 42, 88].
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mutation operation**: A straightforward method to generate a valid mutated
    individual from an existing one is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each item in the list, with a specified probability, the item will be replaced
    by one that does exist in the current list.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if we consider the list [11, 42, 88], there’s a possibility that
    the second item (42) could be replaced with, say, 27, resulting in the list [11,
    27, 88].
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Python implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following sections, we will implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A document classifier that will train on document data from two newsgroups and
    use n-grams to predict to which newsgroup each document belongs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A genetic algorithms-driven optimizer that seeks to find the best subset of
    n-grams to use for this classification task, given the desired size of the subset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start with the class implementing the classifier, as described in the
    next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Newsgroup document classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start with a Python class called `NewsgroupClassifier`, implementing a `scikit-learn`-based
    document classifier that uses n-grams as features and learns to distinguish between
    posts from two different newsgroups. This class can be found in the `newsgroup_classifier.py`
    file, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/newsgroup_classifier.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main functionality of this class is highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class’s **init_data()** method, called by **__init__()**, creates training
    and testing sets from **scikit-learn**’s built-in dataset of newsgroup posts.
    It retrieves posts from two categories, **''rec.autos''** and **''rec.motorcycles''**,
    and preprocesses them to remove headers, footers, and quotes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create two **TfidfVectorizer** objects: one using word n-grams in
    the range of 1 to 3 words, and the other using character n-grams in the range
    of 1 to 10 characters. These vectorizers convert text documents into numerical
    feature vectors based on the relative frequency of n-grams within each document
    compared to the entire set of documents. These two vectorizers are then combined
    into a single **vectorizer** instance to extract features from the provided newsgroup
    messages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We proceed by allowing the **vectorizer** instance to “learn” the relevant
    n-gram information from the training data, and then convert both the training
    and test data into datasets of vectors containing their corresponding n-gram-based
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **get_predictions()** method generates “reduced” versions of both the training
    and testing sets, utilizing the subset of features provided via the **features_indices**
    parameter. It subsequently employs an instance of **MultinomialNB**, a classifier
    commonly used in the context of text classification, which trains on the reduced
    training set and generates predictions for the reduced testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The **get_accuracy()** and **get_f1_score()** methods use the **get_predictions()**
    method to calculate and return the accuracy and the f-score of the classifier,
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `main()` method yields the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that using all 51,280 features, the classifier can achieve an f1-score
    of 0.87, while using a random subset of 100 features has brought the score down
    to 0.59\. Let’s find out if selecting the subset of features using a genetic algorithm
    will enable us to get closer to a higher score.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best feature subset using a genetic algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The genetic algorithm-based search for the best subset of 100 features (out
    of the original 51,280) is implemented by the `02_solve_newsgroups.py` Python
    program, which is located at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py](https://github.com/PacktPublishing/Hands-On-Genetic-Algorithms-with-Python-Second-Edition/blob/main/chapter_11/02_solve_newsgroups.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps describe the main parts of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating an instance of the **NewsgroupClassifier** class that
    will allow us to test the various fixed-size feature subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We then define two specialized fixed-subset genetic operators, **cxSubset()**—implementing
    the crossover—and **mutSubset()**— implementing the mutation, as we discussed
    earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since our goal is to maximize the f1-score of the classifier, we define a single-objective
    strategy for maximizing fitness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create random individuals representing feature indices, we create a **randomOrder()**
    function, which utilizes **random.sample()** to generate a random set of indices
    within the desired range of 51,280\. We can then use this function to create individuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **get_score()** function is used to evaluate the fitness of each solution
    (or subset of features) by calling the **get_f1_score()** method of the **NewsgroupClassifier**
    instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to define genetic operators. While for the *selection* operator,
    we use the usual *tournament selection* with a tournament size of 2, we choose
    the specialized *crossover* and *mutation* functions that we defined earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, it is time to invoke the genetic algorithm flow, where we continue
    to employ the elitist approach, where the HOF members—the current best individuals—are
    always passed untouched to the next generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By running the algorithm for 5 generations with a population size of 30, we
    get the following outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The results demonstrate that we successfully identified a subset of 100 features
    with an f1-score of 85.2%, which is remarkably close to the 87.2% score achieved
    using all 51,280 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'When examining the plots displaying the maximum and average fitness over the
    generations, shown next, it suggests that further improvements might have been
    possible had we extended the evolutionary process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: Stats of the program searching for the best feature subset](img/B20851_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Stats of the program searching for the best feature subset'
  prefs: []
  type: TYPE_NORMAL
- en: Further reducing the subset size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What if we aim to further reduce the subset size to just 10 features? The outcome
    may surprise you. By adjusting the `SUBSET_SIZE` constant to 10, we still achieve
    a commendable f1-score of 76.1%. Notably, when we examine the 10 selected features,
    they appear to be fragments of familiar words. In the context of our classification
    task, which involves distinguishing between posts in a newsgroup dedicated to
    motorcycles and those related to cars, these features start to reveal their relevance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Removing the character n-grams
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The preceding results raise the question of whether we should exclusively utilize
    word n-grams and eliminate character n-grams. We can implement this by employing
    a single vectorizer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Initializing newsgroup data...
  prefs: []
  type: TYPE_NORMAL
- en: Number of features = 2666, train set size = 1192, test set size = 794
  prefs: []
  type: TYPE_NORMAL
- en: 'f1 score using all features: 0.8551359241014413'
  prefs: []
  type: TYPE_NORMAL
- en: 'f1 score using random subset of 100 features: 0.6333756056319708'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: -- Best Ever Fitness =  0.750101164515984
  prefs: []
  type: TYPE_NORMAL
- en: -- Features subset selected =
  prefs: []
  type: TYPE_NORMAL
- en: 1:    1669 = oil change
  prefs: []
  type: TYPE_NORMAL
- en: 2:    472 = cars
  prefs: []
  type: TYPE_NORMAL
- en: 3:    459 = car
  prefs: []
  type: TYPE_NORMAL
- en: 4:    361 = bike
  prefs: []
  type: TYPE_NORMAL
- en: 5:    725 = detector
  prefs: []
  type: TYPE_NORMAL
- en: 6:    303 = autos
  prefs: []
  type: TYPE_NORMAL
- en: 7:    296 = auto
  prefs: []
  type: TYPE_NORMAL
- en: 8:    998 = ford
  prefs: []
  type: TYPE_NORMAL
- en: 9:    2429 = toyota
  prefs: []
  type: TYPE_NORMAL
- en: 10:    2510 = v6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
