<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear Algebra</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be covering the main concepts of linear algebra, and the concepts learned here will serve as the backbone on which we will learn all the concepts in the chapters to come, so it is important that you pay attention. </p>
<p>It is very important for you to know that these chapters cannot be substituted for an education in mathematics; they exist merely to help you better grasp the concepts of deep learning and how various architectures work and to develop an intuition for why that is, so you can become a better practitioner in the field.</p>
<p>At its core, algebra is nothing more than the study of mathematical symbols and the rules for manipulating these symbols. The field of algebra acts as a unifier for all of mathematics and provides us with a way of thinking. Instead of using numbers, we use letters to represent variables. </p>
<p>Linear algebra, however, concerns only linear transformations and vector spaces. It allows us to represent information through vectors, matrices, and tensors, and having a good understanding of linear algebra will take you a long way on your journey toward getting a very strong understanding of deep learning. It is said that a mathematical problem can only be solved if it can be reduced to a calculation in linear algebra. This speaks to the power and usefulness of linear algebra.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Comparing scalars and vectors</li>
<li>Linear equations</li>
<li>Matrix operations</li>
<li>Vector spaces and subspaces</li>
<li>Linear maps</li>
<li>Matrix decompositions</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Comparing scalars and vectors</h1>
                </header>
            
            <article>
                
<p>Scalars are regular numbers, such as 7, 82, and 93,454. They only have a magnitude and are used to represent time, speed, distance, length, mass, work, power, area, volume, and so on.</p>
<p>Vectors, on the other hand, have magnitude and direction in many dimensions. We use vectors to represent velocity, acceleration, displacement, force, and momentum. We write vectors in bold—such as <strong><em>a</em></strong> instead of <em>a—</em>and they are usually an array of multiple numbers, with each number in this array being an element of the vector. </p>
<p>We denote this as follows:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="Images/182b5235-dd83-4e93-bb5f-7ffb0b494aa2.png" style="width:6.58em;height:8.17em;"/></p>
<p>Here, <img class="fm-editor-equation" src="Images/4e0dc3f8-4384-43dd-a3ee-ae289462d55f.png" style="width:4.33em;height:1.25em;"/>shows the vector is in <em>n</em>-dimensional real space, which results from taking the Cartesian product of <img class="fm-editor-equation" src="Images/e4c8153a-ac55-41a2-bded-96c31ad09cb7.png" style="width:1.08em;height:1.25em;"/> <em>n</em> times; <sub><img class="fm-editor-equation" src="Images/be3c5f05-bafc-4fa7-ba6a-7ef8fa77a41a.png" style="width:4.00em;height:1.42em;"/></sub> shows each element is a real number; <em>i</em> is the position of each element; and, finally, <img class="fm-editor-equation" src="Images/2e60c6b4-990a-48c3-ab6c-7be222119a91.png" style="width:3.58em;height:1.25em;"/> is a natural number, telling us how many elements are in the vector. </p>
<p>As with regular numbers, you can add and subtract vectors. However, there are some limitations. </p>
<p>Let's take the vector we saw earlier (<em>x</em>) and add it with another vector (<em>y</em>), both of which are in <img class="fm-editor-equation" src="Images/4e64ae8f-de5b-4140-9e5f-c10c59a33e28.png" style="width:1.75em;height:1.25em;"/>, so that the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3289f2fc-8c7f-4a4e-b3cd-326e542b31b2.png" style="width:18.08em;height:8.17em;"/></p>
<p>However, we cannot add vectors with vectors that do not have the same dimension or scalars.</p>
<p>Note that when <img class="fm-editor-equation" src="Images/f5721bc0-a2b6-415e-90d0-13304d25a4e7.png" style="width:3.00em;height:1.08em;"/> in <img class="fm-editor-equation" src="Images/8d0ad35c-1f10-4286-8a31-584a4c2afe33.png" style="width:1.50em;height:1.08em;"/>, we reduce to 2-dimensions (for example, the surface of a sheet of paper), and when <em>n = 3</em>, we reduce to 3-dimensions (the real world). </p>
<p>We can, however, multiply scalars with vectors. Let λ be an arbitrary scalar, which we will multiply with the vector <img class="fm-editor-equation" src="Images/a3312634-96a4-46f8-bdc8-624d2ab782aa.png" style="width:4.33em;height:1.25em;"/>, so that the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/eb17ae75-c9c2-47a1-a950-17b930f6b6b4.png" style="width:14.92em;height:8.17em;"/></p>
<p>As we can see, λ gets multiplied by each <em>x<sub>i</sub></em> in the vector. The result of this operation is that the vector gets scaled by the value of the scalar.</p>
<p>For example, let <img class="fm-editor-equation" src="Images/6479accd-0fcb-4c42-9a5e-f9d36bd87288.png" style="width:3.42em;height:1.25em;"/>, and <img class="fm-editor-equation" src="Images/017c6d94-ddc4-4711-9039-49e898bd9771.png" style="width:4.42em;height:4.17em;"/>. Then, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/77f4a99d-ec7c-4f3a-8474-5d7cf0bda91f.png" style="width:26.58em;height:5.50em;"/></p>
<p>While this works fine for multiplying by a whole number, it doesn't help when working with fractions, but you should be able to guess how it works. Let's see an example.</p>
<p>Let <img class="fm-editor-equation" src="Images/10896d38-44f3-4208-b47d-e9b8c3c7ad1f.png" style="width:3.00em;height:2.42em;"/>, and <img class="fm-editor-equation" src="Images/15e24cac-aff8-4358-a2ee-51bc9e15a589.png" style="width:4.25em;height:4.08em;"/>. Then, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7d0fc872-48e8-4ab3-bba0-5d6136ca205f.png" style="width:12.92em;height:6.25em;"/></p>
<p>There is a very special vector that we can get by multiplying any vector by the scalar, <strong>0</strong>. We denote this as <strong>0</strong> and call it the <strong>zero vector</strong> (a vector containing only zeros). </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear equations</h1>
                </header>
            
            <article>
                
<p>Linear algebra, at its core, is about solving a set of linear equations, referred to as <strong>a system of equations</strong>. A large number of problems can be formulated as a system of linear equations. </p>
<p>We have two equations and two unknowns, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7517d76f-e4f5-4316-90b5-447587adf16d.png" style="width:5.33em;height:2.67em;"/></p>
<p class="mce-root">Both equations produce straight lines. The solution to both these equations is the point where both lines meet. In this case, the answer is the point (3, 1). </p>
<p class="mce-root">But for our purposes, in linear algebra, we write the preceding equations as a vector equation that looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/76e54c6c-f5a2-4080-b865-6f014b81ca89.png" style="width:13.17em;height:2.75em;"/></p>
<p>Here, <strong>b</strong> is the result vector.</p>
<p class="mce-root">Placing the point (3, 1) into the vector equation, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/486783f2-84f5-4d1f-988e-6589190e3b32.png" style="width:10.17em;height:10.25em;"/></p>
<p>As we can see, the left-hand side is equal to the right-hand side, so it is, in fact, a solution! However, I personally prefer to write this as a coefficient matrix, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/372c5f68-002c-4d39-84ea-db6b6e783b5c.png" style="width:6.67em;height:2.67em;"/></p>
<p>Using the coefficient matrix, we can express the system of equations as a matrix problem in the form <img class="fm-editor-equation" src="Images/6b7983fe-ee48-44a0-b74c-59fb3cd58c61.png" style="width:3.83em;height:1.00em;"/>, where the column vector <em>v</em> is the variable vector. We write this as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2adc65f3-31fd-471e-9eef-d2074df5d327.png" style="width:9.83em;height:2.67em;"/>. </p>
<p>Going forward, we will express all our problems in this format. </p>
<p>To develop a better understanding, we'll break down the multiplication of matrix <em>A</em> and vector <em>v</em>. It is easiest to think of it as a<span> linear combination of vectors. Let's take a look at the following example with a 3x3 matrix and a 3x1 vector:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/791c49d6-f452-406a-904c-c786dd91dbd8.png" style="width:31.92em;height:4.00em;"/></p>
<p>It is important to note that matrix and vector multiplication is only possible when the number of columns in the matrix is equal to the number of rows (elements) in the vector.</p>
<p>For example, let's look at the following matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/40d1e7b3-1f2f-4475-a5b5-58972acaf62a.png" style="width:9.00em;height:5.00em;"/></p>
<p>This can be multiplied since the number of columns in the matrix is equal to the number of rows in the vector, but the following matrix <span>cannot be multiplied as the number of columns and number of rows are not equal:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e1fbdf70-7cf3-4e4c-8cac-fc3a61635393.png" style="width:7.42em;height:4.92em;"/></p>
<p>Let's visualize some of the operations on vectors to create an intuition of how they work. Have a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-543 image-border" src="Images/a1d03a07-cc24-4dbf-be7f-db99e763e835.png" style="width:40.58em;height:14.58em;"/></p>
<p>The preceding vectors we dealt with are all in <img class="fm-editor-equation" src="Images/76445ae4-3f8e-42c9-adea-ff3339448905.png" style="width:1.67em;height:1.50em;"/> (in 2-dimensional space), and all resulting combinations of these vectors will also be in <img class="fm-editor-equation" src="Images/faf91354-3538-4a30-924e-42dc074da709.png" style="width:1.67em;height:1.50em;"/>. The same applies for vectors in <img class="fm-editor-equation" src="Images/8eb58b6f-21bf-4860-873f-ed3e5a65f586.png" style="width:1.67em;height:1.50em;"/>, <img class="fm-editor-equation" src="Images/2b7e9af7-c89e-44e8-af1c-7c14ece9527f.png" style="width:1.67em;height:1.50em;"/>, and <img class="fm-editor-equation" src="Images/533cd992-1a9f-4589-9ffa-520098985a13.png" style="width:1.75em;height:1.25em;"/>.</p>
<p>There is another very important vector operation called the dot product, which is a type of multiplication. Let's take two arbitrary vectors in <img class="fm-editor-equation" src="Images/643c9f21-82b7-49a6-b8ef-eaf073e41d6c.png" style="width:1.42em;height:1.25em;"/>, <strong>v</strong> and <strong>w</strong>, and find its dot product, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c194786f-cefc-4ae2-8306-9afe5231bfa7.png" style="width:14.92em;height:3.42em;"/></p>
<p>The following is the product:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c113d54b-9c6e-4691-a17c-df055b4386fc.png" style="width:12.50em;height:1.25em;"/>.</p>
<p>Let's continue, using the same vectors we dealt with before, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/07226613-36d6-4cd9-96ea-609f4e4736a3.png" style="width:13.42em;height:3.17em;"/></p>
<p>And by taking their dot product, we get zero, which tells us that the two vectors are perpendicular (there is a 90° angle between them), as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9a7ed323-155b-45e6-a628-d7424b8cce1a.png" style="width:19.33em;height:3.33em;"/></p>
<p class="mce-root"/>
<p>The most common example of a perpendicular vector is seen with the vectors that represent the <em>x</em> axis, the <em>y</em> axis, and so on. In <img class="fm-editor-equation" src="Images/66ae3c43-3e63-4682-a9a7-fc82c039451c.png" style="width:1.67em;height:1.50em;"/>, we write the <em>x</em> axis vector as <sub><img class="fm-editor-equation" src="Images/63b86189-6d7f-4aa7-baab-a1ebfb6ac991.png" style="width:3.92em;height:2.83em;"/></sub>and the <em>y</em> axis vector as <sub><img class="fm-editor-equation" src="Images/5c747d51-ecf1-4d10-ac7e-5af1dfde6bc9.png" style="width:4.00em;height:2.83em;"/></sub>. If we take the dot product <em>i</em>•<em>j</em>, we find that it is equal to zero, and they are thus perpendicular. </p>
<p>By combining <em>i</em> and <em>j</em> into a 2x2 matrix, we get the following identity matrix, which is a very important matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0fe94128-1604-47f8-a2b8-a999f83cd53b.png" style="width:5.58em;height:2.75em;"/></p>
<p>The following are some of the scenarios we will face when solving linear equations of the type <img class="fm-editor-equation" src="Images/6b7983fe-ee48-44a0-b74c-59fb3cd58c61.png" style="width:3.83em;height:1.00em;"/>:</p>
<ul>
<li><span>Let's consider the matrix </span><span><sub><img style="color: #333333;width:3.75em;height:2.83em;" class="fm-editor-equation" src="Images/59d7a363-9573-4da7-9d5e-e40f4c8a433d.png"/></sub></span><span>and the equations</span> <span><sub><img style="color: #333333;width:7.25em;height:1.25em;" class="fm-editor-equation" src="Images/e6b6fc6a-c0e5-4e8e-89f9-c3f767e5e9bf.png"/></sub></span><span> and </span><span><sub><img style="color: #333333;width:7.25em;height:1.25em;" class="fm-editor-equation" src="Images/37a4cfcb-522a-4687-9f9d-4cef1a955fa0.png"/></sub></span><span>. If we do the algebra and multiply the first equation by 3, we get</span> <span><sub><img style="color: #333333;width:7.42em;height:1.17em;" class="fm-editor-equation" src="Images/ccc9c421-0c52-4f15-a3aa-e0d9fc771341.png"/></sub></span><span>. But the second equation is equal to zero, which means that these two equations do not intersect and therefore have no solution. When one column is dependent on another—that is, is a multiple of another column—all combinations of </span><span><sub><img style="color: #333333;width:1.50em;height:2.00em;" class="fm-editor-equation" src="Images/50950cd5-e2c2-409e-8183-deacbc2d2ed0.png"/></sub></span><span> and </span><span><sub><img style="color: #333333;width:1.50em;height:2.00em;" class="fm-editor-equation" src="Images/9fb0c18b-5ed3-49f9-a5a0-ef39e12d4767.png"/></sub></span><span> lie in the same direction. However, seeing as </span><span><sub><img style="color: #333333;width:1.58em;height:2.00em;" class="fm-editor-equation" src="Images/caa1a971-d158-4ab0-93b6-1b04e55e2fc0.png"/></sub></span><span> is not a combination of the two aforementioned column vectors and does not lie on the same line, it cannot be a solution to the equation.</span></li>
<li>Let's take the same matrix as before, but this time, <sub><img class="fm-editor-equation" src="Images/d49790e7-c5f8-47b4-9bf4-2e64bbcb2700.png" style="width:4.42em;height:2.58em;"/></sub>. Since <strong>b</strong> is on the line and is a combination of the dependent vectors, there is an infinite number of solutions. We say that <strong>b</strong> is in the column space of A. While there is only one specific combination of <strong>v</strong> that produces <strong>b</strong>, there are infinite combinations of the column vectors that result in the zero vector (<strong>0</strong>). For example, for any value, <em>a</em>, we have the following:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c4eeebdc-0420-47b0-96ce-1d4e2e90bfab.png" style="width:12.67em;height:3.08em;"/></p>
<p>This leads us to another very important concept, known as the complete solution. The complete solution is all the possible ways to produce <sub><img class="fm-editor-equation" src="Images/f6146508-ced5-40a4-a9d0-1bfb8c6e4ac5.png" style="width:4.08em;height:2.42em;"/></sub>. We write this as <sub><img class="fm-editor-equation" src="Images/dd655452-f231-4ca3-9157-3fd61d2eb9ee.png" style="width:8.33em;height:1.42em;"/></sub>, where <sub><img class="fm-editor-equation" src="Images/a4763160-2b3e-4dd3-a820-89698095c2ad.png" style="width:19.42em;height:3.17em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Solving linear equations in n-dimensions</h1>
                </header>
            
            <article>
                
<p>Now that we've dealt with linear equations in 2-dimensions and have developed an understanding of them, let's go a step further and look at equations in 3-dimensions.</p>
<p class="mce-root">Earlier, our equations produced curves in the 2-dimensional space (<em>xy</em>-plane). Now, the equations we will be dealing with will produce planes in 3-dimensional space (<em>xyz</em>-plane). </p>
<p>Let's take an arbitrary 3x3 matrix, as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2d9e7043-cc8b-4743-a537-7d71f68e6688.png" style="width:5.92em;height:4.42em;"/></p>
<p>We know from earlier in having dealt with linear equations in two dimensions that our solution <strong>b</strong>, as before, is a linear combination of the three column vectors, so that <sub><img class="fm-editor-equation" src="Images/b6269a28-f488-440d-856a-abec7d06726e.png" style="width:23.83em;height:1.33em;"/></sub>.</p>
<p>The equation <sub><img class="fm-editor-equation" src="Images/ccd382f2-fa54-459a-be3a-8043fd2ba13d.png" style="width:10.25em;height:1.25em;"/></sub> (equation 1) produces a plane, as do <sub><img class="fm-editor-equation" src="Images/2467172b-cec3-4323-b81a-fc6acc1a5059.png" style="width:9.92em;height:1.25em;"/></sub> (equation 2), and <sub><img class="fm-editor-equation" src="Images/5fa43033-d4aa-4585-9de8-35138fdbb6eb.png" style="width:10.42em;height:1.33em;"/></sub> (equation 3).</p>
<p>When two planes intersect, they intersect at a line; however, when three planes intersect, they intersect at a point. That point is the vector <sub><img class="fm-editor-equation" src="Images/1d78b343-c418-4a7e-9216-a937b806d8bf.png" style="width:4.42em;height:3.83em;"/></sub>, which is the solution to our problem.</p>
<p>However, if the three planes do not intersect at a point, there is no solution to the linear equation. This same concept of solving linear equations can be extended to many more dimensions. </p>
<p>Suppose now that we have a system with 15 linear equations and 15 unknown variables. We can use the preceding method and, according to it, we need to find the point that satisfies all the 15 equations—that is, where they intersect (if it exists).</p>
<p>It will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c72baa97-897f-4a90-94c0-850d1abd9c00.png" style="width:30.92em;height:1.50em;"/></p>
<p>As you can tell, that's a lot of equations we have to deal with, and the greater the number of dimensions, the harder this becomes to solve.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Solving linear equations using elimination</h1>
                </header>
            
            <article>
                
<p>One of the best ways to solve linear equations is by a systematic method known as <strong>elimination</strong>. This is a method that allows us to systematically eliminate variables and use substitution to solve equations. </p>
<p>Let's take a look at two equations with two variables, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4f496b41-d3fd-4919-ac59-2eb5d4300ada.png" style="width:5.50em;height:2.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">After elimination, this becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ba4ab482-dfc9-49f6-adae-aad2f001d085.png" style="width:5.50em;height:2.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">As we can see, the <em>x</em> variable is no longer in the second equation. We can plug the <em>y</em> value back into the first equation and solve for <em>x</em>. Doing this, we find that <em>x = 3</em> and <em>y = 1</em>.</p>
<p>We call this <strong>triangular factorization</strong>. There are two types—lower triangular and upper triangular. We solve the upper triangular system from top to bottom using a process known as <strong>back substitution</strong>, and this works for systems of any size.</p>
<div class="mce-root packt_infobox">While this is an effective method, it is not fail-proof. We could come across a scenario where we have more equations than variables, or more variables than equations, which are unsolvable. Or, we could have a scenario such as <em>0x = 7</em>, and, as we know, dividing by zero is impossible.</div>
<p>Let's solve three equations with three variables, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/de168eab-7179-44f1-a395-d1da75a426c9.png" style="width:15.75em;height:4.17em;"/></p>
<p>We will use upper triangular factorization and eliminate variables, starting with <em>y</em> and then <em>z</em>. Let's start by putting this into our matrix form, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8fc3e9e1-ee34-4538-9177-9ed91be78ac4.png" style="width:15.17em;height:4.25em;"/></p>
<p>For our purposes and to make things simpler, we will drop <strong><em>v</em></strong>, the column vector, and get the following result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/587fb1a2-2227-4b48-8a83-2f4feaa340aa.png" style="width:13.42em;height:4.50em;"/></p>
<p>Then, exchange row 2 and row 3 with each other, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/961c5862-e493-4a5e-bf05-1faa01c3542d.png" style="width:13.17em;height:4.42em;"/></p>
<p>Then, add row 2 and row 1 together to eliminate the first value in row 2, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/66ed810d-f02d-4d1b-a71c-0a3614fdd40f.png" style="width:12.58em;height:4.50em;"/></p>
<p>Next, multiply row 1 by 3/2 and subtract it from row 3, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4b9907b5-99c4-4fe0-b5d8-9cc881331eb1.png" style="width:13.33em;height:4.58em;"/></p>
<p>Finally, multiply row 2 by 6 and subtract it from row 3, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d1af6fb1-b98c-416a-81de-9ffe836300b6.png" style="width:13.67em;height:4.50em;"/></p>
<p>As you can notice, the values in the matrix now form a triangle pointing upward, which is why we call it upper triangular. By substituting the values back into the previous equation backward (from bottom to top), we can solve, and find that <sub><img class="fm-editor-equation" src="Images/a01e9e4a-2748-428d-aebd-1ea043400ed1.png" style="width:2.75em;height:1.00em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/5f884d7a-e7a1-499a-a4f1-54c2dcc6377c.png" style="width:3.58em;height:1.25em;"/></sub>, and <sub><img class="fm-editor-equation" src="Images/ed84a059-cdf7-4685-8b2e-a6c294e503a9.png" style="width:2.58em;height:1.00em;"/></sub>.</p>
<p>In summary, <img class="fm-editor-equation" src="Images/d9373151-de9b-4942-812a-70b29d99c5fb.png" style="width:4.17em;height:1.08em;"/> becomes <img class="fm-editor-equation" src="Images/9621aa55-0027-41be-84c4-8417adb89d0d.png" style="width:3.75em;height:1.00em;"/>, as illustrated here:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1242 image-border" src="Images/e1a093f9-bad6-47ba-a9bf-47741619dff3.png" style="width:32.75em;height:6.92em;"/></p>
<div class="mce-root packt_infobox"><strong>Note</strong>: The values across the diagonal in the triangular factorized matrix are called pivots, and when factorized, the values below the diagonal are all zeros.</div>
<p>To check that our found solution is right, we solve <img class="fm-editor-equation" src="Images/1194653c-0df1-4954-ac63-f40786e74eac.png" style="width:3.58em;height:0.92em;"/>, using our found values for <em>x</em>, <em>y,</em> and <em>z</em>, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8067f820-eff7-435d-980b-6daff80a91e5.png" style="width:16.25em;height:4.33em;"/></p>
<p>This then becomes the following equation:</p>
<p style="padding-left: 150px"><img class="aligncenter size-full wp-image-1244 image-border" src="Images/c7cf7b61-401c-4446-b92b-84bcfa47f15f.png" style="width:20.25em;height:9.17em;"/></p>
<p>And as we can see, the left-hand side is equal to the right-hand side.</p>
<p>After upper triangular factorization, an arbitrary 4x4 matrix will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aaf4ec59-37bf-41ad-b82e-5b0028860e2b.png" style="width:9.75em;height:5.67em;"/></p>
<p>We could take this a step further and factorize the upper triangular matrix until we end up with a matrix that contains only the pivot values along the diagonal, and zeros everywhere else. This resulting matrix <strong>P</strong> essentially fully solves the problem for us<span> without us having to resort to forward or backward substitution, and it looks like this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/574b75b9-9eb5-4b4c-bb07-ce0154c385df.png" style="width:9.67em;height:5.67em;"/></p>
<p>But as you can tell, there are a lot of steps involved in getting us from <strong>A</strong> to <strong>P</strong>.</p>
<p>There is one other very important factorization method called <strong>lower-upper</strong> (<strong>LU) decomposition</strong>. The way it works is we factorize <strong>A</strong> into an upper triangular matrix <strong>U</strong>, and record the steps of Gaussian elimination in a lower triangular matrix <strong>L</strong>, such that <sub><img class="fm-editor-equation" src="Images/b5ee1b4e-a497-4c37-873c-b147e73f74d1.png" style="width:3.92em;height:0.92em;"/></sub>.</p>
<p>Let's revisit the matrix we upper-triangular factorized before and put it into LU factorized form, like this:</p>
<p style="padding-left: 150px"><img class="aligncenter size-full wp-image-1331 image-border" src="Images/37dce8d5-7c0a-45fb-a51b-80f7269227ec.png" style="width:23.42em;height:4.50em;"/></p>
<p>If we multiply the two matrices on the right, we will get the original matrix <strong>A</strong>. But how did we get here? Let's go through the steps, as follows:</p>
<ol>
<li>We start with <img class="fm-editor-equation" src="Images/42357787-303e-434f-8a90-864135a19ec1.png" style="width:4.00em;height:1.00em;"/>, so that the following applies:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5f11564a-313c-4613-a616-89d647c9ad71.png" style="width:24.17em;height:4.42em;"/></p>
<ol start="2">
<li>We add -1 to what was the identity matrix at <em>l<sub>2,1</sub></em> to represent the operation (row 2)-(-1)(row 1), so it becomes the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a9dac746-1c92-4770-aa75-3c6ef67aff43.png" style="width:26.00em;height:4.75em;"/></p>
<ol start="3">
<li>We then add <sub><img class="fm-editor-equation" src="Images/5931f8e1-70cc-4886-9256-66f4880fb85d.png" style="width:0.92em;height:2.25em;"/></sub> to the matrix at <em>l<sub>3,1</sub></em> to represent the <sub><img class="fm-editor-equation" src="Images/cd8b62e1-0be4-491f-a862-2a64a5781bf4.png" style="width:9.17em;height:2.42em;"/> </sub>operation, so it becomes the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d999f17f-ba8e-4058-af67-e4ff6a6f7555.png" style="width:24.75em;height:4.75em;"/></p>
<ol start="4">
<li>We then add 6 to the matrix at <em>l<sub>3,2</sub></em> to represent the operation (row 3)-6(row 2), so it becomes the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/231a078a-f411-4fc1-8631-016ecd7609a1.png" style="width:25.17em;height:4.83em;"/></p>
<p>This is the LU factorized matrix we saw earlier.</p>
<p>You might now be wondering what this has to do with solving <img class="fm-editor-equation" src="Images/60491e0d-7150-433b-ba6b-bf0c5bf0caeb.png" style="width:3.83em;height:1.00em;"/>, which is very valid. The elimination process tends to work quite well, but we have to additionally apply all the operations we did on <strong>A</strong> to <em><strong>b</strong> </em>as well, and this involves extra steps. However, LU factorization is only applied to <strong>A</strong>. </p>
<p>Let's now take a look at how we can solve our system of linear equations using this method.</p>
<p>For simplicity, we drop the variables vector and write <strong>A</strong><span> and <strong>b</strong></span> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/961c5862-e493-4a5e-bf05-1faa01c3542d.png" style="width:12.17em;height:4.08em;"/></p>
<p>But even this can get cumbersome to write as we go, so we will instead write it in the following way for further simplicity:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c766d0c3-0dec-4001-926a-43f3392d6f22.png" style="width:9.83em;height:1.33em;"/></p>
<p>We then multiply both sides by <img class="fm-editor-equation" src="Images/799cb620-2b53-4a46-a42a-f0d351b4a4bc.png" style="width:2.25em;height:1.42em;"/>and get the following result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ff2aaab2-8480-4537-afa7-5242a9ef6029.png" style="width:11.58em;height:1.50em;"/></p>
<p>This tells us that <img class="fm-editor-equation" src="Images/6635e792-8f7a-4022-a2de-cd3f398a60c9.png" style="width:3.83em;height:1.08em;"/>, and we already know from the preceding equation that <sub><img class="fm-editor-equation" src="Images/43d8a95a-f8c4-49dd-92af-c96973a49357.png" style="width:4.08em;height:1.08em;"/></sub> (so <sub><img class="fm-editor-equation" src="Images/50f2f8f9-db05-4674-9982-fcf5570d9f71.png" style="width:5.08em;height:1.25em;"/></sub>). And by using back substitution, we can find the vector <strong>v</strong>.</p>
<p>In the preceding example, you may have noticed some new notation that I have not yet introduced, but not to worry—we will observe all the necessary notation and operations in the next section. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix operations</h1>
                </header>
            
            <article>
                
<p>Now that we understand how to solve systems of linear equations of the type <img class="fm-editor-equation" src="Images/c3085ddc-2bf6-43b2-81df-37ede45e7ed2.png" style="width:3.25em;height:0.83em;"/> where we multiplied a matrix with a column vector, let's move on to dealing with the types of operations we can do with one or more matrices.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding matrices</h1>
                </header>
            
            <article>
                
<p>As with scalars and vectors, sometimes we may have to add two or more matrices together, and the process of doing so is rather straightforward. Let's take two <img class="fm-editor-equation" src="Images/b70e9807-7600-4d82-bb97-4fe0408bf0ff.png" style="width:3.42em;height:1.33em;"/> matrices, <em>A</em> and <em>B</em>, and add them:</p>
<p><img class="aligncenter size-full wp-image-1250 image-border" src="Images/d1a55f22-b8bb-421d-9e27-7ed05f8e1a72.png" style="width:60.42em;height:8.42em;"/></p>
<p>It is important to note that we can only add matrices that have the same dimensions, and, as you have probably noticed, we add the matrices element-wise. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multiplying matrices</h1>
                </header>
            
            <article>
                
<p>So far, we have only multiplied a matrix by a column vector. But now, we will multiply a matrix <em>A</em> with another matrix <em>B</em>.</p>
<p>There are four simple rules that will help us in multiplying matrices, listed here:</p>
<ul>
<li>Firstly, we can only multiply two matrices when the number of columns in matrix <em>A</em> is equal to the number of rows in matrix <em>B</em>.</li>
<li>Secondly, the first row of matrix <em>A</em> multiplied by the first column of matrix <em>B</em> gives us the first element in the matrix <em>AB</em>, and so on. </li>
</ul>
<ul>
<li>Thirdly, when multiplying, order matters—specifically, <em>AB</em> ≠ <em>BA</em>.</li>
<li>Lastly, the element at row <em>i</em>, column <em>j</em> is the product of the <em>i<sup>th</sup></em> row of matrix <em>A</em> and the <em>j<sup>th</sup></em> column of matrix <em>B</em>.</li>
</ul>
<p class="CDPAlignLeft CDPAlign">Let's multiply an arbitrary 4x5 matrix with an arbitrary 5x6 matrix, as follows: </p>
<p style="padding-left: 90px"><img class="aligncenter size-full wp-image-1248 image-border" src="Images/cf39a6fc-ea74-4d5c-9e27-b94ea3aff4c7.png" style="width:32.08em;height:8.08em;"/></p>
<p>This results in a 4x6 matrix, like this:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1245 image-border" src="Images/7f013907-7b56-409b-86c9-6c5ff9c29919.png" style="width:25.25em;height:5.50em;"/></p>
<p class="CDPAlignLeft CDPAlign">From that, we can deduce that in general, the following applies:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1329 image-border" src="Images/3e89a845-02b9-437f-8f25-aa241cfdacf5.png" style="width:33.08em;height:1.33em;"/></p>
<p>Let's take the following two matrices and multiply them, like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d32786c0-cc3f-454e-86be-d69a00a1f9af.png" style="width:9.08em;height:4.42em;"/> and <img class="fm-editor-equation" src="Images/23235442-c6f3-44fe-b693-076fc232e5dd.png" style="width:8.00em;height:4.33em;"/></p>
<p class="CDPAlignLeft CDPAlign">This will give us the following matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bcc54925-a1dc-4772-b339-7d8ce304aac2.png" style="width:23.67em;height:4.33em;"/>.</p>
<div class="mce-root packt_infobox"><strong>Note</strong>: In this example, the matrix <em>B</em> is the identity matrix, usually written as <em>I</em>. </div>
<p>The identity matrix has two unique properties in matrix multiplication. When multiplied by any matrix, it returns the original matrix unchanged, and the order of multiplication does not matter—so, <em>AI = IA = A</em>.</p>
<p>For example, let's use the same matrix <em>A</em> from earlier, and multiply it by another matrix <em>B</em>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/73ae2b9e-33a7-4bb1-93bc-f2fc03ff02be.png" style="width:31.42em;height:4.42em;"/></p>
<p>Another very special matrix is the inverse matrix, which is written as <em>A<sup>-1</sup></em>. And when we multiply <em>A</em> with <em>A<sup>-1</sup></em>, we receive <em>I</em>, the identity matrix.</p>
<p>As mentioned before, the order in which we multiply matters. We must keep the matrices in order, but we do have some flexibility. As we can see in the following equation, the parentheses can be moved:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2ee7c381-74a9-4daa-a773-3c492e1cf2e5.png" style="width:13.17em;height:1.33em;"/></p>
<p>This is the first law of matrix operations, known as <strong>associativity</strong>.</p>
<p>The following are three important laws that cannot be broken:</p>
<ul>
<li><strong>commutativity</strong>: <sub><img class="fm-editor-equation" src="Images/b548464d-80d3-4bba-8ec9-997a3707ffd9.png" style="width:8.17em;height:1.08em;"/></sub></li>
<li><strong>distributivity</strong>: <sub><img class="fm-editor-equation" src="Images/2d775123-c5e6-431e-b436-894b598fad3d.png" style="width:10.25em;height:1.33em;"/></sub> or <sub><img class="fm-editor-equation" src="Images/44a52be2-f47b-4e4d-bad5-70277c329a46.png" style="width:10.92em;height:1.25em;"/></sub></li>
<li><strong>associativity</strong>: <sub><img class="fm-editor-equation" src="Images/2d6c6496-8c34-42be-880b-91170d0091f0.png" style="width:14.25em;height:1.33em;"/></sub> </li>
</ul>
<p>As proof that <em>AB ≠ BA</em>, let's take a look at the following example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f79ed756-09fc-4be7-9a14-997774bd40e7.png" style="width:16.92em;height:5.92em;"/></p>
<p class="CDPAlignLeft CDPAlign">This conclusively shows that the two results are not the same.</p>
<p>We know that we can raise numbers to powers, but we can also raise matrices to powers.</p>
<p>If we raise the matrix <em>A</em> to power <em>p</em>, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/71d83d9f-212c-4d9d-a6e1-8d0e8c98ccc9.png" style="width:8.67em;height:1.08em;"/> (multiplying the matrix by itself <em>p</em> times)</p>
<p>There are two additional power laws for matrices—<sub><img class="fm-editor-equation" src="Images/3a088ac9-9eb6-48b3-b774-eb53eaeff2f7.png" style="width:8.33em;height:1.42em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/ef86610b-94de-449e-85ec-14afe594593f.png" style="width:6.17em;height:1.33em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inverse matrices</h1>
                </header>
            
            <article>
                
<p>Let's revisit the concept of inverse matrices and go a little more in depth with them. We know from earlier that <em>AA<sup>-1 </sup></em>= <em>I</em>, but not every matrix has an inverse. </p>
<p>There are, again, some rules we must follow when it comes to finding the inverses of matrices, as follows:</p>
<ul>
<li>The inverse only exists if, through the process of upper or lower triangular factorization, we obtain all the pivot values on the diagonal.</li>
<li>If the matrix is invertible, it has only one unique inverse matrix—that is, if <em>AB</em> = <em>I</em> and <em>AC</em> = <em>I</em>, then <em>B</em> = <em>C</em>.</li>
<li>If <em>A</em> is invertible, then to solve <em>Av</em> = <em>b</em> we multiply both sides by <em>A<sup>-1</sup></em> and get <em>AA<sup>-1</sup>v</em> = <em>A<sup>-1</sup>b</em>, which finally gives us = <em>A<sup>-1</sup>b</em>.</li>
<li>If <em>v</em> is nonzero and <em>b</em> = 0, then the matrix does not have an inverse.</li>
<li>2 x 2 matrices are invertible only if <em>ad</em> - <em>bc</em> ≠ 0, where the following applies:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/013c60f9-7633-490c-aece-0fada2543d5e.png" style="width:15.42em;height:3.08em;"/></p>
<p style="padding-left: 90px">And <em>ad</em> - <em>bc</em> is called the <strong>determinant</strong> of <em>A</em>. <em>A<sup>-1</sup></em> involves dividing each element in the matrix by the determinant.</p>
<ul>
<li>Lastly, if the matrix has any zero values along the diagonal, it is non-invertible.</li>
</ul>
<p>Sometimes, we may have to invert the product of two matrices, but that is only possible when both the matrices are individually invertible (follow the rules outlined previously). </p>
<p>For example, let's take two matrices A and B, which are both invertible. Then, <sub><img class="fm-editor-equation" src="Images/0c509cb9-9272-488f-a67b-f6374d0bdc38.png" style="width:9.25em;height:1.50em;"/></sub> so that <sub><img class="fm-editor-equation" src="Images/bb4a926f-aed2-4f85-983d-bf9a3547518e.png" style="width:19.00em;height:1.50em;"/></sub>.</p>
<div class="mce-root packt_infobox"><strong>Note</strong>: Pay close attention to the order of the inverse—it too must follow the order. The left-hand side is the mirror image of the right-hand side.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix transpose</h1>
                </header>
            
            <article>
                
<p>Let's take an <img class="fm-editor-equation" src="Images/ab7680f6-95f0-4d68-ba9c-3d2e38318c94.png" style="width:3.42em;height:1.33em;"/> matrix <em>A</em>. If the matrix's transpose is <em>B</em>, then the dimensions of <em>B</em> are <img class="fm-editor-equation" src="Images/94bb719d-cb12-4f62-9cce-0bc2b057effe.png" style="width:3.42em;height:1.33em;"/>, such that: <sub><img class="fm-editor-equation" src="Images/ee4e4aed-0235-496e-82b8-1afe39ab4c5e.png" style="width:5.58em;height:1.58em;"/></sub>.  Here is the matrix <em>A</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/867914db-b723-48b4-b247-5e036bf6bcb7.png" style="width:14.67em;height:6.83em;"/></p>
<p>Then, the matrix <em>B</em> is as given:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/073aeb58-5497-4697-9ae1-2841d7b48e03.png" style="width:15.17em;height:7.08em;"/>.</p>
<p>Essentially, we can think of this as writing the columns of <em>A</em> as the rows of the transposed matrix, <em>B</em>.</p>
<div class="mce-root packt_tip">We usually write the transpose of <em>A</em> as <em>A<sup>T</sup></em>.</div>
<p>A symmetric matrix is a special kind of matrix. It is an <em>n×n</em> matrix that, when transposed, is exactly the same as before we transposed it.</p>
<p>The following are the properties of inverses and transposes:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/8305ca1b-6dc0-4caa-8d2d-ba4b96592abc.png" style="width:9.50em;height:1.25em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/2f9790a9-9bc9-404e-aa53-f4ff7be116e2.png" style="width:9.25em;height:1.50em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/53efb269-ac19-435c-8350-28696549f039.png" style="width:11.25em;height:1.42em;"/></sub></li>
</ul>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/47a5e35c-f82f-47e2-92d1-8a9f6d7b0299.png" style="width:5.25em;height:1.42em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/a1710de3-2ebc-4e00-aae4-9822a0affca5.png" style="width:10.00em;height:1.42em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/0846e2c5-db1a-43da-a3e8-24c6ef96fe48.png" style="width:8.00em;height:1.50em;"/></sub></li>
</ul>
<div class="mce-root packt_infobox"><q>If A is an invertible matrix, then so is A<sup>T</sup>, and so (A<sup>-1</sup>)<sup>T</sup> = (A<sup>T</sup>)<sup>-1</sup> = A<sup>-T</sup>.</q></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Permutations</h1>
                </header>
            
            <article>
                
<p>In the example on solving systems of linear equations, we swapped the positions of rows 2 and 3. This is known as a <strong>permutation</strong>. </p>
<p>When we are doing triangular factorization, we want our pivot values to be along the diagonal of the matrix, but this won't happen every time—in fact, it usually won't. So, instead, what we do is swap the rows so that we get our pivot values where we want them. </p>
<p>But that is not their only use case. We can also use them to scale individual rows by a scalar value or add rows to or subtract rows from other rows.</p>
<p>Let's start with some of the more basic permutation matrices that we obtain by swapping the rows of the identity matrix. In general, we have <em>n!</em> possible permutation matrices that can be formed from an <em>n</em>x<em>n</em> identity matrix. In this example, we will use a 3×3 matrix and therefore have six permutation matrices, and they are as follows:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/8edbd7d1-57ee-44ce-bdac-feaf61fe18b1.png" style="width:8.00em;height:3.75em;"/></sub> This matrix makes no change to the matrix it is applied on.</li>
<li><sub><img class="fm-editor-equation" src="Images/de0bdfe3-c2d9-4c81-be0b-738ab96987e1.png" style="width:8.17em;height:3.83em;"/></sub> This matrix swaps rows two and three of the matrix it is applied on.</li>
<li><sub><img class="fm-editor-equation" src="Images/a9635367-129f-45b6-9fc8-21e88a13af91.png" style="width:8.17em;height:3.83em;"/> </sub>This matrix swaps rows one and two of the matrix it is applied on.</li>
</ul>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/20133a22-4f28-4529-8da3-d6a0ae7eee8a.png" style="width:8.17em;height:3.83em;"/> </sub>This matrix shifts rows two and three up one and moves row one to the position of row three of the matrix it is applied on.</li>
<li><sub><img class="fm-editor-equation" src="Images/66df650f-aff2-4922-8f6a-df9e89e07d6e.png" style="width:8.17em;height:3.83em;"/></sub> This matrix shifts rows one and two down one and moves row three to the row-one position of the matrix it is applied on.</li>
<li><sub><img class="fm-editor-equation" src="Images/9b553584-caaf-4548-9864-6bcbde8f35dc.png" style="width:8.17em;height:3.83em;"/></sub> This matrix swaps rows one and three of the matrix it is applied on.</li>
</ul>
<p>It is important to note that there is a particularly fascinating property of permutation matrices that states that if we have a matrix <img class="fm-editor-equation" src="Images/26f5430f-c8bb-41e6-9a89-543e2bf9950a.png" style="width:6.00em;height:1.33em;"/> and it is invertible, then there exists a permutation matrix that when applied to <em>A</em> will give us the LU factor of <em>A</em>. We can express this like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/92bc5c01-f9d3-489f-9ed6-8783132ac7ab.png" style="width:5.17em;height:1.00em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vector spaces and subspaces</h1>
                </header>
            
            <article>
                
<p>In this section, we will explore the concepts of vector spaces and subspaces. These are very important to our understanding of linear algebra. In fact, if we do not have an understanding of vector spaces and subspaces, we do not truly have an understanding of how to solve linear algebra problems.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spaces</h1>
                </header>
            
            <article>
                
<p>Vector spaces are one of the fundamental settings for linear algebra, and, as the name suggests, they are spaces where all vectors reside. We will denote the vector space with V.</p>
<p>The easiest way to think of dimensions is to count the number of elements in the column vector. Suppose we have <sub><img class="fm-editor-equation" src="Images/f844062d-c870-4c63-8da4-9c367d12665a.png" style="width:9.50em;height:1.33em;"/></sub>, then <img class="fm-editor-equation" src="Images/b83dee00-ee90-44cc-a62b-be53a3b1779b.png" style="width:3.58em;height:1.33em;"/>. <img class="fm-editor-equation" src="Images/c0be8f8b-2060-45d3-a5d3-84aac64ebf02.png" style="width:1.42em;height:1.25em;"/> is a straight line, <img class="fm-editor-equation" src="Images/b306fc56-e969-41c7-8bfa-f37854133f01.png" style="width:1.33em;height:1.17em;"/> is all the possible points in the <em>xy</em>-plane, and <img class="fm-editor-equation" src="Images/37a7dfff-6c23-4f1a-86af-e7ccc17cce8b.png" style="width:1.67em;height:1.50em;"/> is all the possible points in the <em>xyz</em>-plane—that is, 3-dimensional space, and so on.</p>
<p>The following are some of the rules for vector spaces:</p>
<ul>
<li>There exists in <em>V</em> an additive identity element such that <img class="fm-editor-equation" src="Images/9beb757e-6a2d-462b-8634-ee8c79d8ab11.png" style="width:5.25em;height:1.08em;"/> for all <img class="fm-editor-equation" src="Images/8c64162a-985e-488b-9afd-e2065e102868.png" style="width:2.92em;height:1.00em;"/>.</li>
<li>For all <img class="fm-editor-equation" src="Images/72e37558-dab0-4707-95d1-e3531553ee79.png" style="width:2.67em;height:0.92em;"/>, there exists an additive inverse such that <sub><img class="fm-editor-equation" src="Images/7a4abfc2-17f6-48ca-9db4-f9754cc3ee0c.png" style="width:6.17em;height:1.25em;"/></sub>.</li>
<li>For all <img class="fm-editor-equation" src="Images/73b70efa-1d94-44f8-9bed-8f936ca38de0.png" style="width:2.67em;height:0.92em;"/>, there exists a multiplicative identity such that <img class="fm-editor-equation" src="Images/e6918d7b-3df3-4d99-952e-3f8655fc3501.png" style="width:3.42em;height:1.00em;"/>.</li>
<li>Vectors are commutative, such that for all <sub><img class="fm-editor-equation" src="Images/8a616fa2-317e-45d6-b02f-d0aba9a4280d.png" style="width:3.92em;height:1.17em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/12689855-e010-4a34-acde-56945f94ab77.png" style="width:6.50em;height:1.00em;"/></sub>.</li>
<li>Vectors are associative, such that <sub><img class="fm-editor-equation" src="Images/58b58922-0bd4-4d4e-bc68-c0c13907c741.png" style="width:12.50em;height:1.33em;"/></sub>.</li>
<li>Vectors have distributivity, such that <sub><img class="fm-editor-equation" src="Images/447375df-bf81-4687-be00-faa44d6fae45.png" style="width:7.42em;height:1.00em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/4ef06655-1d74-49ea-b3b3-992d1569536b.png" style="width:7.42em;height:1.00em;"/></sub> for all <sub><img class="fm-editor-equation" src="Images/6a84499a-6142-43bc-9dd0-e436f6af128f.png" style="width:3.92em;height:1.17em;"/></sub> and for all <sub><img class="fm-editor-equation" src="Images/9e915871-db33-4314-b024-9f8562c0a820.png" style="width:3.92em;height:1.17em;"/></sub>.</li>
</ul>
<p class="mce-root">A set of vectors is said to be linearly independent if <sub><img class="fm-editor-equation" src="Images/81baafd7-b0d0-40f0-b313-132bb8b4012a.png" style="width:10.67em;height:1.08em;"/></sub>, which implies that <sub><img class="fm-editor-equation" src="Images/f78c7ab6-2fa7-47ab-ac6d-d9e95aea9cd3.png" style="width:11.83em;height:1.17em;"/></sub>.</p>
<p>Another important concept for us to know is called <strong>span</strong>. The span of <sub><img class="fm-editor-equation" src="Images/7dd1d26b-d04e-4e28-b6c8-c7a64cfeb149.png" style="width:8.92em;height:1.42em;"/></sub> is the set of all linear combinations that can be made using the <em>n</em> vectors. Therefore, <sub><img class="fm-editor-equation" src="Images/c4e15e74-1733-4695-adba-78e26a18609c.png" style="width:34.42em;height:1.42em;"/></sub> if the vectors are linearly independent and span <em>V</em> completely; then, the vectors <sub><img class="fm-editor-equation" src="Images/dbc93f18-3f2b-4d8c-a3e9-5688c00f0aeb.png" style="width:5.75em;height:1.08em;"/></sub> are the basis of <em>V</em>.</p>
<p>Therefore, the dimension of <em>V</em> is the number of basis vectors we have, and we denote it <em>dimV</em>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Subspaces</h1>
                </header>
            
            <article>
                
<p>Subspaces are another very important concept that state that we can have one or many vector spaces inside another vector space. Let's suppose <em>V</em> is a vector space, and we have a subspace <sub><img class="fm-editor-equation" src="Images/a602de35-3055-4e2e-82aa-563c44767ff1.png" style="width:3.17em;height:1.17em;"/></sub>. Then, <em>S</em> can only be a subspace if it follows the three rules, stated as follows:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/f230abc5-6336-415b-b5bf-6c54332c66fa.png" style="width:2.75em;height:1.00em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/61436647-3352-487a-9d71-4def0832edfe.png" style="width:4.17em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/b3fb2604-8b33-4e9e-b336-36115f0f1a0f.png" style="width:4.75em;height:1.25em;"/></sub>, which implies that <em>S</em> is closed under addition</li>
<li><sub><img class="fm-editor-equation" src="Images/e314788f-8f13-4302-92a0-d9e818fdbb12.png" style="width:3.00em;height:1.08em;"/></sub> and <img class="fm-editor-equation" src="Images/d1a7dedd-4412-4dee-a314-cca86bc4332d.png" style="width:2.92em;height:1.00em;"/> so that <sub><img class="fm-editor-equation" src="Images/ab71728c-9292-4dfc-8014-756cbebb2ea2.png" style="width:3.50em;height:1.00em;"/></sub>, which implies that <em>S</em> is closed under scalar multiplication</li>
</ul>
<p>If <sub><img class="fm-editor-equation" src="Images/8d92b7e1-bbfb-4388-a191-0e954364794b.png" style="width:4.83em;height:1.17em;"/></sub>, then their sum is <sub><img class="fm-editor-equation" src="Images/cda6b403-b85a-4810-8de1-a32ec98b2afb.png" style="width:16.83em;height:1.33em;"/></sub>, where the result is also a subspace of <em>V</em>.</p>
<p>The dimension of the sum <sub><img class="fm-editor-equation" src="Images/b464bbc9-fca3-4bce-b0ed-041fc6ba0dab.png" style="width:3.42em;height:1.00em;"/></sub> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f60e2d10-8138-4538-bc8f-68ed61411b04.png" style="width:23.42em;height:1.42em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear maps</h1>
                </header>
            
            <article>
                
<p>A linear map is a function <sub><img class="fm-editor-equation" src="Images/ac0809fa-111a-40d8-b331-d7157fdd43b3.png" style="width:6.42em;height:1.17em;"/></sub>, where <em>V</em> and <em>W</em> are both vector spaces. They must satisfy the following criteria:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/e1a8d556-6c13-4aee-896d-0b3ed210e527.png" style="width:11.42em;height:1.50em;"/></sub>, for all <sub><img class="fm-editor-equation" src="Images/36462293-378b-48df-928f-ad0affdb936e.png" style="width:4.25em;height:1.25em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/fd7d3f66-580f-4603-84dc-b46f1ffec174.png" style="width:7.58em;height:1.50em;"/></sub>, for all <img class="fm-editor-equation" src="Images/9f555bb5-8a65-46e2-99b2-e42246724dde.png" style="width:3.17em;height:1.08em;"/> and <img class="fm-editor-equation" src="Images/72e1df16-deb3-4998-afe5-0d497cd2d20f.png" style="width:2.92em;height:1.00em;"/></li>
</ul>
<p>Linear maps tend to preserve the properties of vector spaces under addition and scalar multiplication. A linear map is called a <strong>homomorphism of vector spaces;</strong> however, if the homomorphism is invertible (where the inverse is a homomorphism), then we call the mapping an <strong>isomorphism</strong>. </p>
<p>When <em>V</em> and <em>W</em> are isomorphic, we denote this as <sub><img class="fm-editor-equation" src="Images/26027ade-7a86-45e5-bb09-ac388bb121c9.png" style="width:3.83em;height:1.08em;"/></sub>, and they both have the same algebraic structure.</p>
<p>If <em>V</em> and <em>W</em> are vector spaces in <img class="fm-editor-equation" src="Images/910e77b1-f616-4036-bada-4c99ea52bede.png" style="width:1.75em;height:1.25em;"/>, and <sub><img class="fm-editor-equation" src="Images/09622b11-f715-4c5e-a913-ef23d82cd630.png" style="width:10.17em;height:1.08em;"/></sub>, then it is called a <strong>natural isomorphism</strong>. We write this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/22fa3783-7c2e-49c3-86ca-981bb4223707.png" style="width:17.92em;height:3.25em;"/></p>
<p class="CDPAlignLeft CDPAlign">Here, <sub><img class="fm-editor-equation" src="Images/601b901c-1bb0-46fa-8917-68127c269361.png" style="width:5.75em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/4d96e1bb-bd16-47aa-83ef-6f74d21eb605.png" style="width:6.33em;height:1.08em;"/></sub> are the bases of <em>V</em><span> and <em>W</em></span>. Using the preceding equation, we can see that <sub><img class="fm-editor-equation" src="Images/34f9d116-e22a-4fb1-b265-f392c2ce11a7.png" style="width:3.83em;height:1.08em;"/></sub>, which tells us that <img class="fm-editor-equation" src="Images/67ebcbe5-26df-4898-90db-b048a6ff8448.png" style="width:0.92em;height:1.17em;"/> is an isomorphism. </p>
<p>Let's take the same vector spaces <em>V</em><span> and <em>W</em></span> as before, with bases <sub><img class="fm-editor-equation" src="Images/601b901c-1bb0-46fa-8917-68127c269361.png" style="width:5.75em;height:1.08em;"/></sub><span> and <sub><img class="fm-editor-equation" src="Images/12bd3b39-0d26-4da0-9d90-9cc966fe50d2.png" style="width:6.58em;height:1.08em;"/></sub></span> respectively. We know that <sub><img class="fm-editor-equation" src="Images/ac0809fa-111a-40d8-b331-d7157fdd43b3.png" style="width:5.92em;height:1.08em;"/></sub> is a linear map, and the matrix <em>T</em> that has entries <em>A<sub>ij</sub></em>, where <sub><img class="fm-editor-equation" src="Images/e0df65fc-46d3-428d-9a8a-102f64731a6d.png" style="width:7.00em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/6774c8d6-2f4f-4648-a039-903a08738ab9.png" style="width:6.33em;height:1.33em;"/></sub> can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7cda3d08-be4c-413c-b1ff-60d1f1b9780f.png" style="width:17.33em;height:1.50em;"/>.</p>
<p>From our knowledge of matrices, we should know that the <em>j<sup>th</sup></em> column of A contains <em>Tv<sub>j</sub></em> in the basis of <em>W</em>.</p>
<p>Thus, <sub><img class="fm-editor-equation" src="Images/4d5d4c14-13fc-4851-b71f-e678d1f5074e.png" style="width:5.58em;height:1.17em;"/></sub> produces a linear map <sub><img class="fm-editor-equation" src="Images/c358e717-1517-46e6-ad53-d3feed59270f.png" style="width:7.00em;height:1.08em;"/></sub>, which we write as <sub><img class="fm-editor-equation" src="Images/6865cab1-d211-4b93-8a1a-615255d165b2.png" style="width:5.00em;height:1.08em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image and kernel</h1>
                </header>
            
            <article>
                
<p>When dealing with linear mappings, we will often encounter two important terms: the image and the kernel, both of which are vector subspaces with rather important properties.</p>
<p>The <strong>kernel</strong> (sometimes called the <strong>null space</strong>) is 0 (the zero vector) and is produced by a linear map, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/09bdec99-0281-4c85-ac22-23a61bf2b888.png" style="width:14.83em;height:1.50em;"/></p>
<p>And the <strong>image</strong> (sometimes called the <strong>range</strong>) of T is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/79f69b4a-54ec-4660-a8fc-7941a335376d.png" style="width:14.58em;height:1.50em;"/> such that <img class="fm-editor-equation" src="Images/84c1e82c-9ade-4597-8e3f-01c258a99038.png" style="width:4.50em;height:1.33em;"/>.</p>
<p><span><em>V</em> and <em>W</em></span> are also sometimes known as the <strong>domain</strong> and <strong>codomain</strong> of <em>T</em>. </p>
<p>It is best to think of the kernel as a linear mapping that maps the vectors <sub><img class="fm-editor-equation" src="Images/7fb41e44-9879-45ab-9014-041563a056a1.png" style="width:3.17em;height:1.08em;"/></sub> to <sub><img class="fm-editor-equation" src="Images/03d14613-91ca-452a-be46-a829033a023f.png" style="width:3.50em;height:1.08em;"/></sub>. The image, however, is the set of all possible linear combinations of <img class="fm-editor-equation" src="Images/f57a5c66-6e0e-440c-98e6-8de3fad7c04e.png" style="width:3.17em;height:1.08em;"/> that can be mapped to the set of vectors <sub><img class="fm-editor-equation" src="Images/07b20ab5-840d-4c18-b6ae-6d3ce53df6ad.png" style="width:3.75em;height:1.08em;"/></sub>. </p>
<p>The <strong>Rank-Nullity theorem</strong> (sometimes referred to as the <strong>fundamental theorem of linear mappings</strong>) states that given two vector spaces <em>V</em><span> and <em>W</em></span> and a linear mapping <sub><img class="fm-editor-equation" src="Images/c1fc64ab-aeaa-4265-bf37-e1f82c113979.png" style="width:6.83em;height:1.25em;"/></sub>, the following will remain true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6fc2d842-a5e5-4b4b-86f4-d591b8978768.png" style="width:20.33em;height:1.50em;"/>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Metric space and normed space</h1>
                </header>
            
            <article>
                
<p>Metrics help define the concept of distance in Euclidean space (denoted by <img class="fm-editor-equation" src="Images/b7bcefa4-b4a6-4848-8a71-a5d63b1ecf70.png" style="width:1.67em;height:1.25em;"/>). Metric spaces, however, needn't always be vector spaces. We use them because they allow us to define limits for objects besides real numbers.</p>
<p>So far, we have been dealing with vectors, but what we don't yet know is how to calculate the length of a vector or the distance between two or more vectors, as well as the angle between two vectors, and thus the concept of orthogonality (perpendicularity). This is where Euclidean spaces come in handy. In fact, they are the fundamental space of geometry. This may seem rather trivial at the moment, but their importance will become more apparent to you as we get further on in the book.</p>
<div class="packt_infobox"><q>In Euclidean space, we tend to refer to vectors as points. </q></div>
<p>A metric on a set <em>S</em> is defined as a function <img class="fm-editor-equation" src="Images/6f786cc5-9264-4891-a248-778807493a47.png" style="width:6.83em;height:1.00em;"/> and satisfies the following criteria:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/8b8bb6a5-d500-45b4-ab76-6c08e4480d65.png" style="width:5.17em;height:1.33em;"/></sub>, and when <sub><img class="fm-editor-equation" src="Images/112dca43-e1ca-4662-9ae2-e384dfef930d.png" style="width:3.17em;height:1.17em;"/></sub> then <sub><img class="fm-editor-equation" src="Images/45075729-42e3-4fc0-b53a-63ec239465fd.png" style="width:5.17em;height:1.33em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/331b5c2e-39e4-4584-b838-0cdff3ab4323.png" style="width:7.75em;height:1.33em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/52f95b13-d6af-4ed9-a372-42e71b11508a.png" style="width:12.17em;height:1.33em;"/></sub> (known as the <strong>triangle inequality</strong>)</li>
</ul>
<p>For all <sub><img class="fm-editor-equation" src="Images/fb6600a3-ac68-4c0c-8740-f92106e6bff5.png" style="width:5.08em;height:1.33em;"/></sub>.</p>
<p>That's all well and good, but how exactly do we calculate distance? </p>
<p>Let's suppose we have two points, <sub><img class="fm-editor-equation" src="Images/e6b07452-f8d1-424c-a80d-7365c8467686.png" style="width:4.08em;height:1.50em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/dc41fe2e-6d5c-422a-8fe3-0e90c22efb13.png" style="width:3.83em;height:1.42em;"/></sub>; then, the distance between them can be calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/65b4a10a-7be3-40ad-a9a0-e5b28ef19972.png" style="width:16.17em;height:2.17em;"/></p>
<p>And we can extend this to find the distance of points in <img class="fm-editor-equation" src="Images/93d0f60e-ce6a-4ff9-b140-11ce34ca9ad2.png" style="width:1.75em;height:1.25em;"/>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/298ce5eb-d7eb-480f-97ac-c2519734d4a5.png" style="width:12.58em;height:3.75em;"/></p>
<p>While metrics help with the notion of distance, norms define the concept of length in Euclidean space.</p>
<p>A norm on a vector space is a function <sub><img class="fm-editor-equation" src="Images/c834d5a8-9576-47b0-a9ae-c1822af7b8e8.png" style="width:6.50em;height:1.42em;"/></sub>, and satisfies the following conditions:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/72bc7a6f-2a09-4ba9-bafa-9c695dcd10de.png" style="width:3.83em;height:1.33em;"/></sub>, and when <img class="fm-editor-equation" src="Images/a03f156f-b86c-4430-b2f3-3811be4bb295.png" style="width:3.00em;height:1.08em;"/> then <sub><img class="fm-editor-equation" src="Images/25c15f48-aae3-4452-9b36-3d9abe6fc113.png" style="width:3.58em;height:1.25em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/9f86425e-9412-4554-991e-0beea79e92bf.png" style="width:6.92em;height:1.25em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/a58c4997-49ac-4e80-8eb4-86efdab0d37f.png" style="width:10.33em;height:1.33em;"/></sub> (also known as the triangle inequality)</li>
</ul>
<p>For all <sub><img class="fm-editor-equation" src="Images/cab63016-71db-4057-ab15-3703ef992ad9.png" style="width:4.25em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/0f046b15-4f88-4a7b-a403-58bba1534d5b.png" style="width:2.92em;height:1.00em;"/></sub><span>. </span></p>
<p>It is important to note that any norm on the vector space creates a distance metric on the said vector space, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6994d830-0dca-4777-be75-49cc53797436.png" style="width:9.17em;height:1.42em;"/></p>
<p>This satisfies the rules for metrics, telling us that a normed space is also a metric space.</p>
<p>In general, for our purposes, we will only be concerned with four norms on <img class="fm-editor-equation" src="Images/0d75292c-8c3c-4994-ac3c-78a27dd24bac.png" style="width:1.75em;height:1.25em;"/>, as follows:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/cf34c63d-eda9-4b4c-bb3f-064673f30e83.png" style="width:8.00em;height:3.58em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/f5ce1683-a04d-47b4-acad-6c74e6b2e768.png" style="width:8.00em;height:3.67em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/82f4939a-4a81-430f-93e6-ab044683ec6f.png" style="width:9.92em;height:3.42em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/7954e61a-5c7b-4e5b-8e0b-dd7a90b03c55.png" style="width:9.42em;height:2.25em;"/></sub> (this applies <span>only if </span><sub><img class="fm-editor-equation" src="Images/93e7929c-5409-4644-be2c-d041a2ebb7ed.png" style="width:3.33em;height:1.42em;"/></sub>)</li>
</ul>
<p>If you look carefully at the four norms, you can notice that the 1- and 2-norms are versions of the p-norm. The <img class="fm-editor-equation" src="Images/ea30e469-f5e2-49af-ab89-7aa947c30178.png" style="width:1.42em;height:0.83em;"/>-norm, however, is a limit of the p-norm, as p tends to infinity.</p>
<p>Using these definitions, we can define two vectors to be orthogonal if the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c826bdcc-b2bd-4ae8-9bce-6d974fb3159c.png" style="width:12.33em;height:1.67em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inner product space</h1>
                </header>
            
            <article>
                
<p>An inner product on a vector space is a function <sub><img class="fm-editor-equation" src="Images/11fa8b10-ff24-4368-b7d8-ce2916a811d6.png" style="width:8.75em;height:1.17em;"/></sub>, and satisfies the following rules:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/edecd649-b285-42da-95d0-50ef97703613.png" style="width:5.08em;height:1.42em;"/></sub> </li>
<li><sub><img class="fm-editor-equation" src="Images/c6897dba-84d1-4113-9a2c-376d7b02c3bf.png" style="width:12.67em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/068f5509-eba5-4b14-9dce-5c73cc7cb6e4.png" style="width:8.92em;height:1.42em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/aa9ee405-2b72-4510-8146-801e44f27a2c.png" style="width:7.00em;height:1.33em;"/></sub></li>
</ul>
<p>For all <sub><img class="fm-editor-equation" src="Images/360827c7-e6c8-48a2-b8b9-0d6c7c9c8915.png" style="width:5.00em;height:1.17em;"/></sub> and <img class="fm-editor-equation" src="Images/26da689b-7b4c-4897-bfea-5d3e05701f31.png" style="width:2.67em;height:0.92em;"/>. </p>
<p>It is important to note that any inner product on the vector space creates a norm on the said vector space, which we see as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9f4d1f5a-b9b4-4569-9a97-91b60b372e5c.png" style="width:6.58em;height:2.08em;"/></p>
<p>We can notice from these rules and definitions that all inner product spaces are also normed spaces, and therefore also metric spaces.</p>
<p>Another very important concept is orthogonality, which in a nutshell means that two vectors are perpendicular to each other (that is, they are at a right angle to each other) from Euclidean space. </p>
<p>Two vectors are orthogonal if their inner product is zero—that is, <sub><img class="fm-editor-equation" src="Images/9bfe6457-f3ef-4fa4-8dc3-edd1c2e176a6.png" style="width:6.00em;height:1.67em;"/></sub>. As a shorthand for perpendicularity, we write <sub><img class="fm-editor-equation" src="Images/d93ae4ed-625f-4062-8d2c-d621e791349e.png" style="width:3.58em;height:1.50em;"/></sub>. </p>
<p>Additionally, if the two orthogonal vectors are of unit length—that is, <sub><img class="fm-editor-equation" src="Images/65b6d59f-e6d0-47e8-994d-1ebd7b956809.png" style="width:7.58em;height:1.42em;"/></sub>, then they are called <strong>orthonormal</strong>.</p>
<p>In general, the inner product in <img class="fm-editor-equation" src="Images/94ad46fa-e5a2-4bdd-9200-ecc374b7114c.png" style="width:1.75em;height:1.25em;"/> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/25a6f7b2-28ab-42b3-ba0b-d7abecd25de0.png" style="width:12.58em;height:3.58em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix decompositions</h1>
                </header>
            
            <article>
                
<p>Matrix decompositions are a set of methods that we use to describe matrices using more interpretable matrices and give us insight to the matrices' properties.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Determinant</h1>
                </header>
            
            <article>
                
<p class="mce-root">Earlier, we got a quick glimpse of the determinant of a square 2x2 matrix when we wanted to determine whether a square matrix was invertible. The determinant is a very important concept in linear algebra and is used frequently in the solving of systems of linear equations. </p>
<div class="mce-root packt_infobox"><strong>Note</strong>: The determinant only exists when we have square matrices.</div>
<p>Notationally, the determinant is usually written as either <sub><img class="fm-editor-equation" src="Images/0b5d46e7-aab7-404f-9c5a-2a521719a65e.png" style="width:4.25em;height:1.67em;"/></sub> or <sub><img class="fm-editor-equation" src="Images/7bcc7667-d12b-4b59-8555-698b7a657630.png" style="width:2.00em;height:1.67em;"/></sub>. </p>
<p>Let's take an arbitrary <em>n</em>×<em>n</em> matrix A, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fc549b9a-a279-4403-8cf3-d969793bb540.png" style="width:15.42em;height:7.50em;"/></p>
<p>We will also take its determinant, as follows:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="Images/fd70cd28-87b3-499f-831f-f601aa3060e7.png" style="width:17.42em;height:7.67em;"/></p>
<p>The determinant reduces the matrix to a real number (or, in other words, maps <em>A</em> onto a real number). </p>
<p>We start by checking if a square matrix is invertible. Let's take a 2x2<span> matrix, and from the earlier definition, we know that the matrix applied to its inverse produces the identity matrix. It works no differently than when we multiply <em>a</em> with <sub><img class="fm-editor-equation" src="Images/12381ce9-b5a4-484d-a30b-398b96719371.png" style="width:0.92em;height:2.33em;"/></sub> (only true when <sub><img class="fm-editor-equation" src="Images/5d8ed9bc-035b-4b1f-8f4e-2f7c5055d0a9.png" style="width:3.00em;height:1.42em;"/></sub> ), which produces 1, except with matrices. Therefore, <em>AA<sup>-1</sup></em> = <em>I</em>.</span></p>
<p>Let's go ahead and find the inverse of our matrix, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/389534d5-c840-4a54-9660-9d435f4bf5eb.png" style="width:20.17em;height:3.17em;"/></p>
<p>A is invertible only when <sub><img class="fm-editor-equation" src="Images/e692bb68-b0ff-4b7b-833d-11d882613c56.png" style="width:11.50em;height:1.42em;"/></sub>, and this resulting value is what we call the <strong>determinator</strong>.</p>
<p>Now that we know how to find the determinant in the 2x2 case, let's move on to a 3x3 matrix and find its determinant. It looks like this:</p>
<p><img class="aligncenter size-full wp-image-1251 image-border" src="Images/dcaacbbd-301c-442c-8df0-943aaa3a4dd1.png" style="width:48.00em;height:5.75em;"/></p>
<p class="mce-root">This produces the following:</p>
<p><img class="aligncenter size-full wp-image-1252 image-border" src="Images/b34b0f2a-e554-41c0-b9ba-8b6f21a6419d.png" style="width:52.42em;height:1.67em;"/></p>
<p>I know that probably looks more intimidating, but it's really not. Take a moment to look carefully at what we did and how this would work for a larger <em>n</em>×<em>n</em> matrix.</p>
<p>If we have an <em>n</em><span>×</span><em>n</em> matrix and if it can be triangularly factorized (upper or lower), then its determinant will be the product of all the pivot values. For the sake of simplicity, we will represent all triangularly factorizable matrices with <em>T</em>. Therefore, the determinant can be written like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/da4fdfad-6530-4e3d-a1b0-890ab28ddc31.png" style="width:8.58em;height:3.42em;"/></p>
<p>Looking at the preceding 3<span>×</span>3<span> matrix example, I'm sure you've figured out that computing the determinant for matrices where <em>n &gt; 3</em> is quite a lengthy process. Luckily, there is a way in which we can simplify the calculation, and this is where the Laplace expansion comes to the </span>rescue. </p>
<p>When we want to find the determinant of an n×n matrix, the Laplace expansion finds the determinant of (<em>n</em><span>×</span><em>1</em>)<span>×</span>(<em>n</em><span>×</span><em>1</em>) matrices and does so repeatedly until we get to 2×2 matrices. In general, we can calculate the determinant of an n×n matrix using 2×2 matrices.</p>
<p>Let's again take an <em>n</em>-dimensional square matrix, where <img class="fm-editor-equation" src="Images/71135850-46ac-4950-b5f2-fec0700bb77c.png" style="width:4.50em;height:1.00em;"/>. We then expand for all <sub><img class="fm-editor-equation" src="Images/292e75e4-2d2c-4510-89bd-edf85b8ea306.png" style="width:5.75em;height:1.17em;"/></sub>, as follows:</p>
<ul>
<li>Expansion along row <em>i</em>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/740c3339-ac7b-43e0-b951-b5a16a378603.png" style="width:16.50em;height:3.58em;"/></p>
<ul>
<li>Expansion along row <em>j</em>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3e2ee856-f224-4ed0-8a26-d0ba52f6e264.png" style="width:17.25em;height:3.75em;"/></p>
<p>And <sub><img class="fm-editor-equation" src="Images/7c55b884-5d5a-423d-96f4-b24ed9d40d33.png" style="width:8.92em;height:1.58em;"/></sub> is a sub-matrix of <img class="fm-editor-equation" src="Images/d3e70cf9-6931-454a-a598-aedda811b7ce.png" style="width:5.25em;height:1.17em;"/>, which we get after removing row <em>i</em> and column <em>j</em>. </p>
<p>For example, we have a 3×3 matrix, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><sub><img class="fm-editor-equation" src="Images/fdfc3731-cb32-4bba-9f3a-4fb2d67d3fa0.png" style="width:7.75em;height:4.17em;"/></sub></p>
<p>We want to find its determinant using the Laplace expansion along the first row. This results in the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8e2a7584-c66f-447b-bcb9-e52b961806c2.png" style="width:37.92em;height:4.58em;"/></p>
<p>We can now use the preceding equation from the 2<span>×</span>2 case and calculate the determinant for <em>A</em>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d2636f71-4cb6-4e75-93a3-7dde21e0a3ab.png" style="width:17.92em;height:1.42em;"/>.</p>
<p>Here are some of the very important properties of determinants that are important to know:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/16f6d3c7-74ba-43bf-92f7-2f16b8e8579b.png" style="width:5.42em;height:1.42em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/e3a76aee-a665-4fed-8547-9d627251a41f.png" style="width:9.25em;height:1.58em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/d74c7ad1-a991-49d4-a5a4-c6c4fe85a941.png" style="width:12.50em;height:1.33em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/e9868c92-8b5d-4db8-ba77-71ec39270d0b.png" style="width:11.42em;height:1.67em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/e8510fbc-b008-4249-b409-1e1ff4453953.png" style="width:10.92em;height:1.42em;"/></sub></li>
</ul>
<p>There is one other additional property of the determinant, and it is that we can use it to find the volume of an object in <img class="fm-editor-equation" src="Images/8ef7980e-5e44-44fc-a825-f5a9b7af77ce.png" style="width:1.75em;height:1.25em;"/> whose vertices are formed by the column vectors in the matrix. </p>
<p class="CDPAlignLeft CDPAlign">As an example, let's take a parallelogram in <img class="fm-editor-equation" src="Images/cb11cc18-bb9b-47d9-9429-042daa54d451.png" style="width:1.67em;height:1.50em;"/> with the vectors <sub><img class="fm-editor-equation" src="Images/d639af5c-9a15-4fac-ac1a-aaba700f41b2.png" style="width:4.00em;height:2.75em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/0264b501-74e2-4b51-b168-40bac0458137.png" style="width:3.75em;height:2.67em;"/></sub>. By taking the determinant of the 2<span>×</span>2 matrix, we find the area of the shape (we can only find the volume for objects in <img class="fm-editor-equation" src="Images/cdb89bea-609f-4c72-9b15-180eecd91e80.png" style="width:1.50em;height:1.33em;"/> or higher), as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/364c1723-9301-4179-976d-be8f35b9d566.png" style="width:19.08em;height:1.42em;"/></p>
<p>You are welcome to try it for any 3×3 matrix for yourselves as practice.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Eigenvalues and eigenvectors</h1>
                </header>
            
            <article>
                
<p>Let's imagine an arbitrary real n×n matrix, A. It is very possible that when we apply this matrix to some vector, they are scaled by a constant value. If this is the case, we say that the nonzero <img class="fm-editor-equation" src="Images/c05b0be4-1382-4acb-b4bc-e5c2684a8f53.png" style="width:0.92em;height:0.92em;"/>-dimensional vector is an eigenvector of <em>A</em>, and it corresponds to an eigenvalue λ. We write this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e06e0f16-b9c4-43ec-beec-7324ed13a11d.png" style="width:4.50em;height:1.00em;"/></p>
<div class="mce-root packt_infobox"><strong>Note</strong>: The zero vector (0) cannot be an eigenvector of <em>A</em>, since <em>A</em>0 = 0 = λ0 for all λ.</div>
<p>Let's consider again a matrix <em>A</em> that has an eigenvector <strong>x</strong> and a corresponding eigenvalue λ. Then, the following rules will apply:</p>
<ul>
<li>If we have a matrix <em>A</em> and it has been shifted from its current position to <sub><img class="fm-editor-equation" src="Images/d338804b-269b-4add-bc22-baccc681304c.png" style="width:3.50em;height:1.25em;"/></sub>, then it has the eigenvector <strong>x</strong> and the <span>corresponding eigenvalue </span><sub><img class="fm-editor-equation" src="Images/7a3942f7-0d59-4f5c-aded-9de201f01fa7.png" style="width:2.17em;height:1.00em;"/></sub><span>, for all </span><sub><img class="fm-editor-equation" src="Images/866b4328-a0bd-4c19-9f44-71ded4428307.png" style="width:2.08em;height:0.83em;"/></sub><span>, so that <sub><img class="fm-editor-equation" src="Images/54b269b0-a6ac-49eb-840c-5dc60ea59105.png" style="width:10.75em;height:1.33em;"/></sub>.</span></li>
<li>If the matrix <em>A</em> is invertible, then <strong>x</strong> is also an eigenvector of the inverse of the matrix, <sub><img class="fm-editor-equation" src="Images/92fb6002-13bb-400b-a9ef-a95ab7c53723.png" style="width:2.08em;height:1.25em;"/></sub>, with the corresponding eigenvalue <sub><img class="fm-editor-equation" src="Images/1ad20846-524a-423b-8aab-e1b2c4d2bb04.png" style="width:1.92em;height:1.25em;"/></sub>.</li>
<li><sub><img class="fm-editor-equation" src="Images/034a9f84-edde-414f-bf6f-1be21c06138e.png" style="width:6.17em;height:1.42em;"/></sub> for any <sub><img class="fm-editor-equation" src="Images/e22e2d20-7d9f-434d-acc9-157ed4de652c.png" style="width:2.50em;height:0.92em;"/></sub>.</li>
</ul>
<p>We know from earlier in the chapter that whenever we multiply a matrix and a vector, the direction of the vector is changed, but this is not the case with eigenvectors. They are in the same direction as <em>A</em>, and thus <strong>x</strong> remains unchanged. The eigenvalue, being a scalar value, tells us whether the eigenvector is being scaled, and if so, how much, as well as if the direction of the vector has changed. </p>
<p>Another very fascinating property the determinant has is that it is equivalent to the product of the eigenvalues of the matrix, and it is written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5f0dc73e-df42-4b6f-8403-e51d4e14fb74.png" style="width:9.58em;height:2.67em;"/></p>
<p>But this isn't the only relation that the determinant has with eigenvalues. We can rewrite <img class="fm-editor-equation" src="Images/e06e0f16-b9c4-43ec-beec-7324ed13a11d.png" style="width:4.08em;height:0.92em;"/> in the form<sub><img class="fm-editor-equation" src="Images/54f8feb6-24e6-40ca-a150-a81609a01707.png" style="width:6.25em;height:1.17em;"/></sub>. And since this is equal to zero, this means it is a non-invertible matrix, and therefore its determinant too must be equal to zero. Using this, we can use the determinant to find the eigenvalues. Let's see how.</p>
<p>Suppose we have <sub><img class="fm-editor-equation" src="Images/8b38bf6a-c7d7-4140-82e5-d4448a14148a.png" style="width:4.17em;height:1.08em;"/></sub>. Then, its determinant is shown as follows:</p>
<p style="padding-left: 90px"><img class="aligncenter size-full wp-image-1253 image-border" src="Images/c8f73db5-d21e-4319-8523-232236f34680.png" style="width:36.08em;height:3.75em;"/></p>
<p>We can rewrite this as the following quadratic equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b69adf5d-20df-41b5-a9a5-178ac85c62f0.png" style="width:22.33em;height:1.50em;"/></p>
<p>We know that the quadratic equation will give us both the eigenvalues <sub><img class="fm-editor-equation" src="Images/cc10d4f1-77ba-4e34-b505-8163033da02c.png" style="width:3.58em;height:1.50em;"/></sub>. So, we plug our values into the quadratic formula and get our roots.</p>
<p><span>Another interesting property is that when we have triangular matrices such as the ones we found earlier in this chapter, their eigenvalues are the pivot values. So, if we want to find the determinant of a triangular matrix, then all we have to do is find the product of all the entries along the diagonal.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Trace</h1>
                </header>
            
            <article>
                
<p>Given an <span><em>n</em>×<em>n</em></span><span> matrix <em>A</em></span>, the sum of all the entries on the diagonal is called the <strong>trace</strong>. We write it like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/364d9cba-7c3e-47b9-aa4c-63463575bf6b.png" style="width:8.50em;height:3.58em;"/></p>
<p>The following are four important properties of the trace:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/f55f233e-e4eb-47ae-9e2f-90995eafbea4.png" style="width:13.17em;height:1.33em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/d913abcb-54c7-4539-9e57-732fab57c84c.png" style="width:8.83em;height:1.42em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/d225bee2-8f51-4b81-a891-eb1c01968470.png" style="width:7.42em;height:1.50em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/ed5a77f4-db75-4421-9830-c6c19b4ce1c6.png" style="width:24.92em;height:1.25em;"/></sub></li>
</ul>
<p>A very interesting property of the trace is that it is equal to the sum of its eigenvalues, so that the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6b6d57a8-b831-442e-8ed4-f3555b173032.png" style="width:9.08em;height:2.67em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Orthogonal matrices</h1>
                </header>
            
            <article>
                
<p>The concept of orthogonality arises frequently in linear algebra. It's really just a fancy word for perpendicularity, except it goes beyond two dimensions or a pair of vectors.</p>
<p>But to get an understanding, let's start with two column vectors <sub><img class="fm-editor-equation" src="Images/d048e20b-765d-4b6f-8f31-cd58d6b3d93c.png" style="width:5.75em;height:1.58em;"/></sub>. If they are orthogonal, then the following holds:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fbc99477-3307-41b6-9603-98bb8dd7cf1c.png" style="width:20.08em;height:1.67em;"/>.</p>
<p>Orthogonal matrices are a special kind of matrix where the columns are pairwise orthonormal. What this means is that we have a matrix <sub><img class="fm-editor-equation" src="Images/7d7de76e-8356-43d4-88db-62dedaeb9347.png" style="width:5.08em;height:1.33em;"/></sub>with the following property:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e795f258-7b2f-461b-8bf9-052485c05a31.png" style="width:9.33em;height:1.50em;"/></p>
<p>Then, we can deduce that <sub><img class="fm-editor-equation" src="Images/85df4b42-b4b9-4739-a142-15905a07a1d0.png" style="width:5.58em;height:1.50em;"/></sub> (that is, the transpose of <em>Q</em> is also the inverse of <em>Q</em>).</p>
<p>As with other types of matrices, orthogonal matrices have some special properties. </p>
<p>Firstly, they preserve inner products, so that the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4fbd99a0-b27c-42e0-bb75-467eafd134ac.png" style="width:19.00em;height:1.50em;"/>.</p>
<p>This brings us to the second property, which states that 2-norms are preserved for orthogonal matrices, which we see as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a378af5b-63c5-47c5-b9b5-220bd5fddea5.png" style="width:19.08em;height:2.17em;"/></p>
<p>When multiplying by orthogonal matrices, you can think of it as a transformation that preserves length, but the vector may be rotated about the origin by some degree.</p>
<p><span>The most well-known orthogonal matrix that is also orthonormal is a special matrix we have dealt with a few times already. It is the identity matrix <em>I</em></span><span>, and since it represents a unit of length in the direction of axes, we generally refer to it as the standard basis.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Diagonalization and symmetric matrices</h1>
                </header>
            
            <article>
                
<p>Let's suppose we have a matrix <sub><img class="fm-editor-equation" src="Images/d8bc4a80-34a1-4747-bf06-08764233ed32.png" style="width:4.17em;height:0.92em;"/></sub> that has <img class="fm-editor-equation" src="Images/551f57d1-22cf-4fd0-9ca6-c835a358402c.png" style="width:0.92em;height:0.92em;"/> eigenvectors. We put these vectors into a matrix <em>X</em> that is invertible and multiply the two matrices. This gives us the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/52b2fc21-cdc0-4d05-87d8-3e9ec34e8f84.png" style="width:25.75em;height:1.25em;"/></p>
<p>We know from <img class="fm-editor-equation" src="Images/7f206c31-9575-42b3-ab95-0284067d480c.png" style="width:4.50em;height:1.00em;"/> that when dealing with matrices, this becomes <img class="fm-editor-equation" src="Images/e8874b7e-1006-4644-8838-c68a2d2d573f.png" style="width:5.33em;height:1.00em;"/>, where <sub><img class="fm-editor-equation" src="Images/5d66989d-b7b4-409d-a682-045e3f9b0998.png" style="width:10.83em;height:1.42em;"/></sub> and each <em>x<sub>i</sub></em> has a unique λ<em><sub>i</sub></em>. Therefore, <sub><img class="fm-editor-equation" src="Images/029ad3b2-e319-4292-b49c-9d0cb059f951.png" style="width:6.25em;height:1.17em;"/></sub>.</p>
<p>Let's move on to symmetric matrices. These are special matrices that, when transposed, are the same as the original, implying that <sub><img class="fm-editor-equation" src="Images/678972cc-c1f8-4a32-9573-ae1a26083511.png" style="width:4.33em;height:1.33em;"/></sub> and for all <sub><img class="fm-editor-equation" src="Images/a0a6af0d-8910-4371-b3f3-31a7b0bcab0d.png" style="width:2.83em;height:1.67em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/023b2d00-ee92-4a7e-8dcb-972b27e910d1.png" style="width:5.58em;height:1.33em;"/></sub>. This may seem rather trivial, but its implications are rather strong.</p>
<p>The spectral theorem states that if a matrix <img class="fm-editor-equation" src="Images/d8bc4a80-34a1-4747-bf06-08764233ed32.png" style="width:4.50em;height:1.00em;"/> is a symmetric matrix, then there exists an orthonormal basis for <img class="fm-editor-equation" src="Images/879be859-9317-4137-9128-f37eb55dedad.png" style="width:1.42em;height:1.00em;"/>, which contains the eigenvectors of A.</p>
<p>This theorem is important to us because it allows us to factorize symmetric matrices. We call this <strong>spectral decomposition</strong> (also sometimes referred to as <strong>Eigendecomposition</strong>).</p>
<p>Suppose we have an orthogonal matrix <em>Q</em>, with the orthonormal basis of eigenvectors <sub><img class="fm-editor-equation" src="Images/86835d82-66f0-4cdd-a720-32f72739bc8b.png" style="width:5.75em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/5d66989d-b7b4-409d-a682-045e3f9b0998.png" style="width:10.83em;height:1.42em;"/></sub> being the matrix with corresponding eigenvalues.</p>
<p>From earlier, we know that <sub><img class="fm-editor-equation" src="Images/0d19d219-6c28-4bb3-a122-c6b3469f13b3.png" style="width:5.92em;height:1.33em;"/></sub> for all <sub><img class="fm-editor-equation" src="Images/fc112d41-bab9-4fb3-9ffa-c8c20461ad16.png" style="width:6.42em;height:1.25em;"/></sub>; therefore, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1b6c7941-bb3b-423d-b124-92890c550716.png" style="width:5.92em;height:1.33em;"/></p>
<div class="packt_infobox"><strong>Note</strong>: Λ comes after <em>Q</em> because it is a diagonal matrix, and the <sub><img class="fm-editor-equation" src="Images/c342b78a-6b41-47a3-9c17-eb7d0eb6726c.png" style="width:1.33em;height:1.50em;"/></sub>s need to multiply the individual columns of <em>Q</em>.</div>
<p>By multiplying both sides by <em>Q<sup>T</sup></em>, we get the following result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5e69ce71-fb08-43b4-8c6f-4b0d4a45f36b.png" style="width:6.08em;height:1.42em;"/> </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Singular value decomposition</h1>
                </header>
            
            <article>
                
<p><strong>Singular Value Decomposition</strong> (<strong>SVD</strong>) is widely used in linear algebra and is known for its strength, particularly arising from the fact that every matrix has an SVD. It looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2a9d9100-4f2c-4d8e-a28d-2c79f1c394b7.png" style="width:6.42em;height:1.33em;"/></p>
<p>For our purposes, let's suppose <sub><img class="fm-editor-equation" src="Images/c983f0bd-e4c4-4f1c-b5a9-48f3864ef227.png" style="width:5.92em;height:1.25em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/c2a55e81-b23d-4b32-85c0-5a2549e917e9.png" style="width:5.75em;height:1.17em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/b0f481fc-deb2-4819-aeed-1b8187780c09.png" style="width:5.50em;height:1.17em;"/></sub>, and <sub><img class="fm-editor-equation" src="Images/f09fa2f7-db96-4689-b1cf-f2f61f5195f1.png" style="width:4.92em;height:1.08em;"/></sub>, and that <em>U, V </em>are orthogonal matrices, whereas ∑ is a matrix that contains singular values (denoted by σ<sub>i</sub>) of <em>A</em> along the diagonal. </p>
<p><em>∑</em> in the preceding equation looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/be5c5338-fbd4-4dec-a368-1dd17f879ae2.png" style="width:13.33em;height:5.08em;"/></p>
<p>We can also write the SVD like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e2638ca7-7dbf-4dc0-a887-ae2af1fb7b08.png" style="width:8.67em;height:3.67em;"/></p>
<p>Here, <em>u<sub>i</sub></em>, <em>v<sub>i</sub></em> are the column vectors of <em>U, V</em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cholesky decomposition</h1>
                </header>
            
            <article>
                
<p>As I'm sure you've figured out by now, there is more than one way to factorize a matrix, and there are special methods for special matrices.</p>
<p>The Cholesky decomposition is square root-like and works only on symmetric positive definite matrices. </p>
<p>This works by factorizing <em>A</em> into the form <em>LL<sup>T</sup></em>. Here, <em>L</em>, as before, is a lower triangular matrix.</p>
<p>Do develop some intuition. It looks like this:</p>
<p style="padding-left: 30px"><img class="aligncenter size-full wp-image-1249 image-border" src="Images/dc5af252-82e4-4b19-9309-28bd423b9c64.png" style="width:42.17em;height:8.25em;"/></p>
<p>However, here, <em>L</em> is called a <strong>Cholesky factor</strong>. </p>
<p>Let's take a look at the case where <sub><img class="fm-editor-equation" src="Images/95c6b814-1506-4f8e-bad3-9f7771f0fab6.png" style="width:4.50em;height:1.17em;"/></sub>.</p>
<p class="CDPAlignLeft CDPAlign">We know from the preceding matrix that <sub><img class="fm-editor-equation" src="Images/14988fc6-2973-4534-9dbb-d2f0fa493a60.png" style="width:5.00em;height:1.33em;"/></sub>; therefore, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1cb6fb08-04e8-4ebe-8d22-8bee05f7c6a0.png" style="width:31.67em;height:5.42em;"/></p>
<p>Let's multiply the upper and lower triangular matrices on the right, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6bc01725-9a50-450c-98fa-acb8d95d0de7.png" style="width:24.42em;height:5.42em;"/></p>
<p>Writing out <em>A</em> fully and equating it to our preceding matrix gives us the following:</p>
<p style="padding-left: 60px"><img class="aligncenter size-full wp-image-1254 image-border" src="Images/055c9913-37fa-491e-a439-7e0da68f09cc.png" style="width:36.83em;height:6.08em;"/></p>
<p>We can then compare, element-wise, the corresponding entries of <em>A</em> and <em>LL<sup>T</sup></em> and solve algebraically for <sub><img class="fm-editor-equation" src="Images/26f39821-1aaa-4fb9-a9f1-c44a475c2371.png" style="width:1.75em;height:1.75em;"/>, </sub>as follows:</p>
<p style="padding-left: 240px"><img class="aligncenter size-full wp-image-1255 image-border" src="Images/46bd6583-b5d0-4563-8b92-cf2fc765ffd0.png" style="width:16.42em;height:20.83em;"/></p>
<p>We can repeat this process for any symmetric positive definite matrix, and compute the <em>l<sub>i,j</sub></em> values given <em>a<sub>i,j</sub></em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>With this, we conclude our chapter on linear algebra. So far, we have learned all the fundamental concepts of linear algebra, such as matrix multiplication and factorization, that will lead you on your way to gaining a deep understanding of how <strong>deep neural networks</strong> (<strong>DNNs</strong>) work and are designed, and what it is that makes them so powerful. </p>
<p>In the next chapter, we will be learning about calculus and will combine it with the concepts learned earlier on in this chapter to understand vector calculus.</p>


            </article>

            
        </section>
    </div></body></html>