- en: Linear Algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be covering the main concepts of linear algebra, and
    the concepts learned here will serve as the backbone on which we will learn all
    the concepts in the chapters to come, so it is important that you pay attention.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important for you to know that these chapters cannot be substituted
    for an education in mathematics; they exist merely to help you better grasp the
    concepts of deep learning and how various architectures work and to develop an
    intuition for why that is, so you can become a better practitioner in the field.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, algebra is nothing more than the study of mathematical symbols
    and the rules for manipulating these symbols. The field of algebra acts as a unifier
    for all of mathematics and provides us with a way of thinking. Instead of using
    numbers, we use letters to represent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra, however, concerns only linear transformations and vector spaces.
    It allows us to represent information through vectors, matrices, and tensors,
    and having a good understanding of linear algebra will take you a long way on
    your journey toward getting a very strong understanding of deep learning. It is
    said that a mathematical problem can only be solved if it can be reduced to a
    calculation in linear algebra. This speaks to the power and usefulness of linear
    algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing scalars and vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear equations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector spaces and subspaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix decompositions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing scalars and vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalars are regular numbers, such as 7, 82, and 93,454\. They only have a magnitude
    and are used to represent time, speed, distance, length, mass, work, power, area,
    volume, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors, on the other hand, have magnitude and direction in many dimensions.
    We use vectors to represent velocity, acceleration, displacement, force, and momentum.
    We write vectors in bold—such as ***a*** instead of *a—*and they are usually an
    array of multiple numbers, with each number in this array being an element of
    the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We denote this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/182b5235-dd83-4e93-bb5f-7ffb0b494aa2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/4e0dc3f8-4384-43dd-a3ee-ae289462d55f.png)shows the vector is in
    *n*-dimensional real space, which results from taking the Cartesian product of ![](img/e4c8153a-ac55-41a2-bded-96c31ad09cb7.png) *n*
    times; [![](img/be3c5f05-bafc-4fa7-ba6a-7ef8fa77a41a.png)] shows each element
    is a real number; *i* is the position of each element; and, finally, ![](img/2e60c6b4-990a-48c3-ab6c-7be222119a91.png) is
    a natural number, telling us how many elements are in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: As with regular numbers, you can add and subtract vectors. However, there are
    some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the vector we saw earlier (*x*) and add it with another vector
    (*y*), both of which are in ![](img/4e64ae8f-de5b-4140-9e5f-c10c59a33e28.png),
    so that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3289f2fc-8c7f-4a4e-b3cd-326e542b31b2.png)'
  prefs: []
  type: TYPE_IMG
- en: However, we cannot add vectors with vectors that do not have the same dimension
    or scalars.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when ![](img/f5721bc0-a2b6-415e-90d0-13304d25a4e7.png) in ![](img/8d0ad35c-1f10-4286-8a31-584a4c2afe33.png),
    we reduce to 2-dimensions (for example, the surface of a sheet of paper), and
    when *n = 3*, we reduce to 3-dimensions (the real world).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, however, multiply scalars with vectors. Let λ be an arbitrary scalar,
    which we will multiply with the vector ![](img/a3312634-96a4-46f8-bdc8-624d2ab782aa.png),
    so that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb17ae75-c9c2-47a1-a950-17b930f6b6b4.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, λ gets multiplied by each *x[i]* in the vector. The result of
    this operation is that the vector gets scaled by the value of the scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let ![](img/6479accd-0fcb-4c42-9a5e-f9d36bd87288.png), and ![](img/017c6d94-ddc4-4711-9039-49e898bd9771.png).
    Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77f4a99d-ec7c-4f3a-8474-5d7cf0bda91f.png)'
  prefs: []
  type: TYPE_IMG
- en: While this works fine for multiplying by a whole number, it doesn't help when
    working with fractions, but you should be able to guess how it works. Let's see
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/10896d38-44f3-4208-b47d-e9b8c3c7ad1f.png), and ![](img/15e24cac-aff8-4358-a2ee-51bc9e15a589.png).
    Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d0fc872-48e8-4ab3-bba0-5d6136ca205f.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a very special vector that we can get by multiplying any vector by
    the scalar, **0**. We denote this as **0** and call it the **zero vector** (a
    vector containing only zeros).
  prefs: []
  type: TYPE_NORMAL
- en: Linear equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear algebra, at its core, is about solving a set of linear equations, referred
    to as **a system of equations**. A large number of problems can be formulated
    as a system of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two equations and two unknowns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7517d76f-e4f5-4316-90b5-447587adf16d.png)'
  prefs: []
  type: TYPE_IMG
- en: Both equations produce straight lines. The solution to both these equations
    is the point where both lines meet. In this case, the answer is the point (3,
    1).
  prefs: []
  type: TYPE_NORMAL
- en: 'But for our purposes, in linear algebra, we write the preceding equations as
    a vector equation that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76e54c6c-f5a2-4080-b865-6f014b81ca89.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **b** is the result vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Placing the point (3, 1) into the vector equation, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/486783f2-84f5-4d1f-988e-6589190e3b32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the left-hand side is equal to the right-hand side, so it is,
    in fact, a solution! However, I personally prefer to write this as a coefficient
    matrix, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/372c5f68-002c-4d39-84ea-db6b6e783b5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the coefficient matrix, we can express the system of equations as a matrix
    problem in the form ![](img/6b7983fe-ee48-44a0-b74c-59fb3cd58c61.png), where the
    column vector *v* is the variable vector. We write this as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2adc65f3-31fd-471e-9eef-d2074df5d327.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we will express all our problems in this format.
  prefs: []
  type: TYPE_NORMAL
- en: 'To develop a better understanding, we''ll break down the multiplication of matrix
    *A* and vector *v*. It is easiest to think of it as a linear combination of vectors.
    Let''s take a look at the following example with a 3x3 matrix and a 3x1 vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/791c49d6-f452-406a-904c-c786dd91dbd8.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that matrix and vector multiplication is only possible
    when the number of columns in the matrix is equal to the number of rows (elements)
    in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40d1e7b3-1f2f-4475-a5b5-58972acaf62a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be multiplied since the number of columns in the matrix is equal to
    the number of rows in the vector, but the following matrix cannot be multiplied
    as the number of columns and number of rows are not equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1fbdf70-7cf3-4e4c-8cac-fc3a61635393.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize some of the operations on vectors to create an intuition of
    how they work. Have a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1d03a07-cc24-4dbf-be7f-db99e763e835.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding vectors we dealt with are all in ![](img/76445ae4-3f8e-42c9-adea-ff3339448905.png) (in
    2-dimensional space), and all resulting combinations of these vectors will also
    be in ![](img/faf91354-3538-4a30-924e-42dc074da709.png). The same applies for
    vectors in ![](img/8eb58b6f-21bf-4860-873f-ed3e5a65f586.png), ![](img/2b7e9af7-c89e-44e8-af1c-7c14ece9527f.png),
    and ![](img/533cd992-1a9f-4589-9ffa-520098985a13.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another very important vector operation called the dot product, which
    is a type of multiplication. Let''s take two arbitrary vectors in ![](img/643c9f21-82b7-49a6-b8ef-eaf073e41d6c.png),
    **v** and **w**, and find its dot product, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c194786f-cefc-4ae2-8306-9afe5231bfa7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c113d54b-9c6e-4691-a17c-df055b4386fc.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue, using the same vectors we dealt with before, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07226613-36d6-4cd9-96ea-609f4e4736a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And by taking their dot product, we get zero, which tells us that the two vectors
    are perpendicular (there is a 90° angle between them), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a7ed323-155b-45e6-a628-d7424b8cce1a.png)'
  prefs: []
  type: TYPE_IMG
- en: The most common example of a perpendicular vector is seen with the vectors that
    represent the *x* axis, the *y* axis, and so on. In ![](img/66ae3c43-3e63-4682-a9a7-fc82c039451c.png),
    we write the *x* axis vector as [![](img/63b86189-6d7f-4aa7-baab-a1ebfb6ac991.png)]and
    the *y* axis vector as [![](img/5c747d51-ecf1-4d10-ac7e-5af1dfde6bc9.png)]. If
    we take the dot product *i*•*j*, we find that it is equal to zero, and they are
    thus perpendicular.
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining *i* and *j* into a 2x2 matrix, we get the following identity matrix,
    which is a very important matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fe94128-1604-47f8-a2b8-a999f83cd53b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some of the scenarios we will face when solving linear equations
    of the type ![](img/6b7983fe-ee48-44a0-b74c-59fb3cd58c61.png):'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the matrix [![](img/59d7a363-9573-4da7-9d5e-e40f4c8a433d.png)]and
    the equations [![](img/e6b6fc6a-c0e5-4e8e-89f9-c3f767e5e9bf.png)] and [![](img/37a4cfcb-522a-4687-9f9d-4cef1a955fa0.png)].
    If we do the algebra and multiply the first equation by 3, we get [![](img/ccc9c421-0c52-4f15-a3aa-e0d9fc771341.png)].
    But the second equation is equal to zero, which means that these two equations
    do not intersect and therefore have no solution. When one column is dependent
    on another—that is, is a multiple of another column—all combinations of [![](img/50950cd5-e2c2-409e-8183-deacbc2d2ed0.png)] and [![](img/9fb0c18b-5ed3-49f9-a5a0-ef39e12d4767.png)] lie
    in the same direction. However, seeing as [![](img/caa1a971-d158-4ab0-93b6-1b04e55e2fc0.png)] is
    not a combination of the two aforementioned column vectors and does not lie on
    the same line, it cannot be a solution to the equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take the same matrix as before, but this time, [![](img/d49790e7-c5f8-47b4-9bf4-2e64bbcb2700.png)].
    Since **b** is on the line and is a combination of the dependent vectors, there
    is an infinite number of solutions. We say that **b** is in the column space of
    A. While there is only one specific combination of **v** that produces **b**,
    there are infinite combinations of the column vectors that result in the zero
    vector (**0**). For example, for any value, *a*, we have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c4eeebdc-0420-47b0-96ce-1d4e2e90bfab.png)'
  prefs: []
  type: TYPE_IMG
- en: This leads us to another very important concept, known as the complete solution.
    The complete solution is all the possible ways to produce [![](img/f6146508-ced5-40a4-a9d0-1bfb8c6e4ac5.png)].
    We write this as [![](img/dd655452-f231-4ca3-9157-3fd61d2eb9ee.png)], where [![](img/a4763160-2b3e-4dd3-a820-89698095c2ad.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear equations in n-dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've dealt with linear equations in 2-dimensions and have developed
    an understanding of them, let's go a step further and look at equations in 3-dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, our equations produced curves in the 2-dimensional space (*xy*-plane).
    Now, the equations we will be dealing with will produce planes in 3-dimensional
    space (*xyz*-plane).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an arbitrary 3x3 matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d9e7043-cc8b-4743-a537-7d71f68e6688.png)'
  prefs: []
  type: TYPE_IMG
- en: We know from earlier in having dealt with linear equations in two dimensions
    that our solution **b**, as before, is a linear combination of the three column
    vectors, so that [![](img/b6269a28-f488-440d-856a-abec7d06726e.png)].
  prefs: []
  type: TYPE_NORMAL
- en: The equation [![](img/ccd382f2-fa54-459a-be3a-8043fd2ba13d.png)] (equation 1)
    produces a plane, as do [![](img/2467172b-cec3-4323-b81a-fc6acc1a5059.png)] (equation
    2), and [![](img/5fa43033-d4aa-4585-9de8-35138fdbb6eb.png)] (equation 3).
  prefs: []
  type: TYPE_NORMAL
- en: When two planes intersect, they intersect at a line; however, when three planes
    intersect, they intersect at a point. That point is the vector [![](img/1d78b343-c418-4a7e-9216-a937b806d8bf.png)],
    which is the solution to our problem.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the three planes do not intersect at a point, there is no solution
    to the linear equation. This same concept of solving linear equations can be extended
    to many more dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose now that we have a system with 15 linear equations and 15 unknown variables.
    We can use the preceding method and, according to it, we need to find the point
    that satisfies all the 15 equations—that is, where they intersect (if it exists).
  prefs: []
  type: TYPE_NORMAL
- en: 'It will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c72baa97-897f-4a90-94c0-850d1abd9c00.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can tell, that's a lot of equations we have to deal with, and the greater
    the number of dimensions, the harder this becomes to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Solving linear equations using elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best ways to solve linear equations is by a systematic method known
    as **elimination**. This is a method that allows us to systematically eliminate
    variables and use substitution to solve equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at two equations with two variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f496b41-d3fd-4919-ac59-2eb5d4300ada.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After elimination, this becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba4ab482-dfc9-49f6-adae-aad2f001d085.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the *x* variable is no longer in the second equation. We can
    plug the *y* value back into the first equation and solve for *x*. Doing this,
    we find that *x = 3* and *y = 1*.
  prefs: []
  type: TYPE_NORMAL
- en: We call this **triangular factorization**. There are two types—lower triangular
    and upper triangular. We solve the upper triangular system from top to bottom
    using a process known as **back substitution**, and this works for systems of
    any size.
  prefs: []
  type: TYPE_NORMAL
- en: While this is an effective method, it is not fail-proof. We could come across
    a scenario where we have more equations than variables, or more variables than
    equations, which are unsolvable. Or, we could have a scenario such as *0x = 7*,
    and, as we know, dividing by zero is impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s solve three equations with three variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de168eab-7179-44f1-a395-d1da75a426c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use upper triangular factorization and eliminate variables, starting
    with *y* and then *z*. Let''s start by putting this into our matrix form, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc3e9e1-ee34-4538-9177-9ed91be78ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our purposes and to make things simpler, we will drop ***v***, the column
    vector, and get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/587fb1a2-2227-4b48-8a83-2f4feaa340aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, exchange row 2 and row 3 with each other, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/961c5862-e493-4a5e-bf05-1faa01c3542d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, add row 2 and row 1 together to eliminate the first value in row 2, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66ed810d-f02d-4d1b-a71c-0a3614fdd40f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, multiply row 1 by 3/2 and subtract it from row 3, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b9907b5-99c4-4fe0-b5d8-9cc881331eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, multiply row 2 by 6 and subtract it from row 3, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1af6fb1-b98c-416a-81de-9ffe836300b6.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can notice, the values in the matrix now form a triangle pointing upward,
    which is why we call it upper triangular. By substituting the values back into
    the previous equation backward (from bottom to top), we can solve, and find that [![](img/a01e9e4a-2748-428d-aebd-1ea043400ed1.png)], [![](img/5f884d7a-e7a1-499a-a4f1-54c2dcc6377c.png)],
    and [![](img/ed84a059-cdf7-4685-8b2e-a6c294e503a9.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, ![](img/d9373151-de9b-4942-812a-70b29d99c5fb.png) becomes ![](img/9621aa55-0027-41be-84c4-8417adb89d0d.png),
    as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1a093f9-bad6-47ba-a9bf-47741619dff3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note**: The values across the diagonal in the triangular factorized matrix
    are called pivots, and when factorized, the values below the diagonal are all
    zeros.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check that our found solution is right, we solve ![](img/1194653c-0df1-4954-ac63-f40786e74eac.png),
    using our found values for *x*, *y,* and *z*, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8067f820-eff7-435d-980b-6daff80a91e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This then becomes the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7cf7b61-401c-4446-b92b-84bcfa47f15f.png)'
  prefs: []
  type: TYPE_IMG
- en: And as we can see, the left-hand side is equal to the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: 'After upper triangular factorization, an arbitrary 4x4 matrix will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaf4ec59-37bf-41ad-b82e-5b0028860e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We could take this a step further and factorize the upper triangular matrix
    until we end up with a matrix that contains only the pivot values along the diagonal,
    and zeros everywhere else. This resulting matrix **P** essentially fully solves
    the problem for us without us having to resort to forward or backward substitution,
    and it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/574b75b9-9eb5-4b4c-bb07-ce0154c385df.png)'
  prefs: []
  type: TYPE_IMG
- en: But as you can tell, there are a lot of steps involved in getting us from **A** to
    **P**.
  prefs: []
  type: TYPE_NORMAL
- en: There is one other very important factorization method called **lower-upper**
    (**LU) decomposition**. The way it works is we factorize **A** into an upper triangular
    matrix **U**, and record the steps of Gaussian elimination in a lower triangular
    matrix **L**, such that [![](img/b5ee1b4e-a497-4c37-873c-b147e73f74d1.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the matrix we upper-triangular factorized before and put it
    into LU factorized form, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37dce8d5-7c0a-45fb-a51b-80f7269227ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we multiply the two matrices on the right, we will get the original matrix
    **A**. But how did we get here? Let''s go through the steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with ![](img/42357787-303e-434f-8a90-864135a19ec1.png), so that the
    following applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5f11564a-313c-4613-a616-89d647c9ad71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We add -1 to what was the identity matrix at *l[2,1]* to represent the operation
    (row 2)-(-1)(row 1), so it becomes the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a9dac746-1c92-4770-aa75-3c6ef67aff43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then add [![](img/5931f8e1-70cc-4886-9256-66f4880fb85d.png)] to the matrix
    at *l[3,1]* to represent the [![](img/cd8b62e1-0be4-491f-a862-2a64a5781bf4.png) ]operation,
    so it becomes the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d999f17f-ba8e-4058-af67-e4ff6a6f7555.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then add 6 to the matrix at *l[3,2]* to represent the operation (row 3)-6(row
    2), so it becomes the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/231a078a-f411-4fc1-8631-016ecd7609a1.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the LU factorized matrix we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: You might now be wondering what this has to do with solving ![](img/60491e0d-7150-433b-ba6b-bf0c5bf0caeb.png),
    which is very valid. The elimination process tends to work quite well, but we
    have to additionally apply all the operations we did on **A** to ***b** *as well,
    and this involves extra steps. However, LU factorization is only applied to **A**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now take a look at how we can solve our system of linear equations using
    this method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we drop the variables vector and write **A** and **b** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/961c5862-e493-4a5e-bf05-1faa01c3542d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But even this can get cumbersome to write as we go, so we will instead write
    it in the following way for further simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c766d0c3-0dec-4001-926a-43f3392d6f22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then multiply both sides by ![](img/799cb620-2b53-4a46-a42a-f0d351b4a4bc.png)and
    get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff2aaab2-8480-4537-afa7-5242a9ef6029.png)'
  prefs: []
  type: TYPE_IMG
- en: This tells us that ![](img/6635e792-8f7a-4022-a2de-cd3f398a60c9.png), and we
    already know from the preceding equation that [![](img/43d8a95a-f8c4-49dd-92af-c96973a49357.png)] (so
    [![](img/50f2f8f9-db05-4674-9982-fcf5570d9f71.png)]). And by using back substitution,
    we can find the vector **v**.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, you may have noticed some new notation that I have
    not yet introduced, but not to worry—we will observe all the necessary notation
    and operations in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand how to solve systems of linear equations of the type ![](img/c3085ddc-2bf6-43b2-81df-37ede45e7ed2.png) where
    we multiplied a matrix with a column vector, let's move on to dealing with the
    types of operations we can do with one or more matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Adding matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with scalars and vectors, sometimes we may have to add two or more matrices
    together, and the process of doing so is rather straightforward. Let''s take two
    ![](img/b70e9807-7600-4d82-bb97-4fe0408bf0ff.png) matrices, *A* and *B*, and add
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1a55f22-b8bb-421d-9e27-7ed05f8e1a72.png)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that we can only add matrices that have the same dimensions,
    and, as you have probably noticed, we add the matrices element-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have only multiplied a matrix by a column vector. But now, we will
    multiply a matrix *A* with another matrix *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four simple rules that will help us in multiplying matrices, listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we can only multiply two matrices when the number of columns in matrix
    *A* is equal to the number of rows in matrix *B*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the first row of matrix *A* multiplied by the first column of matrix
    *B* gives us the first element in the matrix *AB*, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirdly, when multiplying, order matters—specifically, *AB* ≠ *BA*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the element at row *i*, column *j* is the product of the *i^(th)* row
    of matrix *A* and the *j^(th)* column of matrix *B*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s multiply an arbitrary 4x5 matrix with an arbitrary 5x6 matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf39a6fc-ea74-4d5c-9e27-b94ea3aff4c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This results in a 4x6 matrix, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f013907-7b56-409b-86c9-6c5ff9c29919.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From that, we can deduce that in general, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e89a845-02b9-437f-8f25-aa241cfdacf5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take the following two matrices and multiply them, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d32786c0-cc3f-454e-86be-d69a00a1f9af.png) and ![](img/23235442-c6f3-44fe-b693-076fc232e5dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will give us the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcc54925-a1dc-4772-b339-7d8ce304aac2.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: In this example, the matrix *B* is the identity matrix, usually written
    as *I*.'
  prefs: []
  type: TYPE_NORMAL
- en: The identity matrix has two unique properties in matrix multiplication. When
    multiplied by any matrix, it returns the original matrix unchanged, and the order
    of multiplication does not matter—so, *AI = IA = A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s use the same matrix *A* from earlier, and multiply it by
    another matrix *B*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73ae2b9e-33a7-4bb1-93bc-f2fc03ff02be.png)'
  prefs: []
  type: TYPE_IMG
- en: Another very special matrix is the inverse matrix, which is written as *A^(-1)*.
    And when we multiply *A* with *A^(-1)*, we receive *I*, the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, the order in which we multiply matters. We must keep the
    matrices in order, but we do have some flexibility. As we can see in the following
    equation, the parentheses can be moved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ee7c381-74a9-4daa-a773-3c492e1cf2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the first law of matrix operations, known as **associativity**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are three important laws that cannot be broken:'
  prefs: []
  type: TYPE_NORMAL
- en: '**commutativity**: [![](img/b548464d-80d3-4bba-8ec9-997a3707ffd9.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**distributivity**: [![](img/2d775123-c5e6-431e-b436-894b598fad3d.png)] or [![](img/44a52be2-f47b-4e4d-bad5-70277c329a46.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**associativity**: [![](img/2d6c6496-8c34-42be-880b-91170d0091f0.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As proof that *AB ≠ BA*, let''s take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f79ed756-09fc-4be7-9a14-997774bd40e7.png)'
  prefs: []
  type: TYPE_IMG
- en: This conclusively shows that the two results are not the same.
  prefs: []
  type: TYPE_NORMAL
- en: We know that we can raise numbers to powers, but we can also raise matrices
    to powers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we raise the matrix *A* to power *p*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71d83d9f-212c-4d9d-a6e1-8d0e8c98ccc9.png) (multiplying the matrix by
    itself *p* times)'
  prefs: []
  type: TYPE_IMG
- en: There are two additional power laws for matrices—[![](img/3a088ac9-9eb6-48b3-b774-eb53eaeff2f7.png)] and [![](img/ef86610b-94de-449e-85ec-14afe594593f.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Inverse matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's revisit the concept of inverse matrices and go a little more in depth
    with them. We know from earlier that *AA^(-1 )*= *I*, but not every matrix has
    an inverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are, again, some rules we must follow when it comes to finding the inverses
    of matrices, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The inverse only exists if, through the process of upper or lower triangular
    factorization, we obtain all the pivot values on the diagonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the matrix is invertible, it has only one unique inverse matrix—that is,
    if *AB* = *I* and *AC* = *I*, then *B* = *C*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *A* is invertible, then to solve *Av* = *b* we multiply both sides by *A^(-1)* and
    get *AA^(-1)v* = *A^(-1)b*, which finally gives us = *A^(-1)b*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *v* is nonzero and *b* = 0, then the matrix does not have an inverse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2 x 2 matrices are invertible only if *ad* - *bc* ≠ 0, where the following
    applies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/013c60f9-7633-490c-aece-0fada2543d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: And *ad* - *bc* is called the **determinant** of *A*. *A^(-1)* involves dividing
    each element in the matrix by the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, if the matrix has any zero values along the diagonal, it is non-invertible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, we may have to invert the product of two matrices, but that is only
    possible when both the matrices are individually invertible (follow the rules
    outlined previously).
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's take two matrices A and B, which are both invertible. Then, [![](img/0c509cb9-9272-488f-a67b-f6374d0bdc38.png)] so
    that [![](img/bb4a926f-aed2-4f85-983d-bf9a3547518e.png)].
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: Pay close attention to the order of the inverse—it too must follow
    the order. The left-hand side is the mirror image of the right-hand side.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix transpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take an ![](img/ab7680f6-95f0-4d68-ba9c-3d2e38318c94.png) matrix *A*.
    If the matrix''s transpose is *B*, then the dimensions of *B* are ![](img/94bb719d-cb12-4f62-9cce-0bc2b057effe.png),
    such that: [![](img/ee4e4aed-0235-496e-82b8-1afe39ab4c5e.png)].  Here is the matrix
    *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/867914db-b723-48b4-b247-5e036bf6bcb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the matrix *B* is as given:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/073aeb58-5497-4697-9ae1-2841d7b48e03.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, we can think of this as writing the columns of *A* as the rows
    of the transposed matrix, *B*.
  prefs: []
  type: TYPE_NORMAL
- en: We usually write the transpose of *A* as *A^T*.
  prefs: []
  type: TYPE_NORMAL
- en: A symmetric matrix is a special kind of matrix. It is an *n×n* matrix that,
    when transposed, is exactly the same as before we transposed it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the properties of inverses and transposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/8305ca1b-6dc0-4caa-8d2d-ba4b96592abc.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2f9790a9-9bc9-404e-aa53-f4ff7be116e2.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/53efb269-ac19-435c-8350-28696549f039.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/47a5e35c-f82f-47e2-92d1-8a9f6d7b0299.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a1710de3-2ebc-4e00-aae4-9822a0affca5.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/0846e2c5-db1a-43da-a3e8-24c6ef96fe48.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <q>If A is an invertible matrix, then so is A^T, and so (A^(-1))^T = (A^T)^(-1)
    = A^(-T).</q>
  prefs: []
  type: TYPE_NORMAL
- en: Permutations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the example on solving systems of linear equations, we swapped the positions
    of rows 2 and 3\. This is known as a **permutation**.
  prefs: []
  type: TYPE_NORMAL
- en: When we are doing triangular factorization, we want our pivot values to be along
    the diagonal of the matrix, but this won't happen every time—in fact, it usually
    won't. So, instead, what we do is swap the rows so that we get our pivot values
    where we want them.
  prefs: []
  type: TYPE_NORMAL
- en: But that is not their only use case. We can also use them to scale individual
    rows by a scalar value or add rows to or subtract rows from other rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with some of the more basic permutation matrices that we obtain
    by swapping the rows of the identity matrix. In general, we have *n!* possible
    permutation matrices that can be formed from an *n*x*n* identity matrix. In this
    example, we will use a 3×3 matrix and therefore have six permutation matrices,
    and they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/8edbd7d1-57ee-44ce-bdac-feaf61fe18b1.png)] This matrix makes no change
    to the matrix it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/de0bdfe3-c2d9-4c81-be0b-738ab96987e1.png)] This matrix swaps rows
    two and three of the matrix it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a9635367-129f-45b6-9fc8-21e88a13af91.png) ]This matrix swaps rows
    one and two of the matrix it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/20133a22-4f28-4529-8da3-d6a0ae7eee8a.png) ]This matrix shifts rows
    two and three up one and moves row one to the position of row three of the matrix
    it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/66df650f-aff2-4922-8f6a-df9e89e07d6e.png)] This matrix shifts rows
    one and two down one and moves row three to the row-one position of the matrix
    it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9b553584-caaf-4548-9864-6bcbde8f35dc.png)] This matrix swaps rows
    one and three of the matrix it is applied on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is important to note that there is a particularly fascinating property of
    permutation matrices that states that if we have a matrix ![](img/26f5430f-c8bb-41e6-9a89-543e2bf9950a.png) and
    it is invertible, then there exists a permutation matrix that when applied to
    *A* will give us the LU factor of *A*. We can express this like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92bc5c01-f9d3-489f-9ed6-8783132ac7ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector spaces and subspaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the concepts of vector spaces and subspaces.
    These are very important to our understanding of linear algebra. In fact, if we
    do not have an understanding of vector spaces and subspaces, we do not truly have
    an understanding of how to solve linear algebra problems.
  prefs: []
  type: TYPE_NORMAL
- en: Spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector spaces are one of the fundamental settings for linear algebra, and, as
    the name suggests, they are spaces where all vectors reside. We will denote the
    vector space with V.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to think of dimensions is to count the number of elements in
    the column vector. Suppose we have [![](img/f844062d-c870-4c63-8da4-9c367d12665a.png)],
    then ![](img/b83dee00-ee90-44cc-a62b-be53a3b1779b.png). ![](img/c0be8f8b-2060-45d3-a5d3-84aac64ebf02.png) is
    a straight line, ![](img/b306fc56-e969-41c7-8bfa-f37854133f01.png) is all the
    possible points in the *xy*-plane, and ![](img/37a7dfff-6c23-4f1a-86af-e7ccc17cce8b.png) is
    all the possible points in the *xyz*-plane—that is, 3-dimensional space, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the rules for vector spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: There exists in *V* an additive identity element such that ![](img/9beb757e-6a2d-462b-8634-ee8c79d8ab11.png) for
    all ![](img/8c64162a-985e-488b-9afd-e2065e102868.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all ![](img/72e37558-dab0-4707-95d1-e3531553ee79.png), there exists an additive
    inverse such that [![](img/7a4abfc2-17f6-48ca-9db4-f9754cc3ee0c.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all ![](img/73b70efa-1d94-44f8-9bed-8f936ca38de0.png), there exists a multiplicative
    identity such that ![](img/e6918d7b-3df3-4d99-952e-3f8655fc3501.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors are commutative, such that for all [![](img/8a616fa2-317e-45d6-b02f-d0aba9a4280d.png)], [![](img/12689855-e010-4a34-acde-56945f94ab77.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors are associative, such that [![](img/58b58922-0bd4-4d4e-bc68-c0c13907c741.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors have distributivity, such that [![](img/447375df-bf81-4687-be00-faa44d6fae45.png)] and [![](img/4ef06655-1d74-49ea-b3b3-992d1569536b.png)] for
    all [![](img/6a84499a-6142-43bc-9dd0-e436f6af128f.png)] and for all [![](img/9e915871-db33-4314-b024-9f8562c0a820.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of vectors is said to be linearly independent if [![](img/81baafd7-b0d0-40f0-b313-132bb8b4012a.png)],
    which implies that [![](img/f78c7ab6-2fa7-47ab-ac6d-d9e95aea9cd3.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Another important concept for us to know is called **span**. The span of [![](img/7dd1d26b-d04e-4e28-b6c8-c7a64cfeb149.png)] is
    the set of all linear combinations that can be made using the *n* vectors. Therefore, [![](img/c4e15e74-1733-4695-adba-78e26a18609c.png)] if
    the vectors are linearly independent and span *V* completely; then, the vectors
    [![](img/dbc93f18-3f2b-4d8c-a3e9-5688c00f0aeb.png)] are the basis of *V*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the dimension of *V* is the number of basis vectors we have, and
    we denote it *dimV*.
  prefs: []
  type: TYPE_NORMAL
- en: Subspaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Subspaces are another very important concept that state that we can have one
    or many vector spaces inside another vector space. Let''s suppose *V* is a vector
    space, and we have a subspace [![](img/a602de35-3055-4e2e-82aa-563c44767ff1.png)].
    Then, *S* can only be a subspace if it follows the three rules, stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/f230abc5-6336-415b-b5bf-6c54332c66fa.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/61436647-3352-487a-9d71-4def0832edfe.png)] and [![](img/b3fb2604-8b33-4e9e-b336-36115f0f1a0f.png)],
    which implies that *S* is closed under addition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e314788f-8f13-4302-92a0-d9e818fdbb12.png)] and ![](img/d1a7dedd-4412-4dee-a314-cca86bc4332d.png) so
    that [![](img/ab71728c-9292-4dfc-8014-756cbebb2ea2.png)], which implies that *S*
    is closed under scalar multiplication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If [![](img/8d92b7e1-bbfb-4388-a191-0e954364794b.png)], then their sum is [![](img/cda6b403-b85a-4810-8de1-a32ec98b2afb.png)],
    where the result is also a subspace of *V*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dimension of the sum [![](img/b464bbc9-fca3-4bce-b0ed-041fc6ba0dab.png)] is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f60e2d10-8138-4538-bc8f-68ed61411b04.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A linear map is a function [![](img/ac0809fa-111a-40d8-b331-d7157fdd43b3.png)],
    where *V* and *W* are both vector spaces. They must satisfy the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/e1a8d556-6c13-4aee-896d-0b3ed210e527.png)], for all [![](img/36462293-378b-48df-928f-ad0affdb936e.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/fd7d3f66-580f-4603-84dc-b46f1ffec174.png)], for all ![](img/9f555bb5-8a65-46e2-99b2-e42246724dde.png) and ![](img/72e1df16-deb3-4998-afe5-0d497cd2d20f.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear maps tend to preserve the properties of vector spaces under addition
    and scalar multiplication. A linear map is called a **homomorphism of vector spaces;** however,
    if the homomorphism is invertible (where the inverse is a homomorphism), then
    we call the mapping an **isomorphism**.
  prefs: []
  type: TYPE_NORMAL
- en: When *V* and *W* are isomorphic, we denote this as [![](img/26027ade-7a86-45e5-bb09-ac388bb121c9.png)],
    and they both have the same algebraic structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *V* and *W* are vector spaces in ![](img/910e77b1-f616-4036-bada-4c99ea52bede.png),
    and [![](img/09622b11-f715-4c5e-a913-ef23d82cd630.png)], then it is called a **natural
    isomorphism**. We write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22fa3783-7c2e-49c3-86ca-981bb4223707.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/601b901c-1bb0-46fa-8917-68127c269361.png)] and [![](img/4d96e1bb-bd16-47aa-83ef-6f74d21eb605.png)] are
    the bases of *V* and *W*. Using the preceding equation, we can see that [![](img/34f9d116-e22a-4fb1-b265-f392c2ce11a7.png)],
    which tells us that ![](img/67ebcbe5-26df-4898-90db-b048a6ff8448.png) is an isomorphism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the same vector spaces *V* and *W* as before, with bases [![](img/601b901c-1bb0-46fa-8917-68127c269361.png)] and
    [![](img/12bd3b39-0d26-4da0-9d90-9cc966fe50d2.png)] respectively. We know that [![](img/ac0809fa-111a-40d8-b331-d7157fdd43b3.png)] is
    a linear map, and the matrix *T* that has entries *A[ij]*, where [![](img/e0df65fc-46d3-428d-9a8a-102f64731a6d.png)] and
    [![](img/6774c8d6-2f4f-4648-a039-903a08738ab9.png)] can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cda3d08-be4c-413c-b1ff-60d1f1b9780f.png).'
  prefs: []
  type: TYPE_NORMAL
- en: From our knowledge of matrices, we should know that the *j^(th)* column of A contains
    *Tv[j]* in the basis of *W*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, [![](img/4d5d4c14-13fc-4851-b71f-e678d1f5074e.png)] produces a linear
    map [![](img/c358e717-1517-46e6-ad53-d3feed59270f.png)], which we write as [![](img/6865cab1-d211-4b93-8a1a-615255d165b2.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Image and kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When dealing with linear mappings, we will often encounter two important terms:
    the image and the kernel, both of which are vector subspaces with rather important
    properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **kernel** (sometimes called the **null space**) is 0 (the zero vector)
    and is produced by a linear map, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09bdec99-0281-4c85-ac22-23a61bf2b888.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the **image** (sometimes called the **range**) of T is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79f69b4a-54ec-4660-a8fc-7941a335376d.png) such that ![](img/84c1e82c-9ade-4597-8e3f-01c258a99038.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '*V* and *W* are also sometimes known as the **domain** and **codomain** of
    *T*.'
  prefs: []
  type: TYPE_NORMAL
- en: It is best to think of the kernel as a linear mapping that maps the vectors [![](img/7fb41e44-9879-45ab-9014-041563a056a1.png)] to
    [![](img/03d14613-91ca-452a-be46-a829033a023f.png)]. The image, however, is the
    set of all possible linear combinations of ![](img/f57a5c66-6e0e-440c-98e6-8de3fad7c04e.png) that
    can be mapped to the set of vectors [![](img/07b20ab5-840d-4c18-b6ae-6d3ce53df6ad.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Rank-Nullity theorem** (sometimes referred to as the **fundamental theorem
    of linear mappings**) states that given two vector spaces *V* and *W* and a linear
    mapping [![](img/c1fc64ab-aeaa-4265-bf37-e1f82c113979.png)], the following will
    remain true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fc2d842-a5e5-4b4b-86f4-d591b8978768.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Metric space and normed space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics help define the concept of distance in Euclidean space (denoted by ![](img/b7bcefa4-b4a6-4848-8a71-a5d63b1ecf70.png)). Metric
    spaces, however, needn't always be vector spaces. We use them because they allow
    us to define limits for objects besides real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been dealing with vectors, but what we don't yet know is how
    to calculate the length of a vector or the distance between two or more vectors,
    as well as the angle between two vectors, and thus the concept of orthogonality
    (perpendicularity). This is where Euclidean spaces come in handy. In fact, they
    are the fundamental space of geometry. This may seem rather trivial at the moment,
    but their importance will become more apparent to you as we get further on in
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: <q>In Euclidean space, we tend to refer to vectors as points. </q>
  prefs: []
  type: TYPE_NORMAL
- en: 'A metric on a set *S* is defined as a function ![](img/6f786cc5-9264-4891-a248-778807493a47.png) and
    satisfies the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/8b8bb6a5-d500-45b4-ab76-6c08e4480d65.png)], and when [![](img/112dca43-e1ca-4662-9ae2-e384dfef930d.png)] then [![](img/45075729-42e3-4fc0-b53a-63ec239465fd.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/331b5c2e-39e4-4584-b838-0cdff3ab4323.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/52f95b13-d6af-4ed9-a372-42e71b11508a.png)] (known as the **triangle
    inequality**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all [![](img/fb6600a3-ac68-4c0c-8740-f92106e6bff5.png)].
  prefs: []
  type: TYPE_NORMAL
- en: That's all well and good, but how exactly do we calculate distance?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have two points, [![](img/e6b07452-f8d1-424c-a80d-7365c8467686.png)] and [![](img/dc41fe2e-6d5c-422a-8fe3-0e90c22efb13.png)];
    then, the distance between them can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65b4a10a-7be3-40ad-a9a0-e5b28ef19972.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And we can extend this to find the distance of points in ![](img/93d0f60e-ce6a-4ff9-b140-11ce34ca9ad2.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/298ce5eb-d7eb-480f-97ac-c2519734d4a5.png)'
  prefs: []
  type: TYPE_IMG
- en: While metrics help with the notion of distance, norms define the concept of
    length in Euclidean space.
  prefs: []
  type: TYPE_NORMAL
- en: 'A norm on a vector space is a function [![](img/c834d5a8-9576-47b0-a9ae-c1822af7b8e8.png)],
    and satisfies the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/72bc7a6f-2a09-4ba9-bafa-9c695dcd10de.png)], and when ![](img/a03f156f-b86c-4430-b2f3-3811be4bb295.png) then
    [![](img/25c15f48-aae3-4452-9b36-3d9abe6fc113.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9f86425e-9412-4554-991e-0beea79e92bf.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a58c4997-49ac-4e80-8eb4-86efdab0d37f.png)] (also known as the triangle
    inequality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all [![](img/cab63016-71db-4057-ab15-3703ef992ad9.png)] and [![](img/0f046b15-4f88-4a7b-a403-58bba1534d5b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that any norm on the vector space creates a distance
    metric on the said vector space, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6994d830-0dca-4777-be75-49cc53797436.png)'
  prefs: []
  type: TYPE_IMG
- en: This satisfies the rules for metrics, telling us that a normed space is also
    a metric space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, for our purposes, we will only be concerned with four norms on ![](img/0d75292c-8c3c-4994-ac3c-78a27dd24bac.png),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/cf34c63d-eda9-4b4c-bb3f-064673f30e83.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/f5ce1683-a04d-47b4-acad-6c74e6b2e768.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/82f4939a-4a81-430f-93e6-ab044683ec6f.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/7954e61a-5c7b-4e5b-8e0b-dd7a90b03c55.png)] (this applies only if [![](img/93e7929c-5409-4644-be2c-d041a2ebb7ed.png)])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you look carefully at the four norms, you can notice that the 1- and 2-norms
    are versions of the p-norm. The ![](img/ea30e469-f5e2-49af-ab89-7aa947c30178.png)-norm,
    however, is a limit of the p-norm, as p tends to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these definitions, we can define two vectors to be orthogonal if the
    following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c826bdcc-b2bd-4ae8-9bce-6d974fb3159c.png)'
  prefs: []
  type: TYPE_IMG
- en: Inner product space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An inner product on a vector space is a function [![](img/11fa8b10-ff24-4368-b7d8-ce2916a811d6.png)],
    and satisfies the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/edecd649-b285-42da-95d0-50ef97703613.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/c6897dba-84d1-4113-9a2c-376d7b02c3bf.png)] and [![](img/068f5509-eba5-4b14-9dce-5c73cc7cb6e4.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/aa9ee405-2b72-4510-8146-801e44f27a2c.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all [![](img/360827c7-e6c8-48a2-b8b9-0d6c7c9c8915.png)] and ![](img/26da689b-7b4c-4897-bfea-5d3e05701f31.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that any inner product on the vector space creates
    a norm on the said vector space, which we see as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f4d1f5a-b9b4-4569-9a97-91b60b372e5c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can notice from these rules and definitions that all inner product spaces
    are also normed spaces, and therefore also metric spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Another very important concept is orthogonality, which in a nutshell means that
    two vectors are perpendicular to each other (that is, they are at a right angle
    to each other) from Euclidean space.
  prefs: []
  type: TYPE_NORMAL
- en: Two vectors are orthogonal if their inner product is zero—that is, [![](img/9bfe6457-f3ef-4fa4-8dc3-edd1c2e176a6.png)].
    As a shorthand for perpendicularity, we write [![](img/d93ae4ed-625f-4062-8d2c-d621e791349e.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if the two orthogonal vectors are of unit length—that is, [![](img/65b6d59f-e6d0-47e8-994d-1ebd7b956809.png)],
    then they are called **orthonormal**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the inner product in ![](img/94ad46fa-e5a2-4bdd-9200-ecc374b7114c.png) is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25a6f7b2-28ab-42b3-ba0b-d7abecd25de0.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix decompositions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Matrix decompositions are a set of methods that we use to describe matrices
    using more interpretable matrices and give us insight to the matrices' properties.
  prefs: []
  type: TYPE_NORMAL
- en: Determinant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we got a quick glimpse of the determinant of a square 2x2 matrix when
    we wanted to determine whether a square matrix was invertible. The determinant
    is a very important concept in linear algebra and is used frequently in the solving
    of systems of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: The determinant only exists when we have square matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: Notationally, the determinant is usually written as either [![](img/0b5d46e7-aab7-404f-9c5a-2a521719a65e.png)] or [![](img/7bcc7667-d12b-4b59-8555-698b7a657630.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an arbitrary *n*×*n* matrix A, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc549b9a-a279-4403-8cf3-d969793bb540.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will also take its determinant, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd70cd28-87b3-499f-831f-f601aa3060e7.png)'
  prefs: []
  type: TYPE_IMG
- en: The determinant reduces the matrix to a real number (or, in other words, maps
    *A* onto a real number).
  prefs: []
  type: TYPE_NORMAL
- en: We start by checking if a square matrix is invertible. Let's take a 2x2 matrix,
    and from the earlier definition, we know that the matrix applied to its inverse
    produces the identity matrix. It works no differently than when we multiply *a* with [![](img/12381ce9-b5a4-484d-a30b-398b96719371.png)] (only
    true when [![](img/5d8ed9bc-035b-4b1f-8f4e-2f7c5055d0a9.png)] ), which produces
    1, except with matrices. Therefore, *AA^(-1)* = *I*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and find the inverse of our matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/389534d5-c840-4a54-9660-9d435f4bf5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: A is invertible only when [![](img/e692bb68-b0ff-4b7b-833d-11d882613c56.png)],
    and this resulting value is what we call the **determinator**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how to find the determinant in the 2x2 case, let''s move on
    to a 3x3 matrix and find its determinant. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcaacbbd-301c-442c-8df0-943aaa3a4dd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b34b0f2a-e554-41c0-b9ba-8b6f21a6419d.png)'
  prefs: []
  type: TYPE_IMG
- en: I know that probably looks more intimidating, but it's really not. Take a moment
    to look carefully at what we did and how this would work for a larger *n*×*n* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have an *n*×*n* matrix and if it can be triangularly factorized (upper
    or lower), then its determinant will be the product of all the pivot values. For
    the sake of simplicity, we will represent all triangularly factorizable matrices
    with *T*. Therefore, the determinant can be written like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da4fdfad-6530-4e3d-a1b0-890ab28ddc31.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the preceding 3×3 matrix example, I'm sure you've figured out that
    computing the determinant for matrices where *n > 3* is quite a lengthy process.
    Luckily, there is a way in which we can simplify the calculation, and this is
    where the Laplace expansion comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: When we want to find the determinant of an n×n matrix, the Laplace expansion
    finds the determinant of (*n*×*1*)×(*n*×*1*) matrices and does so repeatedly until
    we get to 2×2 matrices. In general, we can calculate the determinant of an n×n matrix
    using 2×2 matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s again take an *n*-dimensional square matrix, where ![](img/71135850-46ac-4950-b5f2-fec0700bb77c.png).
    We then expand for all [![](img/292e75e4-2d2c-4510-89bd-edf85b8ea306.png)], as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expansion along row *i*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/740c3339-ac7b-43e0-b951-b5a16a378603.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expansion along row *j*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3e2ee856-f224-4ed0-8a26-d0ba52f6e264.png)'
  prefs: []
  type: TYPE_IMG
- en: And [![](img/7c55b884-5d5a-423d-96f4-b24ed9d40d33.png)] is a sub-matrix of ![](img/d3e70cf9-6931-454a-a598-aedda811b7ce.png),
    which we get after removing row *i* and column *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have a 3×3 matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/fdfc3731-cb32-4bba-9f3a-4fb2d67d3fa0.png)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to find its determinant using the Laplace expansion along the first
    row. This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e2a7584-c66f-447b-bcb9-e52b961806c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now use the preceding equation from the 2×2 case and calculate the determinant
    for *A*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2636f71-4cb6-4e75-93a3-7dde21e0a3ab.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the very important properties of determinants that are important
    to know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/16f6d3c7-74ba-43bf-92f7-2f16b8e8579b.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e3a76aee-a665-4fed-8547-9d627251a41f.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d74c7ad1-a991-49d4-a5a4-c6c4fe85a941.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e9868c92-8b5d-4db8-ba77-71ec39270d0b.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e8510fbc-b008-4249-b409-1e1ff4453953.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is one other additional property of the determinant, and it is that we
    can use it to find the volume of an object in ![](img/8ef7980e-5e44-44fc-a825-f5a9b7af77ce.png) whose
    vertices are formed by the column vectors in the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s take a parallelogram in ![](img/cb11cc18-bb9b-47d9-9429-042daa54d451.png) with
    the vectors [![](img/d639af5c-9a15-4fac-ac1a-aaba700f41b2.png)] and [![](img/0264b501-74e2-4b51-b168-40bac0458137.png)].
    By taking the determinant of the 2×2 matrix, we find the area of the shape (we
    can only find the volume for objects in ![](img/cdb89bea-609f-4c72-9b15-180eecd91e80.png) or
    higher), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/364c1723-9301-4179-976d-be8f35b9d566.png)'
  prefs: []
  type: TYPE_IMG
- en: You are welcome to try it for any 3×3 matrix for yourselves as practice.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s imagine an arbitrary real n×n matrix, A. It is very possible that when
    we apply this matrix to some vector, they are scaled by a constant value. If this
    is the case, we say that the nonzero ![](img/c05b0be4-1382-4acb-b4bc-e5c2684a8f53.png)-dimensional vector
    is an eigenvector of *A*, and it corresponds to an eigenvalue λ. We write this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e06e0f16-b9c4-43ec-beec-7324ed13a11d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note**: The zero vector (0) cannot be an eigenvector of *A*, since *A*0 =
    0 = λ0 for all λ.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider again a matrix *A* that has an eigenvector **x** and a corresponding
    eigenvalue λ. Then, the following rules will apply:'
  prefs: []
  type: TYPE_NORMAL
- en: If we have a matrix *A* and it has been shifted from its current position to [![](img/d338804b-269b-4add-bc22-baccc681304c.png)],
    then it has the eigenvector **x** and the corresponding eigenvalue [![](img/7a3942f7-0d59-4f5c-aded-9de201f01fa7.png)],
    for all [![](img/866b4328-a0bd-4c19-9f44-71ded4428307.png)], so that [![](img/54b269b0-a6ac-49eb-840c-5dc60ea59105.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the matrix *A* is invertible, then **x** is also an eigenvector of the inverse
    of the matrix, [![](img/92fb6002-13bb-400b-a9ef-a95ab7c53723.png)], with the corresponding
    eigenvalue [![](img/1ad20846-524a-423b-8aab-e1b2c4d2bb04.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/034a9f84-edde-414f-bf6f-1be21c06138e.png)] for any [![](img/e22e2d20-7d9f-434d-acc9-157ed4de652c.png)].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know from earlier in the chapter that whenever we multiply a matrix and a
    vector, the direction of the vector is changed, but this is not the case with
    eigenvectors. They are in the same direction as *A*, and thus **x** remains unchanged.
    The eigenvalue, being a scalar value, tells us whether the eigenvector is being
    scaled, and if so, how much, as well as if the direction of the vector has changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very fascinating property the determinant has is that it is equivalent
    to the product of the eigenvalues of the matrix, and it is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f0dc73e-df42-4b6f-8403-e51d4e14fb74.png)'
  prefs: []
  type: TYPE_IMG
- en: But this isn't the only relation that the determinant has with eigenvalues.
    We can rewrite ![](img/e06e0f16-b9c4-43ec-beec-7324ed13a11d.png) in the form[![](img/54f8feb6-24e6-40ca-a150-a81609a01707.png)].
    And since this is equal to zero, this means it is a non-invertible matrix, and
    therefore its determinant too must be equal to zero. Using this, we can use the
    determinant to find the eigenvalues. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have [![](img/8b38bf6a-c7d7-4140-82e5-d4448a14148a.png)]. Then,
    its determinant is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8f73db5-d21e-4319-8523-232236f34680.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite this as the following quadratic equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b69adf5d-20df-41b5-a9a5-178ac85c62f0.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the quadratic equation will give us both the eigenvalues [![](img/cc10d4f1-77ba-4e34-b505-8163033da02c.png)].
    So, we plug our values into the quadratic formula and get our roots.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting property is that when we have triangular matrices such as
    the ones we found earlier in this chapter, their eigenvalues are the pivot values.
    So, if we want to find the determinant of a triangular matrix, then all we have
    to do is find the product of all the entries along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Trace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given an *n*×*n* matrix *A*, the sum of all the entries on the diagonal is
    called the **trace**. We write it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/364d9cba-7c3e-47b9-aa4c-63463575bf6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are four important properties of the trace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/f55f233e-e4eb-47ae-9e2f-90995eafbea4.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d913abcb-54c7-4539-9e57-732fab57c84c.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d225bee2-8f51-4b81-a891-eb1c01968470.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ed5a77f4-db75-4421-9830-c6c19b4ce1c6.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A very interesting property of the trace is that it is equal to the sum of
    its eigenvalues, so that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b6d57a8-b831-442e-8ed4-f3555b173032.png)'
  prefs: []
  type: TYPE_IMG
- en: Orthogonal matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of orthogonality arises frequently in linear algebra. It's really
    just a fancy word for perpendicularity, except it goes beyond two dimensions or
    a pair of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'But to get an understanding, let''s start with two column vectors [![](img/d048e20b-765d-4b6f-8f31-cd58d6b3d93c.png)].
    If they are orthogonal, then the following holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbc99477-3307-41b6-9603-98bb8dd7cf1c.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Orthogonal matrices are a special kind of matrix where the columns are pairwise
    orthonormal. What this means is that we have a matrix [![](img/7d7de76e-8356-43d4-88db-62dedaeb9347.png)]with the
    following property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e795f258-7b2f-461b-8bf9-052485c05a31.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we can deduce that [![](img/85df4b42-b4b9-4739-a142-15905a07a1d0.png)] (that
    is, the transpose of *Q* is also the inverse of *Q*).
  prefs: []
  type: TYPE_NORMAL
- en: As with other types of matrices, orthogonal matrices have some special properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, they preserve inner products, so that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fbd99a0-b27c-42e0-bb75-467eafd134ac.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the second property, which states that 2-norms are preserved
    for orthogonal matrices, which we see as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a378af5b-63c5-47c5-b9b5-220bd5fddea5.png)'
  prefs: []
  type: TYPE_IMG
- en: When multiplying by orthogonal matrices, you can think of it as a transformation
    that preserves length, but the vector may be rotated about the origin by some
    degree.
  prefs: []
  type: TYPE_NORMAL
- en: The most well-known orthogonal matrix that is also orthonormal is a special
    matrix we have dealt with a few times already. It is the identity matrix *I*,
    and since it represents a unit of length in the direction of axes, we generally
    refer to it as the standard basis.
  prefs: []
  type: TYPE_NORMAL
- en: Diagonalization and symmetric matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a matrix [![](img/d8bc4a80-34a1-4747-bf06-08764233ed32.png)] that
    has ![](img/551f57d1-22cf-4fd0-9ca6-c835a358402c.png) eigenvectors. We put these
    vectors into a matrix *X* that is invertible and multiply the two matrices. This
    gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52b2fc21-cdc0-4d05-87d8-3e9ec34e8f84.png)'
  prefs: []
  type: TYPE_IMG
- en: We know from ![](img/7f206c31-9575-42b3-ab95-0284067d480c.png) that when dealing
    with matrices, this becomes ![](img/e8874b7e-1006-4644-8838-c68a2d2d573f.png),
    where [![](img/5d66989d-b7b4-409d-a682-045e3f9b0998.png)] and each *x[i]* has
    a unique λ*[i]*. Therefore, [![](img/029ad3b2-e319-4292-b49c-9d0cb059f951.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to symmetric matrices. These are special matrices that, when transposed,
    are the same as the original, implying that [![](img/678972cc-c1f8-4a32-9573-ae1a26083511.png)] and
    for all [![](img/a0a6af0d-8910-4371-b3f3-31a7b0bcab0d.png)], [![](img/023b2d00-ee92-4a7e-8dcb-972b27e910d1.png)].
    This may seem rather trivial, but its implications are rather strong.
  prefs: []
  type: TYPE_NORMAL
- en: The spectral theorem states that if a matrix ![](img/d8bc4a80-34a1-4747-bf06-08764233ed32.png) is
    a symmetric matrix, then there exists an orthonormal basis for ![](img/879be859-9317-4137-9128-f37eb55dedad.png),
    which contains the eigenvectors of A.
  prefs: []
  type: TYPE_NORMAL
- en: This theorem is important to us because it allows us to factorize symmetric
    matrices. We call this **spectral decomposition** (also sometimes referred to
    as **Eigendecomposition**).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an orthogonal matrix *Q*, with the orthonormal basis of eigenvectors [![](img/86835d82-66f0-4cdd-a720-32f72739bc8b.png)] and
    [![](img/5d66989d-b7b4-409d-a682-045e3f9b0998.png)] being the matrix with corresponding
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: 'From earlier, we know that [![](img/0d19d219-6c28-4bb3-a122-c6b3469f13b3.png)] for
    all [![](img/fc112d41-bab9-4fb3-9ffa-c8c20461ad16.png)]; therefore, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b6c7941-bb3b-423d-b124-92890c550716.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note**: Λ comes after *Q* because it is a diagonal matrix, and the [![](img/c342b78a-6b41-47a3-9c17-eb7d0eb6726c.png)]s
    need to multiply the individual columns of *Q*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By multiplying both sides by *Q^T*, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e69ce71-fb08-43b4-8c6f-4b0d4a45f36b.png)'
  prefs: []
  type: TYPE_IMG
- en: Singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**) is widely used in linear algebra
    and is known for its strength, particularly arising from the fact that every matrix
    has an SVD. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a9d9100-4f2c-4d8e-a28d-2c79f1c394b7.png)'
  prefs: []
  type: TYPE_IMG
- en: For our purposes, let's suppose [![](img/c983f0bd-e4c4-4f1c-b5a9-48f3864ef227.png)],
    [![](img/c2a55e81-b23d-4b32-85c0-5a2549e917e9.png)], [![](img/b0f481fc-deb2-4819-aeed-1b8187780c09.png)],
    and [![](img/f09fa2f7-db96-4689-b1cf-f2f61f5195f1.png)], and that *U, V *are orthogonal
    matrices, whereas ∑ is a matrix that contains singular values (denoted by σ[i]) of
    *A* along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '*∑* in the preceding equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be5c5338-fbd4-4dec-a368-1dd17f879ae2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also write the SVD like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2638ca7-7dbf-4dc0-a887-ae2af1fb7b08.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *u[i]*, *v[i]* are the column vectors of *U, V*.
  prefs: []
  type: TYPE_NORMAL
- en: Cholesky decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I'm sure you've figured out by now, there is more than one way to factorize
    a matrix, and there are special methods for special matrices.
  prefs: []
  type: TYPE_NORMAL
- en: The Cholesky decomposition is square root-like and works only on symmetric positive
    definite matrices.
  prefs: []
  type: TYPE_NORMAL
- en: This works by factorizing *A* into the form *LL^T*. Here, *L*, as before, is
    a lower triangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do develop some intuition. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc5af252-82e4-4b19-9309-28bd423b9c64.png)'
  prefs: []
  type: TYPE_IMG
- en: However, here, *L* is called a **Cholesky factor**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the case where [![](img/95c6b814-1506-4f8e-bad3-9f7771f0fab6.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We know from the preceding matrix that [![](img/14988fc6-2973-4534-9dbb-d2f0fa493a60.png)];
    therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cb6fb08-04e8-4ebe-8d22-8bee05f7c6a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s multiply the upper and lower triangular matrices on the right, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bc01725-9a50-450c-98fa-acb8d95d0de7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Writing out *A* fully and equating it to our preceding matrix gives us the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/055c9913-37fa-491e-a439-7e0da68f09cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then compare, element-wise, the corresponding entries of *A* and *LL^T* and
    solve algebraically for [![](img/26f39821-1aaa-4fb9-a9f1-c44a475c2371.png), ]as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46bd6583-b5d0-4563-8b92-cf2fc765ffd0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can repeat this process for any symmetric positive definite matrix, and compute
    the *l[i,j]* values given *a[i,j]*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this, we conclude our chapter on linear algebra. So far, we have learned
    all the fundamental concepts of linear algebra, such as matrix multiplication
    and factorization, that will lead you on your way to gaining a deep understanding
    of how **deep neural networks** (**DNNs**) work and are designed, and what it
    is that makes them so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be learning about calculus and will combine it
    with the concepts learned earlier on in this chapter to understand vector calculus.
  prefs: []
  type: TYPE_NORMAL
