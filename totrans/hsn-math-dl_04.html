<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probability and Statistics</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover two of the most important areas of mathematics—probability and statistics. These are two terms that you've likely come across a number of times in your everyday life. People use it to justify just about everything that occurs or when they're trying to prove a point. Once you are done with this chapter, you will have a firm grasp of both of them and will understand how they both are related and how they differ. </p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Understanding the concepts in probability</li>
<li>Essential concepts in statistics</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the concepts in probability</h1>
                </header>
            
            <article>
                
<p class="mce-root">Probability theory is one of the most important fields of mathematics and is essential to the understanding and creation of deep neural networks. We will explore the specifics of this statement in the coming chapters. For now, however, we will focus our effort toward gaining an intricate understanding of this field.</p>
<p>We use probability theory to create an understanding of how likely it is that a certain event will occur. Generally speaking, probability theory is about understanding and dealing with uncertainty. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classical probability</h1>
                </header>
            
            <article>
                
<p>Let's suppose we have a random variable that maps the results of random experiments to the properties that interest us. The aforementioned random variable measures the likelihood (probability) of one or more sets of outcomes taking place. We call this the <strong>probability distribution</strong>. Consider probability distribution as the foundation of the concepts we will study in this chapter.</p>
<p>There are three ideas that are of great importance in probability theory—probability space, random variables, and probability distribution. Let's start by defining some of the more basic, yet important, concepts.</p>
<p>The sample space is the set of all the possible outcomes. We denote this with Ω. Suppose we have <em>n</em> likely outcomes—then, we have <sub><img class="fm-editor-equation" src="Images/03483c0d-c6a6-40c6-a11b-d549d6f04381.png" style="width:8.58em;height:1.17em;"/></sub>, where <em>w<sub>i</sub></em> is a possible outcome. The subset of the sample space (Ω) is called an <strong>event</strong>.</p>
<p>Probability has a lot to do with sets, so let's go through some of the notation so that we can get a better grasp of the concepts and examples to come.</p>
<p>Suppose we have two events, <em>A</em> and <em>B</em>, ⊆ Ω. We have the following axioms:</p>
<ul>
<li>The complement of <em>A</em> is <em>A<sup>C</sup></em>, so <sub><img class="fm-editor-equation" src="Images/14e34629-3242-4a47-bcb0-43522e1133e2.png" style="width:8.75em;height:1.50em;"/></sub>.</li>
<li>If either <em>A</em> or <em>B</em> occurs, this is written as <em>A </em>∪ <em>B</em> (read as <em>A</em><span> union <em>B</em></span>).</li>
<li>If both <em>A</em><span> and <em>B</em></span><span> occur, this is written as <em>A </em>∩ <em>B</em> (read as <em>A</em> intersect <em>B</em>).</span></li>
<li>If <em>A</em><span> and <em>B</em></span> are mutually exclusive (or disjoint), then we write <sub><img class="fm-editor-equation" src="Images/62e25ee7-b55a-47d4-96b3-62c477ea9b30.png" style="width:4.75em;height:1.08em;"/>.</sub></li>
<li><span>If the occurrence of <em>A</em> implies the occurrence of <em>B</em>, this is written as <em>A </em>⊆ <em>B</em> (so, <sub><img class="fm-editor-equation" src="Images/4b689d79-0135-4b8d-8948-f4bf83eccd1f.png" style="width:6.25em;height:1.33em;"/></sub>).</span></li>
</ul>
<p>Say we have an event, <em>A </em>∈ Ω, and <sub><img class="fm-editor-equation" src="Images/b873dd7c-ea17-454c-91a4-a22a9af938b2.png" style="width:9.25em;height:1.25em;"/></sub>. In this case, the probability of <em>A</em> occurring is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ed32339c-6ea3-4bb9-9942-dd74e74be8a8.png" style="width:20.83em;height:2.75em;"/></p>
<p>This is the number of times <em>A</em> can occur divided by the total number of possible outcomes in the sample space.</p>
<p>Let's go through a simple example of flipping a coin. Here, the sample space consists of all the possible outcomes of flipping the coin. Say we are dealing with two coin tosses instead of one and <em>h</em> means heads and <em>t</em> means tails. So, the sample space is Ω = {<em>hh</em>, <em>ht</em>, <em>th</em>, <em>tt</em>}. </p>
<p>All of the possible results of the experiment make up the event space, <sub><img class="fm-editor-equation" src="Images/7c6782f4-9b02-457e-85e9-4a5cb44dea0c.png" style="width:0.83em;height:0.92em;"/></sub>. On finishing the experiment, we observe whether the outcome, ω ∈ Ω, is in <em>A</em>.</p>
<p>Since, in each event, <sub><img class="fm-editor-equation" src="Images/8f4046d7-1601-4bff-942c-182af8351ed6.png" style="width:3.17em;height:1.08em;"/></sub>, we denote <em>P(A)</em> as the probability that the event will happen and we read <em>P(A)</em> as the probability of <em>A</em> occurring.</p>
<p>Continuing on from the previous axioms, <img class="fm-editor-equation" src="Images/f153cd8f-ff75-4d39-a2e6-2a5ac0c57fa4.png" style="width:0.92em;height:1.25em;"/> must satisfy the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/ce7395f4-137c-41dd-85ee-d8cd61f4a8c6.png" style="width:6.42em;height:1.33em;"/></sub> for all cases of <sub><img class="fm-editor-equation" src="Images/cd1f2c74-89cb-4f58-8bcf-52f941de60a6.png" style="width:2.92em;height:1.00em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/328ba247-2525-48ad-b8f8-cb892d6c9e77.png" style="width:4.67em;height:1.42em;"/>.</sub></li>
<li>If the events <em>A<sub>1</sub>, A<sub>2</sub>, …</em> are disjoint and countably additive—that is, <sub><img class="fm-editor-equation" src="Images/52252828-6a1c-482a-b70f-fdfd4acdcbff.png" style="width:5.83em;height:1.33em;"/></sub> for all cases of <em>i, j—</em>we then have <sub><img class="fm-editor-equation" src="Images/76bfbe42-185c-44c1-863e-8c6596d23690.png" style="width:10.17em;height:3.17em;"/></sub>.</li>
</ul>
<p>The triple <sub><img class="fm-editor-equation" src="Images/b1f980f2-fbe3-4210-baa0-d9054288c774.png" style="width:4.50em;height:1.42em;"/></sub> terms are known as the <strong>probability space</strong>.</p>
<p>As a rule of thumb, when <sub><img class="fm-editor-equation" src="Images/4cba0ddd-83cb-459c-b409-116f6e546a6a.png" style="width:3.92em;height:1.17em;"/></sub>, then event <em>A</em> happens almost surely and when <sub><img class="fm-editor-equation" src="Images/80a9ab87-9a8f-40ec-bb44-9da306b708d3.png" style="width:3.67em;height:1.08em;"/></sub>, then event <em>A</em> happens almost never.</p>
<p>Using the preceding axioms, we can derive the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/cce00473-b188-4254-8b79-23afa7b1d346.png" style="width:17.92em;height:1.42em;"/></p>
<p>So, <sub><img class="fm-editor-equation" src="Images/8db9764d-1664-4563-b77b-309fa3c815b9.png" style="width:3.92em;height:1.25em;"/></sub>.</p>
<p>Additionally, if we have two events, <em>A</em> and <em>B</em>, then we can deduce the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1fc2aa1f-e307-4543-85a9-2f5fb36d29c7.png" style="width:15.67em;height:1.17em;"/>.</p>
<p>Continuing on from the preceding axioms, <img class="fm-editor-equation" src="Images/8371eae4-d87f-4036-b7d4-8a1029cd54ec.png" style="width:0.92em;height:1.25em;"/> must satisfy the following:</p>
<p class="CDPAlignCenter CDPAlign"><sub><img class="fm-editor-equation" src="Images/26f1c3b3-f2c9-4272-8c72-8baf9b72ad71.png" style="width:6.50em;height:1.33em;"/></sub> for all <img class="fm-editor-equation" src="Images/17dae569-967d-4756-8fa8-ae42e139d788.png" style="width:2.92em;height:1.00em;"/></p>
<p>To find the probability of anything, we usually have to count things. Let's say we have a bucket filled with tennis balls and we pick a ball from the bucket <em>r</em> times; so, there are <em>n<sub>1</sub></em> possibilities for the first pick, <em>n<sub>2</sub></em> for the next pick, and so on. The total number of choices ends up being <em>n<sub>1</sub></em>×<em>n<sub>2</sub></em>×…×<em>n<sub>r</sub></em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sampling with or without replacement</h1>
                </header>
            
            <article>
                
<p>Let's now assume that there is a total of <em>n</em> items in the bucket and we must pick <em>r</em> of them. Then, let <em>R </em>= {1, 2,…, <em>r</em>} be the list of items picked and let <em>N </em>= {1, 2, …, <em>n</em>} be the total number of items. This can be written as a function, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/cf6492fe-1af9-473f-b963-143026c23a44.png" style="width:4.75em;height:1.17em;"/></p>
<p>Here, <em>f(i)</em> is the <em>i<sup>th</sup></em> item.</p>
<p>Sampling with replacement is when we pick an item at random and then put it back so that the item can be picked again. </p>
<p>However, sampling without replacement refers to when we choose an item and don't put it back, so we cannot pick it again. Let's see an example of both.</p>
<p>Say we need to open the door to our office and we have a bag containing <em>n</em> keys; they all look identical, so there's no way of differentiating between them. </p>
<p>The first time we try picking a key, we replace each one after trying it, and we manage to find the correct key on the <em>r<sup>th</sup></em> trial, implying we got it wrong <em>r-1</em> times. The probability is <span>then</span><span> </span><span>as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7a8733ed-c9da-4b6d-b46a-6117990688f7.png" style="width:22.67em;height:3.00em;"/></p>
<p>Now, we know that our earlier strategy wasn't the smartest, so this time we try it again but without replacement and eliminate each key that doesn't work. Now, the probability is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/78230e2f-b4fc-4135-9f22-1e5d89b51137.png" style="width:19.67em;height:3.08em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multinomial coefficient</h1>
                </header>
            
            <article>
                
<p>We know from the binomial theorem (which you likely learned in high school) that the following is true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f2970ab3-938a-4597-b2b0-bc5681efece0.png" style="width:26.42em;height:3.00em;"/></p>
<p>Then, the trinomial is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bbdd5078-bca0-45bf-a2d7-6e3271b51324.png" style="width:21.25em;height:3.50em;"/></p>
<p>Say we have <em>n</em> pieces of candy and there are blue- and red-colored candies. The different ways that we can pick the candies is defined as <sub><img class="fm-editor-equation" src="Images/205923c8-e800-4e64-9691-b115fc61afaa.png" style="width:2.17em;height:2.67em;"/></sub>, which is read as <em>n</em> choose <em>k</em>.</p>
<p>The multinomial coefficient is as follows:</p>
<p><img class="aligncenter size-full wp-image-1272 image-border" src="Images/c57dc5e8-171c-4061-861b-bbcb5d65d7ba.png" style="width:40.67em;height:3.08em;"/></p>
<p>This way, we spread <em>n</em> items over <em>k</em> positions, where the <em>i<sup>th</sup></em> position has <em>n<sub>i</sub></em> items.</p>
<p>For example, say we're playing cards and we have four players. A deck of cards has 52 cards and we deal 13 cards to each player. So, the number of possible ways that we can distribute the cards is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/04468143-efde-422b-a276-f0a185113692.png" style="width:17.25em;height:2.58em;"/></p>
<p>This is absolutely massive!</p>
<p>This is where Stirling's formula comes to the rescue. It allows us to approximate the answer.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stirling's formula</h1>
                </header>
            
            <article>
                
<p>For the sake of argument, let's say <sub><img class="fm-editor-equation" src="Images/969912b8-c618-420f-b5d6-f444094be46a.png" style="width:7.75em;height:1.33em;"/></sub>.</p>
<p>We know that the following is true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/120d5df1-3fae-42d9-8977-14c3afc08e6f.png" style="width:8.42em;height:3.17em;"/></p>
<p>However, we now claim the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/77a36317-3913-431f-91b3-3338b59705f8.png" style="width:19.75em;height:3.33em;"/></p>
<p>This can be illustrated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-835 image-border" src="Images/c3bd9297-5033-4be2-90bc-c81f869c67a3.png" style="width:30.83em;height:16.83em;"/></p>
<p>Now, by evaluating the integral, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5996c50a-6dc4-4fd5-9e31-7a79e24cd05a.png" style="width:24.67em;height:1.42em;"/></p>
<p>We now divide both sides by <sub><img class="fm-editor-equation" src="Images/65fcf4e9-46f2-44f1-86a3-6d79546a039a.png" style="width:3.25em;height:1.25em;"/></sub> and take the limit as <em>n</em>→∞. We observe that both sides tend to 1. So, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/02479d10-afcf-4829-b03c-59563dd2e27e.png" style="width:5.75em;height:2.75em;"/></p>
<p>Stirling's formula states that as <em>n</em><span>→</span><span>∞, the following is true</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ee3d7e9a-6bd8-41ff-91aa-81833dc1956c.png" style="width:16.33em;height:3.00em;"/></p>
<p>Furthermore, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4baec502-e54a-49c8-9fcf-f1c08b717816.png" style="width:9.25em;height:1.67em;"/></p>
<div class="packt_infobox">We will avoid looking into the proof for Sterling's formula, but if you're interested in learning more, then I highly recommend looking it up.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Independence</h1>
                </header>
            
            <article>
                
<p>Events are independent when they are not related to each other; that is, the outcome of one has no bearing on the outcome of another.</p>
<p>Suppose we have two independent events, <em>A</em><span> and <em>B</em></span>. Then, we can test the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/cb614af3-1c4a-4f04-8e52-a5318d6cbc33.png" style="width:10.08em;height:1.25em;"/></p>
<p>If this is not true, then the events are dependent. </p>
<p>Imagine you're at a casino and you're playing craps. You throw two dice—their outcomes are independent of each other. </p>
<p>An interesting property of independence is that if <em>A</em><span> and <em>B</em></span> are independent events, then so are <em>A</em> and <em>B<sup>C</sup></em>. </p>
<p>Let's take a look and see how this works:</p>
<p style="padding-left: 210px"><img class="aligncenter size-full wp-image-1192 image-border" src="Images/cd31d53f-a97a-42f1-87e6-63b3e6c6ec8f.png" style="width:14.00em;height:5.50em;"/></p>
<p>When we have multiple events, <em>A<sub>1</sub>, A<sub>2</sub>, …, A<sub>n</sub></em>, we call them mutually independent when <sub><img class="fm-editor-equation" src="Images/695b94c5-04a8-4ff6-a77f-7c414be5023f.png" style="width:21.17em;height:1.25em;"/></sub> for all cases of n ≥ 2. </p>
<p>Let's suppose we conduct two experiments in a lab; we model them independently as <sub><img class="fm-editor-equation" src="Images/eb5c64f6-516f-4b14-92ec-0b12c114fa20.png" style="width:9.25em;height:1.42em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/a6189c28-6064-4f67-8a8d-37a4d3fd4546.png" style="width:8.50em;height:1.33em;"/></sub> and the probabilities of each are <sub><img class="fm-editor-equation" src="Images/c902723a-4564-43f4-8f0b-bcf14067b398.png" style="width:5.08em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/2ee881bc-5e46-45ed-b2a6-73fdef48877f.png" style="width:4.67em;height:1.25em;"/></sub>, respectively. If the two are independent, then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6a48b451-9e46-4b58-bb9f-eebfda13b1bb.png" style="width:7.58em;height:1.42em;"/></p>
<p>This is for all cases of <em>i</em> and <em>j</em>, and our new sample space is Ω = Ω<sub>1 </sub>× Ω<sub>2</sub>. </p>
<p>Now, say <em>A</em><span> and <em>B</em> are events in the</span><span> </span><span>Ω</span><sub>1</sub><span> and Ω</span><sub>2</sub><span> experiments</span><span>, respectively. We can view them as subspaces of the new sample space, Ω, by calculating </span><em>A </em><span>× Ω</span><sub>2</sub><span> and</span> <em>B </em><span>× Ω</span><sub>1</sub><span>, which leads to the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e820dfc2-bb78-4337-9629-b11834f636e2.png" style="width:25.17em;height:2.75em;"/></p>
<p>Even though we normally define independence as different (unrelated) results in the same experiment, we can extend this to an arbitrary number of independent experiments as well.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Discrete distributions</h1>
                </header>
            
            <article>
                
<p>Discrete refers to when our sample space is countable, such as in the cases of coin tosses or rolling dice. </p>
<p>In discrete probability distributions, the sample space is <sub><img class="fm-editor-equation" src="Images/1f0f9681-eaf3-4830-9acc-f33efb7c2a14.png" style="width:10.83em;height:1.42em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/4c061723-d8be-4c59-b3c4-9eee1a942865.png" style="width:6.67em;height:1.42em;"/></sub>. </p>
<p>The following are the six different kinds of discrete distributions that we <span>often</span><span> </span><span>encounter in probability theory:</span></p>
<ul>
<li>Bernoulli distribution</li>
<li>Binomial distribution</li>
<li>Geometric distribution</li>
<li>Hypergeometric distribution</li>
<li>Poisson distribution</li>
</ul>
<p>Let's define them in order.</p>
<p>For the Bernoulli distribution, let's use the example of a coin toss, where our sample space is Ω = {<em>H</em>, <em>T</em>} (where <em>H</em> is heads and <em>T</em> is tails) and <em>p </em>∈ [0, 1] (that is, 0 ≤ <em>p </em>≤ 1). We denote the distribution as <em>B(1</em>, <em>p)</em>, such that the following applies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/38aeae95-1952-46f4-bc03-e0580c813b3e.png" style="width:4.92em;height:1.42em;"/> and <img class="fm-editor-equation" src="Images/727c7a77-7f02-4e99-be6b-30cdbcf245d1.png" style="width:6.75em;height:1.42em;"/></p>
<p>But now, let's suppose the coin is flipped <em>n</em> times, each with the aforementioned probability of <em>p</em> for the outcome being heads. Then, the binomial distribution, denoted as <em>B(n, p)</em>, states the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/58b3b7cd-d069-43c3-a738-26637fb1d32f.png" style="width:22.17em;height:1.25em;"/></p>
<p>Therefore, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/14d4f623-e99a-4871-beae-e4cd9e66d2a1.png" style="width:16.67em;height:2.92em;"/></p>
<p>Generally, the binomial distribution is written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e9042b37-85f2-43e0-897d-126e311be650.png" style="width:15.17em;height:2.92em;"/></p>
<p>The geometric distribution does not keep any memory of past events and so is memory-less. Suppose we flip our coin again; this distribution does not give us any indication as to when we can expect a heads result or how long it will take. So, we write the probability of getting heads after getting tails <em>k</em> times as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3a7870f1-c865-41d3-a626-2925a1c3ad34.png" style="width:6.67em;height:1.42em;"/></p>
<p>Let's say we have a bucket filled with balls of two colors—red and black (which we will denote as <em>r</em> and <em>b</em>, respectively). From the bucket, we have picked out <em>n</em> balls and we want to figure out the probability that <em>k</em> of the balls are black. For this, we use the hypergeometric distribution, which looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7c3f3385-715f-4863-ac0e-87f255c86da8.png" style="width:10.50em;height:3.50em;"/></p>
<p>The Poisson distribution is a bit different from the other distributions. It is used to model rare events that occur at a rate, λ. It is denoted as <em>P</em>(λ) and is written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2cd9c44f-25d0-47b5-b14a-3f04d224a443.png" style="width:5.67em;height:2.58em;"/></p>
<p class="CDPAlignLeft CDPAlign">This is true for all cases of <img class="fm-editor-equation" src="Images/7f86ec8f-cdfa-4954-9dbe-9ec39271d282.png" style="width:2.92em;height:1.08em;"/>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conditional probability</h1>
                </header>
            
            <article>
                
<p>Conditional probabilities are useful when the occurrence of one event leads to the occurrence of another. If we have two events, <em>A</em> and <em>B</em>, where <em>B</em> has occurred and we want to find the probability of <em>A</em> occurring, we write this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8a27e0f3-306e-40ba-8f96-20cf46c4eaf7.png" style="width:9.17em;height:2.50em;"/></p>
<p class="CDPAlignLeft CDPAlign">Here, <sub><img class="fm-editor-equation" src="Images/a5c8df71-3eed-4f7a-9531-b04e228d1302.png" style="width:3.92em;height:1.17em;"/></sub>.</p>
<p>However, if the two events, <em>A</em><span> and <em>B</em></span><span>, are independent, then we have the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9542fb47-8681-4348-a6fc-a3018fa81ad9.png" style="width:19.50em;height:2.67em;"/></p>
<p>Additionally, if <sub><img class="fm-editor-equation" src="Images/f199d6b1-6442-4d5c-be27-70b5a03cb636.png" style="width:6.58em;height:1.08em;"/></sub>, then it is said that <em>B</em> attracts <em>A</em>. However, if <em>A</em> attracts <em>B<sup>C</sup></em>, then it repels <em>B</em>. </p>
<div class="mce-root packt_infobox">The attraction between <em>A</em><span> and <em>B</em></span> is bidirectional; that is, <em>A</em> can only attract <em>B</em> if <em>B</em> also attracts <em>A</em>.</div>
<p>The following are some of the axioms of conditional probability:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/75903c9e-d098-472c-a75c-94e7bd5a4b13.png" style="width:22.08em;height:1.42em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/1c9aace0-180f-4c10-8a56-bbdccf62233b.png" style="width:22.42em;height:1.42em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/b4681247-54dd-4a50-989e-42c0301cbdb6.png" style="width:14.58em;height:2.92em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/ddc98d3f-4690-4e64-b416-86d99c44a22b.png" style="width:4.17em;height:1.42em;"/></sub> is a probability function that works only for subsets of <em>B</em>. </li>
<li><sub><img class="fm-editor-equation" src="Images/566cc844-450c-4d5c-85d9-4a7493f152d7.png" style="width:10.42em;height:2.83em;"/>.</sub></li>
<li>If <sub><img class="fm-editor-equation" src="Images/0fac1325-7227-45d1-8259-7993ffdcd7e7.png" style="width:3.17em;height:1.08em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/78aa0939-baff-4621-be22-fda466c2b9b3.png" style="width:11.75em;height:2.67em;"/>.</sub></li>
</ul>
<p>The following equation is known as <strong>Bayes' rule</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/eba7347c-2eba-4534-8e0f-5c4b6ceb6c11.png" style="width:11.33em;height:2.58em;"/></p>
<p>This can also be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/08f22faa-5df1-4592-812d-927234cb80ee.png" style="width:12.33em;height:1.33em;"/></p>
<p class="CDPAlignLeft CDPAlign">Here, we have the following: </p>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/d40d8ca3-29b7-4eab-8bdb-fc91e3ac74ba.png" style="width:2.58em;height:1.42em;"/></sub> is called the prior.</li>
<li class="CDPAlignLeft CDPAlign"><span><sub><img style="color: #333333;width:3.92em;height:1.25em;" class="fm-editor-equation" src="Images/60a49fa8-3b1b-4c4f-b49c-5b275bb67e8a.png"/></sub></span> <span>is the posterior.</span></li>
</ul>
<ul>
<li class="CDPAlignLeft CDPAlign"><span><sub><img style="color: #333333;width:3.92em;height:1.25em;" class="fm-editor-equation" src="Images/b95e6129-fc56-45ca-adb4-e7f1708596ea.png"/></sub></span> <span>is the likelihood.</span></li>
<li class="CDPAlignLeft CDPAlign"><span><sub><img style="color: #333333;width:2.08em;height:1.17em;" class="fm-editor-equation" src="Images/e2485bc2-e169-43f7-ade0-1918bc0c93e3.png"/></sub></span> <span>acts as a normalizing constant.</span></li>
</ul>
<div class="mce-root packt_infobox"><span>The </span><img style="font-size: 14.6667px;width:1.17em;height:0.92em;" src="Images/25435825-223f-4374-8b78-d0306c1b6f66.png"/><span> symbol is read as </span><strong><span>proportional t</span><span>o</span></strong><span>.</span></div>
<p>Often, we end up having to deal with complex events, and to effectively navigate them, we need to decompose them into simpler events. </p>
<p>This leads us to the concept of partitions. A partition is defined as a collection of events that together makes up the sample space, such that, for all cases of <em>B<sub>i</sub></em>, <sub><img class="fm-editor-equation" src="Images/198293d4-0671-48fe-97ce-74737f46784f.png" style="width:6.50em;height:2.50em;"/></sub>.</p>
<p>In the coin flipping example, the sample space is partitioned into two possible events—heads and tails. </p>
<p>If <em>A</em> is an event and <em>B<sub>i</sub></em> is a partition of Ω, then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1eabf79c-aa99-4b5b-9605-939e8729ae05.png" style="width:19.42em;height:3.00em;"/></p>
<p>We can also rewrite Bayes' formula with partitions so that we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0736c821-dc23-4e6a-be02-12f01db2d752.png" style="width:15.00em;height:3.00em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/87bd5d8b-31bc-4dbe-a2fb-9d312e0d0478.png" style="width:4.58em;height:1.25em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Random variables</h1>
                </header>
            
            <article>
                
<p>Random variables are variables that have a probability distribution attached to them that determines the values each one can have. We view the random variable as a function, <em>X</em>: Ω → Ω<sub>x</sub>, where <sub><img class="fm-editor-equation" src="Images/b8558ada-ef3c-49aa-a5a6-5e57caf2362c.png" style="width:6.58em;height:1.25em;"/></sub>. The range of the<span> </span><em>X</em><span> function</span><span> is denoted by </span><sub><img class="fm-editor-equation" src="Images/4701c9e8-5093-4914-92fb-be7bab0db877.png" style="width:10.83em;height:1.25em;"/></sub><span>. </span></p>
<p>A discrete random variable is a random variable that can take on finite or countably infinite values. </p>
<p>Suppose we have <em>S</em> ∈ Ω<sub>x</sub>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0bd78772-a702-46ed-b3cd-c3f07ff03847.png" style="width:15.00em;height:1.17em;"/></p>
<p>This is the probability that <em>S</em> is the set containing the result.</p>
<p>In the case of random variables, we look at the probability of a random variable having a certain value instead of the probability of obtaining a certain event. </p>
<p>If our sample space is countable, then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/055c9c4f-a4ca-47f4-b353-8ead0bf17f68.png" style="width:12.25em;height:2.83em;"/></p>
<p>Suppose we have a die and <em>X</em> is the result after a roll. Then, our sample space for <em>X</em> is Ω<sub>x</sub>={1, 2, 3, 4, 5, 6}. Assuming this die is fair (unbiased), then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3ad1fa48-6b05-4bea-a9c6-b93cb0caecaa.png" style="width:6.92em;height:2.58em;"/></p>
<p>When we have a finite number of possible outcomes and each outcome has an equivalent probability assigned to it, such that each outcome is just as likely as any other, we call this a discrete uniform distribution. </p>
<p>Let's say <em>X∼B(n, p)</em>. Then, the probability that the<span> value that</span><span> </span><em>X</em><span> takes on is</span> <em>r</em><span> is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1e2dedc4-e425-4955-a88b-81f971c1b46a.png" style="width:14.42em;height:2.92em;"/></p>
<div class="packt_infobox">Sometimes, in probability literature, <sub><img class="fm-editor-equation" src="Images/c8735e61-805e-49cd-a060-f6218d412033.png" style="width:5.83em;height:1.67em;"/></sub> is written as <sub><img class="fm-editor-equation" src="Images/245fad37-79a6-4e5f-b78d-218e0a473baa.png" style="width:3.75em;height:1.67em;"/></sub>.</div>
<p>A lot of the time, we may need to find the expected (average) value of a random variable. We do this using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/004396e7-e662-4bdc-92e4-a63490b6f256.png" style="width:9.75em;height:2.75em;"/></p>
<p>We can also write the preceding equation in the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5e8c359d-bf77-4e4c-a64d-c5f36f01ef2b.png" style="width:11.92em;height:2.83em;"/></p>
<div class="packt_tip">The preceding two equations only work when our sample space is discrete (countable).</div>
<p>The following are some of the axioms for <sub><img class="fm-editor-equation" src="Images/5cf75ec8-3673-457e-a8fc-0c62f46a0936.png" style="width:3.17em;height:1.67em;"/></sub>:</p>
<ul>
<li>If <sub><img class="fm-editor-equation" src="Images/d4b3ea81-83a1-4553-915c-8bce6eb92ffc.png" style="width:2.58em;height:0.92em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/fbc2a53b-b69d-4dae-9d19-85ce2fdb67a4.png" style="width:4.50em;height:1.33em;"/>.</sub></li>
<li>If <sub><img class="fm-editor-equation" src="Images/03296dd8-2f4e-4e0c-9a7a-3b33676f2e30.png" style="width:2.58em;height:0.92em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/6bb14c02-8a1d-4a8f-9d4f-49ab4dbed99b.png" style="width:4.25em;height:1.25em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/4b0493af-740c-4f82-a7bb-dc7204f67705.png" style="width:6.17em;height:1.25em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/6a2553b3-dbc8-4dd2-b18d-67ad22964f86.png" style="width:12.50em;height:1.33em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/42579c09-73b2-4116-be94-97624bd1790f.png" style="width:17.08em;height:3.42em;"/></sub>, given that α and β are constants and <em>X<sub>i</sub></em> is not independent.</li>
<li><sub><img class="fm-editor-equation" src="Images/ff887879-65ec-4106-8a62-c0fe59bb0152.png" style="width:10.00em;height:3.17em;"/></sub> , which holds for when <em>X<sub>i</sub></em> is independent.</li>
<li><sub><img class="fm-editor-equation" src="Images/6c6a0c03-1d5f-4c5a-a632-214648189d54.png" style="width:2.67em;height:1.42em;"/></sub> minimizes <sub><img class="fm-editor-equation" src="Images/aa32ed28-6eb5-4b9f-91e2-be23c9285d96.png" style="width:5.67em;height:1.50em;"/></sub> over <em>c.</em></li>
</ul>
<p>Suppose we have <em>n</em> random variables. Then, their expected value is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d94a83f2-fdf3-406e-ba58-e943201395e0.png" style="width:10.83em;height:3.33em;"/></p>
<p>Now that we have a good understanding of the expectation of real-valued random variables, it is time to move on to defining two important concepts—variance and standard variables. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Variance </h1>
                </header>
            
            <article>
                
<p>We define the variance of <em>X</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4a3bb4a1-a56d-4d53-94d2-46cc66eba155.png" style="width:12.83em;height:1.58em;"/></p>
<p>The standard deviation of <em>X</em> is the square root of the variance:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2f27bf3c-b2f4-47bb-88b2-5bc2bc53b470.png" style="width:4.33em;height:2.17em;"/></p>
<p>We can think of this as how spread out or close values are from the expected (mean) value. If they are highly dispersed, then they have a high variance, but if they are grouped together, then they have a low variance. </p>
<p>Here are some properties for variance that are important to remember:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/19af9c52-e0b1-49f8-8bbe-93d03d0d730a.png" style="width:5.17em;height:1.25em;"/>.</sub></li>
<li>If <sub><img class="fm-editor-equation" src="Images/3566903a-644f-4cec-a028-6a0123f750b7.png" style="width:5.50em;height:1.33em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/677ce2ec-3e15-4719-9d56-392e0d0633fe.png" style="width:8.00em;height:1.25em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/ea36761b-42fe-4164-a20a-66ab703da52c.png" style="width:11.75em;height:1.42em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/dd3ff0ee-dd34-4695-96b5-d085c3fe207f.png" style="width:14.00em;height:1.67em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/531e3907-2cb6-4aa6-a9bc-1a40cc31f921.png" style="width:13.42em;height:3.42em;"/></sub>, given that all the <em>X<sub>i</sub></em> values are independent.</li>
</ul>
<p>Let's suppose that we now have <img class="fm-editor-equation" src="Images/516e70ce-60bc-44c4-acbd-94bc71b47ef0.png" style="width:0.92em;height:0.92em;"/> discrete random variables. Then, they are independent if we take the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0b1bf323-a745-4bf0-8b7a-4bb63342b3e0.png" style="width:39.17em;height:1.42em;"/></p>
<p>Now, let our <em>n</em> random variables be independent and <strong>identically distributed</strong> (<strong>iid</strong>). We now have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/551a1a1b-6731-4dcd-91db-7a56262636b6.png" style="width:13.50em;height:3.33em;"/></p>
<p>This concept is very important, especially in statistics. It implies that if we want to reduce the variance in the results of our experiment, then we can repeat the experiment a number of times and the sample average will have a small variance.</p>
<p>For example, let's imagine two pieces of rope that have unknown lengths—<em>a</em> and <em>b</em>, respectively. Because the objects are ropes—and, therefore, are non-rigid—we can measure the lengths of the ropes, but our measurements may not be accurate. Let <em>A</em> be the measured value of rope <em>a</em> and <em>B</em> be the measured value of rope <em>b</em> so that we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4c083980-6c63-493c-ae42-32bef1eb65f7.png" style="width:13.08em;height:3.00em;"/></p>
<p>We can increase the accuracy of our measurements by measuring <em>X = A + B</em> and <em>Y = A – B</em>. Now, we can estimate <em>a</em><span> and <em>b</em></span> using the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/23416a25-98bc-4170-93bb-241446ffd02b.png" style="width:5.83em;height:4.92em;"/></p>
<p>Now, <sub><img class="fm-editor-equation" src="Images/76a6252d-ef6a-4d28-8d0f-e986ba6b3f66.png" style="width:4.00em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/822a2a6a-8bdd-489c-aff6-70f4ad2be5d0.png" style="width:4.33em;height:2.08em;"/></sub>, which are both unbiased. Additionally, we can see that the variance has decreased in our measurement using the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/67d1ce11-28dc-4962-8acc-4efbcc8a300d.png" style="width:18.42em;height:2.42em;"/></p>
<p>From this, we can clearly see that measuring the ropes together instead of separately has improved our accuracy significantly.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multiple random variables</h1>
                </header>
            
            <article>
                
<p>A lot of the time, we will end up dealing with more than one random variable. When we do have two or more variables, we can inspect the linear relationships between the random variables. We call this the covariance.</p>
<p>If we have two random variables, <em>X</em> and <em>Y</em>, then the covariance is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bc6d132f-3893-4563-8932-b44b1582dc89.png" style="width:19.17em;height:1.33em;"/></p>
<p>The following are some of the axioms for the covariance:</p>
<ul>
<li>If <em>c</em> is a constant, then <sub><img class="fm-editor-equation" src="Images/ef4a1b71-62cd-4671-b335-ea90679559be.png" style="width:6.17em;height:1.25em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/63f053f8-cc51-48de-9b9e-bd84f5356c62.png" style="width:13.75em;height:1.42em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/23ce2b25-59d8-41f3-9233-d3ba3ec1d647.png" style="width:11.08em;height:1.33em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/a2d96a5d-f76c-4a42-9361-1bc747db4af9.png" style="width:15.33em;height:1.33em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/2f649eb8-737f-4b05-9496-774cc17f0354.png" style="width:10.42em;height:1.42em;"/>.</sub></li>
<li><sub><img class="fm-editor-equation" src="Images/eedd1cad-fc30-4c14-a2f6-b90a20e6f9aa.png" style="width:21.83em;height:1.33em;"/>.</sub></li>
</ul>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/6d88bb9d-91f6-47cd-8259-573532a1a3fa.png" style="width:6.42em;height:1.25em;"/></sub>, given that <em>X</em><span> and <em>Y</em></span> are independent (but it does not imply that the two are independent).</li>
<li><sub><img class="fm-editor-equation" src="Images/26014aa6-4f93-4a08-8d52-30cec07ee128.png" style="width:23.33em;height:1.42em;"/></sub></li>
</ul>
<p>However, sometimes, the covariance doesn't give us the full picture of the correlation between two variables. This could be a result of the variance of <em>X</em><span> and <em>Y</em></span>. For this reason, we normalize the covariance as follows and get the correlation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a5140026-c84e-4c14-a7e2-21ea1ed1b64f.png" style="width:14.25em;height:3.00em;"/></p>
<p>The resulting value will always lie in the<span> </span><span>[-1, 1]</span><span> interval.</span></p>
<p>This leads us to the concept of conditional distributions, where we have two random variables, <em>X</em><span> and <em>Y</em></span>, that are not independent and we have the joint distribution, <sub><img class="fm-editor-equation" src="Images/64a8a6ed-302d-4a67-9093-745dd54328c1.png" style="width:8.08em;height:1.33em;"/></sub>, from which we can get the probabilities, <sub><img class="fm-editor-equation" src="Images/21ffaa8e-d944-4ee7-9e1f-8e509f0afad5.png" style="width:4.67em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/39351bdf-7e86-469a-b743-fe6340495765.png" style="width:4.50em;height:1.33em;"/></sub>. Then, our distribution is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/45c4ff9a-381d-4ebe-b8ee-bf9423196043.png" style="width:17.75em;height:3.00em;"/></p>
<p>From this definition, we can find our conditional distribution of <em>X</em> given <em>Y</em> to be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/796c562d-b757-40ed-81d2-46738adac170.png" style="width:20.17em;height:3.17em;"/></p>
<p>We may also want to find the conditional expectation of <em>X</em><span> given <em>Y</em></span>, which is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/30f06fb6-c3c3-4bad-bc82-21ac9e43f0be.png" style="width:21.50em;height:3.00em;"/></p>
<p>Now, if our random variables are independent, then, <sub><img class="fm-editor-equation" src="Images/b1be2d62-c472-41a1-9bba-ebcd5eb08712.png" style="width:8.50em;height:1.42em;"/></sub>, which we know to be true because <em>Y</em> has no effect on <em>X</em>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Continuous random variables</h1>
                </header>
            
            <article>
                
<p>So far, we've looked at discrete outcomes in the sample space where we could find the probability of a certain outcome. But now, in the continuous space, we will find the probability of our outcome being in a particular interval or range.</p>
<p>Now, to find the distribution of <em>X</em>, we need to define a function, <em>f</em>, so that the probability of <em>X</em> must lie in the<span> </span><span>interval</span><span> </span><sub><img class="fm-editor-equation" src="Images/5794cda3-9f1e-4c28-a0c2-0528c9e9d219.png" style="width:4.92em;height:1.33em;"/></sub></p>
<p>Formally, a random variable, <img class="fm-editor-equation" src="Images/a0f56366-bbe1-473a-932e-2f0dd2433fce.png" style="width:5.25em;height:1.00em;"/>, is continuous if, in a function, <sub><img class="fm-editor-equation" src="Images/5587b9ca-262a-4565-b0ae-050800428fdf.png" style="width:5.92em;height:1.25em;"/></sub> so that we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/397c6a87-7c95-423f-be55-c89b2e07e011.png" style="width:12.83em;height:2.92em;"/></p>
<p><span>We call the function, <em>f</em>, the <strong>probability density function</strong> (<strong>PDF</strong>) and it must satisfy the following:</span></p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/fa2aa7ca-7ccd-4e31-8e3d-6c1773dd1492.png" style="width:2.67em;height:1.25em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/47a2c693-41bf-417a-b87e-07e887e2cffb.png" style="width:5.75em;height:2.50em;"/></sub></li>
</ul>
<p>There is another distribution function that is important for us to know, known as the <strong>cumulative distribution function</strong>. If we have a random variable, <em>X</em>, that could be continuous or discrete, then, <sub><img class="fm-editor-equation" src="Images/be3f65db-4bd9-4d17-862f-9f685064a4db.png" style="width:7.92em;height:1.25em;"/></sub>, where <em>F(x)</em> is increasing so that x→∞ and <em>F(x)→1</em>.</p>
<p>When dealing with continuous random variables such as the following, w<span>e know that</span><span> </span><em>F</em><span> is both continuous and differentiable:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/14e36303-5461-4190-97df-0161bfe46b72.png" style="width:9.42em;height:2.83em;"/></p>
<p>So, when <em>F</em> is differentiable, then <em>F'(x) = f(x)</em>.</p>
<p>An important fact to note is that <sub><img class="fm-editor-equation" src="Images/8110eefa-b4e6-46ab-bce8-eb7a78d14921.png" style="width:14.42em;height:1.42em;"/></sub>.</p>
<p>This leads us to the concept of uniform distribution, which, in general, has the following PDF:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/98c36358-0492-420d-bf4b-2cd22644eec1.png" style="width:6.17em;height:2.42em;"/></p>
<p>So, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/da6dfe92-c875-4bc5-a552-9f7065528072.png" style="width:13.17em;height:2.75em;"/></p>
<p>This is the case for <img class="fm-editor-equation" src="Images/36e6ab23-5de9-4cc8-8bfd-363b3bdada5b.png" style="width:4.83em;height:1.17em;"/>.</p>
<p><span>We write </span><sub><img class="fm-editor-equation" src="Images/f11dfa86-15fb-494d-ab79-46a81e7325d8.png" style="width:5.50em;height:1.33em;"/></sub> if <em>X</em> follows a uniform distribution on the<span> </span><span>[<em>a</em>, <em>b</em>]</span><span> interval.</span></p>
<p>Now, let's suppose our random variable is an exponential random variable and has the added<span> </span><span>λ</span><span> parameter. Then, its PDF is </span><sub><img class="fm-editor-equation" src="Images/eb50042a-970a-498a-abf9-35db9bbd73d0.png" style="width:6.25em;height:1.50em;"/></sub><span> and </span><sub><img class="fm-editor-equation" src="Images/73d7aaf7-fe65-4ee3-b75f-daef267de5ec.png" style="width:7.75em;height:1.50em;"/></sub><span> for all </span><img style="font-size: 1em;width:2.58em;height:1.00em;" class="fm-editor-equation" src="Images/b8ba8fd6-7fa9-4c42-b141-35047f72bf49.png"/><span>.</span></p>
<p>We write this as <sub><img class="fm-editor-equation" src="Images/133a6020-8cb8-41aa-b915-cf44b7e8866d.png" style="width:4.58em;height:1.33em;"/></sub>, so we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/67f53b45-88bc-4033-8617-c9e6c8c96206.png" style="width:21.83em;height:3.33em;"/></p>
<p>It is also very important to note that the exponential random variable, such as the geometric random variable, is memory-less; <span><span>that is,</span></span> the past gives us no information about the future.</p>
<p>Just as in the discrete case, we can define the expectation and variance in the case of continuous random variables. </p>
<p>The expectation for a continuous random variable is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/491ff6f1-6c48-43af-a140-5859c1eb5a37.png" style="width:10.25em;height:2.83em;"/></p>
<p style="text-align: left" class="CDPAlignCenter CDPAlign">But, say <sub><img class="fm-editor-equation" src="Images/6865b917-c2dc-4e80-8bd6-a1820c209abb.png" style="width:5.75em;height:1.67em;"/></sub>. Then, we have the following:</p>
<p><img class="aligncenter size-full wp-image-1337 image-border" src="Images/a6f9e16e-9bcd-43c3-9a6f-48892f6b07ea.png" style="width:17.42em;height:5.75em;"/></p>
<p>In the case of continuous random variables, the variance is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/97c0330b-a838-4516-b36f-abe13945464b.png" style="width:23.33em;height:1.58em;"/></p>
<p>This gives us the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bfa14771-c603-485a-9123-8c93635c78c0.png" style="width:22.33em;height:3.25em;"/></p>
<p>Now, for example, let's take <sub><img class="fm-editor-equation" src="Images/a0003821-f605-4b9a-86aa-ec551887e0a8.png" style="width:5.17em;height:1.25em;"/></sub>. We can find the expected value of <em>X</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/40507b44-b1d1-482b-aee3-b0b8767d27a6.png" style="width:14.25em;height:2.92em;"/></p>
<p><span>Its variance can be found as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2d1dabf8-5441-4def-bf16-179da04a8de5.png" style="width:23.83em;height:3.00em;"/></p>
<p>Now, we have a good handle of expectation and variance in continuous distribution. Let's get acquainted with two additional terms that apply to PDFs—<strong>mode</strong> and <strong>median</strong>.</p>
<p>The mode in a PDF is the value that appears the most; however, it is also possible for the mode to appear more than once. For example, in a uniform distribution, all the <em>x</em> values can be considered as the mode.</p>
<p>Say we have a PDF, <em>f(x)</em>. Then, we denote the mode as <img class="fm-editor-equation" src="Images/f5569622-b675-4c09-a442-6807f4feff3f.png" style="width:0.83em;height:1.25em;"/>, so <sub><img class="fm-editor-equation" src="Images/faba697b-8583-4a38-bca9-c80725a5f714.png" style="width:5.42em;height:1.25em;"/></sub> for all cases of <em>x</em>.</p>
<p>We define the median as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c72d59ef-4d49-4a8b-a1e1-2fee7c9f92e8.png" style="width:14.92em;height:3.00em;"/></p>
<p>However, in a discrete case, the median is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6647fff3-007c-4f71-8dff-c5de1cb8e908.png" style="width:17.17em;height:2.50em;"/></p>
<p>Many times, in probability, we take the sample mean instead of the mean. Suppose we have a distribution that contains all the values that <em>X</em> can take. From it, we randomly sample <em>n</em> values and average it so that we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/07b25103-74cf-473a-b795-f3ade978fcd8.png" style="width:6.08em;height:2.92em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Joint distributions</h1>
                </header>
            
            <article>
                
<p>So far, we have dealt with and learned about distributions that relate to one random variable; but now, say we have two random variables, <em>X</em> and <em>Y</em>. Then, their joint distribution is defined as <sub><img class="fm-editor-equation" src="Images/02e5e753-145e-49c1-8dbf-50b3ec96ab65.png" style="width:12.83em;height:1.33em;"/></sub>, such that <sub><img class="fm-editor-equation" src="Images/642b9988-79c5-41f2-972b-ff7c776ab2fa.png" style="width:7.25em;height:1.50em;"/></sub>.</p>
<p>In joint distributions, we usually tend to know the distribution of a set of variables, but sometimes, we may only want to know the distribution of a subset. We call this the marginal distribution. We define the marginal distribution of <em>X</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/264ab983-aaab-4166-93d7-52d4163cb71e.png" style="width:32.92em;height:2.08em;"/></p>
<p>Let's say our <em>n</em> continuous random variables in <em>A</em> are jointly distributed and have the<span> </span><em>f</em><span> PDF</span><span>. Then, we have the following:</span></p>
<p>                         <img src="Images/3cc0e9d1-769a-44ae-8206-d6e5dee70797.png" style="width:34.50em;height:3.42em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/fc5aaac3-bedc-4b47-8aeb-bc804637c9c5.png" style="width:7.92em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/10954a31-2c2c-40ff-9c51-cb0292cd9623.png" style="width:16.08em;height:2.83em;"/></sub>.</p>
<p>Let's revisit an earlier example, where we have two variables,<span> <em>X</em></span><span> and <em>Y</em></span><span>. If the variables are continuous, then their joint distribution is </span><sub><img class="fm-editor-equation" src="Images/856a0270-c44e-456c-a788-c3b218c6e345.png" style="width:24.17em;height:2.75em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/85602f52-e8c6-4f55-9900-929cc9caaf6d.png" style="width:10.17em;height:2.67em;"/></sub>.</p>
<div class="packt_infobox">If the random variables are jointly continuous, then they are individually continuous.</div>
<p>Now, let's suppose our <em>n</em> continuous random variables are independent. Then, <sub><img src="Images/087a0680-c7e3-4aea-9668-e49bbed64247.png" style="width:40.00em;height:1.42em;"/></sub> for all cases of <sub><img class="fm-editor-equation" src="Images/5c1c0e0e-066a-4871-98b2-40086b5e44b9.png" style="width:4.08em;height:1.17em;"/></sub>.</p>
<p>If <sub><img class="fm-editor-equation" src="Images/cee4f394-934d-462a-91d1-cbf325a20bde.png" style="width:1.67em;height:1.17em;"/></sub> is a cumulative distribution function and <sub><img class="fm-editor-equation" src="Images/9ac7ae41-3c20-487a-82c7-1fc0756d0498.png" style="width:2.00em;height:1.58em;"/></sub> is the PDF, then <sub><img class="fm-editor-equation" src="Images/28678354-b5ca-44e1-b2ba-e4989e8db5fe.png" style="width:25.92em;height:1.50em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/a6c3c3bd-f4a2-471f-bdce-d4fbd7320f90.png" style="width:25.08em;height:1.50em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">More probability distributions</h1>
                </header>
            
            <article>
                
<p>Earlier on in this chapter, we introduced several different types of distributions in the <em>Random variables</em> section. I am sure, at some point, that you thought to yourself <q>there must also be probability distributions for continuous random variables</q>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Normal distribution</h1>
                </header>
            
            <article>
                
<p>The following distribution is quite an important one and is known as the <strong>normal distribution</strong>. It looks like as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-834 image-border" src="Images/7b730080-ba84-4519-af83-737c14403c67.png" style="width:29.42em;height:11.58em;"/></p>
<p>The normal distribution, written as <sub><img class="fm-editor-equation" src="Images/c9515d39-e75d-48b0-b3b5-f7add3cd8a7d.png" style="width:5.25em;height:1.83em;"/></sub>, has the <sub><img class="fm-editor-equation" src="Images/3266f88f-500d-4d4f-aa1c-1a3910ae3819.png" style="width:10.33em;height:3.08em;"/></sub> <span>PDF</span><span> for all cases of </span><img style="font-size: 1em;width:6.58em;height:0.92em;" class="fm-editor-equation" src="Images/d8240eee-37e2-4651-9f72-0e708235db7e.png"/><span>.</span></p>
<p>Additionally, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b7f6d31a-c9ab-4ec9-9941-9b85ad4a14b8.png" style="width:13.58em;height:3.25em;"/></p>
<p>When the normal distribution has <sub><img class="fm-editor-equation" src="Images/26958e7e-a86d-41d1-bc93-decf7b167b56.png" style="width:2.67em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/43298051-a412-46a6-ac32-9b6b2169ca49.png" style="width:2.92em;height:1.08em;"/></sub>, it is called the <strong>standard normal</strong> and we denote <sub><img class="fm-editor-equation" src="Images/d92fbb28-181f-41d3-a34a-c47270d846a8.png" style="width:2.33em;height:1.42em;"/></sub> as its PDF and <sub><img class="fm-editor-equation" src="Images/7333c5a0-2068-4f06-81aa-0a60962c816e.png" style="width:2.50em;height:1.42em;"/></sub> as its cumulative distribution function.</p>
<p>The normal distribution has some rather interesting properties, which are as follows:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/58567752-2609-40f1-aa56-c2f78746af06.png" style="width:4.17em;height:1.25em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/96063efd-866b-46f5-8e96-cbab53db66cf.png" style="width:6.17em;height:1.50em;"/></sub></li>
</ul>
<p>Assuming we have two independent random variables, <sub><img class="fm-editor-equation" src="Images/aab98e9d-b4ee-4ace-827e-72cac46491fa.png" style="width:6.50em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/c4daaf9c-b4e1-419e-8d2d-1bf68cc8aca8.png" style="width:6.33em;height:1.33em;"/></sub>, then we have the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/125b6691-8eee-4e7b-8495-4dec478096f5.png" style="width:14.75em;height:1.50em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/d84498f6-bac9-4f46-b41e-ca2fe6788dcb.png" style="width:9.17em;height:1.50em;"/></sub>, where <em>a</em> is a constant</li>
</ul>
<p>The reason this probability distribution is so important is because of its relation to the central limit theorem, which states that if we have a large number of independent and identically distributed random variables, then their distribution is approximately the same as the normal distribution. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multivariate normal distribution</h1>
                </header>
            
            <article>
                
<p>The normal distribution can also be extended for multiple random variables, which gives us the multivariate normal distribution. </p>
<p>Say we have <em>n</em> iid random variables sampled from <em>N(0, 1)</em>. Then, we define their joint density function as follows:</p>
<p>                                          <img src="Images/74c05128-6313-43d8-9335-32d7b6374910.png" style="width:19.25em;height:14.33em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/e18d1a4d-fa10-4d61-93bb-b602d41fff07.png" style="width:7.83em;height:1.33em;"/></sub>.</p>
<p>Let's take this a step further. Now, let's suppose we have an invertible <em>n×n</em> matrix, <em>A</em>, and are interested in <sub><img class="fm-editor-equation" src="Images/fc7a3657-df9f-400c-90cc-c2ea98ebc7bc.png" style="width:6.17em;height:1.25em;"/></sub>. Then, <sub><img class="fm-editor-equation" src="Images/0cf3cf14-81c9-4eb5-85bb-d5e2fc04ed9e.png" style="width:8.25em;height:1.50em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/a93c60cf-3e11-41c0-bdf7-bc1bea83cfe9.png" style="width:11.50em;height:2.42em;"/></sub>. So, we have the following:</p>
<p style="padding-left: 90px"><img src="Images/6a4c4e92-f63b-42c1-8469-549356da815e.png" style="width:34.58em;height:12.08em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/ded03a25-2c8b-46e4-9343-f85f126ef864.png" style="width:4.33em;height:1.08em;"/></sub>. Therefore, <em>Z</em> is the multivariate normal and is expressed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/75bda44e-7c2b-4a8f-98b9-a655be6e1a58.png" style="width:10.08em;height:3.83em;"/></p>
<p>You're likely wondering what this new matrix, <img class="fm-editor-equation" src="Images/362a6521-6110-43e2-b9e5-401be8c5ce07.png" style="width:1.08em;height:1.25em;"/>, represents. It is the covariance matrix where the <em>i<sup>th</sup></em> and <em>j<sup>th</sup></em> entry is <sub><img class="fm-editor-equation" src="Images/738b1a8c-7eed-4664-ade4-8d27f27cfb9c.png" style="width:18.17em;height:1.42em;"/></sub>.</p>
<p>In the case where the covariances are 0, which implies the variables are independent, then <sub><img class="fm-editor-equation" src="Images/99cb769c-ab24-46e6-aecd-f1fede83389e.png" style="width:9.75em;height:1.42em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bivariate normal distribution</h1>
                </header>
            
            <article>
                
<p>When <em>n = 2</em> in the multivariate normal distribution, this is a special case known as the <strong>bivariate normal</strong>. Its covariance matrix is written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e12e12ad-1541-44fd-8e9b-d5bcb164b869.png" style="width:10.92em;height:3.08em;"/></p>
<p>The inverse of this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/29b2ce8d-f9cd-4553-b232-901decd81a02.png" style="width:18.33em;height:3.25em;"/></p>
<p>In this case, the correlation between the two variables becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0ef43a72-df2e-4d22-b047-fc3ebb7186b1.png" style="width:22.75em;height:3.00em;"/></p>
<p>For the sake of simplicity, we will assume the mean is 0, so the joint PDF of the bivariate normal is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/64738400-590a-4a68-b0cd-5eabede9aa71.png" style="width:24.33em;height:3.75em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gamma distribution</h1>
                </header>
            
            <article>
                
<p>Gamma distribution is a widely used distribution to model positive continuous variables with skewed distributions. </p>
<p>Gamma distribution is denoted by <sub><img class="fm-editor-equation" src="Images/517d5ba1-6a78-43f1-9c50-259a3c6c63ef.png" style="width:2.75em;height:1.08em;"/></sub> and has the following PDF:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/08cab1d3-8add-449d-9b48-94277d4304e1.png" style="width:10.50em;height:3.25em;"/></p>
<p>With that, we conclude our section on probability. We will now start exploring statistics.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Essential concepts in statistics</h1>
                </header>
            
            <article>
                
<p class="mce-root">While probability allows us to measure and calculate the odds of events or outcomes occurring, statistics allows us to make judgments and decisions given data generated by some unknown probability model. We use the data to learn the properties of the underlying probabilistic model. We call this process parametric inference. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Estimation</h1>
                </header>
            
            <article>
                
<p>In estimation, our objective is <span>given <em>n</em></span><span> iid samples with the same distribution as</span> <em>X</em><span> (the probability model). If the PDF and <strong>probability mass function</strong> (<strong>PMF</strong>) is <sub><img class="fm-editor-equation" src="Images/ae2cef0a-5beb-48e1-a5f1-b357c299efdd.png" style="width:3.67em;height:1.25em;"/></sub>, we need to find θ. </span></p>
<p>Formally, we define a statistic as an estimate of θ.</p>
<p>A statistic is a function, <em>T</em>, of the data, <sub><img class="fm-editor-equation" src="Images/a159163d-ddda-4dcc-9526-8d8033b60adb.png" style="width:10.17em;height:1.42em;"/></sub>, so that our estimate is <sub><img class="fm-editor-equation" src="Images/8a3e68cf-e6cb-43c1-806f-38b119a91fec.png" style="width:4.42em;height:1.58em;"/></sub>. Therefore, <em>T(x)</em> is the sampling distribution of the statistic and an estimator of θ.</p>
<p>Going forward, <em>X</em> will denote a random variable and <em>x</em> will denote an observed value. </p>
<p>Let's say we have <sub><img class="fm-editor-equation" src="Images/daf3f330-a78a-4f17-b227-d2a95cf5ccc6.png" style="width:7.58em;height:1.17em;"/></sub>, which are iid <sub><img class="fm-editor-equation" src="Images/f518720d-8500-4abd-9a4a-a38158e29e02.png" style="width:3.58em;height:1.33em;"/></sub>. Then, a possible estimation for μ is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/62bf0b56-be84-44d0-b66f-3f13039485e8.png" style="width:8.33em;height:2.50em;"/></p>
<p>However, our estimate for a particular observed sample, <img class="fm-editor-equation" src="Images/56509ab4-3bbd-48e5-8829-05aa83bd2c87.png" style="width:0.92em;height:0.92em;"/>, is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ea2ae08f-b460-4fcc-9f51-008a2e5460df.png" style="width:7.58em;height:2.42em;"/></p>
<p>A method we use to determine whether or not our estimator is good is the bias. The bias is defined as the difference between the true value and the expected value and is written as <sub><img class="fm-editor-equation" src="Images/bf858bc5-6b59-4958-8d56-71d76ada0425.png" style="width:9.00em;height:1.50em;"/></sub>. The estimator is unbiased if <sub><img class="fm-editor-equation" src="Images/c8594a74-3ed4-4b1f-a404-5a020d51b76f.png" style="width:4.50em;height:1.50em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p>The<strong> mean squared error</strong> (<strong>MSE</strong>) is a measure of how good an estimator is and is a better indicator of this than the bias. We write it as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4c9bb100-93e9-4e4f-bd5b-1bf00e13898e.png" style="width:10.08em;height:1.67em;"/></p>
<p>However, sometimes, we use the root MSE, which is the square root of the MSE.</p>
<p>We can also express the MSE in terms of bias and variance, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0c5ec3d2-a3ff-4d65-ba22-cedc58161600.png" style="width:14.25em;height:1.50em;"/></p>
<p>Sometimes, when we are trying to get a low MSE, it is in our best interest to have a biased estimator with low variance. We call this the <strong>bias-variance trade-off</strong>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sufficiency</h1>
                </header>
            
            <article>
                
<p>A lot of the time, the purpose of conducting our experiments is to find the value of θ and to find the bigger picture. A sufficient statistic is one that gives us all the information we want about θ.</p>
<p>Lucky for us, the factorization theorem gives us the ability to find sufficient statistics. It states that <em>T</em> is sufficient for θ if we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/79a178a9-82b0-4df0-ac68-a81db320635f.png" style="width:11.83em;height:1.25em;"/></p>
<p>Here, <em>g</em> and <em>h</em> are arbitrary functions.</p>
<p>In general, if <em>T</em> is a sufficient statistic, then it does not lose any information about θ and the best statistic is the one that gives us the maximal reduction. We call this the minimal sufficient statistic; in its definition, <em>T(X)</em> is minimal if—and only if—it is a function of every other statistic. So, if <em>T'(X)</em> is sufficient, then <em>T'(X) = T'(Y) ⇒ T(X) = T(Y)</em>.</p>
<p>Suppose <em>T = T(X)</em> is a statistic that satisfies <sub><img class="fm-editor-equation" src="Images/5917f1c3-e8b3-416f-bd2f-1aae73357098.png" style="width:4.00em;height:2.67em;"/></sub>, which does not depend on <img class="fm-editor-equation" src="Images/0d2b7a06-a42c-42d8-b01a-d6739fd0df24.png" style="width:0.67em;height:1.25em;"/> if (and only if) <sub><img class="fm-editor-equation" src="Images/ee76df88-3853-4587-bd53-cd079be5662e.png" style="width:5.75em;height:1.25em;"/></sub>. Then, <em>T</em> is minimally sufficient for θ.</p>
<p>Following this, let's say we have <sub><img class="fm-editor-equation" src="Images/96b42b02-2996-49c5-8dda-1246af243e19.png" style="width:7.58em;height:1.17em;"/></sub>, which are iid <sub><img class="fm-editor-equation" src="Images/b4cd8933-cd49-43b9-b1b7-bf12021c9ea3.png" style="width:3.83em;height:1.33em;"/></sub>. Then, we can deduce the following:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1201 image-border" src="Images/133755f0-1aac-4542-9864-7e5039f719d2.png" style="width:24.50em;height:6.25em;"/></p>
<p>This is a constant function that tells us that <sub><img class="fm-editor-equation" src="Images/783f2304-eb07-484a-98c9-24de5561fd53.png" style="width:7.00em;height:2.50em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/948fb818-3420-4e69-8358-c836dd80365d.png" style="width:6.33em;height:2.33em;"/></sub>. Therefore, <sub><img class="fm-editor-equation" src="Images/a02a5ed6-6067-4dcf-b6ae-14ffa5e4068f.png" style="width:11.75em;height:3.25em;"/></sub> is minimally sufficient. </p>
<p>The advantage of minimally sufficient statistics is that they give us the ability to store our experiments' results in the most efficient way and we can use them to improve our estimator.</p>
<p>This leads us to the Rao-Blackwell theorem, which states that if <img class="fm-editor-equation" src="Images/db6ddaf1-2663-4333-a87d-637de9c065a9.png" style="width:0.75em;height:0.92em;"/> is a sufficient statistic for <em>θ</em>, and if <sub><img class="fm-editor-equation" src="Images/a108d990-0094-4925-a68f-a37bdcf669a8.png" style="width:0.58em;height:1.08em;"/></sub> is an estimator of θ—where for all θ, <sub><img class="fm-editor-equation" src="Images/3bdfcaa3-a418-430c-a307-40296fe59e89.png" style="width:3.92em;height:1.17em;"/></sub>. Let <sub><img class="fm-editor-equation" src="Images/35a206e6-4978-406d-bb04-bbc1490146e6.png" style="width:11.83em;height:1.17em;"/></sub>—then, for all cases of θ, we have <sub><img class="fm-editor-equation" src="Images/7311f9c9-5d24-416b-9796-a6cc5ca7f449.png" style="width:8.08em;height:1.17em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Likelihood</h1>
                </header>
            
            <article>
                
<p>Generally, in practice, when we want to determine whether or not our estimator is good, we usually use the <strong>maximum likelihood estimator</strong> (<strong>MLE</strong>). </p>
<p>Given <em>n</em> random variables (with <sub><img class="fm-editor-equation" src="Images/cd38b80b-487a-484f-ada8-1fddf247ef63.png" style="width:4.08em;height:1.25em;"/></sub> being the joint PDF), then if <em>X</em> = <em>x</em>, the likelihood of θ is defined as <sub><img class="fm-editor-equation" src="Images/c9f407f0-3452-40cf-a527-7a4cb7f6a840.png" style="width:8.92em;height:1.33em;"/></sub>. Therefore, the MLE of θ is an estimate of the value of θ that maximizes <sub><img class="fm-editor-equation" src="Images/0ee0e32d-136d-4f00-925e-b1171a35aaed.png" style="width:3.33em;height:1.42em;"/></sub>.</p>
<p>In practice, however, we maximize the log-likelihood instead of simply the likelihood. </p>
<p>Going back to the example of having <em>n</em><span> iid random variables with the <sub><img class="fm-editor-equation" src="Images/39ab6250-e262-4cb7-851d-ba7c61c7dd5c.png" style="width:4.33em;height:1.33em;"/></sub></span><span> </span><span>PDF,</span><span> the likelihood and log-likelihood are as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4199dc56-b225-46c1-9ddd-c97bbb09fe0f.png" style="width:14.92em;height:6.92em;"/></p>
<p>Suppose our <em>n</em> variables are Bernoulli <em>(p)</em>. Then, <sub><img class="fm-editor-equation" src="Images/3625b2c6-7b7b-43fb-a470-118a60af38eb.png" style="width:21.50em;height:1.67em;"/></sub>. Therefore, <sub><img class="fm-editor-equation" src="Images/ad2a5be1-766b-4e0a-8685-ad6e2e08df1d.png" style="width:7.17em;height:1.67em;"/></sub> when <sub><img class="fm-editor-equation" src="Images/f3e522ff-9961-433c-a58a-a09743fa40d4.png" style="width:3.00em;height:1.67em;"/></sub> is equal to 0 and is an unbiased MLE.</p>
<p>By now, you're probably wondering what exactly the MLE has to do with sufficiency. If <em>T</em> is sufficient for θ, then its likelihood is <sub><img class="fm-editor-equation" src="Images/36758fea-8e6d-4b75-b8d2-3ac7cc1c1477.png" style="width:6.50em;height:1.25em;"/></sub> and to maximize our estimate, we have to maximize <em>g</em>. Therefore, the MLE is a function of the sufficient statistic—voila!</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Confidence intervals</h1>
                </header>
            
            <article>
                
<p>The confidence interval gives us the ability to determine the probability that certain intervals contain <span>θ</span>. We formally define it as follows.</p>
<p>A <sub><img class="fm-editor-equation" src="Images/681ddc42-f012-4a68-b1af-69eebfb41dcf.png" style="width:9.08em;height:1.33em;"/></sub> confidence interval for θ is a random interval <sub><img class="fm-editor-equation" src="Images/35662868-9e85-4d24-91bb-ce235a60c5a1.png" style="width:6.33em;height:1.25em;"/></sub>, such that <sub><img class="fm-editor-equation" src="Images/9be406c6-7923-4112-9901-14fcd25dc2f4.png" style="width:11.67em;height:1.25em;"/></sub>, regardless of the true value of θ.</p>
<p>Suppose that we calculate <sub><img class="fm-editor-equation" src="Images/b7b6d010-e660-4ac0-beeb-ba22b45b0df1.png" style="width:6.33em;height:1.25em;"/></sub> for a number of samples, <em>x</em>. Then, 100γ% of them will cover our true value of θ.</p>
<p>Say we have <sub><img class="fm-editor-equation" src="Images/336d3b2a-848c-433a-91f2-72e188d88ea2.png" style="width:6.50em;height:1.00em;"/></sub>, which are iid <sub><img class="fm-editor-equation" src="Images/95b73743-00c8-4468-8e6c-08ee0d06a391.png" style="width:3.25em;height:1.25em;"/></sub>, and we want to find a 95% confidence interval for θ. We know that <sub><img class="fm-editor-equation" src="Images/afe448f0-f0cb-44cf-b5f6-e5527e674e45.png" style="width:6.42em;height:2.50em;"/></sub> so that <sub><img class="fm-editor-equation" src="Images/3816503b-9384-4341-a0dd-152fd78e9b48.png" style="width:8.75em;height:1.25em;"/></sub>. Then, we choose <em>z<sub>1</sub>, z<sub>2</sub></em>, such that <sub><img class="fm-editor-equation" src="Images/635c0c2d-2cf1-4047-b029-5ef99a37eed4.png" style="width:9.33em;height:1.25em;"/></sub>, where <em>Φ</em> is the normal distribution. So, <img class="fm-editor-equation" src="Images/14036c42-8a8e-4d7f-9328-3adc2a948ff4.png" style="width:15.00em;height:2.50em;"/>, from which we get the following confidence interval:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fe4b4fd6-51a1-4fb2-9a17-c446d83809e0.png" style="width:9.17em;height:2.50em;"/></p>
<p>Here is a three-step approach that is commonly used to find confidence intervals:</p>
<ol>
<li>Find <sub><img class="fm-editor-equation" src="Images/35755383-e8d4-4592-be2c-6b4a1408934e.png" style="width:3.50em;height:1.25em;"/></sub>, such that the <sub><img class="fm-editor-equation" src="Images/8a4308b7-adaf-4688-9fd2-cc383d29121f.png" style="width:9.67em;height:1.50em;"/></sub> of <sub><img class="fm-editor-equation" src="Images/dd9a5964-7bb7-4ad3-a478-09e17182bd07.png" style="width:3.50em;height:1.25em;"/></sub> isn't dependent on θ. We call this a <strong>pivot</strong>.</li>
<li>Above <sub><img class="fm-editor-equation" src="Images/e296bbe4-70ea-4fc5-be23-06bd2f89bdd4.png" style="width:10.08em;height:1.42em;"/></sub>, write the probability statement in the <sub><img class="fm-editor-equation" src="Images/ebc4503c-bac3-42a3-8e73-3b90d065f43d.png" style="width:12.83em;height:1.33em;"/></sub> form.</li>
<li>Rearrange the inequalities to find the interval.</li>
</ol>
<p>Usually, <em>c<sub>1</sub></em> and<em> c<sub>2</sub></em> are percentage points from a known distribution; for example, for a 95% confidence interval, we would have the<span> </span><span>2.5% and 97.5%</span><span> points.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bayesian estimation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout this section on statistics, we have dealt with what is known as the frequentist approach. Now, however, we will look at what is known as the Bayesian approach, where we treat θ as a random variable, we tend to have prior knowledge about the distribution, and, after collecting some additional data, we find the posterior distribution. </p>
<p>Formally, we define the prior distribution as a probability distribution of θ before collecting any additional data; we denote this as π(θ). The posterior distribution is the probability distribution of θ dependent on the outcome of our conducted experiment; we denote this as π(θ|<strong>x</strong>).</p>
<p>The relationship between the prior and posterior distributions are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e762a4cc-1ac5-4755-8824-d3ab137a5ea9.png" style="width:10.67em;height:2.58em;"/></p>
<p>Generally, we avoid calculating <sub><img class="fm-editor-equation" src="Images/9395c0c8-e981-4be2-8458-e38143bb8c98.png" style="width:2.92em;height:1.33em;"/></sub> and we only observe the relationship:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5d66e796-d136-45ce-a578-fbf4afaadb36.png" style="width:11.50em;height:1.33em;"/></p>
<p>We can read this as <sub><img class="fm-editor-equation" src="Images/74de7aaa-f594-410d-bbc6-9b2767b8a3b3.png" style="width:15.75em;height:1.33em;"/></sub>.</p>
<p>After conducting our experiments and coming up with the posterior, we need to determine an estimator, but to find the best estimator, we need a loss function, such as quadratic loss or absolute error loss, to see how far off the true value of θ is from our estimated value of a parameter.</p>
<p>Let's suppose the parameter we are estimating is <em>b</em>. Then, the Bayes estimator, <sub><img class="fm-editor-equation" src="Images/6353d454-a2aa-4bfa-9a35-713cfa57d6ec.png" style="width:0.83em;height:1.58em;"/></sub>, minimizes the expected posterior loss, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c064708e-a605-48fe-b9ff-759a32df49c5.png" style="width:12.17em;height:2.58em;"/></p>
<p>If we choose our loss function to be a quadratic loss, then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5b834307-84b1-417d-8e55-b4e1f71c5741.png" style="width:13.08em;height:2.67em;"/></p>
<p>However, if we choose an absolute error loss, then we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/49fb6cf7-2784-4171-abc0-e23b383eab1b.png" style="width:12.25em;height:2.58em;"/></p>
<p>Since the posterior distribution is our true distribution, we know that by integrating over it, our result is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4ee0ddd1-21fd-472d-afbd-237329a5c0cf.png" style="width:7.83em;height:2.58em;"/></p>
<p><span>If you're wondering how these two schools of statistics compare, think of frequentist versus Bayesian as absolute versus relative, respectively. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hypothesis testing</h1>
                </header>
            
            <article>
                
<p>In statistics, we usually have to test out hypotheses and most likely, we will compare two different hypotheses—the null hypothesis and the alternative hypothesis. The null hypothesis tells us that our experiment contains statistical significance; that is, that no relationship is observed between the variables. The alternative hypothesis tells us that there is a relationship between the variables. </p>
<p>In general, we start with the assumption that the null hypothesis is true, and to reject this, we need to find evidence through our experiments that contradict it. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Simple hypotheses</h1>
                </header>
            
            <article>
                
<p>A simple hypothesis <em>H</em> is one in which the parameters of the distribution are completely specified, otherwise it is called a composite hypothesis. </p>
<p>When testing the null hypothesis (<em>H<sub>0</sub></em>) against the alternative hypothesis (<em>H<sub>1</sub></em>), we use our test to divide <img class="fm-editor-equation" src="Images/82552f43-137b-4265-be68-844864a4cff1.png" style="width:1.50em;height:1.00em;"/> into two regions <em>C</em> and <sub><img class="fm-editor-equation" src="Images/abe39872-54f7-4e0b-a2aa-ca18ce448641.png" style="width:1.08em;height:1.50em;"/></sub>. If <sub><img class="fm-editor-equation" src="Images/4c2ec251-ab81-483f-b8d0-07a756bfdcaa.png" style="width:2.67em;height:0.92em;"/></sub>, then we reject the null hypothesis, but if <sub><img class="fm-editor-equation" src="Images/e919c215-066e-4b12-82e8-3b8c450dc3e8.png" style="width:2.67em;height:1.08em;"/></sub> then we do not reject the null hypothesis. We call <em>C</em> the critical region.</p>
<p>When we perform tests, we hope to arrive at the correct conclusion, but we could make either of the following two errors:</p>
<ul>
<li><strong>Error 1</strong>: rejecting <em>H<sub>0</sub></em> when <em>H<sub>0</sub></em> is true</li>
<li><strong>Error 2</strong>: not rejecting <em>H<sub>0</sub></em> when <em>H<sub>0</sub></em> is false</li>
</ul>
<p>If <em>H<sub>0</sub></em> and <em>H<sub>1</sub></em> are both simple hypotheses, then we have the following:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1211 image-border" src="Images/11f0c189-4377-408a-bc9d-4b168590bb5a.png" style="width:21.42em;height:2.83em;"/></p>
<p>Here, α is the size of our test and 1-β is the power of the test to find <em>H<sub>1</sub></em>.</p>
<p>If we have a simple hypothesis, <img class="fm-editor-equation" src="Images/0c4f64dc-79ba-4632-b984-4d312944fd9e.png" style="width:5.08em;height:1.08em;"/>, then we also want to find its likelihood given <em>x</em>. We do this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7bc50d6b-9756-4804-ace3-7df858919bd4.png" style="width:10.83em;height:1.25em;"/></p>
<p class="mce-root">We can also find the likelihood ratio of <em>H<sub>0</sub></em> and <em>H<sub>1 </sub></em>given <em>x</em><strong> </strong>as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d198ad53-94d2-45e0-bf88-7c431c81f7ea.png" style="width:10.00em;height:2.58em;"/></p>
<p>A likelihood ratio test is where, given <em>k</em>, the critical region is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c22a7637-2624-4853-9740-48b821e72e96.png" style="width:11.08em;height:1.17em;"/></p>
<p>Let's go through an example and develop a bit of intuition about this. Say we have <sub><img class="fm-editor-equation" src="Images/cd11ba88-9c52-4908-96c9-9949498a0168.png" style="width:7.00em;height:1.08em;"/></sub>, which are iid <sub><img class="fm-editor-equation" src="Images/63ca4aac-27dc-4692-8c12-66f6e44b807f.png" style="width:4.08em;height:1.42em;"/></sub>, and <sub><img class="fm-editor-equation" src="Images/60b63337-d86d-4251-943c-46a0f8759010.png" style="width:1.25em;height:1.50em;"/></sub> is a known quantity. Now, we want to find out what the best test size for our null hypothesis,<span> </span><sub><img class="fm-editor-equation" src="Images/c9937c97-b93b-4cfa-a006-c6e329820742.png" style="width:5.58em;height:1.17em;"/></sub><span>, is versus the alternative hypothesis, </span><sub><img class="fm-editor-equation" src="Images/ccc6f54f-ce88-4734-8f97-aea1ab769306.png" style="width:5.92em;height:1.25em;"/></sub><span>. Let's assume that </span><img style="font-size: 1em;width:1.25em;height:1.00em;" class="fm-editor-equation" src="Images/f6ca7aa5-a778-45b8-88c7-fafde0072f14.png"/><span> and </span><img style="font-size: 1em;width:1.25em;height:1.00em;" class="fm-editor-equation" src="Images/53716cc1-40ca-4a63-ae09-698a9405961b.png"/><span> are known to us as well, such that </span><img style="font-size: 1em;width:3.83em;height:1.00em;" class="fm-editor-equation" src="Images/1375019f-9b75-43f2-9e1b-014403651c7c.png"/><span>. Therefore, we have the following:</span></p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1269 image-border" src="Images/0a7735fe-30ac-464f-a9e9-f01a1d32cfd8.png" style="width:24.00em;height:9.67em;"/></p>
<p>We know that this function is increasing, so <sub><img class="fm-editor-equation" src="Images/b13e47c4-7554-40c9-ba8e-fe34d8fd0648.png" style="width:3.33em;height:1.17em;"/></sub> for any case of <em>k</em>, which tells us that for some arbitrary values of <em>c</em>, <img class="fm-editor-equation" src="Images/02ddd37a-721a-4352-8334-3075514fb4a9.png" style="width:2.58em;height:0.83em;"/>.</p>
<p>We choose our value of <img class="fm-editor-equation" src="Images/dc98ae99-744e-4ee4-9ecd-6b2848a6840c.png" style="width:0.67em;height:0.83em;"/> so that <sub><img class="fm-editor-equation" src="Images/3eb7536f-5213-40e6-b75c-550aa4a81aaf.png" style="width:8.67em;height:1.33em;"/></sub>, and if <img class="fm-editor-equation" src="Images/dd522a17-5624-43da-8a0d-3b76c397c0cc.png" style="width:2.58em;height:0.83em;"/>, we reject the null hypothesis. </p>
<p>Under the null hypothesis, <sub><img class="fm-editor-equation" src="Images/fe127bd0-6b64-476d-946e-1193deeca1b4.png" style="width:7.25em;height:3.00em;"/></sub>; so, <sub><img class="fm-editor-equation" src="Images/e668e5a7-9178-45ce-9290-5dcb3353df9d.png" style="width:10.67em;height:2.25em;"/></sub>. Now, since <img class="fm-editor-equation" src="Images/660eb9eb-f940-450e-9762-256b5cea13b3.png" style="width:3.58em;height:1.42em;"/>, the test size rejects the null hypothesis if we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/57823923-6850-4e07-89e5-dac1414e4e70.png" style="width:11.08em;height:2.92em;"/></p>
<p>This is known as the <em>z</em>-test and we use it to test a hypothesis, while the z-score tells us how many standard deviations away from the mean our data point is.</p>
<p class="mce-root">Here, the likelihood ratio rejects the null hypothesis if <em>z &gt; k</em>. The test size is <sub><img class="fm-editor-equation" src="Images/6e3a330d-7de5-4e1a-92f8-9b2c1a2fccea.png" style="width:11.83em;height:1.08em;"/></sub>, which decreases as <em>k</em> increases. The value of <em>z</em> is in the rejection region if <sub><img class="fm-editor-equation" src="Images/b8b93e75-b736-48ca-be20-4f0e33113171.png" style="width:9.33em;height:1.08em;"/></sub>. </p>
<p>In the preceding equation, <em>p<sup>*</sup></em>, is known as the <em>p</em>-value of the data, <em>x</em><span>; </span><span>it is,</span><em> </em><span>in other words, the probability of observing data (evidence) against the null hypothesis.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Composite hypothesis</h1>
                </header>
            
            <article>
                
<p>Now, if we have a composite hypothesis, such as <sub><img class="fm-editor-equation" src="Images/8b35701f-e0c6-4d19-bcd2-6e80c38f4738.png" style="width:4.33em;height:1.08em;"/></sub>, the error probabilities are not singular-valued. </p>
<p>So, we define the power function, which is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c198677c-2b7f-407c-877a-fff0277edb72.png" style="width:19.17em;height:1.33em;"/>. </p>
<p>Ideally, we would like <em>W</em>(<em>θ</em>) to be small on the null hypothesis and large on the alternative hypothesis.</p>
<p>The size of the test is <sub><img class="fm-editor-equation" src="Images/0629f5ba-0683-4dfe-b103-05452e24bfab.png" style="width:6.42em;height:2.08em;"/></sub>, which is not an ideal size. Given <sub><img class="fm-editor-equation" src="Images/9ab50740-e94c-4137-99c0-5b6b5a09709e.png" style="width:3.17em;height:1.17em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/34421f0b-e3c2-4fb8-abcf-d1e494f84515.png" style="width:14.92em;height:1.33em;"/></sub>.</p>
<p>Previously, we saw that the best size of the test of <em>H<sub>0</sub></em> versus <em>H<sub>1</sub></em> is given by the following critical region:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4ca10f44-d854-46ef-9802-96a41a72af55.png" style="width:13.00em;height:2.67em;"/></p>
<p>This depends on <sub><img class="fm-editor-equation" src="Images/f3b75cd5-4cdc-468d-9483-e7302993d0f5.png" style="width:5.17em;height:1.00em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/3e6bfced-2e42-4c0c-944e-022e7ffe5854.png" style="width:3.83em;height:1.00em;"/></sub>, but not the value of <sub><img class="fm-editor-equation" src="Images/c7abc6a3-460c-4012-b9f1-3656ed0db4cc.png" style="width:1.25em;height:1.00em;"/></sub>. </p>
<p>We call a test, which is specified by <em>C</em>, the uniformly most powerful size, <img class="fm-editor-equation" src="Images/87a6c215-5a0b-4030-9883-a9d67f2def82.png" style="width:0.67em;height:0.58em;"/>, of the <sub><img class="fm-editor-equation" src="Images/54025d5d-5f05-4d8d-987e-47649a29d910.png" style="width:5.50em;height:1.17em;"/></sub> <span>test </span><span>versus </span><sub><img class="fm-editor-equation" src="Images/2e70d22d-6b4f-43ad-be0a-e15606eaf717.png" style="width:5.92em;height:1.25em;"/></sub><span>, but only if the following is true:</span></p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/9e9c392e-ebc9-42f2-a4a2-9fb47bc5c02a.png" style="width:6.67em;height:2.17em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/7ffe3029-4743-4483-bd17-9e26f41e4102.png" style="width:6.67em;height:1.25em;"/></sub> for all cases of <sub><img class="fm-editor-equation" src="Images/ce01ff2b-1efa-4023-bb0e-796d2194679d.png" style="width:2.92em;height:1.08em;"/></sub> if <sub><img class="fm-editor-equation" src="Images/fa862a2f-e488-44b1-88a8-9d5de3f95f75.png" style="width:3.00em;height:1.00em;"/></sub></li>
</ul>
<p>Now, as before, we want to find the likelihood of a composite hypothesis, <sub><img class="fm-editor-equation" src="Images/2363d223-0bc5-4c72-80fc-48469a2e188e.png" style="width:4.25em;height:0.92em;"/></sub>, given some data, <em>x</em>. We do so as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f08d4b63-4866-4f79-9055-ee74b3358819.png" style="width:10.00em;height:2.17em;"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The multivariate normal theory</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we have<span> </span><span>so far</span><span> dealt with random variables or a vector of iid random variables. Now, let's suppose we have a random vector, </span><sub><img class="fm-editor-equation" src="Images/34165953-146a-4a7e-bdbc-cb54b61fffaa.png" style="width:10.00em;height:1.25em;"/></sub><span>, where the </span><em>X<sub>i</sub></em><span> values are all correlated.</span></p>
<p>Now, if we want to find the mean of <em>X</em>, we do so as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/91724f93-d33f-4252-a0b4-d7c2c6a83a1e.png" style="width:27.92em;height:1.33em;"/></p>
<p>If it exists, the covariance matrix is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a03239c7-fe1b-4243-9c94-1c3456f01951.png" style="width:24.42em;height:1.58em;"/></p>
<p class="mce-root">Additionally, if we have <sub><img class="fm-editor-equation" src="Images/8a3d1ee9-b059-4ba1-b14b-e3e0e77cd8f8.png" style="width:4.67em;height:1.00em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/b9a771ad-d7ed-4b25-893d-02668aad631b.png" style="width:5.83em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/bb633a7b-f2db-4e49-a496-530c64d1eeec.png" style="width:10.75em;height:1.42em;"/></sub>.</p>
<p>If we're dealing with two random vectors, then we have the following:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/6999b581-2c03-486c-8233-c1438a8ffb3d.png" style="width:15.00em;height:1.42em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><span><sub><img style="text-align: center;color: #333333;width:15.17em;height:1.67em;" class="fm-editor-equation" src="Images/03569a0c-49a8-412b-a383-dd37bd97cc35.png"/></sub></span></li>
</ul>
<p>Now, let's define what a multivariate normal distribution is.</p>
<p>Suppose we have a random vector, <em>X</em>. It has a multivariate normal distribution if for <sub><img class="fm-editor-equation" src="Images/221ff3e8-3a7e-4d6a-8722-a585a321c39b.png" style="width:3.25em;height:1.00em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/ae41ef31-efdc-4ab9-8897-adbc11c19a7f.png" style="width:2.00em;height:1.17em;"/></sub> has a normal distribution. </p>
<p>If <sub><img class="fm-editor-equation" src="Images/9b329d48-78e6-43ca-a217-39089a82f43f.png" style="width:4.25em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/eb305975-3426-4745-ab7a-3e0916025c61.png" style="width:5.83em;height:1.33em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/6fa9183f-6f70-4e8c-bfa6-0754ba3e399c.png" style="width:6.83em;height:1.33em;"/></sub>, where <sub><img class="fm-editor-equation" src="Images/abf8c575-074b-4690-a372-058e15ca0f1d.png" style="width:0.67em;height:0.75em;"/></sub> is a symmetric and positive semi-definite matrix, because <sub><img class="fm-editor-equation" src="Images/9d1a222f-08b8-44c4-afd3-a78c3ae2f1f1.png" style="width:9.83em;height:1.42em;"/></sub>.</p>
<p>Some of you may be wondering what the PDF of a multivariate normal distribution looks like. We'll get there momentarily.</p>
<p>Now, suppose <sub><img class="fm-editor-equation" src="Images/47df59d1-ee7a-432e-b11b-284d529339c9.png" style="width:5.92em;height:1.17em;"/></sub> and we split <img class="fm-editor-equation" src="Images/bd94950c-d5d9-447f-be25-bc21830b3c3d.png" style="width:3.08em;height:0.83em;"/> into two smaller random vectors, such that <sub><img class="fm-editor-equation" src="Images/ebad2aef-2228-4204-ad90-31553a6e01d7.png" style="width:5.33em;height:2.58em;"/></sub>, where <sub><img class="fm-editor-equation" src="Images/241be2f3-74a7-41d1-9e4f-03ea72fcbe9c.png" style="width:4.00em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/e56a6648-0fe9-4d0b-969c-ce63cb6b7462.png" style="width:5.83em;height:1.00em;"/></sub>.</p>
<p>Similarly, <sub><img class="fm-editor-equation" src="Images/b4ae1084-2fda-42bf-8e4c-e52cfb87c9c8.png" style="width:4.83em;height:2.58em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/b1eaf2cd-ccfc-4fd8-b5c4-f14731f259cc.png" style="width:8.83em;height:3.00em;"/></sub>.</p>
<p>Now, we have the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/05f56368-5db1-43cd-b78e-7e4aeecdba89.png" style="width:9.83em;height:1.50em;"/></sub></li>
<li><strong>X</strong><sub>1</sub> and <strong>X</strong><sub>2</sub> are independent if <sub><img class="fm-editor-equation" src="Images/0bd91a15-be9c-459b-b263-e3c80451049d.png" style="width:3.67em;height:1.17em;"/></sub></li>
</ul>
<p>When <sub><img class="fm-editor-equation" src="Images/90d12d00-c683-4797-b74b-b345320de67c.png" style="width:1.08em;height:1.25em;"/></sub> is positive semi-definite, then <em>X</em> has the following PDF:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c8fbf7b2-8fdb-4ccc-92cf-4160b0855de4.png" style="width:21.50em;height:3.17em;"/></p>
<p>Here, <em>n</em> is the dimension of <em>x</em>.</p>
<p>Suppose we have <sub><img class="fm-editor-equation" src="Images/aa0ff384-401a-43ce-9ffd-1404a285509e.png" style="width:7.00em;height:1.08em;"/></sub>, which are iid <sub><img class="fm-editor-equation" src="Images/eacccc89-e573-47ed-93f6-728c9cdd6dbf.png" style="width:4.08em;height:1.42em;"/>,</sub> and <sub><img class="fm-editor-equation" src="Images/5bee595f-7f18-4fff-b5b6-8cea70a4d729.png" style="width:6.00em;height:2.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/cfaf9d3e-ea1e-453d-8cd2-2462cb4ee042.png" style="width:12.83em;height:2.25em;"/></sub>. Then, we have the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/80e49a1d-e173-4898-909c-10488ade217c.png" style="width:6.58em;height:2.50em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/16f5fedf-b5cf-4ddf-b009-47d2e9b4324b.png" style="width:6.33em;height:2.50em;"/></sub></li>
<li><em>X</em> and <em>S<sub>xx</sub></em> are independent</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear models</h1>
                </header>
            
            <article>
                
<p>In statistics, we use linear models to model the relationship between a dependent variable and one or more predictor variables. </p>
<p>As an example, let's suppose we have <em>n</em> observations <em>Y<sub>i</sub></em> and <em>p</em> predictors <em>x<sub>j</sub></em>, where <em>n &gt; p</em>. We can write each observation as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/df12e5e9-4892-4246-88d5-557b45da72a4.png" style="width:19.33em;height:1.33em;"/></p>
<p>For all cases of <sub><img class="fm-editor-equation" src="Images/207301fd-7f26-455f-a11c-cc34a23f14a2.png" style="width:5.75em;height:1.17em;"/></sub>, we can assume the following:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/bb2ff12d-a21e-43aa-ba6e-c4c8020438f5.png" style="width:6.33em;height:1.25em;"/></sub> are the unknown, fixed parameters that we want to figure out</li>
<li><sub><img class="fm-editor-equation" src="Images/95ce8668-c2fa-444f-9c45-5f50812501e6.png" style="width:8.58em;height:1.08em;"/></sub> are the values of the <em>p</em> predictors for the <em>i<sup>th</sup></em> response</li>
<li><sub><img class="fm-editor-equation" src="Images/e4172a8a-7db1-4370-85ce-560dbdaf26a6.png" style="width:6.25em;height:0.92em;"/></sub> are independent random variables with 0 mean and σ<sup>2</sup> variance</li>
</ul>
<p>We generally consider <sub><img class="fm-editor-equation" src="Images/ade791f9-ea58-47c9-9d4f-42c2280954c4.png" style="width:2.83em;height:1.33em;"/></sub> to be casual effects of <em>x<sub>ij</sub></em> and <em>ε<sub>i</sub></em> to be a random error term. Therefore, <span><sub><img style="vertical-align: sub;width:17.83em;height:1.33em;" class="fm-editor-equation" src="Images/faf454cf-3be7-4619-99c1-cc9f90255594.png"/></sub></span>, <span><sub><img style="vertical-align: sub;width:10.50em;height:1.42em;" class="fm-editor-equation" src="Images/ad690f9b-55d0-4bb7-8330-f76049c10f54.png"/></sub></span> and <sub><img class="fm-editor-equation" src="Images/c8fe0611-b6b0-4965-95eb-5f56d07ee718.png" style="width:6.25em;height:1.08em;"/></sub><span> are independent.</span></p>
<p>Given all the data, we may want to plot a straight line on the data, so a possible model could be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ff72d58f-5925-4d99-a129-1e12d3f0ef51.png" style="width:8.58em;height:1.25em;"/></p>
<p>Here, <em>a</em> and <em>b</em> are constants. </p>
<p>We can rewrite the preceding model as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/85985358-f49c-42bf-8c96-476705fa4364.png" style="width:6.08em;height:1.25em;"/></p>
<p>Here, the expanded form is as follows:</p>
<p style="padding-left: 150px"><img class="aligncenter size-full wp-image-1271 image-border" src="Images/9db67e29-19cb-437e-ad70-7bf1d51cc39f.png" style="width:31.83em;height:6.33em;"/></p>
<p>Also, <sub><img class="fm-editor-equation" src="Images/1ea56a87-6168-4feb-9809-fbc4b6577d39.png" style="width:3.92em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/561e8418-62a2-41e1-891c-acf5d40aec5b.png" style="width:5.67em;height:1.25em;"/></sub>.</p>
<p>The least-squares estimator, <sub><img class="fm-editor-equation" src="Images/f4cabe0c-ce00-4585-a0fe-26ed010b1157.png" style="width:0.83em;height:1.67em;"/></sub>, of <sub><img class="fm-editor-equation" src="Images/a54cfaf6-bf6a-430c-9113-6ddd5ad39ded.png" style="width:0.83em;height:1.50em;"/></sub> minimizes our linear model by minimizing the square of the vertical distance between the line and the points, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a41ba500-0f5d-42c5-827d-047520831ed2.png" style="width:13.67em;height:6.17em;"/></p>
<p>To minimize it, we apply the following for all cases of <em>k</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0901e9c0-ac52-4b26-824a-256b6cf425cb.png" style="width:5.92em;height:2.75em;"/></p>
<p>So, we have <sub><img class="fm-editor-equation" src="Images/c17c7155-da0b-402d-a277-8f7636c5b07a.png" style="width:11.50em;height:1.67em;"/></sub> and so, <sub><img class="fm-editor-equation" src="Images/2818372d-b08c-410b-bbf6-fd58a08db06c.png" style="width:8.42em;height:1.58em;"/></sub> for all cases of <em>k</em>. </p>
<p>By putting the preceding function into matrix form, as we did earlier, we get <sub><img class="fm-editor-equation" src="Images/4ede1bec-bd6a-4a52-8f53-a2624335127b.png" style="width:6.00em;height:1.17em;"/></sub>.</p>
<p>We know that <sub><img class="fm-editor-equation" src="Images/3d4a8efc-89f0-45bb-adcb-5f87718c4f3c.png" style="width:2.08em;height:1.00em;"/></sub> is positive, semi-definite, and has an inverse. Therefore, <sub><img class="fm-editor-equation" src="Images/c0249ae1-e0e5-484b-a157-06fa293a0dbb.png" style="width:7.33em;height:1.25em;"/></sub>.</p>
<div class="packt_tip">Under normal assumptions, our least-squares estimator is the same as the MLE.</div>
<p>We now have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/49994f38-c0e3-4edf-93e2-ff48207a1418.png" style="width:22.42em;height:1.50em;"/></p>
<p>This tells us that our estimator is unbiased and <sub><img src="Images/d9b090a4-a01a-4f10-90cb-86ea17972736.png" style="width:31.67em;height:1.67em;"/></sub>.</p>
<p class="CDPAlignLeft CDPAlign"><span>I know what you're thinking—that was intense! Good job on making it this far; we are very close to finishing this chapter, so hang in there. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hypothesis testing</h1>
                </header>
            
            <article>
                
<p>In hypothesis testing, our goal is to ascertain whether certain variables influence the outcome. </p>
<p class="CDPAlignLeft CDPAlign">Let's test a hypothesis of a general linear model. Suppose we have <sub><img class="fm-editor-equation" src="Images/ad70ebb2-7402-4603-b8ce-ab6805402f97.png" style="width:23.08em;height:1.75em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/d081a397-ef2f-4689-a2af-79870d4c9eac.png" style="width:5.75em;height:3.00em;"/></sub>. We would like to test <sub><img class="fm-editor-equation" src="Images/b4e69d79-5f1b-499c-ab9a-d1a0b0214fcb.png" style="width:5.00em;height:1.08em;"/></sub> against <sub><img class="fm-editor-equation" src="Images/3f8e9abf-d6f6-4974-8290-932ff3883f53.png" style="width:6.92em;height:1.58em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/8301d233-ae79-4496-81cb-6690dd9885c7.png" style="width:6.67em;height:1.17em;"/></sub>, since under <em>H<sub>0</sub></em>, <sub><img class="fm-editor-equation" src="Images/e1efcc5e-4253-4549-b74f-63ec5ba0fdd9.png" style="width:2.42em;height:1.08em;"/></sub> vanishes.</p>
<p class="CDPAlignLeft CDPAlign">Under the null hypothesis, the maximum likelihood of <em>β<sub>0</sub></em> and <em>σ<sup>2</sup></em> are <sub><img class="fm-editor-equation" src="Images/ff2c91c9-3f27-443a-9768-0a39ba305872.png" style="width:6.25em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/f09971f6-e977-4270-babe-63267e0689a5.png" style="width:11.33em;height:1.83em;"/>, </sub>which, as we know from earlier, are independent. </p>
<p class="mce-root">The estimators of the null hypothesis wear two hats instead of one and the alternative hypothesis has one.</p>
<p><span>Congratulations! You have officially completed this chapter and you have </span><span>now</span><span> </span><span>developed a</span><span> solid intuition for probability and statistics. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned a lot of concepts. I recommend going through the chapter again if needed because the topics in this chapter are very important to gaining a deep understanding of deep learning. Many of you may be wondering what the chapters you have learned so far have to do with neural networks; we will tie it all together in a couple more chapters. </p>
<p>The next chapter<span> focuses on both convex and non-convex optimization methods and builds the foundation for understanding the optimization algorithms used in training neural networks.</span></p>


            </article>

            
        </section>
    </div></body></html>