- en: Probability and Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover two of the most important areas of mathematics—probability
    and statistics. These are two terms that you've likely come across a number of
    times in your everyday life. People use it to justify just about everything that
    occurs or when they're trying to prove a point. Once you are done with this chapter,
    you will have a firm grasp of both of them and will understand how they both are
    related and how they differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concepts in probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essential concepts in statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concepts in probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability theory is one of the most important fields of mathematics and is
    essential to the understanding and creation of deep neural networks. We will explore
    the specifics of this statement in the coming chapters. For now, however, we will
    focus our effort toward gaining an intricate understanding of this field.
  prefs: []
  type: TYPE_NORMAL
- en: We use probability theory to create an understanding of how likely it is that
    a certain event will occur. Generally speaking, probability theory is about understanding
    and dealing with uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Classical probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose we have a random variable that maps the results of random experiments
    to the properties that interest us. The aforementioned random variable measures
    the likelihood (probability) of one or more sets of outcomes taking place. We
    call this the **probability distribution**. Consider probability distribution
    as the foundation of the concepts we will study in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: There are three ideas that are of great importance in probability theory—probability
    space, random variables, and probability distribution. Let's start by defining
    some of the more basic, yet important, concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The sample space is the set of all the possible outcomes. We denote this with
    Ω. Suppose we have *n* likely outcomes—then, we have [![](img/03483c0d-c6a6-40c6-a11b-d549d6f04381.png)],
    where *w[i]* is a possible outcome. The subset of the sample space (Ω) is called
    an **event**.
  prefs: []
  type: TYPE_NORMAL
- en: Probability has a lot to do with sets, so let's go through some of the notation
    so that we can get a better grasp of the concepts and examples to come.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have two events, *A* and *B*, ⊆ Ω. We have the following axioms:'
  prefs: []
  type: TYPE_NORMAL
- en: The complement of *A* is *A^C*, so [![](img/14e34629-3242-4a47-bcb0-43522e1133e2.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If either *A* or *B* occurs, this is written as *A *∪ *B* (read as *A* union
    *B*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both *A* and *B* occur, this is written as *A *∩ *B* (read as *A* intersect
    *B*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *A* and *B* are mutually exclusive (or disjoint), then we write [![](img/62e25ee7-b55a-47d4-96b3-62c477ea9b30.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the occurrence of *A* implies the occurrence of *B*, this is written as *A *⊆ *B*
    (so, [![](img/4b689d79-0135-4b8d-8948-f4bf83eccd1f.png)]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Say we have an event, *A *∈ Ω, and [![](img/b873dd7c-ea17-454c-91a4-a22a9af938b2.png)].
    In this case, the probability of *A* occurring is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed32339c-6ea3-4bb9-9942-dd74e74be8a8.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the number of times *A* can occur divided by the total number of possible
    outcomes in the sample space.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through a simple example of flipping a coin. Here, the sample space
    consists of all the possible outcomes of flipping the coin. Say we are dealing
    with two coin tosses instead of one and *h* means heads and *t* means tails. So,
    the sample space is Ω = {*hh*, *ht*, *th*, *tt*}.
  prefs: []
  type: TYPE_NORMAL
- en: All of the possible results of the experiment make up the event space, [![](img/7c6782f4-9b02-457e-85e9-4a5cb44dea0c.png)].
    On finishing the experiment, we observe whether the outcome, ω ∈ Ω, is in *A*.
  prefs: []
  type: TYPE_NORMAL
- en: Since, in each event, [![](img/8f4046d7-1601-4bff-942c-182af8351ed6.png)], we
    denote *P(A)* as the probability that the event will happen and we read *P(A)* as
    the probability of *A* occurring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing on from the previous axioms, ![](img/f153cd8f-ff75-4d39-a2e6-2a5ac0c57fa4.png) must
    satisfy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/ce7395f4-137c-41dd-85ee-d8cd61f4a8c6.png)] for all cases of [![](img/cd1f2c74-89cb-4f58-8bcf-52f941de60a6.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/328ba247-2525-48ad-b8f8-cb892d6c9e77.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the events *A[1], A[2], …* are disjoint and countably additive—that is, [![](img/52252828-6a1c-482a-b70f-fdfd4acdcbff.png)] for
    all cases of *i, j—*we then have [![](img/76bfbe42-185c-44c1-863e-8c6596d23690.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The triple [![](img/b1f980f2-fbe3-4210-baa0-d9054288c774.png)] terms are known
    as the **probability space**.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, when [![](img/4cba0ddd-83cb-459c-b409-116f6e546a6a.png)],
    then event *A* happens almost surely and when [![](img/80a9ab87-9a8f-40ec-bb44-9da306b708d3.png)],
    then event *A* happens almost never.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the preceding axioms, we can derive the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cce00473-b188-4254-8b79-23afa7b1d346.png)'
  prefs: []
  type: TYPE_IMG
- en: So, [![](img/8db9764d-1664-4563-b77b-309fa3c815b9.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if we have two events, *A* and *B*, then we can deduce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fc2aa1f-e307-4543-85a9-2f5fb36d29c7.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing on from the preceding axioms, ![](img/8371eae4-d87f-4036-b7d4-8a1029cd54ec.png) must
    satisfy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/26f1c3b3-f2c9-4272-8c72-8baf9b72ad71.png)] for all ![](img/17dae569-967d-4756-8fa8-ae42e139d788.png)'
  prefs: []
  type: TYPE_NORMAL
- en: To find the probability of anything, we usually have to count things. Let's
    say we have a bucket filled with tennis balls and we pick a ball from the bucket
    *r* times; so, there are *n[1]* possibilities for the first pick, *n[2]* for the
    next pick, and so on. The total number of choices ends up being *n[1]*×*n[2]*×…×*n[r]*.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling with or without replacement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now assume that there is a total of *n* items in the bucket and we must
    pick *r* of them. Then, let *R *= {1, 2,…, *r*} be the list of items picked and
    let *N *= {1, 2, …, *n*} be the total number of items. This can be written as
    a function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf6492fe-1af9-473f-b963-143026c23a44.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *f(i)* is the *i^(th)* item.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling with replacement is when we pick an item at random and then put it
    back so that the item can be picked again.
  prefs: []
  type: TYPE_NORMAL
- en: However, sampling without replacement refers to when we choose an item and don't
    put it back, so we cannot pick it again. Let's see an example of both.
  prefs: []
  type: TYPE_NORMAL
- en: Say we need to open the door to our office and we have a bag containing *n* keys;
    they all look identical, so there's no way of differentiating between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first time we try picking a key, we replace each one after trying it, and
    we manage to find the correct key on the *r^(th)* trial, implying we got it wrong
    *r-1* times. The probability is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a8733ed-c9da-4b6d-b46a-6117990688f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we know that our earlier strategy wasn''t the smartest, so this time we
    try it again but without replacement and eliminate each key that doesn''t work.
    Now, the probability is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78230e2f-b4fc-4135-9f22-1e5d89b51137.png)'
  prefs: []
  type: TYPE_IMG
- en: Multinomial coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know from the binomial theorem (which you likely learned in high school)
    that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2970ab3-938a-4597-b2b0-bc5681efece0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the trinomial is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbdd5078-bca0-45bf-a2d7-6e3271b51324.png)'
  prefs: []
  type: TYPE_IMG
- en: Say we have *n* pieces of candy and there are blue- and red-colored candies.
    The different ways that we can pick the candies is defined as [![](img/205923c8-e800-4e64-9691-b115fc61afaa.png)],
    which is read as *n* choose *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multinomial coefficient is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c57dc5e8-171c-4061-861b-bbcb5d65d7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, we spread *n* items over *k* positions, where the *i^(th)* position
    has *n[i]* items.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we''re playing cards and we have four players. A deck of cards
    has 52 cards and we deal 13 cards to each player. So, the number of possible ways
    that we can distribute the cards is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04468143-efde-422b-a276-f0a185113692.png)'
  prefs: []
  type: TYPE_IMG
- en: This is absolutely massive!
  prefs: []
  type: TYPE_NORMAL
- en: This is where Stirling's formula comes to the rescue. It allows us to approximate
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Stirling's formula
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sake of argument, let's say [![](img/969912b8-c618-420f-b5d6-f444094be46a.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/120d5df1-3fae-42d9-8977-14c3afc08e6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, we now claim the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77a36317-3913-431f-91b3-3338b59705f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3bd9297-5033-4be2-90bc-c81f869c67a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, by evaluating the integral, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5996c50a-6dc4-4fd5-9e31-7a79e24cd05a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now divide both sides by [![](img/65fcf4e9-46f2-44f1-86a3-6d79546a039a.png)] and
    take the limit as *n*→∞. We observe that both sides tend to 1\. So, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02479d10-afcf-4829-b03c-59563dd2e27e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Stirling''s formula states that as *n*→∞, the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee3d7e9a-6bd8-41ff-91aa-81833dc1956c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4baec502-e54a-49c8-9fcf-f1c08b717816.png)'
  prefs: []
  type: TYPE_IMG
- en: We will avoid looking into the proof for Sterling's formula, but if you're interested
    in learning more, then I highly recommend looking it up.
  prefs: []
  type: TYPE_NORMAL
- en: Independence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Events are independent when they are not related to each other; that is, the
    outcome of one has no bearing on the outcome of another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have two independent events, *A* and *B*. Then, we can test the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb614af3-1c4a-4f04-8e52-a5318d6cbc33.png)'
  prefs: []
  type: TYPE_IMG
- en: If this is not true, then the events are dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you're at a casino and you're playing craps. You throw two dice—their
    outcomes are independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting property of independence is that if *A* and *B* are independent
    events, then so are *A* and *B^C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look and see how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd31d53f-a97a-42f1-87e6-63b3e6c6ec8f.png)'
  prefs: []
  type: TYPE_IMG
- en: When we have multiple events, *A[1], A[2], …, A[n]*, we call them mutually independent
    when [![](img/695b94c5-04a8-4ff6-a77f-7c414be5023f.png)] for all cases of n ≥
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we conduct two experiments in a lab; we model them independently
    as [![](img/eb5c64f6-516f-4b14-92ec-0b12c114fa20.png)] and [![](img/a6189c28-6064-4f67-8a8d-37a4d3fd4546.png)] and
    the probabilities of each are [![](img/c902723a-4564-43f4-8f0b-bcf14067b398.png)] and
    [![](img/2ee881bc-5e46-45ed-b2a6-73fdef48877f.png)], respectively. If the two
    are independent, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a48b451-9e46-4b58-bb9f-eebfda13b1bb.png)'
  prefs: []
  type: TYPE_IMG
- en: This is for all cases of *i* and *j*, and our new sample space is Ω = Ω[1 ]×
    Ω[2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, say *A* and *B* are events in the Ω[1] and Ω[2] experiments, respectively.
    We can view them as subspaces of the new sample space, Ω, by calculating *A *×
    Ω[2] and *B *× Ω[1], which leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e820dfc2-bb78-4337-9629-b11834f636e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though we normally define independence as different (unrelated) results
    in the same experiment, we can extend this to an arbitrary number of independent
    experiments as well.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discrete refers to when our sample space is countable, such as in the cases
    of coin tosses or rolling dice.
  prefs: []
  type: TYPE_NORMAL
- en: In discrete probability distributions, the sample space is [![](img/1f0f9681-eaf3-4830-9acc-f33efb7c2a14.png)] and
    [![](img/4c061723-d8be-4c59-b3c4-9eee1a942865.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the six different kinds of discrete distributions that we
    often encounter in probability theory:'
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binomial distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypergeometric distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poisson distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's define them in order.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Bernoulli distribution, let''s use the example of a coin toss, where
    our sample space is Ω = {*H*, *T*} (where *H* is heads and *T* is tails) and *p *∈
    [0, 1] (that is, 0 ≤ *p *≤ 1). We denote the distribution as *B(1*, *p)*, such
    that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38aeae95-1952-46f4-bc03-e0580c813b3e.png) and ![](img/727c7a77-7f02-4e99-be6b-30cdbcf245d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But now, let''s suppose the coin is flipped *n* times, each with the aforementioned
    probability of *p* for the outcome being heads. Then, the binomial distribution,
    denoted as *B(n, p)*, states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58b3b7cd-d069-43c3-a738-26637fb1d32f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14d4f623-e99a-4871-beae-e4cd9e66d2a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally, the binomial distribution is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9042b37-85f2-43e0-897d-126e311be650.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The geometric distribution does not keep any memory of past events and so is
    memory-less. Suppose we flip our coin again; this distribution does not give us
    any indication as to when we can expect a heads result or how long it will take.
    So, we write the probability of getting heads after getting tails *k* times as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a7870f1-c865-41d3-a626-2925a1c3ad34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say we have a bucket filled with balls of two colors—red and black (which
    we will denote as *r* and *b*, respectively). From the bucket, we have picked
    out *n* balls and we want to figure out the probability that *k* of the balls
    are black. For this, we use the hypergeometric distribution, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c3f3385-715f-4863-ac0e-87f255c86da8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Poisson distribution is a bit different from the other distributions. It
    is used to model rare events that occur at a rate, λ. It is denoted as *P*(λ) and
    is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cd9c44f-25d0-47b5-b14a-3f04d224a443.png)'
  prefs: []
  type: TYPE_IMG
- en: This is true for all cases of ![](img/7f86ec8f-cdfa-4954-9dbe-9ec39271d282.png).
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conditional probabilities are useful when the occurrence of one event leads
    to the occurrence of another. If we have two events, *A* and *B*, where *B* has
    occurred and we want to find the probability of *A* occurring, we write this as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a27e0f3-306e-40ba-8f96-20cf46c4eaf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/a5c8df71-3eed-4f7a-9531-b04e228d1302.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the two events, *A* and *B*, are independent, then we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9542fb47-8681-4348-a6fc-a3018fa81ad9.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, if [![](img/f199d6b1-6442-4d5c-be27-70b5a03cb636.png)], then it
    is said that *B* attracts *A*. However, if *A* attracts *B^C*, then it repels
    *B*.
  prefs: []
  type: TYPE_NORMAL
- en: The attraction between *A* and *B* is bidirectional; that is, *A* can only attract
    *B* if *B* also attracts *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the axioms of conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/75903c9e-d098-472c-a75c-94e7bd5a4b13.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/1c9aace0-180f-4c10-8a56-bbdccf62233b.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/b4681247-54dd-4a50-989e-42c0301cbdb6.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ddc98d3f-4690-4e64-b416-86d99c44a22b.png)] is a probability function
    that works only for subsets of *B*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/566cc844-450c-4d5c-85d9-4a7493f152d7.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If [![](img/0fac1325-7227-45d1-8259-7993ffdcd7e7.png)], then [![](img/78aa0939-baff-4621-be22-fda466c2b9b3.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following equation is known as **Bayes'' rule**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eba7347c-2eba-4534-8e0f-5c4b6ceb6c11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08f22faa-5df1-4592-812d-927234cb80ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/d40d8ca3-29b7-4eab-8bdb-fc91e3ac74ba.png)] is called the prior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/60a49fa8-3b1b-4c4f-b49c-5b275bb67e8a.png)] is the posterior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/b95e6129-fc56-45ca-adb4-e7f1708596ea.png)] is the likelihood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e2485bc2-e169-43f7-ade0-1918bc0c93e3.png)] acts as a normalizing constant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ![](img/25435825-223f-4374-8b78-d0306c1b6f66.png) symbol is read as **proportional
    to**.
  prefs: []
  type: TYPE_NORMAL
- en: Often, we end up having to deal with complex events, and to effectively navigate
    them, we need to decompose them into simpler events.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the concept of partitions. A partition is defined as a collection
    of events that together makes up the sample space, such that, for all cases of *B[i]*, [![](img/198293d4-0671-48fe-97ce-74737f46784f.png)].
  prefs: []
  type: TYPE_NORMAL
- en: In the coin flipping example, the sample space is partitioned into two possible
    events—heads and tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *A* is an event and *B[i]* is a partition of Ω, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eabf79c-aa99-4b5b-9605-939e8729ae05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also rewrite Bayes'' formula with partitions so that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0736c821-dc23-4e6a-be02-12f01db2d752.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/87bd5d8b-31bc-4dbe-a2fb-9d312e0d0478.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Random variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random variables are variables that have a probability distribution attached
    to them that determines the values each one can have. We view the random variable
    as a function, *X*: Ω → Ω[x], where [![](img/b8558ada-ef3c-49aa-a5a6-5e57caf2362c.png)]. The
    range of the *X* function is denoted by [![](img/4701c9e8-5093-4914-92fb-be7bab0db877.png)].
  prefs: []
  type: TYPE_NORMAL
- en: A discrete random variable is a random variable that can take on finite or countably
    infinite values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have *S* ∈ Ω[x]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bd78772-a702-46ed-b3cd-c3f07ff03847.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the probability that *S* is the set containing the result.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of random variables, we look at the probability of a random variable
    having a certain value instead of the probability of obtaining a certain event.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our sample space is countable, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/055c9c4f-a4ca-47f4-b353-8ead0bf17f68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Suppose we have a die and *X* is the result after a roll. Then, our sample
    space for *X* is Ω[x]={1, 2, 3, 4, 5, 6}. Assuming this die is fair (unbiased),
    then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ad1fa48-6b05-4bea-a9c6-b93cb0caecaa.png)'
  prefs: []
  type: TYPE_IMG
- en: When we have a finite number of possible outcomes and each outcome has an equivalent
    probability assigned to it, such that each outcome is just as likely as any other,
    we call this a discrete uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say *X∼B(n, p)*. Then, the probability that the value that *X* takes
    on is *r* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e2dedc4-e425-4955-a88b-81f971c1b46a.png)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, in probability literature, [![](img/c8735e61-805e-49cd-a060-f6218d412033.png)] is
    written as [![](img/245fad37-79a6-4e5f-b78d-218e0a473baa.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of the time, we may need to find the expected (average) value of a random
    variable. We do this using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/004396e7-e662-4bdc-92e4-a63490b6f256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also write the preceding equation in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e8c359d-bf77-4e4c-a64d-c5f36f01ef2b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding two equations only work when our sample space is discrete (countable).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the axioms for [![](img/5cf75ec8-3673-457e-a8fc-0c62f46a0936.png)]:'
  prefs: []
  type: TYPE_NORMAL
- en: If [![](img/d4b3ea81-83a1-4553-915c-8bce6eb92ffc.png)], then [![](img/fbc2a53b-b69d-4dae-9d19-85ce2fdb67a4.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If [![](img/03296dd8-2f4e-4e0c-9a7a-3b33676f2e30.png)] and [![](img/6bb14c02-8a1d-4a8f-9d4f-49ab4dbed99b.png)],
    then [![](img/4b0493af-740c-4f82-a7bb-dc7204f67705.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/6a2553b3-dbc8-4dd2-b18d-67ad22964f86.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/42579c09-73b2-4116-be94-97624bd1790f.png)], given that α and β are
    constants and *X[i]* is not independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ff887879-65ec-4106-8a62-c0fe59bb0152.png)] , which holds for when
    *X[i]* is independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/6c6a0c03-1d5f-4c5a-a632-214648189d54.png)] minimizes [![](img/aa32ed28-6eb5-4b9f-91e2-be23c9285d96.png)] over
    *c.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suppose we have *n* random variables. Then, their expected value is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d94a83f2-fdf3-406e-ba58-e943201395e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have a good understanding of the expectation of real-valued random
    variables, it is time to move on to defining two important concepts—variance and
    standard variables.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We define the variance of *X* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a3bb4a1-a56d-4d53-94d2-46cc66eba155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The standard deviation of *X* is the square root of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f27bf3c-b2f4-47bb-88b2-5bc2bc53b470.png)'
  prefs: []
  type: TYPE_IMG
- en: We can think of this as how spread out or close values are from the expected
    (mean) value. If they are highly dispersed, then they have a high variance, but
    if they are grouped together, then they have a low variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some properties for variance that are important to remember:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/19af9c52-e0b1-49f8-8bbe-93d03d0d730a.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If [![](img/3566903a-644f-4cec-a028-6a0123f750b7.png)], then [![](img/677ce2ec-3e15-4719-9d56-392e0d0633fe.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ea36761b-42fe-4164-a20a-66ab703da52c.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/dd3ff0ee-dd34-4695-96b5-d085c3fe207f.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/531e3907-2cb6-4aa6-a9bc-1a40cc31f921.png)], given that all the *X[i]* values
    are independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s suppose that we now have ![](img/516e70ce-60bc-44c4-acbd-94bc71b47ef0.png) discrete
    random variables. Then, they are independent if we take the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b1bf323-a745-4bf0-8b7a-4bb63342b3e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let our *n* random variables be independent and **identically distributed**
    (**iid**). We now have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/551a1a1b-6731-4dcd-91db-7a56262636b6.png)'
  prefs: []
  type: TYPE_IMG
- en: This concept is very important, especially in statistics. It implies that if
    we want to reduce the variance in the results of our experiment, then we can repeat
    the experiment a number of times and the sample average will have a small variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s imagine two pieces of rope that have unknown lengths—*a* and
    *b*, respectively. Because the objects are ropes—and, therefore, are non-rigid—we
    can measure the lengths of the ropes, but our measurements may not be accurate.
    Let *A* be the measured value of rope *a* and *B* be the measured value of rope
    *b* so that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c083980-6c63-493c-ae42-32bef1eb65f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can increase the accuracy of our measurements by measuring *X = A + B* and
    *Y = A – B*. Now, we can estimate *a* and *b* using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23416a25-98bc-4170-93bb-241446ffd02b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, [![](img/76a6252d-ef6a-4d28-8d0f-e986ba6b3f66.png)] and [![](img/822a2a6a-8bdd-489c-aff6-70f4ad2be5d0.png)],
    which are both unbiased. Additionally, we can see that the variance has decreased
    in our measurement using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67d1ce11-28dc-4962-8acc-4efbcc8a300d.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can clearly see that measuring the ropes together instead of separately
    has improved our accuracy significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple random variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the time, we will end up dealing with more than one random variable.
    When we do have two or more variables, we can inspect the linear relationships
    between the random variables. We call this the covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have two random variables, *X* and *Y*, then the covariance is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc6d132f-3893-4563-8932-b44b1582dc89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some of the axioms for the covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: If *c* is a constant, then [![](img/ef4a1b71-62cd-4671-b335-ea90679559be.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/63f053f8-cc51-48de-9b9e-bd84f5356c62.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/23ce2b25-59d8-41f3-9233-d3ba3ec1d647.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a2d96a5d-f76c-4a42-9361-1bc747db4af9.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2f649eb8-737f-4b05-9496-774cc17f0354.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/eedd1cad-fc30-4c14-a2f6-b90a20e6f9aa.png).]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/6d88bb9d-91f6-47cd-8259-573532a1a3fa.png)], given that *X* and *Y* are
    independent (but it does not imply that the two are independent).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/26014aa6-4f93-4a08-8d52-30cec07ee128.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, sometimes, the covariance doesn''t give us the full picture of the
    correlation between two variables. This could be a result of the variance of *X* and
    *Y*. For this reason, we normalize the covariance as follows and get the correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5140026-c84e-4c14-a7e2-21ea1ed1b64f.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting value will always lie in the [-1, 1] interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads us to the concept of conditional distributions, where we have two
    random variables, *X* and *Y*, that are not independent and we have the joint
    distribution, [![](img/64a8a6ed-302d-4a67-9093-745dd54328c1.png)], from which
    we can get the probabilities, [![](img/21ffaa8e-d944-4ee7-9e1f-8e509f0afad5.png)] and
    [![](img/39351bdf-7e86-469a-b743-fe6340495765.png)]. Then, our distribution is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45c4ff9a-381d-4ebe-b8ee-bf9423196043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this definition, we can find our conditional distribution of *X* given
    *Y* to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/796c562d-b757-40ed-81d2-46738adac170.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We may also want to find the conditional expectation of *X* given *Y*, which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30f06fb6-c3c3-4bad-bc82-21ac9e43f0be.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if our random variables are independent, then, [![](img/b1be2d62-c472-41a1-9bba-ebcd5eb08712.png)],
    which we know to be true because *Y* has no effect on *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous random variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've looked at discrete outcomes in the sample space where we could
    find the probability of a certain outcome. But now, in the continuous space, we
    will find the probability of our outcome being in a particular interval or range.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to find the distribution of *X*, we need to define a function, *f*, so
    that the probability of *X* must lie in the interval [![](img/5794cda3-9f1e-4c28-a0c2-0528c9e9d219.png)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, a random variable, ![](img/a0f56366-bbe1-473a-932e-2f0dd2433fce.png),
    is continuous if, in a function, [![](img/5587b9ca-262a-4565-b0ae-050800428fdf.png)] so
    that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/397c6a87-7c95-423f-be55-c89b2e07e011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We call the function, *f*, the **probability density function** (**PDF**) and
    it must satisfy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/fa2aa7ca-7ccd-4e31-8e3d-6c1773dd1492.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/47a2c693-41bf-417a-b87e-07e887e2cffb.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is another distribution function that is important for us to know, known
    as the **cumulative distribution function**. If we have a random variable, *X*,
    that could be continuous or discrete, then, [![](img/be3f65db-4bd9-4d17-862f-9f685064a4db.png)],
    where *F(x)* is increasing so that x→∞ and *F(x)→1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with continuous random variables such as the following, we know
    that *F* is both continuous and differentiable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14e36303-5461-4190-97df-0161bfe46b72.png)'
  prefs: []
  type: TYPE_IMG
- en: So, when *F* is differentiable, then *F'(x) = f(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: An important fact to note is that [![](img/8110eefa-b4e6-46ab-bce8-eb7a78d14921.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads us to the concept of uniform distribution, which, in general, has
    the following PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98c36358-0492-420d-bf4b-2cd22644eec1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da6dfe92-c875-4bc5-a552-9f7065528072.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the case for ![](img/36e6ab23-5de9-4cc8-8bfd-363b3bdada5b.png).
  prefs: []
  type: TYPE_NORMAL
- en: We write [![](img/f11dfa86-15fb-494d-ab79-46a81e7325d8.png)] if *X* follows
    a uniform distribution on the [*a*, *b*] interval.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's suppose our random variable is an exponential random variable and
    has the added λ parameter. Then, its PDF is [![](img/eb50042a-970a-498a-abf9-35db9bbd73d0.png)] and [![](img/73d7aaf7-fe65-4ee3-b75f-daef267de5ec.png)] for
    all ![](img/b8ba8fd6-7fa9-4c42-b141-35047f72bf49.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We write this as [![](img/133a6020-8cb8-41aa-b915-cf44b7e8866d.png)], so we
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67f53b45-88bc-4033-8617-c9e6c8c96206.png)'
  prefs: []
  type: TYPE_IMG
- en: It is also very important to note that the exponential random variable, such
    as the geometric random variable, is memory-less; that is, the past gives us no
    information about the future.
  prefs: []
  type: TYPE_NORMAL
- en: Just as in the discrete case, we can define the expectation and variance in
    the case of continuous random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expectation for a continuous random variable is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/491ff6f1-6c48-43af-a140-5859c1eb5a37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But, say [![](img/6865b917-c2dc-4e80-8bd6-a1820c209abb.png)]. Then, we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6f9e16e-9bcd-43c3-9a6f-48892f6b07ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of continuous random variables, the variance is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97c0330b-a838-4516-b36f-abe13945464b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfa14771-c603-485a-9123-8c93635c78c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, for example, let''s take [![](img/a0003821-f605-4b9a-86aa-ec551887e0a8.png)].
    We can find the expected value of *X* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40507b44-b1d1-482b-aee3-b0b8767d27a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Its variance can be found as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d1dabf8-5441-4def-bf16-179da04a8de5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have a good handle of expectation and variance in continuous distribution.
    Let's get acquainted with two additional terms that apply to PDFs—**mode** and
    **median**.
  prefs: []
  type: TYPE_NORMAL
- en: The mode in a PDF is the value that appears the most; however, it is also possible
    for the mode to appear more than once. For example, in a uniform distribution,
    all the *x* values can be considered as the mode.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have a PDF, *f(x)*. Then, we denote the mode as ![](img/f5569622-b675-4c09-a442-6807f4feff3f.png),
    so [![](img/faba697b-8583-4a38-bca9-c80725a5f714.png)] for all cases of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the median as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c72d59ef-4d49-4a8b-a1e1-2fee7c9f92e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in a discrete case, the median is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6647fff3-007c-4f71-8dff-c5de1cb8e908.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Many times, in probability, we take the sample mean instead of the mean. Suppose
    we have a distribution that contains all the values that *X* can take. From it,
    we randomly sample *n* values and average it so that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07b25103-74cf-473a-b795-f3ade978fcd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Joint distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have dealt with and learned about distributions that relate to one
    random variable; but now, say we have two random variables, *X* and *Y*. Then,
    their joint distribution is defined as [![](img/02e5e753-145e-49c1-8dbf-50b3ec96ab65.png)],
    such that [![](img/642b9988-79c5-41f2-972b-ff7c776ab2fa.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In joint distributions, we usually tend to know the distribution of a set of
    variables, but sometimes, we may only want to know the distribution of a subset.
    We call this the marginal distribution. We define the marginal distribution of
    *X* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/264ab983-aaab-4166-93d7-52d4163cb71e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say our *n* continuous random variables in *A* are jointly distributed
    and have the *f* PDF. Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cc0e9d1-769a-44ae-8206-d6e5dee70797.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/fc5aaac3-bedc-4b47-8aeb-bc804637c9c5.png)] and [![](img/10954a31-2c2c-40ff-9c51-cb0292cd9623.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Let's revisit an earlier example, where we have two variables, *X* and *Y*.
    If the variables are continuous, then their joint distribution is [![](img/856a0270-c44e-456c-a788-c3b218c6e345.png)] and [![](img/85602f52-e8c6-4f55-9900-929cc9caaf6d.png)].
  prefs: []
  type: TYPE_NORMAL
- en: If the random variables are jointly continuous, then they are individually continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's suppose our *n* continuous random variables are independent. Then, [![](img/087a0680-c7e3-4aea-9668-e49bbed64247.png)] for
    all cases of [![](img/5c1c0e0e-066a-4871-98b2-40086b5e44b9.png)].
  prefs: []
  type: TYPE_NORMAL
- en: If [![](img/cee4f394-934d-462a-91d1-cbf325a20bde.png)] is a cumulative distribution
    function and [![](img/9ac7ae41-3c20-487a-82c7-1fc0756d0498.png)] is the PDF, then [![](img/28678354-b5ca-44e1-b2ba-e4989e8db5fe.png)] and [![](img/a6c3c3bd-f4a2-471f-bdce-d4fbd7320f90.png)].
  prefs: []
  type: TYPE_NORMAL
- en: More probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier on in this chapter, we introduced several different types of distributions
    in the *Random variables* section. I am sure, at some point, that you thought
    to yourself <q>there must also be probability distributions for continuous random
    variables</q>.
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following distribution is quite an important one and is known as the **normal
    distribution**. It looks like as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b730080-ba84-4519-af83-737c14403c67.png)'
  prefs: []
  type: TYPE_IMG
- en: The normal distribution, written as [![](img/c9515d39-e75d-48b0-b3b5-f7add3cd8a7d.png)],
    has the [![](img/3266f88f-500d-4d4f-aa1c-1a3910ae3819.png)] PDF for all cases
    of ![](img/d8240eee-37e2-4651-9f72-0e708235db7e.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7f6d31a-c9ab-4ec9-9941-9b85ad4a14b8.png)'
  prefs: []
  type: TYPE_IMG
- en: When the normal distribution has [![](img/26958e7e-a86d-41d1-bc93-decf7b167b56.png)] and [![](img/43298051-a412-46a6-ac32-9b6b2169ca49.png)],
    it is called the **standard normal** and we denote [![](img/d92fbb28-181f-41d3-a34a-c47270d846a8.png)] as
    its PDF and [![](img/7333c5a0-2068-4f06-81aa-0a60962c816e.png)] as its cumulative
    distribution function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The normal distribution has some rather interesting properties, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/58567752-2609-40f1-aa56-c2f78746af06.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/96063efd-866b-46f5-8e96-cbab53db66cf.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assuming we have two independent random variables, [![](img/aab98e9d-b4ee-4ace-827e-72cac46491fa.png)] and [![](img/c4daaf9c-b4e1-419e-8d2d-1bf68cc8aca8.png)],
    then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/125b6691-8eee-4e7b-8495-4dec478096f5.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d84498f6-bac9-4f46-b41e-ca2fe6788dcb.png)], where *a* is a constant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason this probability distribution is so important is because of its relation
    to the central limit theorem, which states that if we have a large number of independent
    and identically distributed random variables, then their distribution is approximately
    the same as the normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The normal distribution can also be extended for multiple random variables,
    which gives us the multivariate normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have *n* iid random variables sampled from *N(0, 1)*. Then, we define
    their joint density function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74c05128-6313-43d8-9335-32d7b6374910.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/e18d1a4d-fa10-4d61-93bb-b602d41fff07.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take this a step further. Now, let''s suppose we have an invertible
    *n×n* matrix, *A*, and are interested in [![](img/fc7a3657-df9f-400c-90cc-c2ea98ebc7bc.png)].
    Then, [![](img/0cf3cf14-81c9-4eb5-85bb-d5e2fc04ed9e.png)] and [![](img/a93c60cf-3e11-41c0-bdf7-bc1bea83cfe9.png)].
    So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a4c4e92-f63b-42c1-8469-549356da815e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, [![](img/ded03a25-2c8b-46e4-9343-f85f126ef864.png)]. Therefore, *Z* is
    the multivariate normal and is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75bda44e-7c2b-4a8f-98b9-a655be6e1a58.png)'
  prefs: []
  type: TYPE_IMG
- en: You're likely wondering what this new matrix, ![](img/362a6521-6110-43e2-b9e5-401be8c5ce07.png),
    represents. It is the covariance matrix where the *i^(th)* and *j^(th)* entry
    is [![](img/738b1a8c-7eed-4664-ade4-8d27f27cfb9c.png)].
  prefs: []
  type: TYPE_NORMAL
- en: In the case where the covariances are 0, which implies the variables are independent,
    then [![](img/99cb769c-ab24-46e6-aecd-f1fede83389e.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Bivariate normal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When *n = 2* in the multivariate normal distribution, this is a special case
    known as the **bivariate normal**. Its covariance matrix is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e12e12ad-1541-44fd-8e9b-d5bcb164b869.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The inverse of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b2ce8d-f9cd-4553-b232-901decd81a02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the correlation between the two variables becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ef43a72-df2e-4d22-b047-fc3ebb7186b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the sake of simplicity, we will assume the mean is 0, so the joint PDF
    of the bivariate normal is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64738400-590a-4a68-b0cd-5eabede9aa71.png)'
  prefs: []
  type: TYPE_IMG
- en: Gamma distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gamma distribution is a widely used distribution to model positive continuous
    variables with skewed distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gamma distribution is denoted by [![](img/517d5ba1-6a78-43f1-9c50-259a3c6c63ef.png)] and
    has the following PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08cab1d3-8add-449d-9b48-94277d4304e1.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, we conclude our section on probability. We will now start exploring
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Essential concepts in statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While probability allows us to measure and calculate the odds of events or outcomes
    occurring, statistics allows us to make judgments and decisions given data generated
    by some unknown probability model. We use the data to learn the properties of
    the underlying probabilistic model. We call this process parametric inference.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In estimation, our objective is given *n* iid samples with the same distribution
    as *X* (the probability model). If the PDF and **probability mass function** (**PMF**) is [![](img/ae2cef0a-5beb-48e1-a5f1-b357c299efdd.png)],
    we need to find θ.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we define a statistic as an estimate of θ.
  prefs: []
  type: TYPE_NORMAL
- en: A statistic is a function, *T*, of the data, [![](img/a159163d-ddda-4dcc-9526-8d8033b60adb.png)],
    so that our estimate is [![](img/8a3e68cf-e6cb-43c1-806f-38b119a91fec.png)]. Therefore,
    *T(x)* is the sampling distribution of the statistic and an estimator of θ.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, *X* will denote a random variable and *x* will denote an observed
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have [![](img/daf3f330-a78a-4f17-b227-d2a95cf5ccc6.png)], which
    are iid [![](img/f518720d-8500-4abd-9a4a-a38158e29e02.png)]. Then, a possible
    estimation for μ is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62bf0b56-be84-44d0-b66f-3f13039485e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, our estimate for a particular observed sample, ![](img/56509ab4-3bbd-48e5-8829-05aa83bd2c87.png),
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea2ae08f-b460-4fcc-9f51-008a2e5460df.png)'
  prefs: []
  type: TYPE_IMG
- en: A method we use to determine whether or not our estimator is good is the bias.
    The bias is defined as the difference between the true value and the expected
    value and is written as [![](img/bf858bc5-6b59-4958-8d56-71d76ada0425.png)]. The
    estimator is unbiased if [![](img/c8594a74-3ed4-4b1f-a404-5a020d51b76f.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The** mean squared error** (**MSE**) is a measure of how good an estimator
    is and is a better indicator of this than the bias. We write it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c9bb100-93e9-4e4f-bd5b-1bf00e13898e.png)'
  prefs: []
  type: TYPE_IMG
- en: However, sometimes, we use the root MSE, which is the square root of the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also express the MSE in terms of bias and variance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c5ec3d2-a3ff-4d65-ba22-cedc58161600.png)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, when we are trying to get a low MSE, it is in our best interest to
    have a biased estimator with low variance. We call this the **bias-variance trade-off**.
  prefs: []
  type: TYPE_NORMAL
- en: Sufficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the time, the purpose of conducting our experiments is to find the
    value of θ and to find the bigger picture. A sufficient statistic is one that
    gives us all the information we want about θ.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lucky for us, the factorization theorem gives us the ability to find sufficient
    statistics. It states that *T* is sufficient for θ if we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79a178a9-82b0-4df0-ac68-a81db320635f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *g* and *h* are arbitrary functions.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if *T* is a sufficient statistic, then it does not lose any information
    about θ and the best statistic is the one that gives us the maximal reduction.
    We call this the minimal sufficient statistic; in its definition, *T(X)* is minimal
    if—and only if—it is a function of every other statistic. So, if *T'(X)* is sufficient,
    then *T'(X) = T'(Y) ⇒ T(X) = T(Y)*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose *T = T(X)* is a statistic that satisfies [![](img/5917f1c3-e8b3-416f-bd2f-1aae73357098.png)],
    which does not depend on ![](img/0d2b7a06-a42c-42d8-b01a-d6739fd0df24.png) if
    (and only if) [![](img/ee76df88-3853-4587-bd53-cd079be5662e.png)]. Then, *T* is
    minimally sufficient for θ.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, let''s say we have [![](img/96b42b02-2996-49c5-8dda-1246af243e19.png)],
    which are iid [![](img/b4cd8933-cd49-43b9-b1b7-bf12021c9ea3.png)]. Then, we can
    deduce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/133755f0-1aac-4542-9864-7e5039f719d2.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a constant function that tells us that [![](img/783f2304-eb07-484a-98c9-24de5561fd53.png)] and [![](img/948fb818-3420-4e69-8358-c836dd80365d.png)].
    Therefore, [![](img/a02a5ed6-6067-4dcf-b6ae-14ffa5e4068f.png)] is minimally sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of minimally sufficient statistics is that they give us the ability
    to store our experiments' results in the most efficient way and we can use them
    to improve our estimator.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the Rao-Blackwell theorem, which states that if ![](img/db6ddaf1-2663-4333-a87d-637de9c065a9.png) is
    a sufficient statistic for *θ*, and if [![](img/a108d990-0094-4925-a68f-a37bdcf669a8.png)] is
    an estimator of θ—where for all θ, [![](img/3bdfcaa3-a418-430c-a307-40296fe59e89.png)].
    Let [![](img/35a206e6-4978-406d-bb04-bbc1490146e6.png)]—then, for all cases of
    θ, we have [![](img/7311f9c9-5d24-416b-9796-a6cc5ca7f449.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, in practice, when we want to determine whether or not our estimator
    is good, we usually use the **maximum likelihood estimator** (**MLE**).
  prefs: []
  type: TYPE_NORMAL
- en: Given *n* random variables (with [![](img/cd38b80b-487a-484f-ada8-1fddf247ef63.png)] being
    the joint PDF), then if *X* = *x*, the likelihood of θ is defined as [![](img/c9f407f0-3452-40cf-a527-7a4cb7f6a840.png)].
    Therefore, the MLE of θ is an estimate of the value of θ that maximizes [![](img/0ee0e32d-136d-4f00-925e-b1171a35aaed.png)].
  prefs: []
  type: TYPE_NORMAL
- en: In practice, however, we maximize the log-likelihood instead of simply the likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the example of having *n* iid random variables with the [![](img/39ab6250-e262-4cb7-851d-ba7c61c7dd5c.png)] PDF, the
    likelihood and log-likelihood are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4199dc56-b225-46c1-9ddd-c97bbb09fe0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose our *n* variables are Bernoulli *(p)*. Then, [![](img/3625b2c6-7b7b-43fb-a470-118a60af38eb.png)].
    Therefore, [![](img/ad2a5be1-766b-4e0a-8685-ad6e2e08df1d.png)] when [![](img/f3e522ff-9961-433c-a58a-a09743fa40d4.png)] is
    equal to 0 and is an unbiased MLE.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you're probably wondering what exactly the MLE has to do with sufficiency.
    If *T* is sufficient for θ, then its likelihood is [![](img/36758fea-8e6d-4b75-b8d2-3ac7cc1c1477.png)] and
    to maximize our estimate, we have to maximize *g*. Therefore, the MLE is a function
    of the sufficient statistic—voila!
  prefs: []
  type: TYPE_NORMAL
- en: Confidence intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confidence interval gives us the ability to determine the probability that
    certain intervals contain θ. We formally define it as follows.
  prefs: []
  type: TYPE_NORMAL
- en: A [![](img/681ddc42-f012-4a68-b1af-69eebfb41dcf.png)] confidence interval for
    θ is a random interval [![](img/35662868-9e85-4d24-91bb-ce235a60c5a1.png)], such
    that [![](img/9be406c6-7923-4112-9901-14fcd25dc2f4.png)], regardless of the true
    value of θ.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we calculate [![](img/b7b6d010-e660-4ac0-beeb-ba22b45b0df1.png)] for
    a number of samples, *x*. Then, 100γ% of them will cover our true value of θ.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we have [![](img/336d3b2a-848c-433a-91f2-72e188d88ea2.png)], which are
    iid [![](img/95b73743-00c8-4468-8e6c-08ee0d06a391.png)], and we want to find a
    95% confidence interval for θ. We know that [![](img/afe448f0-f0cb-44cf-b5f6-e5527e674e45.png)] so
    that [![](img/3816503b-9384-4341-a0dd-152fd78e9b48.png)]. Then, we choose *z[1],
    z[2]*, such that [![](img/635c0c2d-2cf1-4047-b029-5ef99a37eed4.png)], where *Φ* is
    the normal distribution. So, ![](img/14036c42-8a8e-4d7f-9328-3adc2a948ff4.png),
    from which we get the following confidence interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe4b4fd6-51a1-4fb2-9a17-c446d83809e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a three-step approach that is commonly used to find confidence intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: Find [![](img/35755383-e8d4-4592-be2c-6b4a1408934e.png)], such that the [![](img/8a4308b7-adaf-4688-9fd2-cc383d29121f.png)] of [![](img/dd9a5964-7bb7-4ad3-a478-09e17182bd07.png)] isn't
    dependent on θ. We call this a **pivot**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Above [![](img/e296bbe4-70ea-4fc5-be23-06bd2f89bdd4.png)], write the probability
    statement in the [![](img/ebc4503c-bac3-42a3-8e73-3b90d065f43d.png)] form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rearrange the inequalities to find the interval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Usually, *c[1]* and* c[2]* are percentage points from a known distribution;
    for example, for a 95% confidence interval, we would have the 2.5% and 97.5% points.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this section on statistics, we have dealt with what is known as the
    frequentist approach. Now, however, we will look at what is known as the Bayesian
    approach, where we treat θ as a random variable, we tend to have prior knowledge
    about the distribution, and, after collecting some additional data, we find the
    posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we define the prior distribution as a probability distribution of
    θ before collecting any additional data; we denote this as π(θ). The posterior
    distribution is the probability distribution of θ dependent on the outcome of
    our conducted experiment; we denote this as π(θ|**x**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between the prior and posterior distributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e762a4cc-1ac5-4755-8824-d3ab137a5ea9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally, we avoid calculating [![](img/9395c0c8-e981-4be2-8458-e38143bb8c98.png)] and
    we only observe the relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d66e796-d136-45ce-a578-fbf4afaadb36.png)'
  prefs: []
  type: TYPE_IMG
- en: We can read this as [![](img/74de7aaa-f594-410d-bbc6-9b2767b8a3b3.png)].
  prefs: []
  type: TYPE_NORMAL
- en: After conducting our experiments and coming up with the posterior, we need to
    determine an estimator, but to find the best estimator, we need a loss function,
    such as quadratic loss or absolute error loss, to see how far off the true value
    of θ is from our estimated value of a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose the parameter we are estimating is *b*. Then, the Bayes estimator, [![](img/6353d454-a2aa-4bfa-9a35-713cfa57d6ec.png)],
    minimizes the expected posterior loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c064708e-a605-48fe-b9ff-759a32df49c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we choose our loss function to be a quadratic loss, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b834307-84b1-417d-8e55-b4e1f71c5741.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, if we choose an absolute error loss, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49fb6cf7-2784-4171-abc0-e23b383eab1b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the posterior distribution is our true distribution, we know that by
    integrating over it, our result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ee0ddd1-21fd-472d-afbd-237329a5c0cf.png)'
  prefs: []
  type: TYPE_IMG
- en: If you're wondering how these two schools of statistics compare, think of frequentist
    versus Bayesian as absolute versus relative, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In statistics, we usually have to test out hypotheses and most likely, we will
    compare two different hypotheses—the null hypothesis and the alternative hypothesis.
    The null hypothesis tells us that our experiment contains statistical significance;
    that is, that no relationship is observed between the variables. The alternative
    hypothesis tells us that there is a relationship between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we start with the assumption that the null hypothesis is true, and
    to reject this, we need to find evidence through our experiments that contradict
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Simple hypotheses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple hypothesis *H* is one in which the parameters of the distribution are
    completely specified, otherwise it is called a composite hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: When testing the null hypothesis (*H[0]*) against the alternative hypothesis
    (*H[1]*), we use our test to divide ![](img/82552f43-137b-4265-be68-844864a4cff1.png) into
    two regions *C* and [![](img/abe39872-54f7-4e0b-a2aa-ca18ce448641.png)]. If [![](img/4c2ec251-ab81-483f-b8d0-07a756bfdcaa.png)],
    then we reject the null hypothesis, but if [![](img/e919c215-066e-4b12-82e8-3b8c450dc3e8.png)] then
    we do not reject the null hypothesis. We call *C* the critical region.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we perform tests, we hope to arrive at the correct conclusion, but we
    could make either of the following two errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error 1**: rejecting *H[0]* when *H[0]* is true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error 2**: not rejecting *H[0]* when *H[0]* is false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *H[0]* and *H[1]* are both simple hypotheses, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11f0c189-4377-408a-bc9d-4b168590bb5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, α is the size of our test and 1-β is the power of the test to find *H[1]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a simple hypothesis, ![](img/0c4f64dc-79ba-4632-b984-4d312944fd9e.png),
    then we also want to find its likelihood given *x*. We do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bc50d6b-9756-4804-ace3-7df858919bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also find the likelihood ratio of *H[0]* and *H[1 ]*given *x*as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d198ad53-94d2-45e0-bf88-7c431c81f7ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A likelihood ratio test is where, given *k*, the critical region is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c22a7637-2624-4853-9740-48b821e72e96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go through an example and develop a bit of intuition about this. Say
    we have [![](img/cd11ba88-9c52-4908-96c9-9949498a0168.png)], which are iid [![](img/63ca4aac-27dc-4692-8c12-66f6e44b807f.png)],
    and [![](img/60b63337-d86d-4251-943c-46a0f8759010.png)] is a known quantity. Now,
    we want to find out what the best test size for our null hypothesis, [![](img/c9937c97-b93b-4cfa-a006-c6e329820742.png)],
    is versus the alternative hypothesis, [![](img/ccc6f54f-ce88-4734-8f97-aea1ab769306.png)].
    Let''s assume that ![](img/f6ca7aa5-a778-45b8-88c7-fafde0072f14.png) and ![](img/53716cc1-40ca-4a63-ae09-698a9405961b.png) are
    known to us as well, such that ![](img/1375019f-9b75-43f2-9e1b-014403651c7c.png).
    Therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a7735fe-30ac-464f-a9e9-f01a1d32cfd8.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that this function is increasing, so [![](img/b13e47c4-7554-40c9-ba8e-fe34d8fd0648.png)] for
    any case of *k*, which tells us that for some arbitrary values of *c*, ![](img/02ddd37a-721a-4352-8334-3075514fb4a9.png).
  prefs: []
  type: TYPE_NORMAL
- en: We choose our value of ![](img/dc98ae99-744e-4ee4-9ecd-6b2848a6840c.png) so
    that [![](img/3eb7536f-5213-40e6-b75c-550aa4a81aaf.png)], and if ![](img/dd522a17-5624-43da-8a0d-3b76c397c0cc.png),
    we reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the null hypothesis, [![](img/fe127bd0-6b64-476d-946e-1193deeca1b4.png)];
    so, [![](img/e668e5a7-9178-45ce-9290-5dcb3353df9d.png)]. Now, since ![](img/660eb9eb-f940-450e-9762-256b5cea13b3.png),
    the test size rejects the null hypothesis if we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57823923-6850-4e07-89e5-dac1414e4e70.png)'
  prefs: []
  type: TYPE_IMG
- en: This is known as the *z*-test and we use it to test a hypothesis, while the
    z-score tells us how many standard deviations away from the mean our data point
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the likelihood ratio rejects the null hypothesis if *z > k*. The test
    size is [![](img/6e3a330d-7de5-4e1a-92f8-9b2c1a2fccea.png)], which decreases as
    *k* increases. The value of *z* is in the rejection region if [![](img/b8b93e75-b736-48ca-be20-4f0e33113171.png)].
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding equation, *p^**, is known as the *p*-value of the data, *x*; it
    is,in other words, the probability of observing data (evidence) against the null
    hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Composite hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, if we have a composite hypothesis, such as [![](img/8b35701f-e0c6-4d19-bcd2-6e80c38f4738.png)],
    the error probabilities are not singular-valued.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we define the power function, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c198677c-2b7f-407c-877a-fff0277edb72.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would like *W*(*θ*) to be small on the null hypothesis and large
    on the alternative hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: The size of the test is [![](img/0629f5ba-0683-4dfe-b103-05452e24bfab.png)], which
    is not an ideal size. Given [![](img/9ab50740-e94c-4137-99c0-5b6b5a09709e.png)], [![](img/34421f0b-e3c2-4fb8-abcf-d1e494f84515.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we saw that the best size of the test of *H[0]* versus *H[1]* is
    given by the following critical region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ca10f44-d854-46ef-9802-96a41a72af55.png)'
  prefs: []
  type: TYPE_IMG
- en: This depends on [![](img/f3b75cd5-4cdc-468d-9483-e7302993d0f5.png)] and [![](img/3e6bfced-2e42-4c0c-944e-022e7ffe5854.png)],
    but not the value of [![](img/c7abc6a3-460c-4012-b9f1-3656ed0db4cc.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We call a test, which is specified by *C*, the uniformly most powerful size, ![](img/87a6c215-5a0b-4030-9883-a9d67f2def82.png),
    of the [![](img/54025d5d-5f05-4d8d-987e-47649a29d910.png)] test versus [![](img/2e70d22d-6b4f-43ad-be0a-e15606eaf717.png)],
    but only if the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/9e9c392e-ebc9-42f2-a4a2-9fb47bc5c02a.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/7ffe3029-4743-4483-bd17-9e26f41e4102.png)] for all cases of [![](img/ce01ff2b-1efa-4023-bb0e-796d2194679d.png)] if [![](img/fa862a2f-e488-44b1-88a8-9d5de3f95f75.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, as before, we want to find the likelihood of a composite hypothesis, [![](img/2363d223-0bc5-4c72-80fc-48469a2e188e.png)],
    given some data, *x*. We do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f08d4b63-4866-4f79-9055-ee74b3358819.png)'
  prefs: []
  type: TYPE_IMG
- en: The multivariate normal theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have so far dealt with random variables or a vector of iid
    random variables. Now, let's suppose we have a random vector, [![](img/34165953-146a-4a7e-bdbc-cb54b61fffaa.png)],
    where the *X[i]* values are all correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to find the mean of *X*, we do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91724f93-d33f-4252-a0b4-d7c2c6a83a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If it exists, the covariance matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a03239c7-fe1b-4243-9c94-1c3456f01951.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, if we have [![](img/8a3d1ee9-b059-4ba1-b14b-e3e0e77cd8f8.png)],
    then [![](img/b9a771ad-d7ed-4b25-893d-02668aad631b.png)] and [![](img/bb633a7b-f2db-4e49-a496-530c64d1eeec.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'If we''re dealing with two random vectors, then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/6999b581-2c03-486c-8233-c1438a8ffb3d.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/03569a0c-49a8-412b-a383-dd37bd97cc35.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's define what a multivariate normal distribution is.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a random vector, *X*. It has a multivariate normal distribution
    if for [![](img/221ff3e8-3a7e-4d6a-8722-a585a321c39b.png)], [![](img/ae41ef31-efdc-4ab9-8897-adbc11c19a7f.png)] has
    a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: If [![](img/9b329d48-78e6-43ca-a217-39089a82f43f.png)] and [![](img/eb305975-3426-4745-ab7a-3e0916025c61.png)],
    then [![](img/6fa9183f-6f70-4e8c-bfa6-0754ba3e399c.png)], where [![](img/abf8c575-074b-4690-a372-058e15ca0f1d.png)] is
    a symmetric and positive semi-definite matrix, because [![](img/9d1a222f-08b8-44c4-afd3-a78c3ae2f1f1.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may be wondering what the PDF of a multivariate normal distribution
    looks like. We'll get there momentarily.
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose [![](img/47df59d1-ee7a-432e-b11b-284d529339c9.png)] and we split
    ![](img/bd94950c-d5d9-447f-be25-bc21830b3c3d.png) into two smaller random vectors,
    such that [![](img/ebad2aef-2228-4204-ad90-31553a6e01d7.png)], where [![](img/241be2f3-74a7-41d1-9e4f-03ea72fcbe9c.png)] and [![](img/e56a6648-0fe9-4d0b-969c-ce63cb6b7462.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, [![](img/b4ae1084-2fda-42bf-8e4c-e52cfb87c9c8.png)] and [![](img/b1eaf2cd-ccfc-4fd8-b5c4-f14731f259cc.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/05f56368-5db1-43cd-b78e-7e4aeecdba89.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X**[1] and **X**[2] are independent if [![](img/0bd91a15-be9c-459b-b263-e3c80451049d.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When [![](img/90d12d00-c683-4797-b74b-b345320de67c.png)] is positive semi-definite,
    then *X* has the following PDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8fbf7b2-8fdb-4ccc-92cf-4160b0855de4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the dimension of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have [![](img/aa0ff384-401a-43ce-9ffd-1404a285509e.png)], which
    are iid [![](img/eacccc89-e573-47ed-93f6-728c9cdd6dbf.png),] and [![](img/5bee595f-7f18-4fff-b5b6-8cea70a4d729.png)] and
    [![](img/cfaf9d3e-ea1e-453d-8cd2-2462cb4ee042.png)]. Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/80e49a1d-e173-4898-909c-10488ade217c.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/16f5fedf-b5cf-4ddf-b009-47d2e9b4324b.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* and *S[xx]* are independent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In statistics, we use linear models to model the relationship between a dependent
    variable and one or more predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s suppose we have *n* observations *Y[i]* and *p* predictors
    *x[j]*, where *n > p*. We can write each observation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df12e5e9-4892-4246-88d5-557b45da72a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For all cases of [![](img/207301fd-7f26-455f-a11c-cc34a23f14a2.png)], we can
    assume the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/bb2ff12d-a21e-43aa-ba6e-c4c8020438f5.png)] are the unknown, fixed
    parameters that we want to figure out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/95ce8668-c2fa-444f-9c45-5f50812501e6.png)] are the values of the *p* predictors
    for the *i^(th)* response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/e4172a8a-7db1-4370-85ce-560dbdaf26a6.png)] are independent random
    variables with 0 mean and σ² variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We generally consider [![](img/ade791f9-ea58-47c9-9d4f-42c2280954c4.png)] to
    be casual effects of *x[ij]* and *ε[i]* to be a random error term. Therefore, [![](img/faf454cf-3be7-4619-99c1-cc9f90255594.png)], [![](img/ad690f9b-55d0-4bb7-8330-f76049c10f54.png)] and [![](img/c8fe0611-b6b0-4965-95eb-5f56d07ee718.png)] are
    independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all the data, we may want to plot a straight line on the data, so a possible
    model could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff72d58f-5925-4d99-a129-1e12d3f0ef51.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *a* and *b* are constants.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the preceding model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85985358-f49c-42bf-8c96-476705fa4364.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the expanded form is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9db67e29-19cb-437e-ad70-7bf1d51cc39f.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, [![](img/1ea56a87-6168-4feb-9809-fbc4b6577d39.png)] and [![](img/561e8418-62a2-41e1-891c-acf5d40aec5b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The least-squares estimator, [![](img/f4cabe0c-ce00-4585-a0fe-26ed010b1157.png)],
    of [![](img/a54cfaf6-bf6a-430c-9113-6ddd5ad39ded.png)] minimizes our linear model
    by minimizing the square of the vertical distance between the line and the points,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a41ba500-0f5d-42c5-827d-047520831ed2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To minimize it, we apply the following for all cases of *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0901e9c0-ac52-4b26-824a-256b6cf425cb.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we have [![](img/c17c7155-da0b-402d-a277-8f7636c5b07a.png)] and so, [![](img/2818372d-b08c-410b-bbf6-fd58a08db06c.png)] for
    all cases of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: By putting the preceding function into matrix form, as we did earlier, we get [![](img/4ede1bec-bd6a-4a52-8f53-a2624335127b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: We know that [![](img/3d4a8efc-89f0-45bb-adcb-5f87718c4f3c.png)] is positive,
    semi-definite, and has an inverse. Therefore, [![](img/c0249ae1-e0e5-484b-a157-06fa293a0dbb.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Under normal assumptions, our least-squares estimator is the same as the MLE.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49994f38-c0e3-4edf-93e2-ff48207a1418.png)'
  prefs: []
  type: TYPE_IMG
- en: This tells us that our estimator is unbiased and [![](img/d9b090a4-a01a-4f10-90cb-86ea17972736.png)].
  prefs: []
  type: TYPE_NORMAL
- en: I know what you're thinking—that was intense! Good job on making it this far;
    we are very close to finishing this chapter, so hang in there.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In hypothesis testing, our goal is to ascertain whether certain variables influence
    the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Let's test a hypothesis of a general linear model. Suppose we have [![](img/ad70ebb2-7402-4603-b8ce-ab6805402f97.png)] and [![](img/d081a397-ef2f-4689-a2af-79870d4c9eac.png)].
    We would like to test [![](img/b4e69d79-5f1b-499c-ab9a-d1a0b0214fcb.png)] against [![](img/3f8e9abf-d6f6-4974-8290-932ff3883f53.png)] and [![](img/8301d233-ae79-4496-81cb-6690dd9885c7.png)],
    since under *H[0]*, [![](img/e1efcc5e-4253-4549-b74f-63ec5ba0fdd9.png)] vanishes.
  prefs: []
  type: TYPE_NORMAL
- en: Under the null hypothesis, the maximum likelihood of *β[0]* and *σ²* are [![](img/ff2c91c9-3f27-443a-9768-0a39ba305872.png)] and [![](img/f09971f6-e977-4270-babe-63267e0689a5.png), ]which,
    as we know from earlier, are independent.
  prefs: []
  type: TYPE_NORMAL
- en: The estimators of the null hypothesis wear two hats instead of one and the alternative
    hypothesis has one.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have officially completed this chapter and you have now developed
    a solid intuition for probability and statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned a lot of concepts. I recommend going through the
    chapter again if needed because the topics in this chapter are very important
    to gaining a deep understanding of deep learning. Many of you may be wondering
    what the chapters you have learned so far have to do with neural networks; we
    will tie it all together in a couple more chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter focuses on both convex and non-convex optimization methods
    and builds the foundation for understanding the optimization algorithms used in
    training neural networks.
  prefs: []
  type: TYPE_NORMAL
