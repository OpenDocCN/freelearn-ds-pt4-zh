<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Optimization</h1>
                </header>
            
            <article>
                
<p class="mce-root">Optimization is a branch of applied mathematics that has applications in a multitude of fields, such as physics, engineering, economics, and so on, and is of vital importance in developing and training of deep neural networks. In this chapter, a lot of what we covered in previous chapters will be very relevant, particularly linear algebra and calculus.</p>
<p>As we know, deep neural networks are developed on computers and are, therefore, expressed mathematically. M<span>ore often than not, t</span>raining deep learning models comes down to finding the correct (or as close to the correct) set of parameters. We will learn more about this as we progress further through this book.</p>
<p>In this chapter, we'll mainly learn about two types of continuous optimization—constrained and unconstrained. However, we will also briefly touch on other forms of optimization, such as genetic algorithms, particle swarm optimization, and simulated annealing. Along the way, we will also learn when and how to use each of these techniques.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Understanding optimization and it's different types</li>
<li>Exploring the various optimization methods</li>
<li>Exploring population methods</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding optimization and it's different types</h1>
                </header>
            
            <article>
                
<p><span>In optimization, our goal is to either minimize or maximize a function. For example, a business wants to minimize its costs while maximizing its profits or a shopper might want to get as much as possible while spending as little as possible. Therefore, the goal of optimization is to find the best case of </span><img class="fm-editor-equation" src="Images/3e222295-5f36-49fc-b8e7-f78d7cbb5e36.png" style="width:2.83em;height:1.00em;"/><span>, which is denoted by <em>x<sup>*</sup></em></span> (where <em>x</em> is a set of points),<span> that satisfies certain criteria. These criteria are, for our purposes, mathematical functions known as <strong>objective functions</strong>.</span></p>
<p>For example, let's suppose we have the <sub><img class="fm-editor-equation" src="Images/a43eb391-efdc-4743-86c1-4440f7b229fb.png" style="width:15.25em;height:1.42em;"/></sub><span> </span><span>equation. </span><span>If we plot it, we get the following</span> graph<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1016 image-border" src="Images/f8646f12-1773-48b4-9095-f1e923f17847.png" style="width:29.42em;height:20.42em;"/></p>
<p>You will recall from <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>, that we can find the gradient of a function by taking its derivative, equating it to 0, and solving for <em>x</em>. We can find the point(s) at which the function has a minimum or maximum, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5961dece-f3e3-42d3-b35e-ee3c05da7850.png" style="width:12.50em;height:2.25em;"/></p>
<p>After solving this equation, we find that it has three distinct solutions (that is, three points where the minima and maxima occur). </p>
<p>To find which of these three solutions are the minima and maxima, we find the second derivative, <sub><img class="fm-editor-equation" src="Images/bcfe75c6-d2ba-4c56-a73e-782316ce5048.png" style="width:9.50em;height:2.25em;"/></sub>, and check whether our stationary points are positive or negative.</p>
<p>Visually, when we see the graph, we can identify the local and global minima, but it isn't as simple as this when we calculate it computationally. So, instead, we start at a value and follow the gradient until we get to the minima (hopefully, the global minima). </p>
<p>Say we start from the right side at <em>x </em>= <em>2</em>. The gradient is negative, which means we move to the left incrementally (these increments are called <strong>step size</strong>) and we get to the local minima, which isn't the one we want to find. However, if we start at <em>x </em>= -<em>2</em>, then we end up at the global minima. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Constrained optimization</h1>
                </header>
            
            <article>
                
<p>Constrained optimization, in general, has certain rules or constraints attached that must be followed. In general, the problem is defined in the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/38e4bb82-504f-4304-850e-df6ab4043796.png" style="width:23.00em;height:1.25em;"/></p>
<p>In the preceding equation, <img class="fm-editor-equation" src="Images/b0caee31-eba0-4c04-8308-d6b60e0d03f1.png" style="width:3.42em;height:1.00em;"/> contains the decision variables, <sub><img class="fm-editor-equation" src="Images/7164aca9-e847-46ab-82bf-c274593ccdbc.png" style="width:5.00em;height:1.17em;"/></sub> is our objective function, <sub><img class="fm-editor-equation" src="Images/a4ddd95d-2b71-4e99-a2d0-fe0128fd0037.png" style="width:6.33em;height:1.00em;"/></sub> and <img class="fm-editor-equation" src="Images/20549c15-3f7f-4667-bff1-48cfaa6f6524.png" style="width:3.50em;height:1.00em;"/> are the functional constraints, while <sub><img class="fm-editor-equation" src="Images/2a7b4195-d394-418c-8833-04f5cc1b3f89.png" style="width:3.67em;height:1.08em;"/></sub> is the regional constraint.</p>
<div class="mce-root packt_infobox">All of these variables are vectors; in fact, all of the variables in this chapter will be vectors, so for simplification, we will not be writing them in boldface as we did previously, in <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>, and <a href="6a34798f-db83-4a32-9222-06ba717fc809.xhtml">Chapter 2</a>, <em>Linear Algebra</em>. </div>
<p>Sometimes, our constraints could be in the form of an inequality, such as <sub><img class="fm-editor-equation" src="Images/89e125d8-972e-47cf-b9b7-d5f9c1185efb.png" style="width:4.17em;height:1.33em;"/></sub>, and we can add in a slack variable, <em>z</em>, which now makes our functional constraint <sub><img class="fm-editor-equation" src="Images/9fa2fd6e-7f91-49b9-974a-7325fd386946.png" style="width:6.00em;height:1.33em;"/></sub> and the regional constraint <em>z ≥ 0</em>.</p>
<p>We could simply write out all the constraints explicitly, but that's just too messy. We generally write them as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c1b8e2a7-df9f-4a15-9c9c-ecf39eb04931.png" style="width:21.00em;height:1.42em;"/></p>
<p>This is the general form of a linear program. The standard form, however, is written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3353ae3b-19be-47df-8c80-ee53bdee815c.png" style="width:21.00em;height:1.42em;"/></p>
<p>I know this may all seem very unclear right now, but don't fear—we will make sense of all of it soon. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Unconstrained optimization</h1>
                </header>
            
            <article>
                
<p>The goal of optimization problems is to minimize <em>f(x)</em>, and we will primarily be dealing with functions that are twice differentiable and where <sub><img class="fm-editor-equation" src="Images/6e2c031b-260a-449e-9a05-67bdbcf4f6b5.png" style="width:5.42em;height:1.25em;"/></sub>. A rather important property to be aware of is that since <em>f</em> is differentiable and convex, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/96c4a8d2-9ed0-4c87-884d-c3be361f804e.png" style="width:5.25em;height:1.25em;"/></p>
<p class="mce-root">This should be apparent if you remember what we learned in <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>.</p>
<p>Unconstrained optimization, as you can probably tell, is the case in which we do not have any constraints whatsoever and any point could be a minimum, maximum, or a saddle point, which doesn't make the problem easy.</p>
<p><span>Let's suppose we have a problem with <em>n</em></span><span> equations and <em>n</em></span><span> variables. Solving this and finding the optimal solution isn't simple, and we generally solve the problem iteratively. Think of this as computing a set sequence of points in the domain of</span> <em>f</em><span>, which gradually gets us to the optima. </span></p>
<p>Now, say we have a function, <sub><img class="fm-editor-equation" src="Images/e887acf6-10cc-4770-9a52-873c549bcd9f.png" style="width:5.00em;height:1.17em;"/></sub>, and <img class="fm-editor-equation" src="Images/90e72e61-e77d-4137-94f1-a437b66d7e5b.png" style="width:3.67em;height:1.00em;"/>, such that <sub><img class="fm-editor-equation" src="Images/0daaddad-4c1a-4c3a-910e-0daead88e939.png" style="width:4.58em;height:1.08em;"/></sub>. The problem now looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b4b1bdff-8536-4309-9df1-7a2ba6231b56.png" style="width:12.83em;height:2.58em;"/></p>
<p>Here, we have <sub><img class="fm-editor-equation" src="Images/b733309e-a4ba-4ba7-acfc-1eaefa417be5.png" style="width:1.58em;height:1.25em;"/></sub>, which we know from previous chapters is the gradient of <em>f</em>. </p>
<p><span>Naturally, to start computing these points, we need a starting point, which we call the initial point, and it must lie within the domain of <em>f</em></span><span>. Then, we iterate and find better points from there until we find the optimal one.</span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convex optimization</h1>
                </header>
            
            <article>
                
<p>Convex optimization concerns minimizing a convex function over a convex set. In general, it takes the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/617cc53f-c166-49c0-9b8e-109f5bc45d2c.png" style="width:24.25em;height:1.33em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/e285ab0c-4a1f-4104-b887-f43bc22d062b.png" style="width:9.50em;height:1.17em;"/></sub> are convex functions and so they satisfy the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e7d109ff-18d8-4688-a0d2-b9a736723a84.png" style="width:15.58em;height:1.42em;"/></p>
<p>This is the case when <sub><img class="fm-editor-equation" src="Images/b8989821-4430-4087-a36d-82b6d05599b9.png" style="width:4.42em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/f61e1754-4fb8-4ff7-b900-e8b5763f9cf1.png" style="width:5.08em;height:1.25em;"/></sub> are non-negative and <sub><img class="fm-editor-equation" src="Images/46b6fb29-b1a3-472a-8a1c-2ef169b59a8e.png" style="width:4.67em;height:1.17em;"/></sub>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convex sets</h1>
                </header>
            
            <article>
                
<p><span>In optimization, we come across the terms convex and non-convex fairly often.</span></p>
<p><span>We define a convex set as one where if we were to take any two random points and draw a line to join them, the line would lie completely within the boundaries of the set. </span></p>
<p>We label our convex set <sub><img class="fm-editor-equation" src="Images/c05c4837-b88a-47ae-bc8f-1edb1666dc1d.png" style="width:3.08em;height:1.00em;"/></sub> and if we have two points, <sub><img class="fm-editor-equation" src="Images/720e4010-3faa-4d96-ab3e-5ca967ab0170.png" style="width:3.58em;height:1.25em;"/></sub> and some scalar <sub><img class="fm-editor-equation" src="Images/24646c89-6d96-4680-b027-57ffd9dff50e.png" style="width:4.42em;height:1.08em;"/></sub> value, then <img class="fm-editor-equation" src="Images/9f6936b2-a543-41f8-8ac4-1474b4f24bd0.png" style="width:7.83em;height:1.25em;"/>.</p>
<p>Now, let's suppose we have the <sub><img class="fm-editor-equation" src="Images/0ba6e809-a9d9-4dce-8573-5c971f3ecbc7.png" style="width:8.00em;height:1.25em;"/></sub><span> </span><span>function.</span><span> Then, if </span><em>θ </em><span>= </span><em>0</em><span>,</span> <em>f </em><span>= </span><em>y</em><span>; but if </span><span><em>θ </em>= <em>1</em></span><span>, then</span> <em>f </em><span>= </span><em>x</em><span>. From this, we can tell that as <em>θ</em> increases,</span> <em>f</em><span> moves gradually from</span> <em>y</em><span> to</span> <em>x</em><span>.</span></p>
<p><span>A function, </span><sub><img class="fm-editor-equation" src="Images/261c3458-9094-4653-81fe-5fe00504816b.png" style="width:5.08em;height:1.33em;"/></sub><span>, is convex if <em>S</em></span><span> is convex for all cases of </span><sub><img class="fm-editor-equation" src="Images/c7694f92-3edf-4144-996a-6c64ae1115e0.png" style="width:3.50em;height:1.17em;"/></sub><span> and </span><sub><img class="fm-editor-equation" src="Images/9722a716-64e8-40d7-a81a-55c40946ab15.png" style="width:3.92em;height:1.25em;"/></sub><span>. We then have </span><sub><img class="fm-editor-equation" src="Images/32d46d71-f9c5-4ee5-a028-193b68162e8b.png" style="width:18.58em;height:1.33em;"/></sub><span>.</span></p>
<p>Additionally, if we have <sub><img class="fm-editor-equation" src="Images/3b24f04a-6e8e-465a-89d2-07fd53de064b.png" style="width:4.58em;height:1.25em;"/></sub>, where the domain of the function is the convex set for all cases of <sub><img class="fm-editor-equation" src="Images/58611348-d5eb-43d7-ba78-0ce672ed94bc.png" style="width:3.42em;height:1.17em;"/></sub>, then <sub><img class="fm-editor-equation" src="Images/5e1fa71b-5172-4f67-84b8-a31c0e534867.png" style="width:18.67em;height:1.33em;"/></sub>.</p>
<p>To aid us in visualizing a convex function, we have the following diagram, where we can see that it looks almost like a bowl and that all the points within the bowl are points in the convex set:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1036 image-border" src="Images/243305a4-c4ad-4527-b159-3bcd670269e2.png" style="width:20.75em;height:16.00em;"/></p>
<p>Now, let's suppose our function can be differentiated twice. Then, <em>f</em> is convex on a convex region and we can define our Hessian matrix as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5ced4b33-92ff-45fd-81eb-d5eed913a677.png" style="width:6.17em;height:2.33em;"/></p>
<p>This is positive semi-definite for all cases of <img class="fm-editor-equation" src="Images/f6c1440a-9f9a-4854-b629-be27ec7ce8b2.png" style="width:2.50em;height:0.92em;"/>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Affine sets</h1>
                </header>
            
            <article>
                
<p>If we have a <img class="fm-editor-equation" src="Images/b31ca2d7-8a28-440d-a9dd-d1d189695d74.png" style="width:3.08em;height:0.92em;"/> <span> </span><span>set, </span><span>it is affine if the line connecting our two points in</span> <img style="font-size: 1em;width:0.67em;height:1.08em;" class="fm-editor-equation" src="Images/e6d29ef2-3f6b-4c6e-a821-41cd646e7e42.png"/><span> lies in</span> <img style="font-size: 1em;width:0.58em;height:1.00em;" class="fm-editor-equation" src="Images/09f97db9-7438-435c-9976-3fde21d6206c.png"/><span>; that is, this space contains a linear combination of the points in</span> <img style="font-size: 1em;width:0.75em;height:1.25em;" class="fm-editor-equation" src="Images/24bfae59-46ca-44b6-8775-2484de5530cb.png"/><span>, but only if the sum of the coefficients is equ</span>al to 1 <span>so that </span><span><sub><img class="fm-editor-equation" src="Images/87aab3a5-ac23-4def-85cc-a35fe03adfed.png" style="width:4.58em;height:1.17em;"/></sub></span><span>, </span><span><sub><img class="fm-editor-equation" src="Images/f1ef638a-aabb-4381-b785-0e921798d34f.png" style="width:3.00em;height:1.08em;"/></sub></span><span> and </span><span><sub><img class="fm-editor-equation" src="Images/81958148-fc61-442a-955f-d4dfc247302b.png" style="width:8.25em;height:1.17em;"/></sub></span><span>.</span></p>
<p>Additionally, if we have more than two points, then <sub><img class="fm-editor-equation" src="Images/2ed96c4c-ac66-404c-95b7-c965ac5e6b39.png" style="width:19.50em;height:1.58em;"/></sub> is an affine combination of <em>n</em> points, given the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ac09bd0f-5404-4c97-9bd8-300eadd416f4.png" style="width:4.67em;height:3.17em;"/></p>
<p>Also, if <img class="fm-editor-equation" src="Images/a9aa04b3-8235-4e3e-9936-54cd8be900b1.png" style="width:0.50em;height:0.83em;"/> is an affine set and we have a <sub><img class="fm-editor-equation" src="Images/fbd3ad9e-84df-4a47-b00e-344e0d5ef1e5.png" style="width:2.83em;height:1.08em;"/></sub><span> </span><span>point, </span><span>then we have the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/55b11d6f-1111-4527-bee5-190ffa236178.png" style="width:13.67em;height:1.25em;"/></p>
<p>This is a subspace of <img class="fm-editor-equation" src="Images/57233124-1946-43aa-bf30-833d46b3f50a.png" style="width:0.50em;height:0.83em;"/>. </p>
<p>Now, suppose we have some <sub><img class="fm-editor-equation" src="Images/87659a7a-26e2-41cb-8f44-7d4840932b71.png" style="width:4.83em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/f60e07cc-3406-487b-b656-702c03a4f656.png" style="width:3.92em;height:1.17em;"/></sub><span> </span><span>points</span><span>. From earlier, we know that</span> <sub><img class="fm-editor-equation" src="Images/ae1e1af3-b81a-481d-a4bb-e94eabd8e448.png" style="width:5.75em;height:1.25em;"/></sub><span> and</span> <sub><img class="fm-editor-equation" src="Images/846c0c58-4bcd-4f33-9c6f-e1670545499d.png" style="width:5.42em;height:1.17em;"/></sub><span>. Therefore, we can express</span> <img style="font-size: 1em;width:0.67em;height:1.08em;" class="fm-editor-equation" src="Images/6e005216-d152-4fe0-a3f8-f400cd4270b9.png"/><span> as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1bc5e4e9-61b1-4c8b-b472-05b32def8259.png" style="width:14.50em;height:1.33em;"/></p>
<p>In general, we call the set of all combinations of points in <img class="fm-editor-equation" src="Images/f1f48a0e-0b60-47e5-957b-47c7410033b1.png" style="width:0.67em;height:1.08em;"/> the affine hull of <img class="fm-editor-equation" src="Images/e010d8ad-0ccb-429f-ab9f-872760dd2dfe.png" style="width:0.67em;height:1.08em;"/>.</p>
<p class="CDPAlignLeft CDPAlign">Let's <span>now</span><span> </span><span>assume that we have a unit sphere in</span> <img style="font-size: 1em;width:1.17em;height:1.08em;" class="fm-editor-equation" src="Images/f30c5049-69a7-46cb-b0fd-26cb7afc2596.png"/><span> where</span> <em>x</em><span> is its center,</span> <em>r</em><span> is the radius, and</span> <sub><img class="fm-editor-equation" src="Images/99bf9568-e1db-4d77-9445-2ff2e226de01.png" style="width:10.83em;height:1.50em;"/></sub><span>. The relative interior of</span> <img style="font-size: 1em;width:0.67em;height:1.08em;" class="fm-editor-equation" src="Images/0f0f2afd-e435-4dcc-9370-1e99a49d0b73.png"/><span>, where the dimension of</span> <img style="font-size: 1em;width:3.08em;height:0.92em;" class="fm-editor-equation" src="Images/9989f0fb-45e6-4430-a025-d2151797ee4b.png"/><span> is less than</span> <em>n</em><span>, is defined as the </span><sub><img class="fm-editor-equation" src="Images/ea052682-bc68-430b-8caa-4ca4f5f610b6.png" style="width:16.58em;height:1.25em;"/></sub><span> </span><span>set, </span><span>where</span> <sub><img class="fm-editor-equation" src="Images/71050a5c-018e-4d4a-a1c8-d4281b973023.png" style="width:13.08em;height:1.33em;"/></sub><span>.</span></p>
<p>Then, the relative boundary of  <img class="fm-editor-equation" src="Images/8e3cf85d-faf0-4087-a762-109bd22884f7.png" style="width:0.58em;height:1.00em;"/> is defined as the difference between the closure of  <img class="fm-editor-equation" src="Images/807b275e-06ac-4d8e-8c26-e5e04667539b.png" style="width:0.58em;height:1.00em;"/> and the relative interior of <img class="fm-editor-equation" src="Images/8b6f6e29-c46b-415e-a937-66e9a5a0e092.png" style="width:0.58em;height:1.00em;"/>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convex functions</h1>
                </header>
            
            <article>
                
<p>A convex function is defined as a <sub><img class="fm-editor-equation" src="Images/e4cab5f6-ec5d-4fc6-973e-7501ad655009.png" style="width:5.42em;height:1.25em;"/></sub> <span> </span><span>function </span><span>if its domain is a convex set and if for </span><em>x</em><span>,</span> <sub><img class="fm-editor-equation" src="Images/0b82b4e6-3472-4b2a-b235-5f743677a3c7.png" style="width:4.83em;height:1.25em;"/></sub><span> and</span> <sub><img class="fm-editor-equation" src="Images/414d8821-0649-4d8e-8527-0961307cf686.png" style="width:4.75em;height:1.17em;"/></sub><span>, which gives us the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/12fb8fb7-308e-4a5b-9ebe-c6e65acb5737.png" style="width:19.83em;height:1.42em;"/></p>
<p>Let's visualize this inequality with the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1017 image-border" src="Images/955b131d-f913-4f5a-a023-6200b5504c0e.png" style="width:25.17em;height:17.25em;"/></p>
<p>The line that connects the two points is above the function, which tells us that it is convex. However, the function is concave when it is -<em>f</em> and is convex otherwise.</p>
<p>Affine functions, on the other hand, have equality and are, therefore, concave and convex. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Optimization problems</h1>
                </header>
            
            <article>
                
<p>We can recall from earlier in this chapter that the optimization problem can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f9575709-0e71-440d-8bdd-e280e15f04fb.png" style="width:20.50em;height:1.33em;"/></p>
<p>The optimal value of our problem is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/24ec5f1d-f26b-44ec-a17e-b10ea4ec05e3.png" style="width:13.83em;height:1.33em;"/></p>
<p>We call <em>x<sup>*</sup></em> an optimal point (or solution to our problem) if <sub><img class="fm-editor-equation" src="Images/03a1b1d2-66b1-41df-ae4d-e9fea8dc1a6e.png" style="width:5.67em;height:1.33em;"/></sub>. Therefore, the optimal set containing all the optimal points is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/66cceb19-535a-46b5-9390-0debe5346716.png" style="width:19.17em;height:1.33em;"/></p>
<p>In convex optimization, there is a rather major property that states that any point that is locally optimal is also globally optimal. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Non-convex optimization </h1>
                </header>
            
            <article>
                
<p>In convex optimization, we deal with having to find a local optimum, which also happens to be the global minimum. However, in non-convex optimization, we have to find the global minimum, which isn't the local minimum; in fact, there could be more than one local minimum, as well as saddle points.</p>
<p>This makes non-convex optimization far more challenging than convex optimization. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring the various optimization methods</h1>
                </header>
            
            <article>
                
<p>Now that you know what optimization is, it's time to explore some of the methods used in practice. We will not be covering the entire field of optimization because that would require an entire book to cover. We will only cover the essential optimization methods that are applicable to deep learning. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Least squares</h1>
                </header>
            
            <article>
                
<p>Least squares is a subclass of convex optimization. It is classified as having no constraints and takes the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/57b89537-b10b-4e6f-a183-8647707b2a4d.png" style="width:22.67em;height:3.50em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/1b466f43-d90e-4adf-bfa1-6acf5f829700.png" style="width:4.58em;height:1.25em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/7960cb35-6307-40bc-ae94-aa999d68919f.png" style="width:1.33em;height:1.50em;"/></sub> are rows of <em>A</em>, and <img class="fm-editor-equation" src="Images/1de284b2-be3b-4bd6-902d-82a40c284be0.png" style="width:3.42em;height:1.00em;"/> is our optimization variable.</p>
<p>We can also express this as a set of linear equations of the <sub><img class="fm-editor-equation" src="Images/bb9c9ef7-5180-402c-a58a-0c247a009c2b.png" style="width:6.75em;height:1.42em;"/></sub><span> </span><span>form. </span><span>Therefore, </span><sub><img class="fm-editor-equation" src="Images/49cc707f-9547-48ff-bb62-a5171fa45bc8.png" style="width:11.00em;height:1.42em;"/></sub><span>.</span></p>
<p>The problem of least squares is very similar to that of maximum likelihood estimation. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lagrange multipliers</h1>
                </header>
            
            <article>
                
<p>When solving constrained optimization problems, it is best to include the constraints in the objective function. This way, anything that is not included in the constraints is not considered a minimum.</p>
<p>Let's revisit our earlier problem:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0faa4e4d-5404-42d7-8216-82fc036851f5.png" style="width:18.83em;height:1.25em;"/></p>
<p>We'll call our constraint <em>C</em>.</p>
<p>So, we define the Lagrangian of <em>C</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7fe1b202-2d3b-4b99-b823-674b2f436e5d.png" style="width:14.25em;height:1.50em;"/></p>
<p>Here, <img class="fm-editor-equation" src="Images/34b89d9d-e947-45fe-8630-6aa56c043a58.png" style="width:3.33em;height:0.92em;"/> and is known as the <strong>Lagrange multiplier</strong>.</p>
<p>When our constraint is satisfied, then <sub><img class="fm-editor-equation" src="Images/56663a20-d766-4a44-ac7c-c2b1e9cfcb87.png" style="width:5.33em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/9fabb59b-5ee3-4f50-b7e7-22731db68787.png" style="width:5.75em;height:1.08em;"/></sub>. By minimizing <em>L</em> over <em>x</em> and λ, we find the solution with respect to the constraints.</p>
<p>Suppose we have <img class="fm-editor-equation" src="Images/ed341058-10d3-4606-b545-264057b205c1.png" style="width:3.25em;height:1.00em;"/> and <img class="fm-editor-equation" src="Images/345f1fab-16c7-45fc-8990-5590ed98da77.png" style="width:3.83em;height:0.92em;"/>, such that we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/824e9d0e-0a0d-4c09-8ebb-5969fc5e7d86.png" style="width:21.00em;height:2.08em;"/></p>
<p>Then, <em>x<sup>*</sup></em> is optimal for <em>C</em>; that is, it minimizes <em>f</em>. This is called <strong>Lagrangian sufficiency</strong>. </p>
<p>To find λ<sup>*</sup> and <em>x<sup>*</sup></em>, we must solve the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e1582730-9cd5-48bf-bec8-9c7f433eaac5.png" style="width:4.83em;height:1.17em;"/> and <img class="fm-editor-equation" src="Images/2ab61b4a-8c1d-44f1-afcb-4c9dd4c35449.png" style="width:4.42em;height:1.42em;"/>.</p>
<p>For example, say we want to minimize <sub><img class="fm-editor-equation" src="Images/32c59e92-7e79-465b-bee9-4c0572d92de2.png" style="width:6.42em;height:1.08em;"/></sub> subject to <sub><img class="fm-editor-equation" src="Images/e4564403-bee3-40be-82f6-544ec526a501.png" style="width:7.83em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/bedccddf-8b17-4a29-bf49-6068e6e551f1.png" style="width:4.92em;height:1.25em;"/></sub>.</p>
<p>So, the equation for Lagrangian sufficiency is as follows:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1213 image-border" src="Images/db3a1b74-5ec6-4886-b403-e4a95aeb2b19.png" style="width:36.17em;height:1.67em;"/></p>
<p>We can rewrite this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/894ee79f-0ded-4dc2-b569-82bd916e1a52.png" style="width:35.33em;height:1.33em;"/></p>
<p>We also need to pick a <span>λ</span><sup>* </sup>and <em>x<sup>*</sup></em> value so that <em>L</em>(<em>x<sup>*</sup>, <span>λ</span><sup>*</sup></em>) is minimal. So, for <span>λ</span><sup>*</sup>, <em>L</em>(<em>x,<span>λ</span><sup>*</sup></em>) must have a finite minimum.</p>
<p>From the preceding equation, we know that <sub><img class="fm-editor-equation" src="Images/d1973200-084f-4928-9b04-92f9e34424e2.png" style="width:3.75em;height:0.83em;"/></sub> has a finite minimum at <sub><img class="fm-editor-equation" src="Images/5a174002-c832-4387-8ac7-db2d09fcf31f.png" style="width:2.83em;height:0.83em;"/></sub> and the <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> terms only have a finite minimum when <sub><img class="fm-editor-equation" src="Images/cd7c0978-9bc3-4f73-b521-a66a73654af1.png" style="width:3.33em;height:1.25em;"/></sub>.</p>
<p>Now, to find a minimum, we take the first derivatives and make them equal to 0, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/20881f89-fd82-44c0-a246-251bc64ab52f.png" style="width:17.00em;height:5.33em;"/></p>
<p>Since the first derivatives must be equal to 0, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/88197c13-42da-4a6d-a305-02ec24699398.png" style="width:4.83em;height:2.67em;"/>, <img class="fm-editor-equation" src="Images/8f9290ee-6789-4557-9288-b71779b1baa3.png" style="width:4.58em;height:2.50em;"/></p>
<p>To confirm that these are the minimum, we find the Hessian matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/78d527db-a928-4d06-a744-897d300de4d1.png" style="width:10.08em;height:2.50em;"/></p>
<p>As we would expect, this is positive semi-definite when <sub><img class="fm-editor-equation" src="Images/34972cc9-724b-4970-9156-0df5942666d5.png" style="width:3.08em;height:1.17em;"/></sub>. </p>
<p>The values of λ that we want are in the <sub><img class="fm-editor-equation" src="Images/82c94aa8-0ad8-4630-9137-f0ed7c6a0a62.png" style="width:14.58em;height:1.42em;"/></sub><span> </span><span>set, </span><span>which tells us that the unique minimum of </span><sub><img class="fm-editor-equation" src="Images/0c6a8ebe-d447-4da8-b8be-4f52a16c4fd7.png" style="width:3.50em;height:1.33em;"/></sub><span> is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c75077b2-748a-4d36-a3f9-0816a559e6e5.png" style="width:11.17em;height:2.75em;"/></p>
<p>All we have to do now is find the values of <em>λ</em> and <em>x</em> for <em>x</em>(<span>λ</span>) that satisfy the constraints.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Newton's method</h1>
                </header>
            
            <article>
                
<p>Newton's method is a second-order optimization method that rescales the gradients in all directions using the inverse of the corresponding eigenvalues of the Hessian. </p>
<p><span>As we know, we are trying to find the value of <em>x<sup>*</sup></em></span><span> that minimizes <em>f(x)</em></span><span> and satisfies <sub><img class="fm-editor-equation" src="Images/e9bcb20c-1acb-4c37-8ef7-6bb613d393ed.png" style="width:4.58em;height:1.08em;"/></sub></span><span>. Imagine that we are currently at a point, <em>x<sub>k</sub></em></span><span>, and we move to</span> <em>x<sub>k+1</sub></em><span>, which is</span><span> closer to <em>x<sup>*</sup></em></span>. We can write this step as <sub><img class="fm-editor-equation" src="Images/dc2066c6-c7ce-47e2-bed8-0489d0086ef7.png" style="width:8.75em;height:1.33em;"/></sub> (or <sub><img class="fm-editor-equation" src="Images/403534da-227e-48b6-9e54-e31c5b5751a6.png" style="width:8.25em;height:1.25em;"/></sub>). </p>
<p><span>The reason why Newton step works well is because it behaves well when </span><em>x</em><span> is near <em>x<sup>*</sup></em></span> since it<span> takes the steepest descent direction at <em>x</em></span><span>. However,</span> its performance is slow when we are at <em>x<sub>0</sub></em> because the second derivative at <em>x<sub>0</sub></em> does not give us reliable information about which direction we need to move in to reach <em>x<sup>*</sup></em>.</p>
<p><span>Now, let's suppose that </span><sub><img class="fm-editor-equation" src="Images/9d926883-fa35-47f7-9aa2-32745227cf4b.png" style="width:4.92em;height:1.25em;"/></sub><span>. Then, we have the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/cc2b9e7d-eb1a-4ac4-b4af-33877ae3ea93.png" style="width:17.67em;height:1.33em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/c9f7842e-6b1d-4883-87db-19f7d4da7519.png" style="width:12.58em;height:1.33em;"/></sub>.</p>
<p>We can rewrite this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/057d4594-ac8a-4f9e-8f85-be59d57effb5.png" style="width:12.33em;height:1.42em;"/></p>
<p>This is known as the Newton step. Therefore, at <em>x<sub>k</sub></em>, <em>x<sub>k+1</sub></em> minimizes the following quadratic function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1bd96824-6494-4a48-aa93-097152d97363.png" style="width:23.67em;height:2.17em;"/></p>
<p>We also know that <em>Hf(x)</em> is positive definite, which tells us <sub><img class="fm-editor-equation" src="Images/9634901f-8c85-41cb-b723-ae9033d24dcd.png" style="width:15.83em;height:1.17em;"/></sub>, unless <sub><img class="fm-editor-equation" src="Images/48d6bf17-bc1e-4f52-ac5d-4a4c5301a129.png" style="width:4.50em;height:1.17em;"/></sub>.</p>
<p>When we receive our new value, <em>x<sub>k+1</sub></em>, we can expect an error in it. This error is proportional to the square of the error in <em>x<sub>k</sub></em>. We can see this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d326693a-4c49-4758-a31a-4ac942cccdb5.png" style="width:11.92em;height:1.25em;"/></p>
<p>This leads to this method converging quadratically (speedily, but only as long as <em>x<sub>k</sub></em> is close to the optimal value).</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The secant method</h1>
                </header>
            
            <article>
                
<p>In Newton's method, we calculated the first and second derivatives, but calculating the Hessian in a large problem is not ideal. </p>
<p>Suppose we have a function, <sub><img class="fm-editor-equation" src="Images/485f8af8-d2f3-4953-a916-803ce127c619.png" style="width:8.50em;height:1.42em;"/></sub>, and <em>n = 50</em>. If we take the first derivative of <em>f</em>, with respect to each case of <em>x<sub>i</sub></em>, we get 50 equations. Now, if we calculate the second derivative, we have 2,500 equations, with respect to <em>x<sub>i</sub></em> and <em>x<span><sub>j</sub></span></em>, in a matrix. However, because Hessians are symmetric, we only really have to calculate 1,275 second derivatives. This is still a considerably large amount. </p>
<p>The secant method uses the Newton method, but instead of computing the second derivative, it estimates them using the first derivative, which makes it better suited to practice.</p>
<p>It approximates the second derivative as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f5ad01f4-7760-40ad-98ea-0e6847f36272.png" style="width:14.25em;height:2.58em;"/></p>
<p>We take this approximation and plug it into the Newton method, which gives us the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5f6c1680-7f83-432a-9da9-34094fedb65c.png" style="width:20.83em;height:2.92em;"/></p>
<p>While this does reduce the computational complexity, it suffers the same fate as Newton's method because it requires additional iterations to converge.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The quasi-Newton method</h1>
                </header>
            
            <article>
                
<p>The secant method approximated the second derivative, but the quasi-Newton method approximates the inverse of the Hessian. The steps are computed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/798eb06b-3128-450d-aa8a-0fbda4134281.png" style="width:12.17em;height:1.33em;"/></p>
<p>Here, <em>Q<sub>k</sub></em> is the approximated inverse of the Hessian at <em>x<sub>k</sub></em>.</p>
<p>We start by letting <em>Q<sub>1 </sub></em>= 1 and use two terms, α and β, to update the matrix at each iteration to aid in improving our estimation. They are defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ac775e6d-394a-44f9-bdcc-2b9387227238.png" style="width:13.17em;height:1.33em;"/> and <img class="fm-editor-equation" src="Images/a391149e-cbe9-4f5d-8b41-1294270c4579.png" style="width:9.25em;height:1.25em;"/></p>
<p>To update the matrix at each iteration, we make use of the <strong>Broyden-Fletcher-Goldfarb-Shanno</strong> (<strong><span>BFGS</span></strong>) method, which works as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a1a73000-b5cf-4aa5-ba36-89eb6ec6c039.png" style="width:31.50em;height:3.25em;"/></p>
<p>For minimization to work, <em>Q</em> must be positive definite. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Game theory</h1>
                </header>
            
            <article>
                
<p>Let's diverge to game theory for a bit. Games that consist of three or more players tend to be very challenging to solve, but two-player games are much simpler and are what we will focus on here.</p>
<p>Let's suppose we have two players that are represented by <sub><img class="fm-editor-equation" src="Images/c8555aa8-778c-4d41-8771-c7ef3a9830f4.png" style="width:5.50em;height:1.08em;"/></sub>, respectively, and they are playing rock paper scissors. As we know, in this game we tend to make decisions without any information about what the other player will choose. Each player naturally wants to win, so each player has the following payoff matrices:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/becec948-a8a0-4fe5-af6c-2c270422a7f0.png" style="width:21.00em;height:3.75em;"/></p>
<p>Personally, I am not the biggest fan of showing the payoff in this way because you have to write two matrices and look up the individual payoff each time. I prefer to write it in the following way:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr style="height: 32px">
<td style="height: 32px;width: 54.5625px" class="CDPAlignCenter CDPAlign"/>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p><strong>R</strong></p>
</td>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p><strong>P</strong></p>
</td>
<td style="height: 32px;width: 210.796875px" class="CDPAlignCenter CDPAlign">
<p><strong>S</strong></p>
</td>
</tr>
<tr style="height: 32px">
<td style="height: 32px;width: 54.5625px" class="CDPAlignCenter CDPAlign">
<p><strong>R</strong></p>
</td>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(0, 0)</p>
</td>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(-1, 1)</p>
</td>
<td style="height: 32px;width: 210.796875px" class="CDPAlignCenter CDPAlign">
<p>(1, -1)</p>
</td>
</tr>
<tr style="height: 32px">
<td style="height: 32px;width: 54.5625px" class="CDPAlignCenter CDPAlign">
<p><strong>P</strong></p>
</td>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(1, -1)</p>
</td>
<td style="height: 32px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(0, 0)</p>
</td>
<td style="height: 32px;width: 210.796875px" class="CDPAlignCenter CDPAlign">
<p>(-1, 1)</p>
</td>
</tr>
<tr style="height: 34.46875px">
<td style="height: 34.46875px;width: 54.5625px" class="CDPAlignCenter CDPAlign">
<p><strong>S</strong></p>
</td>
<td style="height: 34.46875px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(-1, 1)</p>
</td>
<td style="height: 34.46875px;width: 209.328125px" class="CDPAlignCenter CDPAlign">
<p>(1, -1)</p>
</td>
<td style="height: 34.46875px;width: 210.796875px" class="CDPAlignCenter CDPAlign">
<p>(0, 0)</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding table, player 1 chooses a row, <sub><img class="fm-editor-equation" src="Images/0c17fab6-9577-4fa6-a7c1-ab626bb152ed.png" style="width:6.08em;height:1.17em;"/></sub>, and player 2 chooses a column, <sub><img class="fm-editor-equation" src="Images/b0c5dc57-d3cb-4651-9476-f889b8d51c8e.png" style="width:5.83em;height:1.17em;"/></sub>. So, if we look at the preceding table, (-1, 1) tells us that player 1 lost and player 2 won.</p>
<p>In game theory, players have strategies that determine how they act or what actions they can take. </p>
<p>Player <em>X</em>, in our case, has the following set of strategies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c9655716-a652-4f63-8f49-d9b4f20e0ea2.png" style="width:15.67em;height:1.75em;"/></p>
<p>Player <em>Y</em> has the following set of strategies:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4c72aed7-05cd-456a-91da-f7802562d01a.png" style="width:15.83em;height:1.83em;"/></p>
<p>Here, each vector represents the probability of choosing each column or row.</p>
<p>Each case of <sub><img class="fm-editor-equation" src="Images/de1ec537-a985-405b-a59c-98f31802a8bd.png" style="width:6.67em;height:1.25em;"/></sub> represents a strategy profile, and we calculate the expected payoff for player <em>X</em> as <sub><img class="fm-editor-equation" src="Images/4223e048-5b89-402e-b7fe-5c46e7d742a8.png" style="width:7.33em;height:1.50em;"/></sub>. If, for some case of <em>i</em>, <em>x<sub>i </sub></em>= 1, then we always choose <em>i</em> and call <em>x</em> a <strong>pure strategy</strong>.</p>
<p>Let's move on to another well-known example—the <strong>prisoner's dilemma</strong>. Here, we have two people who commit a crime and are caught. They each have two choices that they can make—testify (<em>T</em>) or stay quiet (<em>Q</em>). </p>
<p>The following are the outcomes of the choices they can make:</p>
<ul>
<li>If they both keep quiet, they both end up in jail serving a 2-year sentence.</li>
<li>If one testifies and the other stays quiet, then the one who stays quiet ends up serving a 3-year sentence and the testifier is freed for cooperating with the police.</li>
<li>If they both testify, then they both serve a 5-year sentence.</li>
</ul>
<p>Our payoff table looks as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign">
<p><strong>S</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>T</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>S</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>(2, 2)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>(0, 3)</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>T</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>(3, 0)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>(1, 1)</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Naturally, each person wants to maximize their own payoff; note that neither of the two has the opportunity to know or discuss what the other is going to do, so colluding is not an option. Therefore, each person would prefer to testify since this option is strictly better. We call <em>T</em> a dominant strategy and (1, 1) is Pareto, dominated by (2, 2). </p>
<p>Let's suppose we have a game and a strategy profile (<em>x, y</em>), such that they are in equilibrium (where <em>x</em> is the best response to <em>y</em> and vice versa). Then, we define <img class="fm-editor-equation" src="Images/007884d9-2905-481e-aea0-3ad51a954c68.png" style="width:2.50em;height:0.83em;"/> as having the best response to <sub><img class="fm-editor-equation" src="Images/cc93028a-aedc-4603-98c4-578efdfe4db3.png" style="width:2.92em;height:1.25em;"/></sub>, if for all cases of <img class="fm-editor-equation" src="Images/e9ab4434-9dcc-4bf3-bab7-6f7491422d39.png" style="width:2.92em;height:1.00em;"/> we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/539b5840-f024-428c-ae93-9215e848e352.png" style="width:7.25em;height:1.25em;"/></p>
<p>Many of you will likely have heard the term zero-sum before, but for those of you haven't, it is a special game where the total payoff is 0, such that <sub><img class="fm-editor-equation" src="Images/bbceec62-804a-44a1-a40c-ebd28cfbcd87.png" style="width:5.17em;height:1.08em;"/></sub>. The earlier example of rock-paper-scissor is a good demonstration of this.</p>
<p>A very important solution to the two-player matrix game is the minimax theorem. Suppose we have a <sub><img class="fm-editor-equation" src="Images/94e995ea-4eb6-43f1-bde6-ae17cc74b5a6.png" style="width:5.00em;height:1.08em;"/></sub><span> </span><span>payoff matrix.</span><span> Then, we have the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ed611ad9-a52e-475c-b137-9da12ede3f24.png" style="width:15.17em;height:1.92em;"/></p>
<p>This states that if both players use the minimax strategy, then they are in equilibrium since this results in both player 1 and player 2 getting the worst payoff, which satisfies the criteria. This is quite similar to finding the optimal value of <sub><img class="fm-editor-equation" src="Images/39a0c42c-e60a-480f-bfbe-5fe47a0475b2.png" style="width:9.33em;height:1.67em;"/></sub>, subject to constraints, as in a linear program. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Descent methods</h1>
                </header>
            
            <article>
                
<p>Generally, descent methods take the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bd6e953e-c815-4c3a-83c9-2695c27a1ccf.png" style="width:10.42em;height:1.42em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/fdc594cc-721b-47af-9249-1641bb64234a.png" style="width:6.42em;height:1.08em;"/></sub>, and <sub><img class="fm-editor-equation" src="Images/71014de6-e392-4e10-89cc-c2c7af122bbc.png" style="width:2.67em;height:1.00em;"/></sub>. In the preceding algorithm, <em>k</em> is a sequence of steps, <em>x<sub>k</sub></em> is the optimal point, and <img class="fm-editor-equation" src="Images/7357c2be-114d-4c06-ba07-dbff575f048b.png" style="width:1.50em;height:0.92em;"/> is a step. The scalar value, <em>c<sub>k</sub></em>, is the size of the step at the <em>k<sup>th</sup></em> iteration.</p>
<p>In descent methods, <sub><img class="fm-editor-equation" src="Images/65830918-0eeb-42f0-83cd-8bc5a5e472ea.png" style="width:7.33em;height:1.25em;"/></sub>, except in the case where <em>x<sub>k</sub></em> is the optimal value, which tells us that <sub><img class="fm-editor-equation" src="Images/ccfa74c5-0839-4bd7-b331-cf5756775f01.png" style="width:5.08em;height:1.17em;"/></sub> for all cases of <em>k</em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article>
                
<p>Gradient descent is a widely used first-order optimization problem, and it takes steps in the direction of the negative of the gradient of the function from the point it is currently at until it eventually terminates at the optimal solution. </p>
<p>Imagine you're at a skateboarding park and you have a tennis ball in your hand. You bend down and place the ball on the surface of a ramp and let it go; gravity does its thing and the ball follows the ramp's curvature, finding its way to the bottom. This is the concept behind gradient descent.</p>
<p>In this case, the natural choice for the step is the negative gradient; that is, <sub><img class="fm-editor-equation" src="Images/2cb191ce-de37-4a7d-8213-3516637fd748.png" style="width:6.67em;height:1.25em;"/></sub>. This is known as <strong>gradient descent</strong>, which takes the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/29d1d41f-0ca4-4fd3-b214-30914dea5118.png" style="width:10.08em;height:1.25em;"/></p>
<p>In optimization, we generally define the stopping criteria as a condition that, when satisfied, should stop our algorithm from continuing to optimize. It usually takes the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7735d23d-1601-4df8-9331-58d0064b89d8.png" style="width:6.33em;height:1.25em;"/></p>
<p>Here, η is a small positive number. </p>
<p>We should remember, from the previous chapter, that if we have a function, <em>f</em>(<em>x, y</em>), then its gradient is <sub><img class="fm-editor-equation" src="Images/beb5c523-0bd6-4196-b675-22768cf354d5.png" style="width:6.67em;height:2.25em;"/></sub>. Therefore, we can compute the magnitude (or steepness) of the function at (<em>x, y</em>) as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ed63a23a-902f-411d-83ef-60baeda060e5.png" style="width:12.17em;height:3.08em;"/></p>
<p>This acts as a guide and tells us the direction that we should move at each step (since the curvature changes as we move downwards) to get to the minima.</p>
<p>However, gradient descent isn't perfect. It can be quite slow if the step size, <em>c<sup>(k)</sup></em>, is too small, and if the step size is too large, we may not reach the optimal point due to overshooting, which would result in our algorithm failing to converge, thus diverging instead.</p>
<p>To understand this better, let's take a look at the following two diagrams. The first diagram has a small step size and looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1018 image-border" src="Images/d0232347-0f57-44bb-a724-638332817422.png" style="width:31.42em;height:15.08em;"/></p>
<p>The second diagram shows a large step size:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1019 image-border" src="Images/be9d6a16-bcf1-4514-80da-51bce1372770.png" style="width:32.08em;height:15.42em;"/></p>
<p>As you can see, a good step size is important, and picking it isn't always an easy task. Luckily, there is a method known as <strong>adaptive step size</strong> that adjusts the step size after each iteration. It follows two rules:</p>
<ul>
<li>If the value of the function increases after a step—which means the step size was too large—then undo the step and decrease the step size.</li>
<li>If the value of the function decreases the size of the step, then increase the step size.</li>
</ul>
<p>Still, this isn't perfect either. As you can tell from the diagram, the optimization is somewhat erratic, and when we encounter more flat surfaces, our algorithm tends to slow down. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stochastic gradient descent</h1>
                </header>
            
            <article>
                
<p><span>By now, you should be able to tell that computing the gradient and getting to the optima isn't easy and is time-consuming.</span></p>
<p>This is why computing an approximation that points us in the same general direction <span>instead</span><span> </span><span>is useful. We call this method</span> <strong>stochastic Gradient Descent</strong> <span>(</span><strong>SGD</strong><span>), and it is a very important algorithm that theoretically guarantees convergence. The word <strong>stochastic</strong> comes from the fact that we do not know the precise value of the gradient, only an approximation of it.</span></p>
<p>Let's suppose we have <em>M</em> points <sub><img class="fm-editor-equation" src="Images/f6ad3959-1bb7-4bef-ac6b-f52f1431d3dc.png" style="width:7.33em;height:1.33em;"/></sub>, where <em>M</em> is very large. This becomes a big optimization problem. So, we take an objective function, <em>L</em>(<em>x</em>), which is a sum of the losses over the points. We write this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b7ea1f74-f73b-4d95-8aa8-99617aca1070.png" style="width:9.92em;height:3.42em;"/></p>
<p class="mce-root">Here, our goal is to minimize the loss as much as possible so that our model best fits the true function, <em>y</em>, as in regression. By minimizing the loss, we reduce the distance between our model's calculated point and the true point. </p>
<p>The reason we use this method is that when we have a lot of points or a large optimization problem, it can be very computationally infeasible to calculate the gradient at each point, even more so if we were to calculate the Hessian. This method is, on the other hand, a lot more computationally feasible.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loss functions</h1>
                </header>
            
            <article>
                
<p>We know that we are trying to approximate a function and we are trying to get as close as possible to the true function. To do this, we need to define a loss function—we have many to choose from. The following are the main ones that are used in practice:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/18e3c4af-4e08-462b-9487-fdbc47868572.png" style="width:11.17em;height:2.75em;"/></sub>, known as <strong>mean squared error</strong></li>
<li><sub><img class="fm-editor-equation" src="Images/92ae23a0-4a80-41bc-b62b-096e4074bfee.png" style="width:11.58em;height:2.92em;"/></sub>, known as <strong>mean absolute error</strong></li>
<li><sub><img class="fm-editor-equation" src="Images/2740f6ba-8d21-4a6a-927e-4a039f373f43.png" style="width:11.75em;height:3.00em;"/></sub>, known as <strong>square loss</strong></li>
<li><sub><img class="fm-editor-equation" src="Images/700d1816-3736-4e8a-91f4-556a2ac823ca.png" style="width:12.75em;height:2.92em;"/></sub>, known as <strong>hinge loss</strong></li>
<li><sub><img class="fm-editor-equation" src="Images/2ce94877-0693-48c6-a7f1-da2ea1b42a0f.png" style="width:19.67em;height:1.33em;"/></sub>, known as <strong>cross-entropy loss</strong></li>
<li><sub><img class="fm-editor-equation" src="Images/6efd84a2-e6d4-4c2d-b783-970bff2d0585.png" style="width:18.83em;height:3.25em;"/></sub>, known as <strong>Huber loss</strong></li>
</ul>
<p>We will revisit them later on and understand when it is best to use each one.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gradient descent with momentum</h1>
                </header>
            
            <article>
                
<p>As we have seen, gradient descent takes some time to find its way to a relatively flat surface. An improvement to the preceding example is gradient descent with momentum, which smoothes the gradient updates so that it is less erratic. Consider a tennis ball and a boulder <span>both</span><span> </span><span>rolling down a mountain. The tennis ball would bounce around more and likely get stuck, but the boulder would gain momentum as it goes and maintain a relatively straight path toward the bottom. That is the key idea behind this improvement. It does so by remembering the previous updates and each update is a combination of the previous and current gradients, as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6572ae6c-8ee4-49a3-a5d7-6853320b02cd.png" style="width:13.17em;height:1.17em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/e3e4f1a1-8a9f-4fb9-bf41-76cef419bd4a.png" style="width:8.25em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/6880e464-a804-4b46-852c-649fb51be543.png" style="width:3.83em;height:1.17em;"/></sub>.</p>
<p>In this method, as you will notice, we not only have to choose the step size, <em>c<sub>k</sub></em>, but also the momentum coefficient, <em>α</em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Nesterov's accelerated gradient</h1>
                </header>
            
            <article>
                
<p>While momentum dampens the oscillations of gradient descent, Nesterov's method allows the ball traveling down the slope to look ahead and calculate the gradient with respect to the future position. </p>
<p>In essence, instead of calculating the gradient at <em>x<sub>k</sub></em>, we use <sub><img class="fm-editor-equation" src="Images/e8029e10-cbab-44a0-a572-293b3453bb0f.png" style="width:6.75em;height:1.42em;"/></sub> (where <sub><img class="fm-editor-equation" src="Images/f18fca39-e776-49de-aaf8-93486da448d1.png" style="width:8.08em;height:1.17em;"/></sub>), which is close to where we would be after the next step. So, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/666cbfa9-a992-4ac2-9356-88af667666ac.png" style="width:17.83em;height:1.25em;"/></p>
<p>We could also combine the momentum update with Nesterov's accelerated gradient by making <em>γ</em> = <em>α</em>, which would give us <sub><img class="fm-editor-equation" src="Images/fbecbba8-da6b-4671-9008-a5c47e4b7317.png" style="width:9.92em;height:1.25em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/4cae029d-44ac-4a8e-82c8-1670ed5e8899.png" style="width:12.58em;height:1.25em;"/></sub>.</p>
<p>Here, as you will notice, we now have three parameters (<em>c</em>, <em>α</em>, and <em>γ</em>) instead of the two that we had in momentum.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adaptive gradient descent</h1>
                </header>
            
            <article>
                
<p>We briefly touched on adaptive step sizes earlier. These methods generally use the gradients from previous steps to guide the search direction and the step size to get us to convergence faster. The two main ones that we will look at are <strong>adaptive gradient</strong> (<strong>Adagrad</strong>) and <span><strong>adaptive moment estimation</strong> (</span><strong>Adam</strong>). </p>
<p>As before, our goal is to find <em>x<sup>*</sup></em>, which minimizes the loss function. </p>
<p>These gradient descent methods take the form of <sub><img class="fm-editor-equation" src="Images/23762e3b-c12e-43bd-accd-e1c492f78c1f.png" style="width:7.92em;height:1.17em;"/></sub>, where <em>G<sub>k</sub></em> is the gradient at the <em>k<sup>th</sup></em> step.</p>
<p>In the case of Adagrad, we have <sub><img class="fm-editor-equation" src="Images/35f9a6ac-b612-47c6-97e3-5b24640f7fcb.png" style="width:6.58em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/ddde34a3-f360-4682-a1e1-9136ab371742.png" style="width:11.67em;height:3.58em;"/></sub>, which, if we plug into the preceding equation, gives us the following: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f9f741ed-9707-4162-9eb5-0a79fa5910ac.png" style="width:18.75em;height:3.67em;"/></p>
<p>As you can see, we use the square root of the sum of the squares of the losses to update the step size at each step, which eliminates the need to do this ourselves.</p>
<p>Adam also keeps a history of the previous gradients, but it differs from Adagrad in that it stores an exponentially moving average of both the squared gradients and the gradients. </p>
<p>We write this as <sub><img class="fm-editor-equation" src="Images/9e4e7164-1d00-4652-b66c-9f3187bd4ee4.png" style="width:11.75em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/e9002cf3-1a66-4b95-a06d-0076f242de72.png" style="width:17.08em;height:3.67em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Simulated annealing</h1>
                </header>
            
            <article>
                
<p>Simulated annealing is inspired by the field of metallurgy, where we use heat to alter the properties of a material. The applied heat increases the energy of ions and moves more freely. As the material starts to cool, it takes on a different shape upon reaching its equilibrium state. The heat needs to be slowly and gradually reduced to avoid the material getting stuck in a metastable state, which represents a local minimum. </p>
<p>In our case, to optimize a problem, we use temperature to control stochasticity. When the temperature is high, this means the process is freely and randomly exploring the space with the hope that it comes across a good convex region with a more favorable minimum. By reducing the temperature, we reduce the stochasticity and make the algorithm converge to a minimum.</p>
<p>Simulated annealing is a non-convex optimization algorithm and is effective because of its ability to escape local minima. </p>
<p>At each iteration, we sample a possible step from a transition distribution, <em>T</em>, which is accepted according to the following probability:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/733ba40e-94f4-43d2-9329-dc715154237e.png" style="width:10.00em;height:3.00em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/91d15cb8-96fb-498c-bf70-b531e9718a19.png" style="width:10.67em;height:1.33em;"/></sub> and <img class="fm-editor-equation" src="Images/f28daa64-d175-458c-a51d-3fa71f73d7e5.png" style="width:0.58em;height:1.17em;"/> is the temperature. This probability is known as the <strong>Metropolis criterion</strong> and is what gives simulated annealing the ability to escape local minima when we have a high temperature.</p>
<p>To gradually bring the temperature down, we use a decay factor, <sub><img class="fm-editor-equation" src="Images/e35b2138-1090-4143-86dd-fe192b6727e2.png" style="width:4.17em;height:1.33em;"/></sub>, which looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2006443d-0556-4f37-9a8c-6e8a18371a3a.png" style="width:5.50em;height:1.25em;"/></p>
<p>The process continues until it meets the stopping criteria; that is, the temperature drops to the point where we see no improvements from <em>n<sub>k</sub></em> to <em>n<sub>k+1</sub></em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Natural evolution</h1>
                </header>
            
            <article>
                
<p>Natural evolution is a method that makes use of gradient descent, and our goal is to minimize <sub><img class="fm-editor-equation" src="Images/205c481b-b412-4d16-b476-874df8b803c8.png" style="width:6.92em;height:1.50em;"/></sub>. We estimate the gradient from the samples, as follows:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1280 image-border" src="Images/adaa8636-4694-4348-bf58-875a7753d43a.png" style="width:28.25em;height:9.33em;"/></p>
<p>Earlier, when looking at gradient descent, we needed to calculate the gradient of the objective function; but here, we work with the log likelihood, <sub><img class="fm-editor-equation" src="Images/b8303fc2-f8ea-467a-b043-69483e92f205.png" style="width:4.83em;height:1.25em;"/></sub>, and we can use this estimation of the gradient in any of the gradient descent methods we covered earlier to improve <em>θ</em>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring population methods</h1>
                </header>
            
            <article>
                
<p>So far, we have dealt with optimization problems where we have a <em>ball</em> or <em>particle</em> that we edge along the curved space gradually and move toward the minima using gradient descent or Newton's method. Now, however, we will take a look at another class of optimization, where we use a population of individuals. </p>
<p>We spread these individuals across the optimization space, which prevents the optimization algorithm from getting stuck at local minima or a saddle point. These individuals can share information with each other about the local area they're in and use this to find an optimal solution that minimizes our function. </p>
<p>With these algorithms, we have an initial population and we would like to distribute them so that we cover as much ground as we can to give us the best chance of finding a globally optimal region.</p>
<p>We can sample our population from a multivariate normal distribution that is entered over a region that we are interested in, or uniformly distribute the population under some constraints; however, these two distributions are only recommended if you want to limit the space your population covers. Alternatively, we can use <strong>Cauchy distribution</strong>, which allows us to cover a larger space.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Genetic algorithms</h1>
                </header>
            
            <article>
                
<p>Genetic algorithms are inspired by Darwinism, where a fitter individual passes on certain heritable characteristics to the next generation. The objective function, in this case, has an inverse relationship with the individual's fitness or ability to reproduce. The chromosomes from the fitter individuals in each generation are passed on to the subsequent generation after having been subjected to crossover and mutation.</p>
<p>The simplest way for us to represent a chromosome is by using a binary string, similar to how DNA is encoded. However, a better method is writing each chromosome as a vector in <img class="fm-editor-equation" src="Images/af6ea1fb-0cd0-400b-b83b-6198660a5911.png" style="width:1.50em;height:1.08em;"/> that represents a point in the optimization space. This allows us to express crossover and mutation with greater ease. </p>
<p>We start with a random population, and from it, we choose a set of chromosomes that will be the parents for the subsequent generation. If we have a population of <em>n</em> chromosomes, then we will select <em>n</em> parental pairs that will produce <em>n</em> children in the subsequent generation.</p>
<p>Our goal is to minimize the objective function. So, we sample <em>k</em> random individuals from the population and pick the top-performing individuals from each of the samples or with the probability of their performance relative to the population. The fitness, then, of each individual has an inverse relation to <sub><img class="fm-editor-equation" src="Images/d16b37b4-541c-4aad-9d1b-d56db8d563fa.png" style="width:5.83em;height:1.50em;"/></sub>, and we can calculate it using <sub><img class="fm-editor-equation" src="Images/98fa3db9-37d1-4156-a26b-c875e9ac6018.png" style="width:14.25em;height:1.50em;"/></sub>.</p>
<p>Crossover, on the other hand, is a combination of the chromosomes of the parents, which results in the children. There are a number of ways that this combination can occur, such as single-point crossover, two-point crossover, or uniform crossover, or we can use one of our own making. </p>
<p>In fitness and crossover, there are only so many traits that can be passed on from the initial population to subsequent generations. However, if only the best traits are passed on, we will end up with a saturated population, which isn't what we want. This is where mutations are useful. They allow new traits to be created and passed on, which enables individuals to explore more of the optimization space. After each crossover, each child in the population experiences some mutation, subject to a probability.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Particle swarm optimization </h1>
                </header>
            
            <article>
                
<p>This algorithm uses the concept of swarm intelligence, where you have a school of fish or a flock of birds. Let's suppose they are trying to find some food. They arrive at an area and spread out a bit, starting to look for food<span> </span><span>individually. W</span><span>hen one of them finds food, it lets the others know so that they can join in.</span></p>
<p>Each individual in the population knows its current position and velocity and <span>only</span><span> </span><span>keeps track of the previous </span><span>best</span><span> </span><span>positions it has visited. The velocity vector determines the direction of the search, and if the individual has a high velocity, then it has a more explorative character, whereas if it has a low velocity, it has a more exploitative character.</span></p>
<p>At the start of each iteration, the whole population is accelerated to the best position that any individual has come across so far. The updates are computed as follows:</p>
<p style="padding-left: 120px"><img class="aligncenter size-full wp-image-1214 image-border" src="Images/dc39b971-d525-47fe-aa0b-8e35460d6988.png" style="width:24.67em;height:3.25em;"/></p>
<p>Here, <em>x<sub>best</sub></em> is the best position found by the group as a whole, <sub><img class="fm-editor-equation" src="Images/53046c2c-9631-4b82-8d0a-2e128da57c7f.png" style="width:2.33em;height:1.83em;"/></sub> is the best position that an individual has found, <em>w</em>, <em>α<sub>1</sub></em>, and <em>α<sub>2</sub></em> are parameters, and <sub><img class="fm-editor-equation" src="Images/50b9c5eb-fd86-4247-a994-3f2c34ee65ee.png" style="width:6.42em;height:1.17em;"/></sub>. </p>
<p>The values of <em>c<sub>1</sub></em> and <em>c<sub>2</sub></em> heavily influence the rate at which they converge.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered a number of different kinds of optimization, such as convex and non-convex optimization, as well as what makes optimization such a challenging problem. We also had a look at how to define an optimization problem and explored a variety of methods, including population methods, simulated annealing, and gradient descent-based methods. In later chapters, we'll come to understand how optimization is used in deep learning and why it is such an important field for us to understand.</p>
<p>In the next chapter, we will learn about graph theory and its uses in the field to solve various problems.</p>


            </article>

            
        </section>
    </div></body></html>