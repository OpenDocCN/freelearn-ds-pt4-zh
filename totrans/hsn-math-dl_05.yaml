- en: Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization is a branch of applied mathematics that has applications in a multitude
    of fields, such as physics, engineering, economics, and so on, and is of vital
    importance in developing and training of deep neural networks. In this chapter,
    a lot of what we covered in previous chapters will be very relevant, particularly
    linear algebra and calculus.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, deep neural networks are developed on computers and are, therefore,
    expressed mathematically. More often than not, training deep learning models comes
    down to finding the correct (or as close to the correct) set of parameters. We
    will learn more about this as we progress further through this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll mainly learn about two types of continuous optimization—constrained
    and unconstrained. However, we will also briefly touch on other forms of optimization,
    such as genetic algorithms, particle swarm optimization, and simulated annealing.
    Along the way, we will also learn when and how to use each of these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding optimization and it's different types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the various optimization methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring population methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding optimization and it's different types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In optimization, our goal is to either minimize or maximize a function. For
    example, a business wants to minimize its costs while maximizing its profits or
    a shopper might want to get as much as possible while spending as little as possible.
    Therefore, the goal of optimization is to find the best case of ![](img/3e222295-5f36-49fc-b8e7-f78d7cbb5e36.png),
    which is denoted by *x^** (where *x* is a set of points), that satisfies certain
    criteria. These criteria are, for our purposes, mathematical functions known as
    **objective functions**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s suppose we have the [![](img/a43eb391-efdc-4743-86c1-4440f7b229fb.png)] equation. If
    we plot it, we get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8646f12-1773-48b4-9095-f1e923f17847.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will recall from [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*, that we can find the gradient of a function by taking its derivative,
    equating it to 0, and solving for *x*. We can find the point(s) at which the function
    has a minimum or maximum, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5961dece-f3e3-42d3-b35e-ee3c05da7850.png)'
  prefs: []
  type: TYPE_IMG
- en: After solving this equation, we find that it has three distinct solutions (that
    is, three points where the minima and maxima occur).
  prefs: []
  type: TYPE_NORMAL
- en: To find which of these three solutions are the minima and maxima, we find the
    second derivative, [![](img/bcfe75c6-d2ba-4c56-a73e-782316ce5048.png)], and check
    whether our stationary points are positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: Visually, when we see the graph, we can identify the local and global minima,
    but it isn't as simple as this when we calculate it computationally. So, instead,
    we start at a value and follow the gradient until we get to the minima (hopefully,
    the global minima).
  prefs: []
  type: TYPE_NORMAL
- en: Say we start from the right side at *x *= *2*. The gradient is negative, which
    means we move to the left incrementally (these increments are called **step size**)
    and we get to the local minima, which isn't the one we want to find. However,
    if we start at *x *= -*2*, then we end up at the global minima.
  prefs: []
  type: TYPE_NORMAL
- en: Constrained optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Constrained optimization, in general, has certain rules or constraints attached
    that must be followed. In general, the problem is defined in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38e4bb82-504f-4304-850e-df6ab4043796.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/b0caee31-eba0-4c04-8308-d6b60e0d03f1.png) contains
    the decision variables, [![](img/7164aca9-e847-46ab-82bf-c274593ccdbc.png)] is
    our objective function, [![](img/a4ddd95d-2b71-4e99-a2d0-fe0128fd0037.png)] and ![](img/20549c15-3f7f-4667-bff1-48cfaa6f6524.png) are
    the functional constraints, while [![](img/2a7b4195-d394-418c-8833-04f5cc1b3f89.png)] is
    the regional constraint.
  prefs: []
  type: TYPE_NORMAL
- en: All of these variables are vectors; in fact, all of the variables in this chapter
    will be vectors, so for simplification, we will not be writing them in boldface
    as we did previously, in [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*, and [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml),
    *Linear Algebra*.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, our constraints could be in the form of an inequality, such as [![](img/89e125d8-972e-47cf-b9b7-d5f9c1185efb.png)],
    and we can add in a slack variable, *z*, which now makes our functional constraint [![](img/9fa2fd6e-7f91-49b9-974a-7325fd386946.png)] and
    the regional constraint *z ≥ 0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could simply write out all the constraints explicitly, but that''s just
    too messy. We generally write them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1b8e2a7-df9f-4a15-9c9c-ecf39eb04931.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the general form of a linear program. The standard form, however, is
    written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3353ae3b-19be-47df-8c80-ee53bdee815c.png)'
  prefs: []
  type: TYPE_IMG
- en: I know this may all seem very unclear right now, but don't fear—we will make
    sense of all of it soon.
  prefs: []
  type: TYPE_NORMAL
- en: Unconstrained optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of optimization problems is to minimize *f(x)*, and we will primarily
    be dealing with functions that are twice differentiable and where [![](img/6e2c031b-260a-449e-9a05-67bdbcf4f6b5.png)].
    A rather important property to be aware of is that since *f* is differentiable
    and convex, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96c4a8d2-9ed0-4c87-884d-c3be361f804e.png)'
  prefs: []
  type: TYPE_IMG
- en: This should be apparent if you remember what we learned in [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*.
  prefs: []
  type: TYPE_NORMAL
- en: Unconstrained optimization, as you can probably tell, is the case in which we
    do not have any constraints whatsoever and any point could be a minimum, maximum,
    or a saddle point, which doesn't make the problem easy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we have a problem with *n* equations and *n* variables. Solving
    this and finding the optimal solution isn't simple, and we generally solve the
    problem iteratively. Think of this as computing a set sequence of points in the
    domain of *f*, which gradually gets us to the optima.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, say we have a function, [![](img/e887acf6-10cc-4770-9a52-873c549bcd9f.png)],
    and ![](img/90e72e61-e77d-4137-94f1-a437b66d7e5b.png), such that [![](img/0daaddad-4c1a-4c3a-910e-0daead88e939.png)]. The
    problem now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4b1bdff-8536-4309-9df1-7a2ba6231b56.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have [![](img/b733309e-a4ba-4ba7-acfc-1eaefa417be5.png)], which we
    know from previous chapters is the gradient of *f*.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, to start computing these points, we need a starting point, which
    we call the initial point, and it must lie within the domain of *f*. Then, we
    iterate and find better points from there until we find the optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: Convex optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convex optimization concerns minimizing a convex function over a convex set.
    In general, it takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/617cc53f-c166-49c0-9b8e-109f5bc45d2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, [![](img/e285ab0c-4a1f-4104-b887-f43bc22d062b.png)] are convex functions
    and so they satisfy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7d109ff-18d8-4688-a0d2-b9a736723a84.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the case when [![](img/b8989821-4430-4087-a36d-82b6d05599b9.png)] and
    [![](img/f61e1754-4fb8-4ff7-b900-e8b5763f9cf1.png)] are non-negative and [![](img/46b6fb29-b1a3-472a-8a1c-2ef169b59a8e.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Convex sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In optimization, we come across the terms convex and non-convex fairly often.
  prefs: []
  type: TYPE_NORMAL
- en: We define a convex set as one where if we were to take any two random points
    and draw a line to join them, the line would lie completely within the boundaries
    of the set.
  prefs: []
  type: TYPE_NORMAL
- en: We label our convex set [![](img/c05c4837-b88a-47ae-bc8f-1edb1666dc1d.png)] and
    if we have two points, [![](img/720e4010-3faa-4d96-ab3e-5ca967ab0170.png)] and
    some scalar [![](img/24646c89-6d96-4680-b027-57ffd9dff50e.png)] value, then ![](img/9f6936b2-a543-41f8-8ac4-1474b4f24bd0.png).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's suppose we have the [![](img/0ba6e809-a9d9-4dce-8573-5c971f3ecbc7.png)] function. Then,
    if *θ *= *0*, *f *= *y*; but if *θ *= *1*, then *f *= *x*. From this, we can tell
    that as *θ* increases, *f* moves gradually from *y* to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: A function, [![](img/261c3458-9094-4653-81fe-5fe00504816b.png)], is convex if
    *S* is convex for all cases of [![](img/c7694f92-3edf-4144-996a-6c64ae1115e0.png)] and [![](img/9722a716-64e8-40d7-a81a-55c40946ab15.png)].
    We then have [![](img/32d46d71-f9c5-4ee5-a028-193b68162e8b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if we have [![](img/3b24f04a-6e8e-465a-89d2-07fd53de064b.png)],
    where the domain of the function is the convex set for all cases of [![](img/58611348-d5eb-43d7-ba78-0ce672ed94bc.png)],
    then [![](img/5e1fa71b-5172-4f67-84b8-a31c0e534867.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To aid us in visualizing a convex function, we have the following diagram,
    where we can see that it looks almost like a bowl and that all the points within
    the bowl are points in the convex set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/243305a4-c4ad-4527-b159-3bcd670269e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s suppose our function can be differentiated twice. Then, *f* is
    convex on a convex region and we can define our Hessian matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ced4b33-92ff-45fd-81eb-d5eed913a677.png)'
  prefs: []
  type: TYPE_IMG
- en: This is positive semi-definite for all cases of ![](img/f6c1440a-9f9a-4854-b629-be27ec7ce8b2.png).
  prefs: []
  type: TYPE_NORMAL
- en: Affine sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have a ![](img/b31ca2d7-8a28-440d-a9dd-d1d189695d74.png)  set, it is affine
    if the line connecting our two points in ![](img/e6d29ef2-3f6b-4c6e-a821-41cd646e7e42.png) lies
    in ![](img/09f97db9-7438-435c-9976-3fde21d6206c.png); that is, this space contains
    a linear combination of the points in ![](img/24bfae59-46ca-44b6-8775-2484de5530cb.png),
    but only if the sum of the coefficients is equal to 1 so that [![](img/87aab3a5-ac23-4def-85cc-a35fe03adfed.png)], [![](img/f1ef638a-aabb-4381-b785-0e921798d34f.png)] and [![](img/81958148-fc61-442a-955f-d4dfc247302b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if we have more than two points, then [![](img/2ed96c4c-ac66-404c-95b7-c965ac5e6b39.png)] is
    an affine combination of *n* points, given the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac09bd0f-5404-4c97-9bd8-300eadd416f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, if ![](img/a9aa04b3-8235-4e3e-9936-54cd8be900b1.png) is an affine set
    and we have a [![](img/fbd3ad9e-84df-4a47-b00e-344e0d5ef1e5.png)] point, then
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55b11d6f-1111-4527-bee5-190ffa236178.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a subspace of ![](img/57233124-1946-43aa-bf30-833d46b3f50a.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose we have some [![](img/87659a7a-26e2-41cb-8f44-7d4840932b71.png)] and
    [![](img/f60e07cc-3406-487b-b656-702c03a4f656.png)] points. From earlier, we know
    that [![](img/ae1e1af3-b81a-481d-a4bb-e94eabd8e448.png)] and [![](img/846c0c58-4bcd-4f33-9c6f-e1670545499d.png)].
    Therefore, we can express ![](img/6e005216-d152-4fe0-a3f8-f400cd4270b9.png) as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bc5e4e9-61b1-4c8b-b472-05b32def8259.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, we call the set of all combinations of points in ![](img/f1f48a0e-0b60-47e5-957b-47c7410033b1.png) the
    affine hull of ![](img/e010d8ad-0ccb-429f-ab9f-872760dd2dfe.png).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now assume that we have a unit sphere in ![](img/f30c5049-69a7-46cb-b0fd-26cb7afc2596.png) where
    *x* is its center, *r* is the radius, and [![](img/99bf9568-e1db-4d77-9445-2ff2e226de01.png)]. The
    relative interior of ![](img/0f0f2afd-e435-4dcc-9370-1e99a49d0b73.png), where
    the dimension of ![](img/9989f0fb-45e6-4430-a025-d2151797ee4b.png) is less than
    *n*, is defined as the [![](img/ea052682-bc68-430b-8caa-4ca4f5f610b6.png)] set, where
    [![](img/71050a5c-018e-4d4a-a1c8-d4281b973023.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Then, the relative boundary of  ![](img/8e3cf85d-faf0-4087-a762-109bd22884f7.png) is
    defined as the difference between the closure of  ![](img/807b275e-06ac-4d8e-8c26-e5e04667539b.png) and
    the relative interior of ![](img/8b6f6e29-c46b-415e-a937-66e9a5a0e092.png).
  prefs: []
  type: TYPE_NORMAL
- en: Convex functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A convex function is defined as a [![](img/e4cab5f6-ec5d-4fc6-973e-7501ad655009.png)]  function if
    its domain is a convex set and if for *x*, [![](img/0b82b4e6-3472-4b2a-b235-5f743677a3c7.png)] and
    [![](img/414d8821-0649-4d8e-8527-0961307cf686.png)], which gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12fb8fb7-308e-4a5b-9ebe-c6e65acb5737.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize this inequality with the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/955b131d-f913-4f5a-a023-6200b5504c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: The line that connects the two points is above the function, which tells us
    that it is convex. However, the function is concave when it is -*f* and is convex
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Affine functions, on the other hand, have equality and are, therefore, concave
    and convex.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can recall from earlier in this chapter that the optimization problem can
    be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9575709-0e71-440d-8bdd-e280e15f04fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimal value of our problem is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ec5f1d-f26b-44ec-a17e-b10ea4ec05e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We call *x^** an optimal point (or solution to our problem) if [![](img/03a1b1d2-66b1-41df-ae4d-e9fea8dc1a6e.png)].
    Therefore, the optimal set containing all the optimal points is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66cceb19-535a-46b5-9390-0debe5346716.png)'
  prefs: []
  type: TYPE_IMG
- en: In convex optimization, there is a rather major property that states that any
    point that is locally optimal is also globally optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Non-convex optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In convex optimization, we deal with having to find a local optimum, which also
    happens to be the global minimum. However, in non-convex optimization, we have
    to find the global minimum, which isn't the local minimum; in fact, there could
    be more than one local minimum, as well as saddle points.
  prefs: []
  type: TYPE_NORMAL
- en: This makes non-convex optimization far more challenging than convex optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the various optimization methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know what optimization is, it's time to explore some of the methods
    used in practice. We will not be covering the entire field of optimization because
    that would require an entire book to cover. We will only cover the essential optimization
    methods that are applicable to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Least squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Least squares is a subclass of convex optimization. It is classified as having
    no constraints and takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57b89537-b10b-4e6f-a183-8647707b2a4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/1b466f43-d90e-4adf-bfa1-6acf5f829700.png)], [![](img/7960cb35-6307-40bc-ae94-aa999d68919f.png)] are
    rows of *A*, and ![](img/1de284b2-be3b-4bd6-902d-82a40c284be0.png) is our optimization
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: We can also express this as a set of linear equations of the [![](img/bb9c9ef7-5180-402c-a58a-0c247a009c2b.png)] form. Therefore, [![](img/49cc707f-9547-48ff-bb62-a5171fa45bc8.png)].
  prefs: []
  type: TYPE_NORMAL
- en: The problem of least squares is very similar to that of maximum likelihood estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Lagrange multipliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When solving constrained optimization problems, it is best to include the constraints
    in the objective function. This way, anything that is not included in the constraints
    is not considered a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit our earlier problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0faa4e4d-5404-42d7-8216-82fc036851f5.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll call our constraint *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we define the Lagrangian of *C* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fe1b202-2d3b-4b99-b823-674b2f436e5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/34b89d9d-e947-45fe-8630-6aa56c043a58.png) and is known as the
    **Lagrange multiplier**.
  prefs: []
  type: TYPE_NORMAL
- en: When our constraint is satisfied, then [![](img/56663a20-d766-4a44-ac7c-c2b1e9cfcb87.png)] and [![](img/9fabb59b-5ee3-4f50-b7e7-22731db68787.png)].
    By minimizing *L* over *x* and λ, we find the solution with respect to the constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have ![](img/ed341058-10d3-4606-b545-264057b205c1.png) and ![](img/345f1fab-16c7-45fc-8990-5590ed98da77.png),
    such that we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/824e9d0e-0a0d-4c09-8ebb-5969fc5e7d86.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, *x^** is optimal for *C*; that is, it minimizes *f*. This is called **Lagrangian
    sufficiency**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find λ^* and *x^**, we must solve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1582730-9cd5-48bf-bec8-9c7f433eaac5.png) and ![](img/2ab61b4a-8c1d-44f1-afcb-4c9dd4c35449.png).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, say we want to minimize [![](img/32c59e92-7e79-465b-bee9-4c0572d92de2.png)] subject
    to [![](img/e4564403-bee3-40be-82f6-544ec526a501.png)] and [![](img/bedccddf-8b17-4a29-bf49-6068e6e551f1.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the equation for Lagrangian sufficiency is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db3a1b74-5ec6-4886-b403-e4a95aeb2b19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/894ee79f-0ded-4dc2-b569-82bd916e1a52.png)'
  prefs: []
  type: TYPE_IMG
- en: We also need to pick a λ^(* )and *x^** value so that *L*(*x^*, λ^**) is minimal.
    So, for λ^*, *L*(*x,λ^**) must have a finite minimum.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding equation, we know that [![](img/d1973200-084f-4928-9b04-92f9e34424e2.png)] has
    a finite minimum at [![](img/5a174002-c832-4387-8ac7-db2d09fcf31f.png)] and the
    *x[1]* and *x[2]* terms only have a finite minimum when [![](img/cd7c0978-9bc3-4f73-b521-a66a73654af1.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to find a minimum, we take the first derivatives and make them equal to
    0, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20881f89-fd82-44c0-a246-251bc64ab52f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the first derivatives must be equal to 0, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88197c13-42da-4a6d-a305-02ec24699398.png), ![](img/8f9290ee-6789-4557-9288-b71779b1baa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To confirm that these are the minimum, we find the Hessian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78d527db-a928-4d06-a744-897d300de4d1.png)'
  prefs: []
  type: TYPE_IMG
- en: As we would expect, this is positive semi-definite when [![](img/34972cc9-724b-4970-9156-0df5942666d5.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of λ that we want are in the [![](img/82c94aa8-0ad8-4630-9137-f0ed7c6a0a62.png)] set, which
    tells us that the unique minimum of [![](img/0c6a8ebe-d447-4da8-b8be-4f52a16c4fd7.png)] is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c75077b2-748a-4d36-a3f9-0816a559e6e5.png)'
  prefs: []
  type: TYPE_IMG
- en: All we have to do now is find the values of *λ* and *x* for *x*(λ) that satisfy
    the constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Newton's method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Newton's method is a second-order optimization method that rescales the gradients
    in all directions using the inverse of the corresponding eigenvalues of the Hessian.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, we are trying to find the value of *x^** that minimizes *f(x)* and
    satisfies [![](img/e9bcb20c-1acb-4c37-8ef7-6bb613d393ed.png)]. Imagine that we
    are currently at a point, *x[k]*, and we move to *x[k+1]*, which is closer to
    *x^**. We can write this step as [![](img/dc2066c6-c7ce-47e2-bed8-0489d0086ef7.png)] (or
    [![](img/403534da-227e-48b6-9e54-e31c5b5751a6.png)]).
  prefs: []
  type: TYPE_NORMAL
- en: The reason why Newton step works well is because it behaves well when *x* is
    near *x^** since it takes the steepest descent direction at *x*. However, its
    performance is slow when we are at *x[0]* because the second derivative at *x[0]* does
    not give us reliable information about which direction we need to move in to reach
    *x^**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s suppose that [![](img/9d926883-fa35-47f7-9aa2-32745227cf4b.png)].
    Then, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc2b9e7d-eb1a-4ac4-b4af-33877ae3ea93.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/c9f7842e-6b1d-4883-87db-19f7d4da7519.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/057d4594-ac8a-4f9e-8f85-be59d57effb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is known as the Newton step. Therefore, at *x[k]*, *x[k+1]* minimizes
    the following quadratic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bd96824-6494-4a48-aa93-097152d97363.png)'
  prefs: []
  type: TYPE_IMG
- en: We also know that *Hf(x)* is positive definite, which tells us [![](img/9634901f-8c85-41cb-b723-ae9033d24dcd.png)],
    unless [![](img/48d6bf17-bc1e-4f52-ac5d-4a4c5301a129.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'When we receive our new value, *x[k+1]*, we can expect an error in it. This
    error is proportional to the square of the error in *x[k]*. We can see this as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d326693a-4c49-4758-a31a-4ac942cccdb5.png)'
  prefs: []
  type: TYPE_IMG
- en: This leads to this method converging quadratically (speedily, but only as long
    as *x[k]* is close to the optimal value).
  prefs: []
  type: TYPE_NORMAL
- en: The secant method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Newton's method, we calculated the first and second derivatives, but calculating
    the Hessian in a large problem is not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a function, [![](img/485f8af8-d2f3-4953-a916-803ce127c619.png)],
    and *n = 50*. If we take the first derivative of *f*, with respect to each case
    of *x[i]*, we get 50 equations. Now, if we calculate the second derivative, we
    have 2,500 equations, with respect to *x[i]* and *x[j]*, in a matrix. However,
    because Hessians are symmetric, we only really have to calculate 1,275 second
    derivatives. This is still a considerably large amount.
  prefs: []
  type: TYPE_NORMAL
- en: The secant method uses the Newton method, but instead of computing the second
    derivative, it estimates them using the first derivative, which makes it better
    suited to practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'It approximates the second derivative as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5ad01f4-7760-40ad-98ea-0e6847f36272.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We take this approximation and plug it into the Newton method, which gives
    us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f6c1680-7f83-432a-9da9-34094fedb65c.png)'
  prefs: []
  type: TYPE_IMG
- en: While this does reduce the computational complexity, it suffers the same fate
    as Newton's method because it requires additional iterations to converge.
  prefs: []
  type: TYPE_NORMAL
- en: The quasi-Newton method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The secant method approximated the second derivative, but the quasi-Newton
    method approximates the inverse of the Hessian. The steps are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/798eb06b-3128-450d-aa8a-0fbda4134281.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Q[k]* is the approximated inverse of the Hessian at *x[k]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by letting *Q[1 ]*= 1 and use two terms, α and β, to update the matrix
    at each iteration to aid in improving our estimation. They are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac775e6d-394a-44f9-bdcc-2b9387227238.png) and ![](img/a391149e-cbe9-4f5d-8b41-1294270c4579.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To update the matrix at each iteration, we make use of the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) method, which works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1a73000-b5cf-4aa5-ba36-89eb6ec6c039.png)'
  prefs: []
  type: TYPE_IMG
- en: For minimization to work, *Q* must be positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: Game theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's diverge to game theory for a bit. Games that consist of three or more
    players tend to be very challenging to solve, but two-player games are much simpler
    and are what we will focus on here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have two players that are represented by [![](img/c8555aa8-778c-4d41-8771-c7ef3a9830f4.png)],
    respectively, and they are playing rock paper scissors. As we know, in this game
    we tend to make decisions without any information about what the other player
    will choose. Each player naturally wants to win, so each player has the following
    payoff matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/becec948-a8a0-4fe5-af6c-2c270422a7f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Personally, I am not the biggest fan of showing the payoff in this way because
    you have to write two matrices and look up the individual payoff each time. I
    prefer to write it in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **R** | **P** | **S** |'
  prefs: []
  type: TYPE_TB
- en: '| **R** | (0, 0) | (-1, 1) | (1, -1) |'
  prefs: []
  type: TYPE_TB
- en: '| **P** | (1, -1) | (0, 0) | (-1, 1) |'
  prefs: []
  type: TYPE_TB
- en: '| **S** | (-1, 1) | (1, -1) | (0, 0) |'
  prefs: []
  type: TYPE_TB
- en: In the preceding table, player 1 chooses a row, [![](img/0c17fab6-9577-4fa6-a7c1-ab626bb152ed.png)],
    and player 2 chooses a column, [![](img/b0c5dc57-d3cb-4651-9476-f889b8d51c8e.png)].
    So, if we look at the preceding table, (-1, 1) tells us that player 1 lost and
    player 2 won.
  prefs: []
  type: TYPE_NORMAL
- en: In game theory, players have strategies that determine how they act or what
    actions they can take.
  prefs: []
  type: TYPE_NORMAL
- en: 'Player *X*, in our case, has the following set of strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9655716-a652-4f63-8f49-d9b4f20e0ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Player *Y* has the following set of strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c72aed7-05cd-456a-91da-f7802562d01a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each vector represents the probability of choosing each column or row.
  prefs: []
  type: TYPE_NORMAL
- en: Each case of [![](img/de1ec537-a985-405b-a59c-98f31802a8bd.png)] represents
    a strategy profile, and we calculate the expected payoff for player *X* as [![](img/4223e048-5b89-402e-b7fe-5c46e7d742a8.png)].
    If, for some case of *i*, *x[i ]*= 1, then we always choose *i* and call *x* a
    **pure strategy**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to another well-known example—the **prisoner's dilemma**. Here,
    we have two people who commit a crime and are caught. They each have two choices
    that they can make—testify (*T*) or stay quiet (*Q*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the outcomes of the choices they can make:'
  prefs: []
  type: TYPE_NORMAL
- en: If they both keep quiet, they both end up in jail serving a 2-year sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one testifies and the other stays quiet, then the one who stays quiet ends
    up serving a 3-year sentence and the testifier is freed for cooperating with the
    police.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If they both testify, then they both serve a 5-year sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our payoff table looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **S** | **T** |'
  prefs: []
  type: TYPE_TB
- en: '| **S** | (2, 2) | (0, 3) |'
  prefs: []
  type: TYPE_TB
- en: '| **T** | (3, 0) | (1, 1) |'
  prefs: []
  type: TYPE_TB
- en: Naturally, each person wants to maximize their own payoff; note that neither
    of the two has the opportunity to know or discuss what the other is going to do,
    so colluding is not an option. Therefore, each person would prefer to testify
    since this option is strictly better. We call *T* a dominant strategy and (1,
    1) is Pareto, dominated by (2, 2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a game and a strategy profile (*x, y*), such that they
    are in equilibrium (where *x* is the best response to *y* and vice versa). Then,
    we define ![](img/007884d9-2905-481e-aea0-3ad51a954c68.png) as having the best
    response to [![](img/cc93028a-aedc-4603-98c4-578efdfe4db3.png)], if for all cases
    of ![](img/e9ab4434-9dcc-4bf3-bab7-6f7491422d39.png) we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/539b5840-f024-428c-ae93-9215e848e352.png)'
  prefs: []
  type: TYPE_IMG
- en: Many of you will likely have heard the term zero-sum before, but for those of
    you haven't, it is a special game where the total payoff is 0, such that [![](img/bbceec62-804a-44a1-a40c-ebd28cfbcd87.png)].
    The earlier example of rock-paper-scissor is a good demonstration of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very important solution to the two-player matrix game is the minimax theorem.
    Suppose we have a [![](img/94e995ea-4eb6-43f1-bde6-ae17cc74b5a6.png)] payoff matrix. Then,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed611ad9-a52e-475c-b137-9da12ede3f24.png)'
  prefs: []
  type: TYPE_IMG
- en: This states that if both players use the minimax strategy, then they are in
    equilibrium since this results in both player 1 and player 2 getting the worst
    payoff, which satisfies the criteria. This is quite similar to finding the optimal
    value of [![](img/39a0c42c-e60a-480f-bfbe-5fe47a0475b2.png)], subject to constraints,
    as in a linear program.
  prefs: []
  type: TYPE_NORMAL
- en: Descent methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally, descent methods take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6e953e-c815-4c3a-83c9-2695c27a1ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/fdc594cc-721b-47af-9249-1641bb64234a.png)], and [![](img/71014de6-e392-4e10-89cc-c2c7af122bbc.png)].
    In the preceding algorithm, *k* is a sequence of steps, *x[k]* is the optimal
    point, and ![](img/7357c2be-114d-4c06-ba07-dbff575f048b.png) is a step. The scalar
    value, *c[k]*, is the size of the step at the *k^(th)* iteration.
  prefs: []
  type: TYPE_NORMAL
- en: In descent methods, [![](img/65830918-0eeb-42f0-83cd-8bc5a5e472ea.png)], except
    in the case where *x[k]* is the optimal value, which tells us that [![](img/ccfa74c5-0839-4bd7-b331-cf5756775f01.png)] for
    all cases of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent is a widely used first-order optimization problem, and it takes
    steps in the direction of the negative of the gradient of the function from the
    point it is currently at until it eventually terminates at the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you're at a skateboarding park and you have a tennis ball in your hand.
    You bend down and place the ball on the surface of a ramp and let it go; gravity
    does its thing and the ball follows the ramp's curvature, finding its way to the
    bottom. This is the concept behind gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the natural choice for the step is the negative gradient; that
    is, [![](img/2cb191ce-de37-4a7d-8213-3516637fd748.png)]. This is known as **gradient
    descent**, which takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29d1d41f-0ca4-4fd3-b214-30914dea5118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In optimization, we generally define the stopping criteria as a condition that,
    when satisfied, should stop our algorithm from continuing to optimize. It usually
    takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7735d23d-1601-4df8-9331-58d0064b89d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, η is a small positive number.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should remember, from the previous chapter, that if we have a function, *f*(*x,
    y*), then its gradient is [![](img/beb5c523-0bd6-4196-b675-22768cf354d5.png)].
    Therefore, we can compute the magnitude (or steepness) of the function at (*x,
    y*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed63a23a-902f-411d-83ef-60baeda060e5.png)'
  prefs: []
  type: TYPE_IMG
- en: This acts as a guide and tells us the direction that we should move at each
    step (since the curvature changes as we move downwards) to get to the minima.
  prefs: []
  type: TYPE_NORMAL
- en: However, gradient descent isn't perfect. It can be quite slow if the step size, *c^((k))*,
    is too small, and if the step size is too large, we may not reach the optimal
    point due to overshooting, which would result in our algorithm failing to converge,
    thus diverging instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this better, let''s take a look at the following two diagrams.
    The first diagram has a small step size and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0232347-0f57-44bb-a724-638332817422.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram shows a large step size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be9d6a16-bcf1-4514-80da-51bce1372770.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, a good step size is important, and picking it isn''t always
    an easy task. Luckily, there is a method known as **adaptive step size** that
    adjusts the step size after each iteration. It follows two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: If the value of the function increases after a step—which means the step size
    was too large—then undo the step and decrease the step size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the value of the function decreases the size of the step, then increase the
    step size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Still, this isn't perfect either. As you can tell from the diagram, the optimization
    is somewhat erratic, and when we encounter more flat surfaces, our algorithm tends
    to slow down.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should be able to tell that computing the gradient and getting to
    the optima isn't easy and is time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: This is why computing an approximation that points us in the same general direction instead is
    useful. We call this method **stochastic Gradient Descent** (**SGD**), and it
    is a very important algorithm that theoretically guarantees convergence. The word
    **stochastic** comes from the fact that we do not know the precise value of the
    gradient, only an approximation of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have *M* points [![](img/f6ad3959-1bb7-4bef-ac6b-f52f1431d3dc.png)],
    where *M* is very large. This becomes a big optimization problem. So, we take
    an objective function, *L*(*x*), which is a sum of the losses over the points.
    We write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7ea1f74-f73b-4d95-8aa8-99617aca1070.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, our goal is to minimize the loss as much as possible so that our model best
    fits the true function, *y*, as in regression. By minimizing the loss, we reduce
    the distance between our model's calculated point and the true point.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we use this method is that when we have a lot of points or a large
    optimization problem, it can be very computationally infeasible to calculate the
    gradient at each point, even more so if we were to calculate the Hessian. This
    method is, on the other hand, a lot more computationally feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that we are trying to approximate a function and we are trying to get
    as close as possible to the true function. To do this, we need to define a loss
    function—we have many to choose from. The following are the main ones that are
    used in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/18e3c4af-4e08-462b-9487-fdbc47868572.png)], known as **mean squared
    error**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/92ae23a0-4a80-41bc-b62b-096e4074bfee.png)], known as **mean absolute
    error**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2740f6ba-8d21-4a6a-927e-4a039f373f43.png)], known as **square loss**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/700d1816-3736-4e8a-91f4-556a2ac823ca.png)], known as **hinge loss**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2ce94877-0693-48c6-a7f1-da2ea1b42a0f.png)], known as **cross-entropy
    loss**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/6efd84a2-e6d4-4c2d-b783-970bff2d0585.png)], known as **Huber loss**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will revisit them later on and understand when it is best to use each one.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent with momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, gradient descent takes some time to find its way to a relatively
    flat surface. An improvement to the preceding example is gradient descent with
    momentum, which smoothes the gradient updates so that it is less erratic. Consider
    a tennis ball and a boulder both rolling down a mountain. The tennis ball would
    bounce around more and likely get stuck, but the boulder would gain momentum as
    it goes and maintain a relatively straight path toward the bottom. That is the
    key idea behind this improvement. It does so by remembering the previous updates
    and each update is a combination of the previous and current gradients, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6572ae6c-8ee4-49a3-a5d7-6853320b02cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/e3e4f1a1-8a9f-4fb9-bf41-76cef419bd4a.png)] and [![](img/6880e464-a804-4b46-852c-649fb51be543.png)].
  prefs: []
  type: TYPE_NORMAL
- en: In this method, as you will notice, we not only have to choose the step size, *c[k]*,
    but also the momentum coefficient, *α*.
  prefs: []
  type: TYPE_NORMAL
- en: The Nesterov's accelerated gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While momentum dampens the oscillations of gradient descent, Nesterov's method
    allows the ball traveling down the slope to look ahead and calculate the gradient
    with respect to the future position.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, instead of calculating the gradient at *x[k]*, we use [![](img/e8029e10-cbab-44a0-a572-293b3453bb0f.png)] (where
    [![](img/f18fca39-e776-49de-aaf8-93486da448d1.png)]), which is close to where
    we would be after the next step. So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/666cbfa9-a992-4ac2-9356-88af667666ac.png)'
  prefs: []
  type: TYPE_IMG
- en: We could also combine the momentum update with Nesterov's accelerated gradient
    by making *γ* = *α*, which would give us [![](img/fbecbba8-da6b-4671-9008-a5c47e4b7317.png)] and [![](img/4cae029d-44ac-4a8e-82c8-1670ed5e8899.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Here, as you will notice, we now have three parameters (*c*, *α*, and *γ*) instead
    of the two that we had in momentum.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly touched on adaptive step sizes earlier. These methods generally use
    the gradients from previous steps to guide the search direction and the step size
    to get us to convergence faster. The two main ones that we will look at are **adaptive
    gradient** (**Adagrad**) and **adaptive moment estimation** (**Adam**).
  prefs: []
  type: TYPE_NORMAL
- en: As before, our goal is to find *x^**, which minimizes the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: These gradient descent methods take the form of [![](img/23762e3b-c12e-43bd-accd-e1c492f78c1f.png)],
    where *G[k]* is the gradient at the *k^(th)* step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Adagrad, we have [![](img/35f9a6ac-b612-47c6-97e3-5b24640f7fcb.png)] and
    [![](img/ddde34a3-f360-4682-a1e1-9136ab371742.png)], which, if we plug into the
    preceding equation, gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9f741ed-9707-4162-9eb5-0a79fa5910ac.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we use the square root of the sum of the squares of the losses
    to update the step size at each step, which eliminates the need to do this ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Adam also keeps a history of the previous gradients, but it differs from Adagrad
    in that it stores an exponentially moving average of both the squared gradients
    and the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: We write this as [![](img/9e4e7164-1d00-4652-b66c-9f3187bd4ee4.png)] and [![](img/e9002cf3-1a66-4b95-a06d-0076f242de72.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simulated annealing is inspired by the field of metallurgy, where we use heat
    to alter the properties of a material. The applied heat increases the energy of
    ions and moves more freely. As the material starts to cool, it takes on a different
    shape upon reaching its equilibrium state. The heat needs to be slowly and gradually
    reduced to avoid the material getting stuck in a metastable state, which represents
    a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, to optimize a problem, we use temperature to control stochasticity.
    When the temperature is high, this means the process is freely and randomly exploring
    the space with the hope that it comes across a good convex region with a more
    favorable minimum. By reducing the temperature, we reduce the stochasticity and
    make the algorithm converge to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing is a non-convex optimization algorithm and is effective
    because of its ability to escape local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each iteration, we sample a possible step from a transition distribution, *T*,
    which is accepted according to the following probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/733ba40e-94f4-43d2-9329-dc715154237e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/91d15cb8-96fb-498c-bf70-b531e9718a19.png)] and ![](img/f28daa64-d175-458c-a51d-3fa71f73d7e5.png) is
    the temperature. This probability is known as the **Metropolis criterion** and
    is what gives simulated annealing the ability to escape local minima when we have
    a high temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 'To gradually bring the temperature down, we use a decay factor, [![](img/e35b2138-1090-4143-86dd-fe192b6727e2.png)],
    which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2006443d-0556-4f37-9a8c-6e8a18371a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: The process continues until it meets the stopping criteria; that is, the temperature
    drops to the point where we see no improvements from *n[k]* to *n[k+1]*.
  prefs: []
  type: TYPE_NORMAL
- en: Natural evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Natural evolution is a method that makes use of gradient descent, and our goal
    is to minimize [![](img/205c481b-b412-4d16-b476-874df8b803c8.png)]. We estimate
    the gradient from the samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adaa8636-4694-4348-bf58-875a7753d43a.png)'
  prefs: []
  type: TYPE_IMG
- en: Earlier, when looking at gradient descent, we needed to calculate the gradient
    of the objective function; but here, we work with the log likelihood, [![](img/b8303fc2-f8ea-467a-b043-69483e92f205.png)],
    and we can use this estimation of the gradient in any of the gradient descent
    methods we covered earlier to improve *θ*.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring population methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have dealt with optimization problems where we have a *ball* or *particle* that
    we edge along the curved space gradually and move toward the minima using gradient
    descent or Newton's method. Now, however, we will take a look at another class
    of optimization, where we use a population of individuals.
  prefs: []
  type: TYPE_NORMAL
- en: We spread these individuals across the optimization space, which prevents the
    optimization algorithm from getting stuck at local minima or a saddle point. These
    individuals can share information with each other about the local area they're
    in and use this to find an optimal solution that minimizes our function.
  prefs: []
  type: TYPE_NORMAL
- en: With these algorithms, we have an initial population and we would like to distribute
    them so that we cover as much ground as we can to give us the best chance of finding
    a globally optimal region.
  prefs: []
  type: TYPE_NORMAL
- en: We can sample our population from a multivariate normal distribution that is
    entered over a region that we are interested in, or uniformly distribute the population
    under some constraints; however, these two distributions are only recommended
    if you want to limit the space your population covers. Alternatively, we can use
    **Cauchy distribution**, which allows us to cover a larger space.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Genetic algorithms are inspired by Darwinism, where a fitter individual passes
    on certain heritable characteristics to the next generation. The objective function,
    in this case, has an inverse relationship with the individual's fitness or ability
    to reproduce. The chromosomes from the fitter individuals in each generation are
    passed on to the subsequent generation after having been subjected to crossover
    and mutation.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way for us to represent a chromosome is by using a binary string,
    similar to how DNA is encoded. However, a better method is writing each chromosome
    as a vector in ![](img/af6ea1fb-0cd0-400b-b83b-6198660a5911.png) that represents
    a point in the optimization space. This allows us to express crossover and mutation
    with greater ease.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a random population, and from it, we choose a set of chromosomes
    that will be the parents for the subsequent generation. If we have a population
    of *n* chromosomes, then we will select *n* parental pairs that will produce *n* children
    in the subsequent generation.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to minimize the objective function. So, we sample *k* random individuals
    from the population and pick the top-performing individuals from each of the samples
    or with the probability of their performance relative to the population. The fitness,
    then, of each individual has an inverse relation to [![](img/d16b37b4-541c-4aad-9d1b-d56db8d563fa.png)],
    and we can calculate it using [![](img/98fa3db9-37d1-4156-a26b-c875e9ac6018.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Crossover, on the other hand, is a combination of the chromosomes of the parents,
    which results in the children. There are a number of ways that this combination
    can occur, such as single-point crossover, two-point crossover, or uniform crossover,
    or we can use one of our own making.
  prefs: []
  type: TYPE_NORMAL
- en: In fitness and crossover, there are only so many traits that can be passed on
    from the initial population to subsequent generations. However, if only the best
    traits are passed on, we will end up with a saturated population, which isn't
    what we want. This is where mutations are useful. They allow new traits to be
    created and passed on, which enables individuals to explore more of the optimization
    space. After each crossover, each child in the population experiences some mutation,
    subject to a probability.
  prefs: []
  type: TYPE_NORMAL
- en: Particle swarm optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This algorithm uses the concept of swarm intelligence, where you have a school
    of fish or a flock of birds. Let's suppose they are trying to find some food.
    They arrive at an area and spread out a bit, starting to look for food individually.
    When one of them finds food, it lets the others know so that they can join in.
  prefs: []
  type: TYPE_NORMAL
- en: Each individual in the population knows its current position and velocity and only keeps
    track of the previous best positions it has visited. The velocity vector determines
    the direction of the search, and if the individual has a high velocity, then it
    has a more explorative character, whereas if it has a low velocity, it has a more
    exploitative character.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of each iteration, the whole population is accelerated to the
    best position that any individual has come across so far. The updates are computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc39b971-d525-47fe-aa0b-8e35460d6988.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x[best]* is the best position found by the group as a whole, [![](img/53046c2c-9631-4b82-8d0a-2e128da57c7f.png)] is
    the best position that an individual has found, *w*, *α[1]*, and *α[2]* are parameters,
    and [![](img/50b9c5eb-fd86-4247-a994-3f2c34ee65ee.png)].
  prefs: []
  type: TYPE_NORMAL
- en: The values of *c[1]* and *c[2]* heavily influence the rate at which they converge.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a number of different kinds of optimization, such
    as convex and non-convex optimization, as well as what makes optimization such
    a challenging problem. We also had a look at how to define an optimization problem
    and explored a variety of methods, including population methods, simulated annealing,
    and gradient descent-based methods. In later chapters, we'll come to understand
    how optimization is used in deep learning and why it is such an important field
    for us to understand.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about graph theory and its uses in the field
    to solve various problems.
  prefs: []
  type: TYPE_NORMAL
