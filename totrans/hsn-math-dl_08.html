<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear Neural Networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will go over some of the concepts in machine learning. It is expected that you have previously studied and have an understanding of machine learning. So this chapter will serve as a refresher for some of the concepts that will be needed throughout this book, rather than a comprehensive study of all the machine learning approaches.</p>
<p>In this chapter, we will focus on linear neural networks, which are the simplest type of neural networks and are used for tasks such as linear regression, polynomial regression, logistic regression, and softmax regression, which are used most frequently in statistical learning.</p>
<p>We use regression to explain the relationship between one or more independent variables and a dependent variable. The concepts we will learn in this chapter are crucial for furthering our understanding of how machine learning works before we dive into deep neural networks in the next chapter.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Linear regression</li>
<li>Polynomial regression</li>
<li>Logistic regression</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>The purpose of regression is to find the relationship that exists between data (denoted by <em>x</em>) and its corresponding output (denoted by <em>y</em>) and predict it. The output of all regression problems is a real number (<sub><img class="fm-editor-equation" src="Images/7efc43e8-d5e1-4eb3-87fe-f9ca8505fbb6.png" style="width:3.42em;height:1.50em;"/></sub>). This can be applied to a range of problems, such as predicting the price of a house or what rating a movie will have. </p>
<p>In order for us to make use of regression, we need to use the following:</p>
<ul>
<li>Input data, which could be either scalar values or vectors. This is sometimes referred to as <strong>features</strong>.</li>
<li>Training examples, which include a good number of (<em>x<sub>i</sub>, y<sub>i</sub></em>) pairs; that is, the output for each input.</li>
<li>A function that captures the relationship between the input and output—the model.</li>
<li>A loss or an objective function, which tells us how accurate our model is.</li>
<li>Optimization, to minimize the loss or the objective function.</li>
</ul>
<p>Before we go further, let's look back to <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>, where we noted that the equation of a straight line is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/03e99142-929a-483a-a117-8c5bbf4cd01d.png" style="width:5.08em;height:1.17em;"/></p>
<p>Here, <em>m</em> is the gradient (or slope) and <em>b</em> is a correction term. We found the slope using two pairs of points on the line using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/96e28355-1c7e-4e21-b50c-dfbb297181f0.png" style="width:5.83em;height:2.25em;"/></p>
<p>As we know, this is easy to do. In linear regression, however, we are given many <span>(</span><em>x<sub>i</sub>, y<sub>i</sub></em><span>)</span> points, and our goal is to find the line of best fit that best captures the relationship. This line is what our model learns. We can represent this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ccc299b3-996f-466a-aa27-0d79e448d6c3.png" style="width:7.67em;height:1.25em;"/></p>
<p>Here, <em>ε</em> represents an error, which we assume to be Gaussian, <em>y</em><span> is the true label, and <sub><img class="fm-editor-equation" src="Images/9b75ef25-3a95-4910-adc3-9f65764a128f.png" style="width:0.83em;height:1.50em;"/></sub> is the prediction that our model provides.</span></p>
<p>Let's now consider a case where we have multiple independent variables and we want to find the relationship between one dependent variable. This type of regression is known as <strong>multiple regression</strong>. In this case, each of the independent variables has an impact on the predicted output. </p>
<p>Our inputs, in this case, will take the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c93c9b31-e539-48b7-aeee-0034b9e6e53f.png" style="width:7.17em;height:1.25em;"/></p>
<p>Here, <em>n</em> is the number of independent variables. </p>
<p>To find <sub><img class="fm-editor-equation" src="Images/60f67eb9-ce2a-4ae4-97bc-3ca0cb11d226.png" style="width:0.75em;height:1.33em;"/></sub>, we could just average over all the dependent variables or sum them together, but this is <span>not</span><span> </span><span>likely</span><span> to </span><span>give us the desired result. Suppose we want to predict the price of a house; our inputs could be the square footage of the lot, the number of bedrooms, the number of bathrooms, and whether or not it has a swimming pool. </span></p>
<p>Each of the inputs will have a corresponding weight, which the model will learn from the data points, that best describes the importance of each of the inputs. This then becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fd5f2d12-81cb-4dc4-a89c-469792047467.png" style="width:15.50em;height:1.17em;"/></p>
<p>Or, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4d84d7a6-3064-41ce-95a2-041b17f3b191.png" style="width:7.25em;height:3.00em;"/></p>
<p class="mce-root">We can also rewrite this in matrix form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8744799f-898d-41e7-9bdb-9e06e6f48bc0.png" style="width:20.25em;height:5.92em;"/></p>
<p>But now, the obvious question arises—<em>how does our model learn these weights and this relationship?</em> This is easy for us to do because our brains instantly spot patterns and we can analytically spot relationships. However, if our machine is to learn this relationship, it needs a guide. This guide is the loss function, which tells the model how off its prediction is and which direction it needs to move in to improve. </p>
<p class="mce-root">The loss is generally the distance between the prediction (<sub><img class="fm-editor-equation" src="Images/83b719ec-1803-448b-8545-dbefd29cbeb4.png" style="width:1.33em;height:1.58em;"/></sub>) and the true value (<em>y<sub>i</sub></em>), which we can write as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a2e32feb-4a43-48c6-ba93-67919c751f52.png" style="width:9.67em;height:2.25em;"/></p>
<p>But that still doesn't give us the full picture. Our goal is to minimize the loss over all the data samples that the model is trained on, so we average the sum of the losses over all the data samples. This looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/56c0c641-3f99-4185-9e46-f457657b0c31.png" style="width:14.83em;height:2.92em;"/></p>
<p>The goal of training is to find the optimal parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9a48ee26-6d2b-424e-b15f-75728bda8228.png" style="width:10.92em;height:1.92em;"/></p>
<p>Having learned what linear regression is, let's now see what polynomial regression is all about in the following section.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Polynomial regression</h1>
                </header>
            
            <article>
                
<p>Linear regression, as you might imagine, isn't a one-size-fits-all solution that we can use for any problem. A lot of the relationships that exist between variables in the real world are not linear; that is, a straight line isn't able to capture the relationship. For these problems, we use a variant of the preceding linear regression known as <strong>polynomial regression</strong>, which can capture more complexities, such as curves. This method makes use of applying different powers to the explanatory variable to discover non-linear problems. This looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3421bf9d-521d-4462-8fc5-9da468a13ab3.png" style="width:17.00em;height:1.42em;"/></p>
<p>Or, we could have the following: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/adcf6e47-e525-4804-a39c-85748eb667ed.png" style="width:7.42em;height:3.08em;"/></p>
<p>This is the case for <sub><img class="fm-editor-equation" src="Images/76d7c0d5-2e79-4a23-8894-87157a9444be.png" style="width:6.83em;height:1.17em;"/></sub>.</p>
<p>As you can see from the preceding equation, a model such as this is not only able to capture a straight line (if needed) but can also generate a second-order, third-order, or <em>n<sup>th</sup>-</em>order equation that fits the data points. </p>
<p>Let's suppose we have the following data points:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1045 image-border" src="Images/57eeebcf-d187-4f30-9f18-35ec13d15f45.png" style="width:31.50em;height:19.92em;"/></p>
<p>We can immediately tell that a straight line will not do the job, but after we apply polynomial regression to it, we can see that our model learns to fit the curve, which resembles a sinusoidal wave. We can observe this in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1046 image-border" src="Images/84e5960a-bcf8-43c7-9fd5-2987fcbdd39e.png" style="width:31.42em;height:20.17em;"/></p>
<p>Let's now take a look at a case where we are trying to learn a surface and we have two inputs, <sub><img class="fm-editor-equation" src="Images/84a44442-ce63-47b7-a866-8db81fae6ea6.png" style="width:7.00em;height:1.67em;"/></sub>, and one output, <em>y</em>. Again, as we can see in the following diagram, the surface is not flat; in fact, it is quite bumpy:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1047 image-border" src="Images/39c2e48d-16a4-4568-822c-e1c8f934b7c7.png" style="width:29.58em;height:19.33em;"/></p>
<p>We could approximate model this using the following third-order polynomial:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/89bb7807-34c1-4707-9c30-ed689c514603.png" style="width:36.58em;height:1.33em;"/></p>
<p>If this gives us a satisfactory result, we can add another higher-degree polynomial (and so on) until there is one that models the surface.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>There is another kind of regression that we often <span>use</span><span> </span><span>in practice—</span><strong>logistic regression</strong><span>. Suppose we want to determine whether or not an email is spam. In this case, our</span> <em>x</em><span><em>(s)</em> value could be occurrences of <em>!(s)</em> or the total number of spelling errors in the email. Then, </span><em>y</em><span> can take on the value o</span>f 1 (for spam) and 0 (for n<span>ot spam).</span></p>
<p>In this kind of case, linear regression will simply not work since we are not predicting a real value—we are trying to predict which class the email belongs to. </p>
<p>This will usually end up looking as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1048 image-border" src="Images/01bd01b2-49ee-42d4-866e-8d2f08bc04f4.png" style="width:23.50em;height:13.17em;"/></p>
<p>As you can see, the data is grouped into two areas—one that represents non-spam and another that represents spam. </p>
<p>We can calculate this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ecd42f71-3b83-4fbb-ae7f-908e0f070183.png" style="width:5.33em;height:2.50em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/998103ce-5957-4ba8-8240-06606f4d11f3.png" style="width:6.58em;height:2.75em;"/></sub>. </p>
<p>However, this only works for binary classification. What if we want to classify multiple classes? Then, we can use softmax regression, which is an extension of logistic regression. This will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a32f1773-9136-4f13-b444-72dd19877d15.png" style="width:6.92em;height:3.33em;"/></p>
<p>This is the case for <sub><img class="fm-editor-equation" src="Images/b75a1fe0-065b-4db0-999f-2ffab4675152.png" style="width:5.67em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/93f8591c-8ab0-4b16-b862-a352eca326e4.png" style="width:6.58em;height:0.83em;"/></sub>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about various forms of regression, such as (multiple) linear regression, polynomial regression, logistic regression, and softmax regression. Each of these models has aided us in figuring out the relationship that exists between one or more independent variable(s) and a dependent variable. For some of you, these concepts may seem very rudimentary, but they will serve us well on our journey throughout this book and in gaining a deeper understanding of the concepts to come.</p>
<p>In the next chapter, we will learn about feedforward neural networks.</p>


            </article>

            
        </section>
    </div></body></html>