- en: Linear Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will go over some of the concepts in machine learning. It
    is expected that you have previously studied and have an understanding of machine
    learning. So this chapter will serve as a refresher for some of the concepts that
    will be needed throughout this book, rather than a comprehensive study of all
    the machine learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on linear neural networks, which are the simplest
    type of neural networks and are used for tasks such as linear regression, polynomial
    regression, logistic regression, and softmax regression, which are used most frequently
    in statistical learning.
  prefs: []
  type: TYPE_NORMAL
- en: We use regression to explain the relationship between one or more independent
    variables and a dependent variable. The concepts we will learn in this chapter
    are crucial for furthering our understanding of how machine learning works before
    we dive into deep neural networks in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of regression is to find the relationship that exists between data
    (denoted by *x*) and its corresponding output (denoted by *y*) and predict it.
    The output of all regression problems is a real number ([![](img/7efc43e8-d5e1-4eb3-87fe-f9ca8505fbb6.png)]).
    This can be applied to a range of problems, such as predicting the price of a
    house or what rating a movie will have.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for us to make use of regression, we need to use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Input data, which could be either scalar values or vectors. This is sometimes
    referred to as **features**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training examples, which include a good number of (*x[i], y[i]*) pairs; that
    is, the output for each input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that captures the relationship between the input and output—the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A loss or an objective function, which tells us how accurate our model is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization, to minimize the loss or the objective function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we go further, let''s look back to [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*, where we noted that the equation of a straight line is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03e99142-929a-483a-a117-8c5bbf4cd01d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *m* is the gradient (or slope) and *b* is a correction term. We found
    the slope using two pairs of points on the line using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96e28355-1c7e-4e21-b50c-dfbb297181f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we know, this is easy to do. In linear regression, however, we are given
    many (*x[i], y[i]*) points, and our goal is to find the line of best fit that
    best captures the relationship. This line is what our model learns. We can represent
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccc299b3-996f-466a-aa27-0d79e448d6c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ε* represents an error, which we assume to be Gaussian, *y* is the true
    label, and [![](img/9b75ef25-3a95-4910-adc3-9f65764a128f.png)] is the prediction
    that our model provides.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now consider a case where we have multiple independent variables and we
    want to find the relationship between one dependent variable. This type of regression
    is known as **multiple regression**. In this case, each of the independent variables
    has an impact on the predicted output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our inputs, in this case, will take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c93c9b31-e539-48b7-aeee-0034b9e6e53f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the number of independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: To find [![](img/60f67eb9-ce2a-4ae4-97bc-3ca0cb11d226.png)], we could just average
    over all the dependent variables or sum them together, but this is not likely to give
    us the desired result. Suppose we want to predict the price of a house; our inputs
    could be the square footage of the lot, the number of bedrooms, the number of
    bathrooms, and whether or not it has a swimming pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the inputs will have a corresponding weight, which the model will learn
    from the data points, that best describes the importance of each of the inputs.
    This then becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd5f2d12-81cb-4dc4-a89c-469792047467.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d84d7a6-3064-41ce-95a2-041b17f3b191.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also rewrite this in matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8744799f-898d-41e7-9bdb-9e06e6f48bc0.png)'
  prefs: []
  type: TYPE_IMG
- en: But now, the obvious question arises—*how does our model learn these weights
    and this relationship?* This is easy for us to do because our brains instantly
    spot patterns and we can analytically spot relationships. However, if our machine
    is to learn this relationship, it needs a guide. This guide is the loss function,
    which tells the model how off its prediction is and which direction it needs to
    move in to improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss is generally the distance between the prediction ([![](img/83b719ec-1803-448b-8545-dbefd29cbeb4.png)]) and
    the true value (*y[i]*), which we can write as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2e32feb-4a43-48c6-ba93-67919c751f52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But that still doesn''t give us the full picture. Our goal is to minimize the
    loss over all the data samples that the model is trained on, so we average the
    sum of the losses over all the data samples. This looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56c0c641-3f99-4185-9e46-f457657b0c31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal of training is to find the optimal parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a48ee26-6d2b-424e-b15f-75728bda8228.png)'
  prefs: []
  type: TYPE_IMG
- en: Having learned what linear regression is, let's now see what polynomial regression
    is all about in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression, as you might imagine, isn''t a one-size-fits-all solution
    that we can use for any problem. A lot of the relationships that exist between
    variables in the real world are not linear; that is, a straight line isn''t able
    to capture the relationship. For these problems, we use a variant of the preceding
    linear regression known as **polynomial regression**, which can capture more complexities,
    such as curves. This method makes use of applying different powers to the explanatory
    variable to discover non-linear problems. This looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3421bf9d-521d-4462-8fc5-9da468a13ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, we could have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adcf6e47-e525-4804-a39c-85748eb667ed.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the case for [![](img/76d7c0d5-2e79-4a23-8894-87157a9444be.png)].
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding equation, a model such as this is not only
    able to capture a straight line (if needed) but can also generate a second-order,
    third-order, or *n^(th)-*order equation that fits the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have the following data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57eeebcf-d187-4f30-9f18-35ec13d15f45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can immediately tell that a straight line will not do the job, but after
    we apply polynomial regression to it, we can see that our model learns to fit
    the curve, which resembles a sinusoidal wave. We can observe this in the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84e5960a-bcf8-43c7-9fd5-2987fcbdd39e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now take a look at a case where we are trying to learn a surface and
    we have two inputs, [![](img/84a44442-ce63-47b7-a866-8db81fae6ea6.png)], and one
    output, *y*. Again, as we can see in the following diagram, the surface is not
    flat; in fact, it is quite bumpy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39c2e48d-16a4-4568-822c-e1c8f934b7c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We could approximate model this using the following third-order polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89bb7807-34c1-4707-9c30-ed689c514603.png)'
  prefs: []
  type: TYPE_IMG
- en: If this gives us a satisfactory result, we can add another higher-degree polynomial
    (and so on) until there is one that models the surface.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is another kind of regression that we often use in practice—**logistic
    regression**. Suppose we want to determine whether or not an email is spam. In
    this case, our *x**(s)* value could be occurrences of *!(s)* or the total number
    of spelling errors in the email. Then, *y* can take on the value of 1 (for spam)
    and 0 (for not spam).
  prefs: []
  type: TYPE_NORMAL
- en: In this kind of case, linear regression will simply not work since we are not
    predicting a real value—we are trying to predict which class the email belongs
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will usually end up looking as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01bd01b2-49ee-42d4-866e-8d2f08bc04f4.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the data is grouped into two areas—one that represents non-spam
    and another that represents spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecd42f71-3b83-4fbb-ae7f-908e0f070183.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/998103ce-5957-4ba8-8240-06606f4d11f3.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this only works for binary classification. What if we want to classify
    multiple classes? Then, we can use softmax regression, which is an extension of
    logistic regression. This will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a32f1773-9136-4f13-b444-72dd19877d15.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the case for [![](img/b75a1fe0-065b-4db0-999f-2ffab4675152.png)] and [![](img/93f8591c-8ab0-4b16-b862-a352eca326e4.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about various forms of regression, such as (multiple)
    linear regression, polynomial regression, logistic regression, and softmax regression.
    Each of these models has aided us in figuring out the relationship that exists
    between one or more independent variable(s) and a dependent variable. For some
    of you, these concepts may seem very rudimentary, but they will serve us well
    on our journey throughout this book and in gaining a deeper understanding of the
    concepts to come.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about feedforward neural networks.
  prefs: []
  type: TYPE_NORMAL
