- en: Feedforward Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered linear neural networks, which have proven
    to be effective for problems such as regression and so are widely used in the
    industry. However, we also saw that they have their limitations and are unable
    to work effectively on higher-dimensional problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take an in-depth look at the **multilayer perceptron**
    (**MLP**), a type of **feedforward neural network** (**FNN**). We will start by
    taking a look at how biological neurons process information, then we will move
    onto mathematical models of biological neurons. The **artificial neural networks**
    (**ANNs**) we will study in this book are made up of mathematical models of biological
    neurons (we will learn more about this shortly). Once we have built a foundation,
    we will move on to understanding how MLPs—which are the FNNs—work and their involvement
    with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: What FNNs allow us to do is approximate a function that maps input to output
    and this can be used in a variety of tasks, such as predicting the price of a
    house or a stock or determining whether or not an event will occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding biological neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the perceptron and the McCulloch-Pitts neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLPs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding biological neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The human brain is capable of some remarkable feats—it performs very complex
    information processing. The neurons that make up our brains are very densely connected
    and perform in parallel with others. These biological neurons receive and pass
    signals to other neurons through the connections (synapses) between them. These
    synapses have strengths associated with them and increasing or weakening the strength
    of the connections between neurons is what facilitates our learning and allows
    us to continuously learn and adapt to the dynamic environments we live in.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, the brain consists of neurons—in fact, according to recent studies,
    it is estimated that the human brain contains roughly 86 billion neurons. That
    is a lot of neurons and a whole lot more connections. A very large number of these
    neurons are used simultaneously every day to allow us to carry out a variety of
    tasks and be functional members of society. Neurons by themselves are said to
    be quite slow, but it is this large-scale parallel operation that gives our brains
    its extraordinary capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a diagram of a biological neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ed8f991-d8d2-4c8d-bedb-1386771c801d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the preceding diagram, each neuron has three main components—the
    body, an axon, and many dendrites. The synapses connect the axon of one neuron
    to the dendrites of other neurons and determine the weight of the information
    that is received from other neurons. Only when the sum of the weighted inputs
    to the neuron exceeds a certain threshold does the neuron fire (activate); otherwise,
    it is at rest. This communication between neurons is done through electrochemical
    reactions, involving potassium, sodium, and chlorine (which we will not go into
    as it is beyond the scope of this book; however, if this interests you, there
    is a lot of literature you can find on it).
  prefs: []
  type: TYPE_NORMAL
- en: The reason we are looking at biological neurons is that the neurons and neural
    networks we will be learning about and developing in this book are largely biologically
    inspired. If we are trying to develop artificial intelligence, where better to
    learn than from actual intelligence?
  prefs: []
  type: TYPE_NORMAL
- en: Since the goal of this book is to teach you how to develop ANNs on computers,
    it is relatively important that we take a look at the differences between the
    computational power of our brains as opposed to computers.
  prefs: []
  type: TYPE_NORMAL
- en: Computers have a significant advantage over our brains as they can perform roughly
    10 billion operations per second, whereas the human brain can only perform around
    800 operations per second. However, the brain requires roughly 10 watts to operate,
    which is 10 times less than what a computer requires. Another advantage that computers
    have is their precision; they can perform operations millions of times more accurately.
    Lastly, computers perform operations sequentially and cannot deal with data they
    have not been programmed to deal with, but the brain performs operations in parallel
    and is well equipped to deal with new data.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the perceptron and the McCulloch-Pitts neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover two mathematical models of biological neurons—the
    **McCulloch-Pitts** (**MP**) neuron and Rosenblatt's perceptron—which create the
    foundation for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The MP neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MP neuron was created in 1943 by Warren McCulloch and Walter Pitts. It
    was modeled after the biological neuron and is the first mathematical model of
    a biological neuron. It was created primarily for classification tasks. The MP
    neuron takes as input binary values and outputs a binary value based on a threshold
    value. If the sum of the inputs is greater than the threshold, then the neuron
    outputs `1` (if it is under the threshold, it outputs `0`). In the following diagram,
    we can see what a basic neuron with three inputs and one output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46f43996-76b7-40b2-9fcf-a3e8c0a66941.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this isn't entirely dissimilar to the biological neuron we saw
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fed4244-c23d-4d05-a473-4d82d7431cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x[i]* = `0` or `1`.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of this as outputting Boolean answers; that is, `true` or `false`
    (or `yes` or `no`).
  prefs: []
  type: TYPE_NORMAL
- en: While the MP neuron may look simple, it has the ability to model any logic function,
    such as `OR`, `AND`, and `NOT`; but it is unable to classify the `XOR` function.
    Additionally, it does not have the ability to learn, so the threshold (*b*) needs
    to be adjusted analytically to fit our data.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The perceptron model, created by Frank Rosenblatt in 1958, is an improved version
    of the MP neuron and can take any real value as input. Each input is then multiplied
    by a real-valued weight. If the sum of the weighted inputs is greater than the
    threshold, then the output is `1`, and if it is below the threshold, then the
    output is `0`. The following diagram illustrates a basic perceptron model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1feb9992-9a74-441f-90d7-9319a5c58453.png)'
  prefs: []
  type: TYPE_IMG
- en: This model shares a lot of similarities with the MP neuron, but it is more similar
    to the biological neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can write this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eabb472d-f1e6-4d0b-ba24-91da720f7cad.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/abe9bae6-e784-4fd1-b427-4eb016761781.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we rewrite the perceptron equation in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6db254bd-9049-4af9-bed8-a0e63848161b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows how the perceptron equation will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320b5a92-8882-41fe-917a-50f3c47904f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/22c4d4ac-b870-4c16-bc5a-d53df9a04aa6.png)] and [![](img/056f6b67-7dff-452e-aa0e-352cf607637c.png)].
    This prevents us from having to hardcode the threshold, which makes the threshold
    a learnable parameter instead of something we have to manually adjust (as is the
    case with the MP neuron).
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of the MP neuron and perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantage the perceptron model has over the MP neuron is that it is able
    to learn through error correction and it linearly separates the problem using
    a hyperplane, so anything that falls below the hyperplane is `0` and anything
    above it is `1`. This error correction allows the perceptron to adjust the weights
    and move the position of the hyperplane so that it can properly classify the data.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we mentioned that the perceptron learns to linearly classify a problem—but
    what exactly does it learn? Does it learn the nature of the question that is asked?
    No. It learns the effect of the input on the output. *So, the greater the weight
    associated with a certain input, the greater its impact on the prediction (classification). *
  prefs: []
  type: TYPE_NORMAL
- en: 'The update for the weights (learning) happens as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40e6f30f-a89f-4952-ae9f-8d37a3dde434.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *δ* = expected value – predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also add a learning rate ([![](img/b3ec0dca-3cfe-45b8-ba4c-97f0f728a4cf.png)])
    if we want to speed up the learning; so, the update will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/801f9c62-f4a2-4ad9-87df-e292a6e5e2ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During these updates, the perceptron calculates the distance of the hyperplane
    from the points to be classified and adjusts itself to find the best position
    that it can perfectly linearly classify the two target classes. So, it maximally
    separates both points on either side, which we can see in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47d7de84-900c-4b21-8635-e037cbe4f383.png)'
  prefs: []
  type: TYPE_IMG
- en: What is even more fascinating about this is that because of the aforementioned
    learning rule, the perceptron is guaranteed to converge when given a finite number
    of updates and so will work on any binary classification task.
  prefs: []
  type: TYPE_NORMAL
- en: But alas, the perceptron is not perfect either and it also has limitations.
    As it is a linear classifier, it is unable to deal with nonlinear problems, which
    makes up the majority of the problems we usually wish to develop solutions for.
  prefs: []
  type: TYPE_NORMAL
- en: MLPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, both the MP neuron and perceptron models are unable to deal with
    nonlinear problems. To combat this issue, modern-day perceptrons use an activation
    function that introduces nonlinearity to the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptrons (neurons, but we will mostly refer to them as **nodes** going
    forward) we will use are of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7e24253-38b4-46c3-af8b-368270efcc83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *y* is the output, *φ* is a nonlinear activation function, *x[i]* is
    the inputs to the unit, *w[i]* is the weights, and *b* is the bias. This improved
    version of the perceptron looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbda6328-f94d-4726-a9c2-156876ef5b39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, the activation function is generally the sigmoid
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d31b4c77-65c0-4313-a34f-ed9189ce7367.png)'
  prefs: []
  type: TYPE_IMG
- en: What the sigmoid activation function does is squash all the output values into
    the `(0, 1)` range. The sigmoid activation function is largely used for historical
    purposes since the developers of the earlier neurons focused on thresholding.
    When gradient-based learning was introduced, the sigmoid function turned out to
    be the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MLP is the simplest type of FNN. It is basically a lot of nodes combined
    together and the computation is carried out sequentially. The network looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ed4f81b-6ef0-47d0-abd4-f29af3de7bf8.png)'
  prefs: []
  type: TYPE_IMG
- en: An FNN is essentially a directed acyclic graph; that is, the connections are
    always moving in one direction. There are no connections that feed the outputs
    back into the network.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding diagram, the nodes are arranged in layers
    and the nodes in each layer are connected to each of the neurons in the next layer.
    However, there aren't any connections between nodes in the same layer. We refer
    to networks such as this as being fully connected.
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is referred to as the input layer, the last layer is referred
    to as the output layer, and all the layers in between are called hidden layers.
    The number of nodes in the output layer depends on the type of problem we build
    our MLP for. It is important that you remember that the inputs to and outputs
    from layers are not the same as the inputs to and outputs from the network.
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice that in the preceding architecture, there is only one unit
    in the output layer. This is generally the case when we have a regression or binary
    classification task. So, if we want our network to be able to detect multiple
    classes, then our output layer will have *K* nodes, where *K* is the number of
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the depth of the network is the number of layers it has and the width
    is the number of nodes in a layer.
  prefs: []
  type: TYPE_NORMAL
- en: However, what makes neural networks so powerfully effective, and the reason
    we are studying them, is that they are universal function approximators. The universal
    approximation theorem states that "<q>a feedforward neural network with a single
    hidden layer containing a finite number of neurons can approximate continuous
    functions on compact subsets of ![](img/1f059592-7ad0-46d4-89f1-9c34a491c915.png),
    under mild assumptions on the activation function.</q>" What this means is that
    if the hidden layer contains a specific number of neurons, then our neural network
    can reasonably approximate any known function.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that it is unclear exactly how many neurons are needed in the
    hidden layer for it to be able to approximate any function. This could vary greatly,
    depending on the function we want it to learn.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you might be thinking that if MLPs have been around since the late 1960s,
    why has it taken nearly 50 years for them to take off and be used as widely as
    they are today? This is because the computing power that was available 50 years
    ago was nowhere near as powerful as what is available today, nor was the same
    amount of data that is available now available back then. So, because of the lack
    of results that MLPs were able to achieve back then, they faded into obscurity.
    Because of this, as well as the universal approximation theorem, researchers at
    the time hadn't looked deeper than into a couple of layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's break the model down and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know now that MLPs (and so FNNs) are made of three different kinds of layers—input,
    hidden, and output. We also know what a single neuron looks like. Let's now mathematically
    explore MLPs and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have an MLP with ![](img/d94ac979-7201-4477-80eb-3dd96760aa3f.png) input (where ![](img/da399926-f0d2-4ca9-a852-de9c9d2b5785.png)),
    *L* layers, *N* neurons in each layer, an activation function [![](img/b7b71384-b2f7-4c6f-9348-6b7bb274e79c.png)],
    and the network output, *y*. The MLP looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24293b6e-1760-42d5-813a-77d6b949fb16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, this network has four inputs—the first hidden layer has five
    nodes, the second hidden layer has three nodes, the third hidden layer has five
    nodes, and there is one node for the output. Mathematically, we can write this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d1d4d07-243c-42ee-bd77-db5cb042f6d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/a99c32c3-e4c0-4b0d-a607-67feeb0586c6.png)] is the *i^(th)* node
    in the *l^(th)* layer, [![](img/49bdaee9-222a-428a-8b5d-a06e1c30b442.png)] is
    an activation function for the *l**^(th)* layer, *x[j]* is the *j^(th)* input
    to the network, [![](img/2090f186-37fd-445d-b5a1-10d0b59368da.png)] is the bias
    for the *i**^(th)* node in the *l**^(th)* layer, and [![](img/35365769-aa9c-4837-9481-8096a06f8480.png)] is
    the directed weight that connects the *j^(th)* node in the *l–1^(st)* layer to
    the *i^(th)* node in the *l^(th)* layer.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move forward, let's take a look at the preceding equations. From them,
    we can easily observe that each hidden node depends on the weights from the previous
    layer. If you take a pencil and draw out the network (or use your fingers to trace
    the connections), you will notice that the deeper we get into the network, the
    more complex the relationship nodes in the later hidden layers have with those
    in the earlier layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have an idea of how each neuron is computed in an MLP, you might
    have realized that explicitly writing out the computation on each node in each
    layer can be a daunting task. So, let''s rewrite the preceding equation in a cleaner
    and simpler manner. We generally do not express neural networks in terms of the
    computation that happens on each node. We instead express them in terms of layers
    and because each layer has multiple nodes, we can write the previous equations
    in terms of vectors and matrices. The previous equations can now be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8261ea86-c6a6-4edc-b0be-de27390d03cd.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a whole lot simpler to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Remember from [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml), *Linear
    Algebra*, that when you multiply a vector or matrix with a scalar value, the scalar
    value is applied to all the entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the networks we want to build, the input more than likely will not be a
    vector, as it is in the preceding examples; it will be a matrix, so we can then
    rewrite it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b2e05de-f12e-409f-bb52-d383fbb0be89.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, X is the matrix containing all the data we want to train our model on,
    H^([l]) contains the hidden nodes at each layer for all the data samples, and
    everything else is the same as it was earlier.
  prefs: []
  type: TYPE_NORMAL
- en: If you have been paying attention, you will have noticed that the order of the
    multiplication taking place in the matrix is different than what took place earlier.
    Why do you think that is? (I'll give you a hint—transpose.)
  prefs: []
  type: TYPE_NORMAL
- en: You should now have a decent, high-level understanding of how neural networks
    are constructed. Let's now lift up the hood and take a look at what is going on
    underneath. We know from the previous equations that neural networks are comprised
    of a series of matrix multiplications and matrix additions and scalar multiplications.
    Since we are now dealing with vectors and matrices, their dimensions are important
    because if they don't line up properly, we can't multiply and add them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's view the preceding MLP in its full matrix form. (To keep things simple,
    we will go through it layer by layer and we will use the second form since our
    input is in vector form.) To simplify the view and to properly understand what
    is happening, we will now denote ![](img/ca351150-a3f6-4865-963e-7ee02f9b96da.png) and [![](img/62411681-fe78-4517-bee2-413408f0ed74.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate *z^([1])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bedf5dd0-6d85-47b3-a534-7b9d61fc79c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *h^([1])*  as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c63f4068-7687-4f18-bf31-7f525f49e4b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *z^([2])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ede43e9-b1e2-4853-acab-4852ba7b9340.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *h^([2])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c65fd29b-e5c2-4f09-a629-55a199e1c46d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *z^([3])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4eef0a30-7d90-4f5f-bcfa-dfa210079eed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *h^([3])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4511cb88-0b8a-410e-ba8a-43da43868ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate *z^([4])* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e798ced7-1c46-48d6-9c68-aadfa57c6889.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculate ***y*** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/424fa6be-32b2-4d38-999f-e13ffa5c8bc0.png)'
  prefs: []
  type: TYPE_IMG
- en: There we have it. Those are all the operations that take place in our MLP.
  prefs: []
  type: TYPE_NORMAL
- en: I have slightly tweaked the preceding notation by putting [![](img/6269823e-af7e-4a18-b7ac-49cd77c585ed.png)] in
    brackets and writing *y* as a vector, even though it is clearly a scalar. This
    was only done to keep the flow and to avoid changing the notation. *y* is a vector
    if we use the *k*-class classification (giving us multiple output neurons).
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you think back to [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml),
    *Linear Algebra*, where we did matrix multiplication, we learned that when a matrix
    or vector is multiplied by another matrix with differing dimensions, then the
    resulting matrix or vector is of a different shape (except, of course, when we
    multiply by the identity matrix). We call this mapping because our matrix maps
    points in one space to points in another space. Keeping this in mind, let's take
    a look again at the operations that were carried out in our MLP. From this, we
    can deduce that our neural network maps our input vector from one Euclidean space
    to our output vector in another Euclidean space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this observation, we can generalize and write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32adc558-0a11-49ed-9097-85e4291b0b22.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/0b1e6f17-8765-410b-8f0e-8764a5481669.png)] is our MLP, [![](img/d4d6931d-2030-4b68-8c7f-82d126da64e7.png)] is
    the number of nodes in the dimension of the input layer, [![](img/5c4afa24-34e8-41be-9565-176e8bf53239.png)] is
    the number of nodes in the output layer, and *L* is the total number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are a number of matrix multiplications that take place in the
    preceding network and each has different dimensions, which tells us that a sequence
    of mappings takes place (from one layer to the next).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then write the mappings individually, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/461302a4-1d57-4d82-a5a0-cf6d12ea57a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each *f^i* value maps the *l^(th)* layer to the *l+1^(st)* layer. To make
    sure we have covered all of our bases, ![](img/c90c9a0b-7e57-426e-ba83-d952c0292587.png) and ![](img/58a84bec-44f5-4c0a-ac8c-8b2c9b24651d.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can summarize our MLP in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79a27b1a-f32a-41e2-85a8-e1b0f1480971.png)'
  prefs: []
  type: TYPE_IMG
- en: With that done, we can now move on to the next subsection where we will understand
    activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have mentioned activation functions a few times so far and we introduced
    one of them as well—the sigmoid activation function. However, this isn't the only
    activation function that we use in neural networks. In fact, it is an active area
    of research, and today, there are many different types of activation functions.
    They can be classified into two types—linear and non-linear. We will focus on
    the latter because they are differentiable and this property is very important
    for us when we train neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start, we will take a look at sigmoid since we''ve already encountered it.
    The sigmoid function is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65343faf-854d-42ef-b64e-62a778b610be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d904765c-afb9-4ec4-a8bc-71d32c02a4ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid activation function takes the sum of the weighted inputs and bias
    as input and compresses the value into the `(0, 1)` range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its derivative is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b02c4eb1-3e11-4dd7-b385-45cbe316d190.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbc8a1ff-d0f2-423a-affc-e76fcd497b49.png)'
  prefs: []
  type: TYPE_IMG
- en: This activation function is usually used in the output layer for predicting
    a probability-based output. We avoid using it in the hidden layers of deep neural
    networks because it leads to what is known as the vanishing gradient problem.
    When the value of *x* is either greater than `2` or less than `-2`, then the output
    of the sigmoid function is very close to `1` or `0`, respectively. This hinders
    the network's ability to learn or slows it down drastically.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic tangent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another activation function used instead of the sigmoid is the hyperbolic tangent
    (*tanh*). It is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29a6dbcc-29d9-48dd-8ce1-5dd8eee8612c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c34fe00f-f828-46e6-acd4-0bba19b8686a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `tanh` function squashes all the output values into the `(-1, 1)` range.
    Its derivative is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a78c193d-d87d-4b24-8dbd-245871bc6541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/611ab700-5c35-47a8-ae99-da0ee9ce946e.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph you can tell that the `tanh` function is zero-centered,
    which allows us to model values that are very positive, very negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The softmax activation function normalizes a vector containing *K* elements
    into a probability distribution over the *K* elements. For this reason, it is
    generally used in the output layer to predict the probability of it being one
    of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/078e746d-755c-49e1-ad8e-eb385d1bd08a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Its derivative can be found using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bea56c6-9781-4b10-8394-5e5853583f38.png)'
  prefs: []
  type: TYPE_IMG
- en: Rectified linear unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified linear unit** (**ReLU**) is one of the most widely used activation
    functions because it is more computationally efficient than the activation functions
    we have already seen; therefore, it allows the network to train a lot faster and
    so converge more quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8f560b9-eeda-4c48-9b5a-02ced40f9feb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abefcb80-ff62-440c-a9b2-b194a6a3b7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, all the negative values for *x* are clipped off and turn into
    `0`. It may surprise you to know that even though this looks like a linear function,
    it has a derivative that is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b76861-ee1f-4b43-93ea-a958ad18ec33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4686eec-9555-4d48-b6d4-515c8688ba4a.png)'
  prefs: []
  type: TYPE_IMG
- en: This, too, faces some problems in training—particularly, the dying ReLU problem.
    This occurs when the input values are negative and this hinders learning because
    we cannot differentiate `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leaky ReLU is a modification of the ReLU function that we saw in the previous
    section and it not only enables the network to learn faster but it is also more
    balanced as it helps deal with vanishing gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The leaky ReLU function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a63e576-a013-457c-b768-b511947fca9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d8475c2-09b7-4ab2-9f40-07f8719824a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the difference here is that the negative values of *x* that
    were clipped off before are now rescaled to ![](img/fe6ed447-d293-4927-8093-23183b8cb1af.png),
    which overcomes the dying ReLU problem. The derivative of this activation function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7b04e0f-b2e7-4f90-8da3-de579d948f75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c33ce17c-74ce-4b21-98aa-3f6a08ca261d.png)'
  prefs: []
  type: TYPE_IMG
- en: Parametric ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Parametric ReLU** (**PReLU**) is a variation of the leaky ReLU activation
    function and has similar performance improvements to it, except that here, the
    parameters are learnable whereas before they were not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PReLU function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d29916bd-ccbb-42a3-b1f8-bf7ab4f3ed13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a26d887b-874d-4131-972b-c714c0c2625b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea942e7c-4a0a-4453-8bbe-66e5b96e55de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6903a6f6-6f11-4383-a04c-2a3b1234fdd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponential linear unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exponential linear unit** (**ELU**) is another variation of the leaky ReLU
    activation function, where instead of having a straight line for all cases of ![](img/e0e33eee-fef0-45c8-9c61-885d9b83f959.png),
    it is a log curve.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ELU activation function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12b5ff24-5b55-422f-a554-2fccbc506809.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13d45385-b514-4f7b-a22e-2925ce8343c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative of this activation function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5bb5a77-8275-4ed0-9868-6bfdf38b94a5.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The loss function is a very critical part of neural networks and their training.
    They give us a means of calculating the error of a network after a forward pass
    has been computed. This error compares the neural network output with the target
    output that was specified in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are two errors in particular that are of concern to us—the local error
    and the global error. The local error is the difference between the output expected
    of a neuron and its actual output. The global error, however, is the total error
    (the sum of all the local errors) and it tells us how well our network is performing
    on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of methods that we use in practice and each has its own use
    cases, advantages, and disadvantages. Conventionally, the loss function is referred
    to as the cost function and is denoted as *J(θ)* (or, equivalently, *J(W,b)*).
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) is the same as the L1 loss we saw in [Chapter
    3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml), *Probability and Statistics*,
    and it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0621604-a42a-4f1a-a340-cfee0baf3b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of samples in our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: What we are doing here is calculating the absolute distance between the prediction
    and the true value and averaging over the sum of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) is one of the most commonly used loss functions,
    especially for regression tasks (it takes in a vector and outputs a scalar). It
    calculates the square of the difference between the output and the expected output.
    It looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/975006bd-f561-4375-8b79-780d31b47f91.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of samples in our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding equation, we calculate the square of the L2 norm. Intuitively,
    we should be able to tell that when [![](img/6ede8b8e-9f4b-4a57-9be8-409aa2efd1aa.png)] ,
    the error is 0, and the larger the distance between the points, the larger the
    error. The reason we use this is that it always outputs a positive value and by
    squaring the distance between the output and expected output, it allows us to
    differentiate between small and large errors with greater ease and correct them.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Root mean squared error** (**RMSE**) is simply the square root of the preceding
    MSE function and it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/852bea1b-21f1-4452-913a-00c654336837.png)'
  prefs: []
  type: TYPE_IMG
- en: The reason we use this is that it scales back the MSE function to the scale
    it was originally at before we squared the errors, which gives us a better idea
    of the error with respect to the target(s).
  prefs: []
  type: TYPE_NORMAL
- en: The Huber loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Huber loss looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4bdc492-ee0d-4da4-9caa-b13cf5c69164.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ε is a constant term that we can configure. The smaller it is, the more
    insensitive the loss is to large errors and outliers, and the larger it is, the
    more sensitive the loss is to large errors and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you look closely, you should notice that when ε is very small, the Huber
    loss is similar to MAE, but when it is very large, it is similar to MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Cross entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross entropy loss is used mostly when we have a binary classification problem;
    that is, where the network outputs either 1 or 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we are given a training dataset, [![](img/89873e28-1e09-4bbf-a107-d969446d59c2.png)] and
    [![](img/d21a5ae5-3daf-47d5-bf91-2871160d90d7.png)]. We can then write this in
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e66a7df7-8d7a-44d0-ade2-1bb56a78564f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *θ* is the parameters of the network (weights and biases). We can express
    this in terms of a Bernoulli distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29d4cb2b-be80-4664-965b-7dfa4bf1a434.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability, given the entire dataset, is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3f8a931-1ad0-4460-a978-83b0a96478f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take its negative-log likelihood, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42581895-dccc-430f-98fa-f1bd6b524fa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be5d8e99-7a02-44cd-9d97-b26ddd07e808.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cross entropy is also used when we have more than two classes. This is known
    as **multiclass cross entropy**. Suppose we have *K* output units, then, we would
    calculate the loss for each class and then sum them together, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bbf3b40-e925-4231-91b6-09f8522ec81e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/1ec5fd38-ea69-4982-bc7d-e79cd01d8667.png)] is the probability
    that observation (*i*) belongs to class *k*.
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler divergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kullback-Leibler** (KL)** divergence** measures the divergence of two probability
    distributions, *p* and *q*. It looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73a11d7b-8708-41af-bbab-82d8c5fc26ad.png)'
  prefs: []
  type: TYPE_IMG
- en: So, when *p(x)=q(x)*, the KL divergence value is 0 at all points. This is usually
    used in generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Jensen-Shannon divergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the KL divergence, the **Jensen-Shannon** (**JS**) divergence measures
    how similar two probability distributions are; however, it is smoother. The following
    equation represents the JS divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec2e7ec1-cfcf-4c33-8cd5-307053b2dfe2.png)'
  prefs: []
  type: TYPE_IMG
- en: This behaves a lot better than KL divergence when *p(x)* and *q(x)* are both
    small.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how the forward passes are computed in MLPs, as well as how
    to best initialize them and calculate the loss of the network, it is time for
    us to learn about backpropagation—a method that allows us to calculate the gradient
    of the network using the information from the loss function. This is where our
    knowledge of multivariable calculus and partial derivatives comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, this network is fully connected, which means all the nodes in
    each layer are connected to—and so have an impact on—the next layer. It is for
    this reason that in backpropagation we take the derivative of the loss with respect
    to the weights of the layer closest to the output, then the one before that, and
    so on, until we reach the first layer. If you don''t yet understand this, don''t
    worry. We will go through backpropagation in detail and use the network from earlier
    as an example. We will assume that the activation function is sigmoid and our
    loss function is cross entropy. We will first calculate the derivative of the
    loss ([![](img/b3c63103-f3cb-42be-84d8-237d3b9f1ba4.png)]) with respect to *W^([4])*,
    which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5971378d-f9e5-44ed-bfff-00e5806d6bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/74578a41-447c-4c30-9ac3-17dc423088e6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/fbf77b44-2541-4e37-b7d7-78b0629a59f6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/84e2b8f4-2a9f-485c-a166-30ee953c792f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/2f2a92d2-a163-42b7-ba81-52ac4ac6ce80.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/732afee6-3253-4cca-ba9c-14258bb9b268.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, we have finished computing the first derivative. As you can see,
    it takes quite a bit of work, and calculating the derivative for each layer can
    be a very time-consuming process. So, instead, we can make use of the chain rule
    from calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, let''s say [![](img/a8629f59-6322-4371-9765-a8886500192e.png)] and [![](img/ca778ea0-b62b-4a10-b82b-b7983efea97d.png)] and
    assume that [![](img/d67fa046-f726-435b-a2a2-1457433eadef.png)]. Now, if we want
    to calculate the gradient of the loss with respect to *W^([2])*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/593b0c14-ca82-419c-90e9-b4523b2154f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/444ad371-8e2f-4897-90df-4ec18c0f615e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Suppose we do want to find the partial of the loss with respect to *b^([4])*;
    this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f02076f-6afa-4cc7-b5d4-b80104251477.png)'
  prefs: []
  type: TYPE_IMG
- en: Before we move on to the next section, pay close attention to the preceding
    derivative, [![](img/0b1d4d1d-0fa9-4744-b002-edcc2ea7c9a3.png)]. If you look back
    to earlier on in the *Layers* section, [![](img/e03583c2-1673-49bb-acc8-47b9f00164e9.png)] were
    all vectors and matrices. This is still true. Because we are again dealing with
    vectors and matrices, it is important that their dimensions line up.
  prefs: []
  type: TYPE_NORMAL
- en: We know that ![](img/54559173-808e-482d-bfb0-53c473fad4bc.png), but what about
    the others? I will leave this to you as an exercise to determine whether or not
    the other is correct and if it is not, how would you change the order to ensure
    it is?
  prefs: []
  type: TYPE_NORMAL
- en: If you're feeling very confident in your math abilities and are up for a challenge,
    I encourage you to try finding the derivative, [![](img/06d785f2-0ebb-4e1a-9f28-25e3989dd48b.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Training neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an understanding of backpropagation and how gradients are computed,
    you might be wondering what purpose it serves and what it has to do with training
    our MLP. If you will recall from [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml),
    *Vector Calculus*, when we covered partial derivatives, we learned that we can
    use partial derivatives to check the impact that changing one parameter can have
    on the output of a function. When we use the first and second derivatives to plot
    our graphs, we can analytically tell what the local and global minima and maxima
    are. However, it isn't as straightforward as that in our case as our model doesn't
    know where the optima is or how to get there; so, instead, we use backpropagation
    with the gradient descent as a guide to help us get to the (hopefully global)
    minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 4](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml), *Optimization*,
    we learned about gradient descent and how we iteratively move from one point on
    the function to a lower point on the function that is in the direction of the
    local/global minima by taking a step in the direction of the negative of the gradient.
    We expressed it in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d7750e6-5c01-45c1-8a47-3f2aeb580569.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, for neural networks, the update rule for the weights, in this case,
    is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d12e7fe-323f-4fd8-adcc-a9f01f783955.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *θ* = *(W,b)*.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, while this does look similar, it isn't the optimization we have
    learned. Our goal here is to minimize the total loss of the network and update
    our weights accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](feeeb2a4-650e-445a-8f97-8c0ebb2538eb.xhtml), *Optimization*,
    we mentioned that before we start optimizing, we need an initial (starting) point,
    which is the purpose of initialization. This is an extremely important part of
    training neural networks because as mentioned earlier on in this chapter, neural
    networks have a lot of parameters—often, well over tens of millions—which means
    that finding the point in the weight space that minimizes our loss can be very
    time consuming and challenging (because the weight space is non-convex; that is,
    there are lots of local minima and saddle points).
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, finding a good initial point is important because it makes
    it easier to get to the optima and reduce the training time, as well as reducing
    the chances of our weights either vanishing or exploding. Let's now explore the
    various ways that we can initialize our weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: All zeros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, here we set the initial weights and biases of our model
    to be zeros. I don't recommend doing this because, as you may have guessed, this
    means that all the neurons in our model are dead. In fact, this is the very problem
    we want to avoid when training our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens anyway. For the sake of simplicity, let''s suppose
    we have the following linear classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64678005-0a60-463a-b72c-2e0bbf92bff7.png)'
  prefs: []
  type: TYPE_IMG
- en: If the weights are initialized as 0, then our output is always 0, which means
    we lost all the information that was part of our training data and the network
    that we put so much effort into building learns nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Random initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way of initializing our weights to be non-zero is to use random initialization
    and for this, we could use one of two distributions—the normal distribution or
    the uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: To initialize our parameters using the normal distribution, we have to specify
    the mean and the standard deviation. Usually, we choose a mean of 0 and a standard
    deviation of 1\. To initialize using the uniform distribution, we usually use
    the [-1, 1] range (where there is an equal probability of any value in the range
    being picked).
  prefs: []
  type: TYPE_NORMAL
- en: While this gives us weights that we can use in training, it is very slow and
    has previously resulted in vanishing and exploding gradients in deep networks,
    resulting in mediocre performance.
  prefs: []
  type: TYPE_NORMAL
- en: Xavier initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, if our weights are too small, then they vanish, which results
    in dead neurons and, conversely, if our weights are too big, we get exploding
    gradients. We want to avoid both scenarios, which means we need the weights to
    be initialized just right so that our network can learn what it needs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this problem, Xavier Glorot and Yoshua Bengio created a normalized
    initialization method (generally referred to as Xavier initialization). It is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/457d8e0b-c3d7-4d87-b136-0e25d3a09732.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n[k]* is the number of neurons in layer *k*.
  prefs: []
  type: TYPE_NORMAL
- en: But why does this work better than randomly initializing our network? The idea
    is that we want to maintain the variance as we propagate through subsequent layers.
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you will know by now, what we are trying to build here are networks that
    can learn to map an input to an output. For our network to be able to do this,
    it needs to be fed data—and lots of it. Therefore, it is important for us to know
    what the data should look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a classification or regression task. Our data will then
    take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d26f3546-527b-4ce8-889d-7fbe1ac0af4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we assume the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1be36326-723b-4d59-b4bc-b2160f8f5a5b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, each sample in the dataset has the input (*x[i]*) and a corresponding
    output/target (*y[i]*). However, depending on the task, our output will look a
    bit different. In regression, our output can take on any real value, whereas in
    classification, it must be one of the classes we can predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data (*x*), as you may expect, contains all the various information we
    want to use to predict our target variables (*y*) and this, of course, depends
    on the problem. As an example, let''s take the Boston Housing dataset, which is
    a regression task. It contains the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: The per-capita crime rate by town
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion of residential land zoned for lots over 25,000 square feet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion of non-retail business acres per town
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Charles River dummy variable (1 if tract bounds river and 0 if not)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nitric oxide concentration value (parts per 10 million)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average number of rooms per dwelling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion of owner-occupied units built before 1940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted distances to five Boston employment centers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The index of accessibility to radial highways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full-value property tax rate per $10,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pupil-to-teacher ratio by town
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proportion of African Americans by town
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The percentage of the population that is of a lower status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target variable is the median value of owner-occupied homes in $1,000.
  prefs: []
  type: TYPE_NORMAL
- en: All the data is numerical (since the machines don't really read or know what
    those labels mean, but they do know how to parse numbers).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at a classification problem—since we are trying to predict which
    class our data belongs to, the target will become a vector instead of a scalar
    (as it is in the preceding dataset), where the dimension of the target vector
    will be the number of categories. But how do we represent this target vector?
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a dataset of images with the corresponding target labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d724ddb-33aa-4f14-9bc0-858c4cd46694.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, each label has a digit assigned to it and during training,
    our network could mistake these for trainable parameters, which we obviously would
    want to avoid. Instead, we can one-hot encode this, thereby turning the label
    vector into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0088e534-5123-4a3b-ad07-723176a09347.png)'
  prefs: []
  type: TYPE_IMG
- en: Great! Now we know what is in a dataset and how datasets are structured. But
    what now? We split the dataset into training, testing, and validation sets. How
    we split the data into the three respective sets depends largely on how much data
    we have. In the case of deep learning, we will, more often than not, be dealing
    with very large datasets; that is, millions to tens of millions of samples.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, we generally select 80-90% of the dataset to train our network,
    and the remaining 10-20% is split into two portions—the validation and test sets.
    The validation set is used during training to determine whether our network has
    overfit or underfit to the data and the test set is used at the end to check how
    well our model generalizes to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to get into the really fun stuff (and what you picked up this
    book for)—deep neural networks. The depth comes from the number of layers in the
    neural network and for an FNN to be considered deep, it must have more than 10
    hidden layers. A number of today's state-of-the-art FNNs have well over 40 layers.
    Let's now explore some of the properties of deep FNNs and get an understanding
    of why they are so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall, earlier on we came across the universal approximation theorem,
    which stated that an MLP with a single hidden layer could approximate any function.
    But if that is the case, why do we need deep neural networks? Simply put, the
    capacity of a neural network increases with each hidden layer (and the brain has
    a deep structure). What this means is that deeper networks have far greater expressiveness
    than shallower networks. This is something we came across earlier when learning
    about MLPs. We saw that by adding hidden layers, we were able to create a network
    that was able to learn to solve a problem that a linear neural network was not
    able to.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, deeper networks are preferred over wider networks, not because
    they improve the overall performance, but because networks with more hidden layers
    (but less width) have much fewer parameters than wider networks with fewer hidden
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we have two networks—one that is wide and one that is deep. Both
    networks have 20 inputs and 6 output nodes. Let's calculate the total number of
    parameters for both layers; that is, the number of connections between all the
    layers and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our wide neural network has two hidden layers, each with 1,024 neurons. The
    total number of parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dee8c6bc-e85c-4a0b-a207-5b43642263dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our deep neural network has 12 hidden layers, each with 150 neurons. The total
    number of parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcca0a2a-0630-4eda-9e99-502be89e3da5.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the deeper network has less than half the parameters that the
    wider network does.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first learned about a simple FNN, known as the MLP, and
    broke it down into its individual components to get a deeper understanding of
    how they work and are constructed. We then extended these concepts to further
    our understanding of deep neural networks. You should now have intimate knowledge
    of how FNNs work and understand how various models are constructed, as well as
    understand how to build and possibly improve them for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move on to the next chapter, where we will learn how to improve our
    neural networks so that they generalize better on unseen data.
  prefs: []
  type: TYPE_NORMAL
