<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regularization</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned about (deep) feedforward neural networks and how they are structured. We learned how these architectures can leverage their hidden layers and non-linear activations to learn to perform well on some very challenging tasks, which linear models aren't able to do. We also saw that neural networks tend to overfit to the training data by learning noise in the dataset, which leads to errors in the testing data. Naturally, since our goal is to create models that generalize well, we want to close the gap so that our models perform just as well on both datasets. This is the goal of regularization—to reduce test error, sometimes at the expense of greater training error.</p>
<p>In this chapter, we will cover a variety of methods used in regularization, how they work, and why certain techniques are preferred over others. This includes limiting the capacity of a neural network, applying norm penalties and dataset augmentation, and more.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>The need for regularization</li>
<li>Norm penalties</li>
<li>Early stopping</li>
<li>Parameter typing and sharing</li>
<li>Dataset augmentation</li>
<li>Dropout</li>
<li>Adversarial training</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The need for regularization</h1>
                </header>
            
            <article>
                
<p>In previous chapters, we learned how feedforward neural networks are basically a complex function that maps an input to a corresponding target/label by learning the underlying distribution using the training data. We can recall that during training, after an error has been calculated during the forward pass, backpropagation is used to update the parameters in order to reduce the loss and better approximate the data distribution. We also learned about the capacity of neural networks, the bias-variance trade-off, and how neural networks can underfit or overfit to the training data, which prevents it from being able to perform well on unseen data or test data (that is, a generalization error occurs). </p>
<p>Before we get into what exactly regularization is, let's revisit overfitting and underfitting. Neural networks, as we know, are universal function approximators. Deep neural networks have many hidden layers, which means there are a lot of parameters that need to be trained. As a general rule, the more parameters a model has, the more complex it is, which means there's a greater risk of it overfitting to the training data.</p>
<p>This means that our model has perfectly learned all the patterns that exist in the data, including the noise, and has zero loss on the training data, but has a high loss on the test data. Additionally, overfitted models, in general, have a lower bias and a very high variance. Conversely, models with fewer parameters tend to be simpler, which means they are more likely to underfit to the training data because they observe a small portion of the data that doesn't differ much. Therefore, they tend to have a much greater bias, which also leads to high variance. The following diagram illustrates the preceding explanation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1049 image-border" src="Images/165391af-7a2f-46ab-ae26-0993b3440a6d.png" style="width:26.42em;height:23.00em;"/></p>
<p>Somewhere in between overfitting and underfitting is a sweet spot where we have the optimal capacity; that is, the model hyperparameters that are perfectly suited to the task and data at hand—this is what we are aiming for. This tells us that the goal of regularization is to prevent our model from overfitting and that we prefer simpler models over vastly complex ones. However, the best model is one that is large and properly regularized. </p>
<p>Now that we know the purpose of regularization, let's explore some of the many ways that we can regularize our deep neural networks. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Norm penalties</h1>
                </header>
            
            <article>
                
<p><span>Adding a parameter norm penalty to the objective function is t</span>he most classic of the regularization methods. What this does is limit the capacity of the model. This method has been around for several decades and predates the advent of deep learning. We can write this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e8bc159d-b7cf-44a3-824e-064daec6d725.png" style="width:13.75em;height:1.33em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/535e6a8d-0d2d-47bc-8ad3-07bcbb01091e.png" style="width:4.92em;height:1.33em;"/></sub>. The <em>α</em> value, in the preceding equation, is a hyperparameter that determines how large a regularizing effect the regularizer will have on the regularized cost function. The greater the value of <em>α</em> is, the more regularization is applied, and the smaller it is, the less of an effect regularization has on the cost function. </p>
<p>In the case of neural networks, we <span>only</span><span> </span><span>apply the parameter norm penalties to the weights since they control the interaction or relationship between two nodes in successive layers, and we leave the biases as they are since they need less data in comparison to the weights.</span></p>
<p><span>There are a few different choices we can make when it comes to what kind of parameter norm to use, and each has a different effect on the solution. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">L2 regularization </h1>
                </header>
            
            <article>
                
<p>The L2 regularization method is often referred to as <strong>ridge regression</strong> (but more commonly known as <strong>weight decay</strong>). It forces the weights of the network in the direction of the origin through the following regularization term to the objective function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ec96d3c7-c42a-4279-9a24-3d58349d85e2.png" style="width:6.17em;height:2.25em;"/></p>
<div class="packt_infobox">For simplicity, we will assume that <em>θ</em> = <em>w</em> and that all the letters are matrices.</div>
<p>The regularized objective function, in this case, will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aab6dd33-f3d6-4e56-8f1d-de0206215020.png" style="width:14.33em;height:1.92em;"/></p>
<p>If we take its gradient, then it becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ad2c2931-b5b2-4137-ae9b-3346f20ea6df.png" style="width:14.92em;height:1.25em;"/></p>
<p>Using the preceding gradient, we can calculate the update for the weights at each gradient step, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/48b66bb6-9b56-4820-ac10-bdac9cae0707.png" style="width:12.50em;height:1.08em;"/></p>
<p>We can expand and rewrite the right-hand side of the preceding update as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b96ed906-1d3e-4008-a0a9-d1f1bed34247.png" style="width:16.75em;height:1.42em;"/></p>
<p>From this equation, we can clearly see that the modified learning rule causes our weight to shrink by <sub><img class="fm-editor-equation" src="Images/436ce41e-e6f7-4bc4-a7b1-a2221ec84250.png" style="width:4.25em;height:1.42em;"/></sub> at every step, as in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1050 image-border" src="Images/09e2fc82-bb08-45e9-a9eb-18ec80d4362d.png" style="width:26.33em;height:21.00em;"/></p>
<p>In the preceding diagram, we can see the effect that L2 regularization has on our weights. The solid circles toward the top-right side represent contours of equal value of the original object function, <sub><img class="fm-editor-equation" src="Images/78025785-3209-4d95-8579-e7deeb0a9c6f.png" style="width:4.92em;height:1.33em;"/></sub>, which we have not yet applied our regularizer to. The dotted circles, on the other hand, represent the contours of the regularizer term, <img class="fm-editor-equation" src="Images/b633427d-d029-4083-99b0-bdcd65bd2202.png" style="width:3.42em;height:1.33em;"/>. Finally, <img class="fm-editor-equation" src="Images/33f695a7-3bd0-4db4-a59b-d0a340606de8.png" style="width:0.92em;height:1.08em;"/>, the point where both the contours meet, represents when competing objectives reach equilibrium. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">L1 regularization</h1>
                </header>
            
            <article>
                
<p>Another form of norm penalty is to use L1 regularization, which is sometimes referred to as <span><strong>least absolute shrinkage and selection operator</strong> </span><span>(</span><strong><span>lasso</span></strong><span>) regression. In this case, the regularization term is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c3058fed-c4ca-4434-888a-e2ea1c42cb6d.png" style="width:10.75em;height:2.50em;"/></p>
<p>What this does is it sums together the absolute values of the parameters. The effect that this has is that it introduces sparsity to our model by zeroing out some of the values, telling us that they aren't very important. This can be thought of as a form of feature selection.</p>
<p>Similar to the preceding L2 regularization, in L1 regularization, the <em>α</em> <span>hyperparameter </span><span>controls how much of an effect the regularization has on the objective function:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/187af489-baa7-4607-afea-0ac2a430fd27.png" style="width:16.17em;height:1.50em;"/></p>
<p>This is illustrated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1051 image-border" src="Images/ae903d9e-0c3c-47d3-b14f-b793ae1cb04c.png" style="width:25.50em;height:20.25em;"/></p>
<p>As you can see in the preceding diagram, the contours of the objective function now meet at the axes instead of at a point away from it, as was the case in L2 regularization, which is where the sparsity in this method comes from.</p>
<p>Now that we have learned how we can regularize our deep neural networks, let's have a look at what early stopping is in the following section.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Early stopping</h1>
                </header>
            
            <article>
                
<p>During training, we know that our neural networks (which have sufficient capacity to learn the training data) have a tendency to overfit to the training data over many iterations, and then they are unable to generalize what they have learned to perform well on the test set. One way of overcoming this problem is to plot the error on the training and test sets at each iteration and analytically look for the iteration where the error from the training and test sets is the closest. Then, we choose those parameters for our model.</p>
<p>Another advantage of this method is that this in no way alters the objective function in the way that parameter norms do, which makes it easy to use and means it doesn't interfere with the network's learning dynamics, which is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1052 image-border" src="Images/28316bfe-c660-4d02-90b0-ce7936d2fdaf.png" style="width:26.67em;height:15.33em;"/></p>
<p>However, this approach isn't perfect—it does have a downside. It is computationally expensive because we have to train the network longer than is needed and collect more data for it, and then observe the point where the performance started to degrade. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Parameter tying and sharing</h1>
                </header>
            
            <article>
                
<p>The preceding parameter norm penalties work by penalizing the model parameters when they deviate from 0 (a fixed value). But sometimes, we may want to express prior knowledge about which parameters would be better suited to the model. Although we may not know what those parameters are, thanks to domain knowledge and the architecture of the model, we know that there are likely to be some dependencies between the parameters of the model.</p>
<p>These dependencies could be some specific parameters that are closer to some than to others. Let's suppose we have two different models for a classification task and detect the same number of classes. Their input distributions, however, are not the same. Let's name the first model <em>A</em> with <em>θ<sup>(A)</sup></em> <span>parameters </span><span>and the second model</span> <em>B</em><span> with </span><em>θ<sup>(B)</sup></em><span> </span><span>parameters</span><span>. Both of these models map their respective inputs to the outputs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/03ecf77a-5b1e-43e1-84ab-d086d87e5109.png" style="width:7.42em;height:1.50em;"/> and <img class="fm-editor-equation" src="Images/4f0c1b9e-d87a-4b67-8bd6-e4385eaadc24.png" style="width:7.33em;height:1.50em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Naturally, since both of these models are working on relatively similar (maybe even the same) task(s) and so likely have similar (or the same) input distributions, both model <em>A</em> and <span>model <em>B</em>'s parameters should be close</span> to each other. </p>
<p>We can use a parameter norm penalty, such as the L2 penalty, to determine the closeness of the <em>θ<sup>(A)</sup></em> and <em>θ<sup>(B)</sup></em><span> </span><span>parameters</span><span>, as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/589248e9-de5a-45ad-9edb-660777b80a8e.png" style="width:12.42em;height:1.42em;"/></p>
<div class="packt_infobox">We can use other metrics besides the L2 norm<span> to measure the distance.</span></div>
<p>This method of forcing parameters to be close to each other is referred to as <strong>parameter sharing</strong>. The reason for this is that this can be interpreted as the different models sharing a set of parameters. This approach is preferred to parameter norm penalties because it requires less memory since we only have to store a unique set of shared parameters. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dataset augmentation</h1>
                </header>
            
            <article>
                
<p>Deep feedforward networks, as we have learned, are very data-hungry and they use all this data to learn the underlying data distribution so that they can use their gained knowledge to make predictions on unseen data. This is because the more data they see, the more likely it is that what they encounter in the test set will be an interpolation of the distribution they have already learned. But getting a large enough dataset with good-quality labeled data is by no means a simple task (especially for certain problems where gathering data could end up being very costly). A method to circumvent this issue is using data augmentation; that is, generating synthetic data and using it to train our deep neural network. </p>
<p>The way synthetic data generation works is that we use a generative model (more on this in <a href="916c9cb2-14fa-44d9-a899-90948a342c52.xhtml">Chapter 12</a>, <em>Generative Models</em>) to learn the underlying distribution of the dataset that we will use to train our network for the task at hand, and then use the generative model to create synthetic data that is similar to the ground-truth data so that it appears to have come from the same dataset. We can also add small variations to the synthetic data to make the model more robust to noise.</p>
<p>This method has proven to be very effective in the case of computer vision—particularly object detection/classification—which we make use of in convolutional neural networks (which we will learn about in <a href="2c830a26-9964-47fb-8d69-904e4f087b95.xhtml">Chapter 9</a>, <em>C<span>onvolutional Neural Networks</span></em>). </p>
<p>Another type of data augmentation that is often <span>used</span><span> </span><span>in image recognition is image cropping and image rotation, where we either crop a large segment of the input image or rotate it by some angle. These methods have also been proven to increase robustness and improve generalization on unseen data. We could also corrupt, blur, or add in some Gaussian noise to the images to make the network more robust since a lot of real-world data tends to be noisy.</span></p>
<p>However, there are limitations to this. For example, in the case of optical character recognition (where we want to recognize letters and numbers), horizontal flips and 180-degree rotations can affect the class. After a transformation, a <em>b</em> can turn into a <em>d</em> and a <em>6</em> can turn into a <em>9</em>. There are also some problems where data augmentation simply isn't an option; an example of this is in the medical domain where we could be trying to work with MRI and CT scans. However, what we could do, in this case, is apply affine transformations, such as rotations and translations. </p>
<p>Let's focus for a moment on noise injection. There are two ways we can do this—the first is to inject noise to the input data and the second is to inject noise into the hidden units. In fact, it has been found that the addition of noise to hidden units can be a much better regularizer than parameter shrinking because it encourages stability. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dropout </h1>
                </header>
            
            <article>
                
<p>In the preceding section, we learned about applying penalties to the norm of the weights to regularize them, as well as other approaches, such as dataset augmentation and early stopping. However, there is another effective approach that is <span>widely</span><span> </span><span>used in practice, known as dropout. </span></p>
<p>So far, when training neural networks, all the weights have been learned together. However, dropout alters this idea by having the network only learn a fraction of the weights during each iteration. The reason for this is to avoid co-adaptation. This occurs when we train the entire network over all the training data and some connections end up stronger than others, thereby contributing more toward the network's predictive capabilities because the stronger connections overpower the weaker connections, effectively ignoring them. As we train the network with more iterations, some of the weaker connections essentially die out and are no longer trainable, so only a subnetwork ends up being trained, putting part of the network to waste. This is something that the preceding norm penalties are unable to address. </p>
<p>The way that dropout overcomes overfitting is by randomly (according to some predefined probability) removing (dropping out) neurons from a hidden layer; that is, we temporarily zero out some of the incoming and outgoing edges of a node so that it does not have an impact on the network during that training iteration. For example, if we have a <strong>multilayer perceptron</strong> (<strong>MLP</strong>) with one hidden layer that consists of 10 neurons and we have dropout with <em>p </em>= 0.5, then half the neurons are set to 0. If<span> <em>p </em>= 0.2,</span> then 20 percent of the neurons are dropped:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1053 image-border" src="Images/a5a6f686-7382-40c4-8525-104420cec101.png" style="width:22.33em;height:25.83em;"/></p>
<p>Let's consider an MLP with <em>L</em> hidden layers, such that <sub><img class="fm-editor-equation" src="Images/114466dd-a33d-42f4-88b2-9b7b9e0e84fe.png" style="width:6.83em;height:1.17em;"/></sub>, where the vector input to each layer is <em>z<sup>(l)</sup></em> and the output vector of each layer is <em>y<sup>(l)</sup></em> (for the sake of simplicity,<span> <em>y<sup>(0) </sup></em>= <em>x</em></span>). The weights and biases of each layer are denoted by <em>W<sup>(l)</sup></em> and <em>b<sup>(l)</sup></em>, respectively. Then, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5623174a-02fd-42c4-9fc8-e11f03bd3582.png" style="width:12.08em;height:4.83em;"/></p>
<p> </p>
<p>Here, <em>f</em> is any activation function. </p>
<p>Now, with dropout, the feedforward operations<span> </span>become the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0333e648-262e-4b49-91fb-8b0655e27aa6.png" style="width:11.83em;height:6.75em;"/></p>
<p>So, we find that <em>p </em>= 0.5 has the best regularizing effect during training. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adversarial training</h1>
                </header>
            
            <article>
                
<p>Nowadays, neural networks have started to reach human-level accuracy on <span>a number of tasks</span><span>, and in some, they can be seen to have even surpassed humans. But have they really surpassed humans or does it just seem this way? In production environments, we often have to deal with noisy data, which can cause our model to make incorrect predictions. So, we will now learn about another very important method of regularization—</span><strong>adversarial training</strong><span>. </span></p>
<p>Before we get into the what and the how of adversarial training, let's take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1054 image-border" src="Images/e3badced-6166-43aa-b615-25a0bf7b13b8.png" style="width:25.25em;height:7.67em;"/></p>
<p>What we have done, in the preceding diagram, is added in negligible Gaussian noise to the pixels of the original image. To us, the image looks exactly the same, but to a convolutional neural network, it looks entirely different. This is a problem, and it occurs even when our models are perfectly trained and have almost no error. </p>
<p>What we do is find a data point, <em>x'</em>, which is near to <em>x</em>, but the model predicts <em>x'</em> to be part of a different class. Now, to add noise to our image, we can do so as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/06453a6a-bb69-4ffc-af11-cbbae42e4790.png" style="width:14.08em;height:1.42em;"/></p>
<p>The reason this interests us is that adding adversarially perturbed data samples to our training dataset can help reduce the error on our test set.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered a variety of methods that are used to regularize the parameters of a neural network. These methods are very important when it comes to training our models because they help ensure that they can generalize to unseen data by preventing overfitting, thereby performing well on the tasks we want to use them for. In the following chapters, we will learn about different types of neural networks and how each one is best suited for certain types of problems. Each neural network has a form of regularization that it can use to help improve performance.</p>
<p>In the next chapter, we will learn about convolutional neural networks, which are used for computer vision.</p>


            </article>

            
        </section>
    </div></body></html>