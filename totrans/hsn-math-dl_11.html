<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover one of the most popular and widely used deep neural networks—the <strong>convolutional neural network</strong> (<strong>CNN</strong>, also known as <strong>ConvNet</strong>).</p>
<p>It is this class of neural networks that is largely responsible for the incredible feats that have been accomplished in computer vision over the last few years, starting with AlexNet, created by Alex Krizhevsky, Geoffrey Hinton, and <span>Ilya Sutskever,</span> which outperformed all the other models in the 2012 <strong>ImageNet Large Scale Visual Recognition Challenge</strong><span> (</span><strong>ILSVRC</strong><span>)</span>, thus beginning the deep learning revolution.</p>
<p>ConvNets are a very powerful type of neural network for processing data. They have a grid-like topology (that is, there is a spatial correlation between neighboring points) and are tremendously useful in a variety of applications, such as facial recognition, self-driving cars, surveillance, natural language processing, time-series forecasting, and much more.</p>
<p>We will start by introducing the basic building blocks of ConvNets and introduce some of the architectures used in practice, such as AlexNet, VGGNet, and Inception-v1, as well as exploring what makes them so powerful.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The inspiration behind ConvNets</li>
<li>Types of data used in ConvNets</li>
<li>Convolutions and pooling</li>
<li>Working with the ConvNet architecture</li>
<li>Training and optimization</li>
<li>Exploring popular ConvNet architectures</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The inspiration behind ConvNets</h1>
                </header>
            
            <article>
                
<p><span>CNNs are a type of <strong>artificial neural network</strong> (<strong>ANN</strong>); they are loosely inspired by the concept that the human visual cortex processes images and allows our brains to recognize objects in the world and interact with them, which allows us to do a number of things, such as drive, play sports, read, watch movies, and so on. </span></p>
<p>It has been found that computations that somewhat resemble convolutions take place in our brains. Additionally, our brains possess both simple and complex cells. The simple cells pick up basic features, such as edges and curves, while the complex cells show spatial invariance, while also responding to the same cues as the simple cells.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Types of data used in ConvNets </h1>
                </header>
            
            <article>
                
<p>CNNs work exceptionally well on visual tasks, such as object classification and object recognition in images and videos and pattern recognition in music, sound clips, and so on. They work effectively in these areas because they are able to exploit the structure of the data to learn about it. This means that we cannot alter the properties of the data. For example, images have a fixed structure and if we were to alter this, the image would no longer make sense. This differs from ANNs, where the ordering of feature vectors does not matter. Therefore, the data for CNNs is stored in multidimensional arrays. </p>
<p>In computers, images are in grayscale (black and white) or are colored (RGB), and videos (RGB-D) are made of up pixels. A pixel is the smallest unit of a digitized image that can be shown on a computer and holds values in the form of [0, 255]. The pixel value represents its intensity.</p>
<p>If the pixel value is <kbd>0</kbd>, then it is black, if it is <kbd>128</kbd>, then it is gray, and if it is <kbd>255</kbd>, then it is white. We can see this in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-965 image-border" src="Images/1da385c7-7e21-4267-8850-fd52ccd2d275.png" style="width:42.17em;height:3.42em;"/></p>
<p>As we can see, grayscale images only require 1 byte of data, but colored images, on the other hand, are made up of three different values—red, blue, and green—since any color can be shown using a combination of these three colors. We can see the colorspace in the following diagram (refer to the color diagram from the graphic bundle):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-966 image-border" src="Images/10f3ac5f-ca77-4c5b-8ef9-bf25960f2553.jpg" style="width:32.75em;height:26.42em;"/></p>
<p>Depending on where in the cube we are, we clearly get a different color. </p>
<p>Instead of looking at it as a cube or varying color intensities, we can look at it as having three separate channels—red, blue, and green. Then, each pixel requires 3 bytes of storage. </p>
<p>Normally, we cannot see the individual pixels in the images and videos that we see on our monitors because they have a very high resolution. This can vary greatly, but the pixels are usually between several hundred to several thousands of <strong>dots</strong> (pixels) <strong>per inch</strong> (<strong>dpi</strong>).</p>
<div class="packt_infobox">A bit (binary unit) is the fundamental unit of a computer and each bit can take on one of two values—0 or 1. A single byte consists of 8 bits. In case you're wondering, the [0, 255]<span> range comes from the pixel value being stored in 8 bits, (2</span><sup>8</sup> –<span> 1 = 255). However, we could also have a 16-bit data value. In colored images, we can have either 8-</span><span>bit, 16-bit, 24-bit, or 30-bit values, but we usually use 24-bit values since we have three colored pixels, RGB, and each has an 8-bit data value.</span></div>
<p>Suppose we have a grayscale image with a size of 512 <span>× 512 × 1 (height × width × channel)</span>. We can store it in a two-dimensional tensor (matrix), <sub><img class="fm-editor-equation" src="Images/24029c06-3619-459a-a965-f4cd772d2ec1.png" style="width:4.92em;height:1.67em;"/></sub>, where each <em>i</em> and <em>j</em> value is a pixel with some intensity. To store this image on our disk, we need 512 <span>× 512 = 262,144</span> bytes. </p>
<p>Now, suppose we have a colored image with a size of 512 <span>× 512 × 3 (height × width × channel)</span>. We can store it in a three-dimensional tensor, <sub><img class="fm-editor-equation" src="Images/b5ede4c4-93bb-41b5-9258-8274e0c9d967.png" style="width:4.92em;height:1.67em;"/></sub>, where each <em>i, j,</em> and <em>k</em> value is a colored pixel with some intensity. To store this image on our disk, we would need 512 <span>× 512 × 3 = 786,432</span> bytes, which tells us that storing a colored image requires a lot more space and so a longer time to process.</p>
<p>A colored video can be represented as a sequence of frames (images). We start by making the time discrete so that each frame is a fixed time step apart from the other. We can store a regular video (grayscale) in a three-dimensional array, where one axis represents the height of the frame, another represents the width, and the third represents the length of time. </p>
<p class="mce-root"/>
<p>We will learn, later in this chapter, that CNNs also work quite well for audio and time-series data because they are resistant to noise. We represent time-series data as a one-dimensional array, where the length of the array is time, which is what we convolve over.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutions and pooling</h1>
                </header>
            
            <article>
                
<p>In <a href="e1f37008-1ad5-49f6-a229-4d6249c2d7e3.xhtml">Chapter 7</a>, <em>Feedforward Neural Networks</em>, we saw how deep neural networks are built and how weights connect neurons in one layer to neurons in the previous or following layer. The layers in CNNs, however, are connected through a linear operation known as <strong>convolution</strong>, which is where their name comes from and what makes it such a powerful architecture for images. </p>
<p>Here, we will go over the various kinds of convolution and pooling operations used in practice and what the effect of each is. But first, let's see what convolution actually is.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Two-dimensional convolutions</h1>
                </header>
            
            <article>
                
<p>In mathematics, we write convolutions as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6f3cc0d7-eb91-47f5-a207-58f7836505e3.png" style="width:14.33em;height:2.67em;"/></p>
<p>What this means is that we have a function, <em>f</em>, which is our input and a function, <em>g</em>, which is our kernel. By convolving them, we receive an output (sometimes referred to as a feature map). </p>
<p>However, in CNNs, we usually use discrete convolutions, which are written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f90baffb-72db-4880-ba59-908c8c5c4ebf.png" style="width:16.17em;height:3.00em;"/></p>
<p class="CDPAlignLeft CDPAlign">Let's suppose we have a two-dimensional array with a height of 5 and a width of 5, and a two-dimensional kernel with a height of 3 and a width of 3. Then, the convolution and its output will look as follows:</p>
<p class="CDPAlignLeft CDPAlign">                                             <img class="size-full wp-image-1318 image-border" src="Images/6836f4cf-1327-431e-aef8-dd2804d3bd61.png" style="width:23.92em;height:7.25em;"/></p>
<p>Some of the values in the output matrix are left empty as an exercise for us to try the convolution by hand and get a better idea of how this operation works.</p>
<p>As you can see, the kernel slides over the input and produces a feature map with a height of 3 and a width of 3. This feature map tells us the degree to which the functions <em>f</em> and <em>g</em> overlap as one passes over the other. We can think of this as scanning the input for a certain pattern; in other words, the feature map is looking for the same pattern in different places of the input.</p>
<p>To get a better understanding of how the kernel moves over the input, think of a typewriter. The convolution starts at the top left, applies element-wise multiplication and addition, then moves a step to the right and repeats until it reaches the rightmost position without going beyond the bounds of the input. It then moves down one row and repeats this process until it reaches the bottom-right position. </p>
<p>Suppose that we <span>now</span><span> </span><span>have</span><span> a 3 </span><span>× </span><span>3 two-dimensional tensor</span><span> as input</span><span> and apply to it a 2 </span><span>× </span><span>2 kernel. It will look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/1320d8f2-aa05-4358-a6ed-d5d6b17eb917.png" style="width:26.58em;height:5.08em;"/></p>
<p>We can mathematically write the individual outputs in the feature map as follows:</p>
<p>                                              <img src="Images/64ed160e-a6a9-4109-a7db-4df26903c1dc.png" style="width:21.67em;height:5.83em;"/></p>
<p>Now, we can rewrite the preceding discrete convolution equation as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c871a72c-972f-4146-ade9-88a00429107b.png" style="width:22.50em;height:2.67em;"/></p>
<p>This gives us a much clearer idea of what is happening.</p>
<p>From the preceding operation, we can tell<span> that if we keep applying convolutions to feature m</span><span>aps, the height and width of each layer will decrease subsequently. S</span>o, sometimes, we may want to preserve the size of <em>I</em> after the convolution operation (especially if we are building a very deep CNN), in which case, we pad the outside of the matrix with zeros. What this does is it increases the size of the matrix before applying the convolution operation.</p>
<p>So, if <em>I</em> is an n × n array and our kernel is a k × k array and we want our feature map to be n × n as well, then we pad <em>I</em> once, turning it into an (n+2) <span>× (n+2)</span> array. Now, after we convolve the two, the resulting feature map will have an n × n size.</p>
<p>The padding operation looks as follows:</p>
<p>                                                     <img src="Images/5ef9913a-17ea-49e3-a233-af91c053c7a4.png" style="width:20.17em;height:7.92em;"/></p>
<p>In practice, this is referred to as full padding. When we do not pad, we refer to it as zero padding.</p>
<p>Should we want to reduce the size of the feature map, we can use a larger kernel or we can increase the stride size—each will give a different result. When the stride is 1, we slide our kernel as normal, one at a time. However, when we increase the stride to 2, the kernel hops two positions each time.</p>
<p>Let's use the preceding matrix we convolved and see what happens when we change the stride to 2:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6bfff6f5-8d59-48ad-a167-f482188a0c83.png" style="width:21.00em;height:6.92em;"/></p>
<p>Armed with this knowledge, we can calculate the resulting shape of the feature map using the following formula:</p>
<p>                     <img src="Images/ee958e51-e175-4ebb-a789-e692c1a338ca.png" style="width:33.83em;height:3.17em;"/></p>
<p>Here, <em>I</em> is an n <span>× n</span> array, <em>K</em> is a k <span>× k</span> array, <em>p</em> is the padding, and <em>s</em> is the stride. </p>
<p>Additionally, we can repeat this process as many times as we like, using different kernels and producing multiple feature maps. We then stack these outputs together and form a<span> </span><span>three-dimensional array of feature maps, which we call a layer. </span></p>
<p>For example, say we have an image with a size of 52 <span>× 52</span> and a kernel with a size of 12 <span>× 12 </span>and a stride of 2. We apply this to our input 15 times and stack the outputs together. We get a three-dimensional tensor with a size of <sub><img class="fm-editor-equation" src="Images/157eb44b-4df3-4910-820c-d2370dde3dbe.png" style="width:5.08em;height:1.33em;"/></sub>.</p>
<p>When we are building CNNs for real-world applications, it is more than likely that we will want to work with colored images. We saw previously that grayscale images can be expressed as two-dimensional tensors (matrices) and so the convolutions were two-dimensional as well. However, colored images, as we know, are made up of three channels stacked on top of each other—red, blue, and green. The image then has the <sub><img src="Images/c3738393-3ec5-419c-a2c1-d9a3a4353195.png" style="width:4.08em;height:1.33em;"/></sub> shape and <span>so the associated convolution will also have the same shape. But interestingly, convolving a colored image with a three-dimensional convolution gives us a two-dimensional feature map.</span></p>
<p>In the preceding example, we went over how to perform a convolution on a two-dimensional tensor, but colored images have three channels. So, what we are going to do is split the three channels and convolve over them individually, then sum their respective outputs together using an element-wise addition to produce a two-dimensional tensor. To get a better understanding of this, let's assume we have an input with a size of 3 <span>× 3 × 3,</span> which we can split into three channels, as so:</p>
<p>          <img src="Images/23e495bd-09a5-4138-ba52-1c74fea82d3e.png" style="width:40.92em;height:4.50em;"/></p>
<p>This tells us that <em>I<sub>i,j</sub> = R<sub>i,j</sub> + B<sub>i,j</sub> + G<sub>i,j</sub></em>. Now that we have the channels separated, let's convolve them with our<span> </span><span>2 </span><span>× 2</span><span> kernel</span><span>. After convolving each channel with our kernel, we get the following outputs:</span></p>
<ul>
<li>The result after convolving the red channel is as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c8f232b3-e6e4-4c5c-a4df-4db05de55c05.png" style="width:24.25em;height:4.42em;"/></p>
<ul>
<li>The result after convolving the blue channel is as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ba7f6281-88c3-43dc-8333-e6c09a8a5910.png" style="width:25.42em;height:4.67em;"/></p>
<ul>
<li>The result after convolving the green channel is as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/dcb251d7-d738-4461-92e7-65405cce8844.png" style="width:23.83em;height:4.33em;"/></p>
<p>Should we want to go deeper, we can mathematically write out how each element of the output was calculated. This looks as follows:</p>
<p>  <img src="Images/6c806956-9ece-4590-addb-542f10016a1f.png" style="width:46.67em;height:5.42em;"/></p>
<p>We can think of this as applying a three-dimensional convolution to the input. It is important to make a note here that the depth of the kernel is the same as that of the image and so it moves just as the two-dimensional convolution operation does.</p>
<p>Instead of applying a kernel separately to each channel, we apply a single three-dimensional kernel to the input at once and use element-wise multiplication and addition. We do so because this allows us to convolve over volumetric data. </p>
<p>Here, we applied 15 kernels with a size of 12 <span>× 12</span> and a stride of 2 to an input with a size of 52 <span>× 52</span>, and the resulting output had a size of 21 <span>× 21 × 15</span>. Now, to this output, we can apply a convolution with a size of 8 <span>× 8 × 15</span>. So, the output from this operation will have a size of 14 <span>× 14</span>. Of course, as before, we can stack multiple outputs together to form a layer.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">One-dimensional convolutions</h1>
                </header>
            
            <article>
                
<p>Now that we know how convolutions work in two dimensions, it is time for us to see how they work in one dimension. We use these for time-series data, such as those associated with stock prices or audio data. In the preceding section, the kernel moved from the top left along the axis to the top right, then dropped one or more rows (depending on the stride). This process was repeated until it reached the bottom right of the grid.</p>
<p>Here, we only convolve along the time axis—that is, the temporal dimension (from left to right). However, the effects of padding and stride still apply here as well. </p>
<p>Let's suppose we have the following data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4cb3d150-2f51-46c7-a762-3fc9f7ba4d33.png" style="width:13.00em;height:1.17em;"/></p>
<p>We also have the following kernel with a size of 1 <span>× 3</span> that we want to apply to it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/518e3c4f-a8c9-444f-8afa-81137b3c97ce.png" style="width:4.83em;height:1.17em;"/></p>
<p>Then, after convolving it with a stride of 2, we get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9fa27b86-9a7c-4d90-b737-6e13fb18657a.png" style="width:6.08em;height:1.08em;"/></p>
<p>Interestingly, we can also apply one-dimensional convolutions to matrices (images). Let's see how this works. Say we have a 4 <span>× 4</span> input matrix and a 4 <span>× 1</span> kernel. Then, the convolution will be carried out as follows:</p>
<p>                                         <img src="Images/2e4f2a72-d175-4863-9fee-76a21df36236.png" style="width:23.42em;height:5.50em;"/></p>
<p>Let's take a look under the hood and see how each of the outputs is calculated:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/39508250-614a-443b-83db-0a6cfd6398b1.png" style="width:17.67em;height:5.50em;"/></p>
<p>However, our kernel size could be larger as well, just as in the earlier case of two-dimensional convolutions. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">1 × 1 convolutions</h1>
                </header>
            
            <article>
                
<p>In the previous section, we covered two-dimensional convolutions on volumetric data and these convolutions were performed depth-wise (the depth of each convolution is the same as the depth of the input). This is essentially the same as multiplying the values along the depth of the channel with those of the kernel and then summing them together to get a single value.</p>
<p>If we take the same input as previously with a shape of 21 <span>× 21 × 15</span> and apply our 1 <span>× 1</span> kernel, which has a 1 <span>× 1 × 15</span><span> </span><span>shape,</span><span> our output will have a shape of 21. If we apply this operation 12 times, our output will then be 21 </span><span>× 21 × 12</span><span>. We use these shapes because</span> <span>they can reduce the dimensionality of our data because applying kernels</span><span> of a larger size is computationally more expensive. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Three-dimensional convolutions</h1>
                </header>
            
            <article>
                
<p>Now that we have a good idea of how two-dimensional convolutions work, it is time to move on to three-dimensional convolutions. But wait—didn't we just learn about three-dimensional convolutions? Kind of, but not really because, if you remember, they had the same depth as the volume we were convolving over and moved the same as the two-dimensional convolutions did—along the height and the width of the image. </p>
<p>Three-dimensional convolutions work a bit differently in that they convolve over the depth as well as the height and the width. This tells us that the depth of the kernel is smaller than the depth of the volume we want to convolve over, and at each step, it performs element-wise multiplication and addition, resulting in a single scalar value. </p>
<p>If we have volumetric data with a size of 21 <span>× 21 × 15</span> (as we did in the preceding section) and a three-dimensional kernel with a size of 5 <span>× 5 × 5</span> that takes a stride of 1, then the output will have a size of 16 <span>× 16 × 11</span>. </p>
<p>Visually, this looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-967 image-border" src="Images/26a7aa41-7e10-41df-81f6-6a83ca82ce97.png" style="width:31.00em;height:22.83em;"/></p>
<div class="packt_infobox">We can calculate the output shape of the three-dimensional convolution in a similar way to as we did earlier in the two-dimensional case.</div>
<p>This type of convolution is used frequently in tasks that require us to find relationships in 3D. This is <span>particularly</span><span> </span><span>used in the task of three-dimensional object segmentation and detecting actions/motion in videos. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Separable convolutions</h1>
                </header>
            
            <article>
                
<p>Separable convolutions are a rather interesting type of convolution. They work on two-dimensional inputs and can be applied spatially or depthwise. The way this works is we decompose our k <span>× k sized kernel into two smaller</span> kernels with sizes of k <span>× 1</span> and 1 <span>× k</span>. Instead of applying the k <span>× k</span> kernel, we would first apply the k <span>× 1</span> kernel and then, to its output, the 1 <span>× k</span> kernel. The reason this is used is that it reduces the number of parameters in our network. With the original kernel, we would have had to carry out k<sup>2</sup> multiplications at each step, but with separable convolution, we only have to carry out 2,000 multiplications, which is a lot less.</p>
<p>Suppose we have a 3 × 3 kernel that we want to apply to a 6 × 6 input, as follows:</p>
<p>               <img src="Images/12935ccd-895c-4c6b-aad6-a15e5998a486.png" style="width:38.50em;height:8.67em;"/></p>
<p>In the preceding convolution, our kernel will have to perform nine multiplications at each of the 16 positions before it produces our output. This is a total of 144 multiplications. </p>
<p>Let's see how the separable convolution differs and compare its results. We will first decompose our kernel into k <span>× 1</span><span> and</span> 1 <span>× k</span> kernels:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0f84a867-25e6-4fb6-9510-7cbf5a6bb5dc.png" style="width:24.75em;height:4.42em;"/></p>
<p>We will apply the kernels to our input in two steps. This looks as follows:</p>
<ul>
<li>Step 1:</li>
</ul>
<p>                     <img src="Images/033ba50e-e669-4506-a68e-6f6ecac8bf2d.png" style="width:38.33em;height:8.67em;"/></p>
<ul>
<li>Step 2:</li>
</ul>
<p>                        <img src="Images/190cfd2b-7ce7-424d-9eca-f2a3ba8e241a.png" style="width:39.25em;height:6.33em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/6f8912bd-31a5-40c1-a6ad-57f7115cfb6a.png" style="width:2.25em;height:2.00em;"/></sub> is the output from the first convolution operation and <sub><img class="fm-editor-equation" src="Images/be166d15-8da9-4fad-bff5-1313c7ff2c9b.png" style="width:2.25em;height:2.00em;"/></sub> is the output from the second. However, as you can see, we still get an output of the same size as before, but the number of multiplications that had to be carried out is fewer. The first convolution had to carry out three multiplications at each of the 24 positions for a total of 72 multiplications, and the second convolution also carried out<span> three</span><span> multiplications at each of the 16</span><span> </span><span>positions for a total of 48 multiplications. By summing the total multiplications from both convolutions, we find that, together, they carried out 120 multiplications, which is fewer than the 144 that the k × k kernel had to do.</span></p>
<p class="CDPAlignLeft CDPAlign">It is important to clarify that not every kernel is separable. As an example, let's take a look at the Sobel filter and its decomposition:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f884c661-29cb-44dd-8db0-d041e99e42ae.png" style="width:15.67em;height:3.92em;"/></p>
<p>What we just learned was spatially separable convolution. Using what we have learned so far, how do you think depth-wise convolution would work? </p>
<p>You should recall that when we went through two-dimensional convolutions, we introduced a three-dimensional kernel for colored images, where the depth was the same as the image. So, if we had an input of 8 <span>× 8 × 3</span> and a kernel with a size of 3 <span>× 3 × 3</span>, we would get an output of 6 <span>× 6 × 1</span>. However, in depth-wise separable convolutions, we split the 3 <span>× 3 × 3 kernel into three</span> kernels with a size of 3 <span>× 3 × 1</span> each, which convolves one of the channels. After applying our kernels to our input, we have an output with a size of 6 <span>× 6 × 3</span> and to this output, we will apply a kernel with a size of 1 <span>× 1 × 3,</span> which produces an output of 6 <span>× 6 × 1</span>. </p>
<p>If we want to increase the depth of the output to, say, 72, instead of applying 72 3 <span>× 3 × 3</span> kernels, we would apply 72 1 <span>× 1 × 3</span> convolutions. </p>
<p>Let's compare the two and see which is more computationally efficient. The number of multiplications that had to take place in order to compute our 6 <span>× 6 × 72</span> output using the 3 <span>× 3 × 3</span> kernel is (3<span>×3×3) × (6×6) × 72 = 69,984</span>, which is a lot! To compute the same output using depth-wise separable convolution, the number of multiplications required is (3<span>×3×1) × 3 × (6×6) + (1×1×3) × (6×6) × 72 = 8,748</span>, which is a whole lot less and therefore a lot more efficient.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transposed convolutions</h1>
                </header>
            
            <article>
                
<p class="mce-root">We know that applying a convolution repeatedly to an image reduces its size, but what if we would like to go in the opposite direction; that is, go from the shape of the output to the shape of the input while still maintaining local connectivity. To do this, we use transposed convolution, which draws its name from matrix transposition (which you should remember from <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml"/><a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>). </p>
<p>Let's suppose we have a 4 <span>× 4</span> input and a 3 <span>× 3</span> kernel. Then, we can rewrite the kernel as a 4 <span>× 16</span> matrix, which we can use for matrix multiplications to carry out our convolutions. This looks as follows:</p>
<p>  <img src="Images/50b4e204-920b-482e-b971-b83636170cba.png" style="width:49.83em;height:5.00em;"/></p>
<p>If you look closely, you will notice that each row represents one convolution operation. </p>
<p>To use this matrix, we rewrite our input as a 16 <span>× 1</span> column vector, which looks as follows:</p>
<p>                                    <img src="Images/5d3796da-764b-4f47-80fd-c0168f14f008.png" style="width:14.33em;height:21.50em;"/></p>
<p class="CDPAlignCenter CDPAlign"><span>Then, we can multiply our</span> convolution <span>matrix and column vector to get a 4 </span><span>× 1</span><span> column vector,</span> which <span>looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e064da92-100c-4165-9409-d19f95f0687b.png" style="width:3.25em;height:5.50em;"/></p>
<p>We can rewrite this in the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/54bf255f-341a-4a62-a119-ac4c12a150c4.png" style="width:5.33em;height:2.50em;"/></p>
<p>This is the same as what we saw in the previous section. </p>
<p>You might <span>now </span><span>be wondering what this has to do with transposed convolution. It's simple—we use the same concept as before, but now we use the transpose of the convolution matrix to work our way backward from the output to the input. </span></p>
<p>Let's take the preceding convolution matrix and transpose it so that it becomes a matrix with a size of 16 <span>× 4</span>:</p>
<p>                                            <img src="Images/4aff7146-5d39-46b0-810c-0fcfd787e8a6.png" style="width:10.25em;height:20.00em;"/></p>
<p>This time, the input vector we multiply with will be a 4 <span>× 1</span><span> column vector:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/baf87720-d222-4e41-9ce1-e0782f5bc099.png" style="width:3.33em;height:6.00em;"/></p>
<p>We can multiply them and get a 16 <span>× 1</span> output vector, as follows:</p>
<p>                                         <img src="Images/f51138f2-763a-4e12-96d6-bb63271a7e5c.png" style="width:15.33em;height:17.33em;"/></p>
<p>We can rewrite our output vector into a 4 <span>× 4</span> matrix, as follows:</p>
<p>                                          <img src="Images/0c0d08a3-38e4-444a-9c39-9b0193e8adf0.png" style="width:14.33em;height:19.50em;"/></p>
<p>Just like that, we can go from lower dimensional space to a higher dimensional space.</p>
<p>It is important to note that the padding and stride that were applied to the convolution operation can be used in transposed convolution as well. </p>
<p>We can then calculate the size of the output using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aede1fa8-2a1c-4af3-9286-0791d334eb78.png" style="width:18.67em;height:1.25em;"/>.</p>
<p>Here, the input is n <span>× n</span>, the kernel is k <span>× k</span>, <em>p</em> is the pooling, and <em>s</em> is the stride.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pooling</h1>
                </header>
            
            <article>
                
<p>Another often-used operation in CNNs is known as <strong>pooling</strong> (<strong>subsampling</strong> or <strong>downsampling</strong>). This works somewhat like the convolution operation, except it reduces the size of the feature map by sliding a window across the feature map and either averages all the values inside each window at each step or outputs the maximum value. The pooling operation differs from convolution in that it does not have any parameters and so cannot be learned or tuned. We can calculate the size of the feature map after pooling, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b6adcfe2-2156-430a-af6c-6ab89adffc93.png" style="width:26.58em;height:3.08em;"/></p>
<p><span>Here, <em>I</em></span><span> is an n × n-</span><span>shaped two-dimensional tensor, the pooling operation</span><span> is an r × r-</span><span>shaped two-dimensional tensor, and</span><span> <em>s</em></span><span> is the stride. </span></p>
<p>Here is an example of maximum pooling with a stride of 1:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/78461336-cf4f-4e84-902b-19c927bf5be2.png" style="width:12.58em;height:4.83em;"/></p>
<p>Here is an example of average pooling with a stride of 2:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/79a9a2f3-1ade-40b6-a9ad-b402405f7f61.png" style="width:14.50em;height:5.25em;"/></p>
<p>As a rule of thumb, it has been found that the maximum pooling operation performs better.</p>
<div class="packt_infobox">From this, you will probably notice that the output is quite different from the original and doesn't fully represent all the information. In fact, a lot of information has been lost. It is because of this that the pooling operation is used increasingly less in practice.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Global average pooling</h1>
                </header>
            
            <article>
                
<p><strong>Global average pooling</strong> is a variant of the pooling operation that we saw previously, where instead of sliding a subsampling kernel over the feature map, we just take the average of the entire feature map and output a single real value. Suppose we have a feature map with a size of 6 <span>× 6 × 72.</span> After applying this pooling operation, our output would have a size of 1 <span>× 1 × 72</span>. </p>
<p>This is generally used at the last layer, where, normally, we would apply the subsampling and feed the output into a fully connected layer; instead, this allows us to skip the fully connected layer and feed the output of the global average pool directly into our softmax for prediction. </p>
<p>The advantage of using this is that it significantly removes the number of parameters we have to train in our network. Had we flattened the preceding feature map and fed it into a layer of 500 nodes, it would have 1.296 million parameters. This also has the added benefit of reducing overfitting to the training data and improving our classification prediction because the output is closer to the classes.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolution and pooling size</h1>
                </header>
            
            <article>
                
<p>Now that we know the various types of convolution and pooling, it is time to talk about a very important topic associated with them—their size. As you have seen, when we applied a convolution to an image, the output was of a smaller size than the input. The output size is determined by the size of the kernel, the stride, and whether or not we have padding. These are very important things to keep in mind when architecting CNNs. </p>
<p>There are several sizes of convolutions that are used in practice, the most commonly used ones being 7 <span>× 7</span>, 5 <span>× 5</span>, and 3 <span>× 3</span>. However, we can use other sizes as well, including—but not limited to—11 <span>× 11</span>, 13 <span>× 13</span>, 9 <span>× 9</span>, 17 <span>× 17</span>, and so on. </p>
<p>In practice, we generally use larger convolutions with a larger stride to generate a feature map of a smaller size to reduce the computational constraint and default to using 3 <span>× 3</span> and 5 <span>× 5</span> kernels the most. This is because they are computationally more feasible. Generally, having a larger kernel will allow us to look at a larger space in the image and capture more relationships, but having multiple 3 <span>× 3</span> kernels has proven to have a similar performance while being less computationally intensive, which we prefer.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Working with the ConvNet architecture</h1>
                </header>
            
            <article>
                
<p>Now that we know all the different components that make up a ConvNet, we can put it all together and see how to construct a deep CNN. In this section, we will build a full architecture and observe how forward propagation works and how we decide the depth of the network, the number of kernels to apply, when and why to use pooling, and so on. But before we dive in, let's explore some of the ways in which CNNs differ from FNNs. They are as follows:</p>
<ul>
<li>The neurons in CNNs have local connectivity, which means that each neuron in a successive layer receives input from a small local group of pixels from an image, instead of receiving the entire image, as a <strong>feedforward neural network</strong> (<strong>FNN</strong>) would.</li>
<li>Each neuron in the layer of a CNN has the same weight parameters.</li>
<li>The layers in CNNs can be normalized.</li>
<li><span>CNNs</span> are translation invariant, which allows us to detect the same object regardless of its position in the image.</li>
<li><span>CNNs</span> have fewer parameters because the convolution operation weighs the surrounding neurons and sums them into the neuron at the next layer, thereby smoothing the image.</li>
<li>The activation functions typically used in CNNs are ReLU, PReLU, and ELU.</li>
</ul>
<p>The CNN architecture isn't entirely dissimilar to the FNN architecture we saw earlier in this book, except instead of having fully connected layers, we have convolution layers that extract spatial relationships from the inputs and previous layers and learn features from the input at each layer. </p>
<p class="mce-root">In general, what the architecture learns can be demonstrated with the following flow:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3c0e85e6-9b65-4a93-833a-138c34cb6d23.png" style="width:50.75em;height:1.25em;"/></p>
<p><span>As you can see from the preceding flow, the features grow in complexity in the latter layers. What this means is that the earliest layers (those closest to the input layer) learn very basic features, such as edges and lines, textures, or how certain colors differentiate. The latter layers take in the feature map from the previous layer as input and learn more complex patterns from it. For example, if we create a facial recognition model, the earliest layer would learn the simplest possible lines, curves, and gradients. The next layer would take in the feature maps from the previous layer and use it to learn more complex features, such as hair and eyebrows. The layer after that would learn even more complex features, such as eyes, noses, ears, and so on. </span></p>
<p>We can see what a neural network learns in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-968 image-border" src="Images/c92639ed-dc49-4fbf-8c7a-ffd204e13fcd.jpg" style="width:39.75em;height:11.92em;"/></p>
<p><span>CNNs, like FNNs, have a structure that we can use as a guide when we build our own applications. It typically looks as follows:</span></p>
<p>  <img src="Images/f9bdcf87-5029-4b57-9328-a9e5edc6d4d0.png" style="width:56.17em;height:2.75em;"/></p>
<p>We are now going to break down one of the most popular CNN architectures, called AlexNet, which outperformed all other models in the ILSVRC in 2012 with 10% greater accuracy and kickstarted the deep learning revolution. It was created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. We can see its architecture in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-969 image-border" src="Images/aee0ec8d-6d45-4d04-8920-11bfd4b53da6.png" style="width:42.75em;height:15.33em;"/></p>
<p>As you can see, the architecture contains eight trainable layers—five of which are convolutional layers and three of which are fully connected. The ImageNet dataset contains more than 15 million labeled images, but for ILSVRC, we have approximately 1.2 million images in the training set, 50,000 images in the validation set, 150,000 images in the testing set, and nearly 1,000 images for each of the 1,000 classes that the images belong to. Each of the images was rescaled to 256 <span>× 256 × 3</span> because they all varied in size, and from these rescaled images, the authors generated random crops with a size of 256 <span>× 256 × 3</span>. Additionally, the creators of AlexNet used ReLU activations instead of <strong>tanh</strong> because they found that it sped up the training by as much as six times without sacrificing accuracy. </p>
<p>The operations applied to the image at each layer and their sizes are as follows:</p>
<ul>
<li><strong>Convolution layer 1</strong>: 96<span> kernels of size 11 × 11 × 3</span><span> with a stride of 4</span>. This results in a layer with a size of 55 <span>× 55 × 96</span>.</li>
<li><strong>Nonlinearity 1</strong>: ReLU activation applied to the output from convolution layer 1.</li>
<li><span><strong>Subsampling layer 1</strong>: Maximum pool with a size of 3 × 3</span><span> and a stride of 2</span>. <span>This results in a layer with a size of 27 × 27 × 96</span><span>. </span></li>
<li><strong>Convolution layer 2</strong>: 256 kernels with a size of 5 <span>× 5</span>, a padding of 2, and a stride of 1. This results in a layer with a size of 27 <span>× 27 × 256</span>.</li>
<li><strong>Nonlinearity 2</strong>: ReLU activation applied to the output from convolution layer 2.</li>
<li><strong>Subsampling layer 2</strong>: Maximum pool with a size of<span> 3 × 3 and</span><span> a stride of 2</span>. This results in a layer with a size of 13 <span>× 13 × 256</span>. </li>
<li><strong>Convolution layer 3</strong>: 384 kernels with a size of 3 <span>× 3</span>, a padding of 1, and a stride of 1. This results in a layer of size 13 <span>× 13 × 384</span>.</li>
<li><strong>Nonlinearity 3</strong>: ReLU activation applied to the output from convolution layer 3.</li>
<li><strong>Convolution layer 4</strong>: 384<span> kernels of size 3 × 3</span><span> with a padding of 1</span><span> and a stride of 1</span><span>. This results in a layer with a size of 13 × 13 × 384</span>.</li>
<li><strong>Nonlinearity 4</strong>: ReLU activation applied to the output from convolution layer 4.</li>
<li><strong>Convolution layer 5</strong>: 256 kernels <span>with a size of 3 × 3,</span><span> a padding of 1,</span><span> and a stride of 1</span>. This results in a layer with a size of 13 <span>× 13 × 256</span>.</li>
<li><strong>Nonlinearity 5</strong>: ReLU activation applied to the output from convolution layer 5.</li>
<li><strong>Subsampling layer 3</strong>: Maximum pool with a size of 3 <span>× 3</span> and a stride of 2. This results in a layer with a size of 6 <span>× 6 × 256</span>.</li>
<li><strong>Fully connected layer 1</strong>: A fully connected layer <span>with</span> 4,096<span> neurons</span><span>.</span></li>
<li><strong>Nonlinearity 6</strong>: ReLU activation applied to the output from fully connected layer 1.</li>
<li><strong>Fully connected layer 2</strong>: A fully connected layer with 4,096 neurons<span>.</span></li>
<li><strong>Nonlinearity 7</strong>: ReLU activation applied to the output from fully connected layer 2.</li>
<li><strong>Fully connected layer 3</strong>: A fully connected layer with 1,000 neurons.</li>
<li><strong>Nonlinearity 8</strong>: ReLU activation applied to the output from fully connected layer 3.</li>
<li><strong>Output layer</strong>: Softmax applied to the 1,000 neurons to calculate the probability of it being one of the classes.</li>
</ul>
<p>When building architectures, it is important to have an understanding of how many parameters are in the model. The formula we use to calculate the number of parameters at each layer is as follows:</p>
<p>                  <img src="Images/93c029a3-1d12-4f7d-ba43-c300f4ec02e9.png" style="width:35.00em;height:1.50em;"/></p>
<p>Let's calculate the parameters of AlexNet. They are as follows:</p>
<ul>
<li><strong>Convolution layer 1</strong>:<sub> </sub>11 x 11 x 3 x 96 = 34,848</li>
<li><strong>Convolution layer 2</strong>: 5 x 5 x 96 x 256 = 614,400</li>
<li><strong>Convolution layer 3</strong>: 3 x 3 x 256 x 384 = 884,736</li>
<li><strong>Convolution layer 4</strong>:<sub> </sub>3 x 3 x 384 x 384 = 1,327,104</li>
<li><strong>Convolution layer 5</strong>:<sub> </sub><span>3 x 3 x</span> <span>38</span><span>4</span> <span>x 256 = 884,736</span></li>
</ul>
<ul>
<li><strong>Fully connected layer 1</strong>: 256 x 6 x 6 x 4096 = 37,748,736</li>
<li><strong>Fully connected layer 2</strong>: 4096 x 4096 = 16,777,216</li>
<li><strong>Fully connected layer 3</strong>: 4096 x 1000 = 4,096,000</li>
</ul>
<p>Now, if we sum the parameters together, we find that AlexNet has a total of 62.3 million parameters. Roughly 6% of these parameters are from the convolution layers and the remaining 94% are from the fully connected layers. This should give you an idea of why CNNs are so effective and why we like them so much. </p>
<p><span>You may be wondering why we would use a CNN at all and why we wouldn't just use an FNN instead. Couldn't we just flatten the image into a fully connected layer and input every pixel into a single node? We could, but if we did, then our first layer would have 154,587 </span>neurons and our overall network could have well over 1 million neurons and 500 million trainable parameters. This is massive and our network would likely underfit from not having enough training data. Additionally, FNNs do not have the translation-invariant property that CNNs have.</p>
<p>Using the preceding parameters, let's see whether we can generalize the architecture so that we have a framework to follow for future CNNs that we want to build or to understand how other architectures we come across work. The first thing you should have realized in the preceding architecture is that the size of each successive feature map reduces while its depth increases. Also, you may have noticed that the depth is always divisible by 2, many times over, and usually, we use <span>32, 64, 128, 256, 512, and so on in layers.</span></p>
<p>Just as we saw with FNNs previously, the deeper we go, the better our accuracy is, but this doesn't come without its own problems. Larger networks are much harder to train and can either overfit or underfit to the training data. This could be a result of a combination of being too small, being too large, having too much training data, or having too little training data. There is still no fixed recipe for exactly how many layers to use in our CNN; it is very much down to trial and error and building up some intuition after building and training several architectures for a variety of tasks.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training and optimization</h1>
                </header>
            
            <article>
                
<p>Now that we've got that sorted, it's time for us to dive into the really fun stuff. How do we train these fantastic architectures? Do we need a completely new algorithm to facilitate our training and optimization? No! We can still use backpropagation and gradient descent to calculate the error, differentiate it with respect to the previous layers, and update the weights to get us as close to the global optima as possible.</p>
<p class="mce-root">But before we go further, let's go through how backpropagation works in CNNs, particularly with kernels. Let's revisit the example we used earlier on in this chapter, where we convolved a 3 <span>× 3</span> input with a 2 <span>× 2</span> kernel, which looked as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/552a9b5d-cb02-4941-85b3-546e048d2dfc.png" style="width:22.83em;height:4.42em;"/></p>
<p>We expressed each element in the output matrix as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/5221fee2-31c8-4d34-917b-6cb88b9695e8.png" style="width:21.58em;height:5.75em;"/></p>
<p>We should remember from <a href="e1f37008-1ad5-49f6-a229-4d6249c2d7e3.xhtml">Chapter 7</a>, <em>Feedforward Networks</em>, where we introduced backpropagation, that we take derivatives of the loss (error) with respect to the weights and biases at the layers and then use this as a guide to update the parameters to reduce the error of prediction from our network. In CNNs, however, we find the gradient of the error with respect to the kernel. Since our kernel has four elements, the derivatives look as follows:</p>
<p>                            <img src="Images/0f8dd364-0e83-40c5-b695-b2e41e0f663b.png" style="width:29.58em;height:7.08em;"/></p>
<p>If we observe these equations carefully, which represent the output from the feedforward computation, we can see that by taking the partial derivative with respect to each kernel element, we get the respective input element, <em>I</em><sub><em>i,j</em></sub>, that it depends on. If we substitute this value back into the derivatives, we can simplify them to get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2bb73d1c-c3a5-49f0-8b28-150aaca071a3.png" style="width:23.92em;height:6.92em;"/></p>
<p>We can simplify this further by rewriting it as a convolution operation. This looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/48db5231-90e5-4c4e-9004-0459790d4e6f.png" style="width:29.83em;height:5.33em;"/></p>
<p>But what if we wanted to find the derivative with respect to the input? Well, our Jacobian matrix would certainly look a bit different. We would have a 3 <span>× 3</span> matrix since there are nine elements in the input matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6dc2a10a-0689-4fac-97dd-9b6e7024cd8f.png" style="width:29.58em;height:7.00em;"/></p>
<p>We can verify this if we derive it ourselves by hand through the preceding equations, and I encourage you to try this out to get a good understanding of what's happening and why. However, let's now pay particular attention to the kernel we used. If we look carefully, it almost looks like the determinant, but that's not what it is. We just rotated (that is, transposed) the kernel by 180° so that we can compute the gradients. </p>
<p>This is a much-simplified view of how backpropagation works in CNNs; we have left it simple because the rest works exactly as it did in FNNs. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring popular ConvNet architectures</h1>
                </header>
            
            <article>
                
<p>Now that we know how CNNs are built and trained, it is time to explore some of the popular architectures that are used and understand what makes them so powerful.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">VGG-16</h1>
                </header>
            
            <article>
                
<p>The <strong>VGG network</strong> is a derivation of AlexNet that was created by Andrew Zisserman and Karen Simonyan at the <strong>Visual Geometry Group</strong> (<strong>VGG</strong>) at the University of Oxford in 2015. This architecture is simpler than the one we saw earlier, but it gives us a much better framework to work with. VGGNet was also trained on the ImageNet dataset, except it takes images with a size of 224 <span>× 224 × 3</span><span> that</span><span> are sampled from the rescaled images in the dataset</span><span> </span><span>as input</span><span>. You may have noticed that we have headed this section <em>VGG-16</em>—this is because the VGG network has 16 layers. There are variants of this architecture that have 11, 13, and 19 layers. </span></p>
<p>We will first explore the basic building blocks of the network, known as VGG blocks. These blocks are made up of two to three convolutions, followed by a pooling layer. Each of the convolution layers throughout the network uses kernels with a size of 3 <span>× 3</span> and a stride of 1; however, the number of kernels used in each block is the same but can vary from block to block. In the subsampling layer, we use a pooling with a size of 2 <span>× 2,</span> the same padding size, and a stride of 2. </p>
<p>The entire network can be broken down into the following operations:</p>
<ul>
<li><strong>Convolution layer 1</strong>: 64<span> kernels with a size of 3 × 3,</span><span> a stride of 1, and the same padding</span><span>. This results in a layer with a size of 224 × 224 × 64.</span></li>
<li><strong>Nonlinearity 1</strong>: ReLU activation applied to the output from convolution layer 1.</li>
<li><span><strong>Convolution layer 2</strong>: 64 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 224 × 224 × 64.</span></li>
<li><strong>Nonlinearity 2</strong>: ReLU activation applied to the output from convolution layer 2.</li>
<li><strong>Subsampling layer 1</strong>: <span>Maximum pool with a size of</span> 2 <span>× 2 and </span><span>a stride of 2</span>. This results in a layer with a size of 112 <span>× 112 × 64</span>.</li>
<li><strong>Convolution layer 3</strong>: 128<span> kernels with a size of 3 × 3,</span><span> a stride of 1, and the same padding</span><span>. This results in a layer with a size of 112 × 112 × 128.</span></li>
<li><strong>Nonlinearity 3</strong>: ReLU activation applied to the output from convolution layer 3.</li>
<li><span><strong>Convolution layer 4</strong>: 128 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 112 × 112 × 128.</span></li>
<li><strong>Nonlinearity 4</strong>: ReLU activation applied to the output from convolution layer 4.</li>
<li><strong>Subsampling layer 2</strong>: <span>Maximum pool with a size of</span> 2 <span>× 2 and</span><span> a stride of 2</span>. This results in a layer with a size of 56 <span>× 56 × 128</span>.</li>
<li><strong>Convolution layer 5</strong>: 256<span> kernels with a size of 3 × 3,</span><span> a stride of 1, and the same padding</span><span><span>. This results in a layer with a size of 56 × 56 × 256.</span></span></li>
<li><strong>Nonlinearity 5</strong>: ReLU activation applied to the output from convolution layer 5.</li>
<li><span><strong>Convolution layer 6</strong>: 256 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 56 × 56 × 256.</span></li>
<li><strong>Nonlinearity 6</strong>: ReLU activation applied to the output from convolution layer 6.</li>
<li><span><strong>Convolution layer 7</strong>: 256 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 56 × 56 × 256.</span></li>
<li><strong>Nonlinearity 7</strong>: ReLU activation applied to the output from convolution layer 7.</li>
<li><strong>Subsampling layer 3</strong>: <span>Maximum pool with a size of </span>2 <span>× 2</span><span> and a stride of 2</span>. <span>This results in a layer with a size of</span> 28 <span>× 28 × 256</span><span><span>.</span></span></li>
<li><strong>Convolution layer 8</strong>: 512<span> kernels with a size of 3 × 3,</span><span> a stride of 1, and the same padding</span><span>. This results in a layer with a size of 28 × 28 × 512.</span></li>
<li><strong>Nonlinearity 8</strong>: ReLU activation applied to the output from convolution layer 8.</li>
<li><span><strong>Convolution layer 9</strong>: 512 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 28 × 28 × 512.</span></li>
<li><strong>Nonlinearity 9</strong>: ReLU activation applied to the output from convolution layer 9.</li>
<li><span><strong>Convolution layer 10</strong>: 512 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 28 × 28 × 512.</span></li>
<li><strong>Nonlinearity 10</strong>: ReLU activation applied to the output from convolution layer 10.</li>
<li><strong>Subsampling layer 4</strong>: <span>Maximum pool with a size of</span> 2 <span>× 2</span><span> and a stride of 2</span>. <span>This results in a layer with a size of </span>14 <span>× 14 × 512</span>.</li>
<li><strong>Convolution layer 11</strong>: 512<span> kernels with a size of 3×3,</span><span> a stride of 1, and the same padding</span><span>. This results in a layer with a size of 14 × 14 × 512.</span></li>
<li><strong>Nonlinearity 11</strong>: ReLU activation applied to the output from convolution layer 11.</li>
<li><span><strong>Convolution layer 12</strong>: 512 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 14 × 14 × 512.</span></li>
<li><strong>Nonlinearity 12</strong>: ReLU activation applied to the output from convolution layer 12.</li>
<li><span><strong>Convolution layer 13</strong>: 512 kernels with a size of 3 × 3, a stride of 1, and the same padding. This results in a layer with a size of 14 × 14 × 512.</span></li>
<li><strong>Nonlinearity 13</strong>: ReLU activation applied to the output from convolution layer 13.</li>
<li><strong>Subsampling layer 5</strong>: <span>Maximum pool with a size of</span> 2 <span>× 2</span><span> and a stride of 2</span>. <span>This results in a layer with a size of </span>7 <span>× 7 × 512</span>.</li>
<li><strong>Fully connected layer 1</strong>: A fully connected layer <span>with</span> 4,096<span> neurons</span><span><span>.</span></span></li>
<li><strong>Nonlinearity 14</strong>: ReLU activation applied to the output from fully connected layer 1.</li>
<li><strong>Fully connected layer 2</strong>: A fully connected layer <span>with</span> 4,096<span> neurons</span><span>.</span></li>
<li><strong>Nonlinearity 15</strong>: ReLU activation applied to the output from fully connected layer 2.</li>
<li><strong>Output layer</strong>: Softmax applied to the 1,000 neurons to calculate the probability of it being one of the classes.</li>
</ul>
<p>This network placed runner up in the 2014 ILSVRC and has approximately 138 million trainable parameters. So, it is very difficult to train.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inception-v1</h1>
                </header>
            
            <article>
                
<p>The InceptionNet architecture (often referred to as <strong>GoogLeNet</strong>) placed first in the 2014 ILSVRC and achieved near-human performance at 93.3% accuracy. The name <strong>inception</strong> is a reference to the movie <em>Inception</em>, particularly to the need to go deeper (in terms of layers). This architecture is a little different from the ones we saw earlier in that it makes use of Inception modules instead of layers. Each Inception block contains filters of three different sizes—1 <span>× 1</span>, 3 <span>× 3</span>, and 5 <span>× 5</span>. What this does is allow our network to capture sparse patterns through spatial information and variances at different scales, thereby allowing our network to learn even more complex information. However, previously, our networks consistently had a kernel of one size throughout the layer. </p>
<p>The Inception module looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-970 image-border" src="Images/09c99a97-16fa-4ba3-83dd-44c8fb973957.png" style="width:41.58em;height:22.00em;"/></p>
<p>As you can see, each block contains four parallel channels. The first channel contains a 1 <span>× 1</span> kernel, the second channel contains a 1 <span>× 1</span> kernel followed by a 3 <span>× 3</span> kernel, the third channel contains a 1 <span>× 1</span><span> kernel followed by a 5 × 5</span><span> kernel, and the fourth channel contains a 3 × 3 maximum pooling followed by a 1 × 1 kernel. The resulting feature maps are then concatenated and fed as input into the next block. The reason behind</span> applying a 1 <span>× 1</span> kernel before the larger kernels—such as the<span> 3 × 3</span><span> and 5 × 5</span> kernels—is to reduce the dimensionality because larger kernels are more computationally expensive.</p>
<p>This network takes in images with a size of 224 <span>× 224,</span> mean subtraction, and 22 layers with trainable parameters (27 if you count the pooling layers).</p>
<p>The details of the architecture are displayed in the following table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-971 image-border" src="Images/adc875f6-8f3d-41a8-9f25-ea716173ebd9.png" style="width:47.00em;height:30.58em;"/></p>
<p>The network looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-972 image-border" src="Images/11606bdc-5356-48e3-a5e9-88e0ba694bb9.png" style="width:60.00em;height:18.75em;"/></p>
<p class="mce-root">Interestingly, despite this being a much deeper network than AlexNet and VGG-16, there are a lot fewer parameters that we need to train because it uses kernels with smaller sizes, as well as depth reduction. Larger networks, as we know, do tend to perform better than shallower ones. The significance of this architecture is that despite being deep, it is relatively more simple to train that if it had many more parameters.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Congratulations! We have just finished learning about a powerful variant of neural networks known as CNNs, which are very effective in tasks relating to computer vision and time-series prediction. We will revisit CNNs later on in this book, but in the meantime, let's move on to the next chapter and learn about recurrent and recursive neural networks. </p>


            </article>

            
        </section>
    </div></body></html>