- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take an in-depth look at **Recurrent Neural Networks**
    (**RNNs**). In the previous chapter, we looked at **Convolutional Neural Networks**
    (**CNNs**), which are a powerful class of neural networks for computer vision
    tasks because of their ability to capture spatial relationships. The neural networks
    we will be studying in this chapter, however, are very effective for sequential
    data and are used in applications such as algorithmic trading, image captioning,
    sentiment classification, language translation, video classification, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In regular neural networks, all the inputs and outputs are assumed to be independent,
    but in RNNs, each output is dependent on the previous one, which allows them to
    capture dependencies in sequences, such as in language, where the next word depends
    on the previous word and the one before that.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by taking a look at the vanilla RNN, then the bidirectional RNN,
    deep RNNs, **long short-term memory** (**LSTM**), and **gated recurrent units**
    (**GRUs**), as well as some of the state-of-the-art architectures used in industry
    today.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of data used in RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gated recurrent units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about CNNs and their effectiveness on image-
    and time series-related tasks that have data with a grid-like structure. We also
    saw how CNNs are inspired by how the human visual cortex processes visual input.
    Similarly, the RNNs that we will learn about in this chapter are also biologically
    inspired.
  prefs: []
  type: TYPE_NORMAL
- en: The need for this form of neural network arises from the fact that **fuzzy neural
    networks** (**FNNs**) are unable to capture time-based dependencies in data.
  prefs: []
  type: TYPE_NORMAL
- en: The first model of an RNN was created by John Hopfield in 1982 in an attempt
    to understand how associative memory in our brains works. This is known as a **Hopfield
    network**. It is a fully connected single-layer recurrent network and it stores
    and accesses information similarly to how we think our brains do.
  prefs: []
  type: TYPE_NORMAL
- en: The types of data used in RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the introduction to this chapter, RNNs are used frequently for—and
    have brought about tremendous results in—tasks such as natural language processing,
    machine translation, and algorithmic trading. For these tasks, we need sequential
    or time-series data—that is, the data has a fixed order. For example, languages
    and music have a fixed order. When we speak or write sentences, they follow a
    framework, which is what enables us to understand them. If we break the rules
    and mix up words that do not correlate, then the sentence no longer makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have the sentence `The greatest glory in living lies not in never
    falling, but in rising every time we fall` and we pass it through a sentence randomizer.
    The output that we get is `fall. falling, in every in not time but in greatest
    lies The we living glory rising never`, which clearly doesn't make sense. Another
    example is ordering stock prices by date and prices at opening and closing or
    daily prices at fixed time intervals (possibly every hour).
  prefs: []
  type: TYPE_NORMAL
- en: Other examples of sequential data are rainfall measurements over a number of
    successive days, nucleotide base pairs in a DNA strand, or the daily tick values
    for a stock.
  prefs: []
  type: TYPE_NORMAL
- en: We would structure this sort of data in a similar way to how we would for one-dimensional
    convolutions. However, instead of having a kernel that convolves over the data,
    the RNN (which we will become well acquainted with shortly) will take the same
    input, where the node corresponds to the time step of the data (this will become
    clearer momentarily).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word **recurrent** in the name of this neural network comes from the fact
    that it has cyclic connections and the same computation is performed on each element
    of the sequence. This allows it to learn (or memorize) parts of the data to make
    predictions about the future. An RNN's advantage is that it can scale to much
    longer sequences than non-sequence based models are able to.
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Without further ado, let''s take a look at the most basic version of an RNN,
    referred to as a vanilla RNN. It looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1562a2fd-dd46-44a3-8579-2b8502c3d12d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This looks somewhat familiar, doesn''t it? It should. If we were to remove
    the loop, this would be the same as a traditional neural network, but with one
    hidden layer, which we''ve encountered already. Now, if we unroll the loop and
    view the full network, it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1194bb6-7d82-43c8-ac46-154e3fdd42ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x[t]* is the input at time step *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h[t]* is the hidden state at time step *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*o[t]* is the output at time step *t*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the preceding diagram, we can observe that the same calculation is performed
    on the input at each time step and this is what differentiates it from the FNNs
    we came across earlier. The parameters (weights and biases) at each layer of an
    FNN are different, but in this architecture, the parameters (*U*, *V*, and *W*)
    remain the same at each time step. Because of this, RNNs are more memory intensive
    and need to be trained for longer in comparison to CNNs. It is also important
    that you note that in RNNs, the time step doesn't necessarily correspond to the
    time in the real world; it merely means the input sequence is of length *t*.
  prefs: []
  type: TYPE_NORMAL
- en: But why do these weights remain the same across all the time steps? Why can't
    we have separate parameters that need to be learned at different time steps? The
    reason for this is that separate parameters are unable to generalize to sequence
    lengths that aren't encountered during the training process. Having the same three
    weights shared across the sequence and at different time steps enables the network
    to deal with information that can occur at multiple positions, as it tends to
    in language. For example, `the` can appear at a number of positions in a given
    sentence and the RNN should be able to recognize and extract it regardless of
    the position(s) it is in. This shared statistical strength property is advantageous
    in comparison to an FNN because an FNN would need to learn the language's rules
    at every position, which—as you can imagine—can be very challenging to train.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we can think of this as having a sequence [![](img/1c9d1ac3-2fcc-400e-b78c-9ef9f0e41a05.png)] where
    we are trying to find [![](img/21a4f626-7aba-4c9e-a583-e31fdc52b49b.png)], which
    we are already familiar with from [Chapter 3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml),
    *Probability and Statistics*. This is not exactly what is happening; we have simplified
    it to help you understand what the RNN is trying to learn to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the knowledge we have now gained, we can create some very complex RNNs
    for a variety of tasks, such as language translation or converting audio into
    text. Depending on the type of task we want to build our model for, we can choose
    from one of the following types of RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one (one input and one output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-to-many (one input and many outputs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many-to-one (many inputs and one output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many-to-many (multiple inputs and outputs, where the number of inputs and outputs
    are equal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many-to-many (multiple inputs and outputs, where the number of inputs and outputs
    are not equal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a deeper dive into RNNs and see what is happening at each time step
    from the input to the output through all the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can calculate the hidden state at each time step using the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8e66371-838c-4dcf-a551-67f7bcd43fb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f[1]* is a non-linearity, such as ReLU, tanh, or sigmoid. The output
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b338fbf-0c33-4e8a-ad0e-ffccb997bbc2.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the probability vector of the output using a nonlinear function, *f[2]*,
    (such as softmax), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d140200d-2dda-40fb-9925-94d69f95e944.png)'
  prefs: []
  type: TYPE_IMG
- en: By using these equations and applying them repeatedly, we can calculate the
    hidden states and outputs at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the RNN then looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7d358a3-33ee-45ee-a0b6-518ca0c71a83.png)'
  prefs: []
  type: TYPE_IMG
- en: By looking at the preceding diagram and the equations, you should be able to
    venture a guess as to the shape of our weight matrices and our bias vectors—[![](img/b13ffd01-6acb-4470-aa97-d5b9a4d5e587.png)] (connects
    the input to the hidden layer), [![](img/b5da4763-2964-4cba-a61e-444c8286e49b.png)] (connects
    the previously hidden layer to the current hidden layer), [![](img/b1cc40f4-682f-43cf-8c01-6f1a12ef5abd.png)] (connects
    the hidden layer and the output), [![](img/58631c14-e746-4cb9-a213-66e672e4cb83.png)] (the
    bias vector for the hidden layer), and [![](img/26884a78-3b83-493e-9299-aed75440ed44.png)] (the
    bias vector for the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding equations, we can clearly tell that the hidden state at time
    step *t* depends on the current input and the previous hidden state. However,
    the initial hidden state, *h[0]*, must be initialized in a similar way to how
    we initialized the weights and kernels in FNNs and CNNs. The output at each time
    step, on the other hand, is dependent on the current hidden state. Additionally,
    *a* and *b* are biases and so they are trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In RNNs, *h[t]* contains information about everything that has occurred at the
    previous time steps (but in practice, we limit this to a few time steps, instead
    of all previous ones, because of the vanishing/exploding gradient problem) and
    *o[t]* is calculated based on the most recent information. This allows the RNN
    to exploit relationships between the sequence and use it to predict the most likely
    output, which is not entirely dissimilar to how CNNs capture spatial relationships
    in sequential data using one-dimensional convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this isn''t the only way to construct RNNs. Instead of passing outputs
    from hidden layer to hidden layer ([![](img/91060362-403a-472c-9c7d-40c23d4df645.png)])
    as in the preceding diagram, we could pass the output from the previous output
    into the next hidden state ([![](img/31706299-f030-4ca8-b85c-1dd61fb22ff2.png)]),
    changing how we calculate the hidden state. It now becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c54622b9-245b-4978-9d37-8caedfd5c212.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we can see the various operations taking place in
    a hidden state cell at time step *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7c2f057-de92-4542-95e9-b04988aa507a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When working with FNNs, we calculate the loss at the end of each forward pass
    through the network and then backpropagate the errors to update the weights. However,
    in RNNs, we calculate the loss at each time step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef54fcf0-8729-4b96-b4be-9345d20d4bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *L* is the cross-entropy loss function (which we are already familiar
    with), *y[t]* is the target, [![](img/4076ec47-3e82-4d91-89e4-39470c086152.png)] is
    a probability vector, and *n* is the number of outputs/targets.
  prefs: []
  type: TYPE_NORMAL
- en: While effective, these vanilla RNNs aren't perfect. They do have a few problems
    that we usually encounter during training, particularly the vanishing gradient
    problem. This occurs when the weights become very small, preventing the neuron
    from firing, which prevents hidden neurons at later time steps from firing because
    each one depends on the last, and the one before that, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better understanding of this, let's consider the following example.
    Suppose we have a very simple vanilla RNN without any nonlinear activations and
    or inputs. We can express this network as [![](img/98c4a2d5-7a0a-4d2f-b683-36bf669d44e9.png)].
    As you can see, we are applying the same weight over and over again at each unit
    from time step to time step. However, let's focus our attention on the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the vanishing and exploding gradient problem, let''s suppose
    that our weight matrix has a shape of 2 × 2 and is diagonalizable. You should
    remember from [Chapter 2](6a34798f-db83-4a32-9222-06ba717fc809.xhtml), *Linear
    Algebra*, that if our matrix is diagonalizable, then it can be decomposed into
    the form [![](img/893fef47-6833-4bd9-9e88-0bbbaf2989ac.png)], where *Q* is a matrix
    containing the eigenvectors and Λ is a square matrix that contains eigenvalues
    along the diagonal. As previously, if we have eight hidden layers, then our weight
    would be [![](img/dc392677-29f6-45e0-bc98-08e55db5158d.png)]. We can see this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ab20487-fcfb-410a-83f5-6c7ea439ea0b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we get a good glimpse of both the vanishing and the
    exploding gradient problems. We assumed we have eight hidden units and by multiplying
    them over and over, we can see that the values become either very small or very
    large, which makes training RNNs rather challenging because of their instability.
    The small weights make it difficult for our RNN to learn long-term dependencies
    and is why innovations in the cells, such as **Long short-term models** (**LSTMs**)
    and **gated recurrent units** (**GRUs**) were created (we'll learn about these
    two RNN cell variants shortly).
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we have an RNN with 20 time steps or more and we want our network to
    remember the first, second, or third input, it is more than likely it won't be
    able to remember them, but it will remember the most recent inputs. For example,
    we could have the sentence `I remember when I visited Naples a few years ago...I
    had the best pizza of my life`. In this case, we need to understand the context
    of Naples from further back to understand where this magnificent pizza is from,
    but looking this far back is challenging for RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if our weight is greater than 1, it can get much larger, which results
    in exploding gradients. We can, however, deal with this by using gradient clipping,
    where we rescale the weights so that the norm is at most η. We use the following formula
    to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb36263c-c264-4cc7-938a-be318291fbf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Bidirectional RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how RNNs work at their most basic level, let's take a look
    at a variant of them—the bidirectional RNN. The preceding RNN is feedforward;
    that is, the data passes through the network from left (![](img/b2e68ba7-20c7-4893-ab76-ce2ff624bc5d.png))
    to right (![](img/53ae4383-5d6f-4ba4-a6fe-0d587d0cfe4a.png)), which creates a
    dependency on the past. However, for some of the problems that we may want to
    work with, it could help to look into the future, as well.
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to feed the network training data both forward and backward into
    two separate recurrent layers, respectively. It is important to note that both
    of these layers share the same output layer. This approach allows us to contextualize
    the input data with respect to the past and the future, which produces much better
    results than the previous, unidirectional RNN for tasks relating to speech and
    translation. Naturally, however, bidirectional RNNs are not the answer to every
    time-series task, such as predicting stock prices, because we don't know what
    will happen in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see what a bidirectional RNN looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a61fb748-6287-47d3-82c1-9d490b0f47b3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the network now contains two parallel layers running in opposite
    directions and there are now six different sets of weights applied at every time
    step; namely, input-to-hidden (*U* and *C*), hidden-to-hidden (*A* and *W*), and
    hidden-to-output (*V* and *B*). It is important that we note that there is no
    information shared between the forward layer and the backward layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the operations taking place at each of the hidden states at time ![](img/f7176f55-6205-4278-88ed-b8b8e5821873.png) are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/c3d40ed6-caba-4a59-ba60-aba2a621cf87.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a471826e-2d60-4223-9050-c103b13ff3e2.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'Here, *f[1]* is a non-linearity and *a* and *b* are biases. The output unit
    can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b277585d-dd9c-4e51-b936-17f2bb0f2a91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *d* is a bias. Then, we can find the probability vector using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/751358e0-c8e3-4885-a90d-027a4b775462.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equations show us that the hidden states in the forward layer
    receive information from the previous hidden states and the hidden states in the
    backward layer receive information from the future states.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier, the standard RNN does have some limitations; in particular,
    they suffer from the vanishing gradient problem. The LSTM architecture was proposed
    by Jürgen Schmidhuber ([ftp://ftp.idsia.ch/pub/juergen/lstm.pdf](ftp://ftp.idsia.ch/pub/juergen/lstm.pdf))
    as a solution to the long-term dependency problem that RNNs face.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM cells differ from vanilla RNN cells in a few ways. Firstly, they contain
    what we call a memory block, which is basically a set of recurrently connected
    subnets. Secondly, each of the memory blocks contains not only self-connected
    memory cells but also three multiplicative units that represent the input, output,
    and forget gates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what a single LSTM cell looks like, then we will dive
    into the nitty-gritty of it to gain a better understanding. In the following diagram,
    you can see what an LSTM block looks like and the operations that take place inside
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/076993fb-4bc3-4461-9abb-600ab3523f8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding LSTM cell, a bunch of operations take place
    at each time step and it has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*: The forget gate (an NN with sigmoid)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ad68a289-373f-44b4-a0fd-3026563cdbf8.png)]: The candidate layer (an
    NN with tanh)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I*: The input gate (an NN with sigmoid)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*O*: The output gate (an NN with sigmoid)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H*: The hidden state (a vector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C*: The memory state (a vector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* and *U*: The weights for the forget gate, candidate, input gate, and output
    gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each time step, the memory cell takes the current input (*X[t]*), the previous
    hidden state (*H[t-1]*), and the previous memory state (*C[t-1]*) as input and
    it outputs the current hidden state (*H[t]*) and the current memory state (*C[t]*).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, there are a lot more operations happening
    here than were taking place in the hidden cell of the vanilla RNN. The significance
    of this is that it preserves the gradients throughout the network and allows longer-term
    dependencies, as well as providing a solution to the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how exactly do LSTMs do this? Let''s find out. The memory state stores
    information and continues to do so until the old information is overridden by
    the new information. Each cell can make a decision as to whether or not it wants
    to output this information or store it. Before we go deeper into the explanations,
    let''s first take a look at the mathematical operations that take place in each
    LSTM cell. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/d6efa777-e4c4-40e6-802c-e655c8816c48.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/6d35903b-cab4-40d1-9076-23f9ec363b89.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/39d6e2ef-fba3-4d73-829f-0666e30a640e.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/f3b38e96-c284-45fa-95bd-b303a7cec238.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/c6ca1e21-a5f5-4585-9bfc-103cd8a47a6d.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2b8795af-0a2c-4fa8-aa4e-cae69a6b754b.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we know the different operations that take place in each cell, let''s
    really understand what each of the preceding equations represents. They are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The candidate layer ([![](img/43ea863f-463b-4a35-bd3f-97eecbaa0784.png)]) takes
    as input a word (*X[t]*) and the output from the previous hidden state *H[t-1]* and
    creates a new memory, which includes the new word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input gate (*I*) performs a very important function. It determines whether
    or not the new input word is worth preserving based on the output of the previous
    hidden state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The forget gate (*f*), even though it looks very similar to the input gate,
    performs a different function. It determines the relevance (or usefulness) of
    the previous memory cell when computing the current one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory state (sometimes referred to as the final memory) is produced after
    taking in the forget gate and the input gate as input and then gates the new memory
    and sums the output to product *C[t]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output gate differentiates the memory from the hidden state and determines
    how much of the information present in the memory should be present in the hidden
    state. This produces *O[t]*, which we then use to gate tanh (*C[t]*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gated recurrent units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the LSTM, GRUs are also an improvement on the hidden cells in vanilla
    RNNs. GRUs were also created to address the vanishing gradient problem by storing
    memory from the past to help make better future decisions. The motivation for
    the GRU stemmed from questioning whether all the components that are present in
    the LSTM are necessary for controlling the forgetfulness and time scale of units.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference here is that this architecture uses one gating unit to decide
    what to forget and when to update the state, which gives it a more persistent
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see what the GRU architecture looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/284492cc-e1ec-4ab3-b760-836388db7399.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding diagram, it takes in the current input (*X[t]*) and
    the previous hidden state (*H[t-1]*), and there are a lot fewer operations that
    take place here in comparison to the preceding LSTM. It has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z[t]*: The update gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R[t]*: The reset gate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/5872a37f-3bd5-42f2-83eb-e8035dd1559d.png)]: The new memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H[t]*: The hidden state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To produce the current hidden state, the GRU uses the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/7d8ea1bc-1315-44e8-abc0-3f7274128e34.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/0cb747b3-a746-41b3-bf8e-76342e121253.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/607d3719-69fd-4c0c-bfcb-06ca2632a1c3.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/cafab2cc-ceee-4fea-b2ae-d3a0800e2800.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s break down the preceding equations to get a better idea of what
    the GRU is doing to its two inputs. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The GRU takes in the current input (*X[t]*) and the previous hidden state (*H[t-1]*)
    and contextualizes the word based on the information it has about the previous
    words to produce [![](img/c5fa89f4-32d3-48aa-b737-e505b9a81e00.png)]—the new memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reset gate (*R[t]*) decides the importance of the previous hidden state
    in computing the current hidden state; that is, whether or not it is relevant
    to obtaining the new memory, which helps capture short-term dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The update gate (*Z[t]*) determines how much of the previous hidden state should
    be passed on to the next state to capture long-term dependencies. In a nutshell,
    if *Z[t]≈1*, then most of the previous hidden state is incorporated into the current
    hidden state; but if *Z[t]≈0*, then most of the new memory is passed forward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the present hidden state (*H[t]*) is computed using the new memory
    and the previous hidden state, contingent on the results of the update gate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how adding depth to our neural networks helps
    achieve much greater results; the same is true with RNNs, where adding more layers
    allows us to learn even more complex information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen what RNNs are and have an understanding of how they work,
    let's go deeper and see what deep RNNs look like and what kind of benefits we
    gain from adding additional layers. Going deeper into RNNs is not as straightforward
    as it was when we were dealing with FNNs and CNNs; we have to make a few different
    kinds of considerations here, particularly about how and where we should add the
    nonlinearity between layers.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to go deeper, we can stack more hidden recurrent layers on top of
    each other, which allows our architecture to capture and learn complex information
    at multiple timescales, and before the information is passed from layer to layer,
    we can add either non-linearity or gates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a two-hidden-layer bidirectional RNN, which is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d5d6afe-3202-4192-a0fb-7f894280db2b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it looks like three **multilayer perceptrons** (**MLPs**) stacked
    together side by side, and the hidden layers are connected, as before, forming
    a lattice. There are also no connections between the forward and backward hidden
    units in the same layer. Each hidden node feeds into the node directly above it
    at the same time step and each hidden node takes two parameters from the correlated
    hidden node in the previous layer as input—one from the forward layer and the
    other from the backward layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generalize and write the equations for the deep bidirectional RNN as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the forward layer, we have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3e071034-b2cb-4682-a766-726a012088d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the backward layer, we have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b8b25b2c-ae7a-4e9d-9754-443447208983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the output layer, we have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9147e982-3c41-4ba3-a1de-11326a0dfc23.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this as a guideline, we can do the same for LSTMs or GRUs and add them
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Training and optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in the neural networks we have already encountered, RNNs also update their
    parameters using backpropagation by finding the gradient of the error (loss) with
    respect to the weights. Here, however, it is referred to as **Backpropagation
    Through Time** (**BPTT**) because each node in the RNN has a time step. I know
    the name sounds cool, but it has nothing to do with time travel—it's still just
    good old backpropagation with gradient descent for the parameter updates.
  prefs: []
  type: TYPE_NORMAL
- en: Here, using BPTT, we want to find out how much the hidden units and output affect
    the total error, as well as how much changing the weights (*U, V, W*) affects
    the output. *W*, as we know, is constant throughout the network, so we need to
    traverse all the way back to the initial time step to make an update to it.
  prefs: []
  type: TYPE_NORMAL
- en: When backpropagating in RNNs, we again apply the chain rule. What makes training
    RNNs tricky is that the loss function is dependent not only on the activation
    of the output layer but also on the activation of the current hidden layer and
    its effect on the hidden layer at the next time step. In the following equation,
    we can see how backpropagation works in RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we first find the cross-entropy loss (defined in the *Vanilla
    RNNs* section); our total error is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/088da816-0209-435f-9a9d-fe81b4e4d512.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can expand the preceding equation using the chain rule with respect to the
    losses and hidden layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8fd0426-7be3-4e72-8ade-7febd2e04c2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s focus in on the third term on the right-hand side and expand on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba47a8d3-6f32-48fa-857e-fc4b39805f67.png)'
  prefs: []
  type: TYPE_IMG
- en: You should note that each partial here is a Jacobian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now combine the preceding equations together to get a holistic view
    of how to calculate the error, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cf21858-6ec5-4dca-a0db-ccc21bc3664e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know from earlier on in this chapter that *h[t]* is calculated using the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e7067dd-9b64-4c9c-97bd-a1e177c271b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can calculate the gradient of *h[t]*, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4408bf9d-c095-4d80-82c9-f9d56dded2ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the hidden neurons also take *x[t]* as input, we need to take the derivative
    with respect to *U* as well. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8c2f879-a50e-471c-9489-b2d6274bac29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But wait—the hidden units, as we have seen, take in two inputs. So, let''s
    backpropagate one time step using what we have just seen and see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d74dac3-3b1a-4da1-bdbc-8a12c9ca83a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this, we can now sum over all the previous gradients up to the present
    one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/948d3884-d535-47a1-bae2-028506d42ea1.png)'
  prefs: []
  type: TYPE_IMG
- en: The backward pass in LSTM or GRU is much like what we did with regular RNNs,
    but there are some additional complexities here because of the gates (we will
    not go through the differences between the backward passes in LSTMs or GRUs here).
  prefs: []
  type: TYPE_NORMAL
- en: Popular architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned about all the components that are used to contrast
    RNNs, let's explore a popular architecture that has been developed by researchers
    in the field—the **clockwork RNN **(**CW-RNN**).
  prefs: []
  type: TYPE_NORMAL
- en: Clockwork RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have learned, it is very challenging to discover long-term dependencies
    in RNNs, and LSTMs and GRUs were designed to overcome this limitation. CW-RNN,
    created by a group at IDSIA led by Jürgen Schmidhuber, modifies the vanilla RNN
    module such that the hidden layer is partitioned into separate modules, each of
    which processes its inputs at different temporal granularities. What this means
    is that the hidden layers perform computations at their preset clock rate (which
    is where the name comes from). The effect of this is a reduced number of trainable
    parameters and greater accuracy when compared to regular RNNs and LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Just as our earlier RNNs had input-to-hidden and hidden-to-output connections,
    a CW-RNN also has the same, except the neurons in the hidden layer are partitioned
    into *g* modules of size *k*, each of which has an assigned clock period, [![](img/5e608460-a533-437b-b23b-8bf58c320341.png)].
  prefs: []
  type: TYPE_NORMAL
- en: These modules are fully connected together, but the recurrent connections from
    modules *j* to *i* are not if period [![](img/17d47dfc-b3eb-4917-9a75-5543a327bfc5.png)].
    These modules are sorted by increasing period and, therefore, the connections
    move from slower to faster (right to left).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the architecture of the CW-RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92317b85-89c9-49d9-a965-8b89ecdfc2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The clock period of module *i* can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34b764e1-b9a1-4c40-bb95-d76ce35b41ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input and hidden weight matrices are partitioned into *g* block rows, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9fd2f57-215a-4b89-a645-2ef863af3e32.png) and ![](img/d1b93cb4-09ba-4375-9777-bf337318b18f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *W* is an upper-triangular matrix and each *W[i] *valueis
    partitioned into block columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4112c3b1-df4f-4059-a5fe-deccb3e037e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During the forward pass, only the block rows of the hidden weight matrix and
    the input weight matrix correspond to the executed modules, where the following
    is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf6576cc-a983-4923-a2dd-576e32f5b693.png)'
  prefs: []
  type: TYPE_IMG
- en: The modules with lower clock rates learn and maintain long-term information
    from the input and the modules with the higher clock rates learn local information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We mentioned in the preceding equation that each hidden layer is partitioned
    into *g* modules of size *k*, which means there are a total of *n = kg* neurons.
    Since neurons are only connected to those that have a similar or larger period,
    the number of parameters within the hidden-to-hidden matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a7eb4fd-7f60-4ad2-8e97-13cda5f91413.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compare this with our vanilla RNNs, which have *n²* parameters. Let''s
    see how this is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/354d64a6-bb01-49e1-8720-5cc70f819a29.png)'
  prefs: []
  type: TYPE_IMG
- en: The CW-RNN has approximately half as many parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a very powerful type of neural network—RNNs. We
    also learned about several variations of the RNN cell, such as LSTM cells and
    GRUs. Like the neural networks in prior chapters, these too can be extended to
    deep neural networks, which have several advantages. In particular, they can learn
    a lot more complex information about sequential data, for example, in language.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about attention mechanisms and their increasing
    popularity in language- and vision-related tasks.
  prefs: []
  type: TYPE_NORMAL
