- en: Attention Mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding two chapters, we learned about convolutional neural networks
    and recurrent neural networks, both of which have been very effective for sequential
    tasks such as machine translation, image captioning, object recognition, and so
    on. But we have also seen that they have limitations. RNNs have problems with
    long-term dependencies. In this chapter, we will cover attention mechanisms, which
    have been increasing in popularity and have shown incredible results in language-
    and vision-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding neural Turing machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the types of attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Overview of attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we go about our lives (in the real world), our brains don't observe every
    detail in our environment at all times; instead, we focus on (or pay greater attention
    to) information that is relevant to the task at hand. For example, when we are
    driving, we are able to adjust our focal length to focus on different details,
    some of which are closer and others are further away, and then act on what we
    observe. Similarly, when we are conversing with others, we usually don't listen
    carefully to each and every word; we listen to only part of what is spoken and
    use it to infer the relationships with some of the words to figure out what the
    other person is saying. Often, when we are reading/listening to someone, we can
    use some words to infer what the person is going to say next based on what we
    have already read/heard.
  prefs: []
  type: TYPE_NORMAL
- en: But why do we need these attention mechanisms in deep learning? Let's take a
    stroll down memory lane to [Chapter 10](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml),
    *Recurrent Neural Networks*, where we learned about sequence-to-sequence models
    (RNNs), which, as we saw, can be used for tasks such as language-to-language translation.
    We should recall that these types of models have an encoder-decoder architecture
    where the encoder takes in the input and compresses the information into an embedding
    space (or context vector), while the decoder takes in the context vector and transforms
    it back into the desired output. Both the encoder and decoder are RNNs (possibly
    with LSTMs or GRUs for long-term memory).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we also came across a few limitations that RNNs have –
    in particular, the problem of vanishing or exploding gradients, which hinders
    long-term dependencies. And thus, the attention mechanism was created. It was
    created for the sole purpose of solving this very problem of memorizing long sentences
    that RNNs face.
  prefs: []
  type: TYPE_NORMAL
- en: In RNNs, the hidden state at each time step takes the previous hidden state
    as input (which contains the context of the sequence it has seen so far) and the
    input for that time step, and then the final hidden state is passed to the decoder
    sequence. Attention mechanisms differ by creating connections between the context
    vector and the whole sequence of inputs. This way, we no longer have to worry
    about how much will end up being forgotten. And similar to all the other connections
    in ANNs, these attention connections are weighted, which means they can be adjusted
    for each output. Essentially, the context vector controls the alignment between
    the input and target (output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have an input, [![](img/413fee1b-9c96-4ebb-b7a6-891b310bf9de.png),] and
    a target, [![](img/3ad54583-0855-4162-816a-9258c7deb0b8.png)], where the hidden
    states produced by the encoder (a vanilla RNN for simplicity, though this could
    be any RNN architecture) will be [![](img/ef65b64f-2805-4934-af79-4a87e722fc13.png)].
    The hidden states for the decoder here are slightly different from what we have
    previously seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75092bed-dc07-4216-93f8-bbad1af80139.png)'
  prefs: []
  type: TYPE_IMG
- en: For all [![](img/582257a5-b5d8-4325-95f4-ebc9a5eecf71.png)]. Here, *c[t]* (the
    context vector) is a sum of all hidden states from the input. This is weighted
    by the alignment scores so that [![](img/a299bc60-e389-40a1-ad66-9b7dc1102e23.png)] and [![](img/3f88f612-9a6e-4b6c-afa1-3065390e28d6.png),] which
    determine the alignment of *y[t]* and *x[i]* by assigning a score based on how
    well the two match. Each *α[t,i]* here is a weight that decides the extent to
    which each source's hidden state should have an impact on each output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding score function is parameterized by an MLP with a single hidden
    layer and is calculated using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd8fe6a1-807b-4538-a06c-300672586fc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **V**[a] and **W**[a] are weight matrices to be learned.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive deep into the inner workings of various attention mechanisms,
    let's take a look at neural Turing machines.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural Turing machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Turing machine (TM)** was proposed by Alan Turing in 1936, and it is a
    mathematical model of computation made up of an infinitely long tape and a head
    that interacts with the tape by reading, editing, and moving symbols on it. It
    works by manipulating symbols on the strip according to a predefined set of rules.
    The tape is made up of an endless number of cells, each of which can contain one
    of three symbols – 0, 1, or blank (" "). Therefore, this is referred to as a **three-symbol
    Turing machine**. Regardless of how simple it seems, it is capable of simulating
    any computer algorithm, regardless of complexity. The tape that these computations
    are done on can be considered to be the machine's memory, akin to how our modern-day
    computers have memory. However, the Turing machine differs from modern-day computers
    as it has limited memory and computational limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 10](70ef350e-d1bd-4348-b5ec-aae11454bd69.xhtml), *Recurrent Neural
    Networks*, we learned that this type of ANN is Turing complete, which means that
    when they are properly trained, they can simulate any arbitrary procedure. But
    this is only in theory. In practice, we have seen that this is not the case because
    they do have their limitations. To overcome these limitations, in 2014, Alex Graves
    et al. proposed augmenting an RNN with a large, addressable memory (similar to
    the tape in a TM), thereby giving it the name **neural Turing machine** (**NTM**).
    This model, as the authors stated, *is a differentiable computer that can be trained
    by gradient descent, yielding a practical mechanism for learning programs*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NTM borrows the idea of working memory (a process in human cognition),
    which is the same as short-term memory, and applies it to RNNs, thereby giving
    it the ability to selectively read from and write to memory using an attentional
    controller. We can see what the NTM looks like in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4a1ea46-836f-40e8-930f-97194df0c5f4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that there are two main components to the
    NTM – the controller, which is a neural network, and the memory, which contains
    processed information. The controller interacts with the external world by receiving
    an input vector and outputting an output vector, but it differs from the ANNs
    from previous chapters in that it also interacts with the memory matrix using
    the selective read and write operations.
  prefs: []
  type: TYPE_NORMAL
- en: Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose our memory is an *N*×*M* matrix and that it is denoted by [![](img/89ed3e76-7c89-472c-baa9-0719fa281259.png)],
    where *t* is the time step, *N* is the number of rows (memory locations), and
    *M* is the size of the vector at each location. Then, we have another vector of
    weights, **w***[t]*, which determines how much attention to designate to various
    locations in the memory (that is, the rows of the matrix). Each of the *N* weightings
    in the weight vector is normalized, which tells us that [![](img/f0751cb7-2506-4c20-a962-5770f6760dee.png)] and
    that for all *i*, we have [![](img/ff03c942-ed7d-4fd4-98dd-0107e30786ca.png)],
    where [![](img/8c79fda9-6f9b-4578-877e-779aed81de97.png)] is the *i^(th)* element
    of **w***[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The read vector, **r***[t]*, which is returned by the head, can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c44483be-ed81-472b-8e8d-00fdf2e411b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/df41d01e-24c7-462e-b773-70d193858dad.png)] is the *i**^(th)*
    memory vector. From the preceding equation, we can also see that **r***[t]* can be
    differentiated with respect to both the weight and the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The writing operation takes inspiration from the input and forget gates of LSTMs,
    where some information is erased and then replaced (or added).
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory is then updated using two formulas, the first of which erases the
    memory and the second of which adds memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/822fbdb1-fcc6-4d44-8fe5-ad3d0d8c3e07.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/989ecad8-4f02-44fc-9254-b44a6ec316d2.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, [![](img/7c145547-1bdf-4c9f-bfcd-2e6896726b26.png)] is the erase vector,
    **a***[t]* is the add vector, and **1** is a vector containing only ones. From
    these equations, we can see that the memory at a specific location is only erased
    when both the weight and erase vector are equal to one, and it is left unchanged
    otherwise. Since both the erase and add operations are differentiable, so is the
    entire write operation.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how the reading and writing operations work, let's dive into
    how the weights are produced. The weights are outputs that are a combination of
    two mechanisms—a content-based addressing mechanism and a location-based addressing
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based addressing mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This addressing mechanism focuses on the similarity between the key values, **k***[t]*,
    which are output by the controller based on the input it receives, and the memory
    rows. Based on this similarity, it creates an attention vector. It is calculated
    using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b222ee36-4589-454c-82ee-19c6e0154f62.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/1f70300a-f477-43ac-9aea-422931307a42.png)] is a normalized weight
    and *β[t]* is a strength multiplier.
  prefs: []
  type: TYPE_NORMAL
- en: Location-based address mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we learn how location-based addressing works, we need to define our
    interpolation gate, which blends together the content-based attention at the current
    time step with the weights in the attention vector from the previous time step.
    This can be done using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36836a94-5a7c-49ce-8879-58328cb3b19f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/faa5bda9-522d-4210-90c3-fdb43444d11d.png)] is the scalar interpolation
    gate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Location-based addressing works by summing the values in the attention vector,
    each of which are weighted by a shifting weight, **s***[t]*, which is a distribution
    of the allowable integer shifts. For example, if it can shift between -1 and +1,
    then the allowable shifts that can be performed are -1, 0, and +1\. Now, we can
    formulate this rotation so that the shifting weight applies to [![](img/aacecb17-2a38-4a64-b209-f1af249e8a94.png)] as
    a circular convolution. We can observe this in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8bf7525-a591-4646-b6d4-6ea2f03911a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To prevent any leakage or blurriness being caused by the shifting weight, we
    sharpen the attention vector, **w***[t]*, using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68981f9d-1a84-4010-9f72-699945912555.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/3d7b5ad2-d552-41b0-b704-7a70110a8fb0.png)] is a positive scalar
    value.
  prefs: []
  type: TYPE_NORMAL
- en: And lastly, the values that are output by the controller are unique for each
    read and write head.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the types of attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attention has proven to be so effective in machine translation that it has been
    expanded into natural language processing, statistical learning, speech understanding,
    object detection and recognition, image captioning, and visual question answering.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of attention is to estimate how correlated (connected) two or more
    elements are to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there isn''t just one kind of attention. There are many types, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-attention**: Captures the relationship between different positions of
    a sequence of inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global or soft attention**: Focuses on the entire sequence of inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local or hard attention**: Focuses on only part of the sequence of inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-attention finds relationships between different positions of the input
    sequence and then computes a representation of the same sequence of inputs. You
    can think of this as summarizing the input. This is somewhat similar to how the
    LSTM we saw in the previous chapter works, where it tries to learn a correlation
    between the previous inputs and the current one and decide what is relevant and
    what isn't.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing hard and soft attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These types of attention were created for generating captions for images. A
    CNN is first used to extract features and then compress them into an encoding.
    To decode it, an LSTM is used to produce words that describe the image. But that
    isn't important right now—distinguishing between soft and hard attention is.
  prefs: []
  type: TYPE_NORMAL
- en: In soft attention, the alignment weights that are learned during training are
    softly placed over patches in an image so that it focuses on part(s) of an image
    more than others.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in hard attention, we focus only on part of the image at
    a time. This only makes a binary decision about where to focus on, and it is much
    harder to train in comparison to soft attention. This is because it is non-differentiable
    and needs to be trained using reinforcement learning. Since reinforcement learning
    is beyond the scope of this book, we won't be covering hard attention.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing global and local attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global attention shares some similarities with how soft attention works in that
    it takes all of the input into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Local attention differs from global attention as it can be seen as a mix of
    hard and soft attention and considers only a subset of the input. It starts by
    predicting a single aligned position for the current output. Then, a window centered
    around the current input is used to create a context vector.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those of you who got excited at the title (transformers), this section sadly
    has nothing to do with Optimus Prime or Bumblebee. In all seriousness now, we
    have seen that attention mechanisms work well with architectures such as RNNs
    and CNNs, but they are also powerful enough to be used on their own, as evidenced
    by Vaswani in 2017, in his paper *Attention Is All you Need*.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer model is made entirely out of self-attention mechanisms to perform
    sequence-to-sequence tasks without the need for any form of recurrent unit. Wait,
    but how? Let's break down the architecture and find out how this is possible.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs take in the encoded input and then decode it in order to map it to a target
    output. However, the transformer differs here by instead treating the encoding
    as a set of key-value pairs (**K**, **V**) which has dimensions (=*n*) equal to
    the length of (the sequence of) the inputs. The decoder is treated as a query, *Q*,
    that has dimensions (=*m*) equal to the length of (the sequence of) the outputs.
    Each output is created by mapping the key-value pair to a query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention here is calculated using a scaled dot product that calculates
    the weighted sum of the values using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15adfd36-aa62-4691-88c6-2f3fb55816b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, instead of calculating the attention just once, we do it multiple times
    in parallel. This is referred to as multi-head attention. In the following diagram,
    we can see a visualization of the scaled-dot product attention and multi-head
    attention calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef2a08a1-b527-4b82-9c3e-45482680fffa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output from each of these attention calculations is then concatenated and
    we apply a linear transformation to them to ensure their dimensions match what
    is expected. The reason for this is that multi-head attention enables the model
    to focus on information from different subspaces at different positions at the
    same time, which single attention is unable to do. This works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/576b2c53-92df-47f7-99ec-72ab718cee0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, each head is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9c5f484-f6d4-40f5-9db9-04ab09f37ee2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/3c17b956-2a69-4754-9395-12648bb86c05.png)], [![](img/c62b23de-a698-40f1-92ef-b3e231814ff3.png)],
    [![](img/bc0a1890-75a3-4f9d-ab82-003990c33e59.png)], and [![](img/bda05be2-efc2-405e-b383-366cff0cb582.png)] are
    trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now turn our attention to the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder is composed of a stack of six identical layers, each of which is
    made up of two sub-layers. The first of these sub-layers is a multi-head self-attention
    layer, while the second is an FNN that applies identical weights individually
    to each element in the entire sequence. This is similar to a convolution, which
    would apply the same kernel at each position. The FNN can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b931e2a-73e3-4e27-8fb5-2720f129dfd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each has a residual connection to a layer normalization. What this does is
    identify particular pieces of information in the text/image from the whole, that
    is, the most important parts that we need to pay greater attention to. The computation
    for this encoder is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ded83e2-93f0-443d-949c-72393172e3d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The encoder architecture looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b77edc5-bb87-46be-b191-0a5fa9cf6bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding layer normalization transforms the input so that it has zero
    mean and a variance of one. It does this using the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/cccb8ef3-b601-4fae-b7d8-e1e61632c20f.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ffe595af-befe-4705-b4aa-607a95331212.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, [![](img/ed7445b2-b7b8-4903-8110-2df9990b9276.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder (whose architecture is shown in the following diagram) is also
    composed of a stack of six identical layers; however, each of these layers is
    made up of three sub-layers. The first two of these sub-layers is a multi-head
    attention layer, and each is followed by a layer normalization, which is where
    the residual connection is. The first of the sub-layers is modified with a mask
    so that the positions don''t go to the subsequent positions and try to use future
    predictions to predict the current one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0102dd9b-ba8a-43b9-9da7-cf30151f5a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of the decoder is then passed to a linear layer, which is what we
    apply a softmax to.
  prefs: []
  type: TYPE_NORMAL
- en: In the architecture, we can quite easily notice that there are no convolutions
    or recurrent connections for the model to make use of in the sequence, which is
    where it sees this information. To deal with this, the authors of this method
    used positional encodings to inject information about the absolute and relative
    positions of the elements in the sequence into the input embeddings of the input
    embeddings, which are at the bottom of the encoder and decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us the full transformer architecture, which can be seen in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0d59783-c120-47ba-8063-8cd4e283eb99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The position encodings are calculated using the following two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/2c14a7e8-9ba0-42cf-9cb3-4e59c36b3fe9.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/d39565ad-0607-41f8-b379-d86d677c9120.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, *pos* is the position and *i* is the dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's conclude this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a hot new area in deep learning known as attention
    mechanisms. These are used to allow networks to focus on specific parts of input.
    This helps the network overcome the problem of long-term dependencies. We also
    learned about how these attention mechanisms can be used instead of sequential
    models such as RNNs to produce state-of-the-art results on tasks such as machine
    translation and sentence generation. However, they can also be used to focus on
    relevant parts of images. This can be used for tasks such as visual question answering,
    where we may want our network to tell us what is happening in a given scene.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about generative models.
  prefs: []
  type: TYPE_NORMAL
