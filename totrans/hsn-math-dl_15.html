<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generative Models</h1>
                </header>
            
            <article>
                
<p>So far in this book, we have covered the three main types of neural networks—<strong>feedforward neural networks</strong> (<strong>FNNs</strong>), <strong>convolutional neural networks</strong> (<strong>CNNs</strong>), and <strong>recurrent neural networks</strong> (<strong>RNNs</strong>). Each of them are discriminative models; that is, they learned to discriminate (differentiate) between the classes we wanted them to be able to predict, such as <em>is this language French or English?</em>, <em>is this song classic rock or 90s pop?</em>, and <em>what are the objects present in this scene?</em>. However, deep neural networks don't just stop there. They can also be used to improve image or video resolution or generate entirely new images and data. These types of models are known as <strong>generative models</strong>. </p>
<p>In this chapter, we will cover the following topics related to generative models:</p>
<ul>
<li>Why we need generative models</li>
<li>Autoencoders</li>
<li>Generative adversarial networks</li>
<li>Flow-based networks</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Why we need generative models</h1>
                </header>
            
            <article>
                
<p>All the various neural network architectures we have learned about in this book have served a specific purpose—to make a prediction about some given data. Each of these neural networks has its own respective strengths for various tasks. A CNN is very effective for object recognition tasks or music genre classification, an RNN is very effective for language translation or time series prediction, and FNNs are great for regression or classification. <span>Generative models, on the other hand, are those that model the data, <em>p(x)</em>, that we can sample data from, which is different from discriminative models, which learn to estimate conditional distributions, such as <em>p(•|x)</em>. </span></p>
<p><span>But how does this benefit us? What can we use generative models for? Well, there are a couple of reasons why it is important for us to understand how generative models work. To start, in image recognition, we have to learn to estimate a high-dimensional space of the</span><span> </span><em>p(y<sub>i </sub>| x)</em><span> form</span><em>,</em><span> which we can use to predict which class our data belongs to. You should remember that these models require a lot of training data. Now, what we could do instead is make it so that our data is generated from a low-dimensional latent variable, </span><img style="font-size: 1em;width:0.67em;height:0.83em;" class="fm-editor-equation" src="Images/027d2051-202b-49c6-a219-f8a475795732.png"/><span>, which makes our probability function come to </span><sub><img class="fm-editor-equation" src="Images/4f4a5787-45fc-414c-996b-013313462790.png" style="width:9.17em;height:2.42em;"/></sub><span>. What we need to do now is change</span> <span>our prediction problem top, </span><em>(y<sub>i </sub>| z)</em><span>. Another way that we can make use of generative models is to understand what our neural networks have learned. As we know, deep neural networks are quite complex and knowing what exactly they have or haven't learned is quite challenging to figure out. So, what we can do is sample from them and compare these drawn samples to the real data. Lastly, we can use generative models to create synthetic data to train our models on if we have a shortage of data.</span></p>
<p>Now that we know what generative models can be used for, let's explore some of the more popular ones and learn how they work.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Autoencoders</h1>
                </header>
            
            <article>
                
<p>An <strong>autoencoder</strong> is an unsupervised type of FNN that learns to reconstruct high-dimensional data using latent-encoded data. You can think of it as trying to learn an identity function (that is, take <em>x</em><span><strong> </strong>as input</span><span> and then predict <em>x</em>). </span></p>
<p>Let's start by taking a look at the following diagram, which shows you what an autoencoder looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-999 image-border" src="Images/d72007f7-570f-492d-a99a-81b0f5419821.png" style="width:21.00em;height:27.42em;"/></p>
<p>As you can see, the network is split into two components—an encoder and a decoder—which are mirror images of each other. The two components are connected to each other through a bottleneck layer (sometimes referred to as either a latent-space representation or compression) that has dimensions that are a lot smaller than the input. You should note that the network architecture is symmetric, but that doesn't necessarily mean its weights need be. But why? What does this network learn and how does it do it? Let's take a look at both networks and explore what they're doing.</p>
<p>The encoder network takes in high-dimensional input and reduces it to lower-dimensional latent code (that is, it learns the patterns in the input data). This works similarly to principal component analysis and matrix factorization. It works as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c5caa1c6-61d8-436a-8faa-def8e087c0fd.png" style="width:4.67em;height:1.33em;"/></p>
<p>The decoder network takes as input the lower-dimensional latent code (the patterns), which contains all the main information about the input, and reconstructs the original input (or as close to the original input as possible) from it. It works as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ad1243a9-b986-4eb0-b79a-a37179e012cb.png" style="width:5.42em;height:1.50em;"/></p>
<p>We can combine the preceding two equations and express the autoencoder as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0251ffe4-131f-4928-a10f-d6bb16787405.png" style="width:7.17em;height:1.42em;"/></p>
<p class="CDPAlignLeft CDPAlign">Our goal is for the original input to be as close (ideally, identical) to the reconstructed output—that is, <img class="fm-editor-equation" src="Images/a3f50e2e-0a9a-46ef-b7ad-5e5693082f85.png" style="width:3.08em;height:1.08em;"/>.</p>
<p>Both the encoder and decoder have separate weights, but we learn the parameters together to output the reconstructed data, which is nearly identical to the original input. During training, we can use the <strong>MSE</strong> loss:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/55975824-f4be-4347-812a-195b276dca84.png" style="width:16.17em;height:3.08em;"/></p>
<p>This type of autoencoder is commonly referred to as an <strong>undercomplete autoencoder</strong> because the bottleneck layer is much smaller than the dimensions of the input and the output layer. </p>
<p>But what goes on in this bottleneck layer that allows the decoder to reconstruct the input from it? This latent coding, which is a high-dimensional space being mapped to a lower-dimensional one, learns a manifold, which is a topological space that resembles Euclidean space at each point (we will shine more light on topological spaces and manifolds in <a href="9e02b8b3-2351-4537-9ec1-88f2946ed358.xhtml">Chapter 12</a>, <em>Geometric Deep Learning</em>). We can represent this manifold as a vector field and visualize the data clusters. It is this vector field that the autoencoder is learning to reconstruct inputs from. Each data point can be found on this manifold and we can project this back into higher-dimensional space to reconstruct it.</p>
<p>Let's suppose we have the MNIST dataset, which contains images of handwritten digits from 0-9. In the following screenshot, we can see some of the images from the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1000 image-border" src="Images/6b98beeb-e3ce-4df3-a008-0c6be83c8a93.png" style="width:23.58em;height:12.42em;"/></p>
<p>The encoder network takes this data as input and encodes it into a lower-dimensional latent bottleneck layer, which contains a compressed representation of this higher-dimensional input and shows it to us in two dimensions. This embedding space looks as follows, where e<span>ach of the colors represents a specific digit</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1001 image-border" src="Images/17071535-d5ac-49b4-bc03-e9e34d47f9b6.png" style="width:28.08em;height:28.08em;"/></p>
<p><span>About now, you are probably wondering what purpose an architecture such as this serves. What could we gain from training a model to recreate and output its own input? A number of things, as it turns out—we could use it to compress data and store it to save space and reconstruct it when we need to access it, we could remove noise from images or audio files, or we could use it for dimensionality reduction for data visualization. </span></p>
<p><span>However, just because this architecture can be used to compress images, doesn't mean this is similar to a data compression algorithm such as MP3 or JPEG. An autoencoder is only able to compress data that it has seen during training, so if it was trained on images of cars, it would be quite ineffective in compressing images of horses since the features it has learned are specific to cars, which don't generalize well to horses. Compression algorithms such as MP3 and JPEG, on the other hand, don't learn the features of the inputs they receive; they make general assumptions about their inputs. </span></p>
<p>In the following diagram, you can see an autoencoder compressing an image into latent space and reconstructing it in the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1002 image-border" src="Images/95f51791-3bdc-407e-8563-5ec844780914.png" style="width:31.17em;height:7.75em;"/></p>
<p>You can see, in the diagram, that the autoencoder has managed to reconstruct the input image and it still looks like the number four, but it isn't an exact replica; some of the information has been lost. This isn't an error in training; this is by design. Autoencoders are designed to be <em>lossy</em> and only approximately copy the input data so that it can extract only what is necessary by prioritizing what it deems more useful.</p>
<p>As we have seen so far in this book, adding layers and going deeper into autoencoders does have its advantages; it allows our neural network to capture greater complexities and reduces the required computational cost (in comparison to going wider and shallower). Similarly, we can add additional layers to our encoder and decoder. This is particularly true in the case of dealing with images because we know that convolutional layers bring better results than flattening the image and using it as input. </p>
<p>Let's now explore some of the variations of autoencoders that allow us to achieve the aforementioned tasks.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The denoising autoencoder</h1>
                </header>
            
            <article>
                
<p>The <strong>denoising autoencoder</strong> (<strong>DAE</strong>) is a variation of the preceding autoencoder as it learns to reconstruct corrupted or noisy inputs with near certainty. Suppose we have an image and, for some reason, it is blurry or some of the pixels have been corrupted and we'd like to improve the resolution of the image (kind of how they do in the movies when they can find clues in images with relatively low resolution). We can pass it through our DAE and get back a fully reconstructed image. </p>
<p>We start by corrupting the initial input using a conditional distribution, <sub><img class="fm-editor-equation" src="Images/48d9fd15-b6d5-4734-bc0c-0ea2d324eef4.png" style="width:4.00em;height:1.33em;"/></sub>—which is basically a stochastic mapping—and it returns back to us the corrupted samples. Now that we have our new input, our autoencoder will learn how to reconstruct the uncorrupted data—that is, <sub><img class="fm-editor-equation" src="Images/cdcfb74f-cb73-4ede-8f26-47bbeb429aea.png" style="width:3.83em;height:1.33em;"/></sub>—and to train this, our data will be the <sub><img class="fm-editor-equation" src="Images/ac839076-6290-470a-90a9-605594b4e505.png" style="width:4.25em;height:1.67em;"/></sub><span> </span><span>pairs</span><span>. What we want the decoder to learn is </span><sub><img class="fm-editor-equation" src="Images/cbeb2203-75c2-49f2-b8e3-20329c634b53.png" style="width:8.33em;height:1.25em;"/>,</sub><span> where as before, <em>z</em> was the output of the encoder.</span></p>
<p>The preceding corruption works as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/994ab74e-bc89-4e82-beec-a58712976674.png" style="width:17.33em;height:1.42em;"/></p>
<p>Here, <em>σ<sup>2</sup></em> is the variance of the noise.</p>
<p class="CDPAlignLeft CDPAlign">We can train our DAE just as any other FNN and perform gradient descent on the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b77edc12-94b3-42b3-aa3d-962f89e3fe00.png" style="width:17.25em;height:1.42em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="Images/ef738a85-58c6-4ba5-a214-3a9d04ef9af2.png" style="width:2.42em;height:1.42em;"/></sub> is the distribution of the training data.</p>
<p>As mentioned, the encoder projects high-dimensional data into a lower-dimensional space, called <strong>latent space</strong>, and learns the shape of the manifold. It then tries to map the corrupted data onto or near to this manifold to figure out what it could be and then pieces it together in the reconstruction process to obtain <em>x</em> by estimating <sub><img class="fm-editor-equation" src="Images/49c3ecbb-84c6-4e50-8271-cb027389c7d8.png" style="width:10.33em;height:1.58em;"/></sub> and minimizing the square error, <sub><img class="fm-editor-equation" src="Images/d7b19ff6-a81c-4be3-8a6f-532cfcce6398.png" style="width:7.42em;height:1.67em;"/></sub>.</p>
<p>We can view this process in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1003 image-border" src="Images/512b9835-4815-4deb-a858-d919d12181cd.png" style="width:43.25em;height:21.67em;"/></p>
<p>Here, the black curve is the learned manifold in the latent space and you can see the noisy points, <img class="fm-editor-equation" src="Images/868b5a11-6951-4911-b0cb-5db8b860c160.png" style="width:0.83em;height:1.17em;"/>, are projected onto the closest point on the manifold to estimate what it could be.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The variational autoencoder</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <strong>variational autoencoder</strong> (<strong>VAE</strong>) is another type of autoencoder, but with some particular differences. In fact, instead of learning functions, <em>f()</em> and <em>g()</em>, it learns the probability density function of the input data.</p>
<p class="mce-root">Let's suppose we have a distribution, <em>p<sub>θ</sub></em>, and it is parameterized by θ. Here, we can express the relationship between <em>x</em> and <em>z</em> as follows:</p>
<ul>
<li><em>p<sub>θ</sub></em>(<em>z)</em>: The prior</li>
<li><em>p<sub>θ</sub></em>(<em>x </em>| <em>z)</em>: The likelihood (the distribution of the input given the latent space)</li>
<li><em>p<sub>θ</sub>(z </em>| <em>x)</em>: The posterior (the distribution of the latent space given the input)</li>
</ul>
<p>The aforementioned distributions are parameterized by neural networks, which enables them to capture complex nonlinearities and, as we know, we train them using gradient descent. </p>
<p>But why did the authors of this method decide to deviate from the previous approach to learning a distribution? There are a few reasons why this is more effective. To start, the data we would often be dealing with is noisy and so instead, modelling the distribution is better for us. The goal here, as you may have guessed, is to generate data that has a statistic that is similar to that of the input.</p>
<p>Before we move further, let's take a look at what a VAE looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1004 image-border" src="Images/05560f37-c3a9-4577-b778-ccd6d466ea74.png" style="width:138.00em;height:41.83em;"/></p>
<p>As you can see, it shares some similarities with the autoencoder but, as we mentioned, instead of <em>z </em>= <em>f</em>(<em>x</em>) and <em>x' </em><span>= </span><em>g</em><span>(</span><em>z</em><span>)</span>, we learn <em>p </em><span>= </span><span>(<em>z </em>| </span><em>x</em><span>)</span> and <em>p </em><span>= </span><span>(</span><em>x</em> | <em>z</em><span>),</span> respectively. However, because there is now a random variable in between the input and the output, this architecture cannot be trained through regular backpropagation; we instead do backpropagation through the latent distribution's parameters.</p>
<p>Once we know the prior and likelihood distributions and the real parameters, <em>θ<sup>*</sup></em>, we can generate samples by repeatedly doing the following:</p>
<ul>
<li>Randomly generate samples from <sub><img class="fm-editor-equation" src="Images/8c20a7d0-fdd4-4ee1-b103-d7fac9970784.png" style="width:5.25em;height:1.33em;"/>.</sub></li>
<li>Generate a <sub><img class="fm-editor-equation" src="Images/f6f5782a-900c-4f3f-a71d-f3a23909dd88.png" style="width:8.58em;height:1.33em;"/> </sub>sample.</li>
</ul>
<p>Using our knowledge of probability from <a href="719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml">Chapter 3</a>, <em>Probability and Statistics</em>, we know that <em>θ<sup>*</sup></em> should maximize the probability of a real data sample being generated; that is, <sub><img class="fm-editor-equation" src="Images/421c4862-83be-4bee-afe8-726d2ec9a759.png" style="width:10.50em;height:3.08em;"/></sub>.</p>
<p class="CDPAlignLeft CDPAlign">The equation used to generate the data now is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9171c9c5-59e2-4b3a-b492-957e990b943e.png" style="width:13.42em;height:2.58em;"/></p>
<p>Now, suppose we can approximate the distribution of <em>x</em> by <span>repeatedly sampling</span> <em>z<sub>i</sub></em>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/20d3ca5b-6f53-4b30-a811-f19e81b43fba.png" style="width:9.83em;height:3.00em;"/></p>
<p>But, in order to do this, we would need a lot of samples, the majority of which would likely be zero or close to zero. This is intractable (that is, not computationally practical). So, what we do instead is learn another distribution (that is tractable)—<sub><img class="fm-editor-equation" src="Images/8e402986-ad06-40b0-bc9b-10840cd86f07.png" style="width:3.92em;height:1.25em;"/></sub>—to approximate the posterior,  <sub><img class="fm-editor-equation" src="Images/f18cf9db-05ce-4863-acaa-9234f32dab91.png" style="width:4.17em;height:1.33em;"/></sub>. Naturally, we want these two distributions to be close to each other so that they are able to better approximate the posterior distribution; so, we use <strong>Kullback-Leibler</strong> (<strong>KL</strong>) <strong>divergence</strong> to measure the distance between them and try to minimize it with respect to φ. We can see how we do this in the following equations:</p>
<p>                 <img src="Images/ed9330f0-f13e-4b44-92d8-ab992675540a.png" style="width:37.75em;height:1.92em;"/></p>
<p>From Bayes' rule, we know the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/626d0eba-dbd7-4a40-b4fd-5a400a618659.png" style="width:11.67em;height:3.00em;"/></p>
<p>If we take its logarithm, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2ac4ede8-3db1-4349-bbf5-f2bfedf63e61.png" style="width:25.17em;height:1.42em;"/></p>
<p>We can plug this back into the equation for KL divergence and get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/290ba423-80cd-429e-8a32-889b6f1a7d9d.png" style="width:43.33em;height:1.67em;"/></p>
<div class="packt_infobox">Since <em>p</em>(<em>x</em>) doesn't depend on <em>z</em>, we can keep it on the outside.</div>
<p>We can now rearrange the equation into the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0f22aa0f-a974-49b8-a609-1af841c05c64.png" style="width:40.42em;height:1.58em;"/></p>
<p>Since <sub><img class="fm-editor-equation" src="Images/cf20ab81-e971-4392-904f-2a9eccabb4a6.png" style="width:28.50em;height:1.67em;"/></sub>, the goal here is to maximize the lower bound of <sub><img class="fm-editor-equation" src="Images/128ecb66-7f7b-4649-bcc9-48e95720cc8f.png" style="width:4.17em;height:1.33em;"/></sub> because <sub><img class="fm-editor-equation" src="Images/9457681b-2850-4c3d-8c49-bb25a5ca47e2.png" style="width:24.58em;height:1.42em;"/></sub>, and we do so because the output of KL divergence is non-zero and non-negative. </p>
<p>But wait—what is the encoder and what is the decoder? This is an autoencoder, after all. Interestingly, it has been right in front of us all along. The encoder in a VAE is <sub><img class="fm-editor-equation" src="Images/607228c7-74f5-4d5a-884b-4fcde1f4979c.png" style="width:3.92em;height:1.25em;"/></sub> and is usually assumed to be Gaussian:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/56baf6da-fe75-4de4-92e0-159d8b3c7c19.png" style="width:11.92em;height:1.50em;"/></p>
<p>The decoder is <sub><img class="fm-editor-equation" src="Images/4bdb1aef-6c4d-4b6b-9220-63d9544af357.png" style="width:5.17em;height:1.67em;"/>.</sub> Both of these are modeled using neural networks.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks</h1>
                </header>
            
            <article>
                
<p>The <strong>generative adversarial network</strong> (<strong>GAN</strong>) is a game theory-inspired neural network architecture that was created by Ian Goodfellow in 2014. It comprises two networks—a generator network and a critic network—both of which compete against each other in a minimax game, which allows both of them to improve simultaneously by trying to better the other.</p>
<p>In the last couple of years, GANs have produced some phenomenal results in tasks such as creating images that are indistinguishable from real images, generating music when given some recordings, and even generating text. But these models are known for being notoriously difficult to train. Let's now find out what exactly GANs are, how they bring about such tremendous results, and what makes them so challenging to train.</p>
<p>As we know, discriminative models learn a conditional distribution and try to predict a label given input data—that is, <em>P(Y | X)</em>. Generative models, on the other hand, model a joint distribution—that is, <em>P(X, Y)</em>—and, using Bayes' rule, they can, when given the label, generate the data. So, like VAEs, they learn the distribution, <em>P(X)</em>.</p>
<p>The critic network is a discriminator (<em>D</em>) with parameters, <em>θ<sup>(D)</sup></em>, and its job is to determine whether the data being fed into it is real or fake. The generator network is a generator (<em>G</em>) with parameters, <em>θ<sup>(G)</sup></em>, whose job is to learn to create synthetic data samples from noise that can fool the discriminator into thinking the synthetic data is real with a high probability. </p>
<p>As we have seen in this book, discriminator models are brilliant at learning to map input data to a desired label (output) and can determine whether an object is present in an image, as well as tracking an object in a video and translating languages. However, they are unable to use what they have learned to generate entirely new data the way we are able to use our imagination.</p>
<p>Before we proceed, let's take a look at what this architecture looks like. In the following diagram, you can see how GANs are structured:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1341 image-border" src="Images/40b465e7-16bb-4cfd-8f66-ba8fb25e9282.png" style="width:31.83em;height:17.33em;"/></p>
<p>Now that we know what GANs look like, let's see how they work. We can summarize a GAN with the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/31fed236-4edc-4fb9-9dec-8f122cc1ab1d.png" style="width:30.58em;height:1.83em;"/></p>
<p>The discriminator's goal is for <sub><img class="fm-editor-equation" src="Images/f81c3f4e-92fd-4fa0-be07-5e0501343897.png" style="width:4.50em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/1da30232-4881-49a6-a8ed-6494bf0a7bf5.png" style="width:5.75em;height:1.25em;"/></sub>, while the generator's goal is for <sub><img class="fm-editor-equation" src="Images/ab926380-284a-4422-8c5c-da1fac2adc7a.png" style="width:6.17em;height:1.33em;"/></sub>. </p>
<p>Since the generator and discriminator have different goals, naturally, they would have different cost functions. The respective losses for the discriminator and the generator are as follows:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/109a40d3-30ba-4b01-b5e0-908ecb13ac35.png" style="width:7.17em;height:1.58em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/7f419fab-1427-43d2-b731-924ca5ed369e.png" style="width:7.17em;height:1.58em;"/></sub></li>
</ul>
<p>Naturally, neither of the two networks has a direct effect on the parameters of the other. As mentioned, since this is a game-theoretic-inspired architecture, we treat this as a two-player game and our objective is to find the Nash equilibria for cases where <em>x</em> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/291c82b4-7896-462f-b7b5-242643fb58dc.png" style="width:9.75em;height:1.58em;"/></p>
<p>This is a saddle point. When we achieve this, the discriminator is unable to differentiate between the real data and the generated data.</p>
<p class="mce-root">How do we now find the optimal value for the discriminator? Well, to start, we know the loss function and from it, we can find the optimal <em>D</em>(<em>x</em>) value:</p>
<p>                                       <img src="Images/260a4d2f-7168-4f94-90d0-e809ffe3d2af.png" style="width:26.92em;height:1.67em;"/></p>
<p>However, when trained, the generator ideally outputs <em>x</em>, so we can rewrite the loss function as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c84c5302-08ae-4e03-9fd9-89205e92bbfe.png" style="width:23.92em;height:1.67em;"/></p>
<p>Here, <em>p<sub>r</sub></em> is the real data distribution and <em>p<sub>g</sub></em> is the generated data distribution. Now, we have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b56b112a-cf62-443a-ad49-af8588a9415c.png" style="width:22.75em;height:2.58em;"/></p>
<p>To make life a bit easier, let's substitute parts of our equation with the following variables:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/6213aff8-ec9c-4e54-9e9d-f4414f7e5e88.png" style="width:5.00em;height:1.42em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/927d0693-2d9f-45d9-8636-c8775eaa0ccb.png" style="width:4.83em;height:1.33em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/baa45245-695d-4137-bd58-bd1ab899d5a1.png" style="width:4.92em;height:1.33em;"/></sub></li>
</ul>
<p>Since we are sampling over all the possible values of <em>x</em>, we can write the preceding three variables as follows:</p>
<p>                                                     <img src="Images/b5919b48-c8b7-4114-ba23-781a606c0ea5.png" style="width:15.00em;height:8.17em;"/></p>
<p>Now, to find the optimal value of the discriminator, we equate the preceding derivative to 0 and get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/923deeaa-23a0-4031-91a2-bb816fba906e.png" style="width:17.33em;height:2.67em;"/></p>
<p>So, when <sub><img class="fm-editor-equation" src="Images/b9b8b765-d95a-456b-a813-32c5c64bd69d.png" style="width:6.25em;height:1.25em;"/></sub>, <sub><img class="fm-editor-equation" src="Images/f8fc37cb-8d6d-411c-bcf4-dfae37d22703.png" style="width:5.08em;height:2.33em;"/></sub>, which satisfies our condition. The loss function now becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ea87562f-0b41-42ad-b2c8-78250f7de0e6.png" style="width:23.58em;height:5.17em;"/></p>
<p>Now that we know how to find the optimal discriminator, naturally, you may be wondering how we can find the optimal generator. Our goal here is to minimize the <strong>Jensen–Shannon</strong> (<strong>JS</strong>) divergence between the true and generated distributions, which is as follows:</p>
<p>                     <img src="Images/ff1eb53f-681e-4029-bd0c-9d5fcda33012.png" style="width:36.33em;height:6.42em;"/></p>
<p>So, <sub><img class="fm-editor-equation" src="Images/69392744-4b77-4862-bc40-0bf406354007.png" style="width:14.58em;height:1.25em;"/></sub>, which tells us that if our generator is in fact optimal, then <img class="fm-editor-equation" src="Images/ed4cef85-4c81-4a68-97b4-cf5a07de2a33.png" style="width:9.08em;height:1.25em;"/>.</p>
<p>There you have it—that's how GANs work. However, there are certain problems associated with GANs. In particular, the convergence of the two networks is not guaranteed since the gradient descent of either model does not directly impact the other, and model parameters tend to oscillate and destabilize. Another problem is mode collapse, which is a result of improper convergence, which means the generator only outputs a select few generated samples, which it knows will trick the discriminator into thinking are real. Since the generator starts to output the same few samples over and over again, the discriminator learns to classify them as fake. Mode collapse is a rather challenging problem to solve. Lastly, our discriminator could become so good that the gradient of the generator vanishes and it ends up not learning anything at all.</p>
<p>If we were to compare VAEs and GANs, both of which are generative models, we would see that with GANs, our goal is to minimize the divergence between the two distributions, while with VAEs, our objective is to minimize a bound on the divergence of the two distributions. This is a much easier task, but it doesn't produce results in quite the same way as the GAN.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Wasserstein GANs</h1>
                </header>
            
            <article>
                
<p>In the preceding section, we learned about GANs, how they work, and how they face some problems during training. Now, we will learn about the <strong>Wasserstein GAN</strong> (<strong>WGAN</strong>), which makes use of the Wasserstein distance. It is a function that measures the distance between two probability distributions on a given metric space. Imagine we're on a beach and we decide to model a three-dimensional probability distribution in the sand. The Wasserstein distance measures the least amount of energy that would be required to move and reshape the distribution into another one. So, we can say that the cost is the product of the total mass of sand we moved and the distance it was moved.</p>
<p>What this does for GANs is it smoothens the gradient and prevents the discriminator from being overtrained. The losses of our discriminator and generator are now, respectively, as follows:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/2dee8404-9204-4be6-a4c5-7f52fc37385d.png" style="width:17.83em;height:1.50em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/b1f96433-8bc8-4995-8e99-42baf1b83a52.png" style="width:11.08em;height:1.58em;"/></sub></li>
</ul>
<p>Why does this perform better than JS and KL divergence? Let's find out using the following example. </p>
<p>We have two distributions, <em>P</em> and <em>Q</em>, with the following parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c1a04e0b-b4ae-4395-9ef6-00320abdfd3d.png" style="width:22.50em;height:2.67em;"/></p>
<p>Now, let's compare KL divergence with JS divergence with the Wasserstein distance. If <em>θ </em>≠ 0, then we can observe the following:</p>
<p>                                 <img src="Images/52a69257-5188-4755-a639-ddba70da71b5.png" style="width:31.42em;height:10.83em;"/></p>
<p>When <sub><img class="fm-editor-equation" src="Images/634e1de6-527d-411e-8c14-56503413042a.png" style="width:2.58em;height:1.00em;"/></sub>, we can observe the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b6ab4a3f-4cc6-4ad8-b9fb-0da9bb363ccb.png" style="width:21.00em;height:2.83em;"/></p>
<p>As you can see, the Wasserstein distance has some clear advantages over KL and JS divergence in that it is differentiable with respect to <em>θ</em>, which improves the stability of the learning. So, the loss function now becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7302f3fe-b94e-4baf-a1e4-d4c3ad5b8889.png" style="width:21.25em;height:2.83em;"/></p>
<p>This is K-Lipschitz continuous—that is, <sub><img class="fm-editor-equation" src="Images/15eb568b-9e53-47ec-ba92-b3f7950ee3fd.png" style="width:15.08em;height:1.42em;"/></sub> for <sub><img class="fm-editor-equation" src="Images/1ee9e4c5-561d-48b6-9f80-bec07dfe502b.png" style="width:3.42em;height:1.17em;"/></sub> and <sub><img class="fm-editor-equation" src="Images/77a62cc9-5f04-4c0d-83e5-3630202c36e7.png" style="width:5.50em;height:1.25em;"/></sub>.</p>
<p>Sadly, despite the benefits of WGAN over GAN, it is still difficult to train. There are a number of variants of GAN that attempt to address this problem.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Flow-based networks</h1>
                </header>
            
            <article>
                
<p>So far in this chapter, we have studied two kinds of generative models—GANs and VAEs—but there is also another kind, known as <strong>flow-based generative models</strong>, which directly learn the probability density function of the data distribution, which is something that the previous models do not do. Flow-based models make use of nor<span>malizing flows, which overcomes the difficulty that GANs and VAEs face in trying to learn the distribution. This approach ca</span><span>n transform a simple distribution into a more complex one through a series of invertible mappings. We repeatedly apply the change of variables rule, which allows the initial probability density to flow through the series of invertible mappings, and at the end, we get the target probability distribution. </span></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Normalizing flows</h1>
                </header>
            
            <article>
                
<p>Before we can proceed with understanding how flow-based models work, let's recap some concepts such as the Jacobian matrix, calculating the determinant of a matrix and the change of the variable theorem in probability, and then go on to understand what a normalizing flow is. </p>
<p>As a refresher, the Jacobian matrix is an <em>m</em>×<em>n</em>-dimensional matrix that contains the first derivatives of a function, which maps an <em>n</em>-dimensional vector to an <em>m</em>-dimensional vector. Each element of this matrix is represented by <sub><img class="fm-editor-equation" src="Images/29e402b5-bbab-476c-b8ac-783cbd303693.png" style="width:4.42em;height:2.42em;"/></sub>. </p>
<p>The determinant can only be found for a square matrix. So, let's suppose we have an <em>n</em><span>×<em>n</em></span> matrix, <em>M</em>. Its determinant can be found using the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e0fa08dd-9519-47da-8127-03198f2c1cab.png" style="width:15.92em;height:2.75em;"/></p>
<p>Here, the sum is calculated over all <em>n</em>! permutations, <sub><img class="fm-editor-equation" src="Images/6dc58252-2789-433d-b0bc-a62a70b168ec.png" style="width:8.25em;height:1.25em;"/></sub> of <sub><img class="fm-editor-equation" src="Images/c4976b6a-35ea-4253-bf9c-b4e3d4e06840.png" style="width:5.33em;height:1.25em;"/></sub>, and <span>σ(</span><span>•)</span> tells us the signature of the permutation. However, if |<em>M</em>|= 0, then <em>M</em> is not invertible. </p>
<p>Now, let's say we have a random variable, <img class="fm-editor-equation" src="Images/8bf87813-7568-4cab-a2e2-5000eb6b5361.png" style="width:0.67em;height:0.83em;"/>, whose probability density function is <em>z </em>∼ <span>π(</span><em>z</em><span>)</span>. Using this, we can make a new random variable as the result of a one-to-one mapping, <em>x </em>= <em>f</em><span>(</span><em>z</em><span>)</span>. Since this function is invertible, we know that <em>z </em>= <em>f<sup>-1</sup></em>(<em>x</em>). But what, then, is the probability density function of our new random variable? From our knowledge of probability distributions, we know that the following is true:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c0ad8a7a-96be-498c-8b98-7791c01540f9.png" style="width:12.17em;height:2.58em;"/></p>
<p>From <a href="3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml">Chapter 1</a>, <em>Vector Calculus</em>, we should remember that an integral is the area under a curve and in probability, this is always equal to 1. This area under the curve can be sliced into infinitesimal rectangles of Δ<em>z</em><span> </span><span>width</span><span> and the height of this rectangle at</span> <em>z</em><span> is π(</span><em>z</em><span>).</span></p>
<p>Knowing<span> </span><em>z</em><span>=</span><em>f<sup>-1</sup></em><span>(</span><em>x</em><span>)</span> tells us that the ratio of a small change in <em>z</em> with respect to a small change in <em>x</em> gives us the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b9c16ee8-b64a-4205-ac5a-e5e0ee1bcdfe.png" style="width:8.92em;height:2.83em;"/> </p>
<p>We can rewrite this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/47dc8756-310c-438d-8db6-9d7fa96abb2a.png" style="width:9.75em;height:1.58em;"/></p>
<p>Now, we can rewrite our preceding distribution as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fb731b44-94a7-4564-90ca-4ce53c7874bf.png" style="width:18.50em;height:3.08em;"/></p>
<p>Since we'll we working with vectors, we can express the preceding equation in terms of multiple variables, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/94dcb68d-ab02-47e0-a80d-cd5ba4d6a80c.png" style="width:21.75em;height:3.08em;"/></p>
<p>Great! Now that we have those concepts fresh in our memory, let's move on to understanding what exactly a normalizing flow is.</p>
<p>Getting a good probability density estimation is quite important in deep learning, but it is often very challenging to do. So instead, we use a normalizing flow to approximate the distribution more efficiently by transforming a simple distribution into a more complex one by applying a series of invertible functions on it. The name comes from the fact that the change of variable normalizes the probability density after applying a mapping and the flow means that these simpler transformations can be applied continuously to create a much more complex transformation. It is also required for these transformation functions to be easily invertible and the determinant needs to be simple to compute.</p>
<p>Let's take an initial distribution, apply <em>K</em> transformations (or mappings) to it, and see how we obtain <em>x</em> from it. It works as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0bb924a7-3778-4797-8e5a-8d722754aa7e.png" style="width:23.42em;height:2.17em;"/></p>
<p class="CDPAlignLeft CDPAlign">We can also use the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f79ea1e1-bc5e-417a-a862-484c16af00a7.png" style="width:20.75em;height:1.42em;"/></p>
<p>Here, we have the following parameters:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/aa1b5a6d-50d9-4f9a-ae3f-bcccc5bceead.png" style="width:8.75em;height:1.42em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/34268cef-0b70-47fa-8db1-633f030d1c7c.png" style="width:7.00em;height:1.50em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/2d50e53a-6e3e-474a-af31-46507e7170d8.png" style="width:7.67em;height:1.67em;"/></sub></li>
<li class="CDPAlignLeft CDPAlign"><sub><img class="fm-editor-equation" src="Images/9d406955-4ba0-42b3-9215-683476e995d6.png" style="width:15.42em;height:3.25em;"/> </sub>(from the change of variables theorem)</li>
</ul>
<div class="packt_infobox">The determinant is a Jacobian matrix.</div>
<p>Let's expand on the fourth equation that we used to find <em>p<sub>i</sub></em>(<em>z<sub>i</sub></em>) to get a clearer picture of it:</p>
<p style="padding-left: 180px">                                                                  <img src="Images/06cb55a0-63b9-43f8-af85-3cecba2acd67.png" style="width:16.92em;height:9.50em;"/></p>
<p>If we take the logarithm of both sides, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aec07547-86dd-4479-a7c9-57712b13e70c.png" style="width:18.25em;height:2.50em;"/></p>
<p>This tells us the relationship that exists between the sequence of variables and from this, we can obtain the relationship between <em>x</em> and the initial distribution, <em>z<sub>0</sub></em>, through expansion, which looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e40121bf-896f-4d9e-afd2-4fc6074ac1e5.png" style="width:25.42em;height:3.33em;"/></p>
<p>It is this process that is referred to as <strong>normalizing the flow</strong>. </p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Real-valued non-volume preserving</h1>
                </header>
            
            <article>
                
<p>So far in this chapter, we have covered two very popular generative neural network architectures—VAEs and GANs—both of which are quite powerful and have brought about tremendous results in generating new data. However, both of these architectures also have their challenges. Flow-based generative models, on the other hand, while not as popular, do have their merits.</p>
<p>Some of the advantages of flow-based generative models are as follows:</p>
<ul>
<li>They have exact latent-variable inference and log-likelihood evaluation, whereas in VAEs, we can only approximately infer from latent variables, and GANs cannot infer the latent as they do not have an encoder.</li>
<li>They are efficient to parallelize for both synthesis and inference.</li>
<li>They have a useful latent space for downstream tasks and so are able to interpolate between data points and modify existing data points.</li>
<li>They are much more memory-efficient in comparison to GANs and VAEs.</li>
</ul>
<p>In this section, we will take an in-depth look at a generative probabilistic model known as <strong>real-valued non-volume preserving</strong> (<strong>real NVP</strong>) transformations, which can tractably model high-dimensional data. This model works by stacking together a sequence of invertible bijective transformations. </p>
<p>Let's suppose we have a <em>D-</em>dimensional input, <em>x</em>, it is split into two parts by <em>d &lt; D</em>, and the output, <em>y</em>, is computed using the following two equations:</p>
<ul>
<li><sub><img class="fm-editor-equation" src="Images/0d2b5289-c930-4d5b-b128-7704840f8530.png" style="width:5.83em;height:1.17em;"/></sub></li>
<li><sub><img class="fm-editor-equation" src="Images/8893c52b-151e-4a91-b563-25a72539d774.png" style="width:17.67em;height:1.75em;"/></sub></li>
</ul>
<p>Here, <img class="fm-editor-equation" src="Images/a5180735-3ffe-4ef0-97ea-f218b54ae4aa.png" style="width:1.08em;height:1.17em;"/> is an element-wise product; <em>s(</em>•<em>)</em> and <em>t(</em>•<em>)</em> are scale and translation functions that map <sub><img class="fm-editor-equation" src="Images/262094d6-d674-45ee-a88d-d32ef4c8d2d3.png" style="width:5.58em;height:1.25em;"/></sub>.</p>
<p>Using our knowledge of normalizing flows, we know that this method must satisfy two properties—it must be easily invertible and its Jacobian matrix must be simple to compute. Let's now check whether this method fits both of these criteria.</p>
<p>In the following equation, we can see that it is, in fact, quite simple to find the inverse:</p>
<p>          <img src="Images/7ff50bf8-de63-4c93-a715-4f04b61cffab.png" style="width:43.00em;height:3.42em;"/></p>
<p>Computing the inverse of the coupling layer doesn't require us to compute the inverse of <em>s(</em>•<em>)</em><span> and <em>t(</em>•<em>)</em></span>, which is great because in this case, both of those functions are CNNs and would be very difficult to invert.</p>
<p>Now, we can determine how easy the Jacobian matrix is to compute:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/354c6a6e-3fa3-4921-8f52-9ac9e565aec5.png" style="width:15.25em;height:4.17em;"/></p>
<p>This is a lower-triangular matrix. Should we want to find the determinant of the Jacobian matrix, we can do so using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a1861661-c791-4e2b-8785-0c0c1d431c48.png" style="width:12.00em;height:2.08em;"/></p>
<p class="mce-root">These two equations for the mapping tell us that when we combine the coupling layers during a forward compute, some of the parts remain unaffected. To overcome this, the authors of this method coupled the layers using an alternating pattern so that all of the parts are updated eventually.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this section, we covered a variety of generative models that learn the distribution of true data and try to generate data that is indistinguishable from it. We started with a simple autoencoder and built on it to understand a variant of it that uses variational inference to generate data similar to the input. We then went on to learn about GANs, which pit two models—a discriminator and a generator—against each other in a game so that the generator tries to learn to create data that looks real enough to fool the discriminator into thinking it is real.</p>
<p>Finally, we learned about flow-based networks, which approximate a complex probability density using a simpler one by applying several invertible transformations on it. These models are used in a variety of tasks, including—but not limited to—synthetic data generation to overcome data limitations and extracting insights from data.</p>
<p><span>In the next chapter, we will learn about transfer and meta-learning, which cover various methods involving transferring the knowledge a network has already learned for one task to bootstrap learning for another task. We will make a distinction between these two methods.</span></p>


            </article>

            
        </section>
    </div></body></html>