- en: Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have covered the three main types of neural networks—**feedforward
    neural networks** (**FNNs**), **convolutional neural networks** (**CNNs**), and
    **recurrent neural networks** (**RNNs**). Each of them are discriminative models;
    that is, they learned to discriminate (differentiate) between the classes we wanted
    them to be able to predict, such as *is this language French or English?*, *is
    this song classic rock or 90s pop?*, and *what are the objects present in this
    scene?*. However, deep neural networks don't just stop there. They can also be
    used to improve image or video resolution or generate entirely new images and
    data. These types of models are known as **generative models**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics related to generative models:'
  prefs: []
  type: TYPE_NORMAL
- en: Why we need generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow-based networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why we need generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the various neural network architectures we have learned about in this book
    have served a specific purpose—to make a prediction about some given data. Each
    of these neural networks has its own respective strengths for various tasks. A
    CNN is very effective for object recognition tasks or music genre classification,
    an RNN is very effective for language translation or time series prediction, and
    FNNs are great for regression or classification. Generative models, on the other
    hand, are those that model the data, *p(x)*, that we can sample data from, which
    is different from discriminative models, which learn to estimate conditional distributions,
    such as *p(•|x)*.
  prefs: []
  type: TYPE_NORMAL
- en: But how does this benefit us? What can we use generative models for? Well, there
    are a couple of reasons why it is important for us to understand how generative
    models work. To start, in image recognition, we have to learn to estimate a high-dimensional
    space of the *p(y[i ]| x)* form*,* which we can use to predict which class our
    data belongs to. You should remember that these models require a lot of training
    data. Now, what we could do instead is make it so that our data is generated from
    a low-dimensional latent variable, ![](img/027d2051-202b-49c6-a219-f8a475795732.png),
    which makes our probability function come to [![](img/4f4a5787-45fc-414c-996b-013313462790.png)].
    What we need to do now is change our prediction problem top, *(y[i ]| z)*. Another
    way that we can make use of generative models is to understand what our neural
    networks have learned. As we know, deep neural networks are quite complex and
    knowing what exactly they have or haven't learned is quite challenging to figure
    out. So, what we can do is sample from them and compare these drawn samples to
    the real data. Lastly, we can use generative models to create synthetic data to
    train our models on if we have a shortage of data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what generative models can be used for, let's explore some
    of the more popular ones and learn how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **autoencoder** is an unsupervised type of FNN that learns to reconstruct
    high-dimensional data using latent-encoded data. You can think of it as trying
    to learn an identity function (that is, take *x*as input and then predict *x*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by taking a look at the following diagram, which shows you what
    an autoencoder looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d72007f7-570f-492d-a99a-81b0f5419821.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the network is split into two components—an encoder and a decoder—which
    are mirror images of each other. The two components are connected to each other
    through a bottleneck layer (sometimes referred to as either a latent-space representation
    or compression) that has dimensions that are a lot smaller than the input. You
    should note that the network architecture is symmetric, but that doesn't necessarily
    mean its weights need be. But why? What does this network learn and how does it
    do it? Let's take a look at both networks and explore what they're doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder network takes in high-dimensional input and reduces it to lower-dimensional
    latent code (that is, it learns the patterns in the input data). This works similarly
    to principal component analysis and matrix factorization. It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5caa1c6-61d8-436a-8faa-def8e087c0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The decoder network takes as input the lower-dimensional latent code (the patterns),
    which contains all the main information about the input, and reconstructs the
    original input (or as close to the original input as possible) from it. It works
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad1243a9-b986-4eb0-b79a-a37179e012cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can combine the preceding two equations and express the autoencoder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0251ffe4-131f-4928-a10f-d6bb16787405.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal is for the original input to be as close (ideally, identical) to the
    reconstructed output—that is, ![](img/a3f50e2e-0a9a-46ef-b7ad-5e5693082f85.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the encoder and decoder have separate weights, but we learn the parameters
    together to output the reconstructed data, which is nearly identical to the original
    input. During training, we can use the **MSE** loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55975824-f4be-4347-812a-195b276dca84.png)'
  prefs: []
  type: TYPE_IMG
- en: This type of autoencoder is commonly referred to as an **undercomplete autoencoder**
    because the bottleneck layer is much smaller than the dimensions of the input
    and the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: But what goes on in this bottleneck layer that allows the decoder to reconstruct
    the input from it? This latent coding, which is a high-dimensional space being
    mapped to a lower-dimensional one, learns a manifold, which is a topological space
    that resembles Euclidean space at each point (we will shine more light on topological
    spaces and manifolds in [Chapter 12](9e02b8b3-2351-4537-9ec1-88f2946ed358.xhtml),
    *Geometric Deep Learning*). We can represent this manifold as a vector field and
    visualize the data clusters. It is this vector field that the autoencoder is learning
    to reconstruct inputs from. Each data point can be found on this manifold and
    we can project this back into higher-dimensional space to reconstruct it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have the MNIST dataset, which contains images of handwritten
    digits from 0-9\. In the following screenshot, we can see some of the images from
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b98beeb-e3ce-4df3-a008-0c6be83c8a93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The encoder network takes this data as input and encodes it into a lower-dimensional
    latent bottleneck layer, which contains a compressed representation of this higher-dimensional
    input and shows it to us in two dimensions. This embedding space looks as follows,
    where each of the colors represents a specific digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17071535-d5ac-49b4-bc03-e9e34d47f9b6.png)'
  prefs: []
  type: TYPE_IMG
- en: About now, you are probably wondering what purpose an architecture such as this
    serves. What could we gain from training a model to recreate and output its own
    input? A number of things, as it turns out—we could use it to compress data and
    store it to save space and reconstruct it when we need to access it, we could
    remove noise from images or audio files, or we could use it for dimensionality
    reduction for data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: However, just because this architecture can be used to compress images, doesn't
    mean this is similar to a data compression algorithm such as MP3 or JPEG. An autoencoder
    is only able to compress data that it has seen during training, so if it was trained
    on images of cars, it would be quite ineffective in compressing images of horses
    since the features it has learned are specific to cars, which don't generalize
    well to horses. Compression algorithms such as MP3 and JPEG, on the other hand,
    don't learn the features of the inputs they receive; they make general assumptions
    about their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see an autoencoder compressing an image into
    latent space and reconstructing it in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95f51791-3bdc-407e-8563-5ec844780914.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see, in the diagram, that the autoencoder has managed to reconstruct
    the input image and it still looks like the number four, but it isn't an exact
    replica; some of the information has been lost. This isn't an error in training;
    this is by design. Autoencoders are designed to be *lossy* and only approximately
    copy the input data so that it can extract only what is necessary by prioritizing
    what it deems more useful.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen so far in this book, adding layers and going deeper into autoencoders
    does have its advantages; it allows our neural network to capture greater complexities
    and reduces the required computational cost (in comparison to going wider and
    shallower). Similarly, we can add additional layers to our encoder and decoder.
    This is particularly true in the case of dealing with images because we know that
    convolutional layers bring better results than flattening the image and using
    it as input.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now explore some of the variations of autoencoders that allow us to achieve
    the aforementioned tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The denoising autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **denoising autoencoder** (**DAE**) is a variation of the preceding autoencoder
    as it learns to reconstruct corrupted or noisy inputs with near certainty. Suppose
    we have an image and, for some reason, it is blurry or some of the pixels have
    been corrupted and we'd like to improve the resolution of the image (kind of how
    they do in the movies when they can find clues in images with relatively low resolution).
    We can pass it through our DAE and get back a fully reconstructed image.
  prefs: []
  type: TYPE_NORMAL
- en: We start by corrupting the initial input using a conditional distribution, [![](img/48d9fd15-b6d5-4734-bc0c-0ea2d324eef4.png)]—which
    is basically a stochastic mapping—and it returns back to us the corrupted samples.
    Now that we have our new input, our autoencoder will learn how to reconstruct
    the uncorrupted data—that is, [![](img/cdcfb74f-cb73-4ede-8f26-47bbeb429aea.png)]—and
    to train this, our data will be the [![](img/ac839076-6290-470a-90a9-605594b4e505.png)] pairs.
    What we want the decoder to learn is [![](img/cbeb2203-75c2-49f2-b8e3-20329c634b53.png),] where
    as before, *z* was the output of the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding corruption works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/994ab74e-bc89-4e82-beec-a58712976674.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *σ²* is the variance of the noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train our DAE just as any other FNN and perform gradient descent on
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b77edc12-94b3-42b3-aa3d-962f89e3fe00.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, [![](img/ef738a85-58c6-4ba5-a214-3a9d04ef9af2.png)] is the distribution
    of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the encoder projects high-dimensional data into a lower-dimensional
    space, called **latent space**, and learns the shape of the manifold. It then
    tries to map the corrupted data onto or near to this manifold to figure out what
    it could be and then pieces it together in the reconstruction process to obtain
    *x* by estimating [![](img/49c3ecbb-84c6-4e50-8271-cb027389c7d8.png)] and minimizing
    the square error, [![](img/d7b19ff6-a81c-4be3-8a6f-532cfcce6398.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'We can view this process in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/512b9835-4815-4deb-a858-d919d12181cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the black curve is the learned manifold in the latent space and you can
    see the noisy points, ![](img/868b5a11-6951-4911-b0cb-5db8b860c160.png), are projected
    onto the closest point on the manifold to estimate what it could be.
  prefs: []
  type: TYPE_NORMAL
- en: The variational autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **variational autoencoder** (**VAE**) is another type of autoencoder, but
    with some particular differences. In fact, instead of learning functions, *f()* and
    *g()*, it learns the probability density function of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a distribution, *p[θ]*, and it is parameterized by θ.
    Here, we can express the relationship between *x* and *z* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p[θ]*(*z)*: The prior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[θ]*(*x *| *z)*: The likelihood (the distribution of the input given the
    latent space)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[θ](z *| *x)*: The posterior (the distribution of the latent space given
    the input)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aforementioned distributions are parameterized by neural networks, which
    enables them to capture complex nonlinearities and, as we know, we train them
    using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: But why did the authors of this method decide to deviate from the previous approach
    to learning a distribution? There are a few reasons why this is more effective.
    To start, the data we would often be dealing with is noisy and so instead, modelling
    the distribution is better for us. The goal here, as you may have guessed, is
    to generate data that has a statistic that is similar to that of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move further, let''s take a look at what a VAE looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05560f37-c3a9-4577-b778-ccd6d466ea74.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it shares some similarities with the autoencoder but, as we
    mentioned, instead of *z *= *f*(*x*) and *x' *= *g*(*z*), we learn *p *= (*z *| *x*) and *p *= (*x*
    | *z*), respectively. However, because there is now a random variable in between
    the input and the output, this architecture cannot be trained through regular
    backpropagation; we instead do backpropagation through the latent distribution's
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know the prior and likelihood distributions and the real parameters, *θ^**, we
    can generate samples by repeatedly doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly generate samples from [![](img/8c20a7d0-fdd4-4ee1-b103-d7fac9970784.png).]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a [![](img/f6f5782a-900c-4f3f-a71d-f3a23909dd88.png) ]sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using our knowledge of probability from [Chapter 3](719fc119-9e7a-4fce-be04-eb1e49bed753.xhtml),
    *Probability and Statistics*, we know that *θ^** should maximize the probability
    of a real data sample being generated; that is, [![](img/421c4862-83be-4bee-afe8-726d2ec9a759.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation used to generate the data now is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9171c9c5-59e2-4b3a-b492-957e990b943e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, suppose we can approximate the distribution of *x* by repeatedly sampling *z[i]*,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20d3ca5b-6f53-4b30-a811-f19e81b43fba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But, in order to do this, we would need a lot of samples, the majority of which
    would likely be zero or close to zero. This is intractable (that is, not computationally
    practical). So, what we do instead is learn another distribution (that is tractable)—[![](img/8e402986-ad06-40b0-bc9b-10840cd86f07.png)]—to
    approximate the posterior,  [![](img/f18cf9db-05ce-4863-acaa-9234f32dab91.png)].
    Naturally, we want these two distributions to be close to each other so that they
    are able to better approximate the posterior distribution; so, we use **Kullback-Leibler**
    (**KL**) **divergence** to measure the distance between them and try to minimize
    it with respect to φ. We can see how we do this in the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed9330f0-f13e-4b44-92d8-ab992675540a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From Bayes'' rule, we know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/626d0eba-dbd7-4a40-b4fd-5a400a618659.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take its logarithm, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ac4ede8-3db1-4349-bbf5-f2bfedf63e61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can plug this back into the equation for KL divergence and get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/290ba423-80cd-429e-8a32-889b6f1a7d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *p*(*x*) doesn't depend on *z*, we can keep it on the outside.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now rearrange the equation into the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f22aa0f-a974-49b8-a609-1af841c05c64.png)'
  prefs: []
  type: TYPE_IMG
- en: Since [![](img/cf20ab81-e971-4392-904f-2a9eccabb4a6.png)], the goal here is
    to maximize the lower bound of [![](img/128ecb66-7f7b-4649-bcc9-48e95720cc8f.png)] because [![](img/9457681b-2850-4c3d-8c49-bb25a5ca47e2.png)],
    and we do so because the output of KL divergence is non-zero and non-negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait—what is the encoder and what is the decoder? This is an autoencoder,
    after all. Interestingly, it has been right in front of us all along. The encoder
    in a VAE is [![](img/607228c7-74f5-4d5a-884b-4fcde1f4979c.png)] and is usually
    assumed to be Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56baf6da-fe75-4de4-92e0-159d8b3c7c19.png)'
  prefs: []
  type: TYPE_IMG
- en: The decoder is [![](img/4bdb1aef-6c4d-4b6b-9220-63d9544af357.png).] Both of
    these are modeled using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **generative adversarial network** (**GAN**) is a game theory-inspired neural
    network architecture that was created by Ian Goodfellow in 2014\. It comprises
    two networks—a generator network and a critic network—both of which compete against
    each other in a minimax game, which allows both of them to improve simultaneously
    by trying to better the other.
  prefs: []
  type: TYPE_NORMAL
- en: In the last couple of years, GANs have produced some phenomenal results in tasks
    such as creating images that are indistinguishable from real images, generating
    music when given some recordings, and even generating text. But these models are
    known for being notoriously difficult to train. Let's now find out what exactly
    GANs are, how they bring about such tremendous results, and what makes them so
    challenging to train.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, discriminative models learn a conditional distribution and try to
    predict a label given input data—that is, *P(Y | X)*. Generative models, on the
    other hand, model a joint distribution—that is, *P(X, Y)*—and, using Bayes' rule,
    they can, when given the label, generate the data. So, like VAEs, they learn the
    distribution, *P(X)*.
  prefs: []
  type: TYPE_NORMAL
- en: The critic network is a discriminator (*D*) with parameters, *θ^((D))*, and
    its job is to determine whether the data being fed into it is real or fake. The
    generator network is a generator (*G*) with parameters, *θ^((G))*, whose job is
    to learn to create synthetic data samples from noise that can fool the discriminator
    into thinking the synthetic data is real with a high probability.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this book, discriminator models are brilliant at learning
    to map input data to a desired label (output) and can determine whether an object
    is present in an image, as well as tracking an object in a video and translating
    languages. However, they are unable to use what they have learned to generate
    entirely new data the way we are able to use our imagination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, let''s take a look at what this architecture looks like.
    In the following diagram, you can see how GANs are structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40b465e7-16bb-4cfd-8f66-ba8fb25e9282.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know what GANs look like, let''s see how they work. We can summarize
    a GAN with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31fed236-4edc-4fb9-9dec-8f122cc1ab1d.png)'
  prefs: []
  type: TYPE_IMG
- en: The discriminator's goal is for [![](img/f81c3f4e-92fd-4fa0-be07-5e0501343897.png)] and [![](img/1da30232-4881-49a6-a8ed-6494bf0a7bf5.png)],
    while the generator's goal is for [![](img/ab926380-284a-4422-8c5c-da1fac2adc7a.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the generator and discriminator have different goals, naturally, they
    would have different cost functions. The respective losses for the discriminator
    and the generator are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/109a40d3-30ba-4b01-b5e0-908ecb13ac35.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/7f419fab-1427-43d2-b731-924ca5ed369e.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naturally, neither of the two networks has a direct effect on the parameters
    of the other. As mentioned, since this is a game-theoretic-inspired architecture,
    we treat this as a two-player game and our objective is to find the Nash equilibria
    for cases where *x* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/291c82b4-7896-462f-b7b5-242643fb58dc.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a saddle point. When we achieve this, the discriminator is unable to
    differentiate between the real data and the generated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we now find the optimal value for the discriminator? Well, to start,
    we know the loss function and from it, we can find the optimal *D*(*x*) value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/260a4d2f-7168-4f94-90d0-e809ffe3d2af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, when trained, the generator ideally outputs *x*, so we can rewrite
    the loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c84c5302-08ae-4e03-9fd9-89205e92bbfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *p[r]* is the real data distribution and *p[g]* is the generated data
    distribution. Now, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b56b112a-cf62-443a-ad49-af8588a9415c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make life a bit easier, let''s substitute parts of our equation with the
    following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/6213aff8-ec9c-4e54-9e9d-f4414f7e5e88.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/927d0693-2d9f-45d9-8636-c8775eaa0ccb.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/baa45245-695d-4137-bd58-bd1ab899d5a1.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we are sampling over all the possible values of *x*, we can write the
    preceding three variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5919b48-c8b7-4114-ba23-781a606c0ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, to find the optimal value of the discriminator, we equate the preceding
    derivative to 0 and get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/923deeaa-23a0-4031-91a2-bb816fba906e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, when [![](img/b9b8b765-d95a-456b-a813-32c5c64bd69d.png)], [![](img/f8fc37cb-8d6d-411c-bcf4-dfae37d22703.png)],
    which satisfies our condition. The loss function now becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea87562f-0b41-42ad-b2c8-78250f7de0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know how to find the optimal discriminator, naturally, you may
    be wondering how we can find the optimal generator. Our goal here is to minimize
    the **Jensen–Shannon** (**JS**) divergence between the true and generated distributions,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff1eb53f-681e-4029-bd0c-9d5fcda33012.png)'
  prefs: []
  type: TYPE_IMG
- en: So, [![](img/69392744-4b77-4862-bc40-0bf406354007.png)], which tells us that
    if our generator is in fact optimal, then ![](img/ed4cef85-4c81-4a68-97b4-cf5a07de2a33.png).
  prefs: []
  type: TYPE_NORMAL
- en: There you have it—that's how GANs work. However, there are certain problems
    associated with GANs. In particular, the convergence of the two networks is not
    guaranteed since the gradient descent of either model does not directly impact
    the other, and model parameters tend to oscillate and destabilize. Another problem
    is mode collapse, which is a result of improper convergence, which means the generator
    only outputs a select few generated samples, which it knows will trick the discriminator
    into thinking are real. Since the generator starts to output the same few samples
    over and over again, the discriminator learns to classify them as fake. Mode collapse
    is a rather challenging problem to solve. Lastly, our discriminator could become
    so good that the gradient of the generator vanishes and it ends up not learning
    anything at all.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to compare VAEs and GANs, both of which are generative models, we
    would see that with GANs, our goal is to minimize the divergence between the two
    distributions, while with VAEs, our objective is to minimize a bound on the divergence
    of the two distributions. This is a much easier task, but it doesn't produce results
    in quite the same way as the GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we learned about GANs, how they work, and how they
    face some problems during training. Now, we will learn about the **Wasserstein
    GAN** (**WGAN**), which makes use of the Wasserstein distance. It is a function
    that measures the distance between two probability distributions on a given metric
    space. Imagine we're on a beach and we decide to model a three-dimensional probability
    distribution in the sand. The Wasserstein distance measures the least amount of
    energy that would be required to move and reshape the distribution into another
    one. So, we can say that the cost is the product of the total mass of sand we
    moved and the distance it was moved.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this does for GANs is it smoothens the gradient and prevents the discriminator
    from being overtrained. The losses of our discriminator and generator are now,
    respectively, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/2dee8404-9204-4be6-a4c5-7f52fc37385d.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/b1f96433-8bc8-4995-8e99-42baf1b83a52.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does this perform better than JS and KL divergence? Let's find out using
    the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two distributions, *P* and *Q*, with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1a04e0b-b4ae-4395-9ef6-00320abdfd3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compare KL divergence with JS divergence with the Wasserstein distance.
    If *θ *≠ 0, then we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52a69257-5188-4755-a639-ddba70da71b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When [![](img/634e1de6-527d-411e-8c14-56503413042a.png)], we can observe the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6ab4a3f-4cc6-4ad8-b9fb-0da9bb363ccb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the Wasserstein distance has some clear advantages over KL
    and JS divergence in that it is differentiable with respect to *θ*, which improves
    the stability of the learning. So, the loss function now becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7302f3fe-b94e-4baf-a1e4-d4c3ad5b8889.png)'
  prefs: []
  type: TYPE_IMG
- en: This is K-Lipschitz continuous—that is, [![](img/15eb568b-9e53-47ec-ba92-b3f7950ee3fd.png)] for [![](img/1ee9e4c5-561d-48b6-9f80-bec07dfe502b.png)] and [![](img/77a62cc9-5f04-4c0d-83e5-3630202c36e7.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, despite the benefits of WGAN over GAN, it is still difficult to train.
    There are a number of variants of GAN that attempt to address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Flow-based networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have studied two kinds of generative models—GANs
    and VAEs—but there is also another kind, known as **flow-based generative models**,
    which directly learn the probability density function of the data distribution,
    which is something that the previous models do not do. Flow-based models make
    use of normalizing flows, which overcomes the difficulty that GANs and VAEs face
    in trying to learn the distribution. This approach can transform a simple distribution
    into a more complex one through a series of invertible mappings. We repeatedly
    apply the change of variables rule, which allows the initial probability density
    to flow through the series of invertible mappings, and at the end, we get the
    target probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing flows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can proceed with understanding how flow-based models work, let's recap
    some concepts such as the Jacobian matrix, calculating the determinant of a matrix
    and the change of the variable theorem in probability, and then go on to understand
    what a normalizing flow is.
  prefs: []
  type: TYPE_NORMAL
- en: As a refresher, the Jacobian matrix is an *m*×*n*-dimensional matrix that contains
    the first derivatives of a function, which maps an *n*-dimensional vector to an
    *m*-dimensional vector. Each element of this matrix is represented by [![](img/29e402b5-bbab-476c-b8ac-783cbd303693.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant can only be found for a square matrix. So, let''s suppose we
    have an *n*×*n* matrix, *M*. Its determinant can be found using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0fa08dd-9519-47da-8127-03198f2c1cab.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the sum is calculated over all *n*! permutations, [![](img/6dc58252-2789-433d-b0bc-a62a70b168ec.png)] of [![](img/c4976b6a-35ea-4253-bf9c-b4e3d4e06840.png)],
    and σ(•) tells us the signature of the permutation. However, if |*M*|= 0, then
    *M* is not invertible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s say we have a random variable, ![](img/8bf87813-7568-4cab-a2e2-5000eb6b5361.png),
    whose probability density function is *z *∼ π(*z*). Using this, we can make a
    new random variable as the result of a one-to-one mapping, *x *= *f*(*z*). Since
    this function is invertible, we know that *z *= *f^(-1)*(*x*). But what, then,
    is the probability density function of our new random variable? From our knowledge
    of probability distributions, we know that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ad8a7a-96be-498c-8b98-7791c01540f9.png)'
  prefs: []
  type: TYPE_IMG
- en: From [Chapter 1](3ce71171-c5fc-46c8-8124-4cb71c9dd92e.xhtml), *Vector Calculus*,
    we should remember that an integral is the area under a curve and in probability,
    this is always equal to 1\. This area under the curve can be sliced into infinitesimal
    rectangles of Δ*z* width and the height of this rectangle at *z* is π(*z*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing *z*=*f^(-1)*(*x*) tells us that the ratio of a small change in *z* with
    respect to a small change in *x* gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9c16ee8-b64a-4205-ac5a-e5e0ee1bcdfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47dc8756-310c-438d-8db6-9d7fa96abb2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can rewrite our preceding distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb731b44-94a7-4564-90ca-4ce53c7874bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we''ll we working with vectors, we can express the preceding equation
    in terms of multiple variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94dcb68d-ab02-47e0-a80d-cd5ba4d6a80c.png)'
  prefs: []
  type: TYPE_IMG
- en: Great! Now that we have those concepts fresh in our memory, let's move on to
    understanding what exactly a normalizing flow is.
  prefs: []
  type: TYPE_NORMAL
- en: Getting a good probability density estimation is quite important in deep learning,
    but it is often very challenging to do. So instead, we use a normalizing flow
    to approximate the distribution more efficiently by transforming a simple distribution
    into a more complex one by applying a series of invertible functions on it. The
    name comes from the fact that the change of variable normalizes the probability
    density after applying a mapping and the flow means that these simpler transformations
    can be applied continuously to create a much more complex transformation. It is
    also required for these transformation functions to be easily invertible and the
    determinant needs to be simple to compute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an initial distribution, apply *K* transformations (or mappings)
    to it, and see how we obtain *x* from it. It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bb924a7-3778-4797-8e5a-8d722754aa7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f79ea1e1-bc5e-417a-a862-484c16af00a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/aa1b5a6d-50d9-4f9a-ae3f-bcccc5bceead.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/34268cef-0b70-47fa-8db1-633f030d1c7c.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/2d50e53a-6e3e-474a-af31-46507e7170d8.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9d406955-4ba0-42b3-9215-683476e995d6.png) ](from the change of variables
    theorem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The determinant is a Jacobian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s expand on the fourth equation that we used to find *p[i]*(*z[i]*) to
    get a clearer picture of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06cb55a0-63b9-43f8-af85-3cecba2acd67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take the logarithm of both sides, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aec07547-86dd-4479-a7c9-57712b13e70c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This tells us the relationship that exists between the sequence of variables
    and from this, we can obtain the relationship between *x* and the initial distribution, *z[0]*,
    through expansion, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e40121bf-896f-4d9e-afd2-4fc6074ac1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: It is this process that is referred to as **normalizing the flow**.
  prefs: []
  type: TYPE_NORMAL
- en: Real-valued non-volume preserving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have covered two very popular generative neural network
    architectures—VAEs and GANs—both of which are quite powerful and have brought
    about tremendous results in generating new data. However, both of these architectures
    also have their challenges. Flow-based generative models, on the other hand, while
    not as popular, do have their merits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages of flow-based generative models are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They have exact latent-variable inference and log-likelihood evaluation, whereas
    in VAEs, we can only approximately infer from latent variables, and GANs cannot
    infer the latent as they do not have an encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are efficient to parallelize for both synthesis and inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have a useful latent space for downstream tasks and so are able to interpolate
    between data points and modify existing data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are much more memory-efficient in comparison to GANs and VAEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will take an in-depth look at a generative probabilistic
    model known as **real-valued non-volume preserving** (**real NVP**) transformations,
    which can tractably model high-dimensional data. This model works by stacking
    together a sequence of invertible bijective transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a *D-*dimensional input, *x*, it is split into two parts
    by *d < D*, and the output, *y*, is computed using the following two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/0d2b5289-c930-4d5b-b128-7704840f8530.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/8893c52b-151e-4a91-b563-25a72539d774.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, ![](img/a5180735-3ffe-4ef0-97ea-f218b54ae4aa.png) is an element-wise product; *s(*•*)* and
    *t(*•*)* are scale and translation functions that map [![](img/262094d6-d674-45ee-a88d-d32ef4c8d2d3.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Using our knowledge of normalizing flows, we know that this method must satisfy
    two properties—it must be easily invertible and its Jacobian matrix must be simple
    to compute. Let's now check whether this method fits both of these criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following equation, we can see that it is, in fact, quite simple to
    find the inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ff50bf8-de63-4c93-a715-4f04b61cffab.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the inverse of the coupling layer doesn't require us to compute the
    inverse of *s(*•*)* and *t(*•*)*, which is great because in this case, both of
    those functions are CNNs and would be very difficult to invert.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can determine how easy the Jacobian matrix is to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/354c6a6e-3fa3-4921-8f52-9ac9e565aec5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a lower-triangular matrix. Should we want to find the determinant of
    the Jacobian matrix, we can do so using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1861661-c791-4e2b-8785-0c0c1d431c48.png)'
  prefs: []
  type: TYPE_IMG
- en: These two equations for the mapping tell us that when we combine the coupling
    layers during a forward compute, some of the parts remain unaffected. To overcome
    this, the authors of this method coupled the layers using an alternating pattern
    so that all of the parts are updated eventually.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we covered a variety of generative models that learn the distribution
    of true data and try to generate data that is indistinguishable from it. We started
    with a simple autoencoder and built on it to understand a variant of it that uses
    variational inference to generate data similar to the input. We then went on to
    learn about GANs, which pit two models—a discriminator and a generator—against
    each other in a game so that the generator tries to learn to create data that
    looks real enough to fool the discriminator into thinking it is real.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about flow-based networks, which approximate a complex probability
    density using a simpler one by applying several invertible transformations on
    it. These models are used in a variety of tasks, including—but not limited to—synthetic
    data generation to overcome data limitations and extracting insights from data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about transfer and meta-learning, which cover
    various methods involving transferring the knowledge a network has already learned
    for one task to bootstrap learning for another task. We will make a distinction
    between these two methods.
  prefs: []
  type: TYPE_NORMAL
