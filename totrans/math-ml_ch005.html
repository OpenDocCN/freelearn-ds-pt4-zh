<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch005.xhtml</title>
  <style>
  </style>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="introduction" class="level2 chapterHead">
<h1 class="chapterHead"><span id="x1-7000"></span><span class="cmss-10x-x-109">Introduction</span></h1>
<p><span class="cmssi-10x-x-109">Why do I have to learn mathematics? </span><span class="cmss-10x-x-109">- This is a question I am asked daily.</span></p>
<p><span class="cmss-10x-x-109">Well, you don’t have to. But you should!</span></p>
<p><span class="cmss-10x-x-109">On the surface, advanced mathematics doesn’t impact software engineering and machine learning in a production setting. You don’t have to calculate gradients, solve linear equations, or find eigenvalues by hand. Basic and advanced algorithms are abstracted away into libraries and APIs, performing all the hard work for you.</span></p>
<p><span class="cmss-10x-x-109">Nowadays, implementing a state-of-the-art deep neural network is almost equivalent to instantiating an object in PyTorch, loading the pre-trained weights, and letting the data blaze through the model. Just like all technological advances, this is a double-edged sword. On the one hand, frameworks that accelerate prototyping and development enable machine learning in practice. Without them, we wouldn’t have seen the explosion in deep learning that we witnessed in the last decade.</span></p>
<p><span class="cmss-10x-x-109">On the other hand, high-level abstractions are barriers between us and the underlying technology. User-level knowledge is only sufficient when one is treading on familiar paths. (Or until something breaks.)</span></p>
<p><span class="cmss-10x-x-109">If you are not convinced, let’s do a thought experiment! Imagine moving to a new country without speaking the language and knowing the way of life. However, you have a smartphone and a reliable internet connection.</span></p>
<p><span class="cmss-10x-x-109">How do you start exploring?</span></p>
<p><span class="cmss-10x-x-109">With Google Maps and a credit card, you can do many awesome things there: explore the city, eat in excellent restaurants, and have a good time. You can do the groceries every day without speaking a word: just put the stuff in your basket and swipe your card at the cashier.</span></p>
<p><span class="cmss-10x-x-109">After a few months, you’ll also start to pick up some language – simple things like saying greetings or introducing yourself. You are off to a good start!</span></p>
<p><span class="cmss-10x-x-109">There are built-in solutions for everyday tasks that just work – food ordering services, public transportation, etc. However, at some point, they will break down. For instance, you need to call the delivery person who dropped off your package at the wrong door. You need to call help if your rental car breaks down.</span></p>
<p><span class="cmss-10x-x-109">You may also want to do more. Get a job, or perhaps even start your own business. For that, you need to communicate with others effectively.</span></p>
<p><span class="cmss-10x-x-109">Learning the language when you plan to live somewhere for a few months is unnecessary. However, if you want to stay there for the rest of your life, it is one of the best investments you can make.</span></p>
<p><span class="cmss-10x-x-109">Now, replace the country with machine learning and the language with mathematics.</span></p>
<p><span class="cmss-10x-x-109">The fact is that algorithms are written in the language of mathematics. To get proficient with algorithms, you have to speak it.</span></p>
<section id="what-is-this-book-about" class="level3 likesectionHead">
<h2 class="likesectionHead" id="sigil_toc_id_1"><span id="x1-8000"></span><span class="cmss-10x-x-109">What is this book about?</span></h2>
<blockquote class="packt_quote">
<p><em class="cmss-10x-x-109">”There is a similarity between knowing one’s way about a town and mastering a field of knowledge; from any given point one should be able to reach any other point. One is even better informed if one can immediately take the most convenient and quickest path from one point to the other.”</em></p>

<p class="cite"><em class="cmss-10x-x-109">— George Pólya and Gábor Szegő, in the introduction of the legendary book Problems and Theorems in Analysis</em></p>
</blockquote>

<p><span class="cmss-10x-x-109">The above quote is one of my all-time favorites. For me, it says that knowledge rests on many pillars. Like a chair has four legs, a well-rounded machine learning engineer also has a broad skill set that enables them to be effective in their job. Each of us focus on a balanced constellation of skills, and mathematics is a great addition for many. You can start machine learning without advanced mathematics, but at some point in your career, getting familiar with the mathematical background of machine learning can help you bring your skills to the next level.</span></p>
<p><span class="cmss-10x-x-109">There are two paths to mastery in deep learning. One starts from the practical parts and the other starts from theory. Both are perfectly viable, and eventually, they intertwine. This book is for those who started on the practical, application-oriented path, like data scientists, machine learning engineers, or even software developers interested in the topic.</span></p>
<p><span class="cmss-10x-x-109">This book is not a 100% pure mathematical treatise. At points, I will make some shortcuts to balance between clarity and mathematical correctness. My goal is to give you the “Eureka!” moments and help you understand the bigger picture instead of preparing you for a PhD in mathematics.</span></p>
<p><span class="cmss-10x-x-109">Most machine learning books I have read fall into one of two categories.</span></p>
<ol>
<li><span id="x1-8002x1"><span class="cmss-10x-x-109">Focus on practical applications, but unclear and imprecise with mathematical concepts.</span></span></li>
<li><span id="x1-8004x2"><span class="cmss-10x-x-109">Focus on theory, involving heavy mathematics with almost no real applications.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">I want this book to offer the best of both approaches: a sound introduction of basic and advanced mathematical concepts, keeping machine learning in sight at all times.</span></p>
<p><span class="cmss-10x-x-109">My goal is not only to cover the bare fundamentals but to give a breadth of knowledge. In my experience, to master a subject, one needs to go both deep and wide. Covering only the very essentials of mathematics would be like a tightrope walk. Instead of performing a balancing act every time you encounter a mathematical subject in the future, I want you to gain a stable footing. Such confidence can bring you very far and set you apart from others.</span></p>
<p><span class="cmss-10x-x-109">During our journey, we are going to follow a roadmap that takes us through</span></p>
<ol>
<li><span id="x1-8006x1"><span class="cmssi-10x-x-109">linear algebra</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-8008x2"><span class="cmssi-10x-x-109">calculus</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-8010x3"><span class="cmssi-10x-x-109">multivariable calculus</span><span class="cmss-10x-x-109">,</span></span></li>
<li><span id="x1-8012x4"><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">probability theory</span><span class="cmss-10x-x-109">.</span></span></li>
</ol>
<p><span class="cmss-10x-x-109">We are going to begin our journey with linear algebra. In machine learning, data is represented by vectors. Training a learning algorithm is the same as finding more descriptive representations of data through a series of transformations.</span></p>
<p><span class="cmss-10x-x-109">Linear algebra is the study of vector spaces and their transformations.</span></p>
<p><span class="cmss-10x-x-109">Simply put, a neural network is just a function that maps the data to a high-level representation. Linear transformations are the fundamental building blocks of these. Developing a good understanding of them will go a long way, as they are everywhere in machine learning.</span></p>
<p><span class="cmss-10x-x-109">While linear algebra shows how to describe predictive models, calculus has the tools to fit them to the data. When you train a neural network, you are almost certainly using gradient descent, a technique rooted in calculus and the study of differentiation.</span></p>
<p><span class="cmss-10x-x-109">Besides differentiation, its “inverse” is also a central part of calculus: integration. Integrals express essential quantities such as expected value, entropy, mean squared error, etc. They provide the foundations for probability and statistics.</span></p>
<p><span class="cmss-10x-x-109">However, when doing machine learning, we deal with functions with millions of variables. In higher dimensions, things work differently. This is where multivariable calculus comes in, where differentiation and integration are adapted to these spaces.</span></p>
<p><span class="cmss-10x-x-109">With linear algebra and calculus under our belt, we are ready to describe and train neural networks. However, we lack the understanding of extracting patterns from data. How do we draw conclusions from experiments and observations? How do we describe and discover patterns in them? These are answered by probability theory and statistics, the logic of scientific thinking. In the final chapter, we extend the classical binary logic and learn to deal with uncertainty in our predictions.</span></p>
</section>
<section id="how-to-read-this-book" class="level3 likesectionHead">
<h2 class="likesectionHead" id="sigil_toc_id_2"><span id="x1-9000"></span><span class="cmss-10x-x-109">How to read this book</span></h2>
<p><span class="cmss-10x-x-109">Mathematics follows a definition-theorem-proof structure that might be difficult to follow at first. If you are unfamiliar with such a flow, don’t worry. I’ll give a gentle introduction right now.</span></p>
<p><span class="cmss-10x-x-109">In essence, mathematics is the study of abstract objects (such as functions) through their fundamental properties. Instead of empirical observations, mathematics is based on logic, making it universal. If we want to use the powerful tool of logic, the mathematical objects need to be precisely defined. Definitions are presented in boxes like this below.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-9001r1"></span> <span class="cmbx-10x-x-109">Definition 1.</span> </span><span class="cmbx-10x-x-109">(An example definition)</span></p>
<p>Definitions appear like this.</p>
</div>
<p><span class="cmss-10x-x-109">Given a definition, results are formulated as </span><span class="cmssi-10x-x-109">if A, then B </span><span class="cmss-10x-x-109">statements, where </span><span class="cmssi-10x-x-109">A </span><span class="cmss-10x-x-109">is the premise, and </span><span class="cmssi-10x-x-109">B </span><span class="cmss-10x-x-109">is the conclusion. Such results are called theorems. For instance, </span><span class="cmssi-10x-x-109">if </span><span class="cmss-10x-x-109">a function is differentiable, </span><span class="cmssi-10x-x-109">then </span><span class="cmss-10x-x-109">it is also continuous. </span><span class="cmssi-10x-x-109">If </span><span class="cmss-10x-x-109">a function is convex, </span><span class="cmssi-10x-x-109">then </span><span class="cmss-10x-x-109">it has global minima. If we have a function, </span><span class="cmssi-10x-x-109">then </span><span class="cmss-10x-x-109">we can approximate it with arbitrary precision using a single-layer neural network. You get the pattern. Theorems are the core of mathematics.</span></p>
<p><span class="cmss-10x-x-109">We must provide a sound logical argument to accept the validity of a proposition, one that deduces the conclusion from the premise. This is called a </span><span class="cmssi-10x-x-109">proof</span><span class="cmss-10x-x-109">, responsible for the steep learning curve of mathematics. Contrary to other scientific disciplines, proofs in mathematics are indisputable statements, set in stone forever. On a practical note, look out for these boxes.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-9002r1"></span> <span class="cmbx-10x-x-109">Theorem 1.</span> </span><span class="cmbxti-10x-x-109">(An example theorem)</span></p>
<p><span class="cmti-10x-x-109">Let </span><span class="cmmi-10x-x-109">x </span><span class="cmti-10x-x-109">be a fancy mathematical object. The following two statements hold.</span></p>
<p><span class="cmti-10x-x-109">(a If </span><span class="cmmi-10x-x-109">A</span><span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">B</span><span class="cmti-10x-x-109">.</span></p>
<p><span class="cmti-10x-x-109">(b) If </span><span class="cmmi-10x-x-109">C </span><span class="cmti-10x-x-109">and </span><span class="cmmi-10x-x-109">D</span><span class="cmti-10x-x-109">, then </span><span class="cmmi-10x-x-109">E</span><span class="cmti-10x-x-109">.</span></p>
</div>
<div id="tcolobox-1" class="tcolorbox proofbox">
<div class="tcolorbox-title">

</div>
<div class="newtheorem">
<p><span class="cmssi-10x-x-109">Proof. </span><span class="cmss-10x-x-109">This is where the proof goes.</span></p>
</div>
</div>
<p><span class="cmss-10x-x-109">To enhance the learning experience, I’ll often make good-to-know but not absolutely essential information into remarks.</span></p>
<div class="newtheorem">
<p><span class="head"> <span id="x1-9003r1"></span> <span class="cmbx-10x-x-109">Remark 1.</span> </span><span class="cmbx-10x-x-109">(An exciting remark)</span></p>
<p>Mathematics is awesome. You’ll be a better engineer because of it.</p>
</div>
<p><span class="cmss-10x-x-109">The most effective way of learning is building things and putting theory into practice. In mathematics, this is the only way to learn. What this means is that you need to read through the text carefully. Don’t take anything for granted just because it is written down. Think through every sentence. Take every argument and calculation apart. Try to prove theorems by yourself before reading the proofs.</span></p>
<p><span class="cmss-10x-x-109">With that in mind, let’s get to it! Buckle up for the ride; the road is long and full of twists and turns.</span></p>
</section>
<section id="conventions-used" class="level3 likesectionHead">
<h2 class="likesectionHead" id="sigil_toc_id_3"><span id="x1-10000"></span><span class="cmss-10x-x-109">Conventions used</span></h2>
<p><span class="cmss-10x-x-109">There are a number of text conventions used throughout this book. </span><span class="cmtt-10x-x-109">CodeInText </span><span class="cmss-10x-x-109">indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, or URLs. For example: “Slicing works by specifying the first and last elements with an optional step size, using the syntax </span><span class="cmtt-10x-x-109">object[first:last:step]</span><span class="cmss-10x-x-109">.”</span></p>
<p><span class="cmss-10x-x-109">A block of code is set as follows:</span></p>
<div id="tcolobox-2" class="tcolorbox">
<div class="tcolorbox-title">

</div>
<div class="tcolorbox-contnt">
<pre class="lstinputlisting"><code>from sklearn.datasets import load_iris 
data = load_iris() 
X, y = data["/span&gt;data, data["/span&gt;target 
X[:10]</code></pre>
</div>
</div>
<p><span class="cmss-10x-x-109">Any command-line input or output is written as follows:</span></p>
<pre class="lstlisting"><code>(3.5, -2.71, 'a string')</code></pre>
<p><span class="cmssi-10x-x-109">Italics </span><span class="cmss-10x-x-109">indicate new concepts or emphasis. For instance, words in menus or dialog boxes appear in the text like this. For example: "This is our first example of a </span><span class="cmssi-10x-x-109">non-differentiable function</span><span class="cmss-10x-x-109">."</span></p>
</section>
<section id="what-this-book-covers" class="level3 likesectionHead">
<h2 class="likesectionHead" id="sigil_toc_id_4"><span id="x1-11000"></span><span class="cmss-10x-x-109">What this book covers</span></h2>
<p><span class="cmssi-10x-x-109">Chapter 1, Vectors and vector spaces </span><span class="cmss-10x-x-109">covers what vectors are and how to work with them. We’ll travel from concrete examples through precise mathematical definitions to implementations, understanding vector spaces and NumPy arrays, which are used to represent vectors efficiently. Besides the fundamentals, we’ll learn</span></p>
<p><span class="cmssi-10x-x-109">Chapter 2, The geometric structure of vector spaces </span><span class="cmss-10x-x-109">moves forward by studying the concept of norms, distances, inner products, angles, and orthogonality, enhancing the algebraic definition of vector spaces with some much-needed geometric structure. These are not just tools for visualization; they play a crucial role in machine learning. We’ll also encounter our first algorithm, the </span><span class="cmssi-10x-x-109">Gram-Schmidt orthogonalization method</span><span class="cmss-10x-x-109">, turning any set of vectors into an orthonormal basis.</span></p>
<p><span class="cmss-10x-x-109">In </span><span class="cmssi-10x-x-109">Chapter 3, Linear algebra in practice</span><span class="cmss-10x-x-109">, we break out NumPy once more, and implement </span><span class="cmssi-10x-x-109">everything </span><span class="cmss-10x-x-109">that we’ve learned so far. Here, we learn how to work with the high-performance NumPy arrays in practice: operations, broadcasting, functions, culminating in the from-scratch implementation of the Gram-Schmidt algorithm. This is also the first time we encounter </span><span class="cmssi-10x-x-109">matrices</span><span class="cmss-10x-x-109">, the workhorses of linear algebra.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 4, Linear transformations </span><span class="cmss-10x-x-109">is about the true nature of matrices; that is, structure-preserving transformations between vector spaces. This way, seemingly arcane things – such as the definition of matrix multiplication – suddenly make sense. Once more, we take the leap from algebraic structures to geometric ones, allowing us to study matrices as transformations that distort their underlying space. We’ll also look at one of the most important descriptors of matrices: the </span><span class="cmssi-10x-x-109">determinants</span><span class="cmss-10x-x-109">, describing how the underlying linear transformations affect the </span><span class="cmssi-10x-x-109">volume </span><span class="cmss-10x-x-109">of the spaces.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 5, Matrices and equations </span><span class="cmss-10x-x-109">presents the third (and for us, the final) face of matrices as </span><span class="cmssi-10x-x-109">systems of linear equations</span><span class="cmss-10x-x-109">. In this chapter, we first learn how to solve systems of linear equations by hand using the </span><span class="cmssi-10x-x-109">Gaussian elimination</span><span class="cmss-10x-x-109">, then supercharge it via our newfound knowledge of linear algebra, obtaining the mighty </span><span class="cmssi-10x-x-109">LU decomposition</span><span class="cmss-10x-x-109">. With the help of the LU decomposition, we go hard and achieve a roughly 70000 </span><span class="cmsy-10x-x-109">× </span><span class="cmss-10x-x-109">speedup on computing determinants.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 6 </span><span class="cmss-10x-x-109">introduces two of the most important descriptors of matrices: </span><span class="cmssi-10x-x-109">eigenvalues </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">eigenvectors</span><span class="cmss-10x-x-109">. Why do we need them?</span></p>
<p><span class="cmss-10x-x-109">Because in </span><span class="cmssi-10x-x-109">Chapter 7, Matrix factorizations</span><span class="cmss-10x-x-109">, we are able to reach the pinnacle of linear algebra with their help. First, we show that real and symmetric matrices can be written in diagonal form by constructing a basis from their eigenvectors, known as the </span><span class="cmssi-10x-x-109">spectral decomposition theorem</span><span class="cmss-10x-x-109">. In turn, a clever application of the spectral decomposition leads to the </span><span class="cmssi-10x-x-109">singular value decomposition</span><span class="cmss-10x-x-109">, the single most important result of linear algebra.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 8, Matrices and graphs </span><span class="cmss-10x-x-109">closes the linear algebra part of the book by studying the fruitful connection between linear algebra and graph theory. By representing matrices as graphs, we are able to show deep results such as </span><span class="cmssi-10x-x-109">the Frobenius normal form</span><span class="cmss-10x-x-109">, or even talk about the eigenvalues and eigenvectors of graphs.</span></p>
<p><span class="cmss-10x-x-109">In </span><span class="cmssi-10x-x-109">Chapter 9, Functions</span><span class="cmss-10x-x-109">, we take a detailed look at </span><span class="cmssi-10x-x-109">functions</span><span class="cmss-10x-x-109">, a concept that we have used intuitively so far. This time, we make the intuition mathematically precise, learning that functions are essentially arrows between dots.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 10, Numbers, sequences, and series </span><span class="cmss-10x-x-109">continues down the rabbit hole, looking at the concept of numbers. Each step from natural numbers towards real numbers represents a conceptual jump, peaking at the study of </span><span class="cmssi-10x-x-109">sequences </span><span class="cmss-10x-x-109">and </span><span class="cmssi-10x-x-109">series</span><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">With </span><span class="cmssi-10x-x-109">Chapter 11, Topology, limits, and continuity</span><span class="cmss-10x-x-109">, we are almost at the really interesting parts. However, in calculus, the objects, concepts, and tools are most often described in terms of limits and continuous functions. So, we take a detailed look at what they are.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 12 </span><span class="cmss-10x-x-109">is about the single most important concept in calculus: </span><span class="cmssi-10x-x-109">Differentiation</span><span class="cmss-10x-x-109">. In this chapter, we learn that the derivative of a function describes 1) the slope of the tangent line, and 2) the best local linear approximation to a function. From a practical side, we also look at how derivatives behave with respect to operations, most importantly the function composition, yielding the essential </span><span class="cmssi-10x-x-109">chain rule</span><span class="cmss-10x-x-109">, the bread and butter of backpropagation.</span></p>
<p><span class="cmss-10x-x-109">After all the setup, </span><span class="cmssi-10x-x-109">Chapter 13, Optimization </span><span class="cmss-10x-x-109">introduces the algorithm that is used to train virtually every neural network: </span><span class="cmssi-10x-x-109">gradient descent</span><span class="cmss-10x-x-109">. For that, we learn how the derivative describes the monotonicity of functions and how local extrema can be characterized with the first and second order derivatives.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 14, Integration </span><span class="cmss-10x-x-109">wraps our study of univariate functions. Intuitively speaking, </span><span class="cmssi-10x-x-109">integration </span><span class="cmss-10x-x-109">describes the (signed) area under the functions’ graph, but upon closer inspection, it also turns out to be the inverse of differentiation. In machine learning (and throughout all of mathematics, really), integrals describe various probabilities, expected values, and other essential quantities.</span></p>
<p><span class="cmss-10x-x-109">Now that we understand how calculus is done in single variables, </span><span class="cmssi-10x-x-109">Chapter 15 </span><span class="cmss-10x-x-109">leads us to the world of </span><span class="cmssi-10x-x-109">Multivariable functions</span><span class="cmss-10x-x-109">, where machine learning is done. There, we have an entire zoo of functions: </span><span class="cmssi-10x-x-109">scalar-vector</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">vector-scalar</span><span class="cmss-10x-x-109">, and </span><span class="cmssi-10x-x-109">vector-vector </span><span class="cmss-10x-x-109">ones.</span></p>
<p><span class="cmss-10x-x-109">In </span><span class="cmssi-10x-x-109">Chapter 16, Derivatives and gradients</span><span class="cmss-10x-x-109">, we continue our journey, overcoming the difficulties of generalizing differentiation to multivariable functions. Here, we have three kinds of derivatives: </span><span class="cmssi-10x-x-109">partial</span><span class="cmss-10x-x-109">, </span><span class="cmssi-10x-x-109">total</span><span class="cmss-10x-x-109">, and </span><span class="cmssi-10x-x-109">directional</span><span class="cmss-10x-x-109">; resulting in the gradient vector and the Jacobian and Hessian matrices.</span></p>
<p><span class="cmss-10x-x-109">As expected, optimization is also slightly more complicated in multiple variables. This issue is cleared up by </span><span class="cmssi-10x-x-109">Chapter 17, Optimization in multiple variables</span><span class="cmss-10x-x-109">, where we learn the analogue of the univariate second-derivative test, and implement the almighty gradient descent in its final form, concluding our study of calculus.</span></p>
<p><span class="cmss-10x-x-109">Now that we have a mechanistic understanding of machine learning, </span><span class="cmssi-10x-x-109">Chapter 18, What is probability? </span><span class="cmss-10x-x-109">shows us how to reason and model under uncertainty. In mathematical terms, </span><span class="cmssi-10x-x-109">probability spaces </span><span class="cmss-10x-x-109">are defined by the </span><span class="cmssi-10x-x-109">Kolmogorov axioms</span><span class="cmss-10x-x-109">, and we’ll also learn the tools that allow us to work with probabilistic models.</span></p>
<p><span class="cmssi-10x-x-109">Chapter 19 </span><span class="cmss-10x-x-109">introduces </span><span class="cmssi-10x-x-109">Random variables and distributions</span><span class="cmss-10x-x-109">, allowing us not only to bring the tools of calculus into probability theory, but to compact probabilistic models into sequences or functions.</span></p>
<p><span class="cmss-10x-x-109">Finally, in </span><span class="cmssi-10x-x-109">Chapter 20</span><span class="cmss-10x-x-109">, we learn the concept of </span><span class="cmssi-10x-x-109">The expected value</span><span class="cmss-10x-x-109">, quantifying probabilistic models and distributions with averages, variances, covariances, and entropy.</span></p>
</section>
<section id="to-get-the-most-out-of-this-book" class="level3 likesectionHead">
<h2 class="likesectionHead" id="sigil_toc_id_5"><span id="x1-12000"></span><span class="cmss-10x-x-109">To get the most out of this book</span></h2>
<p><span class="cmss-10x-x-109">The code for this book is provided in the form of Jupyter notebooks, hosted on GitHub at </span> <a href="https://github.com/cosmic-cortex/mathematics-of-machine-learning-book" class="url"><span class="cmtt-10x-x-109">https://github.com/cosmic-cortex/mathematics-of-machine-learning-book</span></a><span class="cmss-10x-x-109">. To run the notebooks, you’ll need to install the required packages.</span></p>
<p><span class="cmss-10x-x-109">The easiest way to install them is using Conda. Conda is a great package manager for Python. If you don’t have Conda installed on your system, the installation instructions can be found here: </span><a href="https://bit.ly/InstallConda" class="url"><span class="cmtt-10x-x-109">https://bit.ly/InstallConda</span></a><span class="cmss-10x-x-109">.</span></p>
<p><span class="cmss-10x-x-109">Note that Conda’s license might have some restrictions for commercial use. After installing Conda, follow the environment installation instructions in the book’s repository </span><span class="cmssbx-10x-x-109">README.md</span><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="download-the-example-code-files" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc"><span id="x1-13000"></span><span class="cmss-10x-x-109">Download the example code files</span></h2>
<p><span class="cmss-10x-x-109">The code bundle for the book is hosted on GitHub at </span> <a href="https://github.com/cosmic-cortex/mathematics-of-machine-learning-book" class="url"><span class="cmtt-10x-x-109">https://github.com/cosmic-cortex/mathematics-of-machine-learning-book</span></a><span class="cmss-10x-x-109">. We also have other code bundles from our rich catalog of books and videos available at </span><a href="https://github.com/PacktPublishing/" class="url"><span class="cmtt-10x-x-109">https://github.com/PacktPublishing/</span></a><span class="cmss-10x-x-109">. Check them out!</span></p>
</section>
<section id="download-the-color-images" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc"><span id="x1-14000"></span><span class="cmss-10x-x-109">Download the color images</span></h2>
<p><span class="cmss-10x-x-109">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: </span><a href="https://packt.link/gbp/9781837027873" class="url"><span class="cmtt-10x-x-109">https://packt.link/gbp/9781837027873</span></a><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="get-in-touch" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc"><span id="x1-15000"></span><span class="cmss-10x-x-109">Get in touch</span></h2>
<p><span class="cmss-10x-x-109">Feedback from our readers is always welcome. </span><span class="cmssbx-10x-x-109">General feedback</span><span class="cmss-10x-x-109">: Email </span><span class="cmtt-10x-x-109">feedback@packtpub.com </span><span class="cmss-10x-x-109">and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at </span><span class="cmtt-10x-x-109">questions@packtpub.com</span><span class="cmss-10x-x-109">. </span><span class="cmssbx-10x-x-109">Errata</span><span class="cmss-10x-x-109">: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you reported this to us. Please visit </span><a href="http://www.packtpub.com/submit-errata" class="url"><span class="cmtt-10x-x-109">http://www.packtpub.com/submit-errata</span></a><span class="cmss-10x-x-109">, click </span><span class="cmssbx-10x-x-109">Submit Errata</span><span class="cmss-10x-x-109">, and fill in the form. </span><span class="cmssbx-10x-x-109">Piracy</span><span class="cmss-10x-x-109">: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at </span><span class="cmtt-10x-x-109">copyright@packtpub.com </span><span class="cmss-10x-x-109">with a link to the material. </span><span class="cmssbx-10x-x-109">If you are interested in becoming an author</span><span class="cmss-10x-x-109">: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit </span><a href="http://authors.packtpub.com" class="url"><span class="cmtt-10x-x-109">http://authors.packtpub.com</span></a><span class="cmss-10x-x-109">.</span></p>
</section>
<section id="share-your-thoughts" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc"><span id="x1-16000"></span><span class="cmss-10x-x-109">Share your thoughts</span></h2>
<p><span class="cmss-10x-x-109">Once you’ve read </span><span class="cmssi-10x-x-109">Mathematics of Machine Learning</span><span class="cmss-10x-x-109">, we’d love to hear your thoughts! </span><a href="https://packt.link/r/1837027870"><span class="cmss-10x-x-109">Click here to go straight to the Amazon review page</span></a> <span class="cmss-10x-x-109">for this book and share your feedback.</span></p>
<p><span class="cmss-10x-x-109">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</span></p>
</section>
<section id="download-a-free-pdf-copy-of-this-book" class="level3 likesectionHead">
<h2 class="likesectionHead sigil_not_in_toc"><span id="x1-17000"></span><span class="cmss-10x-x-109">Download a free PDF copy of this book</span></h2>
<p><span class="cmss-10x-x-109">Thanks for purchasing this book!</span></p>
<p><span class="cmss-10x-x-109">Do you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?</span></p>
<p><span class="cmss-10x-x-109">Don’t worry; with every Packt book, you now get a DRM-free PDF version of that book at no cost.</span></p>
<p><span class="cmss-10x-x-109">Read anywhere, on any device. Search, copy, and paste code from your favorite technical books directly into your application.</span></p>
<p><span class="cmss-10x-x-109">The perks don’t stop there! You can get exclusive access to discounts, newsletters, and great free content in your inbox daily.</span></p>
<p><span class="cmss-10x-x-109">Follow these simple steps to get the benefits:</span></p>
<ol>
<li><div id="x1-17002x1">
<p><span class="cmss-10x-x-109">Scan the QR code or visit the link below:</span></p>
<div class="center">
<p><img src="../media/file2.png" width="108" height="108" alt="PIC"/></p>
</div>
<div class="center">
<p><a href="https://packt.link/free-ebook/9781837027873" class="url"><span class="cmtt-10x-x-109">https://packt.link/free-ebook/9781837027873</span></a></p>
</div>
</div></li>
<li><span id="x1-17004x2"><span class="cmss-10x-x-109">Submit your proof of purchase.</span></span></li>
<li><span id="x1-17006x3"><span class="cmss-10x-x-109">That’s it! We’ll send your free PDF and other benefits to your email address directly.</span></span></li>
</ol>
</section>
</section>
</body>
</html>