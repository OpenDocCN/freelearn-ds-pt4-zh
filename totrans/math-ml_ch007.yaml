- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vectors and Vector Spaces
  prefs: []
  type: TYPE_NORMAL
- en: ”I want to point out that the class of abstract linear spaces is no larger than
    the class of spaces whose elements are arrays. So what is gained by abstraction?
    First of all, the freedom to use a single symbol for an array; this way we can
    think of vectors as basic building blocks, unencumbered by components. The abstract
    view leads to simple, transparent proofs of results.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Peter D. Lax, in Chapter 1 of his book Linear Algebra and its Applications
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The mathematics of machine learning rests upon three pillars: linear algebra,
    calculus, and probability theory. Linear algebra describes how to represent and
    manipulate data; calculus helps us fit the models; while probability theory helps
    interpret them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These build on top of each other, and we will start at the beginning: representing
    and manipulating data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To guide us throughout this section, we will look at the famous Iris dataset
    ( [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)).
    This contains the measurements from three species of Iris: the lengths and widths
    of sepals and petals. Each data point includes these four measurements, for which
    we also have the corresponding species: Iris setosa, Iris virginica, or Iris versicolor.
    (Sepals are the typically green, leaf-like structures at the base of a flower
    that protect the developing bud before it opens. Petals are the colorful, soft
    parts of a flower that attract pollinators like insects or birds.)'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be loaded right away from scikit-learn ([https://scikit-learn.org/](https://scikit-learn.org/)),
    so let’s take a look!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before going into the mathematical definitions, let’s establish a common vocabulary
    first. The measurements themselves are stored in a tabular format. Rows represent
    samples, and columns represent measurements. A particular measurement type is
    often called a feature. As X.shape tells us, the Iris dataset has 150 data points
    and four features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: (Don’t worry if you are not familiar with NumPy. We’ll learn about the details
    in due time. For now, it’s enough to understand that an array’s shape describes
    its dimensions.)
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given sample, the corresponding species is called the label. In our case,
    this is either Iris setosa, Iris virginica, or Iris versicolor. Here, the labels
    are encoded with the numbers 0, 1, and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In mathematical terms, the Iris dataset forms a matrix, and the data points
    form vectors. Simply speaking, matrices are tables, while vectors are tuples.
    (Tuples are just finite and ordered sequences of objects, like (1.297,−2.35,32.3,29.874).)
    However, this simplistic view doesn’t show us the big picture. Vectors and matrices
    have a beautiful geometrical and algebraic structure, and exploring their mathematical
    theory allows us to see the patterns behind the data.
  prefs: []
  type: TYPE_NORMAL
- en: How so? Say, besides representing the data points in a compact form, we want
    to perform operations on them, like addition and scalar multiplication. Why do
    we need to add data points together? To give you a simple example, it is often
    beneficial if the features are on the same scale. If a given feature is distributed
    on a smaller scale than the others, it will have less influence on the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about this: if somebody is whispering to you something from the next
    room while speakers blast loud music right next to your ear, you won’t hear anything
    of what the person is saying to you. Large-scale features are the blasting music,
    while the smaller ones are the whisper. You may obtain much more information from
    the whisper, but you need to quiet down the music first.'
  prefs: []
  type: TYPE_NORMAL
- en: To see this phenomenon in action, let’s take a look at the distribution of the
    features of our dataset!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The raw features of the Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the figure above that some are more stretched out (like sepal
    length), while others are narrower (like sepal width). In practical scenarios,
    this can hurt the predictive performance of our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: To solve it, we can remove the mean and the standard deviation of a dataset.
    If the dataset consists of the vectors x[1],x[2],…,x[150] , we can calculate their
    mean by
  prefs: []
  type: TYPE_NORMAL
- en: '![ 150 μ = -1--∑ x ∈ ℝ4 150 i=1 i ](img/file4.png)'
  prefs: []
  type: TYPE_IMG
- en: and their standard deviation by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ┌ ---------------- ││ 1 1∑50 σ = ∘ ---- (xi − μ )2 ∈ ℝ4, 150 i=1 ](img/file5.png)'
  prefs: []
  type: TYPE_IMG
- en: where the subtraction and square operation in (x[i] −μ)² is taken elementwise.
  prefs: []
  type: TYPE_NORMAL
- en: The components of μ = (μ[1],μ[2],μ[3],μ[4]) and σ = (σ[1],σ[2],σ[3],σ[4]) are
    the means and variances of the individual features. (Recall that the Iris dataset
    contains 150 samples and 4 features per sample.)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the mean describes the average of samples, while the standard
    deviation represents the average distance from the mean. The larger the standard
    deviation is, the more spread out the samples are.
  prefs: []
  type: TYPE_NORMAL
- en: With these quantities, the scaled dataset can be described as
  prefs: []
  type: TYPE_NORMAL
- en: '![x1-−-μ- x2-−-μ- x150-−-μ σ , σ ,..., σ , ](img/file6.png)'
  prefs: []
  type: TYPE_IMG
- en: where both the subtraction and the division are taken elementwise.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with Python and NumPy, this is how it is done. (Don’t worry
    if you are not – everything you need to know about them will be explained in the
    next chapter, with example code.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: The scaled features of the Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: If you compare the modified version to the original, you can see that its features
    are on the same scale. In other words, we transformed the dataset to a more expressive
    one. From a (very) abstract point of view, machine learning is nothing else but
    a series of learned data transformations, turning raw data into a form where prediction
    is simple.
  prefs: []
  type: TYPE_NORMAL
- en: In a mathematical setting, manipulating data and modeling its relations to the
    labels arise from the concept of vector spaces and transformations between them.
    Let’s take the first steps by making the definition of vector spaces precise!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What is a vector space?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Representing multiple measurements as a tuple (x[1],x[2],…,x[n]) is a natural
    idea that has a ton of merits. The tuple form suggests that the components belong
    together in a precise order, giving a clear and concise way to store information.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this comes at a cost: now we have to work with more complex objects.
    Despite dealing with tuples like (x[1],…,x[n]) instead of numbers, there are similarities.
    For instance, any two tuple x = (x[1],…,x[n]) and y = (y[1],…,y[n])'
  prefs: []
  type: TYPE_NORMAL
- en: can be added together by x+ y = (x[1] + y[1],…,x[n] + y[n]),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and can be multiplied with scalars: if c ∈ℝ, then cx = (cx[1],…,cx[n]).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s almost like using a number.
  prefs: []
  type: TYPE_NORMAL
- en: These operations have clear geometric interpretations as well. Addition is the
    same as translation, while multiplication with a scalar is a simple stretching.
    (Or squeezing, if |c|<1.)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Geometric interpretation of addition and scalar multiplication'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we want to follow our geometric intuition (which we definitely
    do), it is unclear how to define vector multiplication. Even though the definition
  prefs: []
  type: TYPE_NORMAL
- en: '![xy = (x1y1,...,xnyn) ](img/file11.png)'
  prefs: []
  type: TYPE_IMG
- en: makes sense algebraically, we don’t see what it means in a geometric sense.
  prefs: []
  type: TYPE_NORMAL
- en: When we think about vectors and vector spaces, we are thinking about a mathematical
    structure that fits our intuitive views and expectations. So, let’s turn these
    into the definition!
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2\. (Vector spaces)
  prefs: []
  type: TYPE_NORMAL
- en: A vector space is a mathematical structure (V,F,+,⋅), where
  prefs: []
  type: TYPE_NORMAL
- en: (a) V is the set of vectors,
  prefs: []
  type: TYPE_NORMAL
- en: (b) F is a field of scalars (most commonly the real numbers ℝ or the complex
    numbers ℂ),
  prefs: []
  type: TYPE_NORMAL
- en: '(c) + : V ×V → V is the addition operation, satisfying the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: x+ y = y + x (commutativity),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x+ (y + z) = (x+ y) + z (associativity),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there is an element 0 ∈V such that x + 0 = x (existence of the null vector),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and there is an inverse −x ∈V for each x ∈V such that x+(−x) = 0 (existence
    of additive inverses)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for all vectors x,y,z ∈V ,
  prefs: []
  type: TYPE_NORMAL
- en: '(d) and ⋅ : F ×V →V is the scalar multiplication operation, satisfying'
  prefs: []
  type: TYPE_NORMAL
- en: a(bx) = (ab)x (associativity),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a(x+ y) = ax + ay (distributivity),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and 1x = x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for all scalars a,b ∈F and vectors x,y ∈V .
  prefs: []
  type: TYPE_NORMAL
- en: This definition is overloaded with new concepts, so let’s unpack it.
  prefs: []
  type: TYPE_NORMAL
- en: First, looking at operations like addition and scalar multiplication as functions
    might be unusual for you, but this is a perfectly natural representation. (We’ll
    learn about functions later in detail, but for now, feel free to think about them
    intuitively.) In writing, we use the notation x + y , but when thinking about
    + as a function of two variables, we might as well write +(x,y). The form x +
    y is called infix notation, while +(x,y) is called prefix notation.
  prefs: []
  type: TYPE_NORMAL
- en: In vector spaces, the inputs of addition are two vectors and the result is a
    single vector, thus + is a function that maps the Cartesian product V ×V to V
    .
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, scalar multiplication takes a scalar and a vector, resulting in a
    vector; meaning a function that maps F ×V to V .
  prefs: []
  type: TYPE_NORMAL
- en: '(The Cartesian product V ×V is just a set of ordered pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![V × V = {(u,v) : u,v ∈ V }. ](img/file12.png)'
  prefs: []
  type: TYPE_IMG
- en: Feel free to check out the set theory appendix (Appendix [C](ch037.xhtml#basics-of-set-theory))
    for more details, but for now, the intuitive understanding is enough.)
  prefs: []
  type: TYPE_NORMAL
- en: This is also good place to note that mathematical definitions are always formalized
    in hindsight, after the objects themselves are somewhat crystallized and familiar
    to the users. Mathematics is often presented as definitions first, theorems second.
    This is not how it is done in practice. Examples motivate definitions, not the
    other way around.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the field of scalars can be something other than real or complex
    numbers. The term field refers to a well-defined mathematical structure, which
    makes a natural notion mathematically precise. Without going into the technical
    details, we will think about fields as “a set of numbers where addition and multiplication
    work just as for real numbers”.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are not concerned with the most general case, we will use ℝ or ℂ to
    avoid unnecessary difficulty. If you are not familiar with the exact mathematical
    definition of a field, don’t worry – just think of ℝ each time you read the word
    “field”.
  prefs: []
  type: TYPE_NORMAL
- en: When everything is clear from the context, (V,ℝ,+,⋅) will often be referred
    to as V for notational simplicity. So, if the field F is not specified, it is
    implicitly assumed to be ℝ. When we want to emphasize this, we’ll call these real
    vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: At first sight, Definition [2](ch007.xhtml#x1-20004r2) is certainly too complex
    to comprehend. It seems like just a bunch of sets, operations, and properties
    thrown together. However, to help us build a mental model, we can imagine a vector
    as an arrow, starting from the null vector. (Recall that the null vector 0 is
    that special one for which x + 0 = x holds for all x. Thus, it can be considered
    as an arrow with zero length; the origin.)
  prefs: []
  type: TYPE_NORMAL
- en: To further familiarize ourselves with the concept, let’s see some examples of
    vector spaces!
  prefs: []
  type: TYPE_NORMAL
- en: 1.1.1 Examples of vector spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Examples are one of the best ways of building insight into seemingly difficult
    concepts like vector spaces. We humans, usually think in terms of models instead
    of abstractions. (Yes, this includes pure mathematicians. Even though they might
    deny it.)
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. The most ubiquitous instance of the vector space is (ℝ^n,ℝ,+,⋅),
    the same one we used to motivate the definition itself. (ℝ^n refers to the n-fold
    Cartesian product of the set of real numbers. If you are unfamiliar with this
    notion, check the set theory tutorial in Appendix C.)
  prefs: []
  type: TYPE_NORMAL
- en: (ℝ^n,ℝ,+,⋅) is the canonical model, the one we use to guide us throughout our
    studies. If n = 2, we are simply talking about the familiar Euclidean plane.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: The Euclidean plane as a vector space'
  prefs: []
  type: TYPE_NORMAL
- en: Using ℝ² or ℝ³ for visualization can help a lot. What works here will usually
    work in the general case, although sometimes this can be dangerous. Math relies
    on both intuition and logic. We develop ideas using our intuition, but we confirm
    them with our logic.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2\. Vector spaces are not just a collection of finite tuples. An example
    is the space of polynomial functions with real coefficients, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ℝ [x ] = { pixi : pi ∈ ℝ, n = 0,1,...}. i=0 ](img/file14.png)'
  prefs: []
  type: TYPE_IMG
- en: Two polynomials p(x) and q(x) can be added together by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n i p(x)+ q(x) := (pi + qi)x , k=1 ](img/file15.png)'
  prefs: []
  type: TYPE_IMG
- en: and can be multiplied with a real scalar by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n cp(x) = cpixi. k=1 ](img/file16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With these operations, (ℝ[x],ℝ,+,⋅) is a vector space. Although most of the
    time we percieve polynomials as functions, they can be represented as tuples of
    coefficients as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![∑n pixi ← → (p0,...,pn). i=0 ](img/file17.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that n – the degree of the polynomial – is unbounded. As a consequence,
    this vector space has a significantly richer structure than ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3\. The previous example can be further generalized. Let C([0,1]) denote
    the set of all continuous real functions f : [0,1] →ℝ. Then (C(ℝ),ℝ,+,⋅) is a
    vector space, where the addition and scalar multiplication are defined elementwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![(f + g )(x) := f(x)+ g(x), (cf)(x) = cf (x ) ](img/file18.png)'
  prefs: []
  type: TYPE_IMG
- en: for all f,g ∈C(ℝ) and c ∈ℝ. (Although continuity is a concept that we haven’t
    defined yet, feel free to think of a continuous function as one whose graph can
    be drawn without lifting your pen.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, that is right: functions can be thought of as vectors as well. Spaces
    of functions play a significant role in mathematics, and they come in several
    different forms. We often restrict the space to continuous functions, differentiable
    functions, or basically any subset that is closed under the given operations.'
  prefs: []
  type: TYPE_NORMAL
- en: (In fact, ℝ^n can be also thought of as a function space. From an abstract viewpoint,
    each vector x = (x[1],…,x[n]) is a mapping from {1,2,…,n} to ℝ.)
  prefs: []
  type: TYPE_NORMAL
- en: Function spaces are encountered in more advanced topics, such as [inverting
    ResNet architectures](http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf),
    which we won’t deal with in this book. However, it is worth seeing examples that
    are different (and not as straightforward) as ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 The basis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although our vector spaces contain infinitely many vectors, we can reduce the
    complexity by finding special subsets that can express any other vector.
  prefs: []
  type: TYPE_NORMAL
- en: To make this idea precise, let’s consider our recurring example ℝ^n. There,
    we have a special vector set
  prefs: []
  type: TYPE_NORMAL
- en: '| e[1] | = (1,0,…,0) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| e[2] | = (0,1,…,0) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ![.. .](img/file19.png) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| e[n] | = (0,0,…,1) |  |  |'
  prefs: []
  type: TYPE_TB
- en: which can be used to express each vector x = (x[1],…,x[n]) as
  prefs: []
  type: TYPE_NORMAL
- en: '![ n x = ∑ xe , x ∈ ℝ, e ∈ ℝn i i i i i=1 ](img/file20.png)'
  prefs: []
  type: TYPE_IMG
- en: For instance, e[1] = (1,0) and e[2] = (0,1) in ℝ².
  prefs: []
  type: TYPE_NORMAL
- en: What we have just seen feels extremely trivial and it seems to only complicate
    things. Why would we need to write vectors in the form of x = ∑ [i=1]^nx[i]e[i],
    instead of simply using the coordinates (x[1],…,x[n]) ? Because, in fact, the
    coordinate notation depends on the underlying vector set ({e[1],…,e[n]} in our
    case) used to express other vectors.
  prefs: []
  type: TYPE_NORMAL
- en: A vector is not the same as its coordinates! A single vector can have multiple
    different coordinates in different systems, and switching between these is a useful
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the set E = {e[1],…,e[n]}⊆ℝ^n is rather special, as it significantly reduces
    the complexity of representing vectors. With the vector addition and scalar multiplication
    operations, it spans our vector space entirely. E is an instance of a vector space
    basis, a set that serves as a skeleton of ℝ^n.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to introduce and study the concept of vector space
    basis in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Linear combinations and independence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s zoom out from the special case ℝ^n and start talking about general vector
    spaces. From our motivating example regarding bases, we have seen that sums of
    the form
  prefs: []
  type: TYPE_NORMAL
- en: '![∑n xivi, i=1 ](img/file21.png)'
  prefs: []
  type: TYPE_IMG
- en: where the v[i]-s are vectors and the x[i] coefficients are scalars, play a crucial
    role. These are called linear combinations. A linear combination is called trivial
    if all of the coefficients are zero.
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of vectors, the same vector can potentially be expressed as a linear
    combination in multiple ways. For example, if v[1] = (1,0),v[2] = (0,1), and v[3]
    = (1,1), then
  prefs: []
  type: TYPE_NORMAL
- en: '| (2,1) | = 2v[1] + v[2] |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | = v[1] + v[3]. |  |  |'
  prefs: []
  type: TYPE_TB
- en: This suggests that the set S = {v[1],v[2],v[3]} is redundant, as it contains
    duplicate information. The concept of linear dependence and independence makes
    this precise.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3\. (Linear dependence and independence)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space and S = {v[1],…,v[n]} be a subset of its vectors. S
    is said to be linearly dependent if it only contains the zero vector, or there
    is a nonzero v[k] that can be expressed as a linear combination of the other vectors
    v[1],…,v[k−1],v[k+1],…,v[n].
  prefs: []
  type: TYPE_NORMAL
- en: S is said to be linearly independent if it is not linearly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Linear dependence and independence can be looked at from a different angle.
    If
  prefs: []
  type: TYPE_NORMAL
- en: '![ k∑−1 ∑n vk = xivi + xivi, i=1 i=k+1 ](img/file22.png)'
  prefs: []
  type: TYPE_IMG
- en: for some nonzero v[k], then by subtracting v[k], we obtain that the null vector
    can be obtained as a nontrivial linear combination
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n 0 = xivi i=1 ](img/file23.png)'
  prefs: []
  type: TYPE_IMG
- en: for some scalars x[i], where x[k] = −1\. This is an equivalent definition of
    linear dependence. With this, we have proved the following theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2\.
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space and S = {v[1],…,v[n]}be a subset of its vectors.
  prefs: []
  type: TYPE_NORMAL
- en: (a) S is linearly dependent if and only if the null vector 0 can be obtained
    as a nontrivial linear combination.
  prefs: []
  type: TYPE_NORMAL
- en: (b) S is linearly independent if and only if whenever 0 = ∑ [i=1]^nx[i]v[i],
    all coefficients x[i] are zero.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Spans of vector sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear combinations provide a way to take a small set of vectors and generate
    a whole lot of others from them. For a set of vectors S, taking all of its possible
    linear combinations is called spanning, and the generated set is called the span.
    Formally, it is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n span(S ) = { xivi : n ∈ ℕ, vi ∈ S,xi is a scalar}. i=1 ](img/file24.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the vector set S is not necessarily finite. To help illustrate the
    concept of span, we can visualize the process in three dimensions. The span of
    two linearly independent vectors is a plane.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: The span of two linearly independent vectors u,v ∈ℝ³'
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about the span of a finite set {v[1],…,v[n]}, we denote the span
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![span (v ,...,v ). 1 n ](img/file26.png)'
  prefs: []
  type: TYPE_IMG
- en: This helps us avoid overcomplicating notations by naming every set.
  prefs: []
  type: TYPE_NORMAL
- en: Proposition 1\. Let V be a vector space and S,S[1],S[2] ⊆V be subsets of its
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: (a) If S[1] ⊆S[2], then span(S[1]) ⊆ span(S[2]).
  prefs: []
  type: TYPE_NORMAL
- en: (b) span(span(S)) = span(S).
  prefs: []
  type: TYPE_NORMAL
- en: This is our very first proof! Give it a read, and if it’s too difficult, move
    on and revisit it later. Just make sure that you understand what the proposition
    says.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. The property (a) follows directly from the definition. To prove (b),
    we have to show that span(S) ⊆ span(span(S)) and span(span(S)) ⊆ span(S).
  prefs: []
  type: TYPE_NORMAL
- en: '(This is one of those steep learning curve moments, but think about it for
    a second: two sets A and B are equal if and only if A ⊆ B and B ⊆A.)'
  prefs: []
  type: TYPE_NORMAL
- en: The former follows from the definition. For the latter, let x ∈ span(span(S)).
    Then
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n x = xivi i=1 ](img/file27.png)'
  prefs: []
  type: TYPE_IMG
- en: for some v[i] ∈ span(S). Because of v[i] being in the span of S, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑m vi = vi,juj j=1 ](img/file28.png)'
  prefs: []
  type: TYPE_IMG
- en: for some u[j] ∈S. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n ∑n ∑m ∑m ∑n x = xivi = xi vi,juj = ( xivi,j)uj, i=1 i=1 j=1 j=1 i=1 ](img/file29.png)'
  prefs: []
  type: TYPE_IMG
- en: implying that x ∈ span(S) as well.
  prefs: []
  type: TYPE_NORMAL
- en: Because of span(span(S)) = span(S), if S is linearly dependent, we can remove
    the redundant vectors and still keep the span the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about it: if S = {v[1],…,v[n]} and, say, v[n] = ∑ [i=1]^(n−1)x[i]v[i]
    , then v[n] ∈ span(S ∖{v[n]}). So,'
  prefs: []
  type: TYPE_NORMAL
- en: '![span(S ∖{vn }) = span (span(S ∖ {vn})) = span (S ). ](img/file30.png)'
  prefs: []
  type: TYPE_IMG
- en: (The operation A ∖B is the set difference, containing all that are elements
    of A, but not elements of B. Feel free to check out Appendix C for more details.)
  prefs: []
  type: TYPE_NORMAL
- en: Among sets of vectors, those that generate the entire vector space are special.
    After all this setup, we are ready to make a formal definition. Any set of vectors
    S that have the property span(S) = V is called a generating set for V .
  prefs: []
  type: TYPE_NORMAL
- en: 'S can be thought of as a “lossless compression” of V , as it contains all the
    information needed to reconstruct any element in V , yet it is smaller than the
    entire space. Thus, we want to reduce the size of the generating set as much as
    possible. This leads us to one of the most important concepts in linear algebra:
    minimal generating sets, or bases, as we prefer to call them.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Bases, the minimal generating sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With all the intuition we have built so far, let’s jump into the definition
    right away!
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4\. (Basis)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be a vector space and S be a subset of its vectors. S is a basis of V
    if:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) S is linearly independent,
  prefs: []
  type: TYPE_NORMAL
- en: (b) and span(S) = V .
  prefs: []
  type: TYPE_NORMAL
- en: The elements of a basis set are called basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown that these defining properties mean that every vector x can
    be uniquely written as a linear combination of S. (This is left as an exercise
    for the reader.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples! In ℝ³, the set {(1,0,0),(0,1,0),(0,0,1)} is a basis,
    but so is {(1,1,1),(1,1,0),(0,1,1)}. So, there can be more than one basis for
    the same vector space.
  prefs: []
  type: TYPE_NORMAL
- en: For ℝ^n, the most commonly used basis is {e[1],…,e[n]}, where e[i] is a vector
    whose all coordinates are 0, except the i-th one, which is 1\. This is called
    the standard basis.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the “information” contained in a set of vectors, bases hit the sweet
    spot. Adding any new vector to a basis set would introduce redundancy; removing
    any of its elements would cause the set to be incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: These notions are formalized in the two theorems below.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 3\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let V be a vector space and S = {v[1],…,v[n]} be a subset of vectors. The following
    are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) S is a basis.
  prefs: []
  type: TYPE_NORMAL
- en: (b) S is linearly independent and for any x ∈V ∖S, the vector set S ∪{x} is
    linearly dependent. In other words, S is a maximal linearly independent set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proof. To show the equivalence of two propositions, we have to prove two things:
    that (a) implies (b); and that (b) implies (a). Let’s start with the first one!'
  prefs: []
  type: TYPE_NORMAL
- en: (a) ![=⇒](img/file31.png) (b) If S is a basis, then any x ∈V can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ x = xivi i=1 ](img/file32.png)'
  prefs: []
  type: TYPE_IMG
- en: for some x[i] ∈ℝ. Thus, by definition, S ∪{x} is linearly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: '(b) ![=⇒](img/file33.png) (a) Our goal is to show that any x can be written
    as a linear combination of the vectors in S. By our assumption, S ∪{x} is linearly
    dependent, so 0 can be written as a nontrivial linear combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n 0 = αx + xivi, i=1 ](img/file34.png)'
  prefs: []
  type: TYPE_IMG
- en: where not all coefficients are zero. Because S is linearly independent, α cannot
    be zero (as it would imply the linear dependence of S, which would go against
    our assumptions). Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n x = − xivi, i=1 α ](img/file35.png)'
  prefs: []
  type: TYPE_IMG
- en: showing that S is a basis.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to show that every vector of a basis is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 4\.
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space and S = {v[1],…,v[n]}a basis. Then, for any v[i] ∈S,
  prefs: []
  type: TYPE_NORMAL
- en: '![span(S ∖ {vi}) ⊂ V, ](img/file36.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, the span of S ∖{v[i]}is a proper subset of V .
  prefs: []
  type: TYPE_NORMAL
- en: Proof. We are going to prove this by contradiction. Without loss of generality,
    we can assume that i = 1\. If
  prefs: []
  type: TYPE_NORMAL
- en: '![span(S ∖{v1 }) = V, ](img/file37.png)'
  prefs: []
  type: TYPE_IMG
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ v1 = xivi. i=2 ](img/file38.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that S = {v[1],…,v[n]} is not linearly independent, contradicting
    our assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the above results mean that a basis is a maximal linearly independent
    and a minimal generating set at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Given a basis S = {v[1],…,v[n]}, we implictly write the vector x = ∑ [i=1]^nx[i]v[i]
    as x = (x[1],…,x[n]). Since this decomposition is unique, we can do this without
    issues. The coefficients x[i] are also called coordinates. (Note that the coordinates
    strongly depend on the basis. Given two different bases, the coordinates of the
    same vector can be different.)
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Finite dimensional vector spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen previously, a single vector space can have many different bases,
    so bases are not unique. A very natural question that arises in this context is
    the following. If S[1] and S[2] are two bases for V , then does jS[1]j = jS[2]j
    hold? (Where jSj denotes the cardinality of the set S, that is, its “size”.)
  prefs: []
  type: TYPE_NORMAL
- en: In other words, can we do better if we select our basis more cleverly? It turns
    out that we cannot, and the sizes of any two basis sets is equal. We are not going
    to prove this, but here is the theorem in its entirety.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 5\.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![V ](img/file39.png) be a vector space, and let ![S1 ](img/file40.png)
    and ![S2 ](img/file41.png) be two bases of ![V ](img/file42.png). Then, ![|S1|
    = |S2| ](img/file43.png).
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a way to define the dimension of a vector space, which is simply
    the cardinality of its basis. We’ll denote the dimension of V as dim(V ). For
    example, ℝ^n is n-dimensional, as shown by the standard basis {(1,0,…,0),…,(0,0,…,1)}.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall the previous theorems, we assumed that a basis is finite. You
    might ask the question: is this always true? The answer is no. Examples 2 and
    3 show that this is not the case. For instance, the countably infinite set {1,x,x²,x³,…}
    is a basis for ℝ[x]. So, according to the theorem above, no finite basis can exist
    there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This marks an important distinction between vector spaces: those with finite
    bases are called finite-dimensional. I have some good news: all finite-dimensional
    real vector spaces are essentially ℝ^n. (Recall that we call a vector space real
    if its scalars are the real numbers.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why, suppose that V is an n-dimensional real vector space with basis
    {v[1],…,v[n]}, and define the mapping φ : V →ℝ^n by'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n φ : xivi → (x1,...,xn). i=1 ](img/file44.png)'
  prefs: []
  type: TYPE_IMG
- en: φ is invertible and preserves the structure of V , that is, the addition and
    scalar multiplication operations. Indeed, if u,v ∈V and α,β ∈ℝ, then φ(αu + βv)
    = αφ(x) + βφ(y). Such mappings are called isomorphisms. The word itself is derived
    from ancient Greek, with isos meaning same and morphe meaning shape. Even though
    this sounds abstract, the existence of an isomorphism between two vector spaces
    mean that they have the same structure. So, ℝ^n is not just an example of finite
    dimensional real vector spaces, it is a universal model of them. Note that if
    the scalars are not the real numbers, the isomorphism to ℝ^n is not true. (We’ll
    talk more about transformations like this in later chapters.)
  prefs: []
  type: TYPE_NORMAL
- en: Considering that we’ll almost exclusively deal with finite dimensional real
    vector spaces, this is good news. Using ℝ^n is not just a heuristic, it is a good
    mental model.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Why are bases so important?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If every finite-dimensional real vector space is essentially the same as ℝ^n,
    what do we gain from abstraction? Sure, we can just work with ℝ^n without talking
    about bases, but to develop a deep understanding of the core mathematical concepts
    in machine learning, we need the abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look ahead briefly and see an example. If you have some experience with
    neural networks, you know that matrices play an essential role there. Without
    any context, matrices are just a table of numbers with seemingly arbitrary rules
    of computation. Have you ever wondered why matrix multiplication is defined the
    way it is?
  prefs: []
  type: TYPE_NORMAL
- en: Although we haven’t precisely defined matrices yet, you have probably encountered
    them previously. We’ll learn all about them in Chapter [3](ch009.xhtml#linear-algebra-in-practice)
    and Chapter [4](ch010.xhtml#linear-transformations), but for the two matrices
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊ ⌋ ⌊ ⌋ |a1,1 a1,2 ... a1,n| | b1,1 b1,2 ... b1,n| ||a2,1 a2,2 ... a2,n||
    || b2,1 b2,2 ... b2,n|| | . . . . | | . . . . | A = || .. .. .. .. || , B = ||
    .. .. .. .. || , ||a a ... a || ||b b ... b || ⌈ n,1 n,2 n,n⌉ ⌈ n,1 n,2 n,n⌉ ](img/file45.png)'
  prefs: []
  type: TYPE_IMG
- en: their product AB is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ ⌊∑ ∑ ∑ ⌋ nk=1 a1,kbk,1 nk=1 a1,kbk,2 ... nk=1a1,kbk,n ||∑n ∑n ∑n || || k=1
    a2,kbk,1 k=1 a2,kbk,2 ... k=1a2,kbk,n|| AB = || ... ... ... ... || , |∑n ∑n ∑n
    | |⌈ k=1 an,kbk,1 k=1 an,kbk,2 ... k=1an,kbk,n|⌉ ](img/file46.png)'
  prefs: []
  type: TYPE_IMG
- en: that is, the (i,j)-th element of AB is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ∑ a b . i,kk,j k=1 ](img/file47.png)'
  prefs: []
  type: TYPE_IMG
- en: This definition feels random. Why not just take the componentwise product (a[i,j]b[i,j])[i,j=1]^n?
    The definition becomes crystal clear once we look at a matrix as a tool to describe
    linear transformations between vector spaces, as the elements of the matrix describe
    the images of basis vectors. In this context, multiplication of matrices is just
    the composition of linear transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of just putting out the definition and telling you how to use it, I
    want you to understand why it is defined that way. In the next chapters, we are
    going to learn every nook and cranny of matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.6 The existence of bases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At this point, you might ask the question: for a given vector space, are we
    guaranteed to find a basis? Without such a guarantee, the previous setup might
    be wasted. (As there might not be a basis to work with.)'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, this is not the case. As the proof is extremely difficult, we will
    not show this, but this is so important that we should at least state the theorem.
    If you are interested in how this can be done, I included a proof sketch. Feel
    free to skip this, as it is not going to be essential for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 6\.
  prefs: []
  type: TYPE_NORMAL
- en: Every vector space has a basis.
  prefs: []
  type: TYPE_NORMAL
- en: Proof. (Sketch.) The proof of this uses an advanced technique called transfinite
    induction, which is way beyond our scope. (Check out Naive Set Theory by Paul
    Halmos for details.) Instead of being precise, let’s just focus on building intuition
    about how to construct a basis for any vector space.
  prefs: []
  type: TYPE_NORMAL
- en: For our vector space V , we will build a basis one by one. Given any non-null
    vector v[1], if span(S[1])≠V , the set S[1] = {v[1]} is not yet a basis. Thus,
    we can find a vector v[2] ∈ V ∖ span(S[1]) so that S[2] := S[1] ∪{v[2]} is still
    linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: Is S[2] a basis? If not, we can continue the process. In case the process stops
    in finitely many steps, we are done. However, this is not guaranteed. Think about
    ℝ[x], the vector space of polynomials, which is not finite-dimensional, as we
    saw in Section [1.2.4](ch007.xhtml#finite-dimensional-vector-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: This is where we need to employ some set-theoretical heavy machinery (which
    we don’t have).
  prefs: []
  type: TYPE_NORMAL
- en: If the process doesn’t stop, we need to find a set S[ℵ[0]] that contains all
    S[i] as a subset. (Finding this S[ℵ[0]] set is the tricky part.) Is S[ℵ[0]] a
    basis? If not, we continue the process.
  prefs: []
  type: TYPE_NORMAL
- en: This is difficult to show, but the process eventually stops, and we can’t add
    any more vectors to our linearly independent vector set without destroying the
    independence property. When this happens, we have found a maximal linearly independent
    set — that is, a basis.
  prefs: []
  type: TYPE_NORMAL
- en: For finite dimensional vector spaces, the above process is easy to describe.
    In fact, one of the pillars of linear algebra is the so-called Gram-Schmidt process,
    used to explicitly construct special bases for vector spaces. As several quintessential
    results rely on this, we are going to study it in detail during the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.7 Subspaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get our hands dirty with vectors in Python, there is one more subject
    we need to talk about, one that will come in handy when talking about linear transformations.
    (But again, linear transformations are at the heart of machine learning. Everything
    we learn is to get to know them better.) For a given vector space V , we are often
    interested in one of its subsets that is a vector space in its entirety. This
    is described by the concept of subspaces.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 5\. (Subspaces)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space. The set U ⊆V is a subspace of V if it is closed under
    addition and scalar multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: U is a proper subspace if it is a subspace and U ⊂V .
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, subspaces are vector spaces themselves, so we can define their
    dimension as well. There are at least two subspaces of each vector space: itself
    and {0}. These are called trivial subspaces. Besides those, the span of a set
    of vectors is always a subspace. One such example is illustrated in Figure [1.5](#).'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important aspects of subspaces is that we can use them to create
    more subspaces. This notion is made precise below.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 6\. (Direct sum of subspaces)
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space and U[1],U[2] be two of its subspaces. The direct sum
    of U[1] and U[2] is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![U1 + U2 = {u1 + u2 : u1 ∈ U1, u2 ∈ U2}. ](img/file48.png)'
  prefs: []
  type: TYPE_IMG
- en: You can easily verify that U[1] + U[2] is a subspace indeed, moreover U[1] +
    U[2] = span(U[1] ∪U[2]). Subspaces and their direct sum play an essential role
    in several topics, such as matrix decompositions. For example, we’ll see later
    that many of them are equivalent to decomposing a linear space into a sum of vector
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to select a basis whose subsets span certain given subspaces often
    comes in handy. This is formalized by the next result.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 7\.
  prefs: []
  type: TYPE_NORMAL
- en: Let V be a vector space and U[1],U[2] be two of its subspaces such that U[1]
    + U[2] = V . Moreover, let {p[1],…,p[k]} ⊆ U[1] be a basis of U[1] and {q[1],…,q[l]}⊆U[2]
    be a basis of U[2]. Then the union
  prefs: []
  type: TYPE_NORMAL
- en: '![{p1,...,pk} ∪ {q1,...,ql} ](img/file49.png)'
  prefs: []
  type: TYPE_IMG
- en: is a basis in V .
  prefs: []
  type: TYPE_NORMAL
- en: Proof. This follows directly from the direct sum’s definition. If V = U[1] +
    U[2], then any x ∈V can be written in the form x = a + b, where a ∈U[1] and b
    ∈U[2].
  prefs: []
  type: TYPE_NORMAL
- en: In turn, since p[1],…,p[k] form a basis in U[1] and q[1],…,q[l] form a basis
    in U[2], the vectors a and b can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![ k l a = ∑ a p , b = ∑ bq . i=1 i i i=1 i i ](img/file50.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, any x takes the form
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑k ∑ l x = aipi + biqi, i=1 i=1 ](img/file51.png)'
  prefs: []
  type: TYPE_IMG
- en: which is the definition of the basis.
  prefs: []
  type: TYPE_NORMAL
- en: We are barely scratching the surface. Bases are essential, but they only provide
    the skeleton for the vector spaces encountered in practice. To properly represent
    and manipulate data, we need to build a geometric structure around this skeleton.
    How can we measure the “distance” between two measurements? What about their similarity?
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides all that, there is an even more crucial question: how on earth will
    we represent vectors inside a computer? In the next section, we will take a look
    at the data structures of Python, laying the foundation for the data manipulations
    and transformations we’ll do later.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Vectors in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have mostly talked about the theory of vectors and vector spaces.
    However, our ultimate goal is to build computational models for discovering and
    analyzing patterns in data. To put theory into practice, we will take a look at
    how vectors are represented in computations.
  prefs: []
  type: TYPE_NORMAL
- en: In computer science, there is a stark contrast between how we think about mathematical
    structures and how we represent them inside a computer. Until this point, our
    goal was to develop a mathematical framework that enables us to reason about the
    structure of data and its transformations. We want a language that is
  prefs: []
  type: TYPE_NORMAL
- en: expressive,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: easy to speak,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as compact as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, our goals change when we aim to do computations instead of pure logical
    reasoning. We want implementations that are
  prefs: []
  type: TYPE_NORMAL
- en: easy to work with,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: memory-efficient,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fast to access, manipulate and transform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are often contradicting requirements, and particular situations might
    prefer one over the other. For instance, if we have plenty of memory but want
    to perform lots of computations, we can sacrifice size for speed. Because of all
    the potential use-cases, there are multiple formats to represent the same mathematical
    concepts. These are called data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Different programming languages implement vectors differently. Because Python
    is ubiquitous in data science and machine learning, it’ll be our language of choice.
    In this chapter, we are going to study all the possible data structures in Python
    to see which one is suitable to represent vectors for high performance computations.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.1 Tuples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In standard Python, there are (at least) two built-in data structures that
    can be used to represent vectors: tuples and lists. Let’s start with tuples! They
    can be simply defined by enumerating their elements between two parentheses, separating
    them with commas.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: A single tuple can hold elements of various types. Even though we’ll exclusively
    deal with floats in computational linear algebra, this property is extremely useful
    for general-purpose programming.
  prefs: []
  type: TYPE_NORMAL
- en: We can access the elements of a tuple by indexing. Just like in several other
    programming languages, indexing starts from zero. This is in stark contrast with
    mathematics, where we often start indexing from one. Accordingly, in most languages
    designed for scientific computing, such as Fortran, Matlab, or Julia, indexing
    starts from one.
  prefs: []
  type: TYPE_NORMAL
- en: (Don’t tell this to anybody else, but indexing from zero used to drive me crazy.
    I am a mathematician by training.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The size of a tuple can be accessed by calling the built-in len function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Besides indexing, we can also access multiple elements by slicing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Slicing works by specifying the first and last elements with an optional step
    size, using the syntax object[first:last:step].
  prefs: []
  type: TYPE_NORMAL
- en: Tuples are rather inflexible, as you cannot change their components. Attempting
    to do so results in a TypeError, Python’s standard way of telling you that the
    object does not support the method you are trying to call. (In our case, item
    assignment.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Besides that, extending the tuple with additional elements is also not supported.
    As we cannot change the state of a tuple object in any way after it has been instantiated,
    they are immutable. Depending on the use-case, immutability can be an advantage
    and a disadvantage as well. Immutable objects eliminate accidental changes, but
    each operation requires the creation of a new object, resulting in a computational
    overhead. Thus, tuples are not going to be optimal to represent large amounts
    of data in complex computations.
  prefs: []
  type: TYPE_NORMAL
- en: This issue is solved by lists. Let’s take a look at them, and the new problems
    they introduce!
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2 Lists
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lists are the workhorses of Python. In contrast with tuples, lists are extremely
    flexible and easy to use, albeit this comes at the cost of runtime. Similarly
    to tuples, a list object can be created by enumerating its objects between square
    brackets, separated by commas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like tuples, accessing the elements of a list is done by indexing or slicing.
    We can do all kinds of operations on a list: overwrite its elements, append items,
    or even remove others.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This example illustrates that lists can hold elements of various types as well.
    Adding and removing elements can be done with methods like append, push, pop,
    and remove.
  prefs: []
  type: TYPE_NORMAL
- en: Before trying that, let’s quickly take note of the memory address of our example
    list, accessed by calling the id function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This number simply refers to an address in my computer’s memory, where the v_list
    object is located. Quite literally, as this book is compiled on my personal computer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to perform a few simple operations on our list and show that
    the memory address doesn’t change. Thus, no new object is created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, adding lists together achieves a result that is completely different
    from our expectations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Instead of adding the corresponding elements together, like we want vectors
    to behave, the lists are concatenated. This feature is handy when writing general-purpose
    applications. However, this is not well-suited for scientific computations. “Scalar
    multiplication” also has strange results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiplying a list with an integer repeats the list by the specified number
    of times. Given the behavior of the + operator on lists, this seems logical as
    multiplication with an integer is repeated addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![a⋅b = b◟+--⋅◝⋅⋅◜+--b◞. a times ](img/file52.png)'
  prefs: []
  type: TYPE_IMG
- en: Overall, lists can do much more than we need to represent vectors. Although
    we potentially want to change elements of our vectors, we don’t need to add or
    remove elements from them, and we also don’t need to store objects other than
    floats. Can we sacrifice these extra features and obtain an implementation that’s
    suitable for our purposes yet has lightning-fast computational performance? Yes.
    Enter NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3 NumPy arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though Python’s built-in data structures are amazing, they are optimized
    for ease of use, not for scientific computation. This problem was realized early
    on in the language’s development and was addressed by the [NumPy](https://numpy.org/)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main selling points of Python is how fast and straightforward it
    is to write code, even for complex tasks. This comes at the price of speed. However,
    in machine learning, speed is crucial for us. When training a neural network,
    a small set of operations are repeated millions of times. Even a small percentage
    of improvement in performance can save hours, days, or even weeks in the case
    of extremely large models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C language is at the other end of the spectrum. While C code is hard to
    write, it executes blazingly fast when done correctly. As Python is written in
    C, a tried and true method for achieving fast performance is to call functions
    written in C from Python. In a nutshell, this is what NumPy provides: C arrays
    and operations, all in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: To get a glimpse into the deep underlying issues with Python’s built-in data
    structures, we should put numbers and arrays under our magnifying glass. Inside
    a computer’s memory, objects are represented as fixed-length 0-1 sequences. Each
    component is called a bit. Bits are usually grouped into 8-, 16-, 32-, 64-, or
    even 128 sized chunks. Depending on what we want to represent, identical sequences
    can mean different things. For instance, the 8-bit sequence 00100110 can represent
    the integer 38 or the ASCII character “&.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: An 8-bit object in memory'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying the data type, we can decode binary objects. 32-bit integers are
    called int32 types, 64-bit floats are float64, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Since a single bit contains very little information, memory is addressed by
    dividing it into 32- or 64-bit sized chunks and numbering them consecutively.
    This address is a hexadecimal number, starting from 0\. (For simplicity, let’s
    assume that the memory is addressed by 64 bits. This is customary in modern computers.)
  prefs: []
  type: TYPE_NORMAL
- en: A natural way to store a sequence of related objects (with matching data type)
    is to place them next to each other in the memory. This data structure is called
    an array.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: An array of int64 objects'
  prefs: []
  type: TYPE_NORMAL
- en: By storing the memory address of the first object, say 0x23A0, we can instantly
    retrieve the k-th element by accessing the memory at 0x23A0 + k.
  prefs: []
  type: TYPE_NORMAL
- en: We call this the static array or often the C array because this is how it is
    done in the magnificent C language. Although this implementation of arrays is
    lightning fast, it is relatively inflexible. First, you can only store objects
    of a single type. Second, you have to know the size of your array in advance,
    as you cannot use memory addresses that overextend the pre-allocated part. Thus,
    before you start working with your array, you have to allocate memory for it.
    (That is, reserve space so that other programs won’t overwrite it.)
  prefs: []
  type: TYPE_NORMAL
- en: However, in Python, you can store arbitrarily large and different objects in
    the same list, with the option of removing and adding elements to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the example above, l[0] is an integer so large that it doesn’t fit into 128
    bits. Also, there are all kinds of objects in our list, including a function.
    How is this possible?
  prefs: []
  type: TYPE_NORMAL
- en: Python’s list provides a flexible data structure by
  prefs: []
  type: TYPE_NORMAL
- en: Overallocating the memory, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keeping memory addresses to the objects in the list instead of the objects themselves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (At least in the most widespread CPython implementation ( [https://docs.python.org/3/faq/design.html\#how-are-lists-implemented-in-cpython](https://docs.python.org/3/faq/design.html/#how-are-lists-implemented-in-cpython)).)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: CPython implementation of lists'
  prefs: []
  type: TYPE_NORMAL
- en: By checking the memory addresses of each object in our list l, we can see that
    they are all over the memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Due to the overallocation, deletion or insertion can always be done simply by
    shifting the remaining elements. Since the list stores the memory address of its
    elements, all types of objects can be stored within a single structure.
  prefs: []
  type: TYPE_NORMAL
- en: However, this comes at a cost. Because the objects are not contiguous in memory,
    we lose locality of reference ([https://en.wikipedia.org/wiki/Locality_of_reference](https://en.wikipedia.org/wiki/Locality_of_reference)),
    meaning that since we frequently access distant locations of the memory, our reads
    are much slower. Thus, looping over a Python list is not efficient.
  prefs: []
  type: TYPE_NORMAL
- en: So, NumPy arrays are essentially the good old C arrays in Python, with the user-friendly
    interface of Python lists. (If you have ever worked with C, you know how big of
    a blessing this is.) Let’s see how to work with them!
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the numpy library. (To save on the characters, it is customary
    to import it as np.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The main data structure is np.ndarray, short for n-dimensional array. We can
    use the np.array function to create NumPy arrays from standard Python containers
    or initialize from scratch. (Yes, I know. This is confusing, but you’ll get used
    to it. Just take a mental note that np.ndarray is the class, and np.array is the
    function you use to create NumPy arrays from Python objects.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We can even initialize NumPy arrays using random numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Most importantly, when we have a given array, we can initialize another one
    with the same dimensions using the np.zeros_like, np.ones_like, and np.empty_like
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Just like Python lists, NumPy arrays support item assignments and slicing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: However, as expected, you can only store a single data type within each ndarray.
    When trying to assign a string as the first element, we get an error message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: As you might have guessed, every ndarray has a data type attribute that can
    be accessed at ndarray.dtype. If a conversion can be made between the value to
    be assigned and the data type, it is automatically performed, making the item
    assignment successful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: NumPy arrays are iterable, just like other container types in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Are these suitable to represent vectors? Yes. We’ll see why!
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.4 NumPy arrays as vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s talk about vectors once more. From now on, we are going to use NumPy ndarray-s
    to model vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The addition and scalar multiplication operations are supported by default and
    perform as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Because of the dynamic typing of Python, we can (often) plug NumPy arrays into
    functions intended for scalars.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, NumPy arrays satisfy almost everything we require to represent vectors.
    There is only one box to be checked: performance. To investigate this, we measure
    the execution time with Python’s built-in timeit tool.'
  prefs: []
  type: TYPE_NORMAL
- en: In its first argument, timeit ([https://docs.python.org/3/library/timeit.html](https://docs.python.org/3/library/timeit.html))
    takes a function to be executed and timed. Instead of passing a function object,
    it also accepts executable statements as a string. Since function calls have a
    significant computational overhead in Python, we are passing code rather than
    a function object in order to be more precise with the time measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we compare adding together two NumPy arrays vs. Python lists containing
    a thousand zeros.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: NumPy arrays are much-much faster. This is because they are
  prefs: []
  type: TYPE_NORMAL
- en: contiguous in memory,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: homogeneous in type,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: with operations implemented in C.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just the tip of the iceberg. We have only seen a small part of it, but
    NumPy provides much more than a fast data structure. As we progress in the book,
    we’ll slowly dig deeper and deeper, eventually discovering the vast array of functionalities
    it provides.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.5 Is NumPy really faster than Python?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NumPy is designed to be faster than vanilla Python. Is this really the case?
    Not all the time. If you use it wrong, it might even hurt performance! To know
    when it is beneficial to use NumPy, we will look at why exactly it is faster in
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the investigation, our toy problem will be random number generation.
    Suppose that we need just a single random number. Should we use NumPy? Let’s test
    it! We are going to compare it with the built-in random number generator by running
    both ten million times, measuring the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: For generating a single random number, NumPy is significantly slower. Why is
    this the case? What if we need an array instead of a single number? Will this
    also be slower?
  prefs: []
  type: TYPE_NORMAL
- en: This time, let’s generate a list/array of a thousand elements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: (Again, I don’t want to wrap the timed expressions in lambdas since function
    calls have an overhead in Python. I want to be as precise as possible, so I pass
    them as strings to the timeit function.)
  prefs: []
  type: TYPE_NORMAL
- en: Things are looking much different now. When generating an array of random numbers,
    NumPy wins hands down.
  prefs: []
  type: TYPE_NORMAL
- en: There are some curious things about this result as well. First, we generated
    a single random number 10000000 times. Second, we generated an array of 1000 random
    numbers 10000 times. In both cases, we have 10000000 random numbers in the end.
    Using the built-in method, it took ˜2x time when we put them in a list. However,
    with NumPy, we see a ˜30x speedup compared to itself when working with arrays!
    (The actual numbers might be different on your computer.)
  prefs: []
  type: TYPE_NORMAL
- en: To see what happens behind the scenes, we are going to profile the code using
    cProfiler ([https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)).
    With this, we’ll see exactly how many times a given function was called and how
    much time we spent inside it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the built-in function first. In the following function,
    we create 10000000 random numbers, just as before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: From Jupyter Notebooks, where this book is written, cProfiler can be called
    with the magic command %prun.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: There are two important columns here for our purposes. ncalls shows how many
    times a function was called, while tottime is the total time spent in a function,
    excluding time spent in subfunctions.
  prefs: []
  type: TYPE_NORMAL
- en: The built-in function random.random() was called 10000000 times as expected.
    Take note of the total time spent in the function. (I can’t give you an exact
    figure here, as it depends on the machine this book is built on.)
  prefs: []
  type: TYPE_NORMAL
- en: What about the NumPy version? The results are surprising.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, as before, the numpy.random.random() function was indeed called 10000000
    times, as expected. Yet, the script spent significantly more time in this function
    than in the Python built-in random before. Thus, it is more costly per call.
  prefs: []
  type: TYPE_NORMAL
- en: When we start working with large arrays and lists, things change dramatically.
    Next, we generate a list/array of 1000 random numbers, while measuring the execution
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: As we see, about 60% of the time was spent on the list comprehensions. (Note
    that tottime doesn’t count subfunction calls like calls to random.random() here.)
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to see why NumPy is faster when used right.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: With each of the 10000 function calls, we get a numpy.ndarray of 1000 random
    numbers. The reason why NumPy is fast when used right is that its arrays are extremely
    efficient to work with. They are like C arrays instead of Python lists.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, there are two significant differences between them.
  prefs: []
  type: TYPE_NORMAL
- en: Python lists are dynamic, so for instance, you can append and remove elements.
    NumPy arrays have fixed lengths, so you cannot add or delete without creating
    a new one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python lists can hold several data types simultaneously, while a NumPy array
    can only contain one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, NumPy arrays are less flexible but significantly more performant. When this
    additional flexibility is not needed, NumPy outperforms Python.
  prefs: []
  type: TYPE_NORMAL
- en: To see precisely at which size does NumPy overtakes Python in random number
    generation, we can compare the two by measuring the execution times for several
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '![PIC](img/file63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Runtime of random array generation'
  prefs: []
  type: TYPE_NORMAL
- en: Around 20, NumPy starts to beat Python in performance. Of course, this number
    might be different for other operations like calculating the sine or adding numbers
    together, but the tendency will be the same. Python will slightly outperform NumPy
    for small input sizes, but NumPy wins by a large margin as the size grows.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have learned what vectors are and why we must use them in
    data science and machine learning. Vectors are not just a bunch of numbers bundled
    together but a mathematical structure that allows us to reason about data more
    effectively, both in theory and in practice. Contrary to popular belief, vectors
    are vectors not because they have direction and magnitude but because you can
    add them together.
  prefs: []
  type: TYPE_NORMAL
- en: This is formalized by the concept of vector spaces, providing the mathematical
    framework for our studies. Vector spaces are best described by bases, that is,
    minimal and linearly independent generating sets. Understanding vector spaces
    and their bases will pay enormous dividends when we study linear transformations,
    the most important building block of predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the leap of abstraction provided by vectors, we reap significant benefits
    in practice by vectorizing our code, compressing complex logic into one-liners
    such as data scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Besides the conceptual jump from scalars to vectors and matrices, efficient
    data processing is made possible by NumPy (short for Numerical Python), the number
    one library in the machine learning toolkit. If a tensor library doesn’t use NumPy,
    it is inspired by it. We already understand its basics and know why and when to
    use it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue exploring vector spaces. Bases are cool and
    all, but besides them, vector spaces have a beautiful and rich geometric structure.
    Let’s see it!
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem 1\. Not all vector spaces are infinite. There are some that only contain
    a finite number of vectors, as we shall see next in this problem. Define the set
  prefs: []
  type: TYPE_NORMAL
- en: '![ℤ2 := {0,1}, ](img/file64.png)'
  prefs: []
  type: TYPE_IMG
- en: where the operations +,⋅ are defined by the rules
  prefs: []
  type: TYPE_NORMAL
- en: '| 0 + 0 | = 0 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0 + 1 | = 1 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1 + 0 | = 1 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1 + 1 | = 0 |  |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '| 0 ⋅ 0 | = 0 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0 ⋅ 1 | = 0 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1 ⋅ 0 | = 0 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1 ⋅ 1 | = 1. |  |  |'
  prefs: []
  type: TYPE_TB
- en: This is called binary (or modulo-2) arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: (a) Show that (ℤ[2],ℤ[2],+,⋅) is a vector space.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that(ℤ[2]^n,ℤ[2],+,⋅) is also a vector space, where ℤ[2]^n is the n-fold
    Cartesian product
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ℤ 2 = ℤ◟2-×-⋅⋅◝◜⋅×-ℤ2◞, n times ](img/file65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and the addition and scalar multiplication are defined elementwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| x+ y | = (x[1] + y[1],…,x[n] + y[n]), x,y ∈ℤ[2]^n, |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| cx | = (cx[1],…,cx[n]), c ∈ℤ[2]. |  |  |'
  prefs: []
  type: TYPE_TB
- en: Problem 2\. Are the following vector sets linearly independent?
  prefs: []
  type: TYPE_NORMAL
- en: (a) S[1] = {(1,0,0),(1,1,0),(1,1,1)}⊆ℝ³
  prefs: []
  type: TYPE_NORMAL
- en: (b) S[2] = {(1,1,1),(1,2,4),(1,3,9)}⊆ℝ³
  prefs: []
  type: TYPE_NORMAL
- en: (c) S[3] = {(1,1,1),(1,1,−1),(1,−1,−1)}⊆ℝ³
  prefs: []
  type: TYPE_NORMAL
- en: (d) S[4] = {(π,e),(−42,13∕6),(π³,−2)}⊆ℝ²
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3\. Let V be a finite n-dimensional vector space and let S = {v[1],…,v[m]}
    be a linearly independent set of vectors, m/span>n. Show that there is a basis
    set B such that S ⊂B.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 4\. Let V be a vector space and S = {v[1],…,v[n]} be its basis. Show
    that every vector x ∈V can be uniquely written as a linear combination of vectors
    in S. (That is, if x = ∑ [i=1]^nα[i]v[i] = ∑ [i=1]^nβ[i]v[i], then α[i] = β[i]
    for all i = 1,…,n.)
  prefs: []
  type: TYPE_NORMAL
- en: Problem 5\. Let V be an arbitrary vector space and U[1],U[2] ⊆V be two of its
    subspaces. Show that U[1] + U[2] = span(U[1] ∪U[2]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Hint: to prove the equality of these two sets, you need to show two things:
    1) if x ∈U[1] + U[2], then x ∈ span(U[1] ∪U[2]) as well, 2) if x ∈ span(U[1] ∪U[2]),
    then x ∈U[1] + U[2] as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem 6\. Consider the vector space of polynomials with real coefficients,
    defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![ n ℝ [x] = {p(x) = ∑ pxi : p ∈ ℝ, n = 0,1,...}. i i i=0 ](img/file66.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑n i xℝ[x] := {p(x) = pix : pi ∈ ℝ, n = 1,2,...} i=1 ](img/file67.png)'
  prefs: []
  type: TYPE_IMG
- en: is a proper subspace of ℝ[x].
  prefs: []
  type: TYPE_NORMAL
- en: (b) Show that
  prefs: []
  type: TYPE_NORMAL
- en: '![f : ℝ[x] → xℝ [x ], p(x) ↦→ xp(x) ](img/file68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'is a bijective and linear. (A function f : X →Y is bijective if every y ∈Y
    has exactly one x ∈X for which f(x) = y. If you are not comfortable with this
    notion, feel free to revisit this problem after [Chapter 9](#).)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a linear and bijective function f : U →V between vector spaces
    is called an isomorphism. Given the existence of such a function, we call the
    vector spaces U and V isomorphic, meaning that they have an identical algebraic
    structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining (a) and (b), we obtain that ℝ[X] is isomorphic with its proper subspace
    xℝ[X]. This is quite an interesting phenomenon: a vector space that is algebraically
    identical to its proper subspace. (Note that this cannot happen in finite dimensions,
    such as ℝ^n.)'
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read this book alongside other users, Machine Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/math](https://packt.link/math)
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG
